<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">16420</article-id><article-id pub-id-type="doi">10.7554/eLife.16420</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Associative learning changes cross-modal representations in the gustatory cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-55042"><name><surname>Vincis</surname><given-names>Roberto</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5812-7624</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-54300"><name><surname>Fontanini</surname><given-names>Alfredo</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4561-9563</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Neurobiology and Behavior</institution>, <institution>State University of New York at Stony Brook</institution>, <addr-line><named-content content-type="city">Stony Brook</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mrsic-Flogel</surname><given-names>Thomas D</given-names></name><role>Reviewing editor</role><aff id="aff2"><institution>University of Basel</institution>, <country>Switzerland</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>roberto.vincis@stonybrook.edu</email> (RV);</corresp><corresp id="cor2"><email>alfredo.fontanini@stonybrook.edu</email> (AF)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>30</day><month>08</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e16420</elocation-id><history><date date-type="received"><day>26</day><month>03</month><year>2016</year></date><date date-type="accepted"><day>16</day><month>08</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Vincis et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Vincis et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-16420-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.16420.001</object-id><p>A growing body of literature has demonstrated that primary sensory cortices are not exclusively unimodal, but can respond to stimuli of different sensory modalities. However, several questions concerning the neural representation of cross-modal stimuli remain open. Indeed, it is poorly understood if cross-modal stimuli evoke unique or overlapping representations in a primary sensory cortex and whether learning can modulate these representations. Here we recorded single unit responses to auditory, visual, somatosensory, and olfactory stimuli in the gustatory cortex (GC) of alert rats before and after associative learning. We found that, in untrained rats, the majority of GC neurons were modulated by a single modality. Upon learning, both prevalence of cross-modal responsive neurons and their breadth of tuning increased, leading to a greater overlap of representations. Altogether, our results show that the gustatory cortex represents cross-modal stimuli according to their sensory identity, and that learning changes the overlap of cross-modal representations.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.001">http://dx.doi.org/10.7554/eLife.16420.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.16420.002</object-id><title>eLife digest</title><p>Imagine that you are waiting for a cappuccino at your favorite café. You hear the sound of the steamer, and shortly afterwards the barista calls your name and announces that your cappuccino is ready. As they hand it to you, you see the foam sprinkled with cocoa and the aroma of the cappuccino reaches your nose. You can almost taste it. When you finally take your first sip, the taste is hardly a surprise; it is just as your eyes and nose predicted.</p><p>How does the brain deal with such a rich and multisensory experience? How does it learn to associate the sight and smell of a cappuccino with its taste? Specialized regions of the brain called associative areas were traditionally thought to perform this task. These areas receive inputs from every sensory system and can link information from these different sources together. According to this view, the job of each individual sensory system is to pass along information relevant to one particular sense.</p><p>More recent results, however, challenge this strict division of labor and suggest that individual sensory systems may be able to combine information from multiple senses. Thus the sights, sounds and odors associated with our cappuccino may also activate the area of the brain in charge of processing taste: the gustatory cortex. To investigate this possibility, Vincis and Fontanini set out to determine whether neurons in the gustatory cortex of rats can process stimuli belonging to senses other than taste.</p><p>As predicted, neurons in the gustatory cortex did change their firing rates in response to odors, touch, sounds and light. However, more of the gustatory neurons responded to odors and touch than to sounds and light. In addition, of the four stimuli, the rats most easily learned to associate odors and touch with a sugary solution. This is consistent with the fact that rodents rely more upon their whiskers and their sense of smell to find food they do their eyes and ears. Finally, learning to associate a stimulus other than taste with a sugary solution increased the number of neurons in the gustatory cortex that subsequently responded to other senses and changed their response properties.</p><p>Further studies are now required to answer three questions. Why can some senses more effectively influence the activity of the gustatory cortex than others? Can gustatory neurons distinguish between different stimuli of the same type – different odors, for example? What are the neural pathways that convey multisensory information to the gustatory cortex? Answering these questions will help us to better understand how sensory systems link information from multiple senses.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.002">http://dx.doi.org/10.7554/eLife.16420.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>multimodal</kwd><kwd>gustatory cortex</kwd><kwd>electrophysiology</kwd><kwd>expectation</kwd><kwd>learning</kwd><kwd>behavior</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>P2GEP3_151816</award-id><principal-award-recipient><name><surname>Vincis</surname><given-names>Roberto</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>P300PA_161021</award-id><principal-award-recipient><name><surname>Vincis</surname><given-names>Roberto</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000055</institution-id><institution>National Institute on Deafness and Other Communication Disorders</institution></institution-wrap></funding-source><award-id>R01-DC010389</award-id><principal-award-recipient><name><surname>Fontanini</surname><given-names>Alfredo</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Cross-modal stimuli are represented in the primary gustatory cortex according to their sensory identity, associability and predictive value.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Traditionally, sensory cortices have been studied for their ability to encode stimuli of a single sensory modality. However, a growing body of literature has shown that activity in primary sensory cortices can also be modulated by cross-modal stimuli (<xref ref-type="bibr" rid="bib25">Ghazanfar and Schroeder, 2006</xref>). Neurons in the visual cortex respond to acoustic stimuli (<xref ref-type="bibr" rid="bib31">Iurilli et al., 2012</xref>) and activity in the auditory cortex is also shaped by visual (<xref ref-type="bibr" rid="bib4">Calvert et al., 1997</xref>; <xref ref-type="bibr" rid="bib3">Brosch et al., 2005</xref>) and somatosensory stimulation (<xref ref-type="bibr" rid="bib85">Zhou and Fuster, 2000</xref>; <xref ref-type="bibr" rid="bib60">Schroeder et al., 2001</xref>; <xref ref-type="bibr" rid="bib19">Fu et al., 2003</xref>; <xref ref-type="bibr" rid="bib3">Brosch et al., 2005</xref>; <xref ref-type="bibr" rid="bib40">Lakatos et al., 2007</xref>). Similar results have been observed in the olfactory (<xref ref-type="bibr" rid="bib76">Wesson and Wilson, 2010</xref>; <xref ref-type="bibr" rid="bib45">Maier et al., 2012</xref>, <xref ref-type="bibr" rid="bib44">2015</xref>), gustatory (<xref ref-type="bibr" rid="bib11">De Araujo and Rolls, 2004</xref>; <xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>) and somatosensory cortices (<xref ref-type="bibr" rid="bib85">Zhou and Fuster, 2000</xref>). While these studies have been fundamental in demonstrating cross-modality and its potential functions (<xref ref-type="bibr" rid="bib25">Ghazanfar and Schroeder, 2006</xref>; <xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>; <xref ref-type="bibr" rid="bib66">Stein et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Kusumoto-Yoshida et al., 2015</xref>; <xref ref-type="bibr" rid="bib82">Yau et al., 2015</xref>), several key issues regarding the representation of cross-modal stimuli remain largely unaddressed. Specifically, there are two pressing questions that we investigated in this study.</p><p>The first question pertains to the neural representation in a primary sensory cortex of multiple stimuli belonging to different sensory modalities. Studies from the literature relied mostly on cross-modal stimuli of a single or a couple of modalities (<xref ref-type="bibr" rid="bib24">Ghazanfar et al., 2005</xref>; <xref ref-type="bibr" rid="bib37">Kayser and Logothetis, 2007</xref>; <xref ref-type="bibr" rid="bib40">Lakatos et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Driver and Noesselt, 2008</xref>; <xref ref-type="bibr" rid="bib68">Vasconcelos et al., 2011</xref>; <xref ref-type="bibr" rid="bib31">Iurilli et al., 2012</xref>). As a result, it is not known whether cross-modal stimuli pertaining to distinct sensory modalities converge on the same set of sensory cortical neurons or whether they recruit different groups of neurons. The former possibility is compatible with cross-modal responses reflecting a general, modality-independent arousal or distracting signal (<xref ref-type="bibr" rid="bib67">Talsma et al., 2010</xref>; <xref ref-type="bibr" rid="bib31">Iurilli et al., 2012</xref>) and suggests the existence of broadly tuned, multi-modal neurons capable of being activated by any cross-modal stimulus. The latter possibility proposes that primary sensory areas can represent the sensory modality of cross-modal stimuli and predicts the presence of stimulus-specific unimodal representations.</p><p>The second question focuses on whether cross-modal representations can be shaped by associative learning. Most of the studies in the literature relied on cross-modal stimuli that had not been explicitly associated with the modality of the area under investigation (<xref ref-type="bibr" rid="bib25">Ghazanfar and Schroeder, 2006</xref>; <xref ref-type="bibr" rid="bib38">Kayser et al., 2007</xref>). Previous studies focusing on sensory neocortices and on the gustatory cortex suggested that associative learning could dramatically change how predictive cues modulate cortical neurons (<xref ref-type="bibr" rid="bib85">Zhou and Fuster, 2000</xref>; <xref ref-type="bibr" rid="bib3">Brosch et al., 2005</xref>; <xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>). These studies, however, relied on predictive cues pertaining to a few sensory modalities. As a result, little information is available on the effects of associative learning on the neural representations of cross-modal stimuli of all sensory modalities.</p><p>In this study, we addressed these two fundamental questions by studying cross-modal responses in the gustatory cortex (GC) – an area that represents a good model for studying cross-modality (<xref ref-type="bibr" rid="bib35">Katz et al., 2002</xref>; <xref ref-type="bibr" rid="bib63">Simon et al., 2006</xref>; <xref ref-type="bibr" rid="bib5">Carleton et al., 2010</xref>; <xref ref-type="bibr" rid="bib69">Veldhuizen et al., 2010</xref>). We presented stimuli belonging to four sensory modalities: auditory, visual, somatosensory, or olfactory. Cross-modal stimuli were presented to two, distinct groups of animals: a group not trained to associate cross-modal stimuli with sucrose (or any other taste) and another in which rats were trained to associate cross-modal stimuli with sucrose. The first cohort of rats served to determine if GC could respond to the cross-modal stimuli used and to characterize their baseline representations. The second cohort allowed us to determine if and how associative learning modified baseline cross-modal representations. Single unit recordings in alert, untrained rats revealed that GC neurons could represent the identity of cross-modal stimuli via largely non-overlapping representations and show a strong bias toward olfactory and somatosensory stimuli. Olfactory and somatosensory stimuli also led to faster associative learning compared to visual and auditory stimuli. Comparisons between neural responses in untrained and trained rats demonstrated that, upon learning, GC maintained the ability to discriminate different sensory modalities, while also developing associative responses. Association with a common gustatory outcome increased the prevalence of neurons responding to cues, widened the overlap between neural representations and enhanced the similarity between responses to cues with similar associability.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Representation of cross-modal stimuli in GC</title><p>We first investigated how GC neurons encode cross-modal stimuli that have not been explicitly associated with taste. We recorded single-neuron spiking activity and orofacial movements of alert rats (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>; n = 5; untrained) in response to random deliveries of the following sensory stimuli: an odor, a puff of air on the distal portion of the rats’ whiskers (<italic>air puff</italic>), a light and a tone. Sucrose, NaCl, citric acid and quinine were also delivered to test each neuron’s response to taste. (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>; see Materials and methods). Gustatory stimuli were delivered intraorally via a manifold inserted into an intra-oral cannula. Intraoral delivery was chosen for complete control of stimulus delivery and for consistency with previous studies on expectation in GC (<xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>, <xref ref-type="bibr" rid="bib58">2013</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>; <xref ref-type="bibr" rid="bib43">Liu and Fontanini, 2015</xref>). The long and variable inter trial interval (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>; 50 ± 10 s) was designed to avoid associations between cross-modal, 'non-gustatory' stimuli and tastants.</p><p>To assess single neuron responses to cross-modal and gustatory stimuli, we analyzed stimulus-evoked changes in firing rates using a change point analysis (<xref ref-type="bibr" rid="bib33">Jezzini et al., 2013</xref>; <xref ref-type="bibr" rid="bib43">Liu and Fontanini, 2015</xref>; <xref ref-type="bibr" rid="bib46">Mazzucato et al., 2015</xref>). Briefly, for each neuron, we computed the cumulative distribution function (CDF) of spike occurrences across all trials within a temporal window starting 1 s before stimulus onset (i.e., baseline) and ending 2 s (for cross modal stimuli) or 2.5 s (for gustatory stimuli) after stimulus presentation. Sudden changes in firing rates resulted in a change in the CDF slope and in the identification of a 'change point' (CP). A binomial test (p&lt;0.05) was used to compare spike counts before and after each CP to evaluate their statistical significance (<xref ref-type="bibr" rid="bib21">Gallistel et al., 2004</xref>). This method was used because of its sensitivity and reliability.</p><p>As expected, we observed that a substantial percentage of neurons significantly changed their firing rates following gustatory stimulation (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; 38.5% [52/135], 41.4% [56/135], 41.4% [56/135] and 38.5% [52/135] for sucrose, NaCl, citric acid and quinine, respectively). In addition, a consistent proportion of neurons were modulated by other sensory modalities (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; 33.3% [45/135] of recorded neurons responded to at least one non-gustatory stimulus; odor: 16.2% [22/135]; air puff: 15.5% [21/135]; tone: 4.4% [6/135]; light: 3.7% [5/135]). Orofacial movements evoked by non-taste stimuli were hardly detectable and significantly smaller compared to the ones evoked by tastants, indicating that they were not driving the changes in neuronal activity we observed (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>; Wilcoxon sign rank test: p&lt;0.05). The absence of a relationship between responses to cross-modal stimuli and mouth movements can also be visualized for the two representative neurons depicted in <xref ref-type="fig" rid="fig1">Figure 1D–G</xref>. Among the non-gustatory stimuli, odors and somatosensory stimuli appeared to be the most effective in modulating GC neurons. When comparing the prevalence of cross-modal responses for the different stimuli, we observed significant differences in the proportion of neurons that responded to cross-modal stimuli (Pearson’s <bold>χ</bold><sup>2</sup> test; <bold>χ</bold><sup>2</sup><sub>(3)</sub> = 17.35, p&lt;0.001). Specifically, while the proportion of neurons responding to odorant and air puff were similar (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; Marascuillo’s test, p&gt;0.05), both were significantly higher compared to the proportion of neurons responding to sound and light (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; Marascuillo’s test, p&gt;0.05). We then investigated the convergence of stimuli of different sensory modalities onto single GC neurons. We first identified, within the group of neurons whose firing rates were modulated by at least one taste delivery, those that were taste selective. A neuron was deemed to be taste selective if it showed significantly different responses to the four gustatory stimuli (<xref ref-type="bibr" rid="bib33">Jezzini et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Samuelsen et al., 2013</xref>). Within the group of taste selective neurons (87/135), the majority responded exclusively to gustatory stimuli (<xref ref-type="fig" rid="fig1">Figure 1B, D–E</xref>; 70.1%, [61/87]), with only a few also activated by one (<xref ref-type="fig" rid="fig1">Figure 1B, F–G</xref>; 25.3%, [22/87]) or two (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; 4.6%, [4/87]) cross modal stimuli. The percentage of neurons being both taste selective and responsive to cross-modal stimuli was not significantly different from what expected if the probabilities of the two conditions were independent (bootstrap procedure, p&gt;0.05). We then turned our attention to the group of neurons responsive to cross-modal stimuli (45/135) and analyzed the convergence of non-gustatory responses (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). The majority of cross-modal neurons were activated exclusively by one of the non-taste stimuli (either air puff, odor, tone or light stimuli; <xref ref-type="fig" rid="fig1">Figure 1G</xref>; 84.4%, [38/45]) and only a few neurons responded to two (11.1%, [5/45]) or three (4.4%, [2/45]) cross-modal stimuli. These percentages were not significantly different from those expected by chance given the fractions of neurons responding to different cues (bootstrap procedure, p&gt;0.05 for all cases).<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.16420.003</object-id><label>Figure 1.</label><caption><title>Neural representation of different sensory modalities in GC of untrained rats.</title><p>(<bold>A</bold>) Percentage of neurons (n = 135) modulated (excited [gray] or inhibited [white]) by each stimulus. Odor and somatosensory (air puff) stimuli modulate a larger number of neurons compared to tone and light. Asterisks indicate <italic>post-hoc</italic> test corrected for multiple comparisons (Marascuillo’s test, p&lt;0.05). (<bold>B</bold>) Pie chart showing the proportion of taste selective neurons (n = 87), that are modulated exclusively by taste (Taste only) or by taste and either one (Taste &amp; 1) or two (Taste &amp; 2) cross-modal stimuli. No neuron was modulated by taste and three or four cross-modal stimuli. (<bold>C</bold>) Pie chart showing the proportion of cross-modal responsive neuron (n = 45), modulated by one (1 Stimulus), two (2 Stimuli) or three (3 Stimuli) out of the four stimuli used. No neuron was modulated by all four the stimuli. (<bold>D</bold>) Raster plots and PSTHs of a representative GC neuron showing a broadly tuned response to the four tastants. (<bold>E</bold>) Raster plots and PSTHs of a GC neuron (same as the one showed in panel D) showing no response to cross-modal stimuli (air puff, odor, tone and light). (<bold>F</bold>) Raster plots and PSTHs of a second representative GC neuron showing a selective response to taste. (<bold>G</bold>) Raster plots and PSTHs of a GC neuron (same as the one showed in panel D) showing a selective response to a puff of air and no responses to Odor, Tone and Light. For D and F: vertical lines at time 0 indicate the onset of the stimulus. S for sucrose, N for NaCl, C for citric acid and Q for quinine. For E and G: grey shaded areas indicate the time and duration of cross-modal stimulus presentation. Red dotted line represents the time-course of orofacial movement activity.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.003">http://dx.doi.org/10.7554/eLife.16420.003</ext-link></p><p><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.16420.004</object-id><label>Figure 1—source data 1.</label><caption><title>Percentage of GC neurons responding to cross-modal and gustatory stimuli in untrained rats.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.004">http://dx.doi.org/10.7554/eLife.16420.004</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-16420-fig1-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-16420-fig1-v2"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.16420.005</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Electrodes placement and experimental design for the group of untrained rats.</title><p>(<bold>A</bold>) Schematic representation of a coronal section of rat’s brain highlighting the dorso-ventral range of recording and electrodes placement in the gustatory cortex for the right (light blue) and left (light orange) hemisphere. (<bold>B</bold>) Schematic of the experimental design for each trial. Red bars highlight stimulus delivery. Each tastants is delivered through the IOCs in aliquots of 40 µl (duration of pulse: 25–30 ms) while each cross-modal stimuli is presented for 2 s. (<bold>C</bold>) Average mouth movements evoked by tastants (black) and by cross-modal stimuli (blue). Red dotted line indicates stimulus onset. Asterisk indicates p&lt;0.05 Wilcoxon Test.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.005">http://dx.doi.org/10.7554/eLife.16420.005</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-16420-fig1-figsupp1-v2"/></fig></fig-group></p><p>Altogether, our data indicate that GC neurons are modulated by tastants and by stimuli of other sensory modalities. Among non-tastants, olfactory and somatosensory stimuli recruited more neurons compared to visual and auditory stimuli. The representations of stimuli belonging to different sensory modalities overlapped only minimally.</p></sec><sec id="s2-2"><title>Learning associations between cross-modal stimuli and sucrose</title><p>We investigated if the different cross-modal stimuli were equally effective in driving learning of cue-sucrose associations. A second group of rats (trained, n = 5) was conditioned to associate each of the four cross-modal, 'non-gustatory' sensory cues with the intraoral delivery of sucrose (the unconditioned stimulus). In every daily session (for a total of 14 days), rats experienced twenty trials of cue-sucrose pairings for each modality (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>; see Materials and methods). For each trial, the sensory modality of the cue presented was chosen in a pseudo-random manner. In order to track associative learning we monitored orofacial movements in response to conditioned stimuli (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>). A significant increase of mouth movements during the cue compared to baseline indicated the appearance of a conditioned response and allowed us to establish learning (<xref ref-type="fig" rid="fig2">Figure 2A</xref> <italic>leftmost panels</italic>). Analysis of orofacial movements revealed that after 14 days of training rats learned to associate each anticipatory cue with the delivery of sucrose (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The conditioned orofacial movements were significantly stronger on the last day of training compared to the first day (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; somatosensory: two-way ANOVA [factors: days and temporal epochs], F<sub>(1,2)</sub> = 43.8, p&lt;0.01; odor: two-way ANOVA, F<sub>(1,2)</sub> = 45.3, p&lt;0.01; tone: two-way ANOVA, F<sub>(1,2)</sub> = 47.7, p&lt;0.01; light: two-way ANOVA, F<sub>(1,2)</sub> = 76.6, p&lt;0.01). <italic>Post-hoc</italic> analyses revealed that, for each sensory modality, the difference in mouth movements across days was specific to the cue presentation epoch (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; 'cue': p&lt;0.01 for days effect with Tukey’s correction). No difference across days was observed for spontaneous and taste-evoked mouth movements (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; 'spontaneous': somatosensory: p=0.4, odor: p=0.9, tone: p=0.9, light: p=0.9; 'taste': somatosensory: p=0.9, odor: p=0.9, tone: p=0.8, light: p=0.9; Tukey’s correction).<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.16420.006</object-id><label>Figure 2.</label><caption><title>Associative learning for different cross-modal stimuli.</title><p>(<bold>A</bold>) Left most panel: Average time course of orofacial movements evoked by cross-modal cues and sucrose during the first (gold) and 14th (cyan) day of classical conditioning (n = 5 rats). Shaded grey area and red dotted line indicate cues and sucrose presentation respectively. The shaded area around the curve represents ± SEM. Right most panel: average across trained rats (n = 5) of mouth movements (assessed by changes in pixel intensity) in different temporal epochs (1 s before cue onset [−1 to 0 s; Spont.], 1 s after cue onset [0.5–1.5 s; Cue] and 1 s after sucrose onset [2.7–3.7 s; Taste]) for the first (gold) and 14th (cyan) day of classical conditioning. (<bold>B</bold>) Average (n = 5 rats) of mouth movements across the 14 conditioning days. For A and B: asterisks indicate <italic>post-hoc</italic> tests corrected for multiple comparisons (Tukey, p&lt;0.05). Rows from 1 to 4 show the orofacial activity evoked by air puff, odor, tone and light respectively.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.006">http://dx.doi.org/10.7554/eLife.16420.006</ext-link></p><p><supplementary-material id="SD2-data"><object-id pub-id-type="doi">10.7554/eLife.16420.007</object-id><label>Figure 2—source data 1.</label><caption><title>Values of the orofacial movements evoked by the cross-modal stimuli.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.007">http://dx.doi.org/10.7554/eLife.16420.007</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-16420-fig2-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-16420-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.16420.008</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Experimental design for cue-taste association experiments and assessment of conditioned responses.</title><p>(<bold>A</bold>) Schematic of the experimental design used for cue-taste Pavlovian classical conditioning. Red bars highlight stimulus delivery. Between cue-offset and the delivery of sucrose there is a delay of 500 ms. Each cue is presented in a pseudo-random fashion and always precedes the delivery of sucrose. Conditioning lasted 14 days with and every day each rat received 20 trials of each cue-sucrose pair. See experimental procedures for further details. (<bold>B</bold>) Representative record of mouth movements, averaged across trials (Cue is odor; the trace pertain to the first day of training) for a single session (first day of classical conditioning). Note the different temporal epochs used for assessing the time-course of classical conditioning. Sucrose is delivered through IOCs in aliquots of 40 µl. Each cross modal stimulus is presented for 2 s. Between the cross modal stimuli offset and sucrose onset there is a constant delay of 500 ms.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.008">http://dx.doi.org/10.7554/eLife.16420.008</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-16420-fig2-figsupp1-v2"/></fig></fig-group></p><p>We then studied how cue-taste learning unfolds for the different sensory modalities over 14 days. This analysis was performed to determine if cross-modal cues different in associability. Since odor and tactile stimuli were more represented in GC before any cue-taste association was established (<xref ref-type="fig" rid="fig1">Figure 1</xref>), we hypothesized that this bias may predict a difference in learning rate. Indeed, odor and somatosensory cues evoked mouth movements significantly larger than baseline (defined as orofacial movements on the first day of conditioning) on the third day of training (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; p&lt;0.05 with Tukey’s correction). In contrast, visual and auditory cues required seven days of conditioning to display significant changes in orofacial movements (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; p&lt;0.05 with Tukey’s correction). Regardless of the difference in learning rate, the magnitude of the mouth movements evoked by each anticipatory cue was not significantly different on the last day of conditioning (one-way ANOVA [factor: cross-modal cues], F<sub>(3)</sub> = 2.24, p=0.12).</p><p>In conclusion, analysis of conditioned responses revealed that each cross-modal stimulus could be successfully associated with taste within 14 days. Odors and somatosensory stimuli have a higher associability to sucrose compared to visual and auditory cues.</p></sec><sec id="s2-3"><title>Representation of cues after associative learning</title><p>In order to establish if associative learning changes how GC represents information pertaining to cross-modal stimuli, neural activity was recorded after 14 days of conditioning (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Random passive intraoral delivery of one out of four tastants during the experimental session allowed us to define if a GC neuron was taste selective (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Comparison of responses to cross-modal stimuli between trained and untrained animals revealed significant differences. Indeed, cue-taste association correlated with a significantly higher prevalence of neurons that responded to cues of different sensory modalities compared to the group of untrained rats (<xref ref-type="fig" rid="fig4">Figure 4A</xref>; test for equality of proportion for all comparison; odor: 16.2% [22/135] untrained <italic>vs.</italic> 33.0% [39/118] trained, <bold>χ</bold><sup>2</sup><sub>(1)</sub> = 9.65, p&lt;0.01; air puff: 15.5% [21/135] untrained <italic>vs.</italic> 34.7% [41/118] trained, <bold>χ</bold><sup>2</sup><sub>(1)</sub> = 12.53, p&lt;0.01; tone: 4.4% [6/135] untrained <italic>vs.</italic> 16.1% [19/118] trained, <bold>χ</bold><sup>2</sup><sub>(1)</sub> = 9.60, p&lt;0.01; light: 3.7% [5/135] untrained <italic>vs.</italic> 11.8% [14/118] trained, <bold>χ</bold><sup>2</sup><sub>(1)</sub> = 6.03, p&lt;0.05). The increase was specific for responses to cross-modal stimuli, as learning did not change the prevalence of taste selective neurons (64.4% [87/135] for untrained <italic>vs.</italic> 57.6% [68/118] for trained, test for equality of proportion, <bold>χ</bold><sup>2</sup><sub>(1)</sub> = 1.23, p=0.26). Similarly conserved in trained rats was the bias for olfactory and somatosensory stimuli; indeed, significantly more neurons responded to air puff and odor compared to light and sound (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, Marascuillo’s test, p&gt;0.05).<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.16420.009</object-id><label>Figure 3.</label><caption><title>Neural representation of different sensory modalities after cue-taste association.</title><p>(<bold>A</bold>) Percentage of neurons (n = 118) modulated (excited [gray] or inhibited [white]) by each tastants and anticipatory cues. Asterisks indicate <italic>post-hoc</italic> test corrected for multiple comparisons (Marascuillo’s test, p&lt;0.05). (<bold>B</bold>) Pie chart showing the proportion of taste selective neuron (n = 68) that are modulated exclusively by taste (Taste only) or by taste and one (Taste &amp; 1), two (Taste &amp; 2), three (Taste &amp; 3) or four (Taste &amp; 4) anticipatory cues. (<bold>C</bold>) Pie chart showing the proportion of cue responsive neuron (n = 69), which are only modulated by one (1 Cue), two (2 Cues), three (3 Cues) or all anticipatory cues (4 Cues). (<bold>D</bold>) Raster plot and PSTH of a representative GC neuron showing a significant excitatory response to sucrose. (<bold>E</bold>) Raster plot and PSTH of a GC neuron (same as the one showed in panel D) featuring significant excitatory responses to Air puff and Odor and no responses to Tone and Light. (<bold>F</bold>) Raster plot and PSTH of a GC neuron showing inhibitory responses to multiple tastants. (<bold>G</bold>) Raster plot and PSTH of a GC neuron (same as the one showed in panel F) displaying significant inhibitory responses to multiple cues (air puff, odor, tone and light). For D and F: vertical lines at time 0 indicate the onset of the taste delivery. S for sucrose, N for NaCl, C for citric acid and Q for quinine. For E and G: gray rectangular areas indicate the period of cue presentation. Vertical dotted lines at time 2.5 indicate the onset of sucrose delivery. Red dotted line represents the time-course of orofacial movement activity.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.009">http://dx.doi.org/10.7554/eLife.16420.009</ext-link></p><p><supplementary-material id="SD3-data"><object-id pub-id-type="doi">10.7554/eLife.16420.010</object-id><label>Figure 3—source data 1.</label><caption><title>Percentage of GC neurons responding to cross-modal and gustatory stimuli in trained rats.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.010">http://dx.doi.org/10.7554/eLife.16420.010</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-16420-fig3-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-16420-fig3-v2"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.16420.011</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Electrode placement and experimental design for the group of trained rats.</title><p>(<bold>A</bold>) Schematic representation of a coronal section of rat’s brain highlighting the dorso ventral range of recording and electrodes placement in the gustatory cortex for the right (light blue) and left (light orange) hemisphere. (<bold>B</bold>) Sketch of the experimental design for each trial. Red bars highlight stimulus delivery. See experimental procedures for further details. Each tastants is delivered through the IOCs in aliquots of 40 µl. Each cross modal stimulus is presented for 2 s. Between the cross modal stimuli offset and sucrose onset there is a constant delay of 500 ms.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.011">http://dx.doi.org/10.7554/eLife.16420.011</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-16420-fig3-figsupp1-v2"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.16420.012</object-id><label>Figure 3—figure supplement 2.</label><caption><title>The neural bias for somatosensory and olfactory stimuli does not depend on the number of sessions at final performance level.</title><p>(<bold>A</bold>) Percentage of GC neurons modulated by cross-modal stimuli after matching the number of recording session at final performance level. (<bold>B</bold>) Distribution of the number of cross-modal neurons across recording days following reaching final performance level. Each full circle represents the number of responsive neurons recorded for each experimental day. Different colors represent responses to different cross-modal stimuli (see legend). Straight lines represent the best linear fit through each point.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.012">http://dx.doi.org/10.7554/eLife.16420.012</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-16420-fig3-figsupp2-v2"/></fig></fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.16420.013</object-id><label>Figure 4.</label><caption><title>Comparison of cross-modal GC responsiveness between untrained and trained animals.</title><p>(<bold>A</bold>) Percentage of GC neurons that are modulated by cross-modal stimuli in untrained (blue) and trained (black) rats. (<bold>B</bold>) Percentage of taste selective GC neurons that are modulated only by tastants (Taste Only) or by tastants and at least one cross-modal stimulus (Taste &amp; Cue); blue: untrained, black: trained. (<bold>C</bold>) Percentage of cue responsive GC neurons that are modulated only by one or more anticipatory cross-modal cues in untrained (blue) and in trained (black) rats. Asterisk indicates p&lt;0.05 for test for equality of proportion.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.013">http://dx.doi.org/10.7554/eLife.16420.013</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-16420-fig4-v2"/></fig></p><p>Could the post-learning bias for olfactory and somatosensory stimuli depend on the fact that they had been learned faster than visual and auditory stimuli and hence had been presented for more sessions after the establishment of a cue-sucrose association? Additional analyses were performed to determine if the distribution of neurons responsive to cross-modal stimuli depended on the number of sessions spent at the final performance level. To address this issue we computed the fraction of neurons responsive to cross-modal stimuli for sessions recorded on the same number of days (i.e., 12, 13 and 14 days) after reaching final performance level (i.e., days 15, 16, 17 for odor and air puff and days 19, 20, 21 for tone and light). As <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> shows, the bias toward olfactory and somatosensory stimuli persisted within this matched sample of sessions. In addition, a second analysis was performed to determine directly if the number of cue responsive neurons increased over the course of subsequent sessions (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). No increase and no difference in the number of cross-modal responsive neurons were observed with increasing the number of sessions at final performance level. These results demonstrate that the magnitude of cross-modal representations is not related to exposition to the stimulus after learning.</p><p>Next, the convergence of stimuli belonging to different sensory modalities onto single GC neurons was analyzed in trained animals. First, we focused our attention on taste selective neurons (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Out of 68 neurons that were taste selective, 39.7% were activated exclusively by gustatory stimulation (<xref ref-type="fig" rid="fig3">Figure 3B</xref>; 27/68). The remaining taste selective neurons were also activated by one or more anticipatory cues; specifically 35.3% (24/68) were modulated by one cue, 16.1% (11/68) by two, 7.3% (5/68) by three and 1.4% (1/68) by all four. The proportion of neurons that were both taste selective and cue responsive is consistent with what expected from the independent probabilities of the two conditions (bootstrap procedure, p&gt;0.05). However, and more importantly, the convergence between gustatory and cross-modal information was significantly different between untrained and trained animals. When compared with untrained rats, trained rats showed a significant reduction in the number of single units responsive exclusively to taste (<xref ref-type="fig" rid="fig4">Figure 4B</xref>; 70.1% [61/87] for untrained and 39.7% [27/68] for trained rats: test for equality of proportion;<bold>χ</bold><sup>2</sup><sub>(1)</sub> = 14.38, p&lt;0.001) and a significant increase in the number of neurons responding to both taste and cues (<xref ref-type="fig" rid="fig4">Figure 4B</xref>; 33.3% [29/87] for untrained and 60.2% [41/68] for trained rats: test for equality of proportion;<bold>χ</bold><sup>2</sup><sub>(1)</sub> = 11.20, p&lt;0.001). The latter results are consistent with the increase in cue responsive neurons observed after associative learning.</p><p>We then investigated how many cue responsive neurons were activated by more than one cue. Out of 69 neurons, which responded at least to one cross-modal stimulus, half of them (50.7%, [35/69]) were activated only by one out of four anticipatory cues (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). However, a substantial amount of cue responsive neurons were also responsive to two (36.2%, [25/69]), three (11.5%, [8/69]) and four (1.4%, [1/69]) cues. The number of neurons responding to more than one cue was not different from what predicted from the individual probabilities of occurrence (bootstrap procedure, p&gt;0.05 for all cases). More relevantly, there was a significant difference in the percentage of neurons responding to one and two cues between the untrained and trained groups (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Specifically, in the trained groups of rats, we observed fewer neurons that were responsive only to one cue (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; untrained: 84.4% [38/45]; trained: 50.7% [35/69]; test for equality of proportion;<bold>χ</bold><sup>2</sup><sub>(1)</sub> = 9.16, p&lt;0.01), and more neurons that were activated by two cues (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; untrained: 11.1% [5/45]; trained: 36.2% [25/69]; test for equality of proportion; <bold>χ</bold><sup>2</sup><sub>(1)</sub> = 5.60, p&lt;0.05). <xref ref-type="fig" rid="fig3">Figure 3D–G</xref> show two examples of GC neurons that are taste selective and modulated by 2 (<xref ref-type="fig" rid="fig3">Figure 3D–E</xref>) and 3 (<xref ref-type="fig" rid="fig3">Figure 3F–G</xref>) cross-modal cues.</p><p>We performed an additional analysis in the group of trained rats to establish whether neurons that responded to cross-modal cues displayed a preference for any of the tastants. No bias toward any specific tastant was observed (cues/unexpected-sucrose 40.5% [28/69], cues/NaCl 37.6% [26/69], cues/citric acid 43.4% [30/69], cues/quinine 44.9% [31/69]; Pearson’s χ<sup>2</sup> test; χ<sup>2</sup><sub>(3)</sub> = 0.36, p=0.94). This result suggests that in trained animals, cue responsive neurons do not preferentially represent sucrose, but maintain the ability to represent multiple tastants.</p><p>To establish whether the changes described above modified breadth of tuning, we computed an index of response sharpness (<xref ref-type="bibr" rid="bib83">Yoshida and Katz, 2011</xref>). If a GC neuron is modulated only by one stimulus, its sharpness index is 1; if a neuron is responsive to all 8 stimuli, its sharpness index is 0. The results confirm that associative learning broadened the response tuning for both taste selective and cross-modal responsive neurons (taste selective neuron: 0.58 ± 0.02 and 0.48 ± 0.02 untrained and trained respectively, t-test: t<sub>(152)</sub> = −3.31; p&lt;0.01; cross-modal responsive neurons: 0.60 ± 0.02 and 0.46 ± 0.01 untrained and trained respectively, t-test: t<sub>(107)</sub> = −4.77; p&lt;0.01).</p><p>Overall, these results show that, when associated with sucrose, cues of different sensory modalities recruit more neurons in GC compared to untrained animals. Cross-modal cue-taste association does not eliminate the bias for stimuli with strong associability, but enhances the overlap between representations of multiple cues and between representations for cues and taste.</p></sec><sec id="s2-4"><title>Temporal analysis of cue responses</title><p>GC neurons are known to encode taste through dynamic changes in firing rates (<xref ref-type="bibr" rid="bib36">Katz et al., 2001</xref>; <xref ref-type="bibr" rid="bib34">Jones et al., 2007</xref>; <xref ref-type="bibr" rid="bib33">Jezzini et al., 2013</xref>). We performed a series of analyses to investigate the time-course of cross-modal cue responses. <xref ref-type="fig" rid="fig5">Figure 5A</xref> shows the normalized firing rate (auROC) responses for all the neurons that were modulated by at least one anticipatory cue in trained and untrained rats. Visual inspection of these plots suggests that associative learning may be associated with an increase in the number of GC neurons that are inhibited by the cues, and may lead to more time-varying neural responses. We quantified the number of neurons that were either excited or inhibited (only the first change in firing compared to baseline is considered here) by the cue in untrained and trained rats. In the group of trained animals we observed a reduction in the proportion of neurons that were excited by the cue (<xref ref-type="fig" rid="fig5">Figure 5A–B</xref>; 70.3% [38/54] for untrained <italic>vs.</italic> 52.2% [59/113] for trained, test for equality of proportion;<bold>χ</bold><sup>2</sup><sub>(1)</sub> = 4.30, p&lt;0.05) and an increase in the proportion of neurons that were inhibited by the cue (<xref ref-type="fig" rid="fig5">Figure 5A–B</xref>; rightmost panel; 29.6% [16/54] for untrained <italic>vs.</italic> 47.7% [54/113] for trained, test for equality of proportion;<bold>χ</bold><sup>2</sup><sub>(1)</sub> = 4.94, p&lt;0.05). To determine whether learning was associated with responses with more frequent firing rate changes, we computed the average number of firing rate modulations for each response. In the group of trained rats, cues evoked a significantly higher number of firing rate modulation per response compared to what observed in untrained animals (1.22 ± 0.06 and 1.44 ± 0.07 for untrained and trained respectively; t-test: t<sub>(155)</sub> = −2.02; p&lt;0.05) (<xref ref-type="fig" rid="fig5">Figure 5C</xref>).<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.16420.014</object-id><label>Figure 5.</label><caption><title>Time course of GC responses in the groups of trained and untrained rats.</title><p>(<bold>A</bold>) Population plot of all GC neurons modulated by at least one cross-modal stimulus in untrained (left panel) and in trained (right panel) rats. Each row represents a GC neuron. The color of each square along the x-axes (see color bar) represents the normalized firing rate within each 200 ms bin. The red dotted rectangular box indicates stimulus presentation. Neurons are clustered by the sensory modality (S for somatosensory, O for odor, T for tone and L for light) and ranked with excitatory and inhibitory responses from top to bottom respectively. (<bold>B</bold>) Histograms showing the percentage of cue responsive neurons that are excited and inhibited by anticipatory cues. (<bold>C</bold>) Histogram showing the average number of firing rate modulations in neurons responsive to cross-modal stimuli. Asterisk indicates p&lt;0.05 for test for equality of proportion and t-test for panel B and C respectively.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.014">http://dx.doi.org/10.7554/eLife.16420.014</ext-link></p><p><supplementary-material id="SD4-data"><object-id pub-id-type="doi">10.7554/eLife.16420.015</object-id><label>Figure 5—source data 1.</label><caption><title>Fraction and percentage of GC neurons excited or inhibited by the cross-modal cues.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.015">http://dx.doi.org/10.7554/eLife.16420.015</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-16420-fig5-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-16420-fig5-v2"/></fig></p><p>In summary, learning to associate cross-modal cues with sucrose lead to an increase in inhibitory responses and to the development of responses characterized by a more dynamic firing modulations.</p></sec><sec id="s2-5"><title>Coding of cue-related information</title><p>To determine whether the distinct response dynamics observed in trained and untrained rats were associated with differences in coding of cue-related information we performed a decoding analysis (<xref ref-type="bibr" rid="bib34">Jones et al., 2007</xref>; <xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>; <xref ref-type="bibr" rid="bib33">Jezzini et al., 2013</xref>). This approach allowed us to investigate, in both untrained and trained rats, differences in how the GC ensemble activity encodes for the four cross-modal stimuli. The ensembles were composed of all the neurons recorded in a certain condition (untrained: n = 135 [<xref ref-type="fig" rid="fig6">Figure 6A</xref>]; trained: n = 118 [<xref ref-type="fig" rid="fig6">Figure 6B</xref>]). Briefly, each single trial of ensemble activity in response to a specific stimulus was classified by comparing it with the average ensemble responses for each of the four cross-modal stimuli. The time course of the classification performance was analyzed over 2.5 s following stimulus onset and was computed for each 200 ms bin.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.16420.016</object-id><label>Figure 6.</label><caption><title>Coding of different sensory modalities for untrained and trained rats.</title><p>(<bold>A</bold>) Time course of the classification performance for cross-modal stimuli in the group of untrained rats. (<bold>B</bold>) Time course of the classification performance for cross-modal anticipatory cues in the group of rats that underwent an associative learning. For A and B: solid line represents correctly classified trials (as% ) based on population activity; shading gray area around traces represents 95% bootstrapped CI. Thick black horizontal lines below traces indicate significance from the chance level (95% bootstrapped CI above chance). Dotted horizontal lines indicate chance performance. Red dotted boxes represent the period of cross-modal stimulus presentation. (<bold>C</bold>) Left most panel: histograms for average cross-modal stimuli classification performance for neurons recorded in untrained rats. Classification performance is evaluated for a 200 ms time bin after stimulus onset. Error bars indicate SD. Right most panel: average classification for correct hits and false hits (second highest classification values after correct hits). Error bars indicate SEM. (<bold>D</bold>) Left most panel: as in <bold>C</bold>, but for a 200 ms time bin after stimulus offset. (<bold>E</bold>) Left most panels: histograms showing cross-modal cues classification performance for neurons recorded in trained rats. Classification performance is evaluated for a 200 ms time bin after stimulus onset. Error bars indicate SD. Right most panel: average classification for correct hits and false hits (second highest classification values after correct ones). Error bars indicate SEM. (<bold>F</bold>) Left most panels: as in E, but 200 ms after stimulus offset. (<bold>G</bold>) The time course of the CI measured for 200 ms bins over 1 s before and 2.5 s after cue onset in untrained rats. (<bold>H</bold>) Time course of the CI measured for 200 ms bins over 1 s before and 2.5 s after cue onset in trained rats. For <bold>C</bold>, <bold>D</bold>, <bold>E</bold>, <bold>F</bold>: each group of bars shows the percentage of trials classified for each stimulus. Labels under each bar indicate the actual delivered stimulus (S for somatosensory, O for odor, T for tone and L for light). For <bold>G</bold> and <bold>H</bold>: solid lines represent correct CI (measured assuming somatosensory and odor cues being similar). Shading around traces represents 95% bootstrapped CI (where the similarity between cues is shuffled, for example tone-odor or light-somatosensory being similar). Black horizontal lines indicate bins in which the CI is significantly different from chance. Asterisk indicates in panels <bold>C</bold>, <bold>D</bold> and <bold>E</bold> indicate p&lt;0.05 for t-test.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.016">http://dx.doi.org/10.7554/eLife.16420.016</ext-link></p><p><supplementary-material id="SD5-data"><object-id pub-id-type="doi">10.7554/eLife.16420.017</object-id><label>Figure 6—source data 1.</label><caption><title>Average and standard error of 'correct' and 'false' hits.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16420.017">http://dx.doi.org/10.7554/eLife.16420.017</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-16420-fig6-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-16420-fig6-v2"/></fig></p><p>In both conditions, the population of neurons recorded allowed for a significantly above chance classification of the sensory modality of cues. GC neurons were able to decode the different stimulus modalities both in untrained and trained rats (<xref ref-type="fig" rid="fig6">Figure 6A–B</xref>). The peak classification performance was observed 100 ms after stimulus onset and was 0.67 for neurons in untrained rats and 0.70 for neurons recorded in trained animals. The decoding performance decreased after the early peak, but continued to be significantly above chance for the duration of the cross-modal stimulation. In both conditions, classification remained significant in the 500 ms following the cue offset.</p><p>The results of this classification analysis were further analyzed to identify possible differences in decoding for trained and untrained subjects. For each cross-modal stimulus, the proportion of correct hits and errors was computed. Correct hits were trials in which a stimulus was correctly classified as belonging to a specific modality (for example, an air puff being classified as a somatosensory stimulus and not as an odor, tone or light; referred hereafter as correct hits). Errors were trials in which a stimulus was misclassified (for example, an air puff being incorrectly classified as an odor, tone or light). In the case of errors, we focused on the most frequent misclassifications (for example, an air puff being more frequently misclassified as an odor compared to tone or light; referred hereafter as false hits). Percentages of correct and false hits over the total number of trials for each cross-modal stimulus were computed for two temporal windows: shortly after the onset of the cross-modal stimulus (from 0 to 200 ms) and shortly after stimulus offset (from 2.5 to 2.7 s). For untrained animals, neural activity led to qualitatively similar classification performance in both windows. After onset and after offset of cross-modal stimuli, the average of correct hits significantly outnumbered the average of false hits (<xref ref-type="fig" rid="fig6">Figure 6C</xref>; stimulus onset: 0.67 ± 0.16 and 0.26 ± 0.16 correct and false hits respectively; t-test: t<sub>(6)</sub> = 3.54; p&lt;0.05; <xref ref-type="fig" rid="fig6">Figure 6D</xref>; stimulus offset: 0.48 ± 0.03 and 0.25 ± 0.04 correct and false hits respectively; t-test: t<sub>(6)</sub> = 5.31; p&lt;0.05). For trained animals, performance in the two windows was qualitatively different. After cue onset, activity allowed for optimal coding of stimulus identity, with correct hits dramatically outnumbering false hits (<xref ref-type="fig" rid="fig6">Figure 6E</xref>; 0.70 ± 0.17 and 0.18 ± 0.02 correct and false hits respectively; t-test: t<sub>(6)</sub> = 5.89; p&lt;0.05). After the offset of the cue, i.e., in the delay interval (500 ms) preceding sucrose delivery, neural activity lead to an equally large number of correct and false hits (<xref ref-type="fig" rid="fig6">Figure 6F</xref>; 0.45 ± 0.09 and 0.38 ± 0.04 correct and false hits respectively; t-test: t<sub>(6)</sub> = 1.11; p=0.30). It is important to notice that cross-modal stimuli were not randomly misclassified. Indeed the decoder was more likely to confound stimuli that shared similar associability. During the delay period, olfactory and somatosensory cues (the strongly associable cues, as determined by the faster learning rate in the behavioral experiment) could be easily confused with each other; similarly, visual and auditory cues (the two stimuli with weaker associability) could be misclassified. Misclassifications between cues with different associability were significantly less frequent (<xref ref-type="fig" rid="fig6">Figure 6F</xref>).</p><p>To further confirm this observation, a cue-similarity index (CI) was computed (<xref ref-type="fig" rid="fig6">Figure 6G–H</xref>). This index quantifies the similarity of firing rates evoked by stimuli that share associability (see Material and methods for details). Positive values of this index indicate that a neuron is modulated similarly by cues with similar taste-associability power (odor and somatosensory stimuli or tone and light), while negative values indicate the opposite (no similarity between responses to stimuli with similar associability). Comparison of the CIs computed for untrained and trained rats show differences in the time-course between the two conditions (<xref ref-type="fig" rid="fig6">Figure 6G–H</xref>). While before association GC neurons show no above chance peak in the CI throughout the entire time-course (<xref ref-type="fig" rid="fig6">Figure 6G</xref>), after learning GC neurons showed a significant peak in CI right after cue onset and a stronger and sustained peak starting 1.7 s after cue onset and lasting for the duration of the delay (500 ms between cue offset and sucrose presentation) period (<xref ref-type="fig" rid="fig6">Figure 6H</xref>).</p><p>These results show that the sensory modality of non-gustatory stimuli is encoded throughout the stimulation period as well as following the offset of the stimulus in both untrained and trained rats. GC neurons in trained animals can also encode the associability of the cue during the delay period.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this article, we present results unveiling how cross-modal stimuli are represented in a primary sensory cortex in the absence or in the presence of associative learning. Here we defined as 'cross-modal' neural responses evoked by sensory stimuli that do not belong to the modality typically represented in the primary sensory area of interest. In the case of GC, cross-modal responses are changes in firing rates evoked by auditory, visual, olfactory and somatosensory stimuli. While this definition does not distinguish between responses purely representing the sensory features (i.e., the modality) of the different stimuli and responses purely representing learned associations, our data provide evidences that both types of representations can exist in a primary sensory cortex.</p><p>We recorded single unit responses to olfactory, somatosensory, visual and auditory stimuli from GC of alert rats. In the first experiment, cross-modal stimuli were presented without establishing any association with tastants. We found that neurons in GC represent stimuli of all four modalities, but have a strong bias in favor of somatosensory and olfactory stimuli. Analysis of the convergence of different modalities onto the same neuron revealed that in untrained animals GC neurons are largely unimodal. This result implies that, in the absence of taste-association, representations of cross-modal stimuli in GC are non-overlapping. Cross-modal responses were mostly excitatory and tended to be tonic, lasting for the entire duration of the stimulation window and beyond. Decoding analysis revealed that this type of neural activity endowed GC with the ability to successfully recognize stimuli of different sensory modalities.</p><p>In the second experiment, we trained a new group of rats to associate cross-modal stimuli with the delivery of sucrose and compared their responses with those recorded in untrained animals. Training showed that different modalities lead to different learning rates, with olfactory and somatosensory stimuli (e.g., those more represented in GC neurons of untrained animals) becoming associated with sucrose more rapidly than auditory and visual stimuli. Analysis of neural responses after training revealed that associative learning was linked to a dramatic increase in the prevalence of neurons responding to non-gustatory stimuli compared to untrained animals. This increase led to an expansion of the contingent of neurons that responded to multiple stimuli. While in untrained animals only a few taste selective neurons responded to cross-modal stimuli, after associative learning the majority of taste selective neurons were also cue responsive. Similarly, as a result of the expansion of cross-modal representations, the number of neurons responding to more than one cue increased after associative learning. Analysis of taste responses in cue responsive neurons revealed no bias toward any specific tastant, demonstrating that cue-sucrose pairing did not result in a preferential representation of sucrose. Indeed, cue responsive neurons maintained the ability to represent multiple tastants.Quantification of the breadth of tuning revealed that learning reduced stimulus selectivity and increased the breadth of tuning in GC neurons. Learning also increased the number of neurons that were inhibited by cross-modal stimuli and changed the dynamics of the responses. Instead of being tonic and monophasic as in untrained animals, cross-modal responses in trained rats showed multiple modulations lasting for the entire duration of the cue and beyond. In addition, decoding analysis revealed that cue responses after learning retain information about the sensory modality of the cue; in addition, they acquire information regarding the associability of different modalities.</p><p>Altogether, these results provide evidence suggesting that associative learning can modify the representation of cross-modal stimuli in primary sensory cortices.</p><sec id="s3-1"><title>Cross-modal integration in the gustatory cortex</title><p>Cross-modality has been demonstrated in multiple sensory areas (<xref ref-type="bibr" rid="bib36">Katz et al., 2001</xref>; <xref ref-type="bibr" rid="bib26">Gottfried et al., 2002</xref>; <xref ref-type="bibr" rid="bib37">Kayser and Logothetis, 2007</xref>; <xref ref-type="bibr" rid="bib31">Iurilli et al., 2012</xref>; <xref ref-type="bibr" rid="bib50">Olcese et al., 2013</xref>; <xref ref-type="bibr" rid="bib42">Liu et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Ibrahim et al., 2016</xref>). In this study, we investigate cross-modality in the gustatory cortex. Taste is a good system to investigate how primary sensory cortices integrate cross-modal information (<xref ref-type="bibr" rid="bib10">Dalton et al., 2000</xref>; <xref ref-type="bibr" rid="bib36">Katz et al., 2001</xref>; <xref ref-type="bibr" rid="bib63">Simon et al., 2006</xref>; <xref ref-type="bibr" rid="bib12">de Araujo and Simon, 2009</xref>; <xref ref-type="bibr" rid="bib16">Escanilla et al., 2015</xref>). The experience of eating is inherently multimodal, involving the integration of gustatory, somatosensory and olfactory signals in a single percept called 'flavor' (<xref ref-type="bibr" rid="bib36">Katz et al., 2001</xref>; <xref ref-type="bibr" rid="bib11">De Araujo and Rolls, 2004</xref>; <xref ref-type="bibr" rid="bib64">Spence, 2015</xref>). In addition, the approach to food is guided by multisensory cues that predict its availability (<xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>; <xref ref-type="bibr" rid="bib39">Kusumoto-Yoshida et al., 2015</xref>).</p><p>Here we chose to focus on cross-modality related to anticipation because this is a phenomenon that generalizes to other sensory areas (<xref ref-type="bibr" rid="bib18">Freeman, 1983</xref>; <xref ref-type="bibr" rid="bib59">Schoenbaum and Eichenbaum, 1995</xref>; <xref ref-type="bibr" rid="bib47">Meftah el et al., 2002</xref> ; <xref ref-type="bibr" rid="bib62">Shuler and Bear, 2006</xref>) As a result, we deliberately avoided stimuli typically linked to flavor, such as intraoral tactile stimulation and retro-nasal olfactory stimulation (<xref ref-type="bibr" rid="bib79">Yamamoto et al., 1988</xref>; <xref ref-type="bibr" rid="bib63">Simon et al., 2006</xref>; <xref ref-type="bibr" rid="bib23">Gautam and Verhagen, 2012</xref>). Instead, we selected extra-oral, cross-modal stimuli similar to those that are typically used in behavioral and sensory experiments (<xref ref-type="bibr" rid="bib2">Brecht and Sakmann, 2002</xref>; <xref ref-type="bibr" rid="bib13">Derdikman et al., 2003</xref>; <xref ref-type="bibr" rid="bib1">Barnes et al., 2008</xref>; <xref ref-type="bibr" rid="bib75">Wesson et al., 2009</xref>; <xref ref-type="bibr" rid="bib49">Nonkes et al., 2012</xref>; <xref ref-type="bibr" rid="bib52">Ollerenshaw et al., 2012b</xref>; <xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>; <xref ref-type="bibr" rid="bib6">Centanni et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Devore et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Headley and Weinberger, 2013</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>).</p><p>Several studies have explored responses to cross-modal stimuli in GC of anesthetized (<xref ref-type="bibr" rid="bib81">Yamamoto et al., 1981</xref>; <xref ref-type="bibr" rid="bib56">Rodgers et al., 2008</xref>) or alert, untrained animals (<xref ref-type="bibr" rid="bib80">Yamamoto et al., 1989</xref>; <xref ref-type="bibr" rid="bib36">Katz et al., 2001</xref>). Recently it was shown that learning the taste-predictive value of an auditory cue increases the prevalence of auditory responses in GC (<xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>). So far, however, no study has assessed GC responses to cross-modal stimuli in both untrained and trained animals. Our experiments show that GC can encode stimuli of all sensory modalities, even stimuli that have relatively little ethological relationship to taste, like sounds or flashes of light. In untrained animals, each sensory modality engages a selective group of unimodal neurons, endowing GC with the ability to discriminate different stimuli. This result indicates that cross-modal responses in GC are not just reflecting the general processing of non-gustatory information. If cross-modal responses simply encoded the presence of generally arousing (<xref ref-type="bibr" rid="bib67">Talsma et al., 2010</xref>), rewarding (<xref ref-type="bibr" rid="bib62">Shuler and Bear, 2006</xref>; <xref ref-type="bibr" rid="bib53">Pooresmaeili et al., 2014</xref>) or distracting (<xref ref-type="bibr" rid="bib31">Iurilli et al., 2012</xref>) stimuli, they would involve overlapping sets of neurons for each stimulus. The presence of cross-modal responses in untrained animals may reflect GC’s intrinsic tuning toward multisensoriality and its ability to encode different dimensions linked to food expectation. These responses can also represent a substrate upon which learning can operate (<xref ref-type="bibr" rid="bib71">Vogels et al., 2011</xref>). Indeed, pairing stimuli with sucrose leads to an expansion of cross-modal representations and the development of cue responses. Responses become temporally more complex relative to those observed in untrained animals, probably due to GC inputs from multiple regions (<xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>, <xref ref-type="bibr" rid="bib58">2013</xref>; <xref ref-type="bibr" rid="bib43">Liu and Fontanini, 2015</xref>). After learning, fewer neurons are unimodal and more become multimodal, suggesting that GC is now also encoding for a common property of the cross-modal stimuli, e.g., their taste-anticipatory value. In addition to encoding expectation, decoding analysis indicates that, after associative learning, GC can encode both the identity of different cross-modal stimuli, and their associability. These results confirm and significantly expand data from the literature on the functional role of GC (<xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>).</p><p>Analysis of the distribution of responses to each stimulus reveals a key feature of cross-modal responses in GC: their bias toward stimuli that are more promptly associated with gustatory rewards (e.g., odors and whisker stimulation). We cannot exclude that both neural and behavioral biases might reflect differences in detectability or saliency across stimuli sensory modalities before training. Indeed, it is possible that, before learning, the air puff and the odor may be more intense than light and tone. In this regard, it would be interesting for future studies to explore various stimulus intensities and establish the intensity-response relationship of cross-modal responses. We would not be surprised if varying the intensity of stimuli affected the number of neurons recruited, however, we believe that the bias for odors and somatosensory stimuli would still persist. This belief is supported by the strong interconnectivity between GC with olfactory (<xref ref-type="bibr" rid="bib61">Shipley and Geinisman, 1984</xref>) and somatosensory areas (<xref ref-type="bibr" rid="bib28">Guldin and Markowitsch, 1983</xref>).</p><p>While differences in cross-modal stimulus intensity and detectability could partially explain the observed bias before learning, such an interpretation does not apply to the bias observed after the cue-sucrose association. After 14 days of training, behavioral responses for all the cues become similar, indicating that the four cues acquire similar saliency. Despite the absence of a behavioral bias, the neural bias was maintained, suggesting that it cannot be fully accounted by differences in behavioral saliency among stimuli. One possible explanation for the persistence of the bias after learning may involve the non-uniform number of sessions at final performance level for the different cues. Indeed, rapid learning for somatosensory and olfactory cues resulted in a larger number of above-criterion sessions compared to visual and auditory cues. To investigate this possibility, we matched the number of sessions at final performance level across cues and quantified the fraction of neurons responsive to the four cross-modal stimuli. The bias still persisted after matching the number of sessions, demonstrating that it is inherent to GC and does not depend on the amount of cue-sucrose exposure.</p><p>In summary, these results suggest that the imbalance in responsiveness to the different cues might reflect, at least in part, the strong ecological relationship between odors, somatosensory stimuli and taste in rodents.</p></sec><sec id="s3-2"><title>Cross-modal representations and associative learning in cortical areas</title><p>Responses to cross-modal stimuli have been extensively studied in the superior colliculus and associative cortices (<xref ref-type="bibr" rid="bib65">Stein and Arigbede, 1972</xref>; <xref ref-type="bibr" rid="bib48">Meredith and Stein, 1986</xref>; <xref ref-type="bibr" rid="bib41">Lipton et al., 1999</xref>; <xref ref-type="bibr" rid="bib20">Fuster et al., 2000</xref>). This body of literature has demonstrated fundamental principles of how cross-modality is represented and is shaped by experience and learning (<xref ref-type="bibr" rid="bib73">Watanabe, 1992</xref>; <xref ref-type="bibr" rid="bib41">Lipton et al., 1999</xref>; <xref ref-type="bibr" rid="bib20">Fuster et al., 2000</xref>; <xref ref-type="bibr" rid="bib84">Yu et al., 2010</xref>; <xref ref-type="bibr" rid="bib78">Xu et al., 2015</xref>). Alas, our knowledge of cross-modal processing in primary sensory cortices is nowhere near that level of sophistication.</p><p>Classic results from extracellular recordings in somatosensory, visual and auditory cortices of anesthetized rats demonstrated that cross-modal responses exist mostly in transitional zones between sensory areas (<xref ref-type="bibr" rid="bib72">Wallace et al., 2004</xref>; <xref ref-type="bibr" rid="bib50">Olcese et al., 2013</xref>). Recent work from alert animals, however, has shown that cross-modal responses are not limited to border zones (<xref ref-type="bibr" rid="bib40">Lakatos et al., 2007</xref>; <xref ref-type="bibr" rid="bib31">Iurilli et al., 2012</xref>; <xref ref-type="bibr" rid="bib45">Maier et al., 2012</xref>). Multimodality in primary sensory cortical areas has been mostly studied with a focus on bimodality (<xref ref-type="bibr" rid="bib15">Driver and Noesselt, 2008</xref>); to our knowledge only a handful of studies have investigated how single neurons in a primary sensory cortex respond to stimuli of more than three modalities (<xref ref-type="bibr" rid="bib79">Yamamoto et al., 1988</xref>, <xref ref-type="bibr" rid="bib80">1989</xref>). As a result, our understanding of the convergence of information from multiple modalities on individual sensory cortical neurons is limited. Our data address this important issue directly and show that in alert, untrained rats, GC can respond to stimuli of all five sensory modalities, and that the majority of neurons respond to a single modality. The degree of convergence of cross-modal information on single neurons, however, is not fixed; indeed our data demonstrate that it can be modified by associative learning.</p><p>Associative learning is fundamental for shaping cross-modal responses in frontal cortices (<xref ref-type="bibr" rid="bib73">Watanabe, 1992</xref>; <xref ref-type="bibr" rid="bib41">Lipton et al., 1999</xref>; <xref ref-type="bibr" rid="bib20">Fuster et al., 2000</xref>), and in subcortical areas (<xref ref-type="bibr" rid="bib84">Yu et al., 2010</xref>; <xref ref-type="bibr" rid="bib78">Xu et al., 2015</xref>). Associative learning is also known to modify sensory representations in primary sensory cortices (<xref ref-type="bibr" rid="bib85">Zhou and Fuster, 2000</xref>; <xref ref-type="bibr" rid="bib3">Brosch et al., 2005</xref>; <xref ref-type="bibr" rid="bib62">Shuler and Bear, 2006</xref>; <xref ref-type="bibr" rid="bib74">Weinberger, 2007</xref>; <xref ref-type="bibr" rid="bib27">Grossman et al., 2008</xref>; <xref ref-type="bibr" rid="bib7">Chen et al., 2011</xref>), albeit this body of literature focused exclusively on few single modalities. Indeed, little is known about how learning affects cross-modal representations in primary sensory cortices. Our data demonstrate that associative learning can expand the pool of neurons responding to cross-modal stimuli and enhance the ability of neurons to represent multiple modalities. It is worth mentioning that, the set of cross-modal stimuli used in this study is not homogeneous. Some stimuli, those with stronger associability, recruit many more GC neurons than others. Nevertheless, learning is capable of expanding the representations for all the stimuli, regardless of how much they activate GC at baseline. In addition to expanding cross-modal representations, associative learning enhances the similarity of responses to stimuli with similar associability.</p><p>The results from trained rats strongly indicate that GC’s cross-modal representations are plastic and, hence, likely to be affected also by conditioning with aversive gustatory outcomes (quinine) or by extinction. Previous work demonstrated that using two gustatory outcomes that differ in palatability (sucrose, palatable, and quinine, aversive) could lead to the genesis of cue responses representing specific expectations (<xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>). On the basis of that result, we can speculate that the presence of aversive outcomes might drive the expansion of cross-modal representations as much as rewards. In addition, the presence of different outcomes might be important in inducing the development of cue responses showing a bias for specific tastants. As for extinction, findings from the same study (<xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>) suggest that it might shrink cross-modal representations. It is worth mentioning that the results in (<xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>) also showed that neurons in GC could change their firing rates in trials in which sucrose was predicted by a cross-modal cue but omitted. This phenomenon, which was not further investigated in the present paper, demonstrates that cross-modal activity in GC may also reflect learned expectations of reward timing – a phenomenon well studied in the primary visual cortex (<xref ref-type="bibr" rid="bib62">Shuler and Bear, 2006</xref>; <xref ref-type="bibr" rid="bib42">Liu et al., 2015</xref>). Future studies will rely on omission trials as well as sham trials (i.e., trials in which neither a cue nor sucrose is presented, but that have the same ITI as any other trial) to investigate the effects of expectations of stimulus and reward timing on neural activity and mouth movements.</p><p>In conclusion, our experiments demonstrate the importance of learning in shaping the representation of cross-modal information in primary sensory areas. These results further emphasize the importance of considering experience, learned associations and expectations when interpreting cross-modal responses in sensory cortices.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental subjects</title><p>The experiments in this study were performed on ten female Long-Evans rats (250–350 g; Charles River). We chose female rats for their amenability to experiments in restraint and for consistency with previously published work (<xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>; <xref ref-type="bibr" rid="bib33">Jezzini et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Samuelsen et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>; <xref ref-type="bibr" rid="bib43">Liu and Fontanini, 2015</xref>). The stages of the estrous were not monitored. However, since the neural activity was recorded for 14 consecutive days (in both groups of trained and untrained rats), and since the estrous cycle lasts 4–5 days, data were likely collected across all rats’ estrous stages. Rats were individually housed and maintained on a 12 hr light/dark cycle with <italic>ad libitum</italic> access to food and water unless otherwise specified. All experimental procedures were approved by the Institutional Animal Care and Use Committee at Stony Brook University and complied with university, state, and federal regulation on the care and use of laboratory animals.</p></sec><sec id="s4-2"><title>Surgery</title><p>Rats were anesthetized with an intraperitoneal injection of a mixture of ketamine/xylazine/acepromazine (100, 5.2 and 1 mg/kg, respectively). Supplemental doses of anesthetic (30% of initial dose) were administered when needed. Rats’ body temperature was maintained at 37°C throughout the surgery. Once fully anesthetized, rats were placed on a stereotaxic device, their scalp was scrubbed with iodine and excised to expose the skull. Holes were drilled for electrodes above GC (antero-posterior: +1.4 mm, medio-lateral: ± 5 mm from bregma) and for anchoring screws in seven other positions (<xref ref-type="bibr" rid="bib33">Jezzini et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>). Electrodes consisted of drivable bundles of 16 individual formvar-coated nichrome micro-wires (<xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>, <xref ref-type="bibr" rid="bib58">2013</xref>), which were bilaterally inserted above GC (dorso-ventral: −4 mm from dura). Movable bundles were lowered ~200 µm after each experimental session allowing us to record multiple ensembles in the same animal. After insertion of the electrodes bundles, intra-oral cannulae (IOCs) were bilaterally inserted to allow for the delivery of gustatory stimuli directly into the oral cavity. Electrode bundles, IOCs and a head bolt (for the purpose of head restraint) were cemented to the skull with dental acrylic. Animals were allowed to recover for a minimum of 7 days before training began. The proper placement of electrodes was verified, at the end of each experiment by means of standard histological procedures (<xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>; <xref ref-type="bibr" rid="bib33">Jezzini et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>; <xref ref-type="bibr" rid="bib43">Liu and Fontanini, 2015</xref>) (see below).</p></sec><sec id="s4-3"><title>Restraint training and behavioral paradigm</title><p>Following recovery from the surgery, rats from untrained and trained groups were placed on a water restriction regime (30–45 min of daily access to water). After 2–3 days, rats from both groups began training to sit calmly in restraint while receiving deliveries of water (~50 µl of water with a variable inter-trial interval [ITI]) via IOCs. Session duration was progressively increased to 1 hr and 10 min, over 2 to 3 weeks. Animal selection (weight, age and gender), housing, surgical procedures, post-surgical recovery, water restriction, restraint training and IOC training were the same ('common' phase) for both groups of animals used in this study. Procedures differed only following the common phase; the group of trained rats (n = 5; referred as <italic>Trained</italic> in the main text and figures) underwent associative leaning, while the group of untrained rats (n = 5; referred as <italic>Untrained</italic> in main text and figures) entered the experimental phase.</p><sec id="s4-3-1"><title>Untrained rats</title><p>In the experimental phase for this group of animals, we recorded spiking activity and orofacial movements evoked by random delivery of the following cross-modal and gustatory stimuli: an odor, a puff of air on the distal portion of the rats’ whiskers (<italic>air puff</italic>), a light, a tone, unexpected sucrose, unexpected NaCl, unexpected citric acid and unexpected quinine (see Stimuli delivery section below for further information). After each stimulus delivery, with a variable interval (25 ± 5 s), ~50 µl of water (<italic>Rinse</italic>) was delivered. 25 ± 5 s after a rinse a new trial began (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The ITI was 50 ± 10 s in order to avoid associations between cross-modal, 'non-gustatory' stimuli and tastants. Each stimulus was presented from 8 to 14 trials. Recordings were performed for 14 days (one session per day); at the end of each daily session bundles of electrodes were lowered ~150 µm.</p></sec><sec id="s4-3-2"><title>Trained rats</title><p>This group of animals was trained to associate four cross-modal, 'non-gustatory' stimuli (the conditioned stimulus; an air puff on the distal whiskers, an odor, a tone or a light stimulus; see Stimuli delivery section below for further information) with a gustatory stimulus (the unconditioned stimulus; sucrose) in a Pavlovian task. Cross-modal stimuli were presented in a pseudorandom fashion; they always lasted 2 s and their onset preceded the delivery of sucrose by 2.5 s. Trials occurred at a 50 ± 10 s interval (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The entire conditioning period lasted 14 days (see <xref ref-type="fig" rid="fig2">Figure 2</xref>; one session per day). In each daily session, rats received 20 trials of each cue-taste pair. The time-course of classical conditioning was monitored every day by quantifying conditioned mouth movements (see Mouth movement analysissection). After 14 days of training, rats entered the recording phase. In this phase, we recorded spiking activity and orofacial movements evoked by random delivery of the following cross-modal and gustatory stimuli: an odor, a puff of air on the distal portion of the rats’ whiskers (<italic>air puff</italic>), a light, a tone, expected and unexpected sucrose, unexpected NaCl, unexpected citric acid and unexpected quinine (see Stimuli delivery section below for further information). In this experiment, as during the conditioning phase (see above), each cross-modal cue anticipates by 2.5 s the delivery of sucrose (expected sucrose), whereas unexpected sucrose and the other three tastants are delivered without cross-modal stimuli preceding them (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). 25 ± 5 s after each stimulus delivery ~50 µl of water (<italic>Rinse</italic>) was delivered. 25 ± 5 s after a rinse a new trial began (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The ITI was 50 ± 10 s. Between 8 and 14 trials for each stimulus were delivered. Recordings were performed for 14 days (one session per day); at the end of each daily session bundles of electrodes were lowered 150 µm.</p></sec></sec><sec id="s4-4"><title>Stimuli identity and delivery</title><p>Gustatory stimuli were delivered through a manifold made of multiple, fine polyimide tubes slid into the IOC (<xref ref-type="bibr" rid="bib17">Fontanini et al., 2009</xref>; <xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>). A pressurized taste delivery system delivered tastants through the IOCs in aliquots of 40 µl with a 25–30 ms pulse (<xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>). A rinse of water (50 µl) followed the taste after 25 ± 5 s in order to wash the oral cavity. The concentrations of the four basic tastants used were 0.1 M for sucrose, citric acid and NaCl, while a concentration of 0.001 M was used for quinine-HCl. All the tastants were from Sigma-Aldrich. The auditory cue consisted of a 75 dB, 2 kHz single tone. Two green LEDs with a peak emission wavelength of 575 nm delivered the visual cue. The intensity of the light was 0.147 lm. The LEDs were placed bilaterally at a distance of ~5 cm from each eye. The somatosensory cue consisted of a bilateral air puff generated by a pico-spritzer (~5 psi) delivered from two PFA tubes (5 mm diameter). The outlet of the tube was placed ~5 cm from the rats’ whiskers. The air puff was directed to the distal portion of the whiskers. The olfactory cue (0.1% Isoamyl acetate) was delivered via a custom-built, computer-controlled olfactometer (<xref ref-type="bibr" rid="bib70">Verhagen et al., 2007</xref>), and delivered at a ~2 cm distance from the rats’ nose. The duration of all cross-modal sensory stimuli was 2 s. The choice of stimulus identity and intensity was consistent with existing studies showing that rats can easily detect the stimuli we used. Puffs of air on a single or on multiple whiskers have been used as either cues or sensory stimuli in multiple studies (<xref ref-type="bibr" rid="bib2">Brecht and Sakmann, 2002</xref>; <xref ref-type="bibr" rid="bib13">Derdikman et al., 2003</xref>; <xref ref-type="bibr" rid="bib8">Civillico and Contreras, 2006</xref>; <xref ref-type="bibr" rid="bib51">Ollerenshaw et al., 2012a</xref>). Green light cues are typically used in behavioral studies as conditioned stimuli (<xref ref-type="bibr" rid="bib32">Jacobs, 2009</xref>; <xref ref-type="bibr" rid="bib49">Nonkes et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Headley and Weinberger, 2013</xref>). Auditory stimuli with a frequency and intensity similar to the ones used here have been used to condition rats in prior studies from our laboratory (<xref ref-type="bibr" rid="bib57">Samuelsen et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>) and as stimuli to be discriminated (<xref ref-type="bibr" rid="bib6">Centanni et al., 2013</xref>; <xref ref-type="bibr" rid="bib77">Xiong et al., 2015</xref>). Finally, several studies demonstrate that rats can easily detect and discriminate the odor Isoamyl acetate when presented at the concentration equal or lower to the one (0.1%) used in this study (<xref ref-type="bibr" rid="bib1">Barnes et al., 2008</xref>; <xref ref-type="bibr" rid="bib75">Wesson et al., 2009</xref>; <xref ref-type="bibr" rid="bib14">Devore et al., 2013</xref>). While all the stimuli that we relied on can be easily detected, no information is available on their relative saliency in naïve animals.</p></sec><sec id="s4-5"><title>Electrophysiological recording and analyses</title><p>Signals were amplified, band pass filtered (300–8000 Hz), digitized and recorded (Plexon [Dallas]; sampling rate: 40 KHz). Single units were off-line sorted using a template algorithm, cluster-cutting techniques and examination of inter-spike interval plots (Offline Sorter, Plexon). The average number of units recorded per rat was 27 ± 5.2 and 23.6 ± 2.5 for the <italic>Untrained</italic> and <italic>Trained</italic> groups, respectively. Data were analyzed with custom-written scripts in Matlab (MathWorks, Inc., Natick, Ma). Single-neuron and population peristimulus time histograms (PSTHs) were extracted for cross-modal stimuli and taste deliveries. A bin size of 200 ms was used unless otherwise specified. Single units that displayed a large power-spectrum peak around 6–10 Hz were considered 'somatosensory' (<xref ref-type="bibr" rid="bib36">Katz et al., 2001</xref>; <xref ref-type="bibr" rid="bib33">Jezzini et al., 2013</xref>) and excluded from additional analysis.</p><sec id="s4-5-1"><title>Stimulus responsiveness</title><p>Responsiveness to stimuli was assessed using a 'change point' (CP) analysis as in Jezzini et al.,(<xref ref-type="bibr" rid="bib33">Jezzini et al., 2013</xref>) and Liu and Fontanini (<xref ref-type="bibr" rid="bib43">Liu and Fontanini, 2015</xref>). We first computed the cumulative distribution function (CDF) of spike occurrences across all trials for a given stimulus (<xref ref-type="bibr" rid="bib33">Jezzini et al., 2013</xref>). For each neuron and for each stimulus we analyzed the CDF in the time interval starting 1 s before and ending 2 s (cross-modal stimuli) or 2.5 s (taste stimuli) after stimulus delivery. Sudden changes in firing rates resulted in a piecewise change in the CDF slope and in the identification of a CP. If no CP was detected for any of the stimuli, the neuron was deemed not responsive. By definition, a neuron was stimulus responsive if at least one significant CP was found after stimulus presentation. The CP analysis was also used to establish the sign of response, i.e., excitation or inhibition, and the average number of modulations (data shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref> and <xref ref-type="fig" rid="fig5">Figure 5C</xref>, respectively). For the sign of the response only the first change (first CP) was considered. For the number of modulations all CPs detected in 2.5 s after stimulus were quantified.</p></sec><sec id="s4-5-2"><title>Taste selectivity</title><p>A neuron was considered taste selective if it was responsive to at least one of the four tastes and its response varied significantly across different tastants in either magnitude or time course. The latter was defined with two-way ANOVA [taste identity, time course], using 200 ms bins in the 0 to 2.5 s post-delivery intervals. A neuron was defined taste selective if either the taste main effect or interaction of the two effects (taste identity X time course) was found significant.</p></sec><sec id="s4-5-3"><title>Significance of convergence between responses</title><p>A bootstrap analysis was used to determine if the fraction of neurons responding to more than one cross-modal stimulus (or being both taste selective and cross-modal responsive) was significantly different from what expected from the joint probabilities of responses to individual stimuli. The null hypothesis is that responses to different stimuli are independent from each other. Briefly, for the case of cross-modal responses we simulated 10,000 combinations of neurons each with the experimentally observed probability of responding to air puff (p(air puff)), odor (p(odor)), tone (p(tone)), and light (p(light)). Before learning, the experimentally observed parameters were as follows: number of neurons = 135; p(air puff) = 0.15; p(odor) = 0.16; p(tone) = 0.04; p(light) = 0.03. After learning: number of neurons = 118; p(air puff) = 0.34; p(odor) = 0.33; p(tone) = 0.16; p(light) = 0.11. For the case of the convergence between taste selectivity and cross-modal responsiveness we simulated 10,000 combinations of neurons with the following experimentally observed probabilities. Before learning: n = 135; the probability of being taste selective = 0.64; the probability of being cross-modal responsive = 0.33. After learning: n = 118; the probability of being taste selective = 0.57; the probability of being cross-modal responsive = 0.58. The simulations were used to build a distribution of expected probabilities for combined responses, which allowed for the determination of a confidence interval and a significance threshold (p&lt;0.05) for comparison with the experimentally observed data.</p></sec><sec id="s4-5-4"><title>Decoding analysis</title><p>To investigate the time course of cross-modal stimuli coding in GC before and after associative learning, we relied on a stimulus classification procedure applied at each 200 ms time bin. Details of this procedure can be found in <xref ref-type="bibr" rid="bib33">Jezzini et al. (2013)</xref> and <xref ref-type="bibr" rid="bib43">Liu and Fontanini (2015)</xref>. This method determines how well a given cross-modal stimulus is encoded by a population of neurons in a given temporal bin and is grounded on both a cross-validation procedure as well as a Euclidean distance-based classifier. To compute a population decoding analysis of cross-modal stimuli on both untrained and trained rats, we constructed a 'pseudo-population' of neurons, collecting units from different sessions and rats under the same stimulus condition. Each cross-modal stimulus was denoted as a point in a 2 dimensional vector space (R x C) of firing rates, where R is the number of units included in the pseudo-population (only units from sessions with at least 8 trials for each cross-modal stimulus were considered), and C is the number of temporal bins. To assess the statistical significance of the decoding analysis (confidence interval, CI), we used a bootstrap procedure in which we sampled with replacement a subset of 80% of the neurons from the whole population for each bootstrap run (100 bootstrap runs were used). Significant decoding was defined by the lower bound of 95% bootstrapped confidence interval (CI) larger than chance level (0.25).</p></sec><sec id="s4-5-5"><title>Area under the receiver operating characteristic curve (auROC) method</title><p>To compare GC neurons response profile to cross-modal stimuli before and after associative learning (data shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>) and to compute the cue-similarity index (see below) we used the auROC method for normalizing PSTHs (200 ms bin). The detailed description of this method can be found here (<xref ref-type="bibr" rid="bib9">Cohen et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>). Briefly, the auROC method compares two distributions of firing rates before and after stimulus presentation, i.e., baseline vs. evoked. Values larger than 0.5 express the likelihood that the firing rate in the bin is above baseline, whereas values below 0.5 express the likelihood that the firing rate in the bin is below baseline.</p></sec><sec id="s4-5-6"><title>Selectivity index</title><p>To evaluate the stimulus selectivity (four tastants and four cross-modal stimuli), a sharpness index was computed, based on mean firing rates from the post-stimulus epoch (2.5 s) (<xref ref-type="bibr" rid="bib54">Rainer et al., 1998</xref>; <xref ref-type="bibr" rid="bib83">Yoshida and Katz, 2011</xref>). Sharpness was defined as (n − Σ FRi/FRbest)/(n − 1), where FRi is the mean firing rate for each stimulus (i = 1–8), FRbest is the maximum firing rate among stimuli, and n is the total number of stimuli (n = 8). A sharpness index of 1 indicates that a neuron responded to one stimulus, and the value 0 indicates equal responses across stimuli.</p></sec><sec id="s4-5-7"><title>Cue-similarity index</title><p>The cue similarity index (CI) was computed as follows. As a first step, for each neuron we averaged the absolute value of the log-likelihood ratio of responses to cues with similar associability (e.g., odor, O, and whisker stimulation, S or Tone, T, and Light, L; |R|<sub>(Similar)</sub> = 0.5*(|ln O/S| + |ln T/L|) and to cues with different associability (|R|<sub>(Dissimilar)</sub> = 0.25*(|ln O/T| + |ln O/L| + |ln S/T| + |ln S/L|)). O,S,T and L denote the value of the auROC-normalized response for odor, air puff, tone and light trials respectively. As a second step, we defined the CI as the differences between the two averages: CI = |R|<sub>(Dissimilar)</sub>− |R|<sub>(Similar)</sub>. Positive CI values indicate that a neuron responds similarly to cues with similar associability, while negative values indicate a higher similarity for responses to stimuli with different associability. In order to evaluate statistical significance, we shuffled the pairs (e.g., using O and T or L and S as pairs having similar associability) and extract the 95% bootstrapped CI (represented as shading areas in <xref ref-type="fig" rid="fig6">Figure 6 G–H</xref>).</p></sec></sec><sec id="s4-6"><title>Mouth movement analysis</title><p>Orofacial movements were recorded at a rate of 30 frames per second with a camera placed underneath the rat’s mouth. Images were acquired and synchronized with electrophysiological signals (Cineplex; Plexon) and off-line imported in Matlab (MathWorks, Inc., Natick, Ma) for analysis. Mouth movements were assessed by automated frame-by-frame video analysis (<xref ref-type="bibr" rid="bib22">Gardner and Fontanini, 2014</xref>). A region of interest (ROI) for the rat’s orofacial region was selected. We then computed the absolute difference of the average pixel intensity of the ROI across consecutive frames (referred as ∆ pixel intensity in <xref ref-type="fig" rid="fig1">Figures 1</xref>–<xref ref-type="fig" rid="fig3">3</xref>). These values were normalized to background pixel intensity (as obtained from a second region of interest selected away from the orofacial region) to correct for changes due to fluctuations in background light intensity. This procedure allowed us to have a continuous (bin = 33 ms) record of mouth movements (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). To monitor conditioning, mouth movements were averaged across trial types and analyzed in three temporal epochs: before stimulus onset (1 s before; referred in the text and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> as Spontaneous epoch), during stimulus (from 0.5 to 1.5 s following stimulus onset; referred in the text and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> as Cue epoch) and after sucrose presentation (from 2.7 to 3.7 s after cue onset; referred in the text and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> as Taste epoch). For data shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, to avoid differences in the magnitude of orofacial movement recorded from different rats, single mouth movement trials were also normalized to the maximum value recorded in the 'taste' epoch interval analyzed. Trials in which the rat’s mouth was already in motion or in which view of the orofacial region was obstructed were not included in the analysis.</p></sec><sec id="s4-7"><title>Histological procedures</title><p>At the end of the experiment, rats were terminally anesthetized using an intraperitoneal injection of a mixture of ketamine/xylazine/acepromazine and electrolytic lesions were made to several electrode wires to mark recording sites (7 µA cathodal current for 7 s). Subjects were then perfused through the left cardiac ventricle with saline followed by 10% formalin. Brains were sectioned into 80 µm coronal slices and standard histological procedures (cresyl violet or Nissl staining) were performed to track electrode locations.</p></sec><sec id="s4-8"><title>Data and statistical analysis</title><p>No explicit power analysis was used to compute the appropriate sample size. However, number of animals, number of neurons and number of trials are consistent with the majority of sensory electrophysiology studies. All analyses of electrophysiological signals and orofaciall movements were performed using custom Matlab (MathWorks, Inc., Natick, Ma) scripts and <italic>R</italic> software (<xref ref-type="bibr" rid="bib55">R Core Team, 2015</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The authors would like to acknowledge Dr. Arianna Maffei, Dr. Guillermo Esber, Dr. Giancarlo La Camera, Dr. Luca Mazzucato, Dr. Roberta Tatti, Martha Stone and Maffei’s laboratory for their feedback and insightful comments. The authors would also like to acknowledge Dr. Matthew Gardner for insightful discussion during the initial phase of the project and Amy Cheung for histology. This work has been supported by Swiss National Science Foundation Fellowships P2GEP3_151816 and P300PA_161021 to RV and by National Institute on Deafness and Other Communication Disorders Grant R01-DC010389 to AF.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>RV, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>AF, Conception and design, Drafting or revising the article, Contributed unpublished essential data or reagents</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All experimental procedures were performed according to approved Institutional Animal Care and Use Committee protocols (#244930-1) at Stony Brook University, and complied with university, state, and federal regulation on the care and use of laboratory animals.</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnes</surname><given-names>DC</given-names></name><name><surname>Hofacer</surname><given-names>RD</given-names></name><name><surname>Zaman</surname><given-names>AR</given-names></name><name><surname>Rennaker</surname><given-names>RL</given-names></name><name><surname>Wilson</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Olfactory perceptual stability and discrimination</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1378</fpage><lpage>1380</lpage><pub-id pub-id-type="doi">10.1038/nn.2217</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brecht</surname><given-names>M</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Dynamic representation of whisker deflection by synaptic potentials in spiny stellate and pyramidal cells in the barrels and septa of layer 4 rat somatosensory cortex</article-title><source>The Journal of Physiology</source><volume>543</volume><fpage>49</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2002.018465</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brosch</surname><given-names>M</given-names></name><name><surname>Selezneva</surname><given-names>E</given-names></name><name><surname>Scheich</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Nonauditory events of a behavioral procedure activate auditory cortex of highly trained monkeys</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>6797</fpage><lpage>6806</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1571-05.2005</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvert</surname><given-names>GA</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name><name><surname>Brammer</surname><given-names>MJ</given-names></name><name><surname>Campbell</surname><given-names>R</given-names></name><name><surname>Williams</surname><given-names>SC</given-names></name><name><surname>McGuire</surname><given-names>PK</given-names></name><name><surname>Woodruff</surname><given-names>PW</given-names></name><name><surname>Iversen</surname><given-names>SD</given-names></name><name><surname>David</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Activation of auditory cortex during silent lipreading</article-title><source>Science</source><volume>276</volume><fpage>593</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1126/science.276.5312.593</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carleton</surname><given-names>A</given-names></name><name><surname>Accolla</surname><given-names>R</given-names></name><name><surname>Simon</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Coding in the mammalian gustatory system</article-title><source>Trends in Neurosciences</source><volume>33</volume><fpage>326</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2010.04.002</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Centanni</surname><given-names>TM</given-names></name><name><surname>Engineer</surname><given-names>CT</given-names></name><name><surname>Kilgard</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical speech-evoked response patterns in multiple auditory fields are correlated with behavioral discrimination ability</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>177</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1152/jn.00092.2013</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>CF</given-names></name><name><surname>Barnes</surname><given-names>DC</given-names></name><name><surname>Wilson</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Generalized vs. stimulus-specific learned fear differentially modifies stimulus encoding in primary sensory cortex of awake rats</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>3136</fpage><lpage>3144</lpage><pub-id pub-id-type="doi">10.1152/jn.00721.2011</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Civillico</surname><given-names>EF</given-names></name><name><surname>Contreras</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Integration of evoked responses in supragranular cortex studied with optical recordings in vivo</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>336</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1152/jn.00128.2006</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>JY</given-names></name><name><surname>Haesler</surname><given-names>S</given-names></name><name><surname>Vong</surname><given-names>L</given-names></name><name><surname>Lowell</surname><given-names>BB</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neuron-type-specific signals for reward and punishment in the ventral tegmental area</article-title><source>Nature</source><volume>482</volume><fpage>85</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1038/nature10754</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalton</surname><given-names>P</given-names></name><name><surname>Doolittle</surname><given-names>N</given-names></name><name><surname>Nagata</surname><given-names>H</given-names></name><name><surname>Breslin</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The merging of the senses: integration of subthreshold taste and smell</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>431</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1038/74797</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Araujo</surname><given-names>IE</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Representation in the human brain of food texture and oral fat</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>3086</fpage><lpage>3093</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0130-04.2004</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Araujo</surname><given-names>IE</given-names></name><name><surname>Simon</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The gustatory cortex and multisensory integration</article-title><source>International Journal of Obesity</source><volume>33 (Suppl 2)</volume><fpage>S34</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1038/ijo.2009.70</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derdikman</surname><given-names>D</given-names></name><name><surname>Hildesheim</surname><given-names>R</given-names></name><name><surname>Ahissar</surname><given-names>E</given-names></name><name><surname>Arieli</surname><given-names>A</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Imaging spatiotemporal dynamics of surround inhibition in the barrels somatosensory cortex</article-title><source>Journal of Neuroscience</source><volume>23</volume><fpage>3100</fpage><lpage>3105</lpage></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devore</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Linster</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Odor preferences shape discrimination learning in rats</article-title><source>Behavioral Neuroscience</source><volume>127</volume><fpage>498</fpage><lpage>504</lpage><pub-id pub-id-type="doi">10.1037/a0033329</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driver</surname><given-names>J</given-names></name><name><surname>Noesselt</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multisensory interplay reveals crossmodal influences on 'sensory-specific' brain regions, neural responses, and judgments</article-title><source>Neuron</source><volume>57</volume><fpage>11</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.12.013</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Escanilla</surname><given-names>OD</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Di Lorenzo</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Odor-taste convergence in the nucleus of the solitary tract of the awake freely licking rat</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>6284</fpage><lpage>6297</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3526-14.2015</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fontanini</surname><given-names>A</given-names></name><name><surname>Grossman</surname><given-names>SE</given-names></name><name><surname>Figueroa</surname><given-names>JA</given-names></name><name><surname>Katz</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Distinct subtypes of basolateral amygdala taste neurons reflect palatability and reward</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>2486</fpage><lpage>2495</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3898-08.2009</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The physiological basis of mental images</article-title><source>Biological Psychiatry</source><volume>18</volume><fpage>1107</fpage><lpage>1125</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>KM</given-names></name><name><surname>Johnston</surname><given-names>TA</given-names></name><name><surname>Shah</surname><given-names>AS</given-names></name><name><surname>Arnold</surname><given-names>L</given-names></name><name><surname>Smiley</surname><given-names>J</given-names></name><name><surname>Hackett</surname><given-names>TA</given-names></name><name><surname>Garraghty</surname><given-names>PE</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Auditory cortical neurons respond to somatosensory stimulation</article-title><source>Journal of Neuroscience</source><volume>23</volume><fpage>7510</fpage><lpage>7515</lpage></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuster</surname><given-names>JM</given-names></name><name><surname>Bodner</surname><given-names>M</given-names></name><name><surname>Kroger</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Cross-modal and cross-temporal association in neurons of frontal cortex</article-title><source>Nature</source><volume>405</volume><fpage>347</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1038/35012613</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallistel</surname><given-names>CR</given-names></name><name><surname>Fairhurst</surname><given-names>S</given-names></name><name><surname>Balsam</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The learning curve: implications of a quantitative analysis</article-title><source>PNAS</source><volume>101</volume><fpage>13124</fpage><lpage>13131</lpage><pub-id pub-id-type="doi">10.1073/pnas.0404965101</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>MP</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Encoding and tracking of outcome-specific expectancy in the gustatory cortex of alert rats</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>13000</fpage><lpage>13017</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1820-14.2014</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gautam</surname><given-names>SH</given-names></name><name><surname>Verhagen</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Retronasal odor representations in the dorsal olfactory bulb of rats</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>7949</fpage><lpage>7959</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1413-12.2012</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Maier</surname><given-names>JX</given-names></name><name><surname>Hoffman</surname><given-names>KL</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multisensory integration of dynamic faces and voices in rhesus monkey auditory cortex</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>5004</fpage><lpage>5012</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0799-05.2005</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Is neocortex essentially multisensory?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>278</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.04.008</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottfried</surname><given-names>JA</given-names></name><name><surname>O'Doherty</surname><given-names>J</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Appetitive and aversive olfactory learning in humans studied using event-related functional magnetic resonance imaging</article-title><source>Journal of Neuroscience</source><volume>22</volume><fpage>10829</fpage><lpage>10837</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossman</surname><given-names>SE</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name><name><surname>Wieskopf</surname><given-names>JS</given-names></name><name><surname>Katz</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Learning-related plasticity of temporal coding in simultaneously recorded amygdala-cortical ensembles</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>2864</fpage><lpage>2873</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4063-07.2008</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guldin</surname><given-names>WO</given-names></name><name><surname>Markowitsch</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Cortical and thalamic afferent connections of the insular and adjacent cortex of the rat</article-title><source>Journal of Comparative Neurology</source><volume>215</volume><fpage>135</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1002/cne.902150203</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Headley</surname><given-names>DB</given-names></name><name><surname>Weinberger</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Relational associative learning induces cross-modal plasticity in early visual cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>1306</fpage><lpage>1318</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht325</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ibrahim</surname><given-names>LA</given-names></name><name><surname>Mesik</surname><given-names>L</given-names></name><name><surname>Ji</surname><given-names>XY</given-names></name><name><surname>Fang</surname><given-names>Q</given-names></name><name><surname>Li</surname><given-names>HF</given-names></name><name><surname>Li</surname><given-names>YT</given-names></name><name><surname>Zingg</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>LI</given-names></name><name><surname>Tao</surname><given-names>HW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cross-modality sharpening of visual cortical processing through layer-1-mediated inhibition and disinhibition</article-title><source>Neuron</source><volume>89</volume><fpage>1031</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.01.027</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iurilli</surname><given-names>G</given-names></name><name><surname>Ghezzi</surname><given-names>D</given-names></name><name><surname>Olcese</surname><given-names>U</given-names></name><name><surname>Lassi</surname><given-names>G</given-names></name><name><surname>Nazzaro</surname><given-names>C</given-names></name><name><surname>Tonini</surname><given-names>R</given-names></name><name><surname>Tucci</surname><given-names>V</given-names></name><name><surname>Benfenati</surname><given-names>F</given-names></name><name><surname>Medini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sound-driven synaptic inhibition in primary visual cortex</article-title><source>Neuron</source><volume>73</volume><fpage>814</fpage><lpage>828</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.026</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>GH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Evolution of colour vision in mammals</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>364</volume><fpage>2957</fpage><lpage>2967</lpage><pub-id pub-id-type="doi">10.1098/rstb.2009.0039</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jezzini</surname><given-names>A</given-names></name><name><surname>Mazzucato</surname><given-names>L</given-names></name><name><surname>La Camera</surname><given-names>G</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Processing of hedonic and chemosensory features of taste in medial prefrontal and insular networks</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>18966</fpage><lpage>18978</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2974-13.2013</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>LM</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name><name><surname>Sadacca</surname><given-names>BF</given-names></name><name><surname>Miller</surname><given-names>P</given-names></name><name><surname>Katz</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Natural stimuli evoke dynamic sequences of states in sensory cortical ensembles</article-title><source>PNAS</source><volume>104</volume><fpage>18772</fpage><lpage>18777</lpage><pub-id pub-id-type="doi">10.1073/pnas.0705546104</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katz</surname><given-names>DB</given-names></name><name><surname>Nicolelis</surname><given-names>MA</given-names></name><name><surname>Simon</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Gustatory processing is dynamic and distributed</article-title><source>Current Opinion in Neurobiology</source><volume>12</volume><fpage>448</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(02)00341-0</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katz</surname><given-names>DB</given-names></name><name><surname>Simon</surname><given-names>SA</given-names></name><name><surname>Nicolelis</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Dynamic and multimodal responses of gustatory cortical neurons in awake rats</article-title><source>Journal of Neuroscience</source><volume>21</volume><fpage>4478</fpage><lpage>4489</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Do early sensory cortices integrate cross-modal information?</article-title><source>Brain Structure and Function</source><volume>212</volume><fpage>121</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1007/s00429-007-0154-0</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name><name><surname>Augath</surname><given-names>M</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Functional imaging reveals visual modulation of specific fields in auditory cortex</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>1824</fpage><lpage>1835</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4737-06.2007</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kusumoto-Yoshida</surname><given-names>I</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>BT</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name><name><surname>Bonci</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Central role for the insular cortex in mediating conditioned responses to anticipatory cues</article-title><source>PNAS</source><volume>112</volume><fpage>1190</fpage><lpage>1195</lpage><pub-id pub-id-type="doi">10.1073/pnas.1416573112</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>CM</given-names></name><name><surname>O'Connell</surname><given-names>MN</given-names></name><name><surname>Mills</surname><given-names>A</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neuronal oscillations and multisensory interaction in primary auditory cortex</article-title><source>Neuron</source><volume>53</volume><fpage>279</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.12.011</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lipton</surname><given-names>PA</given-names></name><name><surname>Alvarez</surname><given-names>P</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Crossmodal associative memory representations in rodent orbitofrontal cortex</article-title><source>Neuron</source><volume>22</volume><fpage>349</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)81095-8</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>CH</given-names></name><name><surname>Coleman</surname><given-names>JE</given-names></name><name><surname>Davoudi</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Hussain Shuler</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Selective activation of a putative reinforcement signal conditions cued interval timing in primary visual cortex</article-title><source>Current Biology</source><volume>25</volume><fpage>1551</fpage><lpage>1561</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.04.028</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>State dependency of chemosensory coding in the gustatory thalamus (VPMpc) of alert rats</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>15479</fpage><lpage>15491</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0839-15.2015</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maier</surname><given-names>JX</given-names></name><name><surname>Blankenship</surname><given-names>ML</given-names></name><name><surname>Li</surname><given-names>JX</given-names></name><name><surname>Katz</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A multisensory network for olfactory processing</article-title><source>Current Biology</source><volume>25</volume><fpage>2642</fpage><lpage>2650</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.060</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maier</surname><given-names>JX</given-names></name><name><surname>Wachowiak</surname><given-names>M</given-names></name><name><surname>Katz</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Chemosensory convergence on primary olfactory cortex</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>17037</fpage><lpage>17047</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3540-12.2012</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazzucato</surname><given-names>L</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name><name><surname>La Camera</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamics of multistable states during ongoing and evoked cortical activity</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>8214</fpage><lpage>8231</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4819-14.2015</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meftah</surname><given-names>el-M</given-names></name><name><surname>Shenasa</surname><given-names>J</given-names></name><name><surname>Chapman</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Effects of a cross-modal manipulation of attention on somatosensory cortical neuronal responses to tactile stimuli in the monkey</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>3133</fpage><lpage>3149</lpage><pub-id pub-id-type="doi">10.1152/jn.00121.2002</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meredith</surname><given-names>MA</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Visual, auditory, and somatosensory convergence on cells in superior colliculus results in multisensory integration</article-title><source>Journal of Neurophysiology</source><volume>56</volume><fpage>640</fpage><lpage>662</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nonkes</surname><given-names>LJ</given-names></name><name><surname>van de Vondervoort</surname><given-names>II</given-names></name><name><surname>de Leeuw</surname><given-names>MJ</given-names></name><name><surname>Wijlaars</surname><given-names>LP</given-names></name><name><surname>Maes</surname><given-names>JH</given-names></name><name><surname>Homberg</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Serotonin transporter knockout rats show improved strategy set-shifting and reduced latent inhibition</article-title><source>Learning &amp; Memory</source><volume>19</volume><fpage>190</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1101/lm.025908.112</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olcese</surname><given-names>U</given-names></name><name><surname>Iurilli</surname><given-names>G</given-names></name><name><surname>Medini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cellular and synaptic architecture of multisensory integration in the mouse neocortex</article-title><source>Neuron</source><volume>79</volume><fpage>579</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.06.010</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ollerenshaw</surname><given-names>DR</given-names></name><name><surname>Bari</surname><given-names>BA</given-names></name><name><surname>Millard</surname><given-names>DC</given-names></name><name><surname>Orr</surname><given-names>LE</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Stanley</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Detection of tactile inputs in the rat vibrissa pathway</article-title><source>Journal of Neurophysiology</source><volume>108</volume><fpage>479</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1152/jn.00004.2012</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ollerenshaw</surname><given-names>DR</given-names></name><name><surname>Bari</surname><given-names>BA</given-names></name><name><surname>Millard</surname><given-names>DC</given-names></name><name><surname>Orr</surname><given-names>LE</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Stanley</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Detection of tactile inputs in the rat vibrissa pathway</article-title><source>Journal of Neurophysiology</source><volume>108</volume><fpage>479</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1152/jn.00004.2012</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pooresmaeili</surname><given-names>A</given-names></name><name><surname>FitzGerald</surname><given-names>TH</given-names></name><name><surname>Bach</surname><given-names>DR</given-names></name><name><surname>Toelch</surname><given-names>U</given-names></name><name><surname>Ostendorf</surname><given-names>F</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cross-modal effects of value on perceptual acuity and stimulus encoding</article-title><source>PNAS</source><volume>111</volume><fpage>15244</fpage><lpage>15249</lpage><pub-id pub-id-type="doi">10.1073/pnas.1408873111</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rainer</surname><given-names>G</given-names></name><name><surname>Asaad</surname><given-names>WF</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Selective representation of relevant information by neurons in the primate prefrontal cortex</article-title><source>Nature</source><volume>393</volume><fpage>577</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/31235</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><collab>R Core Team</collab></person-group><year iso-8601-date="2015">2015</year><source>R: A Language and Enviroment for Statistical Computing</source><publisher-loc>Vienna, Austria</publisher-loc><publisher-name>R Foundation for Stastical Compting</publisher-name></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodgers</surname><given-names>KM</given-names></name><name><surname>Benison</surname><given-names>AM</given-names></name><name><surname>Klein</surname><given-names>A</given-names></name><name><surname>Barth</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Auditory, somatosensory, and multisensory insular cortex in the rat</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>2941</fpage><lpage>2951</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn054</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samuelsen</surname><given-names>CL</given-names></name><name><surname>Gardner</surname><given-names>MP</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Effects of cue-triggered expectation on cortical processing of taste</article-title><source>Neuron</source><volume>74</volume><fpage>410</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.02.031</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samuelsen</surname><given-names>CL</given-names></name><name><surname>Gardner</surname><given-names>MP</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Thalamic contribution to cortical processing of taste and expectation</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>1815</fpage><lpage>1827</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4026-12.2013</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Information coding in the rodent prefrontal cortex. I. Single-neuron activity in orbitofrontal cortex compared with that in pyriform cortex</article-title><source>Journal of Neurophysiology</source><volume>74</volume><fpage>733</fpage><lpage>750</lpage></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Lindsley</surname><given-names>RW</given-names></name><name><surname>Specht</surname><given-names>C</given-names></name><name><surname>Marcovici</surname><given-names>A</given-names></name><name><surname>Smiley</surname><given-names>JF</given-names></name><name><surname>Javitt</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Somatosensory input to auditory association cortex in the macaque monkey</article-title><source>Journal of Neurophysiology</source><volume>85</volume><fpage>1322</fpage><lpage>1327</lpage></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shipley</surname><given-names>MT</given-names></name><name><surname>Geinisman</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Anatomical evidence for convergence of olfactory, gustatory, and visceral afferent pathways in mouse cerebral cortex</article-title><source>Brain Research Bulletin</source><volume>12</volume><fpage>221</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/0361-9230(84)90049-2</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shuler</surname><given-names>MG</given-names></name><name><surname>Bear</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reward timing in the primary visual cortex</article-title><source>Science</source><volume>311</volume><fpage>1606</fpage><lpage>1609</lpage><pub-id pub-id-type="doi">10.1126/science.1123513</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>SA</given-names></name><name><surname>de Araujo</surname><given-names>IE</given-names></name><name><surname>Gutierrez</surname><given-names>R</given-names></name><name><surname>Nicolelis</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The neural mechanisms of gustation: a distributed processing code</article-title><source>Nature Reviews Neuroscience</source><volume>7</volume><fpage>890</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1038/nrn2006</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Multisensory flavor perception</article-title><source>Cell</source><volume>161</volume><fpage>24</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.03.007</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Arigbede</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Unimodal and multimodal response properties of neurons in the cat's superior colliculus</article-title><source>Experimental Neurology</source><volume>36</volume><fpage>179</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1016/0014-4886(72)90145-8</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Rowland</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Development of multisensory integration from the perspective of the individual neuron</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>520</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1038/nrn3742</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talsma</surname><given-names>D</given-names></name><name><surname>Senkowski</surname><given-names>D</given-names></name><name><surname>Soto-Faraco</surname><given-names>S</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The multifaceted interplay between attention and multisensory integration</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>400</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.06.008</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vasconcelos</surname><given-names>N</given-names></name><name><surname>Pantoja</surname><given-names>J</given-names></name><name><surname>Belchior</surname><given-names>H</given-names></name><name><surname>Caixeta</surname><given-names>FV</given-names></name><name><surname>Faber</surname><given-names>J</given-names></name><name><surname>Freire</surname><given-names>MA</given-names></name><name><surname>Cota</surname><given-names>VR</given-names></name><name><surname>Anibal de Macedo</surname><given-names>E</given-names></name><name><surname>Laplagne</surname><given-names>DA</given-names></name><name><surname>Gomes</surname><given-names>HM</given-names></name><name><surname>Ribeiro</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cross-modal responses in the primary visual cortex encode complex objects and correlate with tactile discrimination</article-title><source>PNAS</source><volume>108</volume><fpage>15408</fpage><lpage>15413</lpage><pub-id pub-id-type="doi">10.1073/pnas.1102780108</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veldhuizen</surname><given-names>MG</given-names></name><name><surname>Nachtigal</surname><given-names>D</given-names></name><name><surname>Teulings</surname><given-names>L</given-names></name><name><surname>Gitelman</surname><given-names>DR</given-names></name><name><surname>Small</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The insular taste cortex contributes to odor quality coding</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><elocation-id>58</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2010.00058</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verhagen</surname><given-names>JV</given-names></name><name><surname>Wesson</surname><given-names>DW</given-names></name><name><surname>Netoff</surname><given-names>TI</given-names></name><name><surname>White</surname><given-names>JA</given-names></name><name><surname>Wachowiak</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sniffing controls an adaptive filter of sensory input to the olfactory bulb</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>631</fpage><lpage>639</lpage><pub-id pub-id-type="doi">10.1038/nn1892</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title><source>Science</source><volume>334</volume><fpage>1569</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1126/science.1211095</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>MT</given-names></name><name><surname>Ramachandran</surname><given-names>R</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A revised view of sensory cortical parcellation</article-title><source>PNAS</source><volume>101</volume><fpage>2167</fpage><lpage>2172</lpage><pub-id pub-id-type="doi">10.1073/pnas.0305697101</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watanabe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Frontal units of the monkey coding the associative significance of visual and auditory stimuli</article-title><source>Experimental Brain Research</source><volume>89</volume><fpage>233</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1007/BF00228241</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weinberger</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Associative representational plasticity in the auditory cortex: a synthesis of two disciplines</article-title><source>Learning &amp; Memory</source><volume>14</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1101/lm.421807</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wesson</surname><given-names>DW</given-names></name><name><surname>Verhagen</surname><given-names>JV</given-names></name><name><surname>Wachowiak</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Why sniff fast? The relationship between sniff frequency, odor discrimination, and receptor neuron activation in the rat</article-title><source>Journal of Neurophysiology</source><volume>101</volume><fpage>1089</fpage><lpage>1102</lpage><pub-id pub-id-type="doi">10.1152/jn.90981.2008</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wesson</surname><given-names>DW</given-names></name><name><surname>Wilson</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Smelling sounds: olfactory-auditory sensory convergence in the olfactory tubercle</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>3013</fpage><lpage>3021</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6003-09.2010</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>Q</given-names></name><name><surname>Znamenskiy</surname><given-names>P</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Selective corticostriatal plasticity during acquisition of an auditory discrimination task</article-title><source>Nature</source><volume>521</volume><fpage>348</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1038/nature14225</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Rowland</surname><given-names>BA</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>What does a neuron learn from multisensory experience?</article-title><source>Journal of Neurophysiology</source><volume>113</volume><fpage>883</fpage><lpage>889</lpage><pub-id pub-id-type="doi">10.1152/jn.00284.2014</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamamoto</surname><given-names>T</given-names></name><name><surname>Matsuo</surname><given-names>R</given-names></name><name><surname>Kiyomitsu</surname><given-names>Y</given-names></name><name><surname>Kitamura</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Sensory inputs from the oral region to the cerebral cortex in behaving rats: an analysis of unit responses in cortical somatosensory and taste areas during ingestive behavior</article-title><source>Journal of Neurophysiology</source><volume>60</volume><fpage>1303</fpage><lpage>1321</lpage></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamamoto</surname><given-names>T</given-names></name><name><surname>Matsuo</surname><given-names>R</given-names></name><name><surname>Kiyomitsu</surname><given-names>Y</given-names></name><name><surname>Kitamura</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Taste responses of cortical neurons in freely ingesting rats</article-title><source>Journal of Neurophysiology</source><volume>61</volume><fpage>1244</fpage><lpage>1258</lpage></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamamoto</surname><given-names>T</given-names></name><name><surname>Yuyama</surname><given-names>N</given-names></name><name><surname>Kawamura</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Cortical neurons responding to tactile, thermal and taste stimulations of the rat's tongue</article-title><source>Brain Research</source><volume>221</volume><fpage>202</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(81)91075-1</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yau</surname><given-names>JM</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dissecting neural circuits for multisensory integration and crossmodal processing</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>370</volume><elocation-id>20140203</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2014.0203</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshida</surname><given-names>T</given-names></name><name><surname>Katz</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Control of prestimulus activity related to improved sensory coding within a discrimination task</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>4101</fpage><lpage>4112</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4380-10.2011</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>L</given-names></name><name><surname>Rowland</surname><given-names>BA</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Initiating the development of multisensory integration by manipulating sensory experience</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>4904</fpage><lpage>4913</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5575-09.2010</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>YD</given-names></name><name><surname>Fuster</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Visuo-tactile cross-modal associations in cortical somatosensory cells</article-title><source>PNAS</source><volume>97</volume><fpage>9777</fpage><lpage>9782</lpage><pub-id pub-id-type="doi">10.1073/pnas.97.17.9777</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.16420.018</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mrsic-Flogel</surname><given-names>Thomas D</given-names></name><role>Reviewing editor</role><aff id="aff3"><institution>University of Basel</institution>, <country>Switzerland</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Associative learning changes cross-modal representations in the gustatory cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, Geoffrey Schoenbaum (Reviewer #2) and Marshall G Hussain Shuler (Reviewer #3), and the evaluation has been overseen by a Reviewing Editor and Sabine Kastner as the Senior Editor.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This is an exciting paper that describes &quot;cross-modal&quot; responses (i.e. neural responses to stimuli that are not directly related to a tastant), and their emergence with associative learning, in gustatory cortex. The authors described responses to stimuli (specifically, an air puff, an odor, a light stimulus, and a tone) from varying modalities (somatosensory, olfaction, vision, and audition) when these stimuli were either paired or unpaired with a subsequent delivery of sucrose. As the animals learned this association, (as measured by anticipatory licking), neurons in gustatory cortex (GC) begin to respond to these non-taste stimuli in greater numbers. The authors' claims are supported by the data and the analyses performed. One strength of this work is that the authors presented trained rats with a battery of tastes to show that an increase in cross-modal responses is likely not due to a general increase in responsiveness. Further, they go to efforts to demonstrate that learned orofacial behaviors in anticipation of sucrose are not distinguishable following stimuli in the learned state, whereas neural response fractions and form differ. And while cross-modal stimuli are quite distinguishable early in training at the beginning and end of the stimulus delivery, in the learned state, responses across the ensemble evolve in such a way that stimuli with similar associability become self-similar.</p><p>Thus, in summary, after conditioning, there was more activity associated with cross modal cues, with many GC neurons responding to two or more cues, thus increasing the overlap of neural representations with similar associability. The study is an elegant demonstration of the effects of learning on the activity in a primary sensory area.</p><p>Essential revisions:</p><p>One very minor interpretation issue regards the meaning of fractions of cells exhibiting responses to cross-model stimuli: as the saliency of the stimuli may not be equal across the set, differences in the speed of acquisition may be accounted for on that basis (a possibility identified by the authors). It may be of some interest to match the number of sessions at the final performance level to determine whether the fractions of cells responsive to the cross-modal stimuli are then equal (i.e., is the fraction a property of the number of sessions spent at final performance level?).</p><p>The authors could do a better job in the Results section of the text of explaining their criteria for a cue evoked response and for saying that a neuron did or did not fire to particular cues, as well as how associability was determined. For example, the examples in <xref ref-type="fig" rid="fig1">Figure 1</xref> are very clear, but the criteria and rationale should be described clearly in the main text. Please spell out, as is done for some of the later analyses.</p><p>The authors note in the Discussion (subsection “Cross-modal integration in the gustatory cortex”) that the GC may be an ideal system to look for cross-modal coding. Leaving aside my question above about this, does this suggest that the findings here may not generalize to other primary association regions? Visual cortex? What about the olfactory system? I believe there are studies showing a variety of responses after training in olfactory bulb and piriform cortex to non-olfactory cues, as well as the visual cortex. This should be expanded on in the Discussion.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.16420.019</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions:</italic> </p><p><italic>One very minor interpretation issue regards the meaning of fractions of cells exhibiting responses to cross-model stimuli: as the saliency of the stimuli may not be equal across the set, differences in the speed of acquisition may be accounted for on that basis (a possibility identified by the authors). It may be of some interest to match the number of sessions at the final performance level to determine whether the fractions of cells responsive to the cross-modal stimuli are then equal (i.e., is the fraction a property of the number of sessions spent at final performance level?).</italic> </p><p>Following the reviewer’s suggestion, we performed two additional analyses aimed at investigating if the fractions of cross-modal neurons varied depending on the number of sessions at final performance level. These new analyses are shown in a new figure (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>) and in additional text (see “Representation of cues after associative learning” section of the Results).</p><p>The first analysis compares the fractions of cells responsive to cross-modal stimuli for sessions recorded on the same number of days after reaching final performance level. As described in the main text and the Methods section of the manuscript, our data set originates from recordings performed from day 15 (included) to 21 (included) after the beginning of cue-sucrose conditioning (referred as day 1). As shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, cue-sucrose associations occurred at day 3 for olfactory and somatosensory cues, and day 7 for auditory and visual cues. In order to evaluate the fraction of cue-responding GC neurons after matching the number of recording sessions at final behavioral performance, we analyzed sessions on days 15, 16, 17 for olfactory and somatosensory stimuli and sessions on days 19, 20, 21 for auditory and visual stimuli. Analysis on this smaller set of data confirmed the results presented for the entire dataset. We observed similar fractions of cue responsive neurons, with more GC neurons responding to odor and somatosensory compared to sound and light stimuli.</p><p>To further investigate the point raised by the reviewer, we also computed the distribution of the number of cross-modal responsive neuron across recording days (from day 15 to day 21). If the fraction of GC neurons responsive to cross-modal stimuli depends on the familiarity with learned stimuli, we should observe a gradual increase across days. However, the number of cross-modal neurons responsive to each cue remained constant, with a slight general decrease over time as highlighted by the linear fit (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>).</p><p>Taken together, these new analyses confirm that the bias observed in the number of cue responsive neurons (with more neurons responding to odor and tactile vs. sound and light stimuli) after cue-sucrose association does not depend on the number of sessions at final performance level.</p><p><italic>The authors could do a better job in the Results section of the text of explaining their criteria for a cue evoked response and for saying that a neuron did or did not fire to particular cues, as well as how associability was determined. For example, the examples in <xref ref-type="fig" rid="fig1">Figure 1</xref> are very clear, but the criteria and rationale should be described clearly in the main text. Please spell out, as is done for some of the later analyses.</italic> </p><p>We have revised the main text (see the second paragraph of the “Representation of cross- modal stimuli in GC” section of the Results) and added a more detailed description of the method used to determine neuron responsiveness to cues. We also added more details on how associability was determined.</p><p><italic>The authors note in the Discussion (subsection “Cross-modal integration in the gustatory cortex”) that the GC may be an ideal system to look for cross-modal coding. Leaving aside my question above about this, does this suggest that the findings here may not generalize to other primary association regions? Visual cortex? What about the olfactory system? I believe there are studies showing a variety of responses after training in olfactory bulb and piriform cortex to non-olfactory cues, as well as the visual cortex. This should be expanded on in the Discussion.</italic> </p><p>We agree with the reviewer. We expanded the Discussion (see “Cross-modal integration in the gustatory cortex” section of the Discussion) to comment on the generality of our results and added references to the olfactory system and visual cortex.</p></body></sub-article></article>