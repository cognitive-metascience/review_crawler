<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">53445</article-id><article-id pub-id-type="doi">10.7554/eLife.53445</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Estimating and interpreting nonlinear receptive field of sensory neural responses with deep neural network models</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-163757"><name><surname>Keshishian</surname><given-names>Menoua</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0368-288X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-164503"><name><surname>Akbari</surname><given-names>Hassan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-62090"><name><surname>Khalighinejad</surname><given-names>Bahar</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-156548"><name><surname>Herrero</surname><given-names>Jose L</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-57873"><name><surname>Mehta</surname><given-names>Ashesh D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7293-1101</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-62020"><name><surname>Mesgarani</surname><given-names>Nima</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2987-759X</contrib-id><email>nima@ee.columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Electrical Engineering, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Zuckerman Mind Brain Behavior Institute, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Feinstein Institute for Medical Research</institution><addr-line><named-content content-type="city">Manhasset</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Department of Neurosurgery, Hofstra-Northwell School of Medicine and Feinstein Institute for Medical Research</institution><addr-line><named-content content-type="city">Manhasset</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serre</surname><given-names>Thomas</given-names></name><role>Reviewing Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>26</day><month>06</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e53445</elocation-id><history><date date-type="received" iso-8601-date="2019-11-08"><day>08</day><month>11</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-06-21"><day>21</day><month>06</month><year>2020</year></date></history><permissions><copyright-statement>Â© 2020, Keshishian et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Keshishian et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-53445-v2.pdf"/><abstract><p>Our understanding of nonlinear stimulus transformations by neural circuits is hindered by the lack of comprehensive yet interpretable computational modeling frameworks. Here, we propose a data-driven approach based on deep neural networks to directly model arbitrarily nonlinear stimulus-response mappings. Reformulating the exact function of a trained neural network as a collection of stimulus-dependent linear functions enables a locally linear receptive field interpretation of the neural network. Predicting the neural responses recorded invasively from the auditory cortex of neurosurgical patients as they listened to speech, this approach significantly improves the prediction accuracy of auditory cortical responses, particularly in nonprimary areas. Moreover, interpreting the functions learned by neural networks uncovered three distinct types of nonlinear transformations of speech that varied considerably from primary to nonprimary auditory regions. The ability of this framework to capture arbitrary stimulus-response mappings while maintaining model interpretability leads to a better understanding of cortical processing of sensory signals.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>computational modeling</kwd><kwd>sensory processing</kwd><kwd>speech</kwd><kwd>human auditory cortex</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NIDCD-DC014279</award-id><principal-award-recipient><name><surname>Keshishian</surname><given-names>Menoua</given-names></name><name><surname>Akbari</surname><given-names>Hassan</given-names></name><name><surname>Khalighinejad</surname><given-names>Bahar</given-names></name><name><surname>Mesgarani</surname><given-names>Nima</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Herrero</surname><given-names>Jose L</given-names></name><name><surname>Mehta</surname><given-names>Ashesh D</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>National Science Foundation CAREER awards</award-id><principal-award-recipient><name><surname>Keshishian</surname><given-names>Menoua</given-names></name><name><surname>Mesgarani</surname><given-names>Nima</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A comprehensive, data-driven and interpretable nonlinear computational modeling framework based on deep neural networks uncovers different nonlinear transformations of speech signal in the human auditory cortex.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Creating computational models to predict neural responses from the sensory stimuli has been one of the central goals of sensory neuroscience research (<xref ref-type="bibr" rid="bib30">Hartline, 1940</xref>; <xref ref-type="bibr" rid="bib36">Hubel and Wiesel, 1959</xref>; <xref ref-type="bibr" rid="bib37">Hubel and Wiesel, 1962</xref>; <xref ref-type="bibr" rid="bib24">DÃ¶ving, 1966</xref>; <xref ref-type="bibr" rid="bib47">Laurent and Davidowitz, 1994</xref>; <xref ref-type="bibr" rid="bib94">Wilson, 2001</xref>; <xref ref-type="bibr" rid="bib7">Boudreau, 1974</xref>; <xref ref-type="bibr" rid="bib68">Mountcastle, 1957</xref>). Computational models can be used to form testable hypotheses by predicting the neural response to arbitrary manipulations of a stimulus and can provide a way of explaining complex stimulus-response relationships. As such, computational models that provide an intuitive account of how sensory stimuli are encoded in the brain have been critical in discovering the representational and computational principles of sensory cortices (<xref ref-type="bibr" rid="bib59">Marr and Poggio, 1976</xref>). One simple yet powerful example of such models is the linear receptive field model, which is commonly used in visual (<xref ref-type="bibr" rid="bib36">Hubel and Wiesel, 1959</xref>; <xref ref-type="bibr" rid="bib89">Theunissen et al., 2001</xref>) and auditory (<xref ref-type="bibr" rid="bib88">Theunissen et al., 2000</xref>; <xref ref-type="bibr" rid="bib44">Klein et al., 2006</xref>) neuroscience research. In the auditory domain, the linear spectrotemporal receptive field (STRF) (<xref ref-type="bibr" rid="bib89">Theunissen et al., 2001</xref>; <xref ref-type="bibr" rid="bib44">Klein et al., 2006</xref>; <xref ref-type="bibr" rid="bib2">Aertsen and Johannesma, 1981</xref>) has led to the discovery of neural tuning to various acoustic dimensions, including frequency, response latency, and temporal and spectral modulation (<xref ref-type="bibr" rid="bib66">Miller et al., 2002</xref>; <xref ref-type="bibr" rid="bib95">Woolley et al., 2005</xref>; <xref ref-type="bibr" rid="bib13">Chi et al., 1999</xref>). However, despite the success and ease of interpretability of linear receptive field models, they lack the necessary computational capacity to account for the intrinsic nonlinearities of the sensory processing pathways (<xref ref-type="bibr" rid="bib19">David and Gallant, 2005</xref>). This shortcoming is particularly problematic in higher cortical areas where stimulus representation becomes increasingly more nonlinear (<xref ref-type="bibr" rid="bib14">Christianson et al., 2008</xref>; <xref ref-type="bibr" rid="bib96">Wu et al., 2006</xref>). Several extensions have been proposed to address the limitations of linear models (<xref ref-type="bibr" rid="bib72">Paninski, 2004</xref>; <xref ref-type="bibr" rid="bib83">Sharpee et al., 2004</xref>; <xref ref-type="bibr" rid="bib8">Brenner et al., 2000</xref>; <xref ref-type="bibr" rid="bib38">Kaardal et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Ahrens et al., 2008</xref>; <xref ref-type="bibr" rid="bib62">Mesgarani et al., 2009</xref>; <xref ref-type="bibr" rid="bib34">Hong et al., 2008</xref>; <xref ref-type="bibr" rid="bib81">Schwartz and Simoncelli, 2001</xref>; <xref ref-type="bibr" rid="bib80">Schwartz et al., 2002</xref>; <xref ref-type="bibr" rid="bib9">Butts et al., 2011</xref>; <xref ref-type="bibr" rid="bib60">McFarland et al., 2013</xref>; <xref ref-type="bibr" rid="bib92">Vintch et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Harper et al., 2016</xref>) (see (<xref ref-type="bibr" rid="bib65">Meyer et al., 2016</xref>) for review). These extensions improve the prediction accuracy of neural responses, but this improvement comes at the cost of reduced interpretability of the underlying computation, hence limiting novel insights that can be gained regarding sensory cortical representation. In addition, these methods assume a specific model structure whose parameters are then fitted to the neural data. This assumed model architecture thus limits the range of the nonlinear transformations that they can account for. This lack of a comprehensive yet interpretable computational framework has hindered our ability to understand the nonlinear signal transformations that are found ubiquitously in the sensory processing pathways (<xref ref-type="bibr" rid="bib34">Hong et al., 2008</xref>; <xref ref-type="bibr" rid="bib1">Abbott, 1997</xref>; <xref ref-type="bibr" rid="bib42">Khalighinejad et al., 2019</xref>).</p><p>A general nonlinear modeling framework that has seen great progress in recent years is the multilayer (deep) neural network model (DNN) (<xref ref-type="bibr" rid="bib33">Hinton et al., 2006</xref>; <xref ref-type="bibr" rid="bib52">LeCun et al., 2015</xref>). Theses biologically inspired models are universal function approximators (<xref ref-type="bibr" rid="bib35">Hornik et al., 1989</xref>) and can model any arbitrarily complex input-output relation. Moreover, these data-driven models can learn any form of nonlinearity directly from the data without any explicit assumption or prior knowledge of the nonlinearities. This property makes these models particularly suitable for studying the encoding properties of sensory stimuli in the nervous system (<xref ref-type="bibr" rid="bib4">Batty et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">McIntosh et al., 2016</xref>; <xref ref-type="bibr" rid="bib45">Klindt et al., 2017</xref>) whose anatomical and functional organization remains speculative particularly for natural stimuli. A major drawback of DNN models, however, is the difficulty in interpreting the computation that they implement because these models are analytically intractable (the so-called black box property) (<xref ref-type="bibr" rid="bib58">Mallat, 2016</xref>). Thus, despite their success in increasing the accuracy of prediction in stimulus-response mapping, their utility in leading to novel insights into the computation of sensory nervous systems is lacking.</p><p>To overcome these challenges, we propose a nonlinear regression framework in which a DNN is used to model sensory receptive fields. An important component of our approach is a novel analysis method that allows for the calculation of the mathematically equivalent function of the trained neural network as a collection of stimulus-dependent, linearized receptive fields. As a result, the exact computation of the neural network model can be explained in a manner similar to that of the commonly used linear receptive field model, which enables direct comparison of the two models. Here, we demonstrate the efficacy of this nonlinear receptive field framework by applying it to neural responses recorded invasively in the human auditory cortex of neurosurgical patients as they listened to natural speech. We demonstrate that not only these models more accurately predict the auditory neural responses, but also uncover distinct nonlinear encoding properties of speech in primary and nonprimary auditory cortical areas. These findings show the critical need for more complete and interpretable encoding models of neural processing, which can lead to better understanding of cortical sensory processing.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Neural recordings</title><p>To study the nonlinear receptive fields of auditory cortical responses, we used invasive electrocorticography (ECoG) to directly measure neural activity from five neurosurgical patients undergoing treatment for epilepsy. One patient had high-density subdural grid electrodes implanted on the left hemisphere, with coverage primarily over the superior temporal gyrus (STG). All five patients had depth electrodes (stereotactic EEG) with coverage of Heschlâs gyrus (HG) and STG (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). While HG and the STG are functionally heterogenous and each contain multiple auditory fields (<xref ref-type="bibr" rid="bib71">Nourski et al., 2014</xref>; <xref ref-type="bibr" rid="bib27">Galaburda and Sanides, 1980</xref>; <xref ref-type="bibr" rid="bib67">Morosan et al., 2001</xref>; <xref ref-type="bibr" rid="bib32">Hickok and Saberi, 2012</xref>; <xref ref-type="bibr" rid="bib28">Hamilton et al., 2018</xref>), HG includes the primary auditory cortex, while the STG is considered mostly a nonprimary auditory cortical area (<xref ref-type="bibr" rid="bib15">Clarke and Architecture, 2012</xref>). The patients listened to stories spoken by four speakers (two females) with a total duration of 30 min. All patients had self-reported normal hearing. To ensure that patients were engaged in the task, we intermittently paused the stories and asked the patients to repeat the last sentence before the pause. Eight separate sentences (40 s total) were used as the test data for evaluation of the encoding models, and each sentence was repeated six times in a random order.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Predicting neural responses using linear and nonlinear regression models.</title><p>(<bold>A</bold>) Neural responses to speech were recorded invasively from neurosurgical patients as they listened to speech.Â The brain plot shows electrode locations and t-value of the difference between the average response of a neural site to speech versus silence. The neural responses are predicted from the stimulus using a linear spectrotemporal receptive field model (Lin) and a nonlinear convolutional neural network model (CNN). Input to both models is a sliding time-frequency window with 400 ms duration. (<bold>B</bold>) Actual and predicted responses of three example sites using the Lin and CNN models. (<bold>C</bold>) Prediction accuracy of neural responses from the Lin and CNN models for sites in STG and HG. (<bold>D</bold>) Improved prediction accuracy over Lin model for linear-nonlinear (LN), short-term plasticity (STP), and CNN models. (<bold>E</bold>) Dependence of prediction accuracy on the duration of training data. Circles show average across electrodes and bars indicate standard error. The dashed line is the upper bound of average prediction accuracy for the Lin model across electrodes. The x-axis is in logarithmic scale.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1âsource data 1.</label><caption><title>A MATLAB file containing four variables â group (location of electrode based on anatomy; 1Â =Â Heschlâs gyrus, 2Â =Â superior temporal gyrus), rsquared_lin (noise-adjusted R-squared values of test set prediction by linear model), rqsuared_cnn (noise-adjusted R-squared values of test set prediction by CNN model), improvement (difference of the last two, as used in the <xref ref-type="fig" rid="fig5">Figure 5B</xref> prediction).</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53445-fig1-data1-v2.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 1.</label><caption><title>Selecting stimulus window length for prediction.</title><p>Prediction accuracy of neural responses with varying the duration of the sliding window for linear and CNN models. Bars indicate average prediction values across electrodes and error bars the standard error.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 2.</label><caption><title>Hyperparameter optimization.</title><p>Choosing the hyperparameters of the network to maximize prediction accuracy of the CNN model. The bars indicate average prediction values across electrodes and error bars the standard error.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig1-figsupp2-v2.tif"/></fig></fig-group><p>We used the envelope of the high-gamma (70â150 Hz) band of the recorded signal as our neural response measure, which correlates with the neural firing in the proximity of electrodes (<xref ref-type="bibr" rid="bib76">Ray and Maunsell, 2011</xref>; <xref ref-type="bibr" rid="bib10">BuzsÃ¡ki et al., 2012</xref>). The high-gamma envelope was extracted by first filtering neural signals with a bandpass filter and then calculating the Hilbert envelope. We restricted our analysis to speech-responsive electrodes, which were selected using a t-test between the average response of each electrode to speech stimuli versus the response in silence (t-valueÂ &gt;Â 2). This criterion resulted in 50 out of 60 electrodes in HG and 47 out of 133 in STG. Electrode locations are plotted in <xref ref-type="fig" rid="fig1">Figure 1A</xref> on the average FreeSurfer brain (<xref ref-type="bibr" rid="bib26">Fischl et al., 2004</xref>) where the colors indicate speech responsiveness (t-values).</p><p>We used the auditory spectrogram of speech utterances as the acoustic representation of the stimulus. Auditory spectrograms were calculated using a model of the peripheral auditory system that estimates a time-frequency representation of the acoustic signal on a tonotopic axis (<xref ref-type="bibr" rid="bib97">Yang et al., 1992</xref>). The speech materials were split into three subsets for fitting the models â training, validation, and test. Repetitions of the test set were used to compute a noise-corrected performance metric to reduce the effect of neural noise on model comparisons. The remainder of the data was split between training and validation subsets (97% and 3%, respectively). There was no stimulus overlap between the three subsets. We used the prediction accuracy on the validation set to choose the best weights for the model throughout training.</p></sec><sec id="s2-2"><title>Linear and nonlinear encoding models</title><p>For each neural site, we fit one linear (Lin) and one nonlinear (CNN) regression model to predict its activity from the auditory spectrogram of the stimulus (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The input to the regression models was a sliding window with a duration of 400 ms with 10 ms steps, which was found by maximizing the prediction accuracy (<xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref>). The linear encoding model was a conventional STRF that calculates the linear mapping from stimulus spectrotemporal features to the neural response (<xref ref-type="bibr" rid="bib89">Theunissen et al., 2001</xref>). The nonlinear regression model was implemented using a deep convolutional neural network (CNN; <xref ref-type="bibr" rid="bib51">LeCun et al., 1998</xref>) consisting of two stages: a feature extraction network and a feature summation network. This commonly used nonlinear regression framework (<xref ref-type="bibr" rid="bib50">LeCun et al., 1990</xref>; <xref ref-type="bibr" rid="bib46">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="bib74">Pinto et al., 2009</xref>) consists of extracting a high-dimensional representation of the input (feature extraction) followed by a feature summation network to predict neural responses. The feature extraction network comprises three convolutional layers each with eight 3 Ã 3 2D convolutional kernels, followed by two convolutional kernels with 1 Ã 1 kernels to reduce the dimensionality of the representation, thus decreasing the number of model parameters. The feature summation stage was a two-layer fullyÂ connected network with 32 nodes in the hidden layer and a single output node. All hyperparameters of the network were determined by optimizing the prediction accuracy (<xref ref-type="fig" rid="fig1s2">Figure 1âfigure supplement 2</xref>). All hidden layers had rectified linear unit (ReLU) nonlinearity (<xref ref-type="bibr" rid="bib70">Nair and Hinton, 2010</xref>), and the output node was linear. A combination of mean-squared error and Pearson correlation was used as the training loss function (see Materials and methods).</p></sec><sec id="s2-3"><title>Predicting neural responses using linear and nonlinear encoding models</title><p>Examples of actual and predicted neural responses from the Lin and CNN models are shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref> for three sample electrodes. We examined the nonlinearity of each neural site by comparing the prediction accuracy of Lin and CNN models. As <xref ref-type="fig" rid="fig1">Figure 1B</xref> shows, the CNN predictions (red) are more similar to the actual responses (black) compared to Lin predictions (blue). This observation confirms that the CNN model can capture the variations in the neural responses to speech stimuli more accurately. To quantify this improvement across all sites, we calculated the noise-adjusted R-squared value (<xref ref-type="bibr" rid="bib22">Dean et al., 2005</xref>) (see Materials and methods) between the predicted and actual neural responses for each model. The scatter plot in <xref ref-type="fig" rid="fig1">Figure 1C</xref> shows the comparison of these values obtained for Lin and CNN models for each electrode, where the electrodes are colored by their respective brain region. <xref ref-type="fig" rid="fig1">Figure 1C</xref> shows higher accuracy for CNN predictions compared to Lin predictions for the majority of electrodes (87 out of 97; avg. difference: 0.10; p&lt;&lt;1, t-test). Even though the overall prediction accuracy is higher for HG electrodes than ones in STG, the absolute improved prediction of CNN over Lin is significantly higher for STG electrodes (avg. improvement 0.07 in HG, 0.13 in STG; p&lt;0.003, two sample one-tailed t-test). This higher improvement in STG electrodes reveals a larger degree of nonlinearity in the encoding of speech in the STG compared to HG.</p><p>To compare the CNN model with other common nonlinear extensions of the linear model, we predicted the neural responses using linear-nonlinear (LN) and short-term plasticity (STP) models (<xref ref-type="bibr" rid="bib1">Abbott, 1997</xref>; <xref ref-type="bibr" rid="bib17">David et al., 2009</xref>; <xref ref-type="bibr" rid="bib20">David and Shamma, 2013</xref>; <xref ref-type="bibr" rid="bib91">Tsodyks et al., 1998</xref>; <xref ref-type="bibr" rid="bib55">Lopez Espejo et al., 2019</xref>). <xref ref-type="fig" rid="fig1">Figure 1D</xref> shows the improvement of each model over the linear model, averaged separately across electrodes in HG and STG. While all models improve the prediction accuracy significantly compared to the linear model, the CNN accuracy is considerably higher than the other two, particularly in higher auditory areas (STG) which presumably contain more nonlinearities.</p><p>Since models with more free parameters are more difficult to fit to limited data, we also examined the effect of training data duration on prediction accuracy for both Lin and CNN models (<xref ref-type="fig" rid="fig1">Figure 1E</xref>, see MaterialsÂ andÂ methods). While we can calculate an upper bound for achievable noise-corrected R-squared for the linear model (<xref ref-type="bibr" rid="bib19">David and Gallant, 2005</xref>), deep neural networks are universal approximators (<xref ref-type="bibr" rid="bib35">Hornik et al., 1989</xref>) and hence do not have a theoretical upper bound. Assuming a logarithmic relationship between prediction error (<inline-formula><mml:math id="inf1"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>) and amount of training data, our data suggests that doubling the amount of training data reduces CNNâs prediction error on average by 10.8% Â± 0.5% (standard error). <xref ref-type="fig" rid="fig1">Figure 1E</xref> also shows that the CNN predicts the neural responses significantly better than the Lin model even if the duration of the training data is short. The consistent superiority of the CNN model shows the efficacy of the regularization methods which help avoid the problem of local minima during the training phase.</p></sec><sec id="s2-4"><title>Interpreting the nonlinear receptive field learned by CNNs</title><p>The previous analysis demonstrates the superior ability of the CNN model to predict the cortical representation of speech particularly in higher order areas, but it does not show what types of nonlinear computation result in improved prediction accuracy. To explain the mapping learned by the CNN model, we developed an analysis framework that finds the mathematical equivalent linear function that the neural network applies to each instance of the stimulus. This equivalent function is found by estimating the derivative of the network output with respect to its input (i.e. the data Jacobian matrix [<xref ref-type="bibr" rid="bib93">Wang et al., 2016</xref>]). We refer to this linearized equivalent function as the dynamic spectrotemporal receptive field (DSTRF). The DSTRF can be considered an STRF whose spectrotemporal tuning depends on every instance of the stimulus. As a result, the linear weighting function of the CNN model that is applied to each stimulus instance can be visualized in a manner similar to that of the STRF (see <xref ref-type="video" rid="video1">Videos 1</xref> and <xref ref-type="video" rid="video2">2</xref>).</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-53445-video1.mp4"><label>Video 1.</label><caption><title>The spectrotemporal receptive field (STRF) model applies a weight function to the stimulus to predict the neural response.</title></caption></media><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-53445-video2.mp4"><label>Video 2.</label><caption><title>The dynamic spectrotemporal receptive field (DSTRF) applies a time-varying, stimulus-dependent weight function to the stimulus to predict the neural response.</title></caption></media><p>Finding the DSTRF is particularly straightforward for a neural network with rectified linear unit nodes (ReLU), because ReLU networks implement piecewise linear functions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). A rectified linear node is inactive when the sum of its inputs is negative or is active and behaves like a linear function when the sum of its inputs is positive (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Obtaining a linear equivalent of a CNN for a given stimulus consists of first removing the weights that connect to the inactive nodes in the network (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) and replacing the active nodes with identity functions. Next, the remaining weights of each layer are multiplied to calculate the overall linear weighting function applied to the stimulus. Because the resulting weighting vector has the same dimension as the input to the network, it can be visualized as a multiplicative template similar to an STRF (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The mathematical derivation of DSTRF is shown in Materials and methods (see also <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Calculating the stimulus-dependent dynamic spectrotemporal receptive field.</title><p>(<bold>A</bold>) Activation of nodes in a neural network with rectified linear (ReLU) nonlinearity for the stimulus at time <inline-formula><mml:math id="inf2"><mml:mi>t</mml:mi></mml:math></inline-formula>.Â (<bold>B</bold>) Calculating the stimulus-dependent dynamic spectrotemporal receptive field (DSTRF) for input instance <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> by first removing all inactive nodes from the network and replacing the active nodes with the identity function. The DSTRF is then computed by multiplying the remaining network weights. Reshaping the resulting weight matrix expresses the DSTRF in the same dimensions as the input stimulus and can be interpreted as a multiplicative template applied to the input. Contours indicate 95% significance (jackknife). (<bold>C</bold>) Comparison of piecewise linear (rectified linear neural network) and linear (STRF) approximations of a nonlinear function. (<bold>D</bold>) DSTRF vectors (columns) shown for 40 samples of the stimulus. Only a limited number of lags and frequencies are shown at each time step to assist visualization. (<bold>E</bold>) Normalized sorted singular values of the DSTRF matrix show higher diversity of the learned linear function in STG sites than in HG sites. The bold lines are the averages of sites in the STG and in HG. The complexity of a network is defined as the sum of the sortedÂ normalized singular values. (<bold>F</bold>) Comparison of network complexity and the improved prediction accuracy over the linear model in STG and HG areas. (<bold>G</bold>) Histogram of the average number of switches from on/off to off/on states at each time step for the neural sites in the STG and HG.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2âsource data 1.</label><caption><title>A MATLAB file containing one variable â complexity (neural network complexity, as used in <xref ref-type="fig" rid="fig5">Figure 5A</xref> prediction).</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53445-fig2-data1-v2.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>Converting convolution to matrix multiplication.</title><p>Converting the convolutional neural networks into a feedforward network helps simplify DSTRF calculation.Â The convolution operation is linear; hence, it can be converted into a matrix multiplication. (<bold>A</bold>) Converting 1-d convolution into a 2-d matrix multiplication. The matrices are the shifted version of the 1-d vectors. The <inline-formula><mml:math id="inf4"><mml:mi>i</mml:mi></mml:math></inline-formula>-th row in the output will be the product of the input vectors (shown in green) and the convolution kernel (red) shifted by <inline-formula><mml:math id="inf5"><mml:mi>i</mml:mi></mml:math></inline-formula> steps. (<bold>B</bold>) Converting 2-d convolution to a 2-d weight matrix using the same principle by flattening both the kernel and the input into one-dimensional forms by stacking the columns vertically on top of each other.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 2.</label><caption><title>DSTRF robustness across initializations.</title><p>We trained 10 instances of the CNN model for each electrode and grouped them into two groups of evens and odds.Â For all time points, we calculated the 2D correlation of the average DSTRF from the even models with the average DSTRF from the odd models. (<bold>A</bold>) A histogram of all values where one data point corresponds to the robustness for a specific electrode at a specific time point. (<bold>B</bold>) The relation of robustness with the gain of the linearized function. When the DSTRF has higher gain the function is more robust.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 3.</label><caption><title>Comparing DSTRFs to STRFs.</title><p>(<bold>A</bold>) The average DSTRF calculated across the test dataset is highly correlated with the calculated STRF, especially for HG electrodes.Â (<bold>B</bold>) Similarity of STRF and the average DSTRF is inversely correlated with the complexity of the function the CNN model has learned, as expected. Each data point is an electrode and the R-value represents Spearman correlation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig2-figsupp3-v2.tif"/></fig></fig-group><p>To assign significance to the lag-frequency coefficients of the DSTRFs, we used the jackknife method (<xref ref-type="bibr" rid="bib25">Efron, 1982</xref>) to fit multiple CNNs (nÂ =Â 20) using different segments of the training data. Each of the 20 CNNs were trained by systematically excluding 1/20 of the training data. As a result, the response to each stimulus sample in the test data can be predicted from each of the 20 CNNs, resulting in 20 DSTRFs. The jackknife method results in a distribution for each lag-frequency coefficient of the DSTRF. The resulting DSTRF is the average of this distribution, where the variance signifies the uncertainty of each coefficient. We denoted the significance of the DSTRF coefficients by requiring all positive or all negative values for at least 19 out of the 20 models (corresponding to p = %95, jackknife) as shown by contours in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. We used this method to mask the insignificant coefficients of the DSTRF (p&gt;0.05, jackknife) by setting them to zero.</p><p>An example of DSTRF at different time points for one electrode is shown in <xref ref-type="fig" rid="fig2">Figure 2D</xref> where each column is the vectorized DSTRF (time lag by frequency) that is applied to the stimulus at that time point (for better visibility only part of the actual matrix is shown). This matrix contains all the variations of the receptive fields that the network applies to the stimulus at different time points. On one extreme, a network could apply a fixed receptive field to the stimulus at all times (similar to the linear model) for which the columns of the matrix in <xref ref-type="fig" rid="fig2">Figure 2D</xref> will all be identical. At the other extreme, a network can learn a unique function for each instance of the stimulus for which the columns of the matrix in <xref ref-type="fig" rid="fig2">Figure 2D</xref> will all be different functions. Because a more nonlinear function results in a higher number of piecewise linear regions (<xref ref-type="bibr" rid="bib73">Pascanu et al., 2014</xref>;Â <xref ref-type="fig" rid="fig2">Figure 2C</xref>), the diversity of the functions in the lag-frequency by time matrix (columns in <xref ref-type="fig" rid="fig2">Figure 2D</xref>) indicates the degree of nonlinearity of the function that the network has learned. To quantify this network nonlinearity, we used singular-value decomposition (<xref ref-type="bibr" rid="bib86">Strang, 1993</xref>) of the lag-frequency by time matrix. Each singular value indicates the variance in its corresponding dimension; therefore, the steepness of the sorted singular values is inversely proportional to the diversity of the learned functions. The sorted normalized singular values for all electrodes in HG and STG are shown in <xref ref-type="fig" rid="fig2">Figure 2E</xref>, demonstrating that the neural network models learn considerably more diverse functions for STG electrodes (statistical analysis provided below). This result confirms the increase in nonlinearity observed earlier in STG electrodes compared to HG electrodes.</p></sec><sec id="s2-5"><title>Complexity of the nonlinear receptive field</title><p>We defined the complexity of the CNN model as the sum of the normalized singular values of the lag-frequency by time matrix (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). The complexity for all electrodes in HG and STG is shown in <xref ref-type="fig" rid="fig2">Figure 2F</xref> and is compared against the improved prediction accuracy of the CNN over the linear model. <xref ref-type="fig" rid="fig2">Figure 2F</xref> shows a significantly higher complexity for STG electrodes than for HG electrodes (HG avg.: 41.0, STG avg.: 59.7; p&lt;0.001, two sample t-test), which also correlates with the prediction improvement of each electrode over the linear model (rÂ =Â 0.66, p&lt;0.001). Alternatively, a separate metric to measure the degree of network nonlinearity is the average number of nodes that switch between active and inactive states at each time step. The histogram of the average switches for HG and STG electrodes shows significantly higher values for STG electrodes (<xref ref-type="fig" rid="fig2">Figure 2G</xref>; p&lt;0.01, one-tailed two sample t-test). This observation validates the finding that the larger improvement of prediction accuracy in STG is due to the implementation of a more diverse set of linear templates, whereas the HG electrodes require a smaller number of linear functions to accurately capture the stimulus response relationships in this area. Importantly, these results are not dependent on network parameters and network initialization. While the hyperparameters and the training of the network can change the internal implementation of the input-output function, the function itself is robust and remains unchanged (MaterialsÂ andÂ methods and <xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2</xref>). Furthermore, the linearized functions averaged over all samples closely resemble the STRF for the corresponding electrode, with the similarity decreasing with the complexity of the neural site (<xref ref-type="fig" rid="fig2s3">Figure 2âfigure supplement 3</xref>).</p></sec><sec id="s2-6"><title>Identifying and quantifying various types of nonlinear receptive field properties</title><p>We showed that the nonlinear function of the CNN model can be expressed as a collection of linearized functions. To investigate the properties of these linearized functions learned by the models for various neural sites, we visually inspected the DSTRFs and observed three general types of variations over time, which we refer to as: I) gain change, II) temporal hold, and III) shape change. We describe and quantify each of these three nonlinear computations in this section using three example electrodes that exhibit each of these types more prominently (<xref ref-type="video" rid="video3">Video 3</xref>). The STRFs for the three examples sites are shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Characterizing the types of DSTRF variations.</title><p>Three types of DSTRF variationsÂ areÂ shown over time for three example sites that exhibit each of these types more prominently.Â The DSTRFs have been masked by 95% significance per jackknifing (<inline-formula><mml:math id="inf6"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula>). (<bold>A</bold>) The STRF of the three example sites. (<bold>B</bold>) Example site E1: Gain change of DSTRF, shown as the time-varying magnitude of the DSTRF at three different time points. Although the overall shape of the spectrotemporal receptive field is the same, its magnitude varies across time. E2: Temporal hold property of an DSTRF, seen as the tuning of this site to a fixed spectrotemporal feature but with shifted latency (lag) in consecutive time frames. E3: Shape change property of DSTRF, seen as the change in the spectrotemporal tuning pattern for different instances of the stimulus. (<bold>C</bold>) From top to bottom, distribution of gain change, temporal hold, and shape change values for sites in HG and STG. Horizontal lines mark quartiles. Temporal hold and shape change show significantly higher values in the STG.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3âsource data 1.</label><caption><title>A MATLAB file containing three variables, one for each type of quantified nonlinearity â nonlin_gain_change (gain change nonlinearity), nonlin_temporal_hold (temporal hold nonlinearity), nonlin_shape_change (shape change nonlinearity).</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53445-fig3-data1-v2.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 1.</label><caption><title>Calculating temporal hold.</title><p>(<bold>A, B</bold>) shows the procedure used for calculating temporal hold for two example electrodes.Â The first row in each panel shows the distributions for DSTRF pair similarities when shifted (<inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>) and without shift (<inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>Î±</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>) for all <inline-formula><mml:math id="inf9"><mml:mi>t</mml:mi></mml:math></inline-formula> as described in the methods for three distinct values of time difference between compared DSTRF pairs (<inline-formula><mml:math id="inf10"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>15</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>25</mml:mn></mml:math></inline-formula>), along with their respective p-values from the one-tailed signed-rank test. The second row shows the p-values for all <inline-formula><mml:math id="inf11"><mml:mi>n</mml:mi><mml:mo>â¤</mml:mo><mml:mn>30</mml:mn></mml:math></inline-formula>. The red horizontal line indicates the threshold <inline-formula><mml:math id="inf12"><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>. (<bold>C</bold>) demonstrates how DSTRF samples are shifted and compared with each other to obtain <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>Î±</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 2.</label><caption><title>Nonlinearity robustness to initialization and data.</title><p>(<bold>A</bold>) We trained 10 instances of the CNN model for each electrode and grouped them into two groups of evens and odds. We measured the three nonlinearity parameters from the average DSTRFs of each group on the test dataset, and compared the values from the two conditions. (<bold>B</bold>) We measured the three nonlinearity parameters independently from two splits of the test dataset, and compared the values from the two conditions. The R-values represent Spearman correlation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig3-figsupp2-v2.tif"/></fig></fig-group><media id="video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-53445-video3.mp4"><label>Video 3.</label><caption><title>Visualization of gain change, temporal hold, and shape change for three example electrodes, each demonstrating one type of nonlinearity more prominently.</title></caption></media><sec id="s2-6-1"><title>Gain change nonlinearity</title><p>The first and simplest type of DSTRF change is gain change, which refers to the time-varying magnitude of the DSTRF. This effect is shown for one example site (E1) at three different time points in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. Although the overall shape of the spectrotemporal filter applied to the stimulus at these three time points is not different, the magnitude of the function varies considerably over time. This variation in the gain of the stimulus-response function that is learned by the CNN may reflect the nonlinear adaptation of neural responses to the acoustic stimulus (<xref ref-type="bibr" rid="bib1">Abbott, 1997</xref>; <xref ref-type="bibr" rid="bib75">Rabinowitz et al., 2011</xref>; <xref ref-type="bibr" rid="bib63">Mesgarani et al., 2014a</xref>). We quantified the degree of gain change for each site by calculating the standard deviation of the DSTRF magnitude over the stimulus duration.</p></sec><sec id="s2-6-2"><title>Temporal hold nonlinearity</title><p>The second type of DSTRF change is temporal hold, which refers to the tuning of a site to a fixed spectrotemporal feature but with shifted latency in consecutive time frames (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). This particular tuning nonlinearity results in a sustained response to arbitrarily fast spectrotemporal features, hence resembling a short-term memory of that feature. An example of temporal hold for a site (E2) is shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref> where the three plots show the DSTRF of this site at three consecutive time steps. Even though the overall shape of the spectrotemporal feature tuning remains the same, the latency (lag) of the feature shifts over time with the stimulus. This nonlinear property decouples the duration of the response to an acoustic feature from the temporal resolution of that feature. For example, temporal hold could allow a network to model a slow response to fast acoustic features. This computation cannot be done with a linear operation because a linear increase in the analysis time scale inevitably results in the loss of temporal resolution, as shown in the STRF of site E2 in <xref ref-type="fig" rid="fig3">Figure 3A</xref>.</p><p>We quantified the temporal hold for each site by measuring the similarity of consecutive DSTRFs once the temporal shift is removed. This calculation assumes that when there is temporal hold nonlinearity, the <inline-formula><mml:math id="inf15"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is most correlated with <inline-formula><mml:math id="inf16"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. To quantify temporal hold, we compared the correlation values for all time points <inline-formula><mml:math id="inf17"><mml:mn>1</mml:mn><mml:mo>â¤</mml:mo><mml:mi>t</mml:mi><mml:mo>â¤</mml:mo><mml:mi>T</mml:mi></mml:math></inline-formula> in two conditions: when the consecutive DSTRF is shifted by <inline-formula><mml:math id="inf18"><mml:mi>n</mml:mi></mml:math></inline-formula> lags and when it is not shifted. For each <inline-formula><mml:math id="inf19"><mml:mi>n</mml:mi></mml:math></inline-formula>, the two conditions are compared using a one-tailed Wilcoxon signed-rank test (MaterialsÂ andÂ methods). The largest <inline-formula><mml:math id="inf20"><mml:mi>n</mml:mi></mml:math></inline-formula> for which there is a significant positive change between the shit and no-shift conditions (pÂ &lt;Â 0.05) quantifies the temporal hold for that electrode (<xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>). This calculation assumes that the duration of temporal hold does not depend on instances of the stimulus. An interesting path for future inquiry would be to study the dependence of this effect on specific stimuli.</p></sec><sec id="s2-6-3"><title>Shape change nonlinearity</title><p>The last dimension of DSTRF variation is shape change, which refers to a change in the spectrotemporal tuning of a site across stimuli. Intuitively, a model that implements a more nonlinear function will have a larger number of piecewise linear regions, each exhibiting a different DSTRF shape. An example of shape change for a site (E3) is shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref>, showing three different spectrotemporal patterns at these three different time points. To quantify the degree of change in the shape of the DSTRF for a site, we repeated the calculation of network complexity but after removing the effect of temporal hold from the DSTRFs (see Materials and methods). Therefore, shape change is defined as the sum of the normalized singular values of the shift-corrected lag-frequency by time matrix (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Thus, the shape change indicates the remaining complexity of the DSTRF function that is not due to temporal hold, or gain change (aligned samples differing only in amplitude are captured by the same set of eigenvectors). This stimulus-dependent change in the spectrotemporal tuning of sites reflects a nonlinearity that appears as the sum of all possible shapes in the STRF, as shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref>.</p><p>The distribution of the three nonlinearity types defined here across all neural sites in HG and STG areas are shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref>. Looking at these distributions can give us a better understanding of the shared nonlinear properties among neural populations of each region. The degree of gain change for electrodes in HG and STG areas spans a wide range. However, we did not observe a significant difference between the gain change values in STG and HG sites (HG avg.: 4.6e-3, STG avg.: 4.9e-3; p=0.23, Wilcoxon rank-sum), suggesting a similar degree of adaptive response in HG and STG sites. The distribution of temporal hold values for all sites in the STG and HG shows significantly larger values in STG sites (HG avg.: 10.1, STG avg.: 15.7; p&lt;1e-4, Wilcoxon rank-sum). This increased temporal hold is consistent with the previous findings showing increased temporal integration in higher auditory cortical areas (<xref ref-type="bibr" rid="bib43">King and Nelken, 2009</xref>; <xref ref-type="bibr" rid="bib5">Berezutskaya et al., 2017</xref>), which allows the stimulus information to be combined across longer time scales. The shape change values are significantly higher on average in STG sites than in HG (HG avg.: 38.8, STG avg.: 46.8; p=0.005, Wilcoxon rank-sum), demonstrating that STG sites have more nonlinear encoding, which requires more diverse receptive field templates than HG sites. Finally, similar to DSTRF shape, these quantified nonlinearity parameters were highly consistent across different network initializations and different segments of the test data (<xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2</xref>).</p></sec></sec><sec id="s2-7"><title>Finding subtypes of receptive fields</title><p>As explained in the shape change nonlinearity, the nonlinear model may learn several subtypes of receptive fields for a neural site. To further investigate the subtypes of receptive fields that the CNN model learns for each site, we used the k-means algorithm (<xref ref-type="bibr" rid="bib54">Lloyd, 1982</xref>) to cluster shift-corrected DSTRFs based on their correlation similarity. The optimal number of clusters for each site was determined using the gap statistic method (<xref ref-type="bibr" rid="bib90">Tibshirani et al., 2001</xref>). The optimal number of clusters across all sites differed from 1 to 6; the majority of sites, however, contained only one main cluster (84.9% of sites; mean number of clustersÂ =Â 1.43Â Â±Â 1.26 SD). <xref ref-type="fig" rid="fig4">Figure 4</xref> shows the DSTRF clustering analysis for two example sites, where for each cluster the average DSTRF and the average auditory stimulus for the time points that have DSTRFs belonging to that cluster are shown. The average DSTRFs in <xref ref-type="fig" rid="fig4">Figure 4</xref> show two distinct receptive field shapes that the CNN models apply to the stimulus at different time points. In addition, the average spectrograms demonstrate the distinct time-frequency power in the stimuli that caused the model to choose the corresponding template.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Characterizing the spectrotemporal tuning of electrodes with clustering.</title><p>(<bold>A, B</bold>) The average shift-corrected DSTRF for two example sites and average k-means clustered DSTRFs based on the similarity of DSTRFs (correlation distance). Each cluster shows tuning to a distinct spectrotemporal feature. The average DSTRF shows the sum of these different features. The average spectrograms over the time points at which each cluster DSTRF was selected by the network demonstrate the distinct time-frequency pattern in the stimuli that caused the network to choose the corresponding template.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig4-v2.tif"/></fig></sec><sec id="s2-8"><title>Contribution of nonlinear variations to network complexity and prediction improvement</title><p>So far, we have defined and quantified three types of nonlinear computation for each neural site â gain change, temporal hold, and shape change â resulting in three numbers describing the stimulus-response nonlinearity of the corresponding neural population. Next, we examined how much of the network complexity (<xref ref-type="fig" rid="fig2">Figure 2F</xref>) and improved accuracy over a linear model (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) can be accounted for by these three parameters. We used linear regression to calculate the complexity and prediction improvement for each site from the gain change, temporal hold, and shape change parameters (<xref ref-type="fig" rid="fig5">Figure 5</xref>). The predicted and actual complexity of the models are shown in <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>. The high correlation value (r = 0.94, p &lt; 1e-41) confirms the efficacy of these three parameters to characterize the complexity of the stimulus-response mapping across sites. Moreover, the main effects of the regression (<xref ref-type="bibr" rid="bib82">Seber and Lee, 2012</xref>) shown in <xref ref-type="fig" rid="fig5">Figure 5A</xref> suggest a significant contribution from all three parameters to the overall complexity of DSTRFs in both HG and STG. The high correlation values between the actual and predicted improved accuracy (<xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>; r = 0.76, p &lt; 1e-16) show that these three parameters also largely predict stimulus-response nonlinearity. The contribution of each nonlinear factor in predicting the improved prediction, however, is different between the HG and STG areas, as shown by the main effects of the regression in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. The gain change (effect = 0.183, p = 0.005) and temporal hold (effect = 0.202, p = 1e-4) factors contributed to the improved accuracy in STG sites, while the gain change is the main predictor of the improvement in HG sites (effect = 0.250, p = 2e-5), with a modest contribution from shape change (effect = 0.087, p = 0.028). This result partly reiterates the encoding distinctions we observed in HG and STG sites where temporal hold and shape change were significantly higher in STG than HG (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Notably, the three types of DSTRF variations are not independent of each other and covary considerably (<xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>). This interdependence may explain why the shape change parameter significantly predicts the improved prediction accuracy in HG, but not in STG, because shape change covaries considerably more with the temporal hold parameter in STG (correlation between shape change and temporal hold <inline-formula><mml:math id="inf21"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.24</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf22"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.09</mml:mn></mml:math></inline-formula> in HG; <inline-formula><mml:math id="inf23"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.57</mml:mn></mml:math></inline-formula>, p &lt; 1e - 4 in STG). In other words, sites in STG appear to exhibit multiple nonlinearities simultaneously, making it more difficult to discern their individual contributions. Together, these results demonstrate how our proposed nonlinear encoding method can lead to a comprehensive, intuitive way of studying nonlinear mechanisms in sensory neural processing by utilizing deep neural networks.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Contribution of DSTRF variations to network complexity and prediction improvement.</title><p>(<bold>A</bold>) Predicting the complexity of the neural networks from gain change, temporal hold, and shape change using a linear regression model.Â The main effect of regression analysis shows the significant contribution of all nonlinear parameters in predicting the network complexity in both STG and in HG sites. Legend shows prediction R-values separately for each region. (<bold>B</bold>) Predicting the improved accuracy of neural networks over the linear model from the three nonlinear parameters for each site in the STG and HG. The main effect of the regression analysis shows different contribution of the three nonlinear parameters in predicting the improved accuracy over the linear model in different auditory cortical areas. Figure legends shows prediction R-values separately for each region.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>Complexity and accuracy improvement predicted values.</title><p>True and predicted values for network complexity and improved model accuracy from the three nonlinear parameters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 2.</label><caption><title>Covarying nonlinear parameters.</title><p>The three parameters characterizing the nonlinear properties of neural sites (gain change, temporal hold, and shape change) are not independent and covary significantly. The values represent Spearman correlation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53445-fig5-figsupp2-v2.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We propose a general nonlinear regression framework to model and interpret any complex stimulus-response mapping in sensory neural responses. This data-driven method provides a unified framework that can discover and model various types of nonlinear transformations without needing any prior assumption about the nature of theÂ nonlinearities. In addition, the function learned by the network can be interpreted as a collection of linear receptive fields from which the network chooses for different instances of the stimulus. We demonstrated how this method can be applied to auditory cortical responses in humans to discover novel types of nonlinear transformation of speech signals, therefore extending our knowledge of the nonlinear cortical computations beyond what can be explained with previous models. While the unexplained noise-corrected variance by a linear model indicates the overall nonlinearity of a neural code, this quantity alone is generic and does not inform the types of nonlinear computations that are being used. In contrast, our proposed method unravels various types of nonlinear computation that are present in the neural responses and provides a qualitative and quantitative account of the underlying nonlinearity.</p><p>Together, our results showed three distinct nonlinear properties which largely account for the complexity of the neural network function and could predict the improved prediction accuracy over the linear model. Extracting these nonlinear components and incorporating them in simpler models such as the STRF can systematically test the contribution and interaction of these nonlinear aspects more carefully. However, such simplification and abstraction proved nontrivial in our data because the different types of variation we describe in this paper are not independent of each other and covary considerably. Interestingly, the prediction accuracy improvement of the STP model was significant only in STG responses. The improved prediction accuracy using STP model was also correlated with the gain change parameter (partial <inline-formula><mml:math id="inf24"><mml:mi>r</mml:mi> <mml:mi/><mml:mo>=</mml:mo> <mml:mi/><mml:mn>0.24</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf25"><mml:mi>p</mml:mi><mml:mo>=</mml:mo> <mml:mi/><mml:mn>0.018</mml:mn></mml:math></inline-formula>, Spearman, controlling for temporal hold and shape change) and not temporal hold or shape change (pÂ &gt;Â 0.2 for partial correlations with improvement), meaning that sites with larger gain change saw a greater increase from the STP model. Our STP models included four recovery time constant <inline-formula><mml:math id="inf26"><mml:mi>Ï</mml:mi></mml:math></inline-formula> and four release probability <inline-formula><mml:math id="inf27"><mml:mi>u</mml:mi></mml:math></inline-formula> parameters. We performed a simplified comparison using the average <inline-formula><mml:math id="inf28"><mml:mi>Ï</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf29"><mml:mi>u</mml:mi></mml:math></inline-formula> for each electrode with our parameters. A partial correlation analysis of <inline-formula><mml:math id="inf30"><mml:mi>u</mml:mi></mml:math></inline-formula>, which is an indicator of plasticity strength, revealed only a positive correlation with gain change (partial <inline-formula><mml:math id="inf31"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.28</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf32"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.006</mml:mn></mml:math></inline-formula>, Spearman, controlling for temporal hold and shape change; pÂ &gt;Â 0.8 for partial correlation of <inline-formula><mml:math id="inf33"><mml:mi>u</mml:mi></mml:math></inline-formula> and temporal hold and shape change). On the other hand, <inline-formula><mml:math id="inf34"><mml:mi>Ï</mml:mi></mml:math></inline-formula> had a negative correlation with temporal hold (partial <inline-formula><mml:math id="inf35"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.22</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf36"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.032</mml:mn></mml:math></inline-formula>, Spearman, controlling for gain change and shape change; pÂ &gt;Â 0.16 for partial correlation with gain change; pÂ &gt;Â 0.7 for partial correlation with shape change). These findings are in line with our hypothesis that the gain change nonlinearity captures the nonlinear adaptation of the neural responses to the short-term history of the stimulus.</p><p>The increasing nonlinearity of the stimulus-response mapping throughout the sensory pathways (<xref ref-type="bibr" rid="bib43">King and Nelken, 2009</xref>; <xref ref-type="bibr" rid="bib12">Chechik et al., 2006</xref>) highlights the critical need for nonlinear computational models of sensory neural processing, particularly in higher cortical areas. These important nonlinear transformations include nonmonotonic and nonlinear stimulus-response functions (<xref ref-type="bibr" rid="bib78">Sadagopan and Wang, 2009</xref>), time-varying response properties such as stimulus adaptation and gain control (<xref ref-type="bibr" rid="bib1">Abbott, 1997</xref>; <xref ref-type="bibr" rid="bib75">Rabinowitz et al., 2011</xref>; <xref ref-type="bibr" rid="bib63">Mesgarani et al., 2014a</xref>; <xref ref-type="bibr" rid="bib23">Dean et al., 2008</xref>), and nonlinear interaction of stimulus dimensions and time-varying stimulus encoding (<xref ref-type="bibr" rid="bib57">Machens et al., 2004</xref>). These nonlinear effects are instrumental in creating robust perception, which requires the formation of invariant perceptual categories from a highly variable stimulus (<xref ref-type="bibr" rid="bib49">Leaver and Rauschecker, 2010</xref>; <xref ref-type="bibr" rid="bib11">Chang et al., 2010</xref>; <xref ref-type="bibr" rid="bib21">de Heer et al., 2017</xref>; <xref ref-type="bibr" rid="bib6">Bidelman et al., 2013</xref>; <xref ref-type="bibr" rid="bib64">Mesgarani et al., 2014b</xref>; <xref ref-type="bibr" rid="bib85">Steinschneider, 2013</xref>; <xref ref-type="bibr" rid="bib77">Russ et al., 2007</xref>). The previous research on extending simple receptive field models that have tried to address the linear system limitations include generalized linear models (<xref ref-type="bibr" rid="bib72">Paninski, 2004</xref>), linear-nonlinear (LN) models (<xref ref-type="bibr" rid="bib83">Sharpee et al., 2004</xref>; <xref ref-type="bibr" rid="bib8">Brenner et al., 2000</xref>; <xref ref-type="bibr" rid="bib38">Kaardal et al., 2017</xref>), input nonlinearity models (<xref ref-type="bibr" rid="bib3">Ahrens et al., 2008</xref>; <xref ref-type="bibr" rid="bib17">David et al., 2009</xref>), gain control models (<xref ref-type="bibr" rid="bib34">Hong et al., 2008</xref>; <xref ref-type="bibr" rid="bib81">Schwartz and Simoncelli, 2001</xref>; <xref ref-type="bibr" rid="bib80">Schwartz et al., 2002</xref>), context-dependent encoding models, and LNLN cascade models (<xref ref-type="bibr" rid="bib9">Butts et al., 2011</xref>; <xref ref-type="bibr" rid="bib60">McFarland et al., 2013</xref>; <xref ref-type="bibr" rid="bib92">Vintch et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Harper et al., 2016</xref>) (see (<xref ref-type="bibr" rid="bib65">Meyer et al., 2016</xref>) for review). Even though all these models improve the prediction accuracy of neural responses, this improvement comes at the cost of reduced interpretability of the computation. For example, the multifilter extensions of the auditory STRF (<xref ref-type="bibr" rid="bib83">Sharpee et al., 2004</xref>; <xref ref-type="bibr" rid="bib8">Brenner et al., 2000</xref>; <xref ref-type="bibr" rid="bib38">Kaardal et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Butts et al., 2011</xref>; <xref ref-type="bibr" rid="bib60">McFarland et al., 2013</xref>; <xref ref-type="bibr" rid="bib92">Vintch et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Harper et al., 2016</xref>) lead to nonlinear interactions of multiple linear models, which is considerably less interpretable than the STRF model. Our approach extends the previous methods by significantly improving the prediction accuracy over the linear, linear-nonlinear, and STP models while at the same time, remaining highly interpretable.</p><p>The computational framework we propose to explain the neural network function (<xref ref-type="bibr" rid="bib69">Nagamine and Mesgarani, 2017</xref>) can be used in any feedforward neural network model with any number of layers and nodes, such as in fully connected networks, locally connected networks (<xref ref-type="bibr" rid="bib16">Coates and Ay, 2011</xref>), or CNNs (<xref ref-type="bibr" rid="bib53">LeCun and Bengio, 1995</xref>). Nonetheless, one limitation of the DSTRF method is that it cannot be used for neural networks with recurrent connections. Because feedforward models use a fixed duration of the signal as the input, the range of the temporal dependencies and contextual effects that can be captured with these models is limited. Nevertheless, sensory signals such as speech have long-range temporal dependencies for which recurrent networks may provide a better fit. Although we found only a small difference between the prediction accuracy of feedforward and recurrent neural networks in our data (about 1% improvement in HG, 3% in STG), the recent extensions of the feedforward architecture, such as dilated convolution (<xref ref-type="bibr" rid="bib56">Luo and Mesgarani, 2018</xref>) or temporal convolutional networks (<xref ref-type="bibr" rid="bib48">Lea et al., 2016</xref>), can implement receptive fields that extend over long durations. Our proposed DSTRF method would seamlessly generalize to these architectures, which can serve as an alternative to recurrent neural networks when modeling the long-term dependencies of the stimulus is crucial. Furthermore, while we trained a separate model for each electrode, it is possible to use a network with shared parameters to predict all neural sites. This direction can also be used to examine the connectivity and specialization of the representation across various regions (<xref ref-type="bibr" rid="bib39">Kell et al., 2018</xref>).</p><p>In summary, our proposed framework combines two desired properties of a computational sensory-response model; the ability to capture arbitrary stimulus-response mappings and maintaining model interpretability. We showed that this data-driven method reveals novel nonlinear properties of cortical representation of speech in the human brain which provides an example for how it can be used to create more complete neurophysiological models of sensory processing in the brain.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants and neural recordings</title><p>Five patients with pharmacoresistant focal epilepsy were included in this study. All patients underwent chronic intracranial encephalography (iEEG) monitoring at Northshore University Hospital to identify epileptogenic foci in the brain for later removal. Four patients were implanted with stereo-electroencephalographic (sEEG) depth arrays only and one with both depth electrodes and a high-density grid (PMT, Chanhassen, MN). Electrodes showing any sign of abnormal epileptiform discharges, as identified in the epileptologistsâ clinical reports, were excluded from the analysis. All included iEEG time series were manually inspected for signal quality and were free from interictal spikes. All research protocols were approved and monitored by the institutional review board at the Feinstein Institute for Medical Research (IRB-AAAD5482), and informed written consent to participate in research studies was obtained from each patient before electrode implantation. A minimum of 45 electrodes per brain area was determined to be sufficient for our between region significance comparison analyses.</p><p>iEEG signals were acquired continuously at 3 kHz per channel (16-bit precision, rangeÂ Â±8 mV, DC) with a data acquisition module (Tucker-Davis Technologies, Alachua, FL). Either subdural or skull electrodes were used as references, as dictated by recording quality at the bedside after online visualization of the spectrogram of the signal. Speech signals were recorded simultaneously with the iEEG for subsequent offline analysis. The envelope of the high-gamma response (70â150 Hz) was extracted by first filtering neural signals with a bandpass filter and then using the Hilbert transform to calculate the envelope. The high-gamma responses were z-scored and resampled to 100 Hz.</p></sec><sec id="s4-2"><title>Brain maps</title><p>Electrode positions were mapped to brain anatomy using registration of the postimplant computed tomography (CT) to the preimplant MRI via the postop MRI. After coregistration, electrodes were identified on the postimplantation CT scan using BioImage Suite. Following coregistration, subdural grid and strip electrodes were snapped to the closest point on the reconstructed brain surface of the preimplantation MRI. We used FreeSurfer automated cortical parcellation (<xref ref-type="bibr" rid="bib26">Fischl et al., 2004</xref>) to identify the anatomical regions in which each electrode contact was located with a resolution of approximately 3 mm (the maximum parcellation error of a given electrode to a parcellated area wasÂ &lt;5 voxels/mm). We used Destrieux parcellation, which provides higher specificity in the ventral and lateral aspects of the medial lobe. Automated parcellation results for each electrode were closely inspected by a neurosurgeon using the patientâs coregistered postimplant MRI.</p></sec><sec id="s4-3"><title>Stimulus</title><p>Speech materials consisted of continuous speech stories spoken by four speakers (two male and two female). The duration of the stimulus wasÂ approximately 30 minutes and was sampled at 11025 Hz. The data was split into two segments for training (30 min) and validation (50 s). Additionally, eight sentences totaling 40 s were used for testing the model and presented to the patients six times to improve the signal-to-noise ratio. There was no overlap between the training, test, and validation sets. The input to the regression models was a sliding window of 400 ms (40 timesteps), which was chosen to optimize prediction accuracy (<xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref>). The windowing stride was set to one to maintain the same final sampling rate, and as a result, the two consecutive input vectors to the regression models overlapped at 39 time points.</p></sec><sec id="s4-4"><title>Acoustic representation</title><p>An auditory spectrogram representation of speech was calculated from a model of the peripheral auditory system (<xref ref-type="bibr" rid="bib97">Yang et al., 1992</xref>). This model consists of the following stages: (1) a cochlear filter bank consisting of 128 constant-Q filters equally spaced on a logarithmic axis, (2) a hair cell stage consisting of a low-pass filter and a nonlinear compression function, and (3) a lateral inhibitory network consisting of a first-order derivative along the spectral axis. Finally, the envelope of each frequency band was calculated to obtain a time-frequency representation simulating the pattern of activity on the auditory nerve. The final spectrogram has a sampling frequency of 100 Hz. The spectral dimension was downsampled from 128 frequency channels to 32 channels to reduce theÂ numberÂ of model parameters.</p></sec><sec id="s4-5"><title>Calculating spectrotemporal receptive fields (STRFs)</title><p>Linear STRF models were fitted using the STRFlab MATLAB toolbox (<xref ref-type="bibr" rid="bib89">Theunissen et al., 2001</xref>; <xref ref-type="bibr" rid="bib87">STRFlab, 2020</xref>). For each electrode, a causal model was trained to predict the neural response at each time point from the past 400 ms of stimulus. The optimal model sparsity and regularization parameters were chosen by maximizing the mutual information between the actual and predicted responses for each electrode.</p></sec><sec id="s4-6"><title>Extensions of the linear model</title><p>We used the Neural Encoding Model System (NEMS) python library (<xref ref-type="bibr" rid="bib18">David, 2018</xref>) to fit both linear-nonlinear (LN) and short-term plasticity (STP) models (<xref ref-type="bibr" rid="bib1">Abbott, 1997</xref>; <xref ref-type="bibr" rid="bib17">David et al., 2009</xref>; <xref ref-type="bibr" rid="bib20">David and Shamma, 2013</xref>), using its TensorFlow backend. For the LN model, we used a rank-4 time-frequency separable model with four gaussian kernels for selecting frequency bands, a 400 ms (40 sample) finite impulse response (FIR) filter for each selected frequency band, followed by a double exponential static nonlinearity with trainable parameters:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo>â</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>â</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>â</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Îº</mml:mi></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>â</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We chose this nonlinearity because it performed better than the ReLU in our data. Each gaussian kernel is applied to the input spectrogram separately to create a four-channel output. Each channel is then convolved with its corresponding FIR filter and the final nonlinearity is applied to the sum of the output channels.</p><p>For the STP model, we added a short-term plasticity module to the setup above, between the frequency selection kernels and the temporal response filters. The STP module is parameterized by the Tsodyks-Markram model (<xref ref-type="bibr" rid="bib91">Tsodyks et al., 1998</xref>; <xref ref-type="bibr" rid="bib55">Lopez Espejo et al., 2019</xref>) and has two parameters for each of the four selected frequency bands: release probability <inline-formula><mml:math id="inf37"><mml:mi>u</mml:mi></mml:math></inline-formula> determining the strength of plasticity, and time constant <inline-formula><mml:math id="inf38"><mml:mi>Ï</mml:mi></mml:math></inline-formula> determining the speed of recovery. The corresponding STP equations for channel <inline-formula><mml:math id="inf39"><mml:mi>i</mml:mi></mml:math></inline-formula> are as follows, where <inline-formula><mml:math id="inf40"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the input and output to the module and <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the change in gain due to plasticity:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula></p></sec><sec id="s4-7"><title>DNN architecture</title><p>We designed a two-stage DNN consisting of feature extraction and feature summation modules (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In this framework, a high-dimensional representation of the input is first calculated (feature extraction network), and this representation is then used to regress the output of the model (feature summation network). The feature extraction stage consists of three convolutional layers with eight 3x3 2D convolutional kernels each, followed by a convolutional layer with four 1x1 kernels to reduce the dimensionality of the representation. The output of this stage is the input to another convolutional layer with a single 1x1 kernel. A 1x1 kernel that is applied to an input with <inline-formula><mml:math id="inf43"><mml:mi>N</mml:mi></mml:math></inline-formula> channels has 1x1xN parameters, and its output is a linear combination of the input channels, thus reducing the dimension of the latent variables and, consequently, the number of trainable parameters. The feature summation stage is a two-layer fully connected network with a hidden layer of 32 nodes, followed by an output layer with a single node. All layers except the output layer have <inline-formula><mml:math id="inf44"><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:math></inline-formula> regularization, dropout (<xref ref-type="bibr" rid="bib84">Srivastava et al., 2014</xref>), ReLU (<xref ref-type="bibr" rid="bib70">Nair and Hinton, 2010</xref>) activations, and no bias. The output layer has regularization, linear activation, and a bias term. The parameters of the model, including the number of layers, the size of the convolutional kernels, and the number of fully connected nodes, were found by optimizing the prediction accuracy (<xref ref-type="fig" rid="fig1s2">Figure 1âfigure supplement 2</xref>).</p></sec><sec id="s4-8"><title>DNN training and cross-validation</title><p>The networks were implemented in Keras using the TensorFlow backend. A separate network was trained for each electrode. Kernel weight initializations were performed using a method specifically developed for DNNs with rectified linear nonlinearities (<xref ref-type="bibr" rid="bib31">He et al., 2015</xref>) for faster convergence. We used ReLU nonlinearities for all layers except the last layer, and dropouts with <inline-formula><mml:math id="inf45"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:math></inline-formula> for the convolutional layers and <inline-formula><mml:math id="inf46"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:math></inline-formula> for the first fully connected layer were used to maximize prediction accuracy. The convolutional layers had strides of one, and their inputs were padded with zeros such that the layerâs output would have the same dimension as the input. We applied an L2 penalty (Ridge) with regularization constant of 0.001 to the weights of all the layers. Each training epoch had a batch size of 128, and optimization was performed using Adam with an initial learning rate of 0.0001. Networks were trained with a maximum of 30 epochs, with early stopping when the validation loss did not decrease for five consecutive epochs. The weights that resulted in the best validation loss during all training epochs were chosen as the final weights. The loss function was a linear combination of the MSE and Pearsonâs correlation coefficient:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>in which <inline-formula><mml:math id="inf47"><mml:mi>y</mml:mi></mml:math></inline-formula> is the high-gamma envelope of the recorded neural data from a given electrode, and <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the predicted response of the neural network. We chose this loss because it outperformed the MSE loss in our data.</p></sec><sec id="s4-9"><title>Evaluating model performance (noise-corrected correlation)</title><p>To account for the variations in neural responses that are not due to the acoustic stimulus, we repeated the test stimulus six times to more accurately measure the explainable variance. To obtain a better measure of the modelâs goodness of fit, we used a noise-corrected R-squared value instead of the simple correlation. Having n responses to the same stimulus, <inline-formula><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, we defined <inline-formula><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as the averages of odd and even numbered trials. Then, we calculated the noise-corrected correlation according to the following equations, where <inline-formula><mml:math id="inf53"><mml:msubsup><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> is our reported R-squared of the noise-corrected Pearson correlation. Assume that <inline-formula><mml:math id="inf54"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> consist of the same true signal (<inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) with variance <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and i.i.d. gaussian noise (<inline-formula><mml:math id="inf58"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) with variance <inline-formula><mml:math id="inf60"><mml:msubsup><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>. Given <inline-formula><mml:math id="inf61"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and model prediction <inline-formula><mml:math id="inf63"><mml:mi>P</mml:mi></mml:math></inline-formula>, we want to find the correlation between <inline-formula><mml:math id="inf64"><mml:mi>P</mml:mi></mml:math></inline-formula> and the true signal <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib79">Schoppe et al., 2016</xref>).<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>â</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>â</mml:mo><mml:mfrac><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:msqrt></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-10"><title>Computing DSTRFs for convolutional neural networks</title><p>The first step for calculating the dynamic spectrotemporal receptive field (DSTRF) of a CNN consists of converting the CNN into a multilayer perceptron (MLP) (<xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>) because calculating DSTRFs for an MLP is more straightforward. To achieve this task, we must first convert each convolutional layer to its equivalent locally connected layer, which is essentially a sparse fully connected layer. To do so, we find the equivalent matrix <inline-formula><mml:math id="inf66"><mml:mi>W</mml:mi></mml:math></inline-formula> for convolutional kernels <inline-formula><mml:math id="inf67"><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf68"><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf69"><mml:mi>i</mml:mi></mml:math></inline-formula>-th kernel of the convolutional layer. Transforming all layers of the CNN into fully connected layers results in an MLP network. The input and output tensors of the fully connected layers have only a single dimension, which is usually not the case for convolutional layers. Hence, the inputs and outputs of all layers in the equivalent network are the flattened versions of the original network.</p><p>Assume that all zeros tensor <inline-formula><mml:math id="inf70"><mml:mi>W</mml:mi></mml:math></inline-formula> has dimensions <inline-formula><mml:math id="inf71"><mml:mi>M</mml:mi><mml:mo>Ã</mml:mo><mml:mi>N</mml:mi><mml:mo>Ã</mml:mo><mml:mi>C</mml:mi><mml:mo>Ã</mml:mo><mml:mi>M</mml:mi><mml:mo>Ã</mml:mo><mml:mi>N</mml:mi><mml:mo>Ã</mml:mo><mml:mi>L</mml:mi></mml:math></inline-formula> where <inline-formula><mml:math id="inf72"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf73"><mml:mi>N</mml:mi></mml:math></inline-formula> are, respectively, the rows and columns of the input to a convolutional layer; <inline-formula><mml:math id="inf74"><mml:mi>C</mml:mi></mml:math></inline-formula> is the number of channels in the input; and <inline-formula><mml:math id="inf75"><mml:mi>L</mml:mi></mml:math></inline-formula> is the number of kernels in a layer. Additionally, assume <inline-formula><mml:math id="inf76"><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (the <inline-formula><mml:math id="inf77"><mml:mi>l</mml:mi></mml:math></inline-formula>-th kernel of the layer) has dimensions <inline-formula><mml:math id="inf78"><mml:mi>H</mml:mi><mml:mo>Ã</mml:mo><mml:mi>W</mml:mi><mml:mo>Ã</mml:mo><mml:mi>C</mml:mi></mml:math></inline-formula>, where H and W are the height and width of the kernel, respectively, and <inline-formula><mml:math id="inf79"><mml:mi>C</mml:mi></mml:math></inline-formula> is defined as before. We begin by populating <inline-formula><mml:math id="inf80"><mml:mi>W</mml:mi></mml:math></inline-formula> according to <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> for all values of <inline-formula><mml:math id="inf81"><mml:mi>m</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf82"><mml:mi>n</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf83"><mml:mi>l</mml:mi></mml:math></inline-formula>. Then, we reshape <inline-formula><mml:math id="inf84"><mml:mi>W</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf85"><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mi>*</mml:mi><mml:mi>N</mml:mi><mml:mi>*</mml:mi><mml:mi>C</mml:mi><mml:mo>)</mml:mo><mml:mo>Ã</mml:mo><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mi>*</mml:mi><mml:mi>N</mml:mi><mml:mi>*</mml:mi><mml:mi>L</mml:mi></mml:math></inline-formula>) to obtain the 2D matrix that will transform the flattened input of the convolutional layer to the flattened output. Of course, <inline-formula><mml:math id="inf86"><mml:mi>W</mml:mi></mml:math></inline-formula> can be directly populated as a 2D matrix for improved performance.<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mi>W</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mfenced close="â" open="â" separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>:</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mfenced close="â" open="â" separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mfenced close="â" open="â" separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>:</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mfenced close="â" open="â" separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>The calculation of DSTRFs for the equivalent MLP network involves few steps (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The DSTRF of a network with ReLU activations and no bias in the intermediate layers is equivalent to the gradient of the networkâs output with respect to the input vector (<xref ref-type="bibr" rid="bib69">Nagamine and Mesgarani, 2017</xref>):<disp-formula id="equ10"><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(10)</mml:mtext></mml:mtd><mml:mtd><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>Î¸</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>â¦</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(11)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>â¦</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf87"><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the weighted sum of inputs to nodes in layer <inline-formula><mml:math id="inf88"><mml:mi>l</mml:mi></mml:math></inline-formula> for input <inline-formula><mml:math id="inf89"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf90"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> represents the output of nodes in layer <inline-formula><mml:math id="inf91"><mml:mi>l</mml:mi></mml:math></inline-formula> to the same input, and <inline-formula><mml:math id="inf92"><mml:mi>Î¸</mml:mi></mml:math></inline-formula> denotes the dependence on the parameters of the network. In a network with ReLU nodes:<disp-formula id="equ11"><label>(12)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>â</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">â</mml:mi><mml:mi>z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>â</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mtext>Â </mml:mtext><mml:mi>if</mml:mi><mml:mo>â¡</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>z</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mtext>Â </mml:mtext><mml:mi>if</mml:mi><mml:mo>â¡</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>z</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>which means that we can replace the product of <inline-formula><mml:math id="inf93"><mml:mfrac><mml:mrow><mml:mo>â</mml:mo><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mo>â</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mo>â</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math></inline-formula> and <inline-formula><mml:math id="inf94"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> with an adjusted weight matrix <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf96"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf97"><mml:mi>n</mml:mi></mml:math></inline-formula> are indices of nodes in layer <inline-formula><mml:math id="inf98"><mml:mi>l</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf99"><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>:<disp-formula id="equ12"><label>(13)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mtext>Â </mml:mtext><mml:mi>if</mml:mi><mml:mo>â¡</mml:mo><mml:mtext>Â </mml:mtext><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mi>m</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mtext>Â </mml:mtext><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ13"><label>(14)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>â¦</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Because DSTRF can be defined as the gradient of the output with respect to the input of the network, we can avoid the manual calculation process by utilizing TensorFlowâs built-in automatic differentiation capability.</p></sec><sec id="s4-11"><title>Inference of statistical bounds on DSTRF coefficients</title><p>To estimate the statistical confidence on the values of DSTRFs, we used the jackknife method (<xref ref-type="bibr" rid="bib25">Efron, 1982</xref>). We partitioned the full training data into 20 segments with roughly the same length. We then removed one segment at a time and fit a model using the remaining 19 segments. This procedure results in 20 total trained models for each electrode. During the prediction phase, we calculate the DSTRF from each model for a given stimulus, which results in a distribution for each lag-frequency value. The resulting DSTRF is the mean of this distribution, and the standard error for each coefficient is calculated using the jackknife formula:<disp-formula id="equ14"><label>(15)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mover><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>Â </mml:mtext><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>S</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mrow><mml:mover><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the estimate of the coefficient when removing the <inline-formula><mml:math id="inf101"><mml:mi>i</mml:mi></mml:math></inline-formula>-th block of the data, <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the average of the <inline-formula><mml:math id="inf103"><mml:mi>n</mml:mi></mml:math></inline-formula> estimates, and <inline-formula><mml:math id="inf104"><mml:mi>S</mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the standard error estimate. To assign significance to the coefficients, we found the lag-frequency coefficients that were all positive or all negative for at least %95 of the models (19 out of 20). We denote patches of significant coefficients with black contours.</p></sec><sec id="s4-12"><title>Complexity estimation</title><p>To quantify the nonlinearity of the network receptive field, we measure the diversity of the equivalent linear functions that the network learns for different instances of the stimulus. To measure this function diversity, we calculated the singular-value decomposition (<xref ref-type="bibr" rid="bib86">Strang, 1993</xref>) of the matrix containing all the linearized equivalent functions of a network (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Each singular value indicates the variance in its corresponding dimension; therefore, the steepness of the sorted singular values is inversely proportional to the diversity of the functions that are learned by the network. We define the complexity of the network as the sum of the normalized singular values where <inline-formula><mml:math id="inf105"><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf106"><mml:mi>i</mml:mi></mml:math></inline-formula>-th element of the singular values vector, and <inline-formula><mml:math id="inf107"><mml:mi>D</mml:mi></mml:math></inline-formula> is the length of the vector:<disp-formula id="equ15"><label>(16)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:munder><mml:mo form="prefix">max</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mtext>Â </mml:mtext><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-13"><title>Estimation of gain change</title><p>We calculated the magnitude of the DSTRF at each time point using its standard deviation (<xref ref-type="disp-formula" rid="equ16">equation 17</xref>, FÂ =Â 32, TÂ =Â 40). The gain change parameter for each site was then defined as the standard deviation of the DSTRF magnitude over the duration of the test stimulus. This quantity measures the degree to which the magnitude of the DSTRF changes across different instances of the stimuli.<disp-formula id="equ16"><label>(17)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mo accent="false">Â¯</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>â</mml:mo><mml:mi>T</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ17"><label>(18)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mtext>Â </mml:mtext><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mover><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mo accent="false">Â¯</mml:mo></mml:mover><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-14"><title>Estimation of temporal hold</title><p>Calculation of the temporal hold parameter for a given recording site involves three steps. For a given DSTRF at time <italic>t</italic>, <inline-formula><mml:math id="inf108"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, we first calculate its correlation with <inline-formula><mml:math id="inf109"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and its shift-corrected version, <inline-formula><mml:math id="inf110"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>:<disp-formula id="equ18"><label>(19)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>Â </mml:mtext><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>Â </mml:mtext><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mn>1</mml:mn><mml:mo>â¤</mml:mo><mml:mi>n</mml:mi><mml:mo>â¤</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ19"><label>(20)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>Â </mml:mtext><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>Â </mml:mtext><mml:mo>,</mml:mo><mml:mtext>Â </mml:mtext><mml:mn>1</mml:mn><mml:mo>â¤</mml:mo><mml:mi>n</mml:mi><mml:mo>â¤</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The upper limit 30 corresponds to 300 ms and was empirically found to be sufficient for the range of temporal hold seen in our data. Next, for each <inline-formula><mml:math id="inf111"><mml:mi>n</mml:mi></mml:math></inline-formula>, we perform a one-tailed Wilcoxon signed-rank test to determine if there is a significant positive change between <inline-formula><mml:math id="inf112"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="inf113"><mml:msub><mml:mrow><mml:mi>Î±</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> pairs across the entire test set (<inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>â¦</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). Finally, the temporal hold is defined as the largest <inline-formula><mml:math id="inf115"><mml:mi>n</mml:mi></mml:math></inline-formula> for which the test yields a significant result (pÂ &lt;Â 0.05). Intuitively, these steps find the largest duration in time for which a spectrotemporal pattern persists, assuming that the latency of that pattern shifts over time to keep it aligned with a specific feature in the stimulus (see <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref> for examples).</p></sec><sec id="s4-15"><title>Estimation of shape change</title><p>The shape change parameter represents the diversity of the linear functions that is not due to the gain change or temporal hold nonlinearities. To calculate this parameter, we first removed the effect of temporal hold nonlinearity by time-aligning the DSTRF instances to the average DSTRF across the entire stimuli. This operation is done by finding the best shift, <inline-formula><mml:math id="inf116"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></inline-formula> for DSTRFs at each time instant, <inline-formula><mml:math id="inf117"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. The values of <inline-formula><mml:math id="inf118"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are found iteratively by maximizing the correlation between <inline-formula><mml:math id="inf119"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and the average DSTRF over the entire stimulus duration: <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mo accent="false">Â¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Â </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. At the end of each iteration, the average DSTRF is updated using the new shifted DSTRFs, and this operation is repeated until <inline-formula><mml:math id="inf121"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> values converge, or a maximum number of iterations is reached. In our data, <inline-formula><mml:math id="inf122"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> converged within fifty iterations. After removing the temporal hold effect, we repeated the same procedure used for the calculation of network complexity (<xref ref-type="disp-formula" rid="equ15">Equation 16</xref>) but instead, used the time-aligned DSTRFs to perform the singular-value decomposition. Aligned DSTRFs with same spectrotemporal features but different gain values are captured by the same eigenvectors. The sum of the sorted normalized singular values indicates the diversity of the linear functions learned by the network due to a change in their spectrotemporal feature tuning.</p></sec><sec id="s4-16"><title>Dependence of DSTRFs on stimulus and initialization</title><p>We used neural networks as our nonlinear encoding model which are fitted using stochastic gradient descent algorithms. Because reaching the global minimum in such optimization methods cannot be guaranteed, it is possible that our results may depend on the initialization of network parameters prior to training and the stochasticity involved in training. We tested the robustness of the DSTRF shapes using <inline-formula><mml:math id="inf123"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math></inline-formula> different random initializations of the network weights (<xref ref-type="bibr" rid="bib31">He et al., 2015</xref>). For each electrode, we split the network instances into two groups of <inline-formula><mml:math id="inf124"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and compared the average <inline-formula><mml:math id="inf125"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> across the two groups. We also looked at the relation between the amplitude of an DSTRF and its consistency (<xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>).</p><p>We did a similar analysis for the nonlinearity values extracted from the network, comparing the parameters extracted from the average DSTRFs of each group. In addition, the DSTRFs are estimated by inputting the held-out data (test set) into the neural networks. As a result, the DSTRFs are inherently stimulus dependent. To study the effect of limited test data on DSTRF shapes and the nonlinearity values, we repeated our analysis twice by using non-overlapping halves of the test data. Results are shown in <xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2</xref>.</p></sec><sec id="s4-17"><title>Code availability</title><p>The codes for pre-processing the ECoG signals and calculating the high-gamma envelope are available at <ext-link ext-link-type="uri" xlink:href="http://naplab.ee.columbia.edu/naplib.html">http://naplab.ee.columbia.edu/naplib.html</ext-link> (<xref ref-type="bibr" rid="bib41">Khalighinejad et al., 2017</xref>). Python codes for training the CNN encoding models and computing the DSTRFs, and MATLAB functions for calculating the nonlinearity parameters (gain change, temporal hold, and shape change), are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/naplab/DSTRF">https://github.com/naplab/DSTRF</ext-link>Â (<xref ref-type="bibr" rid="bib40">Keshishian, 2020</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/DSTRF">https://github.com/elifesciences-publications/DSTRF</ext-link>).Â The videos and a link to the Git repository are also available on the project website: <ext-link ext-link-type="uri" xlink:href="http://naplab.ee.columbia.edu/dstrf.html">http://naplab.ee.columbia.edu/dstrf.html</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was funded by a grant from the National Institutes of Health, NIDCD-DC014279,Â andÂ the National Science Foundation CAREER award.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Investigation, Methodology</p></fn><fn fn-type="con" id="con3"><p>Data curation, Formal analysis</p></fn><fn fn-type="con" id="con4"><p>Resources, Data curation</p></fn><fn fn-type="con" id="con5"><p>Resources, Data curation, Investigation</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Data curation, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All research protocols were approved and monitored by the institutional review board at the Feinstein Institute for Medical Research (IRB-AAAD5482), and informed written consent to participate in research studies was obtained from each patient before electrode implantation.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-53445-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Source data files have been provided for Figures 1-3. Raw data cannot be shared as we do not have ethical approval to share this data. To request access to the data, please contact the corresponding author.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Synaptic depression and cortical gain control</article-title><source>Science</source><volume>275</volume><fpage>221</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1126/science.275.5297.221</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aertsen</surname> <given-names>AMHJ</given-names></name><name><surname>Johannesma</surname> <given-names>PIM</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>The Spectro-Temporal receptive field</article-title><source>Biological Cybernetics</source><volume>42</volume><fpage>133</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1007/BF00336731</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname> <given-names>MB</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Inferring input nonlinearities in neural encoding models</article-title><source>Network: Computation in Neural Systems</source><volume>19</volume><fpage>35</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1080/09548980701813936</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Batty</surname> <given-names>E</given-names></name><name><surname>Merel</surname> <given-names>J</given-names></name><name><surname>Brackbill</surname> <given-names>N</given-names></name><name><surname>Heitman</surname> <given-names>A</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Litke</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multilayer recurrent network models of primate retinal ganglion cell responses</article-title><conf-name>ICLR 2017 Conference Submission</conf-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berezutskaya</surname> <given-names>J</given-names></name><name><surname>Freudenburg</surname> <given-names>ZV</given-names></name><name><surname>GÃ¼Ã§lÃ¼</surname> <given-names>U</given-names></name><name><surname>van Gerven</surname> <given-names>MAJ</given-names></name><name><surname>Ramsey</surname> <given-names>NF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural tuning to Low-Level features of speech throughout the perisylvian cortex</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>7906</fpage><lpage>7920</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0238-17.2017</pub-id><pub-id pub-id-type="pmid">28716965</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bidelman</surname> <given-names>GM</given-names></name><name><surname>Moreno</surname> <given-names>S</given-names></name><name><surname>Alain</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Tracing the emergence of categorical speech perception in the human auditory system</article-title><source>NeuroImage</source><volume>79</volume><fpage>201</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.093</pub-id><pub-id pub-id-type="pmid">23648960</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boudreau</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Neural encoding in cat geniculate ganglion tongue units</article-title><source>Chemical Senses</source><volume>1</volume><fpage>41</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1093/chemse/1.1.41</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brenner</surname> <given-names>N</given-names></name><name><surname>Strong</surname> <given-names>SP</given-names></name><name><surname>Koberle</surname> <given-names>R</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>de Ruyter van Steveninck</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Synergy in a neural code</article-title><source>Neural Computation</source><volume>12</volume><fpage>1531</fpage><lpage>1552</lpage><pub-id pub-id-type="doi">10.1162/089976600300015259</pub-id><pub-id pub-id-type="pmid">10935917</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butts</surname> <given-names>DA</given-names></name><name><surname>Weng</surname> <given-names>C</given-names></name><name><surname>Jin</surname> <given-names>J</given-names></name><name><surname>Alonso</surname> <given-names>JM</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Temporal precision in the visual pathway through the interplay of excitation and stimulus-driven suppression</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>11313</fpage><lpage>11327</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0434-11.2011</pub-id><pub-id pub-id-type="pmid">21813691</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>BuzsÃ¡ki</surname> <given-names>G</given-names></name><name><surname>Anastassiou</surname> <given-names>CA</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The origin of extracellular fields and currents--EEG, ECoG, LFP and spikes</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>407</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1038/nrn3241</pub-id><pub-id pub-id-type="pmid">22595786</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>EF</given-names></name><name><surname>Rieger</surname> <given-names>JW</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Berger</surname> <given-names>MS</given-names></name><name><surname>Barbaro</surname> <given-names>NM</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Categorical speech representation in human superior temporal gyrus</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1428</fpage><lpage>1432</lpage><pub-id pub-id-type="doi">10.1038/nn.2641</pub-id><pub-id pub-id-type="pmid">20890293</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chechik</surname> <given-names>G</given-names></name><name><surname>Anderson</surname> <given-names>MJ</given-names></name><name><surname>Bar-Yosef</surname> <given-names>O</given-names></name><name><surname>Young</surname> <given-names>ED</given-names></name><name><surname>Tishby</surname> <given-names>N</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reduction of information redundancy in the ascending auditory pathway</article-title><source>Neuron</source><volume>51</volume><fpage>359</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.06.030</pub-id><pub-id pub-id-type="pmid">16880130</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname> <given-names>T</given-names></name><name><surname>Gao</surname> <given-names>Y</given-names></name><name><surname>Guyton</surname> <given-names>MC</given-names></name><name><surname>Ru</surname> <given-names>P</given-names></name><name><surname>Shamma</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Spectro-temporal modulation transfer functions and speech intelligibility</article-title><source>The Journal of the Acoustical Society of America</source><volume>106</volume><fpage>2719</fpage><lpage>2732</lpage><pub-id pub-id-type="doi">10.1121/1.428100</pub-id><pub-id pub-id-type="pmid">10573888</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christianson</surname> <given-names>GB</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Linden</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The consequences of response nonlinearities for interpretation of spectrotemporal receptive fields</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>446</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1775-07.2007</pub-id><pub-id pub-id-type="pmid">18184787</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Clarke</surname> <given-names>S</given-names></name><name><surname>Architecture</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Architecture, Connectivity, and Transmitter Receptors of Human Auditory Cortex</chapter-title><person-group person-group-type="editor"><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Overath</surname> <given-names>T</given-names></name><name><surname>Popper</surname> <given-names>A</given-names></name><name><surname>Fay</surname> <given-names>R</given-names></name></person-group><source>The Human Auditory Cortex</source><publisher-name>Springer</publisher-name><fpage>11</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1007/978-1-4614-2314-0_2</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Coates</surname> <given-names>A</given-names></name><name><surname>Ay</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Selecting receptive fields in deep networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>2528</fpage><lpage>2536</lpage></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Fritz</surname> <given-names>JB</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Rapid synaptic depression explains nonlinear modulation of spectro-temporal tuning in primary auditory cortex by natural stimuli</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>3374</fpage><lpage>3386</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5249-08.2009</pub-id><pub-id pub-id-type="pmid">19295144</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>David</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Neural Encoding Model System (NEMS)</source><ext-link ext-link-type="uri" xlink:href="https://github.com/LBHB/NEMS">https://github.com/LBHB/NEMS</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Predicting neuronal responses during natural vision</article-title><source>Network: Computation in Neural Systems</source><volume>16</volume><fpage>239</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1080/09548980500464030</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Integration over multiple timescales in primary auditory cortex</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>19154</fpage><lpage>19166</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2270-13.2013</pub-id><pub-id pub-id-type="pmid">24305812</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Heer</surname> <given-names>WA</given-names></name><name><surname>Huth</surname> <given-names>AG</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name><name><surname>Theunissen</surname> <given-names>FE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The hierarchical cortical organization of human speech processing</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6539</fpage><lpage>6557</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3267-16.2017</pub-id><pub-id pub-id-type="pmid">28588065</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dean</surname> <given-names>I</given-names></name><name><surname>Harper</surname> <given-names>NS</given-names></name><name><surname>McAlpine</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural population coding of sound level adapts to stimulus statistics</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1684</fpage><lpage>1689</lpage><pub-id pub-id-type="doi">10.1038/nn1541</pub-id><pub-id pub-id-type="pmid">16286934</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dean</surname> <given-names>I</given-names></name><name><surname>Robinson</surname> <given-names>BL</given-names></name><name><surname>Harper</surname> <given-names>NS</given-names></name><name><surname>McAlpine</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Rapid neural adaptation to sound level statistics</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>6430</fpage><lpage>6438</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0470-08.2008</pub-id><pub-id pub-id-type="pmid">18562614</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DÃ¶ving</surname> <given-names>KB</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>An electrophysiological study of odour similarities of homologous substances</article-title><source>The Journal of Physiology</source><volume>186</volume><fpage>97</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1966.sp008022</pub-id><pub-id pub-id-type="pmid">5914260</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Efron</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>The Jackknife, the Bootstrap and Other Resampling Plans</source><publisher-name>Society for Industrial and Applied Mathematics</publisher-name></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>van der Kouwe</surname> <given-names>A</given-names></name><name><surname>Destrieux</surname> <given-names>C</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name><name><surname>SÃ©gonne</surname> <given-names>F</given-names></name><name><surname>Salat</surname> <given-names>DH</given-names></name><name><surname>Busa</surname> <given-names>E</given-names></name><name><surname>Seidman</surname> <given-names>LJ</given-names></name><name><surname>Goldstein</surname> <given-names>J</given-names></name><name><surname>Kennedy</surname> <given-names>D</given-names></name><name><surname>Caviness</surname> <given-names>V</given-names></name><name><surname>Makris</surname> <given-names>N</given-names></name><name><surname>Rosen</surname> <given-names>B</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Automatically parcellating the human cerebral cortex</article-title><source>Cerebral Cortex</source><volume>14</volume><fpage>11</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg087</pub-id><pub-id pub-id-type="pmid">14654453</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galaburda</surname> <given-names>A</given-names></name><name><surname>Sanides</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Cytoarchitectonic organization of the human auditory cortex</article-title><source>The Journal of Comparative Neurology</source><volume>190</volume><fpage>597</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1002/cne.901900312</pub-id><pub-id pub-id-type="pmid">6771305</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname> <given-names>LS</given-names></name><name><surname>Edwards</surname> <given-names>E</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A spatial map of onset and sustained responses to speech in the human superior temporal gyrus</article-title><source>Current Biology</source><volume>28</volume><fpage>1860</fpage><lpage>1871</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.04.033</pub-id><pub-id pub-id-type="pmid">29861132</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harper</surname> <given-names>NS</given-names></name><name><surname>Schoppe</surname> <given-names>O</given-names></name><name><surname>Willmore</surname> <given-names>BD</given-names></name><name><surname>Cui</surname> <given-names>Z</given-names></name><name><surname>Schnupp</surname> <given-names>JW</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Network receptive field modeling reveals extensive integration and Multi-feature selectivity in auditory cortical neurons</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005113</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005113</pub-id><pub-id pub-id-type="pmid">27835647</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartline</surname> <given-names>HK</given-names></name></person-group><year iso-8601-date="1940">1940</year><article-title>The receptive fields of optic nerve fibers</article-title><source>American Journal of Physiology-Legacy Content</source><volume>130</volume><fpage>690</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1152/ajplegacy.1940.130.4.690</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Delving deep into rectifiers: surpassing human-level performance on imagenet classification</article-title><conf-name>Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)</conf-name><fpage>1026</fpage><lpage>1034</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2015.123</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hickok</surname> <given-names>G</given-names></name><name><surname>Saberi</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Redefining the functional organization of the planum temporale region: space, objects, and sensoryâmotor integration</chapter-title><person-group person-group-type="editor"><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Overath</surname> <given-names>T</given-names></name><name><surname>Popper</surname> <given-names>A</given-names></name><name><surname>Fay</surname> <given-names>R</given-names></name></person-group><source>The Human Auditory Cortex</source><publisher-name>Springer</publisher-name><fpage>333</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1007/978-1-4614-2314-0_12</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname> <given-names>GE</given-names></name><name><surname>Osindero</surname> <given-names>S</given-names></name><name><surname>Teh</surname> <given-names>YW</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A fast learning algorithm for deep belief nets</article-title><source>Neural Computation</source><volume>18</volume><fpage>1527</fpage><lpage>1554</lpage><pub-id pub-id-type="doi">10.1162/neco.2006.18.7.1527</pub-id><pub-id pub-id-type="pmid">16764513</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname> <given-names>S</given-names></name><name><surname>Lundstrom</surname> <given-names>BN</given-names></name><name><surname>Fairhall</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Intrinsic gain modulation and adaptive neural coding</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000119</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000119</pub-id><pub-id pub-id-type="pmid">18636100</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hornik</surname> <given-names>K</given-names></name><name><surname>Stinchcombe</surname> <given-names>M</given-names></name><name><surname>White</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Multilayer feedforward networks are universal approximators</article-title><source>Neural Networks</source><volume>2</volume><fpage>359</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1016/0893-6080(89)90020-8</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Receptive fields of single neurones in the cat's striate cortex</article-title><source>The Journal of Physiology</source><volume>148</volume><fpage>574</fpage><lpage>591</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1959.sp006308</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex</article-title><source>The Journal of Physiology</source><volume>160</volume><fpage>106</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id><pub-id pub-id-type="pmid">14449617</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaardal</surname> <given-names>JT</given-names></name><name><surname>Theunissen</surname> <given-names>FE</given-names></name><name><surname>Sharpee</surname> <given-names>TO</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A Low-Rank method for characterizing High-Level neural computations</article-title><source>Frontiers in Computational Neuroscience</source><volume>11</volume><elocation-id>68</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2017.00068</pub-id><pub-id pub-id-type="pmid">28824408</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname> <given-names>AJE</given-names></name><name><surname>Yamins</surname> <given-names>DLK</given-names></name><name><surname>Shook</surname> <given-names>EN</given-names></name><name><surname>Norman-Haignere</surname> <given-names>SV</given-names></name><name><surname>McDermott</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A Task-Optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title><source>Neuron</source><volume>98</volume><fpage>630</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.044</pub-id><pub-id pub-id-type="pmid">29681533</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Keshishian</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>DSTRF</data-title><source>GitHub</source><version designator="6e49ddc">6e49ddc</version><ext-link ext-link-type="uri" xlink:href="https://github.com/naplab/DSTRF">https://github.com/naplab/DSTRF</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Khalighinejad</surname> <given-names>B</given-names></name><name><surname>Nagamine</surname> <given-names>T</given-names></name><name><surname>Mehta</surname> <given-names>A</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>NAPLib: an open source toolbox for real-time and offline neural acoustic processing</article-title><conf-name>Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on 2017</conf-name><fpage>846</fpage><lpage>850</lpage><pub-id pub-id-type="doi">10.1109/ICASSP.2017.7952275</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khalighinejad</surname> <given-names>B</given-names></name><name><surname>Herrero</surname> <given-names>JL</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Adaptation of the human auditory cortex to changing background noise</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2509</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10611-4</pub-id><pub-id pub-id-type="pmid">31175304</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>AJ</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Unraveling the principles of auditory cortical processing: can we learn from the visual system?</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>698</fpage><lpage>701</lpage><pub-id pub-id-type="doi">10.1038/nn.2308</pub-id><pub-id pub-id-type="pmid">19471268</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname> <given-names>DJ</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name><name><surname>Depireux</surname> <given-names>DA</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Stimulus-invariant processing and spectrotemporal reverse correlation in primary auditory cortex</article-title><source>Journal of Computational Neuroscience</source><volume>20</volume><fpage>111</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1007/s10827-005-3589-4</pub-id><pub-id pub-id-type="pmid">16518572</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Klindt</surname> <given-names>D</given-names></name><name><surname>Ecker</surname> <given-names>AS</given-names></name><name><surname>Euler</surname> <given-names>T</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural system identification for large populations separating âwhatâ and âwhere.â</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>3509</fpage><lpage>3519</lpage><pub-id pub-id-type="doi">10.12751/nncn.bc2017.0132</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>ImageNet classification with deep convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laurent</surname> <given-names>G</given-names></name><name><surname>Davidowitz</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Encoding of olfactory information with oscillating neural assemblies</article-title><source>Science</source><volume>265</volume><fpage>1872</fpage><lpage>1875</lpage><pub-id pub-id-type="doi">10.1126/science.265.5180.1872</pub-id><pub-id pub-id-type="pmid">17797226</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lea</surname> <given-names>C</given-names></name><name><surname>Vidal</surname> <given-names>R</given-names></name><name><surname>Reiter</surname> <given-names>A</given-names></name><name><surname>Hager</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Temporal convolutional networks: A unified approach to action segmentation</chapter-title><person-group person-group-type="editor"><name><surname>Hua</surname> <given-names>G</given-names></name><name><surname>JÃ©gou</surname> <given-names>H</given-names></name></person-group><source>European Conference on Computer Vision</source><publisher-name>Springer</publisher-name><fpage>47</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-49409-8_7</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leaver</surname> <given-names>AM</given-names></name><name><surname>Rauschecker</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical representation of natural complex sounds: effects of acoustic features and auditory object category</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>7604</fpage><lpage>7612</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0296-10.2010</pub-id><pub-id pub-id-type="pmid">20519535</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Boser</surname> <given-names>BE</given-names></name><name><surname>Denker</surname> <given-names>JS</given-names></name><name><surname>Henderson</surname> <given-names>D</given-names></name><name><surname>Howard</surname> <given-names>RE</given-names></name><name><surname>Hubbard</surname> <given-names>WE</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Handwritten digit recognition with a back-propagation network</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>396</fpage><lpage>404</lpage></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Bottou</surname> <given-names>L</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Haffner</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Gradient-based learning applied to document recognition</article-title><conf-name>Proceedings of the IEEEÂ </conf-name><fpage>2278</fpage><lpage>2323</lpage><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Convolutional networks for images, speech, and time series</article-title><conf-name>Â the Handbook of Brain Theory and Neural Networks</conf-name></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lloyd</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Least squares quantization in PCM</article-title><conf-name>IEEE Transactions on Information Theory</conf-name><fpage>129</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1109/TIT.1982.1056489</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez Espejo</surname> <given-names>M</given-names></name><name><surname>Schwartz</surname> <given-names>ZP</given-names></name><name><surname>David</surname> <given-names>SV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spectral tuning of adaptation supports coding of sensory context in auditory cortex</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007430</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007430</pub-id><pub-id pub-id-type="pmid">31626624</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>Y</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Conv-TasNet: surpassing ideal Time-Frequency magnitude masking for speech separation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1809.07454">https://arxiv.org/abs/1809.07454</ext-link></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machens</surname> <given-names>CK</given-names></name><name><surname>Wehr</surname> <given-names>MS</given-names></name><name><surname>Zador</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Linearity of cortical receptive fields measured with natural sounds</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>1089</fpage><lpage>1100</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4445-03.2004</pub-id><pub-id pub-id-type="pmid">14762127</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mallat</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Understanding deep convolutional networks</article-title><source>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</source><volume>5</volume><elocation-id>203</elocation-id><pub-id pub-id-type="doi">10.1098/rsta.2015.0203</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname> <given-names>D</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1976">1976</year><source>From Understanding Computation to Understanding Neural Circuitry</source><publisher-name>MIT Libraries</publisher-name></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McFarland</surname> <given-names>JM</given-names></name><name><surname>Cui</surname> <given-names>Y</given-names></name><name><surname>Butts</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inferring nonlinear neuronal computation based on physiologically plausible inputs</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003143</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003143</pub-id><pub-id pub-id-type="pmid">23874185</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McIntosh</surname> <given-names>LT</given-names></name><name><surname>Maheswaranathan</surname> <given-names>N</given-names></name><name><surname>Nayebi</surname> <given-names>A</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name><name><surname>Baccus</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep learning models of the retinal response to natural scenes</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1369</fpage><lpage>1377</lpage></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Fritz</surname> <given-names>JB</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Influence of context and behavior on stimulus reconstruction from neural activity in primary auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>3329</fpage><lpage>3339</lpage><pub-id pub-id-type="doi">10.1152/jn.91128.2008</pub-id><pub-id pub-id-type="pmid">19759321</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Fritz</surname> <given-names>JB</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Mechanisms of noise robust representation of speech in primary auditory cortex</article-title><source>PNAS</source><volume>111</volume><fpage>6792</fpage><lpage>6797</lpage><pub-id pub-id-type="doi">10.1073/pnas.1318017111</pub-id><pub-id pub-id-type="pmid">24753585</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Cheung</surname> <given-names>C</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Phonetic feature encoding in human superior temporal gyrus</article-title><source>Science</source><volume>343</volume><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>AF</given-names></name><name><surname>Williamson</surname> <given-names>RS</given-names></name><name><surname>Linden</surname> <given-names>JF</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Models of neuronal Stimulus-Response functions: elaboration, estimation, and evaluation</article-title><source>Frontiers in Systems Neuroscience</source><volume>10</volume><elocation-id>109</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2016.00109</pub-id><pub-id pub-id-type="pmid">28127278</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>LM</given-names></name><name><surname>EscabÃ­</surname> <given-names>MA</given-names></name><name><surname>Read</surname> <given-names>HL</given-names></name><name><surname>Schreiner</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Spectrotemporal receptive fields in the lemniscal auditory thalamus and cortex</article-title><source>Journal of Neurophysiology</source><volume>87</volume><fpage>516</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1152/jn.00395.2001</pub-id><pub-id pub-id-type="pmid">11784767</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morosan</surname> <given-names>P</given-names></name><name><surname>Rademacher</surname> <given-names>J</given-names></name><name><surname>Schleicher</surname> <given-names>A</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name><name><surname>Schormann</surname> <given-names>T</given-names></name><name><surname>Zilles</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Human primary auditory cortex: cytoarchitectonic subdivisions and mapping into a spatial reference system</article-title><source>NeuroImage</source><volume>13</volume><fpage>684</fpage><lpage>701</lpage><pub-id pub-id-type="doi">10.1006/nimg.2000.0715</pub-id><pub-id pub-id-type="pmid">11305897</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mountcastle</surname> <given-names>VB</given-names></name></person-group><year iso-8601-date="1957">1957</year><article-title>Modality and topographic properties of single neurons of cat's somatic sensory cortex</article-title><source>Journal of Neurophysiology</source><volume>20</volume><fpage>408</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1152/jn.1957.20.4.408</pub-id><pub-id pub-id-type="pmid">13439410</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nagamine</surname> <given-names>T</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Understanding the representation and computation of multilayer perceptrons: a case study in speech recognition</article-title><conf-name>International Conference on Machine Learning</conf-name><fpage>2564</fpage><lpage>2573</lpage></element-citation></ref><ref id="bib70"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nair</surname> <given-names>V</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Rectified linear units improve restricted boltzmann machines</article-title><conf-name>Proceedings of the 27th International Conference on Machine Learning (ICML-10</conf-name><fpage>807</fpage><lpage>814</lpage></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nourski</surname> <given-names>KV</given-names></name><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>McMurray</surname> <given-names>B</given-names></name><name><surname>Kovach</surname> <given-names>CK</given-names></name><name><surname>Oya</surname> <given-names>H</given-names></name><name><surname>Kawasaki</surname> <given-names>H</given-names></name><name><surname>Howard</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Functional organization of human auditory cortex: investigation of response latencies through direct recordings</article-title><source>NeuroImage</source><volume>101</volume><fpage>598</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.07.004</pub-id><pub-id pub-id-type="pmid">25019680</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Maximum likelihood estimation of cascade point-process neural encoding models</article-title><source>Network: Computation in Neural Systems</source><volume>15</volume><fpage>243</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_15_4_002</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pascanu</surname> <given-names>R</given-names></name><name><surname>Cho</surname> <given-names>K</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>On the number of linear regions of deep neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1402.1869">https://arxiv.org/abs/1402.1869</ext-link></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname> <given-names>N</given-names></name><name><surname>Doukhan</surname> <given-names>D</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name><name><surname>Cox</surname> <given-names>DD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A high-throughput screening approach to discovering good forms of biologically inspired visual representation</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000579</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000579</pub-id><pub-id pub-id-type="pmid">19956750</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinowitz</surname> <given-names>NC</given-names></name><name><surname>Willmore</surname> <given-names>BD</given-names></name><name><surname>Schnupp</surname> <given-names>JW</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contrast gain control in auditory cortex</article-title><source>Neuron</source><volume>70</volume><fpage>1178</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.04.030</pub-id><pub-id pub-id-type="pmid">21689603</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ray</surname> <given-names>S</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Different origins of gamma rhythm and high-gamma activity in macaque visual cortex</article-title><source>PLOS Biology</source><volume>9</volume><elocation-id>e1000610</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000610</pub-id><pub-id pub-id-type="pmid">21532743</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russ</surname> <given-names>BE</given-names></name><name><surname>Lee</surname> <given-names>YS</given-names></name><name><surname>Cohen</surname> <given-names>YE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural and behavioral correlates of auditory categorization</article-title><source>Hearing Research</source><volume>229</volume><fpage>204</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2006.10.010</pub-id><pub-id pub-id-type="pmid">17208397</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadagopan</surname> <given-names>S</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Nonlinear spectrotemporal interactions underlying selectivity for complex sounds in auditory cortex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>11192</fpage><lpage>11202</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1286-09.2009</pub-id><pub-id pub-id-type="pmid">19741126</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoppe</surname> <given-names>O</given-names></name><name><surname>Harper</surname> <given-names>NS</given-names></name><name><surname>Willmore</surname> <given-names>BD</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name><name><surname>Schnupp</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Measuring the performance of neural models</article-title><source>Frontiers in Computational Neuroscience</source><volume>10</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2016.00010</pub-id><pub-id pub-id-type="pmid">26903851</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schwartz</surname> <given-names>O</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Characterizing neural gain control using spike-triggered covariance</article-title><conf-name>Advances in Neural Information Processing System</conf-name><fpage>269</fpage><lpage>276</lpage></element-citation></ref><ref id="bib81"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schwartz</surname> <given-names>O</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural sound statistics and divisive normalization in the auditory system</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>166</fpage><lpage>172</lpage></element-citation></ref><ref id="bib82"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Seber</surname> <given-names>GAF</given-names></name><name><surname>Lee</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Linear Regression Analysis</source><publisher-name>John Wiley &amp; Sons</publisher-name></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpee</surname> <given-names>T</given-names></name><name><surname>Rust</surname> <given-names>NC</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Analyzing neural responses to natural signals: maximally informative dimensions</article-title><source>Neural Computation</source><volume>16</volume><fpage>223</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1162/089976604322742010</pub-id><pub-id pub-id-type="pmid">15006095</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname> <given-names>N</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Salakhutdinov</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title><source>Journal of Machine Learning Research : JMLR</source><volume>15</volume><fpage>1929</fpage><lpage>1958</lpage></element-citation></ref><ref id="bib85"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Steinschneider</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>Phonemic Representations and Categories</chapter-title><person-group person-group-type="editor"><name><surname>Cohen</surname> <given-names>Y. E</given-names></name><name><surname>Popper</surname> <given-names>A. N</given-names></name><name><surname>Fay</surname> <given-names>R. R</given-names></name></person-group><source>Neural Correlates of Auditory Cognition</source><publisher-name>Springer</publisher-name><fpage>151</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1007/978-1-4614-2350-8_6</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Strang</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1993">1993</year><source>Introduction to Linear Algebra</source><publisher-name>Wellesley-Cambridge Press</publisher-name></element-citation></ref><ref id="bib87"><element-citation publication-type="software"><person-group person-group-type="author"><collab>STRFlab</collab></person-group><year iso-8601-date="2020">2020</year><data-title>STRFlab</data-title><publisher-name>UC Berkeley</publisher-name><ext-link ext-link-type="uri" xlink:href="http://strflab.berkeley.edu">http://strflab.berkeley.edu</ext-link></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname> <given-names>FE</given-names></name><name><surname>Sen</surname> <given-names>K</given-names></name><name><surname>Doupe</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>2315</fpage><lpage>2331</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-06-02315.2000</pub-id><pub-id pub-id-type="pmid">10704507</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname> <given-names>FE</given-names></name><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Singh</surname> <given-names>NC</given-names></name><name><surname>Hsu</surname> <given-names>A</given-names></name><name><surname>Vinje</surname> <given-names>WE</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli</article-title><source>Network: Computation in Neural Systems</source><volume>12</volume><fpage>289</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1080/net.12.3.289.316</pub-id><pub-id pub-id-type="pmid">11563531</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname> <given-names>R</given-names></name><name><surname>Walther</surname> <given-names>G</given-names></name><name><surname>Hastie</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Estimating the number of clusters in a data set via the gap statistic</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>63</volume><fpage>411</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1111/1467-9868.00293</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname> <given-names>M</given-names></name><name><surname>Pawelzik</surname> <given-names>K</given-names></name><name><surname>Markram</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Neural networks with dynamic synapses</article-title><source>Neural Computation</source><volume>10</volume><fpage>821</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.1162/089976698300017502</pub-id><pub-id pub-id-type="pmid">9573407</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vintch</surname> <given-names>B</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A convolutional subunit model for neuronal responses in macaque V1</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>14829</fpage><lpage>14841</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2815-13.2015</pub-id><pub-id pub-id-type="pmid">26538653</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>S</given-names></name><name><surname>Mohamed</surname> <given-names>A-R</given-names></name><name><surname>Caruana</surname> <given-names>R</given-names></name><name><surname>Bilmes</surname> <given-names>J</given-names></name><name><surname>Plilipose</surname> <given-names>M</given-names></name><name><surname>Richardson</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Analysis of deep neural networks with the extended data jacobian matrix</article-title><conf-name>Proceedings of the 33rd International Conference on Machine Learning</conf-name><fpage>718</fpage><lpage>726</lpage></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Receptive fields in the rat piriform cortex</article-title><source>Chemical Senses</source><volume>26</volume><fpage>577</fpage><lpage>584</lpage><pub-id pub-id-type="doi">10.1093/chemse/26.5.577</pub-id><pub-id pub-id-type="pmid">11418503</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolley</surname> <given-names>SM</given-names></name><name><surname>Fremouw</surname> <given-names>TE</given-names></name><name><surname>Hsu</surname> <given-names>A</given-names></name><name><surname>Theunissen</surname> <given-names>FE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Tuning for spectro-temporal modulations as a mechanism for auditory discrimination of natural sounds</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1371</fpage><lpage>1379</lpage><pub-id pub-id-type="doi">10.1038/nn1536</pub-id><pub-id pub-id-type="pmid">16136039</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname> <given-names>MC</given-names></name><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Complete functional characterization of sensory neurons by system identification</article-title><source>Annual Review of Neuroscience</source><volume>29</volume><fpage>477</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113024</pub-id><pub-id pub-id-type="pmid">16776594</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>X</given-names></name><name><surname>Wang</surname> <given-names>K</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Auditory representations of acoustic signals</article-title><source>IEEE Transactions on Information Theory</source><volume>38</volume><fpage>824</fpage><lpage>839</lpage><pub-id pub-id-type="doi">10.1109/18.119739</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53445.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Serre</surname><given-names>Thomas</given-names></name><role>Reviewing Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Englitz</surname><given-names>Bernhard</given-names> </name><role>Reviewer</role><aff><institution>Radboud Universiteit</institution><country>Netherlands</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Lesica</surname><given-names>Nicholas A</given-names></name><role>Reviewer</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This manuscript provides a novel take on modeling cortical responses using general, nonlinear neural network models while maintaining a high and desirable level of interpretability. The quantitative results demonstrate the success of the method, in comparison to more classic approaches, and the subsequent analysis provides an accessible and insightful interpretation of the âstrategiesâ used by the neural network models to achieve this improved performance.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Estimating and interpreting nonlinear receptive fields of sensory responses with deep neural network models&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Bernhard Englitz (Reviewer #1); Nicholas A Lesica (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The reviewers found that the manuscript provides a novel take on modeling cortical responses using general, nonlinear models while maintaining a high and desirable level of interpretability. In particular, the study demonstrates the potential of deep networks for the analysis of intracranial recordings from the human auditory cortex during the presentation of speech. The approach involves fitting deep networks and then analyzing linear equivalents of the fits to provide insights into nonlinear sensory processing. The reported quantitative results demonstrate the potential of the method, in comparison to simpler / classic models such as STRFs, and the subsequent analysis provides an accessible and insightful interpretation of the computational strategies used by the deep network models to achieve this improved performance.</p><p>As a general comment, the reviewers (and the reviewing editor) would like to encourage the authors to make software for their model publicly available.</p><p>Essential revisions:</p><p>The reviewers raised a number of concerns that must be adequately addressed before the paper can be accepted. Some of the required revisions will likely require further experimentation within the framework of the presented studies and techniques.</p><p>â Response evaluation with noise-adjusted correlation: The authors account for the possibility of noise in the responses by computing the noise-adjusted correlation, detailed in the Materials and methods. It was not clear to the reviewers where this method was referenced from since they could not find it mentioned in Dean, Harper and McApline, 2005 (though they could have easily missed it). The description in the Materials and methods is mathematically clear, but does not clarify the properties of this measure, e.g. under which circumstances would a 1 value be achieved? Please comment.</p><p>While this adjustment does something potentially related, one reviewer's suggestion is to use instead, the principled approach of predictive power (Sahani and Linden 2003, NeurIPS), which gives a good sense of absolute performance in the context of noise. Otherwise, the authors should explain/introduce the noise-adjusted correlation more fully.</p><p>â As presented, there is uncertainty about how much of what is being fitted is actually truly reflective of sensory processing vs. variability in the (linearized) fits due to the limited amount of data used for training. Since these networks are universal function approximators, they could in principle capture 100% of the explainable variance in the data if they are given enough training samples. In that perfectly predictive regime, we could be very confident that differences between fits reflect true differences in sensory processing. But if we are not in this regime, then we cannot know what fraction of the differences between fits is ârealâ without further investigation. The authors describe something along these lines in the section âLinearized function robustnessâ. But this only assesses robustness against different initial conditions for the fits, not the number of training samples. A more detailed evaluation of the robustness of the estimates as a function of the amount of available training data would help address this concern. Specifically, the reviewers would like to see some type of analysis that assesses the degree to which adding more data would increase performance and/or change the fits. Perhaps something along the lines of bootstrap resampling to at least put confidence intervals on a statistic would help.</p><p>â On a related note: since performance is measured as explainable variance (rather than just variance) the implicit goal is to capture only the stimulus-driven portion of the response. But the training is performed on single-trial data. With enough training samples the models will learn to ignore non-stimulus driven fluctuations even if they are trained on single-trial data, but how do we know that we have enough training samples to be in this regime? If we are not, then isn't there a worry that the fluctuations the time-point-by-time-point linearized fits reflect attempts to capture non-stimulus driven variance? In short, it is critical to quantify how much the (differences between) (linearized) fits would be expected to change if more training samples were used. Ideally, they wouldn't change at all. But given the reality of limited experimental data, we need a principled approach to derive confidence intervals just as we would for any other statistics, especially if this general approach is to be used meaningfully by non-experts.</p><p>â Comparison with more advanced neural models: While the currently presented method has the clear advantage of time-to-time interpretability of the kernel, the reviewers were not fully convinced that the absolute level of predictability was necessarily best-in-class (which is also not claimed in the text, but it is hard to assess where it falls within the set of existing models). It would thus be important to compare the DNN fit against two more general models, i.e. an STRF with a static nonlinearity and a model including adaptation (e.g., as proposed by Stephen David at OHSU). In particular, relating the extracted three properties to the adaptation parameter of the latter model could provide some unification between modeling approaches.</p><p>â Temporal hold: The identification of the temporal hold property was seen as one of the most exciting results from the manuscript. Hence, the reviewers request that it be analyzed more thoroughly. The criterion for the lag duration was just given as âsignificantly correlatedâ, without specification of a test, p-threshold, or a correction for multiple testing. In particular, it is not clear what happens if it drops briefly below the threshold but is significantly correlated afterward (e.g. oscillations?). Given the novelty of this result, it would be important to devote more detailed analysis to these questions (e.g., using permutation tests a la Koenig and Melie-Garcia, Brain Topography (2010), which is fast to implement and includes the corresponding options.)</p><p>â Method limitation: If one wants to measure the degree of nonlinearity in a given brain area, isn't it sufficient to simply look at how much of the explainable variance is or is not captured by a linear model? What else is learned by looking at the performance of a particular nonlinear model in this context? In fact, it would seem that using the linear model alone would be the only way to make a general statement in this context. Just because one particular nonlinear model fits one area better than another (or improves predictions in one area more than another) doesn't necessarily show that one or the other area is, in general, more nonlinear. It only suggests the nonlinearity in one area is more readily fit by that particular network given limited training samples. Please comment.</p><p>â Statistical testing: The statistical testing performed needs to be detailed more to include effect sizes.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53445.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The reviewers raised a number of concerns that must be adequately addressed before the paper can be accepted. Some of the required revisions will likely require further experimentation within the framework of the presented studies and techniques.</p><p>â Response evaluation with noise-adjusted correlation: The authors account for the possibility of noise in the responses by computing the noise-adjusted correlation, detailed in the Materials and methods. It was not clear to the reviewers where this method was referenced from since they could not find it mentioned in Dean, Harper and McApline, 2005 (though they could have easily missed it). The description in the Materials and methods is mathematically clear, but does not clarify the properties of this measure, e.g. under which circumstances would a 1 value be achieved? Please comment.</p><p>While this adjustment does something potentially related, one reviewer's suggestion is to use instead, the principled approach of predictive power (Sahani and Linden 2003, NeurIPS), which gives a good sense of absolute performance in the context of noise. Otherwise, the authors should explain/introduce the noise-adjusted correlation more fully.</p></disp-quote><p>We have thoroughly expanded our derivation of the formula based on the normalized correlation measure (<inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mtext mathvariant="normal">norm</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>) in (Schoppe et al., 2016) for <inline-formula><mml:math id="inf127"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, and fixed the reference. Also, in that article Schoppe et al. relate the <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mtext mathvariant="normal">norm</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> to the signal power explained proposed by (Sahani and Linden, 2003).</p><p>âHaving &lt;inline-graphic mime-subtype=&quot;x-emf&quot; mimetype=&quot;image&quot; xlink:href=&quot;media/image1.emf&quot; /&gt; responses to the same stimulus, <inline-formula><mml:math id="inf129"><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf130"><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>, we defined <inline-formula><mml:math id="inf131"><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf132"><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> as the averages of odd and even numbered trials. Then, we calculated the noise-corrected correlation according to the following equations, where <inline-formula><mml:math id="inf133"><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is our reported R-squared of the noise-corrected Pearson correlation. Assume that <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> consist of the same true signal (<inline-formula><mml:math id="inf136"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with variance <inline-formula><mml:math id="inf137"><mml:msubsup><mml:mi>Ï</mml:mi><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> and i.i.d. gaussian noise (<inline-formula><mml:math id="inf138"><mml:msub><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf139"><mml:msub><mml:mi>n</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>) with variance <inline-formula><mml:math id="inf140"><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. Given <inline-formula><mml:math id="inf141"><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf142"><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>, and model prediction <inline-formula><mml:math id="inf143"><mml:mi>P</mml:mi></mml:math></inline-formula>, we want to find the correlation between <inline-formula><mml:math id="inf144"><mml:mi>P</mml:mi></mml:math></inline-formula> and the true signal <inline-formula><mml:math id="inf145"><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover></mml:math></inline-formula> (95).</p><p><inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mspace width="0.222em"/></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf148"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> (5)<disp-formula id="equ24"><label>(6)</label><mml:math id="m24"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="normal">COV</mml:mtext><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="normal">COV</mml:mtext><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="normal">COV</mml:mtext><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>â</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ25"><label>(7)</label><mml:math id="m25"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="normal">COV</mml:mtext><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="normal">COV</mml:mtext><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:msub><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>â</mml:mo><mml:mfrac><mml:msubsup><mml:mi>Ï</mml:mi><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf149"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:msub></mml:msqrt></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="normal">COV</mml:mtext><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:msub><mml:mi>Ï</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="normal">COV</mml:mtext><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (8)â</p><p>By this notation, one would expect a value of 1 when the model can fully capture the true signal <inline-formula><mml:math id="inf150"><mml:mover><mml:mi>R</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover></mml:math></inline-formula>.</p><disp-quote content-type="editor-comment"><p>â As presented, there is uncertainty about how much of what is being fitted is actually truly reflective of sensory processing vs. variability in the (linearized) fits due to the limited amount of data used for training. Since these networks are universal function approximators, they could in principle capture 100% of the explainable variance in the data if they are given enough training samples. In that perfectly predictive regime, we could be very confident that differences between fits reflect true differences in sensory processing. But if we are not in this regime, then we cannot know what fraction of the differences between fits is ârealâ without further investigation. The authors describe something along these lines in the section âLinearized function robustnessâ. But this only assesses robustness against different initial conditions for the fits, not the number of training samples. A more detailed evaluation of the robustness of the estimates as a function of the amount of available training data would help address this concern. Specifically, the reviewers would like to see some type of analysis that assesses the degree to which adding more data would increase performance and/or change the fits. Perhaps something along the lines of bootstrap resampling to at least put confidence intervals on a statistic would help.</p></disp-quote><p>We agree with the reviewers that a better measure of uncertainty is needed. To address the question of how the performance of our fits would change with the amount of data, we have fit both the linear and CNN models to varying amounts of training data, specifically 20 random partitions of 5/10/25/60 percent (a la David and Gallant, 2005), in addition to using the full dataset 20 times. (Figure 1E)</p><p>For the relationship between the data length and performance for the linear model, we can use the formulation in (David and Gallant, 2005) to calculate an upper bound on the explained variance of the data by the model (dashed line in the figure shows average upper bound). Since neural networks are universal approximators, theoretically a deep neural network can reach the 100% noise-corrected explanatory power. Assuming added data has roughly a uniform diversity, and the amount of prediction error and data length follow a logarithmic relationship, we can estimate from our data that on average, the amount of noise-corrected unexplained variance will reduce by 10.8% by doubling the amount of training data.</p><p>To address the realness of the linearized fits, we have taken two precautions. First, our training, validation and test sets are disjoint, so that if a single trial noise is learned by the network, the chances of it showing up in the test set (where we do our linearized fit analysis) is minimal. Second, we have added a jackknife significance analysis to our computed linearized fits to determine their significant portions, which is explained in the answer to the next question:</p><disp-quote content-type="editor-comment"><p>â On a related note: since performance is measured as explainable variance (rather than just variance) the implicit goal is to capture only the stimulus-driven portion of the response. But the training is performed on single-trial data. With enough training samples the models will learn to ignore non-stimulus driven fluctuations even if they are trained on single-trial data, but how do we know that we have enough training samples to be in this regime? If we are not, then isn't there a worry that the fluctuations the time-point-by-time-point linearized fits reflect attempts to capture non-stimulus driven variance? In short, it is critical to quantify how much the (differences between) (linearized) fits would be expected to change if more training samples were used. Ideally, they wouldn't change at all. But given the reality of limited experimental data, we need a principled approach to derive confidence intervals just as we would for any other statistics, especially if this general approach is to be used meaningfully by non-experts.</p></disp-quote><p>This is an important point indeed. As noted in the previous section, we have added a jackknife analysis with <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> to measure our confidence in the resulting linearized fits. To this end, we partitioned the training dataset into 20 equal-size segments, then trained 20 models by leaving out one segment at a time. For a given stimulus window, we then compute the linearized fits of each model. This analysis results in 20 values for the weights of each lag-frequency coefficient. We use the empirical mean of the distribution as our DSTRF weights and compute a standard error according to the jackknife formula, where <inline-formula><mml:math id="inf152"><mml:msub><mml:mover><mml:mi>Î¸</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the estimated value from the model that was fit with the <inline-formula><mml:math id="inf153"><mml:mi>i</mml:mi></mml:math></inline-formula>-th block of data removed:</p><p><inline-formula><mml:math id="inf154"><mml:mrow><mml:mover><mml:mi>Î¸</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:msubsup><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mover><mml:mi>Î¸</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mtext mathvariant="normal">jack</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:msubsup><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mover><mml:mi>Î¸</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:mover><mml:mi>Î¸</mml:mi><mml:mo accent="true">Ì</mml:mo></mml:mover><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>, (15)</p><p>For a given lag-frequency coefficient in the DSTRF, if 95% (19/20) of these values are above zero, we denote that as a significantly positive weight, and if 95% are below zero, a significantly negative weight. We show these significant regions with contours in our update Figure 2.</p><p>In our data, almost all visible weights ended up as significant in this analysis. Hence, one can use these significance maps (corresponding to 95%) to mask the non-significant weights of the DSTRF for a given stimulus window, thus improving the signal to noise ratio of the estimated DSTRFs. The samples displayed in the new Figure 3 are masked by the significance maps.</p><p>Additionally, to obtain confidence intervals on the nonlinear parameter estimations extracted from the CNNs for each electrode, one can use a similar jackknifing analysis. Due to the significant computational cost involved in such an endeavor, we used a more practical analysis. We measured the robustness of our extracted parameters to training by computing them on two separate groups of trained models, and to test data by computing them on two splits of the test dataset. (Figure 3âfigure supplement 2)</p><disp-quote content-type="editor-comment"><p>â Comparison with more advanced neural models: While the currently presented method has the clear advantage of time-to-time interpretability of the kernel, the reviewers were not fully convinced that the absolute level of predictability was necessarily best-in-class (which is also not claimed in the text, but it is hard to assess where it falls within the set of existing models). It would thus be important to compare the DNN fit against two more general models, i.e. an STRF with a static nonlinearity and a model including adaptation (e.g., as proposed by Stephen David at OHSU). In particular, relating the extracted three properties to the adaptation parameter of the latter model could provide some unification between modeling approaches.</p></disp-quote><p>We agree that comparison with other nonlinear models is important and necessary. To address this point, we fitted two additional models to our data, a linear model with static nonlinearity (LN), and a short-term plasticity (STP) model as implemented in Stephen Davidâs toolbox (Neural Encoding Model System, available at: https://github.com/LBHB/NEMS/). We compared the performance of the two additional models and our CNN with the baseline linear model (Figure 1D). The results indicate that the CNN significantly outperforms the other models, both in HG and STG (p &lt; 0.001, paired t-test).</p><p>âOur STP models included four recovery time constant <inline-formula><mml:math id="inf156"><mml:mi>Ï</mml:mi></mml:math></inline-formula> and four release probability <inline-formula><mml:math id="inf157"><mml:mi>u</mml:mi></mml:math></inline-formula> parameters. We performed a simplified comparison using the average <inline-formula><mml:math id="inf158"><mml:mi>Ï</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf159"><mml:mi>u</mml:mi></mml:math></inline-formula> for each electrode with our parameters. A partial correlation analysis of <inline-formula><mml:math id="inf160"><mml:mi>u</mml:mi></mml:math></inline-formula>, which is an indicator of plasticity strength, revealed only a positive correlation with gain change (partial <inline-formula><mml:math id="inf161"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.28</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf162"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.006</mml:mn></mml:mrow></mml:math></inline-formula>, Spearman, controlling for temporal hold and shape change; p&gt;0.8 for partial correlation of <inline-formula><mml:math id="inf163"><mml:mi>u</mml:mi></mml:math></inline-formula> and temporal hold and shape change). On the other hand, <inline-formula><mml:math id="inf164"><mml:mi>Ï</mml:mi></mml:math></inline-formula> had a negative correlation with temporal hold (partial <inline-formula><mml:math id="inf165"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>0.22</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.032</mml:mn></mml:mrow></mml:math></inline-formula>, Spearman, controlling for gain change and shape change; p&gt;0.16 for partial correlation with gain change; p&gt;0.7 for partial correlation with shape change). These findings are in line with our hypothesis that the gain change nonlinearity captures the nonlinear adaptation of the neural responses to the short-term history of the stimulus.â</p><disp-quote content-type="editor-comment"><p>â Temporal hold: The identification of the temporal hold property was seen as one of the most exciting results from the manuscript. Hence, the reviewers request that it be analyzed more thoroughly. The criterion for the lag duration was just given as âsignificantly correlatedâ, without specification of a test, p-threshold, or a correction for multiple testing. In particular, it is not clear what happens if it drops briefly below the threshold but is significantly correlated afterward (e.g. oscillations?). Given the novelty of this result, it would be important to devote more detailed analysis to these questions (e.g., using permutation tests a la Koenig and Melie-Garcia, Brain Topography (2010), which is fast to implement and includes the corresponding options.)</p></disp-quote><p>We agree with the reviewers that quantifying this nonlinearity requires more rigorous analysis. To address the concerns about the methodology for calculating temporal hold, we propose an updated method that measures the same phenomenon on a global scale rather than measuring separately for each time point and then averaging. We believe this approach is more reliable and is less prone to unexpected behavior such as local noise or oscillatory behavior as mentioned by the reviewer.</p><p>In our updated method, we compare the similarity (correlation) distribution of all DSTRF pairs separated by <inline-formula><mml:math id="inf167"><mml:mrow><mml:mtext mathvariant="normal">n </mml:mtext><mml:mspace width="0.333em"/></mml:mrow></mml:math></inline-formula> samples when stimulus-aligned (shifting one by <inline-formula><mml:math id="inf168"><mml:mi>n</mml:mi></mml:math></inline-formula> samples), versus when not shifted. This gives us a 1-dimensional vector of distribution difference with increasing pair distance, for which we can use a threshold of significance to select a value for temporal hold.</p><p>âCalculation of the temporal hold parameter for a given recording site involves three steps. For a given DSTRF at time <italic>t</italic>, <inline-formula><mml:math id="inf169"><mml:mrow><mml:mtext mathvariant="normal">DSTR</mml:mtext><mml:msub><mml:mi>F</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, we first calculate its correlation with <inline-formula><mml:math id="inf170"><mml:mrow><mml:mtext mathvariant="normal">DSTR</mml:mtext><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and its shift-corrected version, <inline-formula><mml:math id="inf171"><mml:mrow><mml:mtext mathvariant="normal">DSTR</mml:mtext><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>â</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ26"><label>(19)</label><mml:math id="m26"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.222em"/><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mtext mathvariant="normal">DSTR</mml:mtext><mml:msub><mml:mi>F</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mspace width="0.222em"/><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mspace width="0.222em"/><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:mspace width="0.222em"/><mml:mspace width="0.222em"/><mml:mspace width="0.222em"/><mml:mspace width="0.222em"/><mml:mspace width="0.222em"/><mml:mn>1</mml:mn><mml:mo>â¤</mml:mo><mml:mspace width="0.222em"/><mml:mi>n</mml:mi><mml:mspace width="0.222em"/><mml:mo>â¤</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></disp-formula><disp-formula id="equ27"><label>(20)</label><mml:math id="m27"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.222em"/><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mtext mathvariant="normal">DSTR</mml:mtext><mml:msub><mml:mi>F</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mspace width="0.222em"/><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>â</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mspace width="0.222em"/><mml:mo>,</mml:mo><mml:mspace width="0.222em"/><mml:mspace width="0.222em"/><mml:mn>1</mml:mn><mml:mo>â¤</mml:mo><mml:mspace width="0.222em"/><mml:mi>n</mml:mi><mml:mspace width="0.222em"/><mml:mo>â¤</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></disp-formula></p><p>The upper limit 30 corresponds to 300 ms and was empirically found to be sufficient for the range of temporal hold seen in our data. Next, for each <inline-formula><mml:math id="inf172"><mml:mi>n</mml:mi></mml:math></inline-formula>, we perform a one-tailed Wilcoxon signed-rank test to determine if there is a significant positive change between <inline-formula><mml:math id="inf173"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf174"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> pairs across the entire test set (<inline-formula><mml:math id="inf175"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>â¦</mml:mi><mml:mi>T</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Finally, the temporal hold is defined as the largest <inline-formula><mml:math id="inf176"><mml:mi>n</mml:mi></mml:math></inline-formula> for which the test yields a significant result (p&lt;0.05). Intuitively, these steps find the largest duration in time for which a spectrotemporal pattern persists, assuming that the latency of that pattern shifts over time to keep it aligned with a specific feature in the stimulus (See Figure 3âfigure supplement 1 for examples).â</p><p>Subsequently, since our previous definition of shape change was based on the temporal hold calculation method, we have simplified that as well. In our updated method for computing shape change, instead of selecting specific time samples for this calculation, we align all DSTRF samples to their global average, repeating multiple iterations until convergence, and then calculate the sum of normalized singular values as we did before.</p><p>âTo calculate this parameter, we first removed the effect of temporal hold nonlinearity by time-aligning the DSTRF instances to the average DSTRF across the entire stimuli. This operation is done by finding the best shift, <inline-formula><mml:math id="inf177"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf178"><mml:mtext mathvariant="normal">LLRFs</mml:mtext></mml:math></inline-formula> at each time instant, <inline-formula><mml:math id="inf179"><mml:mrow><mml:mtext mathvariant="normal">DSTR</mml:mtext><mml:msub><mml:mi>F</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The values of <inline-formula><mml:math id="inf180"><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> are found iteratively by maximizing the correlation between <inline-formula><mml:math id="inf181"><mml:mrow><mml:mtext mathvariant="normal">DSTR</mml:mtext><mml:msub><mml:mi>F</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and the average DSTRF over the entire stimulus duration: <inline-formula><mml:math id="inf182"><mml:mrow><mml:mover><mml:mtext mathvariant="normal">DSTRF</mml:mtext><mml:mo accent="true">Â¯</mml:mo></mml:mover><mml:mrow><mml:mo form="prefix" stretchy="true">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.222em"/><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:msubsup><mml:mo>â</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mtext mathvariant="normal">DSTR</mml:mtext><mml:msub><mml:mi>F</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. At the end of each iteration, the average DSTRF is updated using the new shifted DSTRFs, and this operation is repeated until <inline-formula><mml:math id="inf183"><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> values converge, or a maximum number of iterations is reached. In our data, <inline-formula><mml:math id="inf184"><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> converged within fifty iterations. After removing the temporal hold effect, we repeated the same procedure used for the calculation of network complexity (equation 16) but instead, used the time-aligned DSTRFs to perform the singular-value decomposition. The aligned DSTRFs with same spectrotemporal features but different gain values are captured by the same eigenvectors. The sum of the sorted normalized singular values indicates the diversity of the linear functions learned by the network due to a change in their spectrotemporal feature tuning.â</p><disp-quote content-type="editor-comment"><p>â Method limitation: If one wants to measure the degree of nonlinearity in a given brain area, isn't it sufficient to simply look at how much of the explainable variance is or is not captured by a linear model? What else is learned by looking at the performance of a particular nonlinear model in this context? In fact, it would seem that using the linear model alone would be the only way to make a general statement in this context. Just because one particular nonlinear model fits one area better than another (or improves predictions in one area more than another) doesn't necessarily show that one or the other area is, in general, more nonlinear. It only suggests the nonlinearity in one area is more readily fit by that particular network given limited training samples. Please comment.</p></disp-quote><p>We agree with the reviewers that the unexplained noise-corrected variance by the linear model is in fact indicative of the general nonlinearity of a neural population. Nonlinearity, however, is a very generic description of a transformation and does not really inform us about the types of nonlinear computations that are used. The advantage of our method is that not only it shows improved prediction accuracy over the linear model particularly in higher auditory areas, but more importantly, it unravels three distinct types of nonlinear computation that are present in the neural responses. These computations include gain change, temporal hold nonlinearity, and shape change nonlinearity. How these three different computations contribute to the representation of speech that enables recognition is beyond this paper, but that is a direction that we will pursue in our future work. To clarify this point, we have added the following text to the Discussion:</p><p>âWe demonstrated how this method can be applied to auditory cortical responses in humans to discover novel types of nonlinear transformation of speech signals, therefore extending our knowledge of the nonlinear cortical computations beyond what can be explained with previous models. While the unexplained noise-corrected variance by a linear model indicates the overall nonlinearity of a neural code, this quantity alone is generic and does not inform the types of nonlinear computations that are being used. In contrast, our proposed method unravels various types of nonlinear computation that are present in the neural responses and provides a qualitative and quantitative account of the underlying nonlinearity.â</p><disp-quote content-type="editor-comment"><p>â Statistical testing: The statistical testing performed needs to be detailed more to include effect sizes.</p></disp-quote><p>We have added the effect values to the text, and also expanded the statistical parts with detail.</p></body></sub-article></article>