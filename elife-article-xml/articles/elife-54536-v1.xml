<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">54536</article-id><article-id pub-id-type="doi">10.7554/eLife.54536</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Vestigial auriculomotor activity indicates the direction of auditory attention in humans</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-143322"><name><surname>Strauss</surname><given-names>Daniel J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8481-499X</contrib-id><email>daniel.strauss@uni-saarland.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152908"><name><surname>Corona-Strauss</surname><given-names>Farah I</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152909"><name><surname>Schroeer</surname><given-names>Andreas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7904-3622</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152910"><name><surname>Flotho</surname><given-names>Philipp</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152911"><name><surname>Hannemann</surname><given-names>Ronny</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152912"><name><surname>Hackley</surname><given-names>Steven A</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Systems Neuroscience and Neurotechnology Unit, Faculty of Medicine, Saarland University &amp; School of Engineering, htw saar</institution><addr-line><named-content content-type="city">Homburg/Saar</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>Audiological Research Unit, Sivantos GmbH</institution><addr-line><named-content content-type="city">Erlangen</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution>Clinical and Cognitive Neuroscience Laboratory, Department of Psychological Sciences, University of Missouri</institution><addr-line><named-content content-type="city">Columbia</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Groh</surname><given-names>Jennifer M</given-names></name><role>Reviewing Editor</role><aff><institution>Duke University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>03</day><month>07</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e54536</elocation-id><history><date date-type="received" iso-8601-date="2020-01-24"><day>24</day><month>01</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-05-28"><day>28</day><month>05</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Strauss et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Strauss et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-54536-v1.pdf"/><abstract><p>Unlike dogs and cats, people do not point their ears as they focus attention on novel, salient, or task-relevant stimuli. Our species may nevertheless have retained a vestigial pinna-orienting system that has persisted as a 'neural fossil’ within in the brain for about 25 million years. Consistent with this hypothesis, we demonstrate that the direction of auditory attention is reflected in sustained electrical activity of muscles within the vestigial auriculomotor system. Surface electromyograms (EMGs) were taken from muscles that either move the pinna or alter its shape. To assess reflexive, stimulus-driven attention we presented novel sounds from speakers at four different lateral locations while the participants silently read a boring text in front of them. To test voluntary, goal-directed attention we instructed participants to listen to a short story coming from one of these speakers, while ignoring a competing story from the corresponding speaker on the opposite side. In both experiments, EMG recordings showed larger activity at the ear on the side of the attended stimulus, but with slightly different patterns. Upward movement (perking) differed according to the lateral focus of attention only during voluntary orienting; rearward folding of the pinna’s upper-lateral edge exhibited such differences only during reflexive orienting. The existence of a pinna-orienting system in humans, one that is experimentally accessible, offers opportunities for basic as well as applied science.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Dogs, cats, monkeys and other animals perk their ears in the direction of sounds they are interested in. Humans and their closest ape relatives, however, appear to have lost this ability. Some humans are able to wiggle their ears, suggesting that some of the brain circuits and muscles that allow automatic ear movements towards sounds are still present. This may be a ‘vestigial feature’, an ability that is maintained even though it no longer serves its original purpose.</p><p>Now, Strauss et al. show that vestigial movements of muscles around the ear indicate the direction of sounds a person is paying attention to. In the experiments, human volunteers tried to read a boring text while surprising sounds like a traffic jam, a baby crying, or footsteps played. During this exercise, Strauss et al. recorded the electrical activity in the muscles of their ears to see if they moved in response to the direction the sound came from. In a second set of experiments, the same electrical recordings were made as participants listened to a podcast while a second podcast was playing from a different direction. The individuals’ ears were also recorded using high resolution video.</p><p>Both sets of experiments revealed tiny involuntary movements in muscles surrounding the ear closest to the direction of a sound the person is listening to. When the participants tried to listen to one podcast and tune out another, they also made ear ‘perking’ movements in the direction of their preferred podcast.</p><p>The results suggest that movements of the vestigial muscles in the human ear indicate the direction of sounds a person is paying attention to. These tiny movements could be used to develop better hearing aids that sense the electrical activity in the ear muscles and amplify sounds the person is trying to focus on, while minimizing other sounds.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>auditory attention</kwd><kwd>pinna-orienting</kwd><kwd>electromyogram</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id><institution>Bundesministerium für Bildung und Forschung</institution></institution-wrap></funding-source><award-id>BMBF-FZ No. 03FH004IX5</award-id><principal-award-recipient><name><surname>Strauss</surname><given-names>Daniel J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Recordings of ear muscles in humans show that ears attempt to pivot in the direction that requires attention.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Watching the ears allows an equestrian to gauge their mount’s shifting attention. Ear movements are not a useful cue in humans or apes because higher primates have lost the ability to orient by adjusting pinna shape and focal direction. Instead we judge a person’s attention by their gaze direction. In thousands of research reports each year, though, casual observation of ocular orienting is replaced by sophisticated recording techniques. We show in the present paper that similar electrical and optical techniques allow us to extract muscular correlates of pinna-orienting in our species and even render subtle pinna-orienting movements visible. Activation of the ear muscles is directionally specific and it occurs during voluntary as well as reflexive attention.</p><p>A review of research in <xref ref-type="bibr" rid="bib15">Hackley, 2015</xref> on pinna-orienting in humans identified three relevant findings scattered across the preceding 100-or-so years. The first was Wilson’s oculo-auricular phenomenon (<xref ref-type="bibr" rid="bib39">Wilson, 1908</xref>), in which shifting the gaze hard to one side elicits a 1 to 4 mm deflection of the lateral rim of both ears. The relevance to spatial attention is uncertain, though, with diverging results across studies, for example see <xref ref-type="bibr" rid="bib11">Gerstle and Wilkinson, 1929</xref>; <xref ref-type="bibr" rid="bib35">Urban et al., 1993</xref>; <xref ref-type="bibr" rid="bib24">O'Beirne and Patuzzi, 1999</xref>. Additional evidence comes from a 1987 study (<xref ref-type="bibr" rid="bib14">Hackley et al., 1987</xref>) of the bilateral postauricular muscle (PAM) reflex (onset latency = 10 ms) to acoustic onset transients. Increased amplitudes were observed when subjects directed their attention to a stream of tones on the same side as the recorded muscle while ignoring a competing, contralateral stream. Comparisons across left/right stimulus, attention, and PAM combinations localized modulation to the motor limb of the reflex arc. This pattern could indicate that the muscle behind an ear is primed when attention is directed toward that side. Finally, an experiment in <xref ref-type="bibr" rid="bib32">Stekelenburg and van Boxtel, 2002</xref> found that the automatic capture of attention by unexpected sounds coming from a speaker hidden to the left of the participant elicited greater activity in the left than right PAM.</p><p>Apart from the research just described, functional studies of the human auriculomotor system have been mainly limited to the PAM reflex, in the context of audiometry or affective psychophysiology. The auriculomotor system lies essentially untouched in the literature. Here we present evidence that our brains retain vestigial circuitry for orienting the pinnae during both exogenous, stimulus-driven attention to brief, novel sounds and endogenous, goal-directed attention to sustained speech. We also demonstrate a complex interplay of different auricular muscles which may be causally linked to subtle movements of the pinnae.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Experiment 1 - Exogenous attention</title><p>To examine automatic, stimulus-driven attention we used novel sounds similar to those in the <xref ref-type="bibr" rid="bib32">Stekelenburg and van Boxtel, 2002</xref> study, for example traffic jam, baby crying, footsteps. However, we presented them randomly from four different speakers (at ± 30°, ± 120°; <xref ref-type="fig" rid="fig1">Figure 1</xref>) rather than just one, while the subject read a boring essay. As we were interested in the interactive role of distinct muscles in attempting to shape and point the pinnae, we recorded EMG from posterior, anterior, superior, and transverse auricular muscles (PAM, AAM, SAM, and TAM). Visual evidence had previously been limited to still photos of Wilson’s oculo-auricular phenomenon (<xref ref-type="bibr" rid="bib39">Wilson, 1908</xref>), so we supplemented our EMG data with videos from four high-definition cameras, see Methods and <xref ref-type="video" rid="video1">Video 1</xref>, <xref ref-type="video" rid="video2">Video 2</xref>, and <xref ref-type="video" rid="video3">Video 3</xref>. To confirm that our findings would generalize to different age groups, older (62.7 ± 5.9 y) as well as younger (24.1 ± 3.1 y) adults were tested.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental setup.</title><p>(<bold>A</bold>) Four loudspeakers presented novel sounds (Exp. 1) or stories (Exp. 2) at 30° to the left or right of fixation or behind the interaural axis. Instructions, text, or fixation cross was displayed on a 55 in flat screen. (<bold>B</bold>) Surface EMGs were recorded bilaterally from four auricular muscles as well as from left zygomaticus major, frontalis, and sternocleidomastoideus, using a bandpass of 10 – 1000 Hz and a sampling rate of 9600 Hz. Separation of paired auricular electrodes was 1 cm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig1-v1.tif"/></fig><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-54536-video1.mp4"><label>Video 1.</label><caption><title>Experiment 1 –Ear movement example from a trial with a novel sound at the right posterior speaker in Experiment 1.</title><p>The right half of the display portrays evoked movements of the ipsilateral pinna in three ways. The large video clip of the pinna uses digital magnification to render the overall pattern of movement apparent. The color overlay in these videos indicates the motion magnitude. Just below the video and to the right, an unrectified EMG recording of the postauricular muscle is shown in co-registration with the video. The global head motion was reduced by a 2-dimensional rigid pre-registration with respect to a set of manually specified reference points on the head (see also <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The 3-dimensional graph medial to the 2-D graph includes a vector that indicates moment-by-moment changes in EMG activity of the superior auricular muscle (SAM, the vertical axis), transverse auricular muscle (TAM, a horizontal axis), and the difference between activity in the posterior and anterior auricular muscles (PAM-AAM, the other horizontal axis). The left half of this video gives corresponding information for the contralateral ear which, consistent with evidence presented in the main text, was not as active as the ipsilateral one.</p></caption></media><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-54536-video2.mp4"><label>Video 2.</label><caption><title>Experiment 1 –The right ear example from the previous video, but with four different videos in sequence.</title><p>The first video of the sequence shows the raw recording (without digital magnification). The second video shows the digitally magnified motion, the third video shows the magnified motion with color overlay as in the previous video supplement, and the fourth video shows the three dimensional motion from a different angle. This video sequence shows the impact of the digital motion magnification and the depth information about ear motion that can be derived from a stereo computer vision setup such as the one used here.</p></caption></media><media id="video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-54536-video3.mp4"><label>Video 3.</label><caption><title>Experiment 2 –Ear movement example from a participant who exhibited exceptionally large, long-lasting involuntary auricular muscle activations and ear motion during the endogenous attention task in Experiment 2.</title><p>The attention of the participant was directed to the story played from the posterior right speaker. The organization of the plots and co-registration is as in <xref ref-type="video" rid="video1">Video 1</xref>. However, this time the raw videos without digital magnification are shown. The raw videos are played faster, time-locked to the time axis given in minutes in the one dimensional plots of the rectified postauricular muscle activity. Note that time-axis reflects the entire timeline including the instructions and the introduction to the stories before the directional listening task. The listening task started at approximately 2 min. The video also documents the end of the listening task (around 7 min) accompanied with a time–locked offset of the muscle activation and pinna displacement. A causal relation of the rectified postauricular muscle activity and the motion magnitude in the videos is clearly noticeable, especially for the ipsilateral ear.</p></caption></media><p>The signal-averaged EMG waveforms of <xref ref-type="fig" rid="fig2">Figure 2</xref> show well-defined responses with an onset latency of about 70 ms, responses that vary in amplitude, duration, and morphology according to the relative direction of the sound source. The inter- and intra-subject variability for the analyzed auricular muscles is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref>, respectively. These plots portray the consistency of the PAM, AAM, and TAM responses across stimuli and subjects, especially for stimulation from the back. For the statistical analysis, mean amplitudes were subjected to a mixed, repeated-measures analysis of variance, with factors of age group, stimulus-muscle correspondence (ipsi-/contralateral), and anterior/posterior stimulus direction. EMG amplitudes were larger for stimulus sources on the same side as the recorded ear for PAM, AAM, and TAM [<inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>26</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> = 47.44, 17.01, and 47.53, respectively; <inline-formula><mml:math id="inf2"><mml:mi>p</mml:mi></mml:math></inline-formula>-values &lt; 0.001; <inline-formula><mml:math id="inf3"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.65, 0.40, and 0.65] but not SAM.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Experiment 1.</title><p>Grand average (N = 28) of the baseline corrected and normalized event-related electromyograms at the four auricular muscles for the recordings ipsilateral (left panel) and contralateral (right panel) to stimulation; top: front speakers (30°), bottom: back speakers (120°). The contralateral-ipsilateral organization of our data set is justified by a preliminary analysis that obtained null effects for left–versus–right using a more complete factorial structure (left/right stimulus direction × left/right recording site). The following figure supplement is available for <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. Analysis of video recordings from one participant who exhibited submillimeter pinna displacements in response to stimulation from the back speakers. This figure supplement is complemented by <xref ref-type="video" rid="video1">Video 1</xref> and <xref ref-type="video" rid="video2">Video 2</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Analysis of video recordings from one participant who exhibited submillimeter pinna displacements in response to stimulation in Experiment 1 (Exogenous Attention).</title><p>Given the time <inline-formula><mml:math id="inf4"><mml:msup><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:math></inline-formula> of a stimulus, we analyzed 2s video segments with pre-stimulus onset <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>0.25</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The frames at <inline-formula><mml:math id="inf6"><mml:msubsup><mml:mi>t</mml:mi><mml:mn>0</mml:mn><mml:mn>0</mml:mn></mml:msubsup></mml:math></inline-formula> of the first stimulus (direction +120°) of the left and right recordings were manually annotated with four regions of interest (ROI) (left images). The centroid <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> of the ROI located on the ear was used to track the pinna motion and the three centroids <inline-formula><mml:math id="inf8"><mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:msubsup></mml:math></inline-formula> of the ROIs around the ears were used to define a basis <inline-formula><mml:math id="inf9"><mml:msup><mml:mi>B</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> of a coordinate system where the coordinates of points on the ear are invariant under head motion. Each point inside the ROIs sampled as in the first frame was projected to the respective right camera frames and forward-warped with respect to the motion to the reference frame for all cameras. The motion magnitude is given as the norm of <inline-formula><mml:math id="inf10"><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> at time <inline-formula><mml:math id="inf11"><mml:msup><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:math></inline-formula> with respect to the basis <inline-formula><mml:math id="inf12"><mml:msup><mml:mi>B</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> referenced to <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> at <inline-formula><mml:math id="inf14"><mml:msubsup><mml:mi>t</mml:mi><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula>. On the right are the time courses of the mean motion magnitude and standard deviation over the stimuli (± 120°) for the left and the right ear as well as the time course of the first stimulus from the right back speaker. This is the same trial that is portrayed in <xref ref-type="video" rid="video1">Video 1</xref> and <xref ref-type="video" rid="video2">Video 2</xref>. Due to a large head rotation during the recording, the first − 120° stimulus was excluded from the evaluation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig2-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Experiment 1 – Responses of the PAM to stimuli from the back speakers, showing intersubject variability: Top panels: Every row corresponds to the averaged response of one participant.</title><p>Amplitude is encoded in color. The top rows (1-16) represent younger adult participants; the bottom rows (17-28), older adults. Bottom panels: Mean and standard deviation based on the above plots. The following figure supplements are available for <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. The described intersubject variability analysis for the AAM, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>. SAM, and <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>. TAM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Experiment 1 – Responses of the AAM to stimuli from the back speakers, showing intersubject variability: Top panels: Every row corresponds to the averaged response of one participant.</title><p>Amplitude is encoded in color. The top rows (1-16) represent younger adult participants; the bottom rows (17-28), older adults. Bottom panels: Mean and standard deviation based on the above plots.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Experiment 1 – Responses of the SAM to stimuli from the back speakers, showing intersubject variability: Top panels: Every row corresponds to the averaged response of one participant.</title><p>Amplitude is encoded in color. The top rows (1-16) represent younger adult participants; the bottom rows (17-28), older adults. Bottom panels: Mean and standard deviation based on the above plots.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Experiment 1 – Responses of the TAM to stimuli from the back speakers, showing intersubject variability: Top panels: Every row corresponds to the averaged response of one participant.</title><p>Amplitude is encoded in color. The top rows (1-16) represent younger adult participants; the bottom rows (17-28), older adults. Bottom panels: Mean and standard deviation based on the above plots.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig3-figsupp3-v1.tif"/></fig></fig-group><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Experiment 1 – Intrasubject variability of the PAM: Mean and standard deviations of the phasic responses 50 - 300 ms) of every participant.</title><p>Top panels: responses to the front speakers. Bottom panels: responses to the back speakers. Left panels: Responses of the younger adults. Right panels: responses of the older adults. Blue represents ipsilateral responses, red represents contralateral responses. The following figure supplements are available for <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. The described intrasubject variability analysis for the AAM, <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>. SAM, and <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>. TAM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Experiment 1 – Intrasubject variability of the AAM: Mean and standard deviations of the phasic responses ( 50 - 300 ms) of every participant.</title><p>Top panels: responses to the front speakers. Bottom panels: responses to the back speakers. Left panels: Responses of the younger adults. Right panels: responses of the older adults. Blue represents ipsilateral responses, that is EMG recorded from the muscle on the same side as the stimulus, whereas red represents contralateral responses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Experiment 1 – Intrasubject variability of the SAM: Mean and standard deviations of the phasic responses (50 -300 ms) of every participant.</title><p>Top panels: responses to the front speakers. Bottom panels: responses to the back speakers. Left panels: Responses of the younger adults. Right panels: responses of the older adults. Blue represents ipsilateral responses, that is EMG recorded from the muscle on the same side as the stimulus, whereas red represents contralateral responses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Experiment 1 – Intrasubject variability of the TAM: Mean and standard deviations of the phasic responses ( 50 - 300 ms) of every participant.</title><p>Top panels: responses to the front speakers. Bottom panels: responses to the back speakers. Left panels: Responses of the younger adults. Right panels: responses of the older adults. Blue represents ipsilateral responses, that is EMG recorded from the muscle on the same side as the stimulus, whereas red represents contralateral responses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig4-figsupp3-v1.tif"/></fig></fig-group><p>Responses were also larger to sounds emanating from the back than the front speakers for PAM, AAM, and TAM [<inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>26</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> = 32.1, 12.0, and 19.9, respectively; <inline-formula><mml:math id="inf16"><mml:mi>p</mml:mi></mml:math></inline-formula>-values &lt; 0.003, <inline-formula><mml:math id="inf17"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.55, 0.32, and 0.43]. Posterior, ipsilateral stimulation elicited the most vigorous responses from these three muscles. In particular, the interaction between the factors ipsi/contralateral and anterior/posterior for PAM, AAM, and TAM yields <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>26</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> = 40.4, 14.9, and 23.6, respectively; <inline-formula><mml:math id="inf19"><mml:mi>p</mml:mi></mml:math></inline-formula>-values &lt; 0.002; <inline-formula><mml:math id="inf20"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.61, 0.36, and 0.48. There were no interactions involving age group, but a main effect indicated that older participants had smaller AAM responses [<inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>26</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> = 6.0, <inline-formula><mml:math id="inf22"><mml:mi>p</mml:mi></mml:math></inline-formula> &lt; 0.03, <inline-formula><mml:math id="inf23"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.19].</p><p>These results support the hypothesis that the human brain retains circuits that attempt to point the ears in the direction of unexpected, potentially relevant sounds. The corresponding vestigial auriculomotor drive appears to be causally linked to very small ear displacements, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="video" rid="video1">Video 1</xref>, and <xref ref-type="video" rid="video2">Video 2</xref>. Having documented the existence of directionally-appropriate responses of the ear muscles to brief novel sounds, we turn now to a qualitatively distinct type of attention.</p></sec><sec id="s2-2"><title>Experiment 2 – Endogenous Attention</title><p>To examine voluntary, goal-directed attention we used the classic, dichotic-listening paradigm, see <xref ref-type="bibr" rid="bib19">Hillyard et al., 1973</xref>; <xref ref-type="bibr" rid="bib4">Cherry, 1953</xref>. Two competing short stories were played either over the two front speakers or the two back speakers. To increase motivation participants were allowed to choose, after a brief introduction, which of the two stories (podcasts) they would like to listen to. They were then told which speaker that story would be presented from. Our subjects were instructed to listen carefully while looking at a fixation cross and, as in the immediately preceding study, holding their head still on a chin rest. Upon completion, a new story was picked and the listening direction was switched to one of the other speakers. Recording methods were identical to those of the exogenous experiment. Muscle activity was quantified as the mean of the absolute EMG energy over the entire course of each 5 min listening trial in <xref ref-type="fig" rid="fig5">Figure 5</xref> and for consecutive segments of 10 s duration in a temporal analysis shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Experiment 2.</title><p>Grand average of the PAM, AAM, and SAM activity when stories were played from the front (top) and back speakers (bottom). Shown is the normalized (total) energy of the left/right recording channels during attention to the left or right story (bars represent the standard error). The following figure supplements are available for <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>. Subband analysis of the described ipsi- vs. contralateral effect for PAM, AAM, and SAM; <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>. Reported results for a selected narrow frequency band.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Grand average of the PAM, AAM, and SAM activity when stories were played from front (± 30°) and back speakers (± 120°).</title><p>The normalized total energy in each octave-frequency subband is plotted for the left/right recording channels (in the conventional dyadic order) during attention to the left or right story, along with their difference (black line). It is notable that the lowest and highest frequency bands contribute little to the difference. In fact, the middle bands 5 and 6 show the largest attention effect (see also <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). Technical decoding applications might make use of this observation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Analogous to <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig6">Figure 6</xref>, respectively, in the main text but for frequency band 5 (37.5 – 75 Hz): Grand average of the PAM, AAM, and SAM activity when stories were played from the back speakers (± 120°).</title><p>Top: normalized total energy of the left/right recording channels when attending to the left or right story (bars represent the standard error). A significant interaction of recording channel and attention direction was observed [PAM, AAM, TAM, SAM: F(1, 19) = 17.2, 8.2, 0.5, and 24.0, respectively; <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> 0.001, 0.01, 0.51, and 0.001; <inline-formula><mml:math id="inf25"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.48, 0.30, 0.02, and 0.56]; Bottom: time–resolved activity after pooling the ipsi– and contralateral signals with a segment–wise normalization. Each sampling point represents the energy induced in consecutive 10 s segments.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig5-figsupp2-v1.tif"/></fig></fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Experiment 2.</title><p>Time–resolved activity (each sampling point represents the energy induced in consecutive 10 s segments) after pooling the ipsi– and contralateral signals with a segment–wise normalization for the front (top) and back speakers (bottom).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig6-v1.tif"/></fig><p>As in the exogenous study, EMG energy at PAM and AAM was largest on the side to which attention was focused [analysis corresponding to <xref ref-type="fig" rid="fig5">Figure 5</xref>: F(1, 19) = 15.2 and 4.6, respectively; <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> 0.001 and 0.04; <inline-formula><mml:math id="inf27"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.44 and 0.20]; an effect that is particularly strong in narrow–band middle frequency components of the signal, see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref> and <xref ref-type="fig" rid="fig5s2">2</xref> and the tables in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p><p>A different pattern emerged for the other two muscles. Whereas TAM but not SAM activity had reflected lateralization of transient, exogenous attention, the reverse was true for sustained, goal-directed attention. That is to say, mean EMG energy at SAM was larger at the ipsi– than contralateral ear [F(1, 19) = 16.3; <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf29"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.46] in Experiment 2, but there was no such difference for TAM.</p><p>Another main effect indicated that activation of all four muscles was generally enhanced when participants listened to one of the two speakers that were slightly behind as opposed to in front of them [PAM, AAM, TAM, SAM: F(1, 19) = 5.7, 3.1, 8.1, and 12.0, respectively; <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> 0.03, 0.09, 0.01, and 0.003; <inline-formula><mml:math id="inf31"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.23, 0.14, 0.30, and 0.39]. These effects did not interact with each other or with age. Although PAM activity declines over time, EMG energy of all three muscles is clearly sustained across the 5-min sessions, see <xref ref-type="fig" rid="fig6">Figure 6</xref>. A corresponding sustained deflection of the pinna is also noticeable in the co-registered <xref ref-type="video" rid="video3">Video 3</xref>.</p></sec><sec id="s2-3"><title>Potential motor confounds</title><p>An alternative to the account we have been developing is that participants in Experiments 1 and 2 may have shifted their gaze toward the attended source. This would have then triggered <xref ref-type="bibr" rid="bib39">Wilson, 1908</xref> phenomenon, that is auriculomotor activity secondary to large gaze shifts. To test this hypothesis, we segmented the horizontal electrooculogram (EOG) in the same way as the auricular EMG. Voltages were converted to degrees of arc separately for each participant, based on findings from a cursor tracking protocol (± 35°). <xref ref-type="fig" rid="fig7">Figure 7</xref> and <xref ref-type="fig" rid="fig8">Figure 8</xref> document a complete absence of eye movements that were systematically related to attention direction. A limitation of these findings is that electro-oculographic recordings have a resolution of only 1 − 2°. However, gaze shifts less than 30° are rarely accompanied by auriculomotor activity, see <xref ref-type="bibr" rid="bib35">Urban et al., 1993</xref>. Representative examples of macrosaccades during reading in Experiment one with co-registered auricular muscle activity can be found in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>. There is no obvious linkage of saccades and PAM responses. Note that the mean visual angle range observed in this example generalizes across subjects, see <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>. Also when considering all the macrosaccades from all the subjects in Experiment 1, our data do not exhibit a regularity between auditory stimuli and macrosaccades, see <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Experiment 1: Averaged horizontal EOG activity at around the time of stimulation from the back speakers, showing an apparent absence of systematic shifts in gaze direction: Top panels: Every row corresponds to the averaged response of one participant.</title><p>Gaze angle is encoded in color, such that positive values (yellow) indicate rightward eye movements/positive angles. The top rows (1-16) represent younger adult participants; the bottom rows (17-28), older adults. Bottom panels: Mean and standard deviation based on the above plots.The following figure supplements are available for <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>. Macrosaccades during reading for one subject as example; <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>. Boxplots of the EOG for all subjects; <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>. Density of all detected macrosaccades during Experiment 1; <xref ref-type="fig" rid="fig7s4">Figure 7—figure supplement 4</xref>. Responses of the M. sternocleidomastoideus to stimuli from the front speakers; <xref ref-type="fig" rid="fig7s5">Figure 7—figure supplement 5</xref>. Responses of the M. sternocleidomastoideus to stimuli from the back speakers; <xref ref-type="fig" rid="fig7s6">Figure 7—figure supplement 6</xref>. Responses of the M. frontalis to stimuli from the front speakers; <xref ref-type="fig" rid="fig7s7">Figure 7—figure supplement 7</xref>. Responses of the M. frontalis to stimuli from the back speakers; <xref ref-type="fig" rid="fig7s8">Figure 7—figure supplement 8</xref>. Responses of the M. zygomaticus to stimuli from the front speakers; <xref ref-type="fig" rid="fig7s9">Figure 7—figure supplement 9</xref>. Responses of the M. zygomaticus to stimuli from the back speakers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Macrosaccades during reading in Experiment one as recorded by means of horizontal EOG (black line) along with time-synchronized EMG from left and right PAM (blue and orange lines, respectively).</title><p>This participant (# 15) had large, clear PAM responses. Different trials are shown in the four panels. Note that muscle activations occur without saccades and large saccades occur without muscle responses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Boxplots of EOG signals from all subjects in Experiment 1.</title><p>Every stimulus presentation, including 3 s long pre– and poststimulus intervals, were included. Blue dots mark the median, outliers (data points beyond 1.5 x interquartile range) are displayed in red. Gaze shifts between the edges of the text that subjects were reading from during this experiment generated the largest EOG values. These shifts can be estimated in the upper and lower whiskers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig7-figsupp2-v1.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Density of detected macrosaccades during Experiment 1.</title><p>Top plot: Every line corresponds to one stimulus presentation (trials). Detected macrosaccades are marked by black dots. Across all subjects, 1008 stimuli were presented. Bottom plot: Histogram of detected macrosaccades with respect to time. Macrosaccades were detected by calculating the forward differences <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mo mathvariant="normal">⋅</mml:mo><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow><mml:mo mathvariant="normal">=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mo mathvariant="normal">⋅</mml:mo><mml:mo mathvariant="normal">+</mml:mo><mml:mi>n</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow><mml:mo mathvariant="normal">-</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mo mathvariant="normal">⋅</mml:mo><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>n</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">150</mml:mn></mml:mrow></mml:math></inline-formula> samples (62.5 ms) for every point and thresholding <inline-formula><mml:math id="inf34"><mml:mrow><mml:mrow><mml:mo mathvariant="normal" stretchy="false">|</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mo mathvariant="normal">⋅</mml:mo><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathvariant="normal" stretchy="false">|</mml:mo></mml:mrow><mml:mo mathvariant="normal">&gt;</mml:mo><mml:mi>α</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:msup><mml:mn mathvariant="normal">10</mml:mn><mml:mo mathvariant="normal">∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. Both <inline-formula><mml:math id="inf35"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf36"><mml:mi>α</mml:mi></mml:math></inline-formula> were determined empirically to reliably detect saccades when the subjects jump from a given line to the next one.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig7-figsupp3-v1.tif"/></fig><fig id="fig7s4" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 4.</label><caption><title>Experiment 1 – Responses of the M. sternocleidomastoideus (M.SCM) to stimuli from the front speakers, showing intersubject variability: Top panels: Every row corresponds to the averaged response of one participant.</title><p>Amplitude is encoded in color. The top rows (1-16) represent younger adult participants; the bottom rows (17-28), older adults. Bottom panels: Mean and standard deviation based on the above plots.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig7-figsupp4-v1.tif"/></fig><fig id="fig7s5" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 5.</label><caption><title>Experiment 1 – Responses of the M. sternocleidomastoideus (M.SCM) to stimuli from the back speakers, showing intersubject variability: Top panels: Every row corresponds to the averaged response of one participant.</title><p>Amplitude is encoded in color. The top rows (1-16) represent younger adult participants; the bottom rows (17-28), older adults. Bottom panels: Mean and standard deviation based on the above plots.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig7-figsupp5-v1.tif"/></fig><fig id="fig7s6" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 6.</label><caption><title>Experiment 1 – Responses of the frontalis muscle to stimuli from the front speakers, showing intersubject variability: Top panels: Every row corresponds to the averaged response of one participant.</title><p>Amplitude is encoded in color. The top rows (1-16) represent younger adult participants; the bottom rows (17-28), older adults. Bottom panels: Mean and standard deviation based on the above plots.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig7-figsupp6-v1.tif"/></fig><fig id="fig7s7" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 7.</label><caption><title>Experiment 1 – Responses of the frontalis muscle to stimuli from the back speakers, showing intersubject variability: Top panels: Every row corresponds to the averaged response of one participant.</title><p>Amplitude is encoded in color. The top rows (1-16) represent younger adult participants; the bottom rows (17-28), older adults. Bottom panels: Mean and standard deviation based on the above plots.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig7-figsupp7-v1.tif"/></fig><fig id="fig7s8" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 8.</label><caption><title>Experiment 1 – Responses of the zygomaticus muscle to stimuli from the front speakers, showing intersubject variability: Top panels: Every row corresponds to the averaged response of one participant.</title><p>Amplitude is encoded in color. The top rows (1-16) represent younger adult participants; the bottom rows (17-28), older adults. Bottom panels: Mean and standard deviation based on the above plots.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig7-figsupp8-v1.tif"/></fig><fig id="fig7s9" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 9.</label><caption><title>Experiment 1 – Responses of the zygomaticus muscle to stimuli from the back speakers, showing intersubject variability: Top panels: Every row corresponds to the averaged response of one participant.</title><p>Amplitude is encoded in color. The top rows (1-16) represent younger adult participants; the bottom rows (17-28), older adults. Bottom panels: Mean and standard deviation based on the above plots.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig7-figsupp9-v1.tif"/></fig></fig-group><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Experiment 2: Intrasubject variability of the horizontal EOG: Mean and standard deviations of the EOG during the complete trial of every participant.</title><p>Top panels: attending the front speakers. Bottom panels: attending the back speakers. Left panels: younger adults. Right panels: older adults. Blue represents the EOG when attending the left, red when attending the right speaker. A positive EOG indicates that the gaze is directed toward the side of the attended speaker. Note that the deviation of the mean from <inline-formula><mml:math id="inf37"><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> is well within one standard deviation and therefore indicates that participants did not systematically divert their gaze to the attended speaker. The following figure supplements are available for <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>. Time-resolved EOG analysis in Experiment 2; <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>. Activity of the frontalis and zygomaticus muscle in Experiment 2.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig8-v1.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Time-resolved activity (sampling points represent the energy within consecutive 10 s segments) after pooling the ipsi- and contralateral EOG signals for the front (left) and back speakers (right) in Experiment 2.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig8-figsupp1-v1.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Grand average of the zygomaticus and frontalis muscle activity when stories were played from the front (top row) and back speakers (bottom row) in Experiment 2.</title><p>Shown is the normalized (total) energy of the left/right recording channels during attention to the left or right story (bars represent the standard error). There was no significant interaction of recording channel and attention direction [zygomaticus, frontalis: F(1, 19) = 0.13 and 3.44; <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>p</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> 0.73 and 0.08; <inline-formula><mml:math id="inf39"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn mathvariant="normal">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.007 and 0.15].</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-fig8-figsupp2-v1.tif"/></fig></fig-group><p>Another line of evidence that the auricular responses observed in our study were not secondary to eye movements, concerns their pattern of lateralization. Activation of TAM during Wilson’s oculo-auricular phenomenon is more vigorous on the side opposite the direction of gaze, see <xref ref-type="bibr" rid="bib11">Gerstle and Wilkinson, 1929</xref>; <xref ref-type="bibr" rid="bib35">Urban et al., 1993</xref>. By contrast, we found in Experiment one that TAM activation was relatively enhanced at the ear on the same side as the attention-engaging sounds. The PAM component of Wilson's phenomenon does exhibit enhanced activity on the ipsilateral side, but this effect appears to be reliable only for gaze shifts greater than about 40 degrees (<xref ref-type="bibr" rid="bib25">Patuzzi and O'Beirne, 1999</xref>, <xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>Another alternative interpretation is that participants oriented not with their ears or eyes, but by lifting their chin from the chin rest and rotating their head toward the attended sound. If humans have a vestibulo-auricular response as do cats (<xref ref-type="bibr" rid="bib34">Tollin et al., 2009</xref>), such head rotations could have indirectly triggered activity in the ear muscles. However, recent research has shown that azimuthal head rotations have little effect on auricular activity in humans, see <xref ref-type="bibr" rid="bib6">Cook and Patuzzi, 2014</xref>. Moreover, analysis of sternocleidomastoid EMG in our data suggest that movements of the neck were rare, small, and unsystematic, see <xref ref-type="fig" rid="fig7s4">Figure 7—figure supplements 4</xref> and <xref ref-type="fig" rid="fig7s5">5</xref>. An additional statistical analysis in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> also rejects an influence of the sternocleidomastoid EMG and horizontal EOG. Finally, head rotations would have been too slow to generate the rapid responses of around 70 ms onset latency observed in the ear muscles in Experiment 1. We note with interest, though, the possibility that subtle, covert activation of head turning muscles (<xref ref-type="bibr" rid="bib7">Corneil et al., 2008</xref>) might be correlated with ocular and auricular orienting. Note that there was also no corresponding co–activation of the other measured (non–auricular) facial muscles, the zygomaticus and frontalis muscle, see <xref ref-type="fig" rid="fig7s6">Figure 7—figure supplements 6</xref>–<xref ref-type="fig" rid="fig7s9">9</xref> for Experiment one and <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref> for Experiment 2.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>These data provide compelling evidence that our brains retain, in vestigial form, circuitry for orienting the pinnae during both exogenous and endogenous modes of attention. The neural drive to our ear muscles is so weak that the actual movements (see co-registered video data in the Supplementary Information) are at least one to two orders of magnitude smaller compared to those generated during biting, smiling, grimacing, or voluntary ear-wiggling. To understand what remains of the vestigial pinna-orienting system so as to exploit it for practical or scientific purposes it is helpful to take a comparative, phylogenetic approach (<xref ref-type="bibr" rid="bib15">Hackley, 2015</xref>, <xref ref-type="bibr" rid="bib16">Hackley et al., 2017</xref>).</p><sec id="s3-1"><title>Vestigial Pinna-Orienting</title><p>The ability to swivel and point the pinnae seems to have been lost during the transition from the primarily nocturnal lifestyles of prosimians to the diurnal ones of New World monkeys, and then, Old World monkeys (<xref ref-type="bibr" rid="bib5">Coleman and Ross, 2004</xref>). Mobility continued to decline as the ears became shorter and more rigid, see <xref ref-type="bibr" rid="bib38">Waller et al., 2008</xref>; <xref ref-type="bibr" rid="bib5">Coleman and Ross, 2004</xref>. The musculature degenerated. For example, an inferior auricular muscle to oppose SAM still exists in lesser apes such as gibbons and siamangs (<xref ref-type="bibr" rid="bib2">Burrows et al., 2011</xref>), but not in chimpanzees (<xref ref-type="bibr" rid="bib2">Burrows et al., 2011</xref>) or humans (<xref ref-type="bibr" rid="bib3">Cattaneo and Pavesi, 2014</xref>). Given that head rotation has little effect on PAM activity (<xref ref-type="bibr" rid="bib6">Cook and Patuzzi, 2014</xref>), it seems likely that the vestibulo–auricular reflex as documented in cats (<xref ref-type="bibr" rid="bib34">Tollin et al., 2009</xref>) has not been conserved in our species. Also presumably lost is the ability to use proprioceptive information to adjust auditory processing in accordance with pinna position, orientation, and shape as documented in cats, see <xref ref-type="bibr" rid="bib20">Kanold and Young, 2001</xref>. Although the ear muscles of Old World monkeys have spindles (<xref ref-type="bibr" rid="bib23">Lovell et al., 1977</xref>), those of humans do not (<xref ref-type="bibr" rid="bib3">Cattaneo and Pavesi, 2014</xref>).</p><p>When pinna-orienting movements became too small to modify acoustic input substantially, possibly 25 million years ago when lesser apes branched off from Old World monkeys, see <xref ref-type="bibr" rid="bib12">Gibbs et al., 2007</xref> as discussed in <xref ref-type="bibr" rid="bib15">Hackley, 2015</xref>, selective environmental pressure ceased. The neural system became more-or-less 'frozen’ in a form optimized for controlling taller, more flexible ears, mounted on a smaller, more spherical head. This evolutionary perspective helps us to understand the surprising finding that AAM, which pulls the base of the pinna forward, was activated in Experiment one by novel sounds coming from the rear. Co-activation of opposing muscles AAM and PAM in our remote ancestors would have reduced occlusion of the ear canal by the tragus. Note that this occlusion occurs in monkeys when contraction of the PAM homolog is unopposed, see <xref ref-type="bibr" rid="bib38">Waller et al., 2008</xref>, supplementary video clip 17. In addition, PAM-AAM co-activation would have stabilized the base of the pinna and reduced myotendinous elasticity, thereby allowing quick changes in position or orientation. This perspective also illuminates our unexpected finding of ipsilateral SAM suppression in Experiment 1. A study of pinna orienting in cats, whose tall ears resemble those of prosimians, showed that they tend to tilt the ear downward slightly when orienting to a lateral target, see <xref ref-type="bibr" rid="bib28">Populin and Yin, 1998</xref>.</p></sec><sec id="s3-2"><title>Potential neural mechanisms</title><p>Neurobiologists have distinguished two types of pinna-orienting movements in cats, based on onset latency, see <xref ref-type="bibr" rid="bib30">Siegmund and Santibáñez, 1982</xref>. The short-latency response is specific to auditory stimuli and is chronometrically uncorrelated with saccades toward the target. By contrast, the long-latency response can be elicited by visual as well as auditory stimuli and it is roughly synchronous with ocular orienting, see <xref ref-type="bibr" rid="bib28">Populin and Yin, 1998</xref>. Using a 4-speaker set-up similar to that of the present Experiment 1, <xref ref-type="bibr" rid="bib30">Siegmund and Santibáñez, 1982</xref> found cats’ unconditioned pinna responses to have an EMG onset latency that averaged 78 ms, similar to our value of about 70 ms. The animals were then trained to make gaze shifts toward the sound sources. Onset latency of the auriculomotor responses dropped to a remarkable 29 ms and the responses were resistant to extinction over the course of 125 trials. Both findings were replicated by <xref ref-type="bibr" rid="bib28">Populin and Yin, 1998</xref> (mean = 26 ms; failure to extinguish across 10,000 unreinforced trials). The latter authors obtained an even more rapid response (mean = 21 ms) when the sound was preceded by a visual stimulus that served as warning signal and indicated that the cat should maintain gaze at a fixation point. They argued that the short-latency pinna response is too rapid to be mediated by the brain region most centrally involved in orienting, the superior colliculus (SC). This is because an earlier study, <xref ref-type="bibr" rid="bib27">Populin and Yin, 1997</xref>, had found the average first-spike latency in the relevant portion of this structure to be 19 ms.</p><p>Comparisons with these cat studies suggest that our participants' auriculomotor responses may have been primarily also of the short-latency variety that is not mediated by the SC. Two of the conditions tested by <xref ref-type="bibr" rid="bib28">Populin and Yin, 1998</xref> involved brief, lateralized auditory stimuli that, as in the present Experiment 1, were not task-relevant. The stimuli elicited short-latency ipsilateral pinna movements that were temporally uncorrelated with gaze shifts (see their Figures 6 and 8). During the delayed-saccade condition of their study, laterally presented sounds were task-relevant and forward fixation was required, as in our Experiment 2. Ipsilateral pinna movements triggered by onset of these sounds were of the short-latency variety (21 ms, as noted above). Subsequent, smaller movements were then observed in synchrony with ocular orienting, roughly 400 ms after the fixation point was extinguished (<xref ref-type="fig" rid="fig7">Figure 7</xref>). It is long-latency pinna movements of this sort that <xref ref-type="bibr" rid="bib28">Populin and Yin, 1998</xref> suggested might be mediated by the SC.</p><p>Given the major role of the SC in controlling eye fixation (<xref ref-type="bibr" rid="bib21">Krauzlis et al., 2017</xref>), this structure may also be responsible for sustained maintenance of pinna orientation, such as in Experiment 2. Pinna movements can be triggered by electrical stimulation of the deep and intermediate layers of the SC in accordance with a topographical pattern that is in register with that of eye movements (<xref ref-type="bibr" rid="bib31">Stein and Clamann, 1981</xref>). Lesions of this structure reduce the likelihood of pinna orienting as well as its accuracy, see <xref ref-type="bibr" rid="bib8">Czihak et al., 1983</xref>. Although monosynaptic connections from SC to auriculomotor neurons in the facial nucleus do exist (<xref ref-type="bibr" rid="bib37">Vidal et al., 1988</xref>), pinna control is dominated by disynaptic pathways from the SC that include the paralemniscal, oculomotor, or pontine reticular zones, see <xref ref-type="bibr" rid="bib18">Henkel and Edwards, 1978</xref>; <xref ref-type="bibr" rid="bib33">Takeuchi et al., 1979</xref>; <xref ref-type="bibr" rid="bib37">Vidal et al., 1988</xref>. Among these, the paralemniscal zone appears to be the most important, and its auditory input originates in the nearby nucleus sagulum, see <xref ref-type="bibr" rid="bib17">Henkel, 1981</xref>.</p><p>Portions of neocortex also play a role in controlling pinna movements. Lesions of auditory cortex reduce the kinematic complexity of pinna orienting and slow its habituation, see <xref ref-type="bibr" rid="bib1">Alvarado and Santibañez, 1971</xref>. Stimulation and recording studies in the macaque have identified a premotor ear-eye field (area 8B), which is connected with both auditory cortical areas and the SC, see <xref ref-type="bibr" rid="bib22">Lanzilotto et al., 2013</xref>. These animal neuroanatomy and physiology studies, coupled with <xref ref-type="bibr" rid="bib39">Wilson, 1908</xref> seminal report, make it clear that the eyes and pinnae work together during endogenously cued attentional orienting.</p></sec><sec id="s3-3"><title>Future work</title><p>A recent study in humans by <xref ref-type="bibr" rid="bib13">Gruters et al., 2018</xref>, showed a close relationship between movements of the left and right eardrums and multiple parameters of task-related left- and right-directed eye movements. It will be important in future research to test whether muscular responses of the middle and outer ears are linked in a coordinated manner to ocular orienting. Furthermore, exploration of the relationship between human auriculomotor activity and subtle markers of covert attention (see the recent review given in <xref ref-type="bibr" rid="bib36">van Ede et al., 2019</xref>) corticofugal modulation of ascending auditory pathways (<xref ref-type="bibr" rid="bib26">Perrot et al., 2006</xref>) in endogenous attention, and the neural mechanisms of orienting discussed in the preceding section has scarcely begun.</p><p>Our results have implications for applied science, as well. They suggest that patterns of auricular muscle activity might serve as an easily accessible correlate of top-down processing in endogenous modes of attention. As such, the described effects might complement electroencephalographic indices of attentional focus (<xref ref-type="bibr" rid="bib9">de Cheveigné et al., 2018</xref>; <xref ref-type="bibr" rid="bib29">Schäfer et al., 2018</xref>) in that their sensitivity is exclusively spatial, rather than reflecting a context-specific mixture of modality, feature, location, and object representations. Registration of pinna-orienting might better support near real-time decoding of the attentional focus and, as compared to EEG-based stimulus reconstruction approaches, does not require the exogenous sound source, for example see the discussion in <xref ref-type="bibr" rid="bib29">Schäfer et al., 2018</xref>. Thus, auricular muscle monitoring might support the decoding of auditory attention in technical applications such as attentionally controlled hearing aids that preferentially amplify sounds the user is attempting to listen to. We wish to underscore, though, that the development of such applications would benefit crucially from a better understanding of how the auditory and visual attention systems interact. We hope that the results presented here will stimulate research in this direction.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Both older (N = 12, mean age = 62.7 ± 5.9 y, 8 F, all right-handed) and younger adult (N = 16, mean age = 24.1 ± 3.1 y, 8 F, 15 right-handed, one left-handed) volunteers in Experiment one had age-typical, pure tone audiometric thresholds (1, 2, 4, and 8 kHz; young &lt; 20 dB; old &lt; 40 dB). All served in both studies, but after the 8th participant, Experiment two was altered in several ways (e.g., four stimulus directions rather than two). Only data from the final 21 subjects were retained for Experiment 2. The two groups in this experiment comprised 11 older adults (mean age = 62.6 ± 6.2 y, 8 F, all right-handed) and 10 younger adult (mean age = 24.1 ± 3.6 y, 5 F, nine right-handed, one left-handed). After a detailed explanation of the procedure, all subjects signed a consent form. The study was approved by the responsible ethics committee (ethics commission at the Ärztekammer des Saarlandes, Saarbrücken, Germany; Identification Number: 79/16).</p></sec><sec id="s4-2"><title>Stimuli and tasks</title><p>The four active loudspeakers (KH120A, Neumann, Germany) were positioned at head level, 115 cm. Sounds in Experiment 1 and 2 were reproduced with a soundcard (Scarlett 18i20, Focusrite, UK). The experimental paradigms were programmed using software for scientific computing (Matlab, Mathworks, USA) and Psychtoolbox 3. In Experiment 1, sounds lasted 1.7 – 10.0 s, were delivered every 15 – 40 s, and had an average intensity of 70 dBC, except for foot steps (65 dBC). Each of the nine stimuli (lemur howling, dog barking, helicopter flying, cell phone vibrating, birds singing, baby crying, mosquito buzzing, footsteps, and traffic jam) was repeated four times (i.e., once per speaker). In Experiment 2, the stories were 5 min long, with an average intensity of 50 dBA for younger and 60 dBA for older participants. Participants answered content questions at the conclusion of each condition in this experiment.</p></sec><sec id="s4-3"><title>Electrophysiological recordings and signal processing</title><p>Surface EMGs were recorded with non-recessed, Ag/AgCl electrodes (BME4, BioMed Electrodes, USA), which were 4 mm in diameter for TAM and 6 mm (BME6) in all other cases, see <xref ref-type="fig" rid="fig1">Figure 1</xref>. The signals were AD-converted at 9600 Hz and 24 bit resolution per channel (4 × USBamp, g.tec GmbH, Austria). Skin temperature, skin resistance, electrocardiograms, and EOGs were also recorded. All signal processing algorithms were implemented using the scientific computing software Matlab (Mathworks, USA, Version: 2018a). Because surface electrodes had not previously been used to record from intrinsic ear muscles, we conducted preliminary tests with a participant who exhibited a large, reliable Wilson’s phenomenon and who could voluntarily contract her SAM and PAM. Isolation of the corresponding responses indicated that EMG from TAM electrodes was not an artifact of volume conduction from PAM or SAM. In other words, the TAM activity was not correlated with forced SAM/PAM innervation. Sternocleidomastoid EMG signals were zero-phase bandpass filtered from 60 to 1000 Hz (FIR, 2000th order), the auricular EMG signals from 10 to 1000 Hz (FIR, 2000th order) with a notch filter at 50 Hz (IIR, 2nd order). Horizontal EOG signals were zero-phase filtered from 0.01 to 20 Hz (IIR, 2nd order). All filter operations were performed using Matlab’s filtfilt-function for zero-phase filtering. The filtered signals were then downsampled to 2400 Hz for further processing. The statistical analysis was performed using repeated measures ANOVA (with IBM SPSS Statistics 26). Within-subjects factors were stimulus-muscle correspondence (ipsi- vs. contralateral responses) and anteriority (front vs. back speakers). The only between-subjects factor our statistical model accounted for was age and, in association with that, also stimulus level in the endogenous experiment. Other factors like head-size, audiogram shape or small electrode placement differences were not included in the model. All main and interaction effects were tested.</p><sec id="s4-3-1"><title>Exogeneous (transient) data</title><p>Root-mean-square (RMS) envelopes of the filtered and downsampled EMG signals were calculated with a sliding window (step size = 1 sample, window length = 150 samples/62.5 ms.) The data were then segmented into epochs extending from 3 s prior to stimulus onset until 3 s following termination of the auditory stimulus, which was of variable duration. Epochs were baseline corrected with respect to the mean RMS envelope amplitude of the pre-stimulus interval, that is this mean was subtracted from the epoch. For every participant, normalization was performed within every monitored auricular muscle (e.g., left PAM, right SAM) and for a specific stimulus type (e.g., traffic jam). Since every stimulus type was repeated four times, the largest voltage in the corresponding four epochs was used for normalization. The reference value in each case was the largest voltage at any time point within the four relevant epochs (e.g., the four directions/trials with a traffic jam stimulus) in the EMG recordings of that particular muscle. Each participant’s data were pooled according to whether the side of the stimulus and recorded muscle did or did not match, and then were averaged into contralateral and ipsilateral waveforms. Eighteen trials contributed to each of these per subject contralateral and ipsilateral waveforms, nine from the left speaker and nine from the right. Mean amplitudes were computed across a measurement window extending from 100 to 1500 ms following stimulus onset and were then subjected to statistical analysis.</p></sec><sec id="s4-3-2"><title>Endogenous (sustained) data</title><p>Artifacts of the filtered and downsampled EMG data were reduced by averaging the signal energy of 1 s, non-overlapping segments and rejecting segments that deviated by more than two standard deviations from the mean. For each participant, the mean energy of a given channel during the four listening conditions (left/right × front/back) was calculated and then normalized to the largest value across the 5-min run. These normalized data were then averaged into ipsilateral/contralateral categories and subjected to statistical analysis.</p></sec><sec id="s4-3-3"><title>EMG time-frequency decomposition (for <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref> and <xref ref-type="fig" rid="fig5s2">2</xref>)</title><p>As the rather sustained muscle activity during endogenous attention might be reflected in low frequency components according to convolution models of the EMG (<xref ref-type="bibr" rid="bib10">Farina et al., 2014</xref>), filtered and downsampled EMG signals in Experiment two were decomposed into eight frequency bands by a nonsubsampled octave-band filter bank (5th order Daubechies filter). Each frequency band was then further processed in the same fashion as the broadband signals reported in the main text.</p></sec></sec><sec id="s4-4"><title>Computer vision setup and motion analysis (for <xref ref-type="video" rid="video1">Videos 1</xref>, <xref ref-type="video" rid="video2">2</xref> and <xref ref-type="video" rid="video3">3</xref>)</title><p>Videos were acquired using four Ximea MQ022CG–CM color sensors with a resolution of 1936 × 1216 at 120 frames per second and an exposure of 2 ms. Two cameras were positioned on each side of the head and focused on the ears to record pairwise stereo videos. We used hardware triggering for all four cameras and recorded each camera onto a separate m.2 solid-state-drive to reduce frame loss. We used a KOWA 35 mm macro lens with an aperture of F0.4 which gave us a close-up view of the ear with acceptable depth of field to allow slight movements towards the camera and enough distance such that the cameras did not cast shadows on to the scene. We illuminated the face uniformly with flicker-free LED studio illumination. The cameras were calibrated with the stereo camera calibrator app from the Mathworks Matlab Computer Vision System Toolbox. Calibration was performed whenever camera adjustment required re-alignment of relative stereo camera positions or a change of focus of one of the cameras.</p><p>For 3D reconstruction and motion visualization/quantification, we used functions from the Mathworks Matlab Computer Vision System Toolbox and custom written code. Our analysis system was able to reduce redundancies in optic flow and stereo depth estimation by exploiting the unilateral scene composition and limited degrees of freedom for ear and head movements. For 3D reconstructions, we initialized a sequence with one initial estimation of disparity and subsequently tracked points independently for the left and right image sequence.</p><p>We tracked points with respect to the first frame of the sequence as reference frame with dense optical flow initialized with a rigid motion estimation. Motion was visualized with a Lagrangian motion magnification approach that had a constant magnification factor with respect to the reference frame and prior removal of affine motion with respect to manually selected stable points. The results of the motion analysis with and without magnification can be seen in the videos.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This study was partially supported by the German Federal Ministry of Education and Research, Grant No. BMBF-FZ 03FH004IX5 (PI: DJS). We thank Larissa Arand for assistance with data collection and Becca Sullinger for the artwork in <xref ref-type="fig" rid="fig1">Figure 1</xref>. We acknowledge support by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) and Saarland University within the funding programme Open Access Publishing.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Formal analysis, Supervision, Validation, Investigation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con3"><p>Software, Formal analysis, Validation, Investigation, Visualization</p></fn><fn fn-type="con" id="con4"><p>Data curation, Software, Formal analysis, Validation, Investigation, Visualization</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Methodology</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Formal analysis, Investigation, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: The study was approved by the responsible ethics committee (ethics commission at the Ärztekammer des Saarlandes, Saarbrücken, Germany; app. number 79/16) After a detailed explanation of the procedure, all subjects signed a consent form.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Table Supplements for Experiment 2.</title></caption><media mime-subtype="pdf" mimetype="application" xlink:href="elife-54536-supp1-v1.pdf"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-54536-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The code with and data are available from GitHub <ext-link ext-link-type="uri" xlink:href="https://github.com/a-schroeer/ExogEndogEarProject">https://github.com/a-schroeer/ExogEndogEarProject</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/ExogEndogEarProject">https://github.com/elifesciences-publications/ExogEndogEarProject</ext-link>) and the Dryad Digital Repository <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.d4md86r">https://doi.org/10.5061/dryad.d4md86r</ext-link>, respectively.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Strauss</surname><given-names>DJ</given-names></name><name><surname>Corona-Strauss</surname><given-names>FI</given-names></name><name><surname>Schroeer</surname><given-names>A</given-names></name><name><surname>Flotho</surname><given-names>P</given-names></name><name><surname>Hannemann</surname><given-names>R</given-names></name><name><surname>Hackley</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Data from: Vestigial auriculomotor activity indicates the direction of auditory attention in humans</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.d4md86r</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvarado</surname> <given-names>M</given-names></name><name><surname>Santibañez</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Targeting reflex: some features and inhibition targeting reflex: some features and inhibition</article-title><source>Acta Neurobiologiae Experimentalis</source><volume>31</volume><fpage>33</fpage><lpage>45</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burrows</surname> <given-names>AM</given-names></name><name><surname>Diogo</surname> <given-names>R</given-names></name><name><surname>Waller</surname> <given-names>BM</given-names></name><name><surname>Bonar</surname> <given-names>CJ</given-names></name><name><surname>Liebal</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Evolution of the muscles of facial expression in a monogamous ape: evaluating the relative influences of ecological and phylogenetic factors in hylobatids</article-title><source>The Anatomical Record: Advances in Integrative Anatomy and Evolutionary Biology</source><volume>294</volume><fpage>645</fpage><lpage>663</lpage><pub-id pub-id-type="doi">10.1002/ar.21355</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cattaneo</surname> <given-names>L</given-names></name><name><surname>Pavesi</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The facial motor system the facial motor system</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>38</volume><fpage>135</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2013.11.002</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cherry</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="1953">1953</year><article-title>Some experiments on the recognition of speech, with one and with two ears</article-title><source>The Journal of the Acoustical Society of America</source><volume>25</volume><fpage>975</fpage><lpage>979</lpage><pub-id pub-id-type="doi">10.1121/1.1907229</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coleman</surname> <given-names>MN</given-names></name><name><surname>Ross</surname> <given-names>CF</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Primate auditory diversity and its influence on hearing performance primate auditory diversity and its influence on hearing performance</article-title><source>Anatomical Record</source><volume>281A</volume><fpage>1123</fpage><lpage>1137</lpage><pub-id pub-id-type="doi">10.1002/ar.a.20118</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cook</surname> <given-names>A</given-names></name><name><surname>Patuzzi</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rotation of the eyes (not the head) potentiates the postauricular muscle response</article-title><source>Ear and Hearing</source><volume>35</volume><fpage>230</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e3182a4efdf</pub-id><pub-id pub-id-type="pmid">24441738</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corneil</surname> <given-names>BD</given-names></name><name><surname>Munoz</surname> <given-names>DP</given-names></name><name><surname>Chapman</surname> <given-names>BB</given-names></name><name><surname>Admans</surname> <given-names>T</given-names></name><name><surname>Cushing</surname> <given-names>SL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neuromuscular consequences of reflexive covert orienting</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>13</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1038/nn2023</pub-id><pub-id pub-id-type="pmid">18059264</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Czihak</surname> <given-names>E</given-names></name><name><surname>Santibañez</surname> <given-names>M</given-names></name><name><surname>Klimann</surname> <given-names>M</given-names></name><name><surname>Santibañez</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Audio-visual reaction after unilateral lesions of the superior colliculus in cats</article-title><source>Acta Neurobiologiae Experimentalis</source><volume>43</volume><fpage>15</fpage><lpage>25</lpage><pub-id pub-id-type="pmid">6880869</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveigné</surname> <given-names>A</given-names></name><name><surname>Wong</surname> <given-names>DDE</given-names></name><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>Hjortkjær</surname> <given-names>J</given-names></name><name><surname>Slaney</surname> <given-names>M</given-names></name><name><surname>Lalor</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Decoding the auditory brain with canonical component analysis</article-title><source>NeuroImage</source><volume>172</volume><fpage>206</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.033</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farina</surname> <given-names>D</given-names></name><name><surname>Merletti</surname> <given-names>R</given-names></name><name><surname>Enoka</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The extraction of neural strategies from the surface EMG: an update</article-title><source>Journal of Applied Physiology</source><volume>117</volume><fpage>1215</fpage><lpage>1230</lpage><pub-id pub-id-type="doi">10.1152/japplphysiol.00162.2014</pub-id><pub-id pub-id-type="pmid">25277737</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstle</surname> <given-names>M</given-names></name><name><surname>Wilkinson</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1929">1929</year><article-title>The oculo-aural movement</article-title><source>Journal of Neurology and Psychopathology</source><volume>9</volume><fpage>228</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1136/jnnp.s1-9.35.228</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibbs</surname> <given-names>RA</given-names></name><name><surname>Rogers</surname> <given-names>J</given-names></name><name><surname>Katze</surname> <given-names>MG</given-names></name><name><surname>Bumgarner</surname> <given-names>R</given-names></name><name><surname>Weinstock</surname> <given-names>GM</given-names></name><name><surname>Mardis</surname> <given-names>ER</given-names></name><name><surname>Remington</surname> <given-names>KA</given-names></name><name><surname>Strausberg</surname> <given-names>RL</given-names></name><name><surname>Venter</surname> <given-names>JC</given-names></name><name><surname>Wilson</surname> <given-names>RK</given-names></name><name><surname>Batzer</surname> <given-names>MA</given-names></name><name><surname>Bustamante</surname> <given-names>CD</given-names></name><name><surname>Eichler</surname> <given-names>EE</given-names></name><name><surname>Hahn</surname> <given-names>MW</given-names></name><name><surname>Hardison</surname> <given-names>RC</given-names></name><name><surname>Makova</surname> <given-names>KD</given-names></name><name><surname>Miller</surname> <given-names>W</given-names></name><name><surname>Milosavljevic</surname> <given-names>A</given-names></name><name><surname>Palermo</surname> <given-names>RE</given-names></name><name><surname>Siepel</surname> <given-names>A</given-names></name><name><surname>Sikela</surname> <given-names>JM</given-names></name><name><surname>Attaway</surname> <given-names>T</given-names></name><name><surname>Bell</surname> <given-names>S</given-names></name><name><surname>Bernard</surname> <given-names>KE</given-names></name><name><surname>Buhay</surname> <given-names>CJ</given-names></name><name><surname>Chandrabose</surname> <given-names>MN</given-names></name><name><surname>Dao</surname> <given-names>M</given-names></name><name><surname>Davis</surname> <given-names>C</given-names></name><name><surname>Delehaunty</surname> <given-names>KD</given-names></name><name><surname>Ding</surname> <given-names>Y</given-names></name><name><surname>Dinh</surname> <given-names>HH</given-names></name><name><surname>Dugan-Rocha</surname> <given-names>S</given-names></name><name><surname>Fulton</surname> <given-names>LA</given-names></name><name><surname>Gabisi</surname> <given-names>RA</given-names></name><name><surname>Garner</surname> <given-names>TT</given-names></name><name><surname>Godfrey</surname> <given-names>J</given-names></name><name><surname>Hawes</surname> <given-names>AC</given-names></name><name><surname>Hernandez</surname> <given-names>J</given-names></name><name><surname>Hines</surname> <given-names>S</given-names></name><name><surname>Holder</surname> <given-names>M</given-names></name><name><surname>Hume</surname> <given-names>J</given-names></name><name><surname>Jhangiani</surname> <given-names>SN</given-names></name><name><surname>Joshi</surname> <given-names>V</given-names></name><name><surname>Khan</surname> <given-names>ZM</given-names></name><name><surname>Kirkness</surname> <given-names>EF</given-names></name><name><surname>Cree</surname> <given-names>A</given-names></name><name><surname>Fowler</surname> <given-names>RG</given-names></name><name><surname>Lee</surname> <given-names>S</given-names></name><name><surname>Lewis</surname> <given-names>LR</given-names></name><name><surname>Li</surname> <given-names>Z</given-names></name><name><surname>Liu</surname> <given-names>YS</given-names></name><name><surname>Moore</surname> <given-names>SM</given-names></name><name><surname>Muzny</surname> <given-names>D</given-names></name><name><surname>Nazareth</surname> <given-names>LV</given-names></name><name><surname>Ngo</surname> <given-names>DN</given-names></name><name><surname>Okwuonu</surname> <given-names>GO</given-names></name><name><surname>Pai</surname> <given-names>G</given-names></name><name><surname>Parker</surname> <given-names>D</given-names></name><name><surname>Paul</surname> <given-names>HA</given-names></name><name><surname>Pfannkoch</surname> <given-names>C</given-names></name><name><surname>Pohl</surname> <given-names>CS</given-names></name><name><surname>Rogers</surname> <given-names>YH</given-names></name><name><surname>Ruiz</surname> <given-names>SJ</given-names></name><name><surname>Sabo</surname> <given-names>A</given-names></name><name><surname>Santibanez</surname> <given-names>J</given-names></name><name><surname>Schneider</surname> <given-names>BW</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Sodergren</surname> <given-names>E</given-names></name><name><surname>Svatek</surname> <given-names>AF</given-names></name><name><surname>Utterback</surname> <given-names>TR</given-names></name><name><surname>Vattathil</surname> <given-names>S</given-names></name><name><surname>Warren</surname> <given-names>W</given-names></name><name><surname>White</surname> <given-names>CS</given-names></name><name><surname>Chinwalla</surname> <given-names>AT</given-names></name><name><surname>Feng</surname> <given-names>Y</given-names></name><name><surname>Halpern</surname> <given-names>AL</given-names></name><name><surname>Hillier</surname> <given-names>LW</given-names></name><name><surname>Huang</surname> <given-names>X</given-names></name><name><surname>Minx</surname> <given-names>P</given-names></name><name><surname>Nelson</surname> <given-names>JO</given-names></name><name><surname>Pepin</surname> <given-names>KH</given-names></name><name><surname>Qin</surname> <given-names>X</given-names></name><name><surname>Sutton</surname> <given-names>GG</given-names></name><name><surname>Venter</surname> <given-names>E</given-names></name><name><surname>Walenz</surname> <given-names>BP</given-names></name><name><surname>Wallis</surname> <given-names>JW</given-names></name><name><surname>Worley</surname> <given-names>KC</given-names></name><name><surname>Yang</surname> <given-names>SP</given-names></name><name><surname>Jones</surname> <given-names>SM</given-names></name><name><surname>Marra</surname> <given-names>MA</given-names></name><name><surname>Rocchi</surname> <given-names>M</given-names></name><name><surname>Schein</surname> <given-names>JE</given-names></name><name><surname>Baertsch</surname> <given-names>R</given-names></name><name><surname>Clarke</surname> <given-names>L</given-names></name><name><surname>Csürös</surname> <given-names>M</given-names></name><name><surname>Glasscock</surname> <given-names>J</given-names></name><name><surname>Harris</surname> <given-names>RA</given-names></name><name><surname>Havlak</surname> <given-names>P</given-names></name><name><surname>Jackson</surname> <given-names>AR</given-names></name><name><surname>Jiang</surname> <given-names>H</given-names></name><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Messina</surname> <given-names>DN</given-names></name><name><surname>Shen</surname> <given-names>Y</given-names></name><name><surname>Song</surname> <given-names>HX</given-names></name><name><surname>Wylie</surname> <given-names>T</given-names></name><name><surname>Zhang</surname> <given-names>L</given-names></name><name><surname>Birney</surname> <given-names>E</given-names></name><name><surname>Han</surname> <given-names>K</given-names></name><name><surname>Konkel</surname> <given-names>MK</given-names></name><name><surname>Lee</surname> <given-names>J</given-names></name><name><surname>Smit</surname> <given-names>AF</given-names></name><name><surname>Ullmer</surname> <given-names>B</given-names></name><name><surname>Wang</surname> <given-names>H</given-names></name><name><surname>Xing</surname> <given-names>J</given-names></name><name><surname>Burhans</surname> <given-names>R</given-names></name><name><surname>Cheng</surname> <given-names>Z</given-names></name><name><surname>Karro</surname> <given-names>JE</given-names></name><name><surname>Ma</surname> <given-names>J</given-names></name><name><surname>Raney</surname> <given-names>B</given-names></name><name><surname>She</surname> <given-names>X</given-names></name><name><surname>Cox</surname> <given-names>MJ</given-names></name><name><surname>Demuth</surname> <given-names>JP</given-names></name><name><surname>Dumas</surname> <given-names>LJ</given-names></name><name><surname>Han</surname> <given-names>SG</given-names></name><name><surname>Hopkins</surname> <given-names>J</given-names></name><name><surname>Karimpour-Fard</surname> <given-names>A</given-names></name><name><surname>Kim</surname> <given-names>YH</given-names></name><name><surname>Pollack</surname> <given-names>JR</given-names></name><name><surname>Vinar</surname> <given-names>T</given-names></name><name><surname>Addo-Quaye</surname> <given-names>C</given-names></name><name><surname>Degenhardt</surname> <given-names>J</given-names></name><name><surname>Denby</surname> <given-names>A</given-names></name><name><surname>Hubisz</surname> <given-names>MJ</given-names></name><name><surname>Indap</surname> <given-names>A</given-names></name><name><surname>Kosiol</surname> <given-names>C</given-names></name><name><surname>Lahn</surname> <given-names>BT</given-names></name><name><surname>Lawson</surname> <given-names>HA</given-names></name><name><surname>Marklein</surname> <given-names>A</given-names></name><name><surname>Nielsen</surname> <given-names>R</given-names></name><name><surname>Vallender</surname> <given-names>EJ</given-names></name><name><surname>Clark</surname> <given-names>AG</given-names></name><name><surname>Ferguson</surname> <given-names>B</given-names></name><name><surname>Hernandez</surname> <given-names>RD</given-names></name><name><surname>Hirani</surname> <given-names>K</given-names></name><name><surname>Kehrer-Sawatzki</surname> <given-names>H</given-names></name><name><surname>Kolb</surname> <given-names>J</given-names></name><name><surname>Patil</surname> <given-names>S</given-names></name><name><surname>Pu</surname> <given-names>LL</given-names></name><name><surname>Ren</surname> <given-names>Y</given-names></name><name><surname>Smith</surname> <given-names>DG</given-names></name><name><surname>Wheeler</surname> <given-names>DA</given-names></name><name><surname>Schenck</surname> <given-names>I</given-names></name><name><surname>Ball</surname> <given-names>EV</given-names></name><name><surname>Chen</surname> <given-names>R</given-names></name><name><surname>Cooper</surname> <given-names>DN</given-names></name><name><surname>Giardine</surname> <given-names>B</given-names></name><name><surname>Hsu</surname> <given-names>F</given-names></name><name><surname>Kent</surname> <given-names>WJ</given-names></name><name><surname>Lesk</surname> <given-names>A</given-names></name><name><surname>Nelson</surname> <given-names>DL</given-names></name><name><surname>O'brien</surname> <given-names>WE</given-names></name><name><surname>Prüfer</surname> <given-names>K</given-names></name><name><surname>Stenson</surname> <given-names>PD</given-names></name><name><surname>Wallace</surname> <given-names>JC</given-names></name><name><surname>Ke</surname> <given-names>H</given-names></name><name><surname>Liu</surname> <given-names>XM</given-names></name><name><surname>Wang</surname> <given-names>P</given-names></name><name><surname>Xiang</surname> <given-names>AP</given-names></name><name><surname>Yang</surname> <given-names>F</given-names></name><name><surname>Barber</surname> <given-names>GP</given-names></name><name><surname>Haussler</surname> <given-names>D</given-names></name><name><surname>Karolchik</surname> <given-names>D</given-names></name><name><surname>Kern</surname> <given-names>AD</given-names></name><name><surname>Kuhn</surname> <given-names>RM</given-names></name><name><surname>Smith</surname> <given-names>KE</given-names></name><name><surname>Zwieg</surname> <given-names>AS</given-names></name><collab>Rhesus Macaque Genome Sequencing and Analysis Consortium</collab></person-group><year iso-8601-date="2007">2007</year><article-title>Evolutionary and biomedical insights from the rhesus macaque genome</article-title><source>Science</source><volume>316</volume><fpage>222</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1126/science.1139247</pub-id><pub-id pub-id-type="pmid">17431167</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gruters</surname> <given-names>KG</given-names></name><name><surname>Murphy</surname> <given-names>DLK</given-names></name><name><surname>Jenson</surname> <given-names>CD</given-names></name><name><surname>Smith</surname> <given-names>DW</given-names></name><name><surname>Shera</surname> <given-names>CA</given-names></name><name><surname>Groh</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The eardrums move when the eyes move: a multisensory effect on the mechanics of hearing</article-title><source>PNAS</source><volume>115</volume><fpage>E1309</fpage><lpage>E1318</lpage><pub-id pub-id-type="doi">10.1073/pnas.1717948115</pub-id><pub-id pub-id-type="pmid">29363603</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hackley</surname> <given-names>SA</given-names></name><name><surname>Woldorff</surname> <given-names>M</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Combined use of microreflexes and event-related brain potentials as measures of auditory selective attention</article-title><source>Psychophysiology</source><volume>24</volume><fpage>632</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1987.tb00343.x</pub-id><pub-id pub-id-type="pmid">3438427</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hackley</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Evidence for a vestigial pinna-orienting system in humans</article-title><source>Psychophysiology</source><volume>52</volume><fpage>1263</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.1111/psyp.12501</pub-id><pub-id pub-id-type="pmid">26211937</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hackley</surname> <given-names>SA</given-names></name><name><surname>Ren</surname> <given-names>X</given-names></name><name><surname>Underwood</surname> <given-names>A</given-names></name><name><surname>Valle-Inclán</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Prepulse inhibition and facilitation of the postauricular reflex, a vestigial remnant of pinna startle</article-title><source>Psychophysiology</source><volume>54</volume><fpage>566</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1111/psyp.12819</pub-id><pub-id pub-id-type="pmid">28168713</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henkel</surname> <given-names>CK</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Afferent sources of a lateral midbrain tegmental zone associated with the pinnae in the cat as mapped by retrograde transport of horseradish peroxidase</article-title><source>Journal of Comparative Neurology</source><volume>20</volume><fpage>213</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1002/cne.902030205</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henkel</surname> <given-names>CK</given-names></name><name><surname>Edwards</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>The superior colliculus control of pinna movements in the cat: possible anatomical connections</article-title><source>The Journal of Comparative Neurology</source><volume>182</volume><fpage>763</fpage><lpage>776</lpage><pub-id pub-id-type="doi">10.1002/cne.901820502</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillyard</surname> <given-names>SA</given-names></name><name><surname>Hink</surname> <given-names>RF</given-names></name><name><surname>Schwent</surname> <given-names>VL</given-names></name><name><surname>Picton</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Electrical signs of selective attention in the human brain</article-title><source>Science</source><volume>182</volume><fpage>177</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1126/science.182.4108.177</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanold</surname> <given-names>PO</given-names></name><name><surname>Young</surname> <given-names>ED</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Proprioceptive information from the Pinna provides somatosensory input to cat dorsal cochlear nucleus</article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>7848</fpage><lpage>7858</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-19-07848.2001</pub-id><pub-id pub-id-type="pmid">11567076</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauzlis</surname> <given-names>RJ</given-names></name><name><surname>Goffart</surname> <given-names>L</given-names></name><name><surname>Hafed</surname> <given-names>ZM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuronal control of fixation and fixational eye movements</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><elocation-id>20160205</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0205</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanzilotto</surname> <given-names>M</given-names></name><name><surname>Perciavalle</surname> <given-names>V</given-names></name><name><surname>Lucchetti</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A new field in monkey's frontal cortex: premotor ear-eye field (PEEF)</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>37</volume><fpage>1434</fpage><lpage>1444</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2013.05.010</pub-id><pub-id pub-id-type="pmid">23727051</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lovell</surname> <given-names>M</given-names></name><name><surname>Sutton</surname> <given-names>D</given-names></name><name><surname>Lindeman</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Muscle spindles in nonhuman primate extrinsic auricular muscles</article-title><source>The Anatomical Record</source><volume>189</volume><fpage>519</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1002/ar.1091890310</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Beirne</surname> <given-names>GA</given-names></name><name><surname>Patuzzi</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Basic properties of the sound-evoked post-auricular muscle response (PAMR)</article-title><source>Hearing Research</source><volume>138</volume><fpage>115</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(99)00159-8</pub-id><pub-id pub-id-type="pmid">10575120</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patuzzi</surname> <given-names>RB</given-names></name><name><surname>O'Beirne</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of eye rotation on the sound-evoked post-auricular muscle response (PAMR)</article-title><source>Hearing Research</source><volume>138</volume><fpage>133</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(99)00160-4</pub-id><pub-id pub-id-type="pmid">10575121</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrot</surname> <given-names>X</given-names></name><name><surname>Ryvlin</surname> <given-names>P</given-names></name><name><surname>Isnard</surname> <given-names>J</given-names></name><name><surname>Guénot</surname> <given-names>M</given-names></name><name><surname>Catenoix</surname> <given-names>H</given-names></name><name><surname>Fischer</surname> <given-names>C</given-names></name><name><surname>Mauguière</surname> <given-names>F</given-names></name><name><surname>Collet</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Evidence for corticofugal modulation of peripheral auditory activity in humans</article-title><source>Cerebral Cortex</source><volume>16</volume><fpage>941</fpage><lpage>948</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj035</pub-id><pub-id pub-id-type="pmid">16151174</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Populin</surname> <given-names>LC</given-names></name><name><surname>Yin</surname> <given-names>TCT</given-names></name></person-group><year iso-8601-date="1997">1997</year><chapter-title>Sensitivity of auditory cells in the superior colliculus to eye position in the behaving cat</chapter-title><person-group person-group-type="editor"><name><surname>Palmer</surname> <given-names>A. R</given-names></name><name><surname>Rees</surname> <given-names>A</given-names></name><name><surname>Summerfield</surname> <given-names>A</given-names></name><name><surname>Meddis</surname> <given-names>R</given-names></name></person-group><source>Psychophysical and Physiological Advances in Hearing</source><publisher-loc>London</publisher-loc><publisher-name>Whurr</publisher-name><fpage>441</fpage><lpage>448</lpage></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Populin</surname> <given-names>LC</given-names></name><name><surname>Yin</surname> <given-names>TC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Pinna movements of the cat during sound localization</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>4233</fpage><lpage>4243</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-11-04233.1998</pub-id><pub-id pub-id-type="pmid">9592101</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schäfer</surname> <given-names>PJ</given-names></name><name><surname>Corona-Strauss</surname> <given-names>FI</given-names></name><name><surname>Hannemann</surname> <given-names>R</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name><name><surname>Strauss</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Testing the limits of the stimulus reconstruction approach: auditory attention decoding in a Four-Speaker free field environment</article-title><source>Trends in Hearing</source><volume>22</volume><elocation-id>233121651881660</elocation-id><pub-id pub-id-type="doi">10.1177/2331216518816600</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegmund</surname> <given-names>H</given-names></name><name><surname>Santibáñez</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Effector pattern of the audio-visual targeting reflex in cats</article-title><source>Acta Neurobiologiae Experimentalis</source><volume>42</volume><fpage>311</fpage><lpage>326</lpage><pub-id pub-id-type="pmid">7184325</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname> <given-names>BE</given-names></name><name><surname>Clamann</surname> <given-names>HP</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Control of pinna movements and sensorimotor register in cat superior colliculus</article-title><source>Brain, Behavior and Evolution</source><volume>19</volume><fpage>180</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1159/000121641</pub-id><pub-id pub-id-type="pmid">7326575</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stekelenburg</surname> <given-names>JJ</given-names></name><name><surname>van Boxtel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Pericranial muscular, respiratory, and heart rate components of the orienting response</article-title><source>Psychophysiology</source><volume>39</volume><fpage>707</fpage><lpage>722</lpage><pub-id pub-id-type="doi">10.1111/1469-8986.3960707</pub-id><pub-id pub-id-type="pmid">12462499</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takeuchi</surname> <given-names>Y</given-names></name><name><surname>Nakano</surname> <given-names>K</given-names></name><name><surname>Uemura</surname> <given-names>M</given-names></name><name><surname>Matsuda</surname> <given-names>K</given-names></name><name><surname>Matsushima</surname> <given-names>R</given-names></name><name><surname>Mizuno</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Mesencephalic and pontine afferent fiber system to the facial neucleus in the cat: a study using the horseradish peroxidase and silver impregnation techniques</article-title><source>Experimental Neurology</source><volume>66</volume><fpage>330</fpage><lpage>342</lpage><pub-id pub-id-type="doi">10.1016/0014-4886(79)90084-0</pub-id><pub-id pub-id-type="pmid">488225</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tollin</surname> <given-names>DJ</given-names></name><name><surname>Ruhland</surname> <given-names>JL</given-names></name><name><surname>Yin</surname> <given-names>TC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The vestibulo-auricular reflex</article-title><source>Journal of Neurophysiology</source><volume>101</volume><fpage>1258</fpage><lpage>1266</lpage><pub-id pub-id-type="doi">10.1152/jn.90977.2008</pub-id><pub-id pub-id-type="pmid">19129296</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urban</surname> <given-names>PP</given-names></name><name><surname>Marczynski</surname> <given-names>U</given-names></name><name><surname>Hopf</surname> <given-names>HC</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>The oculo-auricular phenomenon: findings in normals and patients with brainstem lesions</article-title><source>Brain : A Journal of Neurology</source><volume>116 </volume><fpage>727</fpage><lpage>738</lpage><pub-id pub-id-type="doi">10.1093/brain/116.3.727</pub-id><pub-id pub-id-type="pmid">8513400</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ede</surname> <given-names>F</given-names></name><name><surname>Chekroud</surname> <given-names>SR</given-names></name><name><surname>Nobre</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Human gaze tracks attentional focusing in memorized visual space</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>462</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0549-y</pub-id><pub-id pub-id-type="pmid">31089296</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidal</surname> <given-names>PP</given-names></name><name><surname>May</surname> <given-names>PJ</given-names></name><name><surname>Baker</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Synaptic organization of the tectal-facial pathways in the cat. I. synaptic potentials following collicular stimulation</article-title><source>Journal of Neurophysiology</source><volume>60</volume><fpage>769</fpage><lpage>797</lpage><pub-id pub-id-type="doi">10.1152/jn.1988.60.2.769</pub-id><pub-id pub-id-type="pmid">3171650</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waller</surname> <given-names>BM</given-names></name><name><surname>Parr</surname> <given-names>LA</given-names></name><name><surname>Gothard</surname> <given-names>KM</given-names></name><name><surname>Burrows</surname> <given-names>AM</given-names></name><name><surname>Fuglevand</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mapping the contribution of single muscles to facial movements in the rhesus macaque</article-title><source>Physiology &amp; Behavior</source><volume>95</volume><fpage>93</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1016/j.physbeh.2008.05.002</pub-id><pub-id pub-id-type="pmid">18582909</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>SAK</given-names></name></person-group><year iso-8601-date="1908">1908</year><article-title>A note on an associated movement of the eyes and ears in man</article-title><source>Review of Neurology and Psychiatry</source><volume>6</volume><fpage>331</fpage><lpage>336</lpage></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54536.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Groh</surname><given-names>Jennifer M</given-names></name><role>Reviewing Editor</role><aff><institution>Duke University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Verhulst</surname><given-names>Sarah</given-names> </name><role>Reviewer</role><aff><institution>Ghent University</institution><country>Belgium</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Shera</surname><given-names>Christopher</given-names> </name><role>Reviewer</role><aff><institution>University of Southern California</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Corneil</surname><given-names>Brian D</given-names></name><role>Reviewer</role><aff><institution>University of Western Ontario</institution><country>Canada</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Unlike cats and dogs, humans can at most barely move their ears. This study reports that the largely vestigial muscles of the ear nevertheless retain a sensitivity to spatial attention: changes in electrical activity of ear muscles could be evoked both in subjects deliberately attempting to listen to a story from one location while ignoring another, and when subjects were surprised by novel sounds while reading an irrelevant essay. The attentional effects on ear muscle electrical activity reported in this study substantially expand the scope of knowledge regarding top down mechanisms in the brain and how they influence the sense organs.</p><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting your work entitled &quot;Vestigial auriculomotor activity indicates the direction of auditory attention in humans&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Christopher Shera (Reviewer #1); Sarah Verhurst (Reviewer #2).</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that this submission of your work will not be considered further for publication in <italic>eLife</italic>. However, the reviewers found merit in the project. Should you choose to revise the manuscript, we ask that authors include the original submission manuscript number, title, and corresponding author and upload a point-by-point response to the reviews including any changes they have made to the manuscript.</p><p>The comments of the reviewers are included below. As you may be aware, at <italic>eLife</italic> those initial reviews set the stage for a subsequent consultation among the reviewers to achieve consensus. Most of our discussion centered on the most challenging comments from reviewer 3 regarding the relationship between your findings and eye movements. A possible relationship to eye movements would be interesting either way, but much of this material is only provided in the supplementary figures, and lacks narrative description to make the implications clear (in particular the series of orange and black figures in the supplementary materials).</p><p>We recognize that EOG recordings provide a limited resolution that likely preclude looking at microsaccades in Experiment 2, but Experiment 1 provides an opportunity to ascertain whether there is any ear muscle activity associated with the macrosaccades that occur during reading.</p><p>We also suggest that the results concerning the frontal speakers that are currently presented only in the supplementary materials be included in the main manuscript. As there is no obvious conceptual reason to discount the results from frontal space, they should be given equal prominence as the results from rear space.</p><p><italic>Reviewer #1:</italic></p><p>The manuscript is generally well written, the experiments thorough and compelling, and the content novel and genuinely intriguing. My comments are relatively minor.</p><p>Abstract: The logic of the first paragraph of the Abstract is confusing and the phrasing sometimes awkward. I suggest reworking it along these lines:</p><p>&quot;Humans, unlike dogs and cats, are not commonly thought to move their ears when focusing auditory attention, either reflexively toward novel sounds or voluntarily toward those that are goal-relevant. Nevertheless, humans may retain a vestigial pinna-orienting system that persists as a &quot;neural fossil&quot; within the brain. Consistent with this hypothesis, we demonstrate that the direction of auditory attention is reflected in the sustained electrical activity of muscles within the vestigial auriculomotor system.&quot;</p><p>Results paragraph two: What is ps? A typo? Or is it somehow supposed to be the plural of p? If the latter, it would be much clearer to write &quot;p values &lt; 0.001&quot;.</p><p>Results paragraph four: This paragraph is out of place in the Results and should be moved to the Discussion.</p><p>Final paragraph of the Results: This paragraph is also rather jarringly out of place and should be moved to the Discussion.</p><p>In the same paragraph: Citation needed for &quot;found in multiple languages&quot;.</p><p>Discussion paragraph two: This might be a good place to include the phrase &quot;about 25 million years ago&quot; which was lost in the rewrite of the Abstract.</p><p>In the same paragraph: Should be &quot;New World monkeys&quot; and &quot;Old World monkeys&quot;</p><p>Subsection “Exogeneous (transient) data”: What was the overlap between consecutive windows (step size)?</p><p>Discussion: The manuscript should cite and discuss the recent and likely related work of Gruters et al., 2018.</p><p><italic>Reviewer #2:</italic></p><p>The manuscript presents how auriculomotor activity forms an objective correlate of exogenous and endogenous auditory attention. To this end, study participants had their heads in a fixed position while listening to sounds coming from different directions to steer auditory attention. At the same time, TAM, PAM, SAM and AAM EMG muscle activity and pinna movements were measured, the latter with a camera. The study is convincing in relating the exogenous auriculomotor activity (increase of PAM/AAM) to auditory attention, because SCM and EOG signals were used to rule out other explanatory variables such as eye gaze and horizontal movement of the head. The presented analysis and supplementary material support the main conclusions. I have one major point related to data-processing, and others are mostly related to interpretation/applicability of results. I am expecting that the data-processing point will not change the main outcome of the study.</p><p>1) I have difficulties understanding the normalization procedure described in the text and its relationship to the values labeled on Figure 2.</p><p>&quot;Epochs were baseline corrected..[].. Amplitudes were normalized separately for each participant and muscle according to the largest value among the four stimulus presentations at any time point within the epoch data&quot;</p><p>When reading this, I interpret that for each person, the peak amplitude of the signal in the largest condition should be one (i.e. normalized), and that in the other three conditions for this person the amplitude should be less than one. Then data was averaged across conditions, after which a mean amplitude across a window of 1400 ms was calculated, or pooled across participants to yield the grand average waveform. When looking at Figure 2, the amplitudes have amplitudes of 100 to 125 [-], and I do not follow the relationship between those numbers and the description in the text. Also, in case magnitudes are normalized to peak maxima, I would expect quite a variability in the baselines of different individuals which should show up strongly in the grand-averaging across people with different base-line estimates.</p><p>2) The characterization of the vestigial network was performed on the basis of a still head during the task, which was necessary to demonstrate the main point of the paper. However, this study does not really go into whether and how strongly this auriculomotor activity plays when people are allowed to move their heads during an attention task. i.e., would this mechanism be complementary to attention-driven gaze, movement steering or does only occur when the head itself cannot move? This differentiation might be important to consider when translating this work to hearing-aid applications. This point is not a drawback of the paper, but should perhaps be discussed more strongly when discussing the potential application areas.</p><p><italic>Reviewer #3:</italic></p><p>General assessment: There are some aspects of the paper that I found intriguing, but there are a number of points that I found either under-explored, or unconvincing. A stronger mechanistic case should also be made relating these findings to others in the literature. The case for using this to aid decoding is also weak. Ultimately, I find that this article falls (fairly far) below the standard of what I would expect for <italic>eLife</italic>.</p><p>Substantive concerns.</p><p>1) The authors could do a much better job placing the current results in the context of other subtle indicators of covert attention. There is an extensive literature on any number of subtle indicators of covert attention (e.g., microsaccades, pupil dilation, even subtle levels of neck muscle recruitment; see van Ede Chekroud Nobre, Nat Human Behavior, 2019 for a recent article in this field), and mechanistic evidence tying these to the superior colliculus. Given that the authors invoke the superior colliculus as a possible node within the auriculomotor pathway, it would help to speculate on how the current results fit in with other work in this literature.</p><p>2) Consideration of these other measures leads to concerns about discounting the possibility of subtle eye or head movements. For eye movements, the use of electrooculography to address gaze orientation is not sufficient. EOG has good temporal resolution, but its spatial resolution is very poor, and can't be used to rule out anything with saccadic amplitudes less than 1-2 deg. Further, given recent results linking eye movements to movements of the eardrum in humans and monkeys (Gruters et al., 2018), a much more precise linking of eye movements are auricular muscle recruitment is warranted, and this could be done much more systematically (e.g., are the auricular muscles actually recruited during eye movements?). Discounting head movements using surface EMG recordings of SCM is also insufficient for a number of reasons. As a powerful head turning muscle, SCM tends not to be recruited for subtle movements of the head. Further, SCM contributes to contralateral, not ipsilateral, head turns, so the focus on the ipsilateral SCM muscle in Experiment 1 supplementary figure 5 is incorrect. Overall, I found the measures used to discount the possibility of subtle eye or head movements to be unconvincing.</p><p>3) The authors speculate that signals from the auricular muscles could be used to decode the locus of auditory spatial attention in near real time. While of potential interest, this claim is highly speculative given the coarseness and apparent variability in the signals shown in the manuscript (which generally show grand averages, with little to no sense of variability). If the authors wish to make the decoding argument, then why not try this? How well can target location actually be extracted from the current data? This would seem to be a tractable question for Experiment 1 (e.g., use data from some subset of trials to train a classifier, and then see how well the classifier works on the other set of trials). Chance performance would be 25% -- can a classifier based on auricular muscle activity do substantially better? I must admit that I am sceptical that signals extracted from these small signals could be useful at all in the real world, given how much the ears move during facial expressions or voluntary ear wiggling. Unless the decoding case can be made more strongly, my advice would be to drop the &quot;decoding&quot; angle from the paper and focus on basic findings.</p><p>4) For Experiment 2, EMG activity is basically averaged across the entire 5 min range. This is a very coarse timeframe and approach, and I can't help but think there would be something more interesting in the data. Is there any way of looking for transient changes in auricular muscle recruitment, and then tying that back to some sort of event during the stream of auditory information? There is the chance of some potentially rich data that really hasn't been mined with the current approach.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Vestigial Auriculomotor Activity Indicates the Direction of Auditory Attention in Humans&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Barbara Shinn-Cunningham (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved and all three reviewers appreciate the importance of this work, but there are some remaining issues that need to be addressed. In particular, reviewer 3 's concerns center on the eye movement results as well as several issues concerning head and neck musculature. These concerns will need to be addressed before the paper can be accepted. All three reviewers and reviewing editor have consulted and agree on the importance of incorporating these additional analyses. The full reviews are included below.</p><p><italic>Reviewer #1:</italic></p><p>I carefully read both rebuttal letter and the revised manuscript and in my view, the revised manuscript is very clear, original and of high scientific standard. The added section on auditory-visual interactions and expanded discussion have turned this paper into a very nice and complete paper.</p><p><italic>Reviewer #2:</italic></p><p>The authors have generally done a fine job addressing the reviewer concerns. …</p><p><italic>Reviewer #3:</italic></p><p>General assessment</p><p>This short manuscript reports that tasks that engage auditory attention either exogenously (Experiment 1) or endogenously (Experiment 2) lead to the recruitment of auricular muscles that subtly change the shape of the pinna, doing so in a spatially-dependent manner. The conclusion is that such recruitment attests to the presence of a vestigial brain circuit. The topic is timely given recent findings linking saccadic eye movements to movements of the eardrum, and a number of other subtle indicators of covert attention driven, for example, by the oculomotor system. The results are intriguing, but more can be done to address other potential confounds, particularly on the oculomotor side.</p><p>Substantive concerns</p><p>The authors have extensively revised the manuscript, and established the phenomena both within and across their subject pool. I still have some concerns about other potential confounds that need to be addressed; as the authors say the neural drive to the ear muscles is so weak that the resultant movements are miniscule compared to those generated during broad smiles or wiggling.</p><p>1) Previously, I had raised concerns about potential confounds from the oculomotor system that orients the line of sight via eye and/or head movements (what the authors term the &quot;visuomotor system&quot; in their response). The authors have added a number of analyses that go some to length to assuage concerns about eye movements. However, grand average measures of EOG across many trials could mask some interactions between eye movements and auricular muscle activity; the data shown in Figure 7 also shows how the variance of the EOG signal decreases after stimulus onset, particularly for stimuli presented at the left-back speaker. More analyses and details are warranted.</p><p>1a) For Experiment 1, the auditory stimuli are presented while subjects are &quot;reading a boring essay&quot;. Please provide details about how large the eye movement excursions were; from Multimodal Figure supplement 1, it appears that the text spanned about +/-12 deg of horizontal visual angle, but this is just one subject.</p><p>1b) The authors acknowledge that they can detect &quot;macro&quot; saccades greater than about 1 degree on average, and these should be analyzed in a more systematic manner than relying on average EOG traces, which could wash out effects. The oculomotor literature on microsaccades has a number of ways of presenting spatial and temporal patterns of saccades timed to external events (e.g., see saccadic “rasters” in Figure 4 of Tian, Yoshida and Hafed Front Syst Neurosci 2016), and I think these should be applied to the data from Experiment 1. See for example the work by Ziad Hafed (Figure 4 in Tian, Yoshida and Hafed Front Syst Neurosci 2016). Something similar for the &quot;macrosaccade&quot; data from Experiment 1 could is needed to establish whether or not stimulus onset is altering the patterning of larger saccadic eye movements.</p><p>1c) For Experiment 2, please provide the &quot;time-resolved&quot; plots for EOG data, similar to what is provided for auricular muscle activity in Figure 6. The EOG data shown in Figure 8 is helpful, but shows data averaged across an entire 5 min segment I believe, which is a very large window.</p><p>2) In regards to a potential concern about head movements, I agree that the vestibular-auricular reflex is unlikely, given that the head was stabilized on a chin-rest. My previous concern was more about whether the act of spatially deploying auditory attention was related to neck muscle contraction that introduced cross-talk at the auricular muscles (the absence of head movement can't be used to infer the absence of neck muscle contraction in this regard). The work by Cooke and Patuzzi, 2014, doesn't address this concern since they examined sternocleidomastoid activity during ipsilateral head turns (right PAM and right SCM recordings during right head turns). SCM is a contralateral, not ipsilateral head turner, so would be directly recruited by leftward turns, which do not appear to have been studied in the Cooke and Patuzzi setup. The Cooke and Patuzzi paper actually mentions the concerns I have about potential cross-talk from other nearby muscles (see end of the first paragraph of their &quot;subjects and methods&quot;). Muscles on the back of the neck (e.g., splenius capitis or suboccipital muscles; insertion on the occiput, which lies close to the mastoid) are ipsilateral head turners. The activity of the suboccipital muscles in particular has also been related to reflexive visuospatial attention in a Posner type task in head-restrained monkeys (e.g., see Corneil et al., 2008). To be clear, I don't think that the entirety of the results could be &quot;explained&quot; by cross-talk from nearby dorsal neck muscles, but the authors should consider this perspective.</p><p>3) A final point about other muscles; Figure 1 shows that the authors recorded EMG activity on zygomaticus major and frontalis, but results are not analyzed. Given the point about how small the movements of interest are compared to those related to smiling, please establish the independence of the auricular muscle recordings from these facial muscles.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54536.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the authors resubmitted a revised version of the paper for consideration. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>The manuscript is generally well written, the experiments thorough and compelling, and the content novel and genuinely intriguing. My comments are relatively minor.</p><p>Abstract: The logic of the first paragraph of the Abstract is confusing and the phrasing sometimes awkward. I suggest reworking it along these lines:</p><p>&quot;Humans, unlike dogs and cats, are not commonly thought to move their ears when focusing auditory attention, either reflexively toward novel sounds or voluntarily toward those that are goal-relevant. Nevertheless, humans may retain a vestigial pinna-orienting system that persists as a &quot;neural fossil&quot; within the brain. Consistent with this hypothesis, we demonstrate that the direction of auditory attention is reflected in the sustained electrical activity of muscles within the vestigial auriculomotor system.&quot;</p></disp-quote><p>Thank you very much for disentangling and sharpening this paragraph. We moved your suggestions 1:1 to the revised version of the manuscript (we just added “for about 25 million years”).</p><disp-quote content-type="editor-comment"><p>Results paragraph two: What is ps? A typo? Or is it somehow supposed to be the plural of p? If the latter, it would be much clearer to write &quot;p values &lt; 0.001&quot;.</p></disp-quote><p>An anecdote: The “ps” was causing a discussion among the authors before the first submission. Some liked it as it is very common in psychology, some disliked it as it is horrible from mathematical point of view. We followed your advice and used “p-values” in the revised version of the manuscript.</p><disp-quote content-type="editor-comment"><p>Results paragraph four: This paragraph is out of place in the Results and should be moved to the Discussion.</p></disp-quote><p>Thanks for the observation. We moved this paragraph along with some associated text to the Discussion.</p><disp-quote content-type="editor-comment"><p>Final paragraph of the Results: This paragraph is also rather jarringly out of place and should be moved to the Discussion.</p></disp-quote><p>Thanks for the observation. We deleted most of this paragraph and moved the rest to the Discussion.</p><disp-quote content-type="editor-comment"><p>In the same paragraph: Citation needed for &quot;found in multiple languages&quot;.</p></disp-quote><p>In the interest of space, we have omitted the comments about ear-straining metaphors from the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Discussion paragraph two: This might be a good place to include the phrase &quot;about 25 million years ago&quot; which was lost in the rewrite of the Abstract.</p></disp-quote><p>Included in the revised version. Thanks</p><disp-quote content-type="editor-comment"><p>In the same paragraph: Should be &quot;New World monkeys&quot; and &quot;Old World monkeys&quot;</p></disp-quote><p>Amended in the revised version. Thanks.</p><disp-quote content-type="editor-comment"><p>Subsection “Exogeneous (transient) data”: What was the overlap between consecutive windows (step size)?</p></disp-quote><p>The RMS value (windows size = 150 samples) was calculated for every sample without overlap or omission (so a step size of 1; that is why we refer to it as an “RMS-envelope”). This is more clearly stated in the revised version (Materials and methods subsection “Electrophysiological Recordings and Data Processing”).</p><disp-quote content-type="editor-comment"><p>Discussion: The manuscript should cite and discuss the recent and likely related work of Gruters et al., 2018.</p></disp-quote><p>We modified the Discussion accordingly, see general comments and comments to reviewer 3. This important citation is now included along with a more complete discussion of eareye interactions.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The manuscript presents how auriculomotor activity forms an objective correlate of exogenous and endogenous auditory attention. To this end, study participants had their heads in a fixed position while listening to sounds coming from different directions to steer auditory attention. At the same time, TAM, PAM, SAM and AAM EMG muscle activity and pinna movements were measured, the latter with a camera. The study is convincing in relating the exogenous auriculomotor activity (increase of PAM/AAM) to auditory attention, because SCM and EOG signals were used to rule out other explanatory variables such as eye gaze and horizontal movement of the head. The presented analysis and supplementary material support the main conclusions. I have one major point related to data-processing, and others are mostly related to interpretation/applicability of results. I am expecting that the data-processing point will not change the main outcome of the study.</p><p>1) I have difficulties understanding the normalization procedure described in the text and its relationship to the values labeled on Figure 2.</p><p>&quot;Epochs were baseline corrected..[].. Amplitudes were normalized separately for each participant and muscle according to the largest value among the four stimulus presentations at any time point within the epoch data&quot;</p><p>When reading this, I interpret that for each person, the peak amplitude of the signal in the largest condition should be one (i.e. normalized), and that in the other three conditions for this person the amplitude should be less than one. Then data was averaged across conditions, after which a mean amplitude across a window of 1400 ms was calculated, or pooled across participants to yield the grand average waveform. When looking at Figure 2, the amplitudes have amplitudes of 100 to 125 [-], and I do not follow the relationship between those numbers and the description in the text. Also, in case magnitudes are normalized to peak maxima, I would expect quite a variability in the baselines of different individuals which should show up strongly in the grand-averaging across people with different base-line estimates.</p></disp-quote><p>In the first step, every epoch was baseline corrected by subtracting the mean baseline value from the epoch. Then, normalization was performed, which you understood correctly: The largest (absolute) value in four conditions (for every stimulus type/subject) is set to 1 (or -1, if the baseline subtraction introduced negative values) and all other values are scaled accordingly. This normalization procedure should be clearer in the revised version. Along these lines, we also point out more clearly that the normalization is not only done independently for every subject and channel, but also for every stimulus type. For example, for subject x, all 4 responses to the stimulus “baby crying” (which was presented 4 times, once from each speaker) recorded at the same channel (for example right PAM) were normalized with respect to each other. The next stimulus, for instance, “dog barking”, was then processed independently. In the next phase, responses of the same stimulus type were pooled (and averaged) to form ipsi- or contralateral responses, defined according to lateral congruence of sound source and recorded muscle. (It was at this point that the orange-and-black epoch matrices presented in the supplement were generated). Then, responses were averaged for every subject, and mean values between 100-1500 ms were calculated for use in the statistical analyses.</p><p>Regarding the plot: For plotting purposes in the previous version of the manuscript, we increased the values to % baseline, which is why the baseline had an average of 100 in the plot. The initial idea was to make those plots more comparable to those in our Stekelenburg and van Boxtel, 2002, reference. However, we see that there was room for confusion, especially as this was not in line with the matrix plots in the supplementary material. Therefore, we removed the percentage-based scale for the y-axis in the revised version. Thanks for drawing our attention to this issue.</p><p>Regarding baseline variability: While there are cases in which peak normalization produces large baseline variability, averaging across epochs ameliorates this problem because the activity is not locked to any event. In the revised version, we have added single-epoch plots along with the average and standard deviation (see Figure 3 and related material in the supplement). Thus, it should be much easier to appreciate the variance within our data in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>2) The characterization of the vestigial network was performed on the basis of a still head during the task, which was necessary to demonstrate the main point of the paper. However, this study does not really go into whether and how strongly this auriculomotor activity plays when people are allowed to move their heads during an attention task. i.e., would this mechanism be complementary to attention-driven gaze, movement steering or does only occur when the head itself cannot move? This differentiation might be important to consider when translating this work to hearing-aid applications. This point is not a drawback of the paper, but should perhaps be discussed more strongly when discussing the potential application areas.</p></disp-quote><p>This is an important insight, and one that we will be addressing in an upcoming paper focused on decoding attention direction from pinna EMG signals. Thank you very much for bringing this up, Dr. Verhulst. However, based on comments from another reviewer, we have decided to reduce discussion of hearing aids and other potential applications in the present manuscript. As you mentioned, it was our main goal in this initial study to isolate the auriculomotor effect as much as possible, so head-ear interactions were not emphasized. Neurobiological research in cats has begun to identify the ways in which pinna orienting is coordinated with head and eye movements (Populin and Yin, 1998). It seems to be more complicated than eye-head coordination due to the acoustic shadow of the head and multidimensionality of ear movements. We briefly allude to the topic in our discussion of the vestibulo-auricular response and future work.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>General assessment: There are some aspects of the paper that I found intriguing, but there are a number of points that I found either under-explored, or unconvincing. A stronger mechanistic case should also be made relating these findings to others in the literature. The case for using this to aid decoding is also weak. Ultimately, I find that this article falls (fairly far) below the standard of what I would expect for eLife.</p></disp-quote><p>The authors would like to thank reviewer 3 for the critical feedback. Our initial submission was motivated by our finding that there is neural drive to auricular muscles when paying attention. We presented evidence in form of a short report which documented for the first time a direct spatial correspondence between sustained auditory attention and sustained auriculomotor activity, as measured electromyographically. The auriculomotor system as such is almost untouched in the literature. Indeed, all previous relevant work for “auriculomotor” was reviewed within the strict space limits of our short report submission. However, Reviewer 3 looked at the data from a new but important angle, focusing mainly on the interactions between the auditory and the visual motor systems. Even though we are convinced that our results are of merit focusing on the auditory modality (the 1<sup>st</sup> and 2<sup>nd</sup> reviewer are also positive in this sense), we completely agree that this new aspect is a very important component for the discussion of our results. This is especially true in light of the recent work linking eye to eardrum movements. Furthermore, we believe that a careful consideration of this topic, embedding our findings within the literature, will extend the study’s impact and more effectively stimulate further research. We clearly stated this now in “Future Work”.</p><disp-quote content-type="editor-comment"><p>Substantive concerns.</p><p>1) The authors could do a much better job placing the current results in the context of other subtle indicators of covert attention. There is an extensive literature on any number of subtle indicators of covert attention (e.g., microsaccades, pupil dilation, even subtle levels of neck muscle recruitment; see van Ede Chekroud Nobre, Nat Human Behavior, 2019 for a recent article in this field), and mechanistic evidence tying these to the superior colliculus. Given that the authors invoke the superior colliculus as a possible node within the auriculomotor pathway, it would help to speculate on how the current results fit in with other work in this literature.</p></disp-quote><p>As we mentioned before, we completely agree with the reviewer regarding this critique. We have added a new paragraph in the Discussion section focusing on this topic.</p><disp-quote content-type="editor-comment"><p>2) Consideration of these other measures leads to concerns about discounting the possibility of subtle eye or head movements. For eye movements, the use of electrooculography to address gaze orientation is not sufficient. EOG has good temporal resolution, but its spatial resolution is very poor, and can't be used to rule out anything with saccadic amplitudes less than 1-2 deg. Further, given recent results linking eye movements to movements of the eardrum in humans and monkeys (Gruters et al., 2018), a much more precise linking of eye movements are auricular muscle recruitment is warranted, and this could be done much more systematically (e.g., are the auricular muscles actually recruited during eye movements?). Discounting head movements using surface EMG recordings of SCM is also insufficient for a number of reasons. As a powerful head turning muscle, SCM tends not to be recruited for subtle movements of the head. Further, SCM contributes to contralateral, not ipsilateral, head turns, so the focus on the ipsilateral SCM muscle in Experiment 1 supplementary figure 5 is incorrect. Overall, I found the measures used to discount the possibility of subtle eye or head movements to be unconvincing.</p></disp-quote><p>The focus of the initial submission as short report was the analysis of auricular muscle activation during spatial auditory attention and not the analysis of the interaction between the auditory and visual systems per se. We merely wanted to monitor relationships between visual and auditory motor systems that are currently known to exist: the oculo-auricular phenomenon of Wilson. In this way, we could exclude that the described effect is secondary to the Wilson’s phenomenon (as we understand it today) or to gross neck movements. Note that the latter has also been ruled out earlier (see Cooke and Patuzzi, 2014). We did not plan our study or look at the data/results from the new but important angle recommended by the third reviewer—that of stressing possible aspects of interactions between the auditory and the visual motor systems that might beyond Wilson’s phenomenon. Accepting the reviewer’s critique and following also the editors’ advice, we analyzed and discussed much more carefully the interactions between the visual and the auditory systems reflected in our data. We also stated clearly that our setup only allows us to rule out spatial attention effects that are secondary to Wilson’s phenomenon. In particular, it becomes clear that:</p><p>1) Grand-average waveforms (shown in the supplementary information, Experiment 1) show a complete absence of location-specific eye movements prior to or synchronous with auricular responses. This does not rule out an effect of eye movements that were too small to be recorded with EOG which, as the reviewer notes, has a resolution of about 2 degrees of arc.</p><p>2) However, Wilson’s phenomenon is rarely elicited by eye movements less than 30 degrees (Urban, Marczynski, and Hopf, 1993). Furthermore, the enhancement of PAM activity appears to become laterally specific (ipsi &gt; contra) only for gaze shifts greater than about 40 degrees (Patuzzi and O’Beirne, 1999, Figure 4). Our EOG recordings were certainly adequate for detecting ocular movements of this size.</p><p>3) There were large saccades in Experiment 2 as participants moved from the right edge of one line of text to the left edge of the next line of text. Inspection of single epochs failed to identify any systematic association between PAM responses and large saccades, see <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref> for four representative trials.</p><fig id="sa2fig1"><label>Author response image 1.</label><caption><title>Macrosaccades in the EOG (black line) during reading in Experiment 1 for subject 15 as example (a subject with large, clear PAM activations).</title><p>It is noticeable that the muscle activations are not linked to the macrosaccades.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-resp-fig1-v1.tif"/></fig><p>4) Activation of TAM during Wilson’s phenomenon exhibits lateral asymmetry in the direction opposite to that of our findings. Visible movements (Gertle and Wilkinson, 1929) and EMG activity (Urban et al., 1993) for this muscle are greater on the side opposite to the direction of gaze. By contrast, in our study TAM activation was enhanced when attention was elicited by sounds on the same side as the muscle.</p><p>5) Only the very fastest saccades (Fischer and Ramsperger, 1984) have a latency comparable to that of the auricular responses documented by our study (70 ms).</p><p>6) The latency of TAM responses with respect to onset of the eye movement in Wilson’s phenomenon averages about 340 ms (Schmidt and Thoden, 1978), far too slow to contribute to the attention effects we have documented in Experiment 1.</p><p>Nonetheless, we clearly state now in the revision that our experiment is not designed to discover or analyze interactions beyond of what is currently known about Wilson’s phenomenon in ear-eye interaction (i.e., that it is slow and only occurs during large, sustained gaze shifts). Especially in the light of recent of work of Jennifer Groh’s group, there is a lot of room for future investigations on this topic. We hope that our results help to stimulate such research.</p><disp-quote content-type="editor-comment"><p>3) The authors speculate that signals from the auricular muscles could be used to decode the locus of auditory spatial attention in near real time. While of potential interest, this claim is highly speculative given the coarseness and apparent variability in the signals shown in the manuscript (which generally show grand averages, with little to no sense of variability). If the authors wish to make the decoding argument, then why not try this? How well can target location actually be extracted from the current data? This would seem to be a tractable question for Experiment 1 (e.g., use data from some subset of trials to train a classifier, and then see how well the classifier works on the other set of trials). Chance performance would be 25% -- can a classifier based on auricular muscle activity do substantially better? I must admit that I am sceptical that signals extracted from these small signals could be useful at all in the real world, given how much the ears move during facial expressions or voluntary ear wiggling. Unless the decoding case can be made more strongly, my advice would be to drop the &quot;decoding&quot; angle from the paper and focus on basic findings.</p></disp-quote><p>Because of the current interest in decoding spatial auditory attention from the EEG signal, the authors thought that this might be an interesting application of the proposed auriculomotor monitoring. Apart from the stimulus reconstruction approach in which EEG signals are employed to reconstruct the envelope of the attended speaker (identification of attention direction requires correlating EEG and speech envelopes), the decoding of endogenous spatial attention from the EEG only (i.e., without the speech envelope) still remains a challenge. One could assume that an activation of the auricular muscles due to spatial attention might “amplify” the endogenous signal. In fact, originally, we found the demonstrated effect of the auricular muscle activation in endogenous modes of attention by trying to decode spatial auditory attention from the EEG (in the BMBF Attentional Microphone project; PI: DJS). A robust effect was just identified for the mastoid electrodes for larger frequencies which turned out to be related to the PAM activation. Driven by this, we just took the EMG with carefully attached electrodes and combined a hybrid machine learning scheme developed before in DJSs group (Strauss and Steidl, JCAM, 2002) to decode spatial auditory attention. For left/right decisions, the decoding scheme reached a performance a way above chance level (abstract at IEEE EMBC 2018 and SPR 2018). However, instead of using a black box learning scheme with abstract EMG features, we were interested in quantifying and analyzing the regularities between spatial auditory attention and the auricular EMG and, perhaps, associated pinna movements. This was the motivation for the present study. The experiments are not designed for a machine learning-based decoding in which many other factors matter, such as recording time and the associated electrodeskin interface stability. We agree with the reviewer that we should remove the engineering application / decoding from the paper as this is indeed not the subject here. In the revised version, we only mention a possible decoding application briefly in the discussion to stimulate a possible interested in those who work on EEG based decoding.</p><p>However, just to complete this response, we would like show that decoding the left/right listening direction from the back speakers (similar as in the EEG literature; a related setup was used in Schäfer et al., 2018) is possible with the described data. In particular, we demonstrate the performance of 2 different endogenous attention decoding approaches in <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref> and <xref ref-type="fig" rid="sa2fig3">Author response image 3</xref> (i.e., for Experiment 2).</p><p>Here <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref> shows the application of a hybrid wavelet-support vector machine classification of waveforms (Strauss and Steidl, “Hybrid Wavelet-Support Vector Machine Classification of Waveforms”, J of Comput and Appl. Math 2002) of the right/left listening direction jointly for the PAM and SAM. This machine is individualized (learned) for each participant and side of the head/ear using EMG data segments of 1s. If the (trained) machines on both sides give the same output at segment n, the corresponding direction is decoded. Otherwise the decoding system is in a doubt state and there is no decision in the decoded direction (i.e., the decoding scheme outputs the same the direction that it had at n-1, i.e., its previous state). For this simple test the filter bank was not adapted, nor the hyperparameters of the support vector machine were particularly tuned. The training set consisted of 162 observations (based on K-nearest-neighbor to mean) and the independent test set of 226 observations with a standard deviation of 32 (because of an energy-threshold based artefact rejection) for each of the 21 participants in Experiment 2. It is easy to see that the performance with &gt;90 % is far above chance level (50%). The technical details of such an individualized decoding scheme with hybrid kernel learning machines for hearing aids can be found in Corona-Strauss, Hannemann, Strauss. Method for Operating a Hearing Aid Device. US Patent Application # 16102983 (Priority date: 14.08.2017 from the German application # 102017214163.8).</p><p>Apart from these adaptive concepts, we also evaluated the analysis of the mean energy (broadband signal) described in the paper as feature for a support vector classifier by means of a “leave-oneparticipant-out” cross validation: That is, the machine is trained with N-1 participants (N=21 in Experiment 2) and participant #N (which was not part of the training set) is classified; then shuffled such that a new set of N-1 participants is used for training and another participant #N-1 is classified (i.e., one participant serves as test set). As we have already mentioned, the mean energy is used as feature as in the time resolved analysis in Figure 6 but with segments of 1s (instead of 10s in the paper) to provide near real time decoding. Note that here we applied a learning across participants without individualization (as in in the analysis in <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>). 10602+/-21 data segments/observations were used for training (from 20 participants) and an independent test set of 530+/-21 from 1 participant. <xref ref-type="fig" rid="sa2fig3">Author response image 3</xref> shows the result for the 21 independently classified participants. Even here the mean classification performance is with 75% above chance level.</p><fig id="sa2fig2"><label>Author response image 2.</label><caption><title>Left/right decoding performance for a conjoint classification of PAM/SAM EMG using an individualized decoding scheme in Experiment 2.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-resp-fig2-v1.tif"/></fig><fig id="sa2fig3"><label>Author response image 3.</label><caption><title>Left/right decoding performance for a conjoint classification of PAM/SAM EMG using an(non-individualized) decoding scheme in Experiment 2 with a leave-one-participant-out cross validation.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-resp-fig3-v1.tif"/></fig><p>The results for the very same “leave-one-participant-out” cross validation is shown in <xref ref-type="fig" rid="sa2fig4">Author response image 4</xref> Experiment 1, i.e., the exogeneous attention setting. Here we used the rms-value of the entire response to generate the learning/testing associations for the support vector machine. We used a training set of 486 observations (subjects<sup>-1</sup>*nstimuli*directions/27*9*2) and an independent test set of 18 observations (nstimuli*directions/9*2). Also here the classification accuracy is far above chance level.</p><fig id="sa2fig4"><label>Author response image 4.</label><caption><title>Left/right decoding performance for a conjoint classification of PAM/AAM/SAM/TAM EMG using an (non-individualized) decoding scheme in Experiment 1 with a leave-one-participant-out cross validation.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54536-resp-fig4-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>4) For Experiment 2, EMG activity is basically averaged across the entire 5 min range. This is a very coarse timeframe and approach, and I can't help but think there would be something more interesting in the data. Is there any way of looking for transient changes in auricular muscle recruitment, and then tying that back to some sort of event during the stream of auditory information? There is the chance of some potentially rich data that really hasn't been mined with the current approach.</p></disp-quote><p>The reviewer is posing an excellent question regarding events which trigger particular movements. This is certainly a subject for future research, which we now note in the Discussion section. However, as we focused on sustained spatial listening in Experiment 2, the speech material was not designed for segmentation into events. In fact, we have chosen speech material that has a rather balanced saliency, arousal level, and a homogeneous information density. There was also, as described, the freedom for participants to pick a story and, consequently, the attended material varied across participants. Nevertheless, we describe more carefully in the revised version that Figure 6 and Figure 7 have a time-resolved analysis (resolution of 10s segments) that shows a sustained activation for the course of the experiment.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>General assessment</p><p>This short manuscript reports that tasks that engage auditory attention either exogenously (Experiment 1) or endogenously (Experiment 2) lead to the recruitment of auricular muscles that subtly change the shape of the pinna, doing so in a spatially-dependent manner. The conclusion is that such recruitment attests to the presence of a vestigial brain circuit. The topic is timely given recent findings linking saccadic eye movements to movements of the eardrum, and a number of other subtle indicators of covert attention driven, for example, by the oculomotor system. The results are intriguing, but more can be done to address other potential confounds, particularly on the oculomotor side.</p></disp-quote><p>The authors would like to thank reviewer 3 for the critical feedback and the suggested new analyses to strengthen our argumentation regarding possible confounds between the auriculo- and oculomotor system or other muscular co-activations. The suggested new analyses were really a clever set of ideas and the results are now reported in the new submission (see figure supplements specified below). The authors really appreciate this interest in oculo-auriculomotor interactions. The suggested analysis techniques along with the recently increasing interest in these interactions provide a solid plan for future research in the involved research labs and hopefully for others too. But as discussed already, our study was really designed for the analysis of auricular muscle activation during spatial auditory attention and not the analysis of the interaction between the auditory and visual systems per se. We merely wanted to monitor relationships between eye and ear movements that are currently known to exist: the oculo-auricular phenomenon of Wilson. In this way, we could exclude that the described effect is secondary to the Wilson’s phenomenon (as we understand it today) or to gross neck movements. Therefore, we tried not to over-analyze our data regarding oculo-auriculomotor interactions and drawing possibly too steep conclusions. The authors really appreciate that reviewer 3 (and the editors) left room for this. Thanks to reviewer 3, the limits of our techniques regarding the oculomotor system are now carefully discussed in the manuscript. Our discussion states clearly that exploring these interactions will be an interesting future research path. Unlike the present studies, which included a fixation cross or reading task, it would be better to employ a free-gaze paradigm. This would maximize the chances of observing oculo-auricular co-activation.</p><disp-quote content-type="editor-comment"><p>Substantive concerns</p><p>The authors have extensively revised the manuscript, and established the phenomena both within and across their subject pool. I still have some concerns about other potential confounds that need to be addressed; as the authors say the neural drive to the ear muscles is so weak that the resultant movements are miniscule compared to those generated during broad smiles or wiggling.</p><p>1) Previously, I had raised concerns about potential confounds from the oculomotor system that orients the line of sight via eye and/or head movements (what the authors term the &quot;visuomotor system&quot; in their response). The authors have added a number of analyses that go some to length to assuage concerns about eye movements. However, grand average measures of EOG across many trials could mask some interactions between eye movements and auricular muscle activity; the data shown in Figure 7 also shows how the variance of the EOG signal decreases after stimulus onset, particularly for stimuli presented at the left-back speaker. More analyses and details are warranted.</p></disp-quote><p>As mentioned above, we added all the suggested new analyses. In particular, we extended supplements of Figure 7 and 8, see the specifications below.</p><disp-quote content-type="editor-comment"><p>1a) For Experiment 1, the auditory stimuli are presented while subjects are &quot;reading a boring essay&quot;. Please provide details about how large the eye movement excursions were; from Multimodal Figure supplement 1, it appears that the text spanned about +/-12 deg of horizontal visual angle, but this is just one subject.</p></disp-quote><p>We have now included the box-plot analysis for all the subjects, see Figure 7—figure supplement 2.</p><disp-quote content-type="editor-comment"><p>1b) The authors acknowledge that they can detect &quot;macro&quot; saccades greater than about 1 degree on average, and these should be analyzed in a more systematic manner than relying on average EOG traces, which could wash out effects. The oculomotor literature on microsaccades has a number of ways of presenting spatial and temporal patterns of saccades timed to external events (e.g., see saccadic “rasters” in Figure 4 of Tian, Yoshida and Hafed Front Syst Neurosci 2016), and I think these should be applied to the data from Experiment 1. See for example the work by Ziad Hafed (Figure 4 in Tian, Yoshida and Hafed Front Syst Neurosci 2016). Something similar for the &quot;macrosaccade&quot; data from Experiment 1 could is needed to establish whether or not stimulus onset is altering the patterning of larger saccadic eye movements.</p></disp-quote><p>The authors would like to thank reviewer 3 for this excellent suggestion. We included a similar (macrosaccadic) raster plot. This shows a rather uniform distribution of the macrosaccades along the time axis/after the stimulus onset, see Figure 7—figure supplement 3.</p><disp-quote content-type="editor-comment"><p>1c) For Experiment 2, please provide the &quot;time-resolved&quot; plots for EOG data, similar to what is provided for auricular muscle activity in Figure 6. The EOG data shown in Figure 8 is helpful, but shows data averaged across an entire 5 min segment I believe, which is a very large window.</p></disp-quote><p>We agree. A time-resolved plot, similar to the one used for the auricular muscles, is now shown in “Figure 8—figure supplement 1”.</p><disp-quote content-type="editor-comment"><p>2) In regards to a potential concern about head movements, I agree that the vestibular-auricular reflex is unlikely, given that the head was stabilized on a chin-rest. My previous concern was more about whether the act of spatially deploying auditory attention was related to neck muscle contraction that introduced cross-talk at the auricular muscles (the absence of head movement can't be used to infer the absence of neck muscle contraction in this regard). The work by Cooke and Patuzzi, 2014, doesn't address this concern since they examined sternocleidomastoid activity during ipsilateral head turns (right PAM and right SCM recordings during right head turns). SCM is a contralateral, not ipsilateral head turner, so would be directly recruited by leftward turns, which do not appear to have been studied in the Cooke and Patuzzi setup. The Cooke and Patuzzi paper actually mentions the concerns I have about potential cross-talk from other nearby muscles (see end of the first paragraph of their &quot;subjects and methods&quot;). Muscles on the back of the neck (e.g., splenius capitis or suboccipital muscles; insertion on the occiput, which lies close to the mastoid) are ipsilateral head turners. The activity of the suboccipital muscles in particular has also been related to reflexive visuospatial attention in a Posner type task in head-restrained monkeys (e.g., see Corneil et al., 2008). To be clear, I don't think that the entirety of the results could be &quot;explained&quot; by cross-talk from nearby dorsal neck muscles, but the authours should consider this perspective.</p></disp-quote><p>The authors agree, even though our bipolar configuration of electrodes would be rather robust to the possible volume conduction effects mentioned in the Cooke and Patuzzi paper. Also, the fact that far apart auricular muscles (not just PAM, but also AAM and SAM) show these effects is of course promising with respect to a co-activation interpretation. We note in the revised manuscript the possibility that subtle, covert activation of head turning muscles as suggested by the reviewer might be correlated with ocular and auricular orienting. Thanks for mentioning this and for the reference. We certainly did consider this perspective, see the last paragraph before the discussion.</p><disp-quote content-type="editor-comment"><p>3) A final point about other muscles; Figure 1 shows that the authors recorded EMG activity on zygomaticus major and frontalis, but results are not analyzed. Given the point about how small the movements of interest are compared to those related to smiling, please establish the independence of the auricular muscle recordings from these facial muscles.</p></disp-quote><p>Indeed, we had electrodes at these positions, mainly for a different subsequent experiment that analyzed the interaction of these muscles and the auricular ones during positive affective states. The frontalis and zygomaticus data are now reported for Experiment 1 and 2, see Figure 7—figure supplement 6-9 and “Figure 8—figure supplement 2”, respectively.</p></body></sub-article></article>