<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">53588</article-id><article-id pub-id-type="doi">10.7554/eLife.53588</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural dynamics of perceptual inference and its reversal during imagery</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-101986"><name><surname>Dijkstra</surname><given-names>Nadine</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1423-9277</contrib-id><email>n.dijkstra@donders.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-166680"><name><surname>Ambrogioni</surname><given-names>Luca</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-166681"><name><surname>Vidaurre</surname><given-names>Diego</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9650-2229</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-113897"><name><surname>van Gerven</surname><given-names>Marcel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2206-9098</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Donders Centre for Cognition, Radboud University, Donders Institute for Brain, Cognition and Behaviour</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution>Wellcome Centre for Human Neuroimaging, University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>Oxford Centre for Human Brain Activity, Oxford University</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution>Department of Clinical Health, Aarhus University</institution><addr-line><named-content content-type="city">Aarhus</named-content></addr-line><country>Denmark</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Varoquaux</surname><given-names>Gaël</given-names></name><role>Reviewing Editor</role><aff><institution>Inria Saclay</institution><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>20</day><month>07</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e53588</elocation-id><history><date date-type="received" iso-8601-date="2019-11-13"><day>13</day><month>11</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-06-30"><day>30</day><month>06</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Dijkstra et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Dijkstra et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-53588-v1.pdf"/><abstract><p>After the presentation of a visual stimulus, neural processing cascades from low-level sensory areas to increasingly abstract representations in higher-level areas. It is often hypothesised that a reversal in neural processing underlies the generation of mental images as abstract representations are used to construct sensory representations in the absence of sensory input. According to predictive processing theories, such reversed processing also plays a central role in later stages of perception. Direct experimental evidence of reversals in neural information flow has been missing. Here, we used a combination of machine learning and magnetoencephalography to characterise neural dynamics in humans. We provide direct evidence for a reversal of the perceptual feed-forward cascade during imagery and show that, during perception, such reversals alternate with feed-forward processing in an 11 Hz oscillatory pattern. Together, these results show how common feedback processes support both veridical perception and mental imagery.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>perception</kwd><kwd>mental imagery</kwd><kwd>predictive processing</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Netherlands Organisation for Scientific Research</institution></institution-wrap></funding-source><award-id>639.072.51</award-id><principal-award-recipient><name><surname>Ambrogioni</surname><given-names>Luca</given-names></name><name><surname>van Gerven</surname><given-names>Marcel</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Perceptual feedforward information flow is reversed during mental imagery and later stages of perception.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>When light hits the retina, a complex cascade of neural processing is triggered. Light waves are transformed into electrical signals that travel via the lateral geniculate nucleus of the thalamus to the visual cortex (<xref ref-type="bibr" rid="bib6">Card and Moore, 1989</xref>; <xref ref-type="bibr" rid="bib49">Reid and Alonso, 1995</xref>) (but see [<xref ref-type="bibr" rid="bib5">Bullier, 2001</xref>] for other routes). First, low-level visual features such as orientation and spatial frequency are processed in primary, posterior visual areas (<xref ref-type="bibr" rid="bib24">Hubel and Wiesel, 1968</xref>) after which activation spreads forward towards secondary, more anterior visual areas where high-level features such as shape and eventually semantic category are processed (<xref ref-type="bibr" rid="bib39">Maunsell and Newsome, 1987</xref>; <xref ref-type="bibr" rid="bib56">Thorpe and Fabre-Thorpe, 2001</xref>; <xref ref-type="bibr" rid="bib60">Vogels and Orban, 1996</xref>). This initial feed-forward flow through the visual hierarchy is completed within 150 ms (<xref ref-type="bibr" rid="bib51">Seeliger et al., 2018</xref>; <xref ref-type="bibr" rid="bib55">Thorpe et al., 1996</xref>) after which feedback processes are assumed to further sharpen representations over time until a stable percept is achieved (<xref ref-type="bibr" rid="bib7">Cauchoix et al., 2014</xref>; <xref ref-type="bibr" rid="bib31">Kok et al., 2012</xref>).</p><p>Activation in visual areas can also be triggered internally, in the absence of external sensory signals. During mental imagery, information from memory is used to generate rich visual representations. Neural representations activated during imagery are highly similar to those activated during perception (<xref ref-type="bibr" rid="bib11">Dijkstra et al., 2019</xref>). Imagining an object activates similar object representations in high-level visual cortex (<xref ref-type="bibr" rid="bib9">Dijkstra et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Ishai et al., 2000</xref>; <xref ref-type="bibr" rid="bib35">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib48">Reddy et al., 2010</xref>) and generating a mental image with simple visual features such as oriented gratings or letters is associated with perception-like activation of low-level visual areas (<xref ref-type="bibr" rid="bib2">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Pearson et al., 2008</xref>; <xref ref-type="bibr" rid="bib52">Senden et al., 2019</xref>).</p><p>In contrast, the temporal dynamics underlying the activation within the visual system during perception and mental imagery are presumably very different. The neural dynamics of the early stages of perception have been extensively characterised with intracranial electrophysiological recordings in primates (<xref ref-type="bibr" rid="bib55">Thorpe et al., 1996</xref>; <xref ref-type="bibr" rid="bib56">Thorpe and Fabre-Thorpe, 2001</xref>; <xref ref-type="bibr" rid="bib24">Hubel and Wiesel, 1968</xref>). However, the neural dynamics of imagery, i.e. how activation travels through the brain during internally generated visual experience, remain unclear. Researchers from various fields have proposed that the direction of information flow during internally generated visual experience is reversed compared to perception (<xref ref-type="bibr" rid="bib1">Ahissar and Hochstein, 2004</xref>; <xref ref-type="bibr" rid="bib22">Hochstein and Ahissar, 2002</xref>; <xref ref-type="bibr" rid="bib32">Kosslyn et al., 2001</xref>; <xref ref-type="bibr" rid="bib45">Pearson and Keogh, 2019</xref>). In line with this idea, a recent study showed that during memory recall, high-level, semantic representations were active before low-level, perceptual representations (<xref ref-type="bibr" rid="bib36">Linde-Domingo et al., 2019</xref>). However, the localisation of activation in this study was ambiguous such that it is possible that all processing happened within high-level visual cortex but that only the dimension to which the neurons were sensitive changed from abstract features to perceptual features over time (for an example of dynamic neural tuning, see <xref ref-type="bibr" rid="bib53">Spaak et al., 2017</xref>. Moreover, memory recall was not directly compared with memory encoding. Therefore, it remains unclear whether the same perceptual cascade of neural activation is reactivated in reverse order during internally generated visual experience.</p><p>According to predictive processing (PP) theories, reversals of information flow also play an important role during perception. PP states that the brain deals with the inherent ambiguity of incoming sensory signals by incorporating prior knowledge about the world (<xref ref-type="bibr" rid="bib20">Helmholtz, 1925</xref>). This knowledge is used to generate top-down sensory predictions which are compared to the bottom-up sensory input. Perceptual inference is then accomplished by iteratively updating the model of the world until the difference between prediction and input is minimised (<xref ref-type="bibr" rid="bib13">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib30">Knill and Pouget, 2004</xref>). Therefore, the neural dynamics of stimulus information during perception should be characterised by an interplay between feed-forward and feedback sweeps. Simulations based on PP models predict that these recurrent dynamics are dominated by slow-wave oscillations (<xref ref-type="bibr" rid="bib4">Bastos et al., 2012</xref>; <xref ref-type="bibr" rid="bib37">Lozano-Soldevilla and VanRullen, 2019</xref>).</p><p>In this study, we used magnetoencephalography (MEG) and machine learning to characterise the spatio-temporal dynamics of information flow during mental imagery and perception. We first characterised neural activity during the initial perceptual feed-forward sweep using multivariate classifiers at different time points which served as proxies for representations in different visual areas. That is, decoding at early perception time points was taken to reflect stimulus representations in low-level, posterior visual areas while decoding at later time points was taken to reflect high-level, anterior visual representations. Then, we estimated when these feed-forward perception models were reactivated during imagery and later stages of perception to infer the neural dynamics of information processing.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Twenty-five participants executed a retro-cue task while MEG was measured. During the task, two consecutive stimuli were presented, a face and a house or a house and a face, followed by a cue indicating whether participants had to imagine the first or the second stimulus. They then imagined the cued stimulus as vividly as possible and indicated their experienced imagery vividness. There were eight exemplars per stimulus category which were chosen to be highly similar to minimise variation in low-level details within categories and to maximise between-category differences. To ensure that participants were generating detailed mental images, we included catch trials during which participants had to indicate which of four highly similar exemplars they had just imagined. An accuracy of 89.9% (<italic>SD</italic> = 5.4%) indicated that participants did indeed imagine the stimuli with a high degree of visual detail.</p><sec id="s2-1"><title>Inferring information flow using perceptual feed-forward classifier models</title><p>Classifier models representing neural representations at different time points during perception were obtained using linear discriminant analysis (LDA). An LDA classifier was trained to decode the stimulus class ('face' vs 'house') from sensor level activity at each time point, giving different perception models for different time points (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). We decided to only focus on the time period between 70 ms and 130 ms after stimulus onset, which, with a sampling rate of 300 Hz, contained 19 perception models. Perceptual content could be decoded significantly better than chance level at 70 ms after stimulus onset (<xref ref-type="bibr" rid="bib10">Dijkstra et al., 2018</xref>). This is in line with intracranial and MEG studies which showed that visual information is detectable in early visual cortex from 50 ms onwards (<xref ref-type="bibr" rid="bib56">Thorpe and Fabre-Thorpe, 2001</xref>; <xref ref-type="bibr" rid="bib47">Ramkumar et al., 2013</xref>). Furthermore, both intracranial as well as scalp electrophysiology has shown that around 130 ms, high-level object representations first get activated (<xref ref-type="bibr" rid="bib26">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Maunsell and Newsome, 1987</xref>; <xref ref-type="bibr" rid="bib51">Seeliger et al., 2018</xref>; <xref ref-type="bibr" rid="bib60">Vogels and Orban, 1996</xref>). Therefore, this early time window is representative of the feed-forward sweep during perception. In line with this, source reconstruction of the sensor-level activation patterns at different time points shows that stimulus information (i.e. the difference in activation between faces and houses [<xref ref-type="bibr" rid="bib19">Haufe et al., 2014</xref>] spreads from low-level visual areas towards higher-level visual areas during this period (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Furthermore, cross-correlation between the information flow in early visual cortex (EVC) and inferior temporal cortex (IT; <xref ref-type="fig" rid="fig1">Figure 1B</xref>) confirms that stimulus information is available in low-level EVC 26.3 ms (<italic>CI</italic> = 13.1 to 32.9 ms) before it reaches high-level IT.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Inferring information flow using perceptual feed-forward classifier models.</title><p>(<bold>A–B</bold>) Perception models. At each point in time between 70 and 130 ms after stimulus onset, a perception model (classifier) was estimated using Linear Discriminant Analysis (LDA) on the activation patterns over sensors. (<bold>A</bold>) The source-reconstructed difference in activation between faces and houses (i.e. decoding weights or stimulus information) is shown for different time points during perception. (<bold>B</bold>) Stimulus information standardised over time is shown for low-level early visual cortex (EVC: blue) and high-level inferior temporal cortex (IT: green). These data confirm a feed-forward flow during the initial stages of perception. (<bold>C</bold>) Imagery reactivation. For each trial and time point during imagery, the distance to the perceptual hyperplane of each perception model is calculated. (<bold>D</bold>) To remove high-frequency noise, a low-pass filter is applied to the distance measured. (<bold>E</bold>) The timing of the reactivation of each perception model during imagery is determined by finding the peak distance for each trial. (<bold>F</bold>) Hypothesised results. This procedure results in a measure of the imagery reactivation time for each trial, for each perception model time point. If perception models are reactivated in the same order during imagery, there would be a positive relation between reactivation imagery time and perception model time. If instead, perception models are reactivated in reverse order, there would be a negative relation. Source data associated with this figure can be found in the <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref> and <xref ref-type="supplementary-material" rid="fig1sdata2">2</xref>.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Source data for panel A.</title><p>Cfg = a configuration structure with the analysis options. Source = a FieldTrip source-reconstruction structure with the source-reconstructed stimulus information averaged over participants in the field source.avg.pow2. Bnd = the vertices which can be used to plot source-activation as ft_plot_mesh(bnd,. . . ).</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig1-data1-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig1sdata2"><label>Figure 1—source data 2.</label><caption><title>Source data for panel B.</title><p>Activation = subject x area x time source-reconstructed stimulus information.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig1-data2-v1.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53588-fig1-v1.tif"/></fig><p>To identify when these representations were reactivated during imagery, we tested the perception models on imagery to obtain the distances to the classifier hyperplanes per trial (<xref ref-type="fig" rid="fig1">Figure 1C–E</xref>). The distance to the hyperplane indicates the amount of classifier evidence present in the data. Distance measures have previously been used as a measure of model activation (<xref ref-type="bibr" rid="bib27">Kerrén et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Linde-Domingo et al., 2019</xref>) and have been linked to reaction time measurements (<xref ref-type="bibr" rid="bib16">Grootswagers et al., 2018</xref>). For each perception model and for each imagery trial, we identified the time of the absolute peak distance (<xref ref-type="bibr" rid="bib36">Linde-Domingo et al., 2019</xref>). This resulted in a trial-by-trial estimate of the reactivation timing for the different perception models. If processing happens in a similar order during imagery as during perception, we would expect that during imagery, early perception models are reactivated earlier in time than late perception models (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). This would result in a positive relation between perception model time and imagery reactivation time. If instead, processing happens in reverse order, with late, high-level models being active before earlier models, we would see a negative relation between perception model time and imagery model time.</p><p>We tested whether this analysis approach was indeed able to infer the order of reactivation of neural representations using simulations (see Methods; Simulations). The perceptual feedforward sweep was simulated as the sequential activation of five neural representations, operationalised as pseudorandom sensor projections (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). We then tested whether distance measures could successfully identify the order of reactivation in a testing (i.e. imagery) set with either small temporal jitter (SD of the onset between trials = 0.1 s) or large temporal jitter (SD = 0.5 s). First the cross-decoding accuracy is plotted for each combination of training and testing time point, computed by averaging classifier performance over trials per time point (<xref ref-type="fig" rid="fig2">Figure 2B–D</xref>; top panels). For small amounts of temporal jitter, the temporal profile of the decoding accuracy clearly differentiates between same or reversed order of reactivation (compare top-left panel of <xref ref-type="fig" rid="fig2">Figure 2B</xref> and <xref ref-type="fig" rid="fig2">Figure 2C</xref>). However, for larger temporal jitter, which is supposedly the case during mental imagery (<xref ref-type="bibr" rid="bib10">Dijkstra et al., 2018</xref>), the temporal profile of the decoding accuracy smears out over the testing time axis, completely obscuring the order of reactivations. In contrast, the reactivation time defined as the peak distance time point per trial accurately indicates the direction of the relationship with the training sequence, irrespective of the amount of temporal jitter.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Simulations reactivation analysis.</title><p>(<bold>A</bold>) Five different neural representations, activated sequentially over time, were modelled as separate sensor activation patterns. Stimulus information is defined as the difference in activation between the two classes (i.e. what is picked up by the classifier). Results on a testing data-set in which the order of activation was either the same (<bold>B</bold>) or reversed (<bold>C</bold>). Top panels: cross-decoding accuracy obtained by averaging over trials. Bottom panels: reactivation times inferred per trial. Source data associated with this figure can be found in the <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Source data from the simulations.</title><p>jitSD_vals = the SD values used to create the between-trial jitter of the different data sets. models = senors x perception time points x stimulus classes activation patterns. accSO = per jitter value a cell containing nsubjects x perception training time points x imagery testing points decoding accuracy for a dataset in which reactivation happened in the same order. accRO = same for reactivation in reversed order. reactSO = per jitter value a cell containing nsubjects x ntrials x perception training time points reactivation times for a dataset in which reactivation happened in the same order. reactRO = same for reactivation in reversed order.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig2-data1-v1.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53588-fig2-v1.tif"/></fig><p>Our approach has a close relationship with temporal generalisation decoding analysis (<xref ref-type="bibr" rid="bib29">King and Dehaene, 2014</xref>). Temporal generalisation characterises the stability of neural representations over time by training and testing a classifier on different time points of either the same or different conditions, resulting in a time by time decoding accuracy matrix. Above chance accuracy between two different time points indicates that the neural representation of the stimulus at those time points is similar, whereas chance decoding indicates that the representation between two time points is not similar and has therefore changed over time. In our approach we also train and test classifiers at different time points. However, instead of computing the accuracy between different training and testing time-points by averaging over trials per time point, we determine per training time point and per trial, which testing time point is most similar to the training time point, indicated by the time point with the maximum LDA distance in favour of the correct class (<xref ref-type="bibr" rid="bib36">Linde-Domingo et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Kerrén et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Grootswagers et al., 2018</xref>). In other words, our method was targeted at the ordering of the representations, and, critically, did not assume a consistent timing between trials, which has been shown to be a limitation of the temporal generalisation decoding scheme (<xref ref-type="bibr" rid="bib59">Vidaurre et al., 2019</xref>). When applied to different time epochs, our method revealed whether the order of activation of neural representations is the same or reversed while at the same time allowing for variation in the exact onset of reactivation between trials. We re-emphasise that this is especially important in the current context given that the timing of imagery likely substantially differs between trials, which means that the temporal dynamics will be obscured when computing accuracy by averaging over trials per time point (<xref ref-type="bibr" rid="bib10">Dijkstra et al., 2018</xref>).</p></sec><sec id="s2-2"><title>Perceptual feed-forward sweep is reversed during imagery</title><p>The reactivation time during imagery for the different perception models is shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. To test whether there was a significant ordering in the reactivation, we ran a linear mixed-effects model (LMM) with the reactivation time during imagery as dependent variable, the perception model time as fixed predictor, and subject and trial as random variables. Five models with different combinations of random effects were estimated and the model with the highest Schwarz Bayesian Information Criterion (BIC) was used to ensure best-fit with minimum number of predictors (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a</xref>). The winning model contained a random effect for the intercept of each subject and trial. This means that this model allowed the reactivation of the perception sequence to start at different time points per trial and per subject, which is in line with the idea that there is a large variation in timing of imagery between trials and subjects (<xref ref-type="bibr" rid="bib10">Dijkstra et al., 2018</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Imagery reactivation results.</title><p>(<bold>A</bold>) Reactivation time during imagery for every perception model averaged over trials. The shaded area represents the 95% confidence interval. The linear equation shows how imagery reactivation time (I) can be calculated using perception model time (P) in seconds. (<bold>B</bold>) Same results after removing stimulus information by permuting the class-labels (<bold>C–D</bold>) Stimulus information during imagery was estimated by realigning the trials based on the reactivation time points and using the linear equation to estimate the imagery time axis. (<bold>C</bold>) Stimulus information standardised over time for low-level early visual cortex (EVC) and high-level inferior temporal cortex (IT). (<bold>D</bold>) Stimulus information in EVC and IT averaged over time before and after realignment. Stimulus information below 0 indicates that the amount of information did not exceed the permutation distribution. Source data associated with this figure can be found in the <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 2—source data 1</xref>–<xref ref-type="supplementary-material" rid="fig3sdata6">6</xref>.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Contains the reactivation time per perception model and per trial, the subject ID ('S').</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig3-data1-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig3sdata2"><label>Figure 3—source data 2.</label><caption><title>Contains the source-reconstructed stimulus information as subject x source parcel x time points as well as the corresponding time vector and source-parcel names for the realigned data.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig3-data2-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig3sdata3"><label>Figure 3—source data 3.</label><caption><title>Contains the source-reconstructed stimulus information as subject x source parcel x time points as well as the corresponding time vector and source-parcel names for the unaligned data.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig3-data3-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig3sdata4"><label>Figure 3—source data 4.</label><caption><title>Contains the reactivation time per perception model and per trial, the subject ID ('S') for the permuted classifier.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig3-data4-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig3sdata5"><label>Figure 3—source data 5.</label><caption><title>Contains the reactivation time per perception model and per trial, the subject ID ('S') for the unfiltered data.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig3-data5-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig3sdata6"><label>Figure 3—source data 6.</label><caption><title>Contains the reactivation time per perception model and per trial, the subject ID ('S') for the unfiltered and permuted data.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig3-data6-v1.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53588-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Imagery reactivation results without the low-pass filter.</title><p>(<bold>A</bold>) Unpermuted data. (<bold>B</bold>) Permuted data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53588-fig3-figsupp1-v1.tif"/></fig></fig-group><p>The model showed a significant main effect of perception time (<italic>t</italic>(98160) = −5.403, p=6.58e-8) with a negative slope (β0 = 2.05, <italic>SD</italic> = 0.28; β1 = −1.17,<italic>SD</italic> = 0.22) indicating that models of later perception time points were associated with earlier imagery reactivation times. The fact that the absolute slope is so close to one suggests that reactivation during imagery of the perception model sequence happens at a similar speed as the original activation during perception.</p><p>Next, we reconstructed the imagery activation by realigning the trials based on the identified peak time points for each perception model time point. The imagery time line was inferred using the linear equation obtained from the LMM. The temporal dynamics of high-level IT and low-level EVC (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) confirm the conclusion that during imagery, information flows from high-level to low-level visual areas. Cross-correlation between these realigned signals shows that information in IT precedes information in EVC by 11.2 ms (<italic>CI</italic> = 0 to 29.9 ms). Furthermore, before realignment, time-locked decoding during imagery only revealed a small amount of information in high-level visual cortex (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). In contrast, after realignment, stimulus information was also clearly present in EVC. This emphasises how time-locked analyses obscure neural processing during complex cognitive processes such as mental imagery.</p><p>To ensure that these results were not due to confounds in the structure of the data irrelevant to reactivation of stimulus representations, we performed the same analysis after permuting the class-labels of the trials, erasing stimulus information while keeping the temporal structure of the data unaltered. Specifically, we trained the perception models using random class assignments and then again calculated the reactivations during imagery. The results are shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. The sequential reactivation disappeared when using shuffled classifiers as perception time did not significantly predict imagery reactivation time anymore (<italic>t</italic>(98160) = −0.762, p=0.446). Furthermore, for the main analysis, we removed high frequency noise from the imagery distance traces. To check whether this filter somehow altered the results, we ran the same analysis without the low-pass filter, which gave highly similar results (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Without the filter, there was still a significant main effect of perception time (<italic>t</italic>(98160) = −4.153, p=0.0000328) where models of later perception time points were associated with earlier imagery reactivation times (<inline-formula><mml:math id="inf1"><mml:mi>β</mml:mi></mml:math></inline-formula>=−0.8942, SD = 0.2153) and the results of the shuffled classifier remained non-significant. Finally, because we used cross-validation within subjects to calculate reactivation timing, there is a dependence between trials of the same subject, violating the independence assumption of first-order statistical tests. As a sanity check, we performed a second test that did not require splitting trials: we used LMM models with reactivation times averaged over trials within subjects, and only 'subject' as a random variable. In this case, the model containing both a random effect of intercept as well as slope per subject best explained the data (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1c</xref>). This between-subject model still showed a significant main effect of perception time (<italic>t</italic>(24) = −3.24, p=0.003) with a negative slope (β0 = 2.05, <italic>SD</italic> = 0.03, β1 = −1.19, <italic>SD</italic> = 0.37) confirming that the effect was not dependent on between-trial statistics.</p></sec><sec id="s2-3"><title>Reactivation during imagery reveals recurrent processing during later stages of perception</title><p>In the previous analysis, we focused on the first 150 ms after stimulus presentation because this period reflects the initial perceptual feed-forward sweep. A negative relationship with imagery reactivation time therefore indicated feedback processing during imagery (<xref ref-type="fig" rid="fig1">Figure 1</xref>). However, feedback processes are assumed to play a fundamental role in later stages of perception (<xref ref-type="bibr" rid="bib34">Lamme and Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="bib46">Pennartz et al., 2019</xref>). For these later stages of perception, we would therefore expect a positive relation with imagery reactivation, indicating that information flows in the same direction. To investigate this, we next calculated the imagery reactivation time for all time points during perception (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The results between 70 ms and 130 ms are equivalent to <xref ref-type="fig" rid="fig3">Figure 3A</xref>.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Reactivation timing during imagery for classifiers trained at all perception time points.</title><p>(<bold>A</bold>) Imagery reactivation time for perception models trained on all time points. On the x-axis the training time point during perception is shown and on the y-axis the reactivation time during imagery is shown. The dots represent the mean over trials for individual time points and the shaded area represents the 95% confidence interval. (<bold>B</bold>) Left: time-frequency decomposition using a Morlet wavelet at 10 Hz. Right: power at different frequencies using a Fast Fourier Transformation. The purple line represents the true data and the grey line represents the results from the shuffled classifier. Shaded areas represent 95% confidence intervals over trials. Red lines indicate time points for which the true and shuffled curve differed significantly (FDR corrected). (<bold>C</bold>) Intrinsic mode function and its extrema derived from the reactivation traces using empirical model decomposition. (<bold>D</bold>) Coherence between reactivation trace and raw signal. Left coherence values, right log10 p values. Source data associated with this figure can be found in the <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>–<xref ref-type="supplementary-material" rid="fig4sdata3">3</xref>.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Contains the identified reactivation trace extrema based on EMD of the empirical traces.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig4-data1-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig4sdata2"><label>Figure 4—source data 2.</label><caption><title>Contains the reactivation traces as reactivation sample point per trial x perception model time for the unpermuted classifiers.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig4-data2-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig4sdata3"><label>Figure 4—source data 3.</label><caption><title>Contains the reactivation traces as reactivation sample point per trial x perception model time for the permuted classifiers.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig4-data3-v1.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53588-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Reactivation timing during imagery for all perception time points without low-pass filter.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53588-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Dynamics of the raw signal during perception for occipital channels.</title><p>Top panels: per channel. Bottom panels: averaged over channels. Shaded area represents the 95% CI over trials. (<bold>A</bold>) Raw signal amplitude. (<bold>B</bold>) Frequency decomposition. (<bold>C</bold>) Auto-correlation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53588-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Imagery reactivation traces for the full perception period separately for each subject.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53588-fig4-figsupp3-v1.tif"/></fig></fig-group><p>Interestingly, during the first 400 ms of perception, there seems to be an oscillatory pattern in the relationship between perception time and imagery reactivation time, where positive and negative slopes alternate. This suggests that during perception, the direction of information flow alternates. This pattern repeats four times in 400 ms, roughly reflecting an alpha oscillation. To investigate this further, we quantified 10 Hz power over time using a Morlet decomposition (<xref ref-type="fig" rid="fig4">Figure 4B</xref> left, purple curve) and compared the results with the permuted classifier (<xref ref-type="fig" rid="fig4">Figure 4B</xref> left, grey curve). There was a significant increase in 10 Hz power between 80 and 315 ms after stimulus onset during perception (all FDR corrected <italic>p</italic>-values below 0.003). Furthermore, a Fast Fourier Transform over the first 400 ms revealed that the difference in power was limited to the 11.2 Hz frequency (<xref ref-type="fig" rid="fig4">Figure 4B</xref> right, p=0.001).</p><p>These results indicate that, during perception, stimulus information travels up and down the visual hierarchy aligned to the alpha frequency. To make sure this effect was not due to the low-pass filtering, we applied the same analysis on the unfiltered data, giving almost identical results (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). We next aimed to parse the signal into feed-forward and feedback sweeps. To this end, we used empirical mode decomposition (EMD) over the first 400 ms to separate the signal in intrinsic mode functions (IMFs). We selected identified increasing and decreasing phases by selecting the period between two subsequent extrema (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) and calculated the slope for each phase (single estimated slopes are shown in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). The average decreasing (feed-forward) slope was −0.69 (<italic>CI</italic> = −2.05 to −0.35) and the average increasing (feedback) slope was 1.02 (<italic>CI</italic> = −0.31 to 2.14; <xref ref-type="fig" rid="fig4">Figure 4D</xref>). There was no significant difference in the absolute slope value between increasing and decreasing phases (<italic>M</italic>diff = 0.33, <italic>CI</italic>: −1.80 to 1.74), revealing no evidence for a difference in processing speed for feed-forward and feedback sweeps.</p><p>It might be possible that the oscillation in the reactivation trace is caused by evoked alpha oscillations in the raw signal which modulates signal-to-noise ratio via its amplitude. As can be seen in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>, there is indeed a clear evoked alpha oscillation present in the raw signal. To investigate whether the oscillation of the reactivation trace was related to the oscillation in the raw signal, we calculated the spectral coherence at the peak frequency (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) between the reactivation trace and the raw signal at each sensor. We then compared the coherence between the raw data and the true activation reactivation with the coherence between the raw data and the reactivation of the shuffled classifier using a permutation test with 10000 permutations. None of the sensors showed a significant difference in coherence between the true and permuted classifier (FDR corrected; <xref ref-type="fig" rid="fig4">Figure 4D</xref>), even though the reactivation was only present for the true classifier ( <xref ref-type="fig" rid="fig4">Figure 4B</xref>). Furthermore, the topography of the coherence values appears very unstructured (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). Together, this suggests that the oscillatory pattern in the reactivated trace does not merely reflect evoked alpha in the raw signal. However, future research is necessary to fully characterise the relationship between the reactivation dynamics and the raw signal dynamics.</p></sec><sec id="s2-4"><title>Stimulus representations are iteratively updated during perception</title><p>If there is indeed a recurrent information flow up and down the visual hierarchy during perception, we should also be able to demonstrate this within perception. The previous results suggest specific time windows of feed-forward and feedback phases during perception. To test whether these different phases indeed reflected reversals of information flow, we applied the reactivation analysis previously used for imagery (<xref ref-type="fig" rid="fig1">Figure 1</xref>) to the different perception phases identified in the previous analysis (<xref ref-type="fig" rid="fig4">Figure 4</xref>). We predicted that perception classifiers trained during a decreasing (feed-forward) phase would be reactivated in reverse order during an increasing (feedback) phase and vice versa, showing a negative relationship between training time and reactivation time. In contrast, classifiers trained and tested on the same type of phase (both decreasing or both increasing), should show a positive relationship (see <xref ref-type="fig" rid="fig5">Figure 5B</xref>, for hypothesised results).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Reactivation timing for different perception phases.</title><p>(<bold>A</bold>) The reactivation traces for each testing phase. Blue traces reflect feed-forward phases, pink traces reflect feedback phases (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) and grey traces reflect reactivation traces for permuted classifiers. Shaded area represents the 95% confidence interval over trials. (<bold>B</bold>) Hypothesised (top) and empirical (bottom) slopes between the training and testing phases. The hypothesised matrix assumes recurrent processing such that subsequent phases show a reversal in the direction of information flow. Recurrence index reflects the amount of recurrent processing in the data, which is quantified as the dot product between the vectorised hypothesis matrix and empirical matrix. (<bold>C</bold>) Recurrence index for the permuted classifier, phase specification based on the IMF of the imagery reactivation trace (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) and phase specification on evoked oscillations at various frequencies. (<bold>D</bold>) Slope matrix for phase specification defined at 10 Hz over the entire stimulus period. Source data associated with this figure can be found in the <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>–<xref ref-type="supplementary-material" rid="fig5sdata8">8</xref>.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Contains, for the segmentation of 3 Hz, Cfg = a configuration structure with the analysis options.</title><p>S = per trial, the subject ID. L = for testing x training combination, the sample (time point) of maximum reactivation per trial. Btstrp = bootstrapping samples over trials per phase, per time point. Osc_btstrp = recurrence index calculated for each bootstrapping sample.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig5-data1-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig5sdata2"><label>Figure 5—source data 2.</label><caption><title>Contains, for the segmentation of 5 Hz, Cfg = a configuration structure with the analysis options.</title><p>S = per trial, the subject ID. L = for testing x training combination, the sample (time point) of maximum reactivation per trial. Btstrp = bootstrapping samples over trials per phase, per time point. Osc_btstrp = recurrence index calculated for each bootstrapping sample.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig5-data2-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig5sdata3"><label>Figure 5—source data 3.</label><caption><title>Contains, for the segmentation of 10 Hz, Cfg = a configuration structure with the analysis options.</title><p>S = per trial, the subject ID. L = for testing x training combination, the sample (time point) of maximum reactivation per trial. Btstrp = bootstrapping samples over trials per phase, per time point. Osc_btstrp = recurrence index calculated for each bootstrapping sample.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig5-data3-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig5sdata4"><label>Figure 5—source data 4.</label><caption><title>Contains, for the segmentation of 25 Hz, Cfg = a configuration structure with the analysis options.</title><p>S = per trial, the subject ID. L = for testing x training combination, the sample (time point) of maximum reactivation per trial. Btstrp = bootstrapping samples over trials per phase, per time point. Osc_btstrp = recurrence index calculated for each bootstrapping sample.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig5-data4-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig5sdata5"><label>Figure 5—source data 5.</label><caption><title>Contains, for the segmentation of 30 Hz, Cfg = a configuration structure with the analysis options.</title><p>S = per trial, the subject ID. L = for testing x training combination, the sample (time point) of maximum reactivation per trial. Btstrp = bootstrapping samples over trials per phase, per time point. Osc_btstrp = recurrence index calculated for each bootstrapping sample.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig5-data5-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig5sdata6"><label>Figure 5—source data 6.</label><caption><title>Contains, for the segmentation of 37.5 Hz, Cfg = a configuration structure with the analysis options.</title><p>S = per trial, the subject ID. L = for testing x training combination, the sample (time point) of maximum reactivation per trial. Btstrp = bootstrapping samples over trials per phase, per time point. Osc_btstrp = recurrence index calculated for each bootstrapping sample.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig5-data6-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig5sdata7"><label>Figure 5—source data 7.</label><caption><title>Contains, for the segmentation of 50 Hz, Cfg = a configuration structure with the analysis options.</title><p>S = per trial, the subject ID. L = for testing x training combination, the sample (time point) of maximum reactivation per trial. Btstrp = bootstrapping samples over trials per phase, per time point. Osc_btstrp = recurrence index calculated for each bootstrapping sample.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig5-data7-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig5sdata8"><label>Figure 5—source data 8.</label><caption><title>Contains, for the segmentation of 75 Hz, Cfg = a configuration structure with the analysis options.</title><p>S = per trial, the subject ID. L = for testing x training combination, the sample (time point) of maximum reactivation per trial. Btstrp = bootstrapping samples over trials per phase, per time point. Osc_btstrp = recurrence index calculated for each bootstrapping sample.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-53588-fig5-data8-v1.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53588-fig5-v1.tif"/></fig><p>The reactivation traces for the different test phases are shown in <xref ref-type="fig" rid="fig5">Figure 5A</xref>. Blue traces represent reactivations during feed-forward phases and pink traces represent reactivations during feedback phases. Grey traces show the results for a permuted classifier. In line with the previous findings, for most phases, there is a clear oscillatory pattern between training time and reactivation time within perception. For each phase, the training time corresponding to that testing phase is highlighted in bold. This time period should always show a positive slope, indicating that classifiers trained and tested on time points belonging to the same phase are reactivated in the same order.</p><p>The slopes between the training time and the reactivation time of the different phases are shown below the hypothesised results in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. As expected, the diagonal, reflecting training and testing on the same phase, was always positive. In contrast, training and testing on different phases tended to be associated with negative slopes. Note that for all decoding analyses, cross-validation was used, which means that these results cannot be due to overfitting but reflect true representational overlap between phases. To quantify the effect, we calculated a <italic>Recurrence Index</italic> (RI) which was defined as the dot-product between the vectorised hypothesis-matrix and the empirical-matrix. The RI is positive if the data show the hypothesised oscillatory pattern of slope reversals, zero if there is no clear oscillatory pattern and negative if the data show the opposite pattern. The RI was significantly larger than zero for the true data (RI = 1.83, <italic>CI</italic> = 1.71 to 1.94, <italic>p</italic> &lt; 0.0001) but not for the permuted data (RI = −0.008, <italic>CI</italic> = −0.12 to 0.10, p=0.548). This confirms that during perception, stimulus information flows up and down the visual hierarchy in feedback and feed-forward phases.</p><p>The phases that we used here were identified based on the IMF of the oscillation in the imagery reactivation trace (<xref ref-type="fig" rid="fig4">Figure 4</xref>), leading to phases of different lengths. Next, we investigated whether we could observe the same pattern if we specified the phases based on fixed evoked oscillations at different frequencies. For example, a 10 Hz oscillation resulted in four feed-forward and four feedback phases of 50 ms each within our 400 millisecond time window. The RI for the different frequencies is plotted in <xref ref-type="fig" rid="fig5">Figure 5C</xref>. Whereas the RI was significantly above zero for several low frequencies, the oscillatory pattern was clearest for the IMF based phases and for the 10 Hz oscillation, confirming that the perceptual recurrence is most strongly aligned to the alpha frequency. Furthermore, to investigate whether this pattern of recurrence was indeed specific to the 400 millisecond time window identified previously, we also applied the 10 Hz recurrence analysis to the entire stimulus period (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). The results show that the recurrence pattern is indeed restricted the first 400 ms after stimulus onset.</p><p>An interesting observation is that the recurrence pattern seems to be restricted to around the testing phase, such that only classifiers trained on phases close to the testing phase show a clear positive or negative relation with reactivation time. This is what causes the 'traveling wave' pattern between the rows in <xref ref-type="fig" rid="fig5">Figure 5A</xref>. An intriguing explanation for this observation is that stimulus representations change over subsequent cycles, such that representations only show a reactivation relation with neighbouring phases, but that the representations during later phases are too dissimilar to result in reliable reactivations. We tested whether recurrence was indeed specific to phases around the testing phase by comparing the normalised RI of slopes next to the diagonal in the slope matrix with the other slopes. Neighbouring phases indeed show a significantly higher RI (<italic>M</italic> = 0.073, <italic>CI</italic> = 0.069 to 0.076) than other phases (<italic>M</italic> = 0.002, <italic>CI</italic> = −0.0004 to 0.005, <italic>p</italic> &lt; 0.0001), confirming that recurrence was restricted to a small number of cycles.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we investigated how stimulus representations flow through the visual system during mental imagery and perception. Our results reveal an asymmetry in information processing between perception and imagery. First, we showed that early perception processes are reactivated in reverse order during imagery, demonstrating that during imagery, activation flows from high-level visual areas down to low-level areas over time. Second, for later stages of perception, we found an oscillatory pattern of alternating positive and negative relations with imagery reactivation, indicating recurrent stimulus processing up and down the visual hierarchy aligned to an 11 Hz oscillation. Finally, by focusing on the identified feed-forward and feedback phases, we showed that recurrence during perception was restricted to neighbouring phases, suggesting that the format of neural stimulus representations changed with subsequent cycles of recurrent processing. Together, these findings indicate that during imagery, stimulus representations are activated via feedback processing whereas during perception, stimulus representations are iteratively updated through cycles of recurrent processing.</p><p>Our results are neatly in line with predictive processing (PP) theories. According to PP, recurrent processing during perception reflects dynamic hypothesis testing (<xref ref-type="bibr" rid="bib4">Bastos et al., 2012</xref>; <xref ref-type="bibr" rid="bib13">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib28">Kersten et al., 2004</xref>; <xref ref-type="bibr" rid="bib30">Knill and Pouget, 2004</xref>). Specifically, perceptual inference is assumed to be accomplished via an interplay between top-down prediction signals encoding perceptual hypotheses and bottom-up prediction errors encoding the sensory signal unexplained by these hypotheses. Inferring the cause of sensory input is done by iteratively updating the perceptual hypothesis until the prediction error is minimised, in line with the dynamically changing representations observed here. Importantly, recurrent processing is assumed to happen hierarchically such that each level is activated by both bottom-up evidence as well as top-down predictions (<xref ref-type="bibr" rid="bib13">Friston, 2005</xref>). This is in line with our observation that feed-forward and feedback sweeps proceeded at the same speed. Also in line with the current findings, PP predicts that these recurrent dynamics are dominated by slow-wave oscillations (<xref ref-type="bibr" rid="bib4">Bastos et al., 2012</xref>). To our knowledge, the current study is the first to show these perceptual updating cycles empirically in humans. Furthermore, our results suggest that in the current task context, the perceptual inference process was completed in approximately four updating cycles. An exciting avenue for future research is to investigate whether the number of cycles needed can be modulated by task variables such as attention and stimulus noise.</p><p>Whereas perception was characterised by dynamically changing representations updated through recurrent cycles, we only found evidence for a single feedback flow during imagery. Specifically, all perceptual feed-forward sweeps showed a negative relationship with the same imagery time window and all perceptual feedback sweeps showed a negative relationship with that same imagery time window. This suggests that the imagery feedback flow contained the complete stimulus representation that was inferred via recurrent cycles during perception. This result fits with the idea that imagery uses the same predictive processes that underlie perceptual inference to run off-line simulations of sensory representations under different hypotheses (<xref ref-type="bibr" rid="bib11">Dijkstra et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Gershman, 2019</xref>; <xref ref-type="bibr" rid="bib17">Grush, 2004</xref>; <xref ref-type="bibr" rid="bib21">Hobson and Friston, 2012</xref>; <xref ref-type="bibr" rid="bib42">Moulton and Kosslyn, 2009</xref>). In contrast to perception, during imagery, there is no bottom-up sensory input prompting hypothesis updating. Instead, the perceptual cause is given and feedback connections are used to generate the corresponding low-level sensory representation based on the mapping that was learned during perception. Recurrent processing within the visual system might become important when imagining a more dynamic stimulus in which sensory representations change over time. However, it is also possible that recurrent dynamics were actually present during imagery in this task but that we were unable to reveal them due to signal-to-noise issues. Future research should focus on developing more sensitive techniques to further characterise information flow during imagery. Another interesting question for future research is whether reactivation during mental imagery has to always fully progress down the visual hierarchy. In this study, participants were instructed to generate highly detailed mental images and catch trials were used to ensure that they indeed focused on low-level visual details. It might be the case that if less detail is needed for the task, earlier perception processes are not reactivated and mental simulations stop at a higher level (<xref ref-type="bibr" rid="bib33">Kosslyn and Thompson, 2003</xref>; <xref ref-type="bibr" rid="bib45">Pearson and Keogh, 2019</xref>).</p><p>Central to this predictive processing interpretation of bottom-up and top-down sweeps is that increasingly abstract stimulus features are processed in higher-level brain areas. This is indeed what has generally been observed in the literature (<xref ref-type="bibr" rid="bib24">Hubel and Wiesel, 1968</xref>; <xref ref-type="bibr" rid="bib56">Thorpe and Fabre-Thorpe, 2001</xref>; <xref ref-type="bibr" rid="bib60">Vogels and Orban, 1996</xref>). However, because we have not directly assessed which stimulus features were captured by the classifiers in this study, we cannot be certain that the sweeps through the visual hierarchy observed here genuinely reflect processing of stimulus information at different levels of abstraction. For example, it is possible for the classifier to be driven by other features of the signal irrelevant to processing of visual information such as general amplification of signal due to differences in attentional capture between the two stimuli, which might have also given rise to sweeps through the visual hierarchy (<xref ref-type="bibr" rid="bib40">Michalareas et al., 2016</xref>). It is likely that such high-level cognitive mechanisms have influenced processing during later stages of perception. However, a recent study showed that the initial feedforward sweep during perception does not seem to be influenced by attention (<xref ref-type="bibr" rid="bib3">Alilović et al., 2019</xref>). Moreover, modelling work using Convolutional Neural Networks (CCNs) has shown that the MEG signal during perception does reflect activation of hierarchically increasing complex features over time (<xref ref-type="bibr" rid="bib51">Seeliger et al., 2018</xref>). Further research using explicit encoding models of stimulus features at different levels of abstraction would be necessary to completely address this point. Furthermore, the current study used a number of non-traditional analysis steps. While we aimed to demonstrate the validity of this approach via simulations, it is worth noting that simulations are not a perfect control since simulated data cannot account for all the features present in real data, and might be blind to other issues. Therefore, to fully ensure that this analysis approach does not suffer from any overlooked confounds, future validation studies are needed.</p><p>In conclusion, by using a novel multivariate decoding approach which allowed us to infer the order in which representations were reactivated, we show that, while similar neural representations are activated during imagery and perception, the neural dynamics underlying this activation are different. Whereas perception is characterised by recurrent processing, imagery is dominated by top-down feedback processing. These results are in line with the idea that during perception, high-level causes of sensory input are inferred whereas during imagery, this inferred mapping is reversed to generate sensory representations given these causes. This highlights a fundamental asymmetry in information processing between perception and imagery and sets the stage for exciting new avenues for future research.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>We assumed a medium effect size (d = 0.6) which, to reach a power of 0.8, required twenty four participants. To take into account drop-out, thirty human volunteers with normal or corrected-to-normal vision gave written informed consent and participated in the study. Five participants were excluded: two because of movement in the scanner (movement exceeded 15 mm), two due to incorrect execution of the task (less than 50% correct on the catch trials, as described below) and one due to technical problems. Twenty-five participants (mean age 28.6, <italic>SD</italic> = 7.62) remained for the final analysis. The study was approved by the local ethics committee and conducted according to the corresponding ethical guidelines (CMO Arnhem-Nijmegen). An initial analysis of these data has been published previously (<xref ref-type="bibr" rid="bib10">Dijkstra et al., 2018</xref>).</p></sec><sec id="s4-2"><title>Experimental design</title><p>We adapted a retro-cue paradigm in which the cue was orthogonalised with respect to the stimulus identity (<xref ref-type="bibr" rid="bib18">Harrison and Tong, 2009</xref>). Participants were shown two images after each other followed by a retro-cue indicating which of the images had to be imagined. After the cue, a frame was shown in which the participants had to imagine the cued stimulus as vividly as possible. Next, they had to indicate their experienced imagery vividness by moving a bar on a continuous scale. To ensure that participants were imagining the stimuli with great visual detail, both categories contained eight exemplars, and on 7% of the trials the participants had to indicate which of four exemplars they imagined. The exemplars were chosen to be highly similar in terms of low-level features to minimise within-class variability and increase between-class classification performance. We instructed participants to focus on vividness and not on correctness of the stimulus, to motivate them to generate a mental image including all visual features of the stimulus. The stimuli encompassed 2.7 × 2.7 visual degrees. A fixation bull's-eye with a diameter of 0.1 visual degree remained on screen throughout the trial, except during the vividness rating.</p></sec><sec id="s4-3"><title>MEG recording and preprocessing</title><p>Data were recorded at 1200 Hz using a 275-channel MEG system with axial gradiometers (VSM/CTF Systems, Coquitlam, BC, Canada). For technical reasons, data from five sensors (MRF66, MLC11, MLC32, MLF62, MLO33) were not recorded. Subjects were seated upright in a magnetically shielded room. Head position was measured using three coils: one in each ear and one on the nasion. Throughout the experiment head motion was monitored using a real-time head localiser (<xref ref-type="bibr" rid="bib54">Stolk et al., 2013</xref>). If necessary, the experimenter instructed the participant back to the initial head position during the breaks. This way, head movement was kept below 8 mm in most participants. Furthermore, both horizontal and vertical electro-oculograms (EOGs), as well as an electrocardiogram (ECG) were recorded for subsequent offline removal of eye- and heart-related artefacts. Eye position and pupil size were also measured for control analyses using an Eye Link 1000 Eye tracker (SR Research). Data were analysed with MATLAB version R2018a and FieldTrip (<xref ref-type="bibr" rid="bib43">Oostenveld et al., 2011</xref>) (RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_004849">SCR_004849</ext-link>). Per trial, three events were defined. The first event was defined as 200 ms prior to onset of the first image until 200 ms after the offset of the first image. The second event was defined similarly for the second image. Further analyses focused only on the first event, because the neural response to the second image is contaminated by the neural response to the first image. Finally, the third event was defined as 200 ms prior to the onset of the retro-cue until 500 ms after the offset of the imagery frame. As a baseline correction, for each event, the activity during 300 ms from the onset of the initial fixation of that trial was averaged per channel and subtracted from the corresponding signals. The data were down-sampled to 300 Hz to reduce memory and CPU load. Line noise at 50 Hz was removed from the data using a DFT notch filter. To identify artefacts, the variance of each trial was calculated. Trials with high variance were visually inspected and removed if they contained excessive artefacts. After artefact rejection, on average 108 perception face trials, 107 perception house trials, 105 imagery face trials and 106 imagery house trials remained for analysis. To remove eye movement and heart rate artefacts, independent components of the MEG data were calculated and correlated with the EOG and ECG signals. Components with high correlations were manually inspected before removal. The eye tracker data were cleaned separately by inspecting trials with high variance and removing them if they contained blinks or other excessive artefacts.</p></sec><sec id="s4-4"><title>Reactivation timing analysis</title><p>To estimate when neural representations at different perception time points got reactivated during imagery, we trained classifiers on several time points during perception and applied them to imagery. Specifically, to decode the stimulus category per perception time point, we used a linear discriminant analysis (LDA) classifier with the activity from the 270 MEG sensors as features and a shrinkage regularisation parameter of 0.05 (see [<xref ref-type="bibr" rid="bib41">Mostert et al., 2016</xref>] for more details). To prevent a potential bias in the classifier, the number of trials per class was balanced per fold by randomly removing trials from the class with the most trials until the trial numbers were equal between the classes. In our design, the perception and imagery epochs happened in the same general 'trial'. If the imagery epochs on which the classifier was tested came from the same trials as the perception epochs on which it was trained, auto-correlations in the signal could inflate decoding accuracy. To circumvent this, a five-fold cross-validation procedure was implemented where for each fold the classifier was trained on 80% of the trials and tested on the other 20%.</p><p>Because the amplitude of the signal is usually smaller during imagery than during perception (<xref ref-type="bibr" rid="bib32">Kosslyn et al., 2001</xref>; <xref ref-type="bibr" rid="bib45">Pearson and Keogh, 2019</xref>), we demeaned the data by subtracting the mean over trials per time point and per sensor. This forced the classifier to leverage relative changes in the multivariate signal while avoiding the confounding effect of having global changes in amplitude.</p><p>This resulted in a decision value for each perception model, for each imagery trial and time point. The sign of this value indicates in which category the trial has been classified and the value indicates the distance to the hyperplane. In order to obtain the confidence of the classifier in the correct class, the sign of the distance values in one category was inverted. This means that increasing positive distance values now always reflected increasingly confident classification. To obtain the specific moment within each imagery trial that a given perception model became active, we identified the time point with the highest distance (<xref ref-type="bibr" rid="bib36">Linde-Domingo et al., 2019</xref>; <xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>Identifying the order of reactivation of neural representations was then done by performing a linear regression with the training time points as predictor and the inferred reactivation times points as dependent variable. A positive relationship indicates that reactivations happened in the same order, whereas a negative relationship indicates a reversal in the order of reactivations.</p><p>After discovering the oscillatory pattern between later perception time points and imagery reactivation, we also ran this reactivation analysis within perception by training on all perception time points and testing on specific perception time windows reflecting the identified feed-forward and feedback phases (<xref ref-type="fig" rid="fig4">Figure 4</xref>). This identified, per phase, reversals in information flow.</p><p>For the imagery generalisation, the time window used to obtain the peak distance extended from the cue onset until the vividness instruction onset, covering the entire 4 s during which participants were instructed to imagine the stimulus. For this data, we removed high frequency noise using a low-pass filter of 30 Hz (for the results using the raw data, see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). For the within-perception tests we did not use a low-pass filter because we used smaller testing time windows and were also interested in possible high-frequency effects.</p><p>To ensure that the observed effects were due to dynamics in activation of neural stimulus representations, and not due to dynamics of the raw signals, for each analysis, we performed the same analysis after permuting the class-labels, thereby removing stimulus information from the data without altering the temporal structure of the data.</p></sec><sec id="s4-5"><title>Simulations</title><p>We tested the validity of our approach on a relatively realistic synthetic dataset. Specifically, we tested whether the trial-by-trial LDA distance measures could successfully be used to infer the order of reactivation of neural representations, even when the timing of these reactivations differed between trials. The true neural model consisted of 5 neural representations activated in a partially overlapping sequence over the course of 60 milliseconds, with a sampling rate of 300 Hz, modelled as pseudo random activation of 20 sensors for two classes (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Stimulus information is defined as the difference between the two classes. The training set was generated as 100 trials per class of the true neural model activation. Testing sets were generated as 100 trials per class of the true neural model activation in the same order or in reversed order as the training set and slowed down by a factor of 10. Furthermore, temporal uncertainty between trials was introduced by randomly sampling the onset of the reactivation in each trial from a standard normal distribution with mean 0.8 s and a standard deviation of 0.1 or 0.5. Testing set trials were 2 s long with a sampling frequency of 300 Hz. This stimulus-specific activity was added on top of some generated ongoing activity, which we simulated as realistic 1/f noise. In order to do that, we fitted a multivariate autoregressive model of order 1 to the real data, and then sampled from it trial by trial (<xref ref-type="bibr" rid="bib59">Vidaurre et al., 2019</xref>). We simulated 10 subjects and performed LDA cross-decoding analysis as described above in each subject separately resulting in distance measures per testing trial and time point. Decoding accuracy was computed for each combination of training and testing time point as the number of trials in which the distance was highest for the correct class divided by the total number of trials (i.e. proportion of correctly classified trials). Reactivation time was computed for each training time point and for each trial as the testing time with the peak distance in favour of the correct class (<xref ref-type="bibr" rid="bib36">Linde-Domingo et al., 2019</xref>).</p></sec><sec id="s4-6"><title>Realignment</title><p>To confirm that the order of activation of low and high-visual areas was reversed during imagery compared to perception, we realigned the imagery activation based on the identified distance peaks. This was done by selecting the peak imagery time point for every trial for every perception model time point to create a realigned imagery data set. The time axis for this new data set was inferred using the linear relationship between perception model time and imagery reactivation time established in the main imagery reactivation analysis. Source activation was then calculated using the same procedure as was used for the un-realigned perception and imagery data (more details below under Source localisation).</p></sec><sec id="s4-7"><title>Frequency analysis</title><p>For the time frequency analysis, we used a Morlet Wavelet at 10 Hz defined as:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf2"><mml:mi>t</mml:mi></mml:math></inline-formula> is a time vector from −0.5 to 0.5 in steps of 1/fs, <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mi>c</mml:mi></mml:math></inline-formula> is the number of cycles, this case 1. We only used 200 samples in the centre of the wavelet and convolved this with the mean reactivation trace to obtain the time frequency representation. To calculate power at different frequencies we used the Fast Fourier Transform (FFT). To prevent edge effects, we first multiplied the mean signal with a Hanning taper from −0.2 to 0.6 s prior to performing the FFT. Of the resulting complex numbers, the absolute value was taken and the result was normalised by the length of the signal.</p><p>Within reactivation traces, positive slopes represented reactivation in a similar order whereas negative slopes represented reactivation in reverse order. We wanted to divide the signal into phases of positive and negative slopes because these represented feed-forward and feedback phases. In order to do this, we used empirical mode decomposition (EMD) which separates the signal into intrinsic mode functions (IMF) based on local and global extrema; that is, peaks and troughs (<xref ref-type="bibr" rid="bib23">Huang et al., 1998</xref>; <xref ref-type="bibr" rid="bib50">Rilling et al., 2003</xref>; <xref ref-type="bibr" rid="bib61">Wang et al., 2010</xref>). This technique identifies oscillations without assuming that these oscillations should be sinusoidal. For a 10 Hz frequency we would expect eight extrema in 400 ms, reflecting four full cycles. Therefore, to identify the different phases, we selected the IMF with the number of extrema closest to eight. Decreasing and increasing phases were defined as periods between subsequent extrema (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Slopes between periods were calculated using linear regression and slopes for decreasing and increasing periods were averaged to reflect the speed of feed-forward and feedback processing respectively. To determine the uncertainty of these slopes, for every bootstrapping sample the mean reactivation trace and the corresponding EMD separation was recalculated.</p></sec><sec id="s4-8"><title>Statistics</title><p>To test whether there was a significant linear relationship between perception training model time and imagery reactivation time, we used a generalised linear mixed model (GLMM) with the single trial classifier distance peaks as dependent variable and perception model time during the feed-forward sweep as independent variable. We chose GLMMs because they make fewer assumptions than more commonly used GLMs and because we expected large differences in the onset of reactivation between trials and subjects and, in contrast to GLMs, GLMMs allow for random effects on trials and subjects.</p><p>To obtain 95% confidence intervals for reactivation times, time-frequency and frequency plots, we performed bootstrapping analyses with 10000 bootstrapping samples. For pair-wise comparisons, we obtained p-values by bootstrapping the difference between the two conditions. Source traces represented the mean difference between the stimulus classes which cannot be computed per trial. Therefore, uncertainty in the mean of these values was represented as the standard error of the mean (SEM) over subjects.</p></sec><sec id="s4-9"><title>Source localisation</title><p>To identify brain areas that represented information about the stimuli during perception and imagery, we performed source reconstruction. For LDA classification, the spatial pattern that underlies the classification (i.e. the decoding weights or 'stimulus information’), reduces to the difference in magnetic fields between the two conditions (<xref ref-type="bibr" rid="bib19">Haufe et al., 2014</xref>). Therefore, the difference ERF between faces and houses reflects the contributing brain areas. For the sensor-level activation plots, we calculated the planar gradient for each participant prior to averaging over participants. For the source-level plots, we performed source reconstruction on the axial difference ERF.</p><p>T1-weighted structural MRI images were acquired in separate sessions using a Siemens 3T MRI scanner. Vitamin E markers in both ears indicated the location of the head coils during the MEG measurements, allowing for realignment between the two. The location of the fiducial at the nasion was estimated based on anatomy. The volume conduction model was created based on a single shell model of the inner surface of the skull. The source model was based on a reconstruction of the cortical surface created for each participant using FreeSurfer's anatomical volumetric processing pipeline (RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001847">SCR_001847</ext-link>,(<xref ref-type="bibr" rid="bib12">Fischl, 2012</xref>). MNE-suite (Version 2.7.0; <ext-link ext-link-type="uri" xlink:href="https://mne.tools/">https://mne.tools/</ext-link>, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_005972">SCR_005972</ext-link>, (<xref ref-type="bibr" rid="bib15">Gramfort et al., 2014</xref>) was subsequently used to infer the subject-specific source locations from the surface reconstruction. The resulting head model and source locations were co-registered to the MEG sensors.</p><p>The lead fields were rank reduced for each grid point by removing the sensitivity to the direction perpendicular to the surface of the volume conduction model. Source activity was obtained by estimating linearly constrained minimum variance (LCMV) spatial filters (<xref ref-type="bibr" rid="bib58">Van Veen et al., 1997</xref>). The data covariance was calculated over the interval of 50 ms to 1 s after stimulus onset for perception and over the entire segment for imagery. The data covariance was subsequently regularised using shrinkage with a regularisation parameter of 0.01 (as described in <xref ref-type="bibr" rid="bib38">Manahova et al., 2018</xref>). These filters were then applied to the sensor MEG data, resulting in an estimated two-dimensional dipole moment for each grid point over time.</p><p>To facilitate interpretation and visualisation, we reduced the two-dimensional dipole moments to a scalar value by taking the norm of the vector. This value reflects the degree to which a given source location contributes to activity measured at the sensor level. However, the norm is always a positive value and will therefore, due to noise, suffer from a positivity bias. To counter this bias, we employed a permutation procedure in order to estimate this bias. Specifically, in each permutation, the sign of half of the trials were flipped before averaging and projecting to source space. This way, we cancelled out the systematic stimulus-related part of the signal, leaving only the noise. Reducing this value by taking the norm thus provides an estimate of the positivity bias. This procedure was repeated 1000 times, resulting in a distribution of the noise. We took the mean of this distribution as providing the most likely estimate of the noise and subtracted this from the true, squared source signal. Furthermore, this estimate provides a direct estimate of the artificial amplification factor due to the depth bias. Hence, we also divided the data by the noise estimate to obtain a quantity that allowed visualisation across cortical topography, leading to an unbiased estimate of the amount of stimulus information present in each cortical area. Values below zero therefore reflected no detectable signal compared to noise. For full details, see <xref ref-type="bibr" rid="bib38">Manahova et al., 2018</xref>.</p><p>To perform group averaging, for each subject, the surface-based source points were divided into 74 atlas regions as extracted by FreeSurfer on the basis of the subject-specific anatomy (<xref ref-type="bibr" rid="bib8">Destrieux et al., 2010</xref>). Next, the activation per atlas region was averaged over grid points for each participant. Group-level activations were then calculated by averaging the activity over participants per atlas region (<xref ref-type="bibr" rid="bib57">van de Nieuwenhuijzen et al., 2016</xref>). The early visual cortex ROI (EVC) corresponded to the 'occipital pole' parcels from the Destrieux atlas and the inferior temporal ROI (IT) was a combination of the 'temporal lateral fusiform', 'temporal lateral' and 'temporal lateral and lingual' parcels. Activation in the IT ROI was calculated by applying PCA to the three parcels and taking the first principal component. Because data are z-scored over time during PCA, to ensure that the activation in the EVC ROI was comparable to activation in IT, we also z-scored these data.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The authors would like to thank Emma K Ward for help with the linear mixed model statistics. This work is supported by VIDI grant (639.072.513) from the Netherlands Organization for Scientific Research.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Visualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Resources, Supervision, Funding acquisition, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: All participants gave written informed consent. The study was approved by the local ethics committee and conducted according to the corresponding ethical guidelines (CMO Arnhem-Nijmegen; ECSW2014-0109-246a van Gerven).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Linear Mixed Model Bayesian Information Criterion (BIC) output for the different models.</title><p>(a) Bayesian Information Criterion (BIC) for linear mixed-effects model explaining imagery reactivation time with perception model time. (b) BIC for linear mixed-effects model explaining imagery reactivation time with perception model time after permuting the stimulus-class labels. (c) BIC for linear mixed-effects model explaining imagery reactivation time averaged over trials within subject with perception model time.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53588-supp1-v1.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-53588-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The data used in this paper is available at <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/11633/di.dcc.DSC_2017.00072_245">http://hdl.handle.net/11633/di.dcc.DSC_2017.00072_245</ext-link> and the analysis code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/NadineDijkstra/IMAREV.git">https://github.com/NadineDijkstra/IMAREV.git</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/IMAREV">https://github.com/elifesciences-publications/IMAREV</ext-link>).</p><p>The following previously published dataset was used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>N</given-names></name><name><surname>Mostert</surname><given-names>P</given-names></name><name><surname>de</surname><given-names>Lange FP</given-names></name><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>van</surname><given-names>Gerven MAJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Temporal dynamics of visual imagery and perception</data-title><source>Donders Depository DSC_2017.00072_245:v1</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="http://hdl.handle.net/11633/di.dcc.DSC_2017.00072_245">245:v1</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahissar</surname> <given-names>M</given-names></name><name><surname>Hochstein</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The reverse hierarchy theory of visual perceptual learning</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>457</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.08.011</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albers</surname> <given-names>AM</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Toni</surname> <given-names>I</given-names></name><name><surname>Dijkerman</surname> <given-names>HC</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Shared representations for working memory and mental imagery in early visual cortex</article-title><source>Current Biology</source><volume>23</volume><fpage>1427</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id><pub-id pub-id-type="pmid">23871239</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alilović</surname> <given-names>J</given-names></name><name><surname>Timmermans</surname> <given-names>B</given-names></name><name><surname>Reteig</surname> <given-names>LC</given-names></name><name><surname>van Gaal</surname> <given-names>S</given-names></name><name><surname>Slagter</surname> <given-names>HA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>No evidence that predictions and attention modulate the first feedforward sweep of cortical information processing</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>2261</fpage><lpage>2278</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhz038</pub-id><pub-id pub-id-type="pmid">30877784</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname> <given-names>AM</given-names></name><name><surname>Usrey</surname> <given-names>WM</given-names></name><name><surname>Adams</surname> <given-names>RA</given-names></name><name><surname>Mangun</surname> <given-names>GR</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Canonical microcircuits for predictive coding</article-title><source>Neuron</source><volume>76</volume><fpage>695</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.038</pub-id><pub-id pub-id-type="pmid">23177956</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bullier</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Integrated model of visual processing</article-title><source>Brain Research Reviews</source><volume>36</volume><fpage>96</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/S0165-0173(01)00085-6</pub-id><pub-id pub-id-type="pmid">11690606</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Card</surname> <given-names>JP</given-names></name><name><surname>Moore</surname> <given-names>RY</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Organization of lateral geniculate-hypothalamic connections in the rat</article-title><source>The Journal of Comparative Neurology</source><volume>284</volume><fpage>135</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1002/cne.902840110</pub-id><pub-id pub-id-type="pmid">2754028</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cauchoix</surname> <given-names>M</given-names></name><name><surname>Barragan-Jason</surname> <given-names>G</given-names></name><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Barbeau</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The neural dynamics of face detection in the wild revealed by MVPA</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>846</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3030-13.2014</pub-id><pub-id pub-id-type="pmid">24431443</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destrieux</surname> <given-names>C</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Dale</surname> <given-names>A</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</article-title><source>NeuroImage</source><volume>53</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.06.010</pub-id><pub-id pub-id-type="pmid">20547229</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname> <given-names>N</given-names></name><name><surname>Bosch</surname> <given-names>SE</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Vividness of visual imagery depends on the neural overlap with perception in visual Areas</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>1367</fpage><lpage>1373</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3022-16.2016</pub-id><pub-id pub-id-type="pmid">28073940</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname> <given-names>N</given-names></name><name><surname>Mostert</surname> <given-names>P</given-names></name><name><surname>Lange</surname> <given-names>FP</given-names></name><name><surname>Bosch</surname> <given-names>S</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Differential temporal dynamics during visual imagery and perception</article-title><source>eLife</source><volume>7</volume><elocation-id>e33904</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.33904</pub-id><pub-id pub-id-type="pmid">29807570</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname> <given-names>N</given-names></name><name><surname>Bosch</surname> <given-names>SE</given-names></name><name><surname>van Gerven</surname> <given-names>MAJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Shared neural mechanisms of visual perception and imagery</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>423</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.02.004</pub-id><pub-id pub-id-type="pmid">30876729</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The generative adversarial brain</article-title><source>Technical Report</source><volume>2</volume><elocation-id>18</elocation-id><pub-id pub-id-type="doi">10.3389/frai.2019.00018</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Luessi</surname> <given-names>M</given-names></name><name><surname>Larson</surname> <given-names>E</given-names></name><name><surname>Engemann</surname> <given-names>DA</given-names></name><name><surname>Strohmeier</surname> <given-names>D</given-names></name><name><surname>Brodbeck</surname> <given-names>C</given-names></name><name><surname>Parkkonen</surname> <given-names>L</given-names></name><name><surname>Hämäläinen</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>MNE software for processing MEG and EEG data</article-title><source>NeuroImage</source><volume>86</volume><fpage>446</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.027</pub-id><pub-id pub-id-type="pmid">24161808</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname> <given-names>T</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Finding decodable information that can be read out in behaviour</article-title><source>NeuroImage</source><volume>179</volume><fpage>252</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.06.022</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grush</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The emulation theory of representation: motor control, imagery, and perception</article-title><source>Behavioral and Brain Sciences</source><volume>27</volume><fpage>377</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1017/S0140525X04000093</pub-id><pub-id pub-id-type="pmid">15736871</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname> <given-names>SA</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding reveals the contents of visual working memory in early visual Areas</article-title><source>Nature</source><volume>458</volume><fpage>632</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nature07832</pub-id><pub-id pub-id-type="pmid">19225460</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haufe</surname> <given-names>S</given-names></name><name><surname>Meinecke</surname> <given-names>F</given-names></name><name><surname>Görgen</surname> <given-names>K</given-names></name><name><surname>Dähne</surname> <given-names>S</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name><name><surname>Blankertz</surname> <given-names>B</given-names></name><name><surname>Bießmann</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title><source>NeuroImage</source><volume>87</volume><fpage>96</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.067</pub-id><pub-id pub-id-type="pmid">24239590</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Helmholtz</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1925">1925</year><source>Hysiological Optics, Vol. III: The Perceptions of Vision</source><publisher-name>Optical Society of America</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hobson</surname> <given-names>JA</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Waking and dreaming consciousness: neurobiological and functional considerations</article-title><source>Progress in Neurobiology</source><volume>98</volume><fpage>82</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2012.05.003</pub-id><pub-id pub-id-type="pmid">22609044</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochstein</surname> <given-names>S</given-names></name><name><surname>Ahissar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>View from the top: hierarchies and reverse hierarchies in the visual system</article-title><source>Neuron</source><volume>36</volume><fpage>791</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01091-7</pub-id><pub-id pub-id-type="pmid">12467584</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>N</given-names></name><name><surname>Shen</surname> <given-names>Z</given-names></name><name><surname>Long</surname> <given-names>S</given-names></name><name><surname>Wu</surname> <given-names>M</given-names></name><name><surname>Shih</surname> <given-names>H</given-names></name><name><surname>Zheng</surname> <given-names>Q</given-names></name><name><surname>Yen</surname> <given-names>N</given-names></name><name><surname>Tung</surname> <given-names>C</given-names></name><name><surname>Liu</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The empirical mode decomposition and the Hilbert spectrum for nonlinear and non-stationary</article-title><source>Proceedings Mathematical</source><volume>454</volume><fpage>903</fpage><lpage>995</lpage><pub-id pub-id-type="doi">10.1098/rspa.1998.0193</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Receptive fields and functional architecture of monkey striate cortex</article-title><source>The Journal of Physiology</source><volume>195</volume><fpage>215</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1968.sp008455</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ishai</surname> <given-names>A</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Distributed neural systems for the generation of visual images</article-title><source>Neuron</source><volume>28</volume><fpage>979</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)00168-9</pub-id><pub-id pub-id-type="pmid">11163281</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname> <given-names>L</given-names></name><name><surname>Meyers</surname> <given-names>EM</given-names></name><name><surname>Leibo</surname> <given-names>JZ</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The dynamics of invariant object recognition in the human visual system</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>91</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1152/jn.00394.2013</pub-id><pub-id pub-id-type="pmid">24089402</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerrén</surname> <given-names>C</given-names></name><name><surname>Linde-Domingo</surname> <given-names>J</given-names></name><name><surname>Hanslmayr</surname> <given-names>S</given-names></name><name><surname>Wimber</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An optimal oscillatory phase for pattern reactivation during memory retrieval</article-title><source>Current Biology</source><volume>28</volume><fpage>3383</fpage><lpage>3392</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.08.065</pub-id><pub-id pub-id-type="pmid">30344116</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kersten</surname> <given-names>D</given-names></name><name><surname>Mamassian</surname> <given-names>P</given-names></name><name><surname>Yuille</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Object perception as bayesian inference</article-title><source>Annual Review of Psychology</source><volume>55</volume><fpage>271</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.55.090902.142005</pub-id><pub-id pub-id-type="pmid">14744217</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id><pub-id pub-id-type="pmid">24593982</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname> <given-names>DC</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The Bayesian brain: the role of uncertainty in neural coding and computation</article-title><source>Trends in Neurosciences</source><volume>27</volume><fpage>712</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>265</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id><pub-id pub-id-type="pmid">22841311</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname> <given-names>SM</given-names></name><name><surname>Ganis</surname> <given-names>G</given-names></name><name><surname>Thompson</surname> <given-names>WL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural foundations of imagery</article-title><source>Nature Reviews Neuroscience</source><volume>2</volume><fpage>635</fpage><lpage>642</lpage><pub-id pub-id-type="doi">10.1038/35090055</pub-id><pub-id pub-id-type="pmid">11533731</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname> <given-names>SM</given-names></name><name><surname>Thompson</surname> <given-names>WL</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>When is early visual cortex activated during visual mental imagery?</article-title><source>Psychological Bulletin</source><volume>129</volume><fpage>723</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.129.5.723</pub-id><pub-id pub-id-type="pmid">12956541</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VAF</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title><source>Trends in Neurosciences</source><volume>23</volume><fpage>571</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01657-X</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>S-H</given-names></name><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Disentangling visual imagery and perception of real-world objects</article-title><source>NeuroImage</source><volume>59</volume><fpage>4064</fpage><lpage>4073</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.055</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linde-Domingo</surname> <given-names>J</given-names></name><name><surname>Treder</surname> <given-names>MS</given-names></name><name><surname>Kerrén</surname> <given-names>C</given-names></name><name><surname>Wimber</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evidence that neural information flow is reversed between object perception and object reconstruction from memory</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>300913</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-08080-2</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lozano-Soldevilla</surname> <given-names>D</given-names></name><name><surname>VanRullen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The hidden spatial dimension of alpha: 10-hz perceptual echoes propagate as periodic traveling waves in the human brain</article-title><source>Cell Reports</source><volume>26</volume><fpage>374</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2018.12.058</pub-id><pub-id pub-id-type="pmid">30625320</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manahova</surname> <given-names>ME</given-names></name><name><surname>Mostert</surname> <given-names>P</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Stimulus familiarity and expectation jointly modulate neural activity in the visual ventral stream</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1366</fpage><lpage>1377</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01281</pub-id><pub-id pub-id-type="pmid">29762101</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname> <given-names>JH</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Visual processing in monkey extrastriate cortex</article-title><source>Annual Review of Neuroscience</source><volume>10</volume><fpage>363</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.10.030187.002051</pub-id><pub-id pub-id-type="pmid">3105414</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michalareas</surname> <given-names>G</given-names></name><name><surname>Vezoli</surname> <given-names>J</given-names></name><name><surname>van Pelt</surname> <given-names>S</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name><name><surname>Kennedy</surname> <given-names>H</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Alpha-Beta and gamma rhythms subserve feedback and feedforward influences among human visual cortical Areas</article-title><source>Neuron</source><volume>89</volume><fpage>384</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.018</pub-id><pub-id pub-id-type="pmid">26777277</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mostert</surname> <given-names>P</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociating sensory from decision processes in human perceptual decision making</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>18253</elocation-id><pub-id pub-id-type="doi">10.1038/srep18253</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moulton</surname> <given-names>ST</given-names></name><name><surname>Kosslyn</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Imagining predictions: mental imagery as mental emulation</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>364</volume><fpage>1273</fpage><lpage>1280</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0314</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname> <given-names>J</given-names></name><name><surname>Clifford</surname> <given-names>CW</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The functional impact of mental imagery on conscious perception</article-title><source>Current Biology</source><volume>18</volume><fpage>982</fpage><lpage>986</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.05.048</pub-id><pub-id pub-id-type="pmid">18583132</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname> <given-names>J</given-names></name><name><surname>Keogh</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Redefining visual working memory: a Cognitive-Strategy, Brain-Region approach</article-title><source>Current Directions in Psychological Science</source><volume>28</volume><fpage>266</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1177/0963721419835210</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennartz</surname> <given-names>CMA</given-names></name><name><surname>Dora</surname> <given-names>S</given-names></name><name><surname>Muckli</surname> <given-names>L</given-names></name><name><surname>Lorteije</surname> <given-names>JAM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Towards a unified view on pathways and functions of neural recurrent processing</article-title><source>Trends in Neurosciences</source><volume>42</volume><fpage>589</fpage><lpage>603</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2019.07.005</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramkumar</surname> <given-names>P</given-names></name><name><surname>Jas</surname> <given-names>M</given-names></name><name><surname>Pannasch</surname> <given-names>S</given-names></name><name><surname>Hari</surname> <given-names>R</given-names></name><name><surname>Parkkonen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Feature-specific information processing precedes concerted activation in human visual cortex</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>7691</fpage><lpage>7699</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3905-12.2013</pub-id><pub-id pub-id-type="pmid">23637162</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname> <given-names>L</given-names></name><name><surname>Tsuchiya</surname> <given-names>N</given-names></name><name><surname>Serre</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Reading the mind's eye: Decoding category information during mental imagery</article-title><source>NeuroImage</source><volume>50</volume><fpage>818</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.11.084</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reid</surname> <given-names>RC</given-names></name><name><surname>Alonso</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Specificity of monosynaptic connections from thalamus to visual cortex</article-title><source>Nature</source><volume>378</volume><fpage>281</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1038/378281a0</pub-id><pub-id pub-id-type="pmid">7477347</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rilling</surname> <given-names>G</given-names></name><name><surname>Flandrin</surname> <given-names>P</given-names></name><name><surname>Goncalves</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>On empirical mode decomposition and its algorithms</article-title><conf-name>In IEEE-EURASIP Workshop on Nonlinear Signal and Image Processing</conf-name></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seeliger</surname> <given-names>K</given-names></name><name><surname>Fritsche</surname> <given-names>M</given-names></name><name><surname>Güçlü</surname> <given-names>U</given-names></name><name><surname>Schoenmakers</surname> <given-names>S</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name><name><surname>Bosch</surname> <given-names>SE</given-names></name><name><surname>van Gerven</surname> <given-names>MAJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Convolutional neural network-based encoding and decoding of visual object recognition in space and time</article-title><source>NeuroImage</source><volume>180</volume><fpage>253</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.07.018</pub-id><pub-id pub-id-type="pmid">28723578</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senden</surname> <given-names>M</given-names></name><name><surname>Emmerling</surname> <given-names>TC</given-names></name><name><surname>van Hoof</surname> <given-names>R</given-names></name><name><surname>Frost</surname> <given-names>MA</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reconstructing imagined letters from early visual cortex reveals tight topographic correspondence between visual mental imagery and perception</article-title><source>Brain Structure and Function</source><volume>224</volume><fpage>1167</fpage><lpage>1183</lpage><pub-id pub-id-type="doi">10.1007/s00429-019-01828-6</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spaak</surname> <given-names>E</given-names></name><name><surname>Watanabe</surname> <given-names>K</given-names></name><name><surname>Funahashi</surname> <given-names>S</given-names></name><name><surname>Stokes</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stable and dynamic coding for working memory in primate prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6503</fpage><lpage>6516</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3364-16.2017</pub-id><pub-id pub-id-type="pmid">28559375</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stolk</surname> <given-names>A</given-names></name><name><surname>Todorovic</surname> <given-names>A</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Online and offline tools for head movement compensation in MEG</article-title><source>NeuroImage</source><volume>68</volume><fpage>39</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.11.047</pub-id><pub-id pub-id-type="pmid">23246857</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname> <given-names>S</given-names></name><name><surname>Fize</surname> <given-names>D</given-names></name><name><surname>Marlot</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Speed of processing in the human visual system</article-title><source>Nature</source><volume>381</volume><fpage>520</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/381520a0</pub-id><pub-id pub-id-type="pmid">8632824</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname> <given-names>S</given-names></name><name><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Seeking categories in the brain</article-title><source>Science</source><volume>291</volume><fpage>260</fpage><lpage>263</lpage><pub-id pub-id-type="doi">10.1126/science.1058249</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Nieuwenhuijzen</surname> <given-names>ME</given-names></name><name><surname>van den Borne</surname> <given-names>EW</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Spatiotemporal dynamics of cortical representations during and after stimulus presentation</article-title><source>Frontiers in Systems Neuroscience</source><volume>10</volume><elocation-id>42</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2016.00042</pub-id><pub-id pub-id-type="pmid">27242453</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname> <given-names>BD</given-names></name><name><surname>van Drongelen</surname> <given-names>W</given-names></name><name><surname>Yuchtman</surname> <given-names>M</given-names></name><name><surname>Suzuki</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>44</volume><fpage>867</fpage><lpage>880</lpage><pub-id pub-id-type="doi">10.1109/10.623056</pub-id><pub-id pub-id-type="pmid">9282479</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname> <given-names>D</given-names></name><name><surname>Myers</surname> <given-names>NE</given-names></name><name><surname>Stokes</surname> <given-names>M</given-names></name><name><surname>Nobre</surname> <given-names>AC</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Temporally unconstrained decoding reveals consistent but Time-Varying stages of stimulus processing</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>863</fpage><lpage>874</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy290</pub-id><pub-id pub-id-type="pmid">30535141</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname> <given-names>R</given-names></name><name><surname>Orban</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Coding of stimulus invariances by inferior temporal neurons</article-title><source>Progress in Brain Research</source><volume>112</volume><fpage>195</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/s0079-6123(08)63330-0</pub-id><pub-id pub-id-type="pmid">8979830</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>G</given-names></name><name><surname>Chen</surname> <given-names>X-Y</given-names></name><name><surname>Qiao</surname> <given-names>F-LI</given-names></name><name><surname>Wu</surname> <given-names>Z</given-names></name><name><surname>Huang</surname> <given-names>NE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>On intrinsic mode function</article-title><source>Advances in Adaptive Data Analysis</source><volume>02</volume><fpage>277</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1142/S1793536910000549</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53588.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Varoquaux</surname><given-names>Gaël</given-names></name><role>Reviewing Editor</role><aff><institution>Inria Saclay</institution><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Dijkstra et al. provide empirical evidence for reversal of perceptual inference by analyzing the electrophysiological signature of brain responses to visual stimuli. It reveals a feedback loop from higher-level regions that appear as an oscillation in the delay of the brain response. This empirical evidence is useful to refine theories of perception.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Neural dynamics of perceptual inference and its reversal during imagery&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Timothy Behrens as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This manuscript uses MEG during a working memory/imagery task to investigate the flow of information between low and high level areas during perception and imagery respectively. The authors provide empirical evidence consistent with the notion that high level regions are first during imagery, and last during perception. In addition, they show that recall leads to several oscillations (around 11Hz), interpreted as bottom-up and top-down processes. This evidence is extracted via several methodological innovations. A multivariate classification methods, namely Linear Discriminant Analysis (LDA) is applied to various time points following stimulus presentation and the resulting trained classifiers (one by time-point) are used, to time-align the imagery trials via their ability to discriminate better than others. This time alignment is central to claim that “information” is processed bottom-up or top-down along the ventral visual pathway. Here the distance to the discriminating hyperplane is used as a proxy for the evidence that the signal is present, analyzed at the group level.</p><p>The findings were deemed of high interest by the reviewers. The evidence for oscillations is expected to stimulate greatly neurosciences, in particular predictive coding theory. Indeed, the phenomena revealed are insufficiently known (e.g. the oscillation between feedforward and feedback activity during imagery). The reviewers also appreciated the creative use of classifiers to extract signal as well as the fact that the data from the study are publicly available.</p><p>The discussions however mostly focused on trying to decide whether or not the evidence was conclusive. Given the amount of non-classic data transformations that lead to the evidence, how to assert that the results, in particular the multiple oscillations, are specific to visual recall, and not low-level properties of the neural signal, Indeed, the neural signal has a complex time-frequency structure. Our understanding of the permutations used is that they will create unstructured patterns, which will lead to a random and unstructured ordering of the most-similar pattern. In other terms, the permutations do not capture the temporal structure of the neural signal. A better null procedure is needed to provide solid evidence that the oscillation revealed is indeed related to imagery dynamics.</p><p>Essential revisions:</p><p>1) A first point to clarify are what aspects of the stimuli that drives variations in the perception time. The manuscript is very light on details in terms of the psychological dimensions varied to create the variations in the perception time that are related to variations in the imagery time. The theories invoked (predictive processing theories) relate to ambiguity in the sensory signal, hence they suggest that the variations the perception time should be related to the stimuli. Yet, the manuscript does not offer an explanation on the causes the variation in perception time. Some of the differences between the two classes of stimuli likely reflect &quot;unspecific&quot; mechanisms: i.e. electrophysiological signatures that are not specific to particular neural representations, but to overall differences in e.g. attentional capture. For example, if faces trigger a stronger visual response than houses, then, what is considered to be the reactivation of a face in this study, would simply reflect an overall modulation of visual/IT activity. Such non-specific activation would be expected to trigger feedforward and feedback traveling waves in the ventral and dorsal visual pathways, as extensively described in monkey electrophysiology as well as with MEG (e.g. Michalareas et al., 2016). For these reasons, we feel that the revised manuscript should discuss in detail what aspects of the stimuli drive the variations in time.</p><p>2) A second wider point relates to establishing whether or not the sophisticated analysis enhances oscillations that are specific to mental imagery, or rather general to neural signals. Indeed, neural signals have a complex autocorrelation structure. To address the possible confound that alpha modulation could affect the SNR and explain the corresponding findings the authors perform a coherence analysis on individual sensors. The fact that no significant effect is obtained with this analysis motivates the authors to rule out this hypothesis. However, statistics on single sensors are known to be less statistically powerful techniques, so the result could be explained by the poor statistical power of this supplementary analysis. Overall, all the reviewers found it hard to assess the methods, and hence feel that the whole pipeline should be experimentally shown not to create the signatures that are interpreted. A small number of specific analyses can help establishing this evidence without resorting to fine theoretical analysis of the data-processing pipeline:</p><p>A) Applying the same exact analysis to neural signal where one would not expect such conclusion to arise. A specific instance of such signals can be found using time windows located in the inter-stimuli intervals (for instance before the trial).</p><p>B) Using a complementary analysis based on second-level statistics: i.e. fit a distinct model on each subject separately, and test the reactivation signatures across subjects. Indeed, the authors use mixed-model/first-level statistics with trials and subjects as random variables. Such approach is unusual in decoding-based analyses, because of the cross-validation – i.e. the training of the decoder introduces dependencies across decoders' predictions, and thus breaks the independence assumption of first-order statistical tests.</p><p>C) Using the non low-pass filtered data. Indeed, there is the suspicion that the oscillatory effect observed during perception could be due to a low pass filtering effect. This doubt is suggested by the supplementary analysis using non-low pass filtered data. Figure S2 in supplementary material is however not comparable to the corresponding figure in the main text. If the non-low pass filtered data actually confirm the findings these data should be used in the main text without the need for added analysis.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Neural dynamics of perceptual inference and its reversal during imagery&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Timothy Behrens (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>The reviewers find that the evidence is not rock solid, but the work is stimulating and the findings do not stem from an apparent methodological flaw. They feel that publishing these is beneficial for the field, but would like the potential limitations to be more explicit. No additional analysis is mandatory for resubmitting, but work on the wording is important.</p><p><italic>Reviewer #1:</italic></p><p>The authors answered thoroughly most of the comments raised by the reviewers. Importantly, they performed complementary analyses to make the evidence stronger, for instance the analysis without low-pass filtering. Also, the trial-level data show a weak effect, but one that is present, which is an important piece of evidence to confirm that the effect is not created by the analysis. They unfortunately did not follow our suggestion of analyzing data outside the stimuli, which would have made a strong null hypothesis. Rather, they used an elaborate simulation.</p><p>Overall, I feel comforted that the evidence is not created by the data analysis, and I am in favor of publication.</p><p><italic>Reviewer #2:</italic></p><p>Thanks to the authors for this response.</p><p>Generally speaking, I am not amazed by responses based on simulations – which is the source of a significant set of the new results and analyses in the present revision. Simulations remain soft controls, because, unlike real data, they may be blind to a wide variety of issues. Responses systematically based on novel analyses of the MEG signals would have been more convincing, like we originally proposed (e.g. applying the same analyses on different time windows such as the inter-stimulus interval, in order to ensure that no effect were found there).</p><p>In addition, if I understand correctly, the p-values derived from the mix models may be invalid (reviewing comment 2B): i.e. we cannot easily estimate p-values when the independence hypotheses between samples is not met – which is the case with a CV, because the training models are fitted on partially similar datasets – consequently their predictions are not independent: c.f. e.g. Noirhomme et al., 2014 (NeuroImage: Clinical).</p><p>This arguably minor concern is reinforced by the fact that single-trial-based statistics can dramatically increase the degrees of freedom, and can thus lead to unreasonably confident p-values. Second-level stats across subjects only would be a more conservative approach. Again, the authors did not follow that recommended route.</p><p>That being said, these statistical issues should not undermine the methodological and neuroscientific work. While I am relatively unhappy with the current answer, the paper remains interesting and potentially important.</p><p>Consequently, I will not oppose its acceptation for publication, but would strongly recommend the authors to provide a clear code of their analyses and simulations to facilitate future replications.</p><p><italic>Reviewer #3:</italic></p><p>I would like to thank the authors for the convincing and thorough revision.</p><p>I just have two remaining comments.</p><p>– Related to your answer &quot;We acknowledge that this breaks the independence assumption but we still believe that this is the most valid test for our data, since testing across subjects ignores the large between-trial variability.&quot; Can you add a sentence somewhere in the text about this?</p><p>Just as a comment here, using non-parametric methods like in the EEGLAB LIMO toolbox would address this issue: e.g. Pernet et al., 2011 (Computational Intelligence and Neuroscience).</p><p>– The proper URL for MNE software is https://mne.tools/ and the related</p><p>publication is:</p><p>A. Gramfort, M. Luessi, E. Larson, D. Engemann, D. Strohmeier, C. Brodbeck, L. Parkkonen, M. Hämäläinen, MNE software for processing MEG and EEG data, NeuroImage, Volume 86, 1 February 2014, Pages 446-460, ISSN 1053-8119</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53588.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) A first point to clarify are what aspects of the stimuli that drives variations in the perception time. The manuscript is very light on details in terms of the psychological dimensions varied to create the variations in the perception time that are related to variations in the imagery time. The theories invoked (predictive processing theories) relate to ambiguity in the sensory signal, hence they suggest that the variations the perception time should be related to the stimuli. Yet, the manuscript does not offer an explanation on the causes the variation in perception time. Some of the differences between the two classes of stimuli likely reflect &quot;unspecific&quot; mechanisms: i.e. electrophysiological signatures that are not specific to particular neural representations, but to overall differences in e.g. attentional capture. For example, if faces trigger a stronger visual response than houses, then, what is considered to be the reactivation of a face in this study, would simply reflect an overall modulation of visual/IT activity. Such non-specific activation would be expected to trigger feedforward and feedback traveling waves in the ventral and dorsal visual pathways, as extensively described in monkey electrophysiology as well as with MEG (e.g. Michalareas et al., 2016). For these reasons, we feel that the revised manuscript should discuss in detail what aspects of the stimuli drive the variations in time.</p></disp-quote><p>We thank the reviewers for raising this important point. We agree that it is important to discuss in more detail the stimulus features that can be captured by our classifiers during perception and subsequently during the reactivation during imagery.</p><p>The main analysis is based on the idea that stimulus features of increasing complexity are processed in a hierarchical manner during the initial time points of perception (i.e. “the feedforward sweep”) and that this is what is leveraged by our classifiers. We confirmed that early classifiers indeed picked up differences in neural activation in low-level visual areas whereas later classifiers relied on differences in high-level visual areas using source-reconstruction (Figure 1A-B). Therefore, they support our main claims, which use these classifiers to investigate whether stimulus information – defined as differences in neural activation between the two stimuli – flows up or down the visual hierarchy.</p><p>However, as the reviewers point out, the inference that these classifiers pick up on increasingly complex stimulus features is based on what the literature says about the features that are processed in these different brain areas. We did not directly assess which stimulus features the classifier was sensitive to in this experiment and therefore cannot be certain about which features each classifier decoded. To discuss this point in more detail we have added the following paragraph to the Discussion:</p><p>“Central to this predictive processing interpretation of bottom-up and top-down sweeps is that increasingly abstract stimulus features are processed in higher-level brain areas. […] Further research using explicit encoding models of stimulus features at different levels of abstraction would be necessary to completely address this point.”</p><disp-quote content-type="editor-comment"><p>2) A second wider point relates to establishing whether or not the sophisticated analysis enhances oscillations that are specific to mental imagery, or rather general to neural signals. Indeed, neural signals have a complex autocorrelation structure. To address the possible confound that alpha modulation could affect the SNR and explain the corresponding findings the authors perform a coherence analysis on individual sensors. The fact that no significant effect is obtained with this analysis motivates the authors to rule out this hypothesis. However, statistics on single sensors are known to be less statistically powerful techniques, so the result could be explained by the poor statistical power of this supplementary analysis. Overall, all the reviewers found it hard to assess the methods, and hence feel that the whole pipeline should be experimentally shown not to create the signatures that are interpreted. A small number of specific analyses can help establishing this evidence without resorting to fine theoretical analysis of the data-processing pipeline:</p></disp-quote><p>We agree with the reviewers that the method pipeline is unorthodox and needs to be validated more thoroughly. In order to do this, we ran a simulation, added details on the relationship between our approach and well-established decoding and added the results using the unfiltered data.</p><p>With respect to the relationship between the observed oscillation in reactivations of neural representations and alpha power, we agree that statistics on single sensors are less statistically sensitive. However, the observed coherence is so low and has such a random topography that it is unlikely that the lack of a finding here is only due to statistical power. To clarify this, we have added the coherence topoplot as a sub-panel of the main oscillation figure, see Figure 4D.</p><disp-quote content-type="editor-comment"><p>A) Applying the same exact analysis to neural signal where one would not expect such conclusion to arise. A specific instance of such signals can be found using time windows located in the inter-stimuli intervals (for instance before the trial).</p></disp-quote><p>We agree with the reviewer that this is a good way of testing the validity of our approach. However, we have decided to focus on showing the validity of our approach using simulations, which allowed us to directly test the effect of different parameters (e.g. order of reactivation and amount of temporal uncertainty between trials) on the observed results.</p><disp-quote content-type="editor-comment"><p>B) Using a complementary analysis based on second-level statistics: i.e. fit a distinct model on each subject separately, and test the reactivation signatures across subjects. Indeed, the authors use mixed-model/first-level statistics with trials and subjects as random variables. Such approach is unusual in decoding-based analyses, because of the cross-validation – i.e. the training of the decoder introduce dependencies across decoders' predictions, and thus breaks the independence assumption of first-order statistical tests.</p></disp-quote><p>We are not completely sure that we understand what the reviewers refer to here. For clarification, we do fit a distinct perception decoding model to each subject separately and calculate the reactivation of the subject-specific model per trial during imagery. Because we expected large variation in onset between trials, we applied a Linear Mixed Model which allowed us to set “trial” as a random variable. This is equivalent to what was used in Linde-Domingo et al., 2019, on which we based a large part of our methods. We did use cross-validation because the perception and imagery events were part of the same “trial” period, resulting in auto-correlation between imagery and perception if they belonged to the same trial (see Materials and methods “Temporal Decoding Analysis”). Is the reviewer referring to the fact that there is some dependency across predictions within each subject because the training data for different folds overlaps? We acknowledge that this breaks the independence assumption but we still believe that this is the most valid test for our data, since testing across subjects ignores the large between-trial variability.</p><disp-quote content-type="editor-comment"><p>C) Using the non low-pass filtered data. Indeed, there is the suspicion that the oscillatory effect observed during perception could be due to a low pass filtering effect. This doubt is suggested by the supplementary analysis using non-low pass filtered data. Figure S2 in supplementary material is however not comparable to the corresponding figure in the main text. If the non-low pass filtered data actually confirm the findings these data should be used in the main text without the need for added analysis.</p></disp-quote><p>The low-pass filter was only applied to the imagery distance values and not to the perception data, and therefore could not have induced an oscillation during perception. To clarify this further we have added the low-pass filter step to the methods figure, see Figure 1.</p><p>Furthermore, we agree that the supplementary figures showing the unfiltered results were not clear before as they used a different y-axis. We have now added the results without this filtering step as supplementary figures to both Figure 2 and Figure 3.</p><p>As can be seen, none of the results qualitatively changed when we removed the filter. However, we decided to use the filter because during imagery there is a lot of uninformative high-frequency signal, purely because the signal is quite noisy. This is apparent in the fact that the results become cleaner with the filter, especially for the first imagery reversal. Because the filter could not induce the observed oscillation and because the results became cleaner, we decided to keep the filtered results as the main figures in the paper.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>The reviewers find that the evidence is not rock solid, but the work is stimulating and the findings do not stem from an apparent methodological flaw. They feel that publishing these is beneficial for the field, but would like the potential limitations to be more explicit. No additional analysis is mandatory for resubmitting, but work on the wording is important.</p></disp-quote><p>Thanks. Even though no further analyses were mandatory, we have nevertheless preferred to address the remaining concern regarding the validity of our statistical tests by adding another analysis in which we test between subjects rather than between trials and show that the effect remains significant. Furthermore, we added a paragraph on the limitations of the current analysis pipeline and have made all analysis scripts publicly available on GitHub. We hope that this has sufficiently addressed the raised concerns.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>Thanks to the authors for this response.</p><p>Generally speaking, I am not amazed by responses based on simulations – which is the source of a significant set of the new results and analyses in the present revision. Simulations remain soft controls, because, unlike real data, they may be blind to a wide variety of issues. Responses systematically based on novel analyses of the MEG signals would have been more convincing, like we originally proposed (e.g. applying the same analyses on different time windows such as the inter-stimulus interval, in order to ensure that no effect were found there).</p></disp-quote><p>We understand the reviewer’s point about the potential issues with simulations. To address this, we have added the following to the Discussion:</p><p>“Furthermore, the current study used a number of non-traditional analysis steps. While we aimed to demonstrate the validity of this approach via simulations, it is worth noting that simulations are not a perfect control since simulated data cannot account for all the features present in real data, and might be blind to other issues. Therefore, to fully ensure that this analysis approach does not suffer from any overlooked confounds, future validation studies are needed.”</p><disp-quote content-type="editor-comment"><p>In addition, if I understand correctly, the p-values derived from the mix models may be invalid (reviewing comment 2B): i.e. we cannot easily estimate p-values when the independence hypotheses between samples is not met – which is the case with a CV, because the training models are fitted on partially similar datasets – consequently their predictions are not independent: c.f. e.g. Noirhomme et al., 2014 (NeuroImage: Clinical).</p><p>This arguably minor concern is reinforced by the fact that single-trial-based statistics can dramatically increase the degrees of freedom, and can thus lead to unreasonably confident p-values. Second-level stats across subjects only would be a more conservative approach. Again, the authors did not follow that recommended route.</p><p>That being said, these statistical issues should not undermine the methodological and neuroscientific work. While I am relatively unhappy with the current answer, the paper remains interesting and potentially important.</p><p>Consequently, I will not oppose its acceptation for publication, but would strongly recommend the authors to provide a clear code of their analyses and simulations to facilitate future replications.</p></disp-quote><p>We agree with the reviewer that the statistical analysis that we have used to assess the main imagery effect is not ideal. As outlined in the previous revision, we still believe this is the most appropriate test for our data because it allows for variation in the intercept between trials which can capture the large variation in onset of imagery between trials. However, to take away the worry that our effect is only significant because of the large degrees of freedom or the violation of the independence assumption, we have decided to run additional between-subject LMM models and have added the following to the Results sections:</p><p>“Finally, because we used cross-validation within subjects to calculate reactivation timing, there is a dependence between trials of the same subject, violating the independence assumption of first-order statistical tests. As a sanity check, we performed a second test that did not require splitting trials: we used LMM models with reactivation times averaged over trials within subjects, and only “subject” as a random variable. In this case, the model containing both a random effect of intercept as well as slope per subject best explained the data (see Supplementary File 1C). This between-subject model still showed a significant main effect of perception time (t(24) = -3.24, p = 0.003) with a negative slope (B0 = 2.05, SD = 0.03, B1 = -1.19, SD = 0.37) confirming that the effect was not dependent on between-trial statistics.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>I would like to thank the authors for the convincing and thorough revision.</p><p>I just have two remaining comments.</p><p>– Related to your answer &quot;We acknowledge that this breaks the independence assumption but we still believe that this is the most valid test for our data, since testing across subjects ignores the large between-trial variability.&quot; Can you add a sentence somewhere in the text about this?</p></disp-quote><p>Please see the response to reviewer 2 above.</p><disp-quote content-type="editor-comment"><p>Just as a comment here, using non-parametric methods like in the EEGLAB LIMO toolbox</p><p>would address this issue: e.g. Pernet et al., 2011 (Computational Intelligence and Neuroscience).</p><p>– The proper URL for MNE software is https://mne.tools/ and the related</p><p>publication is:</p><p>A. Gramfort, M. Luessi, E. Larson, D. Engemann, D. Strohmeier, C. Brodbeck, L. Parkkonen, M. Hämäläinen, MNE software for processing MEG and EEG data, NeuroImage, Volume 86, 1 February 2014, Pages 446-460, ISSN 1053-8119</p></disp-quote><p>Thank you, we have added this to the manuscript.</p></body></sub-article></article>