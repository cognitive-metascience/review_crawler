<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">54026</article-id><article-id pub-id-type="doi">10.7554/eLife.54026</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>A decentralised neural model explaining optimal integration of navigational strategies in insects</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-166494"><name><surname>Sun</surname><given-names>Xuelong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9035-5523</contrib-id><email>xsun@lincoln.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-166495"><name><surname>Yue</surname><given-names>Shigang</given-names></name><email>syue@lincoln.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-166496"><name><surname>Mangan</surname><given-names>Michael</given-names></name><email>m.mangan@sheffield.ac.uk</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Computational Intelligence Lab &amp; L-CAS, School of Computer Science, University of Lincoln</institution><addr-line><named-content content-type="city">Lincoln</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>Machine Life and Intelligence Research Centre, Guangzhou University</institution><addr-line><named-content content-type="city">Guangzhou</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution>Sheffield Robotics, Department of Computer Science, University of Sheffield</institution><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ramaswami</surname><given-names>Mani</given-names></name><role>Reviewing Editor</role><aff><institution>Trinity College Dublin</institution><country>Ireland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Eisen</surname><given-names>Michael B</given-names></name><role>Senior Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>26</day><month>06</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e54026</elocation-id><history><date date-type="received" iso-8601-date="2019-11-28"><day>28</day><month>11</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-06-26"><day>26</day><month>06</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Sun et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Sun et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-54026-v2.pdf"/><abstract><p>Insect navigation arises from the coordinated action of concurrent guidance systems but the neural mechanisms through which each functions, and are then coordinated, remains unknown. We propose that insects require distinct strategies to retrace familiar routes (route-following) and directly return from novel to familiar terrain (homing) using different aspects of frequency encoded views that are processed in different neural pathways. We also demonstrate how the Central Complex and Mushroom Bodies regions of the insect brain may work in tandem to coordinate the directional output of different guidance cues through a contextually switched ring-attractor inspired by neural recordings. The resultant unified model of insect navigation reproduces behavioural data from a series of cue conflict experiments in realistic animal environments and offers testable hypotheses of where and how insects process visual cues, utilise the different information that they provide and coordinate their outputs to achieve the adaptive behaviours observed in the wild.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>insect navigation</kwd><kwd>central complex</kwd><kwd>desert ants</kwd><kwd>optimal integration</kwd><kwd>ring attractor</kwd><kwd>mushroom body</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>ULTRACEPT 778062</award-id><principal-award-recipient><name><surname>Sun</surname><given-names>Xuelong</given-names></name><name><surname>Yue</surname><given-names>Shigang</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>STEP2DYNA 691154</award-id><principal-award-recipient><name><surname>Sun</surname><given-names>Xuelong</given-names></name><name><surname>Yue</surname><given-names>Shigang</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The first biology-constrained neural model of insect navigation guidance systems was explained including path integration, visual homing and route following, and their optimal coordination.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Central-place foraging insects navigate using a ‘toolkit’ of independent guidance systems (<xref ref-type="bibr" rid="bib88">Wehner, 2009</xref>) of which the most fundamental are path integration (PI), whereby foragers track the distance and direction to their nest by integrating the series of directions and distances travelled (for reviews see <xref ref-type="bibr" rid="bib32">Heinze et al., 2018</xref>; <xref ref-type="bibr" rid="bib11">Collett, 2019</xref>), and visual memory (VM), whereby foragers derive a homing signal by comparing the difference between current and stored views (for reviews see <xref ref-type="bibr" rid="bib98">Zeil, 2012</xref>; <xref ref-type="bibr" rid="bib10">Collett et al., 2013</xref>). Neurophysiological and computational modelling studies advocate the central complex neuropil (CX) as the PI centre (<xref ref-type="bibr" rid="bib33">Heinze and Homberg, 2007</xref>; <xref ref-type="bibr" rid="bib74">Seelig and Jayaraman, 2015</xref>; <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref>), whereas the mushroom body neuropils (MB) appear well suited to assessing visual valence as needed for VM (<xref ref-type="bibr" rid="bib37">Heisenberg, 2003</xref>; <xref ref-type="bibr" rid="bib1">Ardin et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Müller et al., 2018</xref>). Yet, two key gaps in our understanding remain. Firstly, although current VM models based on the MB architecture can replicate route following (RF) behaviours whereby insects visually recognise the direction previously travelled at the same position (<xref ref-type="bibr" rid="bib1">Ardin et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Müller et al., 2018</xref>), they cannot account for visual homing (VH) behaviours whereby insects return directly to their familiar surroundings from novel locations following a displacement (e.g. after being blown off course by a gust of wind) (<xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref>). Secondly, despite increasing neuroanatomical evidence suggesting that premotor regions of the CX coordinate navigation behaviour (<xref ref-type="bibr" rid="bib67">Pfeiffer and Homberg, 2014</xref>; <xref ref-type="bibr" rid="bib36">Heinze and Pfeiffer, 2018</xref>; <xref ref-type="bibr" rid="bib41">Honkanen et al., 2019</xref>), a theoretical hypothesis explaining how this is achieved by the neural circuitry has yet to be developed. In this work, we present a unified neural navigation model that extends the core guidance modules from two (PI and VM) to three (PI, RF, and VH) and by integrating their outputs optimally using a biologically realistic ring attractor network in the CX produces realistic homing behaviours.</p><p>The foremost challenge in realising this goal is to ensure that the core guidance subsystems provide sufficient directional information across conditions. Contemporary VM models based on the MBs can replicate realistic RF behaviours in complex visual environments (ant environments: <xref ref-type="bibr" rid="bib52">Kodzhabashev and Mangan, 2015</xref>; <xref ref-type="bibr" rid="bib1">Ardin et al., 2016</xref>, bee environments: <xref ref-type="bibr" rid="bib59">Müller et al., 2018</xref>) but do not generalise to visual homing scenarios whereby the animal must return directly to familiar terrain from novel locations (ants: <xref ref-type="bibr" rid="bib61">Narendra, 2007</xref>, bees: <xref ref-type="bibr" rid="bib7">Cartwright and Collett, 1982</xref>, wasps: <xref ref-type="bibr" rid="bib79">Stürzl et al., 2016</xref>). Storing multiple nest-facing views before foraging, inspired by observed learning walks in ants (<xref ref-type="bibr" rid="bib60">Müller and Wehner, 2010</xref>; <xref ref-type="bibr" rid="bib21">Fleischmann et al., 2016</xref>) and flights in bees and wasps (<xref ref-type="bibr" rid="bib96">Zeil et al., 1996</xref>; <xref ref-type="bibr" rid="bib99">Zeil and Fleischmann, 2019</xref>), provides a potential solution (<xref ref-type="bibr" rid="bib26">Graham et al., 2010</xref>; <xref ref-type="bibr" rid="bib91">Wystrach et al., 2013</xref>), but simulation studies have found this approach to be brittle due to high probabilities of aligning with the wrong memory causing catastrophic errors (<xref ref-type="bibr" rid="bib16">Dewar et al., 2014</xref>). Moreover, ants released perpendicularly to their familiar route do not generally align with their familiar visual direction as predicted by the above algorithms (<xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref>), but instead move directly back towards the route (<xref ref-type="bibr" rid="bib23">Fukushi and Wehner, 2004</xref>; <xref ref-type="bibr" rid="bib53">Kohler and Wehner, 2005</xref>; <xref ref-type="bibr" rid="bib61">Narendra, 2007</xref>; <xref ref-type="bibr" rid="bib56">Mangan and Webb, 2012</xref>; <xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref>), which would require a multi-stage mental alignment of views for current models. New computational hypothesis are thus required that can guide insects directly back to their route (often moving perpendicularly to the habitual path), but also allow for the route direction to be recovered (now aligned with the habitual path) upon arrival at familiar surroundings (see <xref ref-type="fig" rid="fig1">Figure 1A</xref> ‘Zero Vector’).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of the unified navigation model and it’s homing capabilities.</title><p>(<bold>A</bold>) The homing behaviours to be produced by the model when displaced either from the nest and having no remaining PI home vector (zero vector), or from the nest with a full home vector (full vector). Distinct elemental behaviours are distinguished by coloured path segments, and stripped bands indicate periods where behavioural data suggests that multiple strategies are combined. Note that this colour coding of behaviour is maintained throughout the remaining figures to help the reader map function to brain region. (<bold>B</bold>) The proposed conceptual model of the insect navigation toolkit from sensory input to motor output. Three elemental guidance systems are modelled in this paper: path integration (PI), visual homing (VH) and route following (RF). Their outputs must then be coordinated in an optimal manner appropriate to the context before finally outputting steering command. (<bold>C</bold>) The unified navigation model maps the elemental guidance systems to distinct processing pathways: <italic>RF</italic>: OL - &gt; AOTU - &gt; BU - &gt; CX; <italic>VH</italic>: OL - &gt; MB - &gt; SMP - &gt; CX; <italic>PI</italic>: OL - &gt; AOTU - &gt; BU - &gt; CX. The outputs are then optimally integrated in the proposed ring attractor networks of the FB in CX to generate a single motor steering command. Connections are shown only for the left brain hemisphere for ease of visualisation but in practice are mirrored on both hemispheres. Hypothesised or assumed pathways are indicated by dashed lines whereas neuroanatomically supported pathways are shown by solid lines (a convention maintained throughout all figures). <italic>OL</italic>: optic lobe, <italic>AOTU</italic>: anterior optic tubercle, <italic>CX</italic>: central complex, <italic>PB</italic>: protocerebrum bridge, <italic>FB</italic>: fan-shape body (or <italic>CBU</italic>: central body upper), <italic>EB</italic>: ellipsoid body (or <italic>CBL</italic>: central body lower), <italic>MB</italic>: mushroom body, <italic>SMP</italic>: superior medial protocerebrum, <italic>BU</italic>: bulb. Images of the brain regions are adapted from the insect brain database <ext-link ext-link-type="uri" xlink:href="https://www.insectbraindb.org">https://www.insectbraindb.org</ext-link>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54026-fig1-v2.tif"/></fig><p>With the necessary elemental guidance systems defined, a unifying model must then convert the various directional recommendations into a single motor command appropriate to the context (<xref ref-type="bibr" rid="bib14">Cruse and Wehner, 2011</xref>; <xref ref-type="bibr" rid="bib38">Hoinville et al., 2012</xref>; <xref ref-type="bibr" rid="bib10">Collett et al., 2013</xref>; <xref ref-type="bibr" rid="bib87">Webb, 2019</xref>). Behavioural studies show that when in unfamiliar visual surroundings (‘Off-Route’) insects combine the outputs of their PI and VH systems (<xref ref-type="bibr" rid="bib8">Collett, 1996</xref>; <xref ref-type="bibr" rid="bib6">Bregy et al., 2008</xref>; <xref ref-type="bibr" rid="bib9">Collett, 2012</xref>) relative to their respective certainties consistent with optimal integration theory (<xref ref-type="bibr" rid="bib54">Legge et al., 2014</xref>; <xref ref-type="bibr" rid="bib92">Wystrach et al., 2015</xref>; <xref ref-type="fig" rid="fig1">Figure 1A</xref> ‘Full Vector’). Upon encountering their familiar route, insects readily recognise their surroundings, recover their previous bearing and retrace their familiar path home (<xref ref-type="bibr" rid="bib30">Harrison et al., 1989</xref>; <xref ref-type="bibr" rid="bib53">Kohler and Wehner, 2005</xref>; <xref ref-type="bibr" rid="bib89">Wystrach et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Mangan and Webb, 2012</xref>). Thus, the navigation coordination model must posses two capabilities: (a) output a directional signal consistent with the optimal integration of PI and VH when Off-Route (b) switch from Off-Route (PI and VH) to On-Route (RF) strategies when familiar terrain is encountered. Mathematical models have been developed that reproduce aspects of cue integration in specific scenarios (<xref ref-type="bibr" rid="bib14">Cruse and Wehner, 2011</xref>; <xref ref-type="bibr" rid="bib39">Hoinville and Wehner, 2018</xref>), but to date no neurobiologically constrained network revealing how insects might realise these capabilities has been developed.</p><p>To address these questions a functional modelling approach is followed that extends the current base model described by <xref ref-type="bibr" rid="bib87">Webb, 2019</xref> to (a) account for the ability of ants to home from novel locations back to the familiar route before retracing their familiar path the rest of the journey home, and (b) propose a neurally based model of the central complex neuropil that integrates competing cues optimally and generates a simple steering command that can drive behaviour directly. Performance is bench-marked by direct comparison to behavioural data reported by <xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref> (showing different navigation behaviours on and off the route), <xref ref-type="bibr" rid="bib54">Legge et al., 2014</xref>; <xref ref-type="bibr" rid="bib92">Wystrach et al., 2015</xref> (demonstrating optimal integration of PI and VM), and through qualitative comparison to extended homing paths where insects switch between strategies according to the context (<xref ref-type="bibr" rid="bib61">Narendra, 2007</xref>). Biological realism is enforced by constraining models to the known anatomy of specific brain areas, but where no data exists an exploratory approach is taken to investigate the mechanisms that insects may exploit. <xref ref-type="fig" rid="fig1">Figure 1A</xref> depicts the adaptive behaviours observed in animals that we wish to replicate accompanied by a functional overview of our unified model of insect navigation (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) mapped to specific neural sites (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Mushroom bodies as drivers of rotational invariant visual homing</title><p>For ants to return directly to their familiar route after a sideways displacement (<xref ref-type="fig" rid="fig1">Figure 1A</xref> 'Zero Vector') without continuous mental or physical realignment they require access to rotational invariant visual cues. <xref ref-type="bibr" rid="bib78">Stone et al., 2018</xref> recently demonstrated that binary images of panoramic skylines converted into their frequency components can provide such a rotationally-invariant encoding of scenes in a compact form (see Image processing for an introduction to frequency transformations of images). Moreover, they demonstrated that the difference between the rotationally invariant features (the amplitudes of the frequency coefficients) between two locations increases monotonically with distance producing an error surface reminiscent of the image difference surfaces reported by <xref ref-type="bibr" rid="bib97">Zeil et al., 2003</xref> which can guide an agent back to familiar terrain. Here we investigate whether the MB neuropils shown capable of assessing the visual valence of learned rotationally-varying panoramic skylines for RF (<xref ref-type="bibr" rid="bib1">Ardin et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Müller et al., 2018</xref>), might instead assess the visual valence of rotationally-invariant properties of views sampled along a familiar route supporting visual homing.</p><p>To this end, the intensity sensitive input neurons of <xref ref-type="bibr" rid="bib1">Ardin et al., 2016</xref>’s MB model are replaced with input neurons encoding rotational invariant amplitudes (<xref ref-type="fig" rid="fig2">Figure 2A</xref> left, blue panel). The network is trained along an <inline-formula><mml:math id="inf1"><mml:mrow><mml:mn>11</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> curved route in a simulated world that mimics the training regime of ants in <xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref> (see Materials and methods and Reproduce visual navigation behaviour for details on simulated world, image processing, model architecture and training and test regime). After training, the firing rate of the MB output neuron (MBON) when placed at locations across the environment at random orientations reveals a gradient that increases monotonically with distance from the familiar route area, providing a homing signal sufficient for VH independent of the animal’s orientation (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Visual homing in the insect brain.</title><p>(<bold>A</bold>) Neural model of visual homing. Rotational-invariant amplitudes are input to the MB calyx which are then projected to the Kenyon cells (KCs) before convergence onto the MB output neuron (MBON) which seeks to memorise the presented data via reinforcement-learning-based plasticity (for more details see Visual homing) (MB circuit: left panels). SMP neurons measure positive increases in visual novelty (through input from the MBON) which causes a shift between the current heading (green cells) and desired headings (red cells) in the rings of the CX (SMP pathway between MB and CX: centre panel; CX circuit: right panels). The CX-based steering circuit then computes the relevant turning angle. Example activity profiles are shown for an increase in visual novelty, causing a shift in desired heading and a command to change direction. Each model component in all figures is labelled with a shaded star to indicate what aspects are new versus those incorporated from previous models (see legend in upper left). (<bold>B</bold>) Schematic of the steering circuit function. First the summed differences between the impact of 45 °left and right turns on the desired heading and the current heading are computed. By comparing the difference between the resultant activity profiles allows an appropriate steering command to be generated. (<bold>C</bold>) Schematic of the visual homing model. When visual novelty drops (<inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) the desired heading is an unshifted copy of the current heading so the current path is maintained but when the visual novelty increases (<inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf5"><mml:mi>t</mml:mi></mml:math></inline-formula>) the desired heading is shifted from the current heading. (<bold>D</bold>) The firing rate of the MBON sampled across locations at random orientations is depicted by the heat-map showing a clear gradient leading back to the route. The grey curve shows the habitual route along which ants were trained. RP (release point) indicates the position where real ants in <xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref> were released after capture at the nest (thus zero-vector) and from which simulations were started. The ability of the VH model to generate realistic homing data is shown by the initial paths of simulated ants which closely match those of real ants (see inserted polar plot showing the mean direction and 95% confidential interval), and also the extended exampled path shown (red line). Note that once the agent arrives in the vicinity of the route, it appears to meander due the flattening of visual novelty gradient and the lack of directional information.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>The frequency information for the locations with random orientations across the world.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-54026-fig2-data1-v2.mat"/></supplementary-material></p><p><supplementary-material id="fig2sdata2"><label>Figure 2—source data 2.</label><caption><title>The visual homing results of the model.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-54026-fig2-data2-v2.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54026-fig2-v2.tif"/></fig><p>Motor output is then generated by connecting the MBON to a steering network recently located in the fan-shaped body (FB/CBU) of the CX that functions by minimising the difference between the animal’s current and desired headings (<xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref>). <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref>’s key insight was that the anatomically observed shifts of activity in the columnar neurons that encode the desired heading in essence simulate 45° turns left and right, and thus by comparing the summed differences between the activity profiles of these predicted headings to the current heading then the appropriate turning command can be computed (see <xref ref-type="fig" rid="fig2">Figure 2B</xref>). We adopt this circuit as the basis for computing steering commands for all strategies as suggested by <xref ref-type="bibr" rid="bib41">Honkanen et al., 2019</xref>.</p><p>In the proposed VH model the current heading input to the steering circuit uses the same celestial global compass used in <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref>’s PI model. Insects track their orientation through head-direction cells <xref ref-type="bibr" rid="bib74">Seelig and Jayaraman, 2015</xref> whose concurrent firing pattern forms a single bump of activity that shifts around the ring as the animal turns (measured through local visual [<xref ref-type="bibr" rid="bib27">Green et al., 2017</xref>; <xref ref-type="bibr" rid="bib85">Turner-Evans et al., 2017</xref>], global visual (<xref ref-type="bibr" rid="bib33">Heinze and Homberg, 2007</xref>) and proprioceptive (<xref ref-type="bibr" rid="bib74">Seelig and Jayaraman, 2015</xref>) cues). Neuroanatomical data (<xref ref-type="bibr" rid="bib49">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib86">Turner-Evans et al., 2019</xref>; <xref ref-type="bibr" rid="bib68">Pisokas et al., 2019</xref>) supports theoretical predictions (<xref ref-type="bibr" rid="bib13">Cope et al., 2017</xref>; <xref ref-type="bibr" rid="bib47">Kakaria and de Bivort, 2017</xref>) that the head-direction system of insects follows a ring attractor (RA) connectivity pattern characterised by local excitatory interconnections between direction selective neurons and global inhibition. In this work, the global compass RA network is not modelled directly but rather we simulate its sinusoidal activity profile in a ring of I-TB1 (locusts and <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> of flies) neurons found in the protocerebral bridge (PCB/PB) (<xref ref-type="fig" rid="fig2">Figure 2A</xref> green ring) (see Current headings).</p><p>A desired heading is then generated by copying the current activity pattern of the global compass neurons to a new neural ring which we speculate could reside in either a distinct subset of I-TB1 neurons (<xref ref-type="bibr" rid="bib5">Beetz et al., 2015</xref>) or in the FB. Crucially, the copied activity profile also undergoes a leftward shift proportional to any increase in visual novelty (a similar shifting mechanisms has been proposed for the head-direction system [<xref ref-type="bibr" rid="bib27">Green et al., 2017</xref>; <xref ref-type="bibr" rid="bib85">Turner-Evans et al., 2017</xref>]) which we propose is measured by neurons in the superior medial protocerebrum (SMP) (<xref ref-type="bibr" rid="bib2">Aso et al., 2014</xref>; <xref ref-type="bibr" rid="bib69">Plath et al., 2017</xref>) (see <xref ref-type="fig" rid="fig2">Figure 2A</xref> centre and activity of red rings). The result is a mechanism that recommends changing direction when the agent moves away from familiar terrain (visual novelty increases) but recommends little change to the current heading when the visual novelty is decreasing (see <xref ref-type="fig" rid="fig2">Figure 2C</xref> for a schematic of the VH mechanism). We note that there is a distinction between a ring network which describes a group of neurons whose pattern of activity forms a circular representation regardless of actual physical arrangement and RA networks which follow a specific connectivity pattern (all modelled RAs labelled in figures). Taken together the model iteratively refines it’s orientation to descend the visual novelty gradient and thus recover familiar terrain (see <xref ref-type="fig" rid="fig2">Figure 2A</xref> for full model).</p><p><xref ref-type="fig" rid="fig2">Figure 2D</xref> demonstrates that the proposed network accurately replicates both the directed initial paths as in <xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref> (see the inserted black arrow), and extended homing paths as in <xref ref-type="bibr" rid="bib61">Narendra, 2007</xref> observed in ants displaced to novel locations perpendicular to their familiar routes. We note that upon encountering the route the model is unable to distinguish the direction in which to travel and thus meanders back and forth along the familiarity valley, unlike real ants, demonstrating the need for additional route recognition and recovery capabilities.</p></sec><sec id="s2-2"><title>Optimally integrating visual homing and path integration</title><p>We have demonstrated how ants could use visual cues to return to the route in the absence of PI but in most natural scenarios (e.g. displacement by a gust of wind) ants will retain a home vector readout offering an alternative, and often conflicting, guidance cue to that provided by VH. In such scenarios, desert ants strike a comprise by integrating their PI and VH outputs in a manner consistent with optimal integration theory by weighting VH relative to the familiarity of the current view (<xref ref-type="bibr" rid="bib54">Legge et al., 2014</xref>) and PI relative to the home vector length (a proxy for directional certainty) (<xref ref-type="bibr" rid="bib92">Wystrach et al., 2015</xref>).</p><p>Various ring-like structures of the CX represent directional cues as bumps of activity with the peak defining the specific target direction, and the spread providing a mechanism to encode cue certainty as required for optimal integration (for an example see increased spread of HD cell activity when only proprioceptive cues are present [<xref ref-type="bibr" rid="bib74">Seelig and Jayaraman, 2015</xref>]). Besides their excellent properties to encode the animal’s heading ring attractors also provide a biologically realistic means to optimally weight cues represented in this format (<xref ref-type="bibr" rid="bib84">Touretzky, 2005</xref>; <xref ref-type="bibr" rid="bib57">Mangan and Yue, 2018</xref>) without the need for dedicated memory circuits to store means and uncertainties of each cue.</p><p>Thus we introduce a pair of integrating ring-attractor networks to the CX model (<xref ref-type="fig" rid="fig3">Figure 3A</xref> grey neural rings: RA_L and RA_R) that take as input the desired headings from the above proposed VH model (red neural rings: VH_L and VH_R) and <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref>’s PI model (orange neural rings: PI_L and PI_R) and output combined Off Route desired heading signals that are sent to the steering circuits (blue neural rings: CPU_L and CPU_R). <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref> mapped the home vector computation to a population of neurons (CPU4) owing to their dual inputs from direction selective compass neurons (I_TB1) and motion-sensitive speed neurons (TN2) as well as their recurrent connectivity patterns facilitating accumulation of activity as the animal moves in a given direction. <xref ref-type="bibr" rid="bib92">Wystrach et al., 2015</xref> showed that the certainty of PI automatically scales with the home-vector length owing to the accumulating effect of the memory neurons which correlates with directional uncertainty, and thus the output PI network is directly input to the ring attractor circuits. In our implementation the VH input has a fixed height and width profile and influences the integration through tuning neurons (TUN) (see the plotted activation function in <xref ref-type="fig" rid="fig3">Figure 3B</xref> and Optimal cue integration) that we suggest reside in the SMP and modulate the PI input to the integration network. Altering the weighting in this manner rather than by scaling the VH input independently allows VH to dominate the integrated output at sites with high visual familiarity even in the presence of a large home vector without having large stored activity. We note, however, that both approaches remain feasible and further neuroanatomical data is required to clarify which, if either, mechanism is employed by insects.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Optimal cue integration in the CX.</title><p>(<bold>A</bold>) Proposed model for optimally integrating PI and VH guidance systems. In each hemisphere, ring attractors (RAs) (grey neural rings) (speculatively located in FB/CBU) receive the corresponding inputs from PI (orange neural rings) and VH (red neural rings) with the outputs sent to the corresponding steering circuits (blue neural rings). Integration is weighted by the visual novelty tracking tuning neuron (TUN) whose activation function is shown in the leftmost panel. (<bold>B</bold>) Examples of optimal integration of PI and VH headings for two PI states with the peak stable state (grey dotted activity profile in the integration neurons) shifting towards VH as the home vector length recedes. (<bold>C</bold>) Replication of optimal integration studies of <xref ref-type="bibr" rid="bib92">Wystrach et al., 2015</xref> and <xref ref-type="bibr" rid="bib54">Legge et al., 2014</xref>. Simulated ants are captured at various points (0.1 m, 1 m, 3 m and 7 m) along their familiar route (grey curve) and released at release point 1 (RP1) thus with the same visual certainty but with different PI certainties as in <xref ref-type="bibr" rid="bib92">Wystrach et al., 2015</xref> (see thick orange arrow). The left polar plot shows the initial headings of simulated ants increasingly weight their PI system (270°) in favour of their VH system (135°) as the home vector length increases and PI directional uncertainty drops. Simulated ants are also transferred from a single point 1 m along their familiar route to ever distant release points (RP1, RP2, RP3) thus with the same PI certainty but increasingly visual uncertainty as in <xref ref-type="bibr" rid="bib54">Legge et al., 2014</xref> (see thick red arrow). The right polar plot shows the initial headings of simulated ants increasingly weight PI (270°) over VH (135°) as visual certainty drops. (see Reproduce the optimal cue integration behaviour for details) (<bold>D</bold>) Example homing paths of the independent and combined guidance systems displaced from the familiar route (grey) to a fictive release point (RP).</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>The results of tuning PI uncertainty.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-54026-fig3-data1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig3sdata2"><label>Figure 3—source data 2.</label><caption><title>The results of tuning VH uncertainty.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-54026-fig3-data2-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig3sdata3"><label>Figure 3—source data 3.</label><caption><title>The extended homing path of PI, VH and combined PI and VH.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-54026-fig3-data3-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54026-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>The extended homing paths and the PI memory in the simulations.</title><p>(<bold>A</bold>) The extended homing path of 20 agents released at RP1 in <xref ref-type="fig" rid="fig3">Figure 3B</xref> with different home vector length. (<bold>B</bold>) The activation of CPU4 neurons (PI memory) encoding home vectors with different lengths from 0 to 7.0 m. (<bold>C</bold>) The extended homing paths of 20 agents released at RP2 and RP3 in <xref ref-type="fig" rid="fig3">Figure 3B</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54026-fig3-figsupp1-v2.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig3">Figure 3C</xref> shows the initial headings produced by the model which replicates the trends reported in cue-conflict experiments by <xref ref-type="bibr" rid="bib54">Legge et al., 2014</xref> and <xref ref-type="bibr" rid="bib92">Wystrach et al., 2015</xref> when the uncertainty of PI and VH cues were altered independently. Example extended paths of independent PI and VH models and the ring-attractor-based combined PI and VH model are plotted in <xref ref-type="fig" rid="fig3">Figure 3D</xref> with the combined model showing the most ant-like behaviour (<xref ref-type="bibr" rid="bib53">Kohler and Wehner, 2005</xref>; <xref ref-type="bibr" rid="bib56">Mangan and Webb, 2012</xref>) by initially following predominantly the home-vector direction before switching to visual homing when the home-vector length drops leading the simulated ant back to familiar terrain. Note that the PI-only and PI+VH models are drawn back towards their fictive nest sites indicated by their home vectors which if left to run would likely result in emergent search-like patterns as in <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref>. Moreover, upon encountering the route the VH-based models (VH-only and PI+VH) are unable to distinguish the direction in which to travel and hence again (see meander around the valley of familiarity <xref ref-type="fig" rid="fig2">Figure 2D</xref> and <xref ref-type="fig" rid="fig3">Figure 3D</xref>) further demonstrating a need for a route recovery mechanism.</p></sec><sec id="s2-3"><title>Route following in the insect brain</title><p>The model described above can guide insects back to their familiar route area, but lacks the means to recover the route direction upon arrival as observed in homing insects. This is not surprisingly as VH relies upon translationally-varying but rotational-invariant information whereas RF requires rotationally varying cues. Thus we introduce a new elemental guidance system that makes use of the rotationally-varying <italic>phase</italic> coefficients of the frequency information derived from the panoramic skyline which tracks the orientation of specific features of the visual surroundings (see Materials and methods). Here, we ask whether by associating the rotationally invariant amplitudes (shown useful for place recognition) with the rotationally-varying <italic>phases</italic> experienced at those locations, insects might recover the familiar route direction.</p><p>Neuroanatomical data with which to constrain a model remains sparse and therefore a standard artificial neural network (ANN) architecture is used to investigate the utility of <italic>phase</italic>-based route recovery with biological plausibility discussed in more detail below. A three-layer ANN was trained to associate the same 81 rotational-invariant amplitudes as used in the VH model with the rotational varying <italic>phase</italic> value of single frequency coefficient experienced when travelling along the habitual route which we encode in an eight neuron-ring (see <xref ref-type="fig" rid="fig4">Figure 4A</xref> and Route Following for detailed model description). Thus, when the route is revisited the network should output the orientation that the <italic>phase</italic> converged upon when at the same location previously, which we note is not necessarily aligned with the actual heading of the animal (e.g. it may track the orientation to vertical bar [<xref ref-type="bibr" rid="bib74">Seelig and Jayaraman, 2015</xref>]). Realignment is possible using the same steering mechanism as described above but which seeks to reduce the offset between the current <italic>phase</italic> readout (e.g. a local compass locked onto visual features of the animals surroundings), and the recalled <italic>phase</italic> readout from the ANN.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Phase-based route following.</title><p>(<bold>A</bold>) Neural model. The visual pathway from the optic lobe via AOTU and Bulb to EB of the CX is modelled by a fully connected artificial neural network (ANN) with one hidden layer. The input layer receives the amplitudes of the frequency encoded views (as for the MB network) and the output layer is an 8-neuron ring whose population encoding represents the desired heading against to which the agent should align. (<bold>B</bold>) Behaviours. Blue and red arrows in the inserted polar plot (top left) display the mean directions and 95% confidential intervals of the initial headings of real (<xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref>) and simulated ants released at the start of the route <inline-formula><mml:math id="inf7"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, respectively. Dark blue curves show the routes followed by the model when released at five locations close to the start of the learned path. The overlaid fan-plots indicate the circular statistics (the mean direction and 95% confidential interval) of the homing directions recommended by the model when sampled across heading directions (20 samples at 18°intervals). Data for entire rotations are shown on the right for specific locations with the upper plot, sampled at <inline-formula><mml:math id="inf8"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1.5</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, demonstrating accurate phase-based tracking of orientation, whereas the lower plot sampled at <inline-formula><mml:math id="inf9"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>2.5</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>3.5</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> shows poor tracking performance and hence produces a wide fan-plot.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>The frequency tracking performance across the world.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-54026-fig4-data1-v2.mat"/></supplementary-material></p><p><supplementary-material id="fig4sdata2"><label>Figure 4—source data 2.</label><caption><title>The RF model results of the agents released on route.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-54026-fig4-data2-v2.mat"/></supplementary-material></p><p><supplementary-material id="fig4sdata3"><label>Figure 4—source data 3.</label><caption><title>The RF model results of the agents released aside from the route.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-54026-fig4-data3-v2.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54026-fig4-v2.tif"/></fig><p>We speculate that the most likely neural pathways for the new desired and current headings are from Optic Lobe via Anterior Optic Tubercle (AOTU) and Bulb (BU) to EB (CBL) of the CX (<xref ref-type="bibr" rid="bib40">Homberg et al., 2003</xref>; <xref ref-type="bibr" rid="bib64">Omoto et al., 2017</xref>) (see <xref ref-type="fig" rid="fig4">Figure 4A</xref>) with the desired heading terminating in the EB, whereas the current heading continues to the PB forming a local compass that sits beside the global compass used by PI and VH systems. This hypothesis is further supported by the recently identified parallel pathways from OL via AOTU to the CX in <italic>Drosophila</italic> (<xref ref-type="bibr" rid="bib83">Timaeus et al., 2020</xref>). That’s to say that, firstly, there are two parallel pathways forming two compass systems- the global (here based on celestial cues) and the local (based on terrestrial cues) compasses modelled by the activation of I-TB1 and II-TB1 neurons, respectively. Four classes of CL1 neurons (or E-PG and P-EG neurons) <xref ref-type="bibr" rid="bib35">Heinze and Homberg, 2009</xref>; <xref ref-type="bibr" rid="bib94">Xu et al., 2020</xref> and three classes of independent TB1 neurons <xref ref-type="bibr" rid="bib5">Beetz et al., 2015</xref> have been identified that provide potential sites for the parallel recurrent loops encoding independent local and global compasses. Secondly, the desired heading, which is the recalled <italic>phase</italic> of a specific view, is generated through the neural plasticity from AOTU to BU and BU to EB, which is line with recent evidence of associative learning between the R-neurons transmitting visual information from BU to EB and the compass neurons (CL1a or E-PG neurons) that receive input from EB (<xref ref-type="bibr" rid="bib50">Kim et al., 2019</xref>; <xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>). This kind of learning endows the animal with the ability to flexibly adapt their local compass and also desired navigational orientation according to the changing visual surroundings. <xref ref-type="bibr" rid="bib29">Hanesch et al., 1989</xref> reported a direct pathway from EB to FB neurons which we model to allow comparison of the local compass activity (II-TB1) with the desired heading. However, we note that this connectivity has not been replicated in recent studies <xref ref-type="bibr" rid="bib34">Heinze and Homberg, 2008</xref> and thus further investigation of potential pathways is required.</p><p>The RF model accurately recovers the initial route heading in a similar manner to real ants returned to the start of their familiar route (<xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref>; <xref ref-type="fig" rid="fig4">Figure 4B</xref>, insert), and then follows the remaining route in its entirety back to the nest again reflecting ant data (<xref ref-type="bibr" rid="bib53">Kohler and Wehner, 2005</xref>; <xref ref-type="bibr" rid="bib56">Mangan and Webb, 2012</xref>; <xref ref-type="fig" rid="fig4">Figure 4B</xref>). The quiver plots displayed in the background of <xref ref-type="fig" rid="fig4">Figure 4B</xref> show the preferred homing direction output by the ANN when rotated on the spot across locations in the environment. The noise in the results are due to errors in the tracking performance (see examples <xref ref-type="fig" rid="fig4">Figure 4B</xref> right) yet as these errors are in largely confined to the magnitude, the steering circuit still drives the ant along the route. We note that this effect is primarily a function of the specific frequency transformation algorithm used which we borrow from computer graphics to investigate the utility of frequency encoding of visual information. The biological realism of such transforms and their potential implementation in the insect vision system are addressed in the Discussion. The displaced routes also highlight the danger of employing solely RF which often shadows rather than converges with the route when displaced sideways, further demonstrating the necessity for integration with the Off-Route strategies that promote route convergence.</p></sec><sec id="s2-4"><title>Route recovery through context-dependent modulation of guidance systems</title><p>Homing insects readily recognise familiar route surroundings, recover their bearing, and retrace their habitual path home, irrespective of the status of other guidance system such as PI. Replicating such context-dependent behavioural switching under realistic conditions is the final task for the proposed model. The visual novelty measured by the MBON provides an ideal signal for context switching with low output when close to the route when RF should dominate versus high output further away from the route when PI and VH should be engaged (see <xref ref-type="fig" rid="fig2">Figure 2D</xref>). Also the fact that Off-route strategies (PI and VH) compute their turning angles with reference to the global compass whereas the On-route RF strategy is driven with reference to a local compass provides a means to modulate their inputs to the steering circuit independently. This is realised through a non-linear weighting of the On and Off-route strategies which we propose acts through the same SMP pathway as the VH model (see the SN1 and SN2 neurons in <xref ref-type="fig" rid="fig5">Figure 5A</xref>) (see Context-dependent switch for neuron details and <xref ref-type="fig" rid="fig6">Figure 6</xref> for a force-directed graph representation of the final unified model).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Unified model realising the full array of coordinated navigational behaviours.</title><p>(<bold>A</bold>) Context-dependent switching is realised using two switching neurons (SN1, SN2) that have mutually exclusive firing states (one active while the other is in active) allowing coordination between On and Off-Route strategies driven by the instantaneous visual novelty output by the MB. Connectivity and activation functions of the SMP neurons are shown in the left side of panel. (<bold>B</bold>) Activation history of the SN1, SN2 and TUN (to demonstrate the instantaneous visual novelty readout of the MB) neurons during the simulated displacement trials. (<bold>C</bold>) Paths generated by the unified model under control of the context-dependent switch circuit during simulated FV (solid line) and ZV (dashed line) displacement trials.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>The navigation results of the whole model.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-54026-fig5-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54026-fig5-v2.tif"/></fig><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>The detailed neural connections of the proposed model.</title><p>(<bold>A</bold>): The detailed neural connections of the navigation coordination system. (<bold>B</bold>): The neural connection of the route following network. The input layer to the hidden layer is fully connected, so does the hidden layer to the output layer. (<bold>C</bold>): The network generating the visual homing memory. (<bold>D</bold>): The detailed neural connection of the ring attractor network for optimal cue integration.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54026-fig6-v2.tif"/></fig><p>The activity of the proposed switching circuit and the paths that it generates in simulated zero vector and full vector displacement trials are shown in <xref ref-type="fig" rid="fig5">Figure 5B and C</xref> respectively. In the full vector trial (<xref ref-type="fig" rid="fig5">Figure 5B</xref> (upper), <xref ref-type="fig" rid="fig5">Figure 5C</xref> (solid line)) as visual novelty is initially high (see high TUN activity until step 78) SN2 is activated which enables Off-Route strategies (PI and VH) while SN1 (always the inverse of SN2) is deactivated which disables On-Route strategies. Note that it is the integration of PI and VH that generates the direct path back to the route area in the FV trial: PI recommends moving at a 45° bearing but VH prevents ascension of the visual novelty gradient that this would cause with the compromise being a bearing closer to 90° that is toward the route. As the route is approached the visual novelty decreases (again see TUN activity), until at step 78 SN2 falls below threshold and deactivates the Off-Route strategies while conversely SN1 activates and engages On-Route strategies. After some initial flip-flopping while the agents converges on the route (steps 78–85) RF becomes dominant and drives the agent back to the nest via the familiar path. In the zero vector trial (<xref ref-type="fig" rid="fig5">Figure 5B</xref> (lower), (<xref ref-type="fig" rid="fig5">Figure 5B</xref> (dashed line)) Off-route strategies (here only VH) largely dominate (some false positive route recognition (e.g step 60)) until the route is recovered (step 93), at which point the same flip-flopping during route convergence occurs (steps 93–96) followed by RF alone which returns the agent to the nest via the familiar path. It should be noted that the data presented utilised different activation functions of the TUN neuron that weights PI and VH (see <xref ref-type="table" rid="table1">Table 1</xref> for parameter settings across trials and Discussion for insights into model limitations and potential extensions), yet the results presented nevertheless provide a proof-of-principle demonstration that the proposed unified navigation model can fulfil all of the criteria defined for replication of key adaptive behaviour observed in insects (<xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>The detailed parameters settings for the simulations.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Para.</th><th>Visual homing</th><th>Optimal integration <break/>tuning PI</th><th>Optimal integration <break/>tuning VH</th><th>Route following</th><th>Whole model ZV</th><th>Whole model FV</th></tr></thead><tbody><tr><td><inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>(<xref ref-type="disp-formula" rid="equ14">Equation 14</xref>)</td><td>0.04</td><td>0.04</td><td>0.04</td><td>0.04</td><td>0.04</td><td>0.04</td></tr><tr><td><inline-formula><mml:math id="inf11"><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>(<xref ref-type="disp-formula" rid="equ16">Equation 16</xref>)</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td></tr><tr><td><inline-formula><mml:math id="inf12"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>(<xref ref-type="disp-formula" rid="equ19">Equation 19</xref>)</td><td>2.0</td><td>2.0</td><td>2.0</td><td>/</td><td>0.5</td><td>0.5</td></tr><tr><td><inline-formula><mml:math id="inf13"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ28">Equation 28</xref>)</td><td>/</td><td>0.1</td><td>0.1</td><td>/</td><td>0.025</td><td>0.0125</td></tr><tr><td><inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>(<xref ref-type="disp-formula" rid="equ32">Equation 32</xref>)</td><td>/</td><td>/</td><td>/</td><td>/</td><td>2.0</td><td>3.0</td></tr><tr><td><inline-formula><mml:math id="inf15"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>(<xref ref-type="disp-formula" rid="equ35">Equation 35</xref>)</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.375</td><td>0.375</td></tr><tr><td><inline-formula><mml:math id="inf16"><mml:msub><mml:mi>S</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula> (cm/step) (<xref ref-type="disp-formula" rid="equ39">Equation 39</xref>)</td><td valign="middle">4</td><td valign="middle">4</td><td valign="middle">4</td><td valign="middle">4</td><td valign="middle">8</td><td valign="middle">8</td></tr><tr><td>initial heading (deg)</td><td>0~360</td><td>0~360</td><td>0~360</td><td>0 / 180</td><td>90</td><td>0</td></tr></tbody></table></table-wrap></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>This work addresses two gaps in the current understanding of insect navigation: what are the core visual guidance systems required by the insect navigational toolkit? And how are they coordinated by the insect brain?</p><p>We propose that the insect navigation toolkit (<xref ref-type="bibr" rid="bib88">Wehner, 2009</xref>; <xref ref-type="bibr" rid="bib87">Webb, 2019</xref>) should be extended to include independent visual homing (VH) and route following (RF) systems (see <xref ref-type="fig" rid="fig1">Figure 1B</xref> for updated Insect Navigation Toolkit). We show how VH and RF can be realised using frequency-encoding of panoramic skylines to separate information into rotationally invariant amplitudes for VH and rotationally varying <italic>phases</italic> for RF. The current model utilises frequency encoding schema from the computer graphics but behavioural studies support the use of spatial frequency by bees (<xref ref-type="bibr" rid="bib42">Horridge, 1997</xref>; <xref ref-type="bibr" rid="bib55">Lehrer, 1999</xref>), with neurons in the lobula of dragonflies (<xref ref-type="bibr" rid="bib62">O'Carroll, 1993</xref>) and locusts <xref ref-type="bibr" rid="bib44">James and Osorio, 1996</xref> found to have receptive fields akin to basis functions, providing a mechanism by which to extract the frequency information necessary for the local compass system. Our model allows for this information extraction process to happen at multiple stages ahead of its usage in the central learning sites such as the MBs opening the possibility for its application in either the optic lobes or subsequent pathways through regions such as the AOTU. Further, neurophysiological data is required to pinpoint both the mechanisms and sites of this data processing in insects. Similarly, following <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref> the global compass signal directly mimics the firing pattern of compass neurons in the CX without reference to sensory input but <xref ref-type="bibr" rid="bib24">Gkanias et al., 2019</xref> recently presented a plausible neural model of the celestial compass processing pipeline that could be easily integrated into the current model to fill this gap. Follow-on neuroanatomically constrained modelling of the optic lobes presents the most obvious extension of this work allowing the neural pathway from sensory input to motor output signal to be mapped in detail. Conversely, modelling the conversion of direction signals into behaviour via motor generating mechanisms such as central pattern generators (see <xref ref-type="bibr" rid="bib75">Steinbeck et al., 2020</xref>) will then allow closure of the sensory-motor loop.</p><p>Visual homing is modelled on neural circuits found along the OL-MB-SMP pathway (<xref ref-type="bibr" rid="bib17">Ehmer and Gronenberg, 2002</xref>; <xref ref-type="bibr" rid="bib28">Gronenberg and López-Riquelme, 2004</xref>) before terminating in the CX steering circuit (<xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref>) and shown capable of producing realistic homing paths. In this schema, the MBs do not measure rotationally varying sensory valence as recently used to replicate RF (<xref ref-type="bibr" rid="bib1">Ardin et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Müller et al., 2018</xref>), but rather the spatially varying (but rotationally invariant) sensory valence more suited to gradient descent strategies such as visual homing (<xref ref-type="bibr" rid="bib97">Zeil et al., 2003</xref>; <xref ref-type="bibr" rid="bib78">Stone et al., 2018</xref>) and other taxis behaviours (<xref ref-type="bibr" rid="bib93">Wystrach et al., 2016</xref>). This is inline with the hypothesis forwarded by <xref ref-type="bibr" rid="bib12">Collett and Collett, 2018</xref> that suggest that the MBs output ‘whether’ the current sensory stimulus is positive or negative and the CX then adapts the animal heading, the ‘whither’, accordingly.</p><p>Route following is shown possible by learned associations between the amplitudes (i.e. the place) and the <italic>phase</italic> (the orientation) experienced along a route, allowing realignment when later at a proximal location. This kind of neural plasticity-based correlation between the visual surroundings and the orientations fits with data recently observed in fruit flies (<xref ref-type="bibr" rid="bib50">Kim et al., 2019</xref>; <xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>). These studies provide the neural explanation for the animal’s ability to make flexible use of visual information to navigate while the proposed model gives a detailed implementation of such ability in the context of insect’s route following schema. Neurophysiological evidence suggests that the layered visual pathway from OL via AOTU and BU to the EB of the CX (<xref ref-type="bibr" rid="bib4">Barth and Heisenberg, 1997</xref>; <xref ref-type="bibr" rid="bib40">Homberg et al., 2003</xref>; <xref ref-type="bibr" rid="bib64">Omoto et al., 2017</xref>) with its suggested neural plasticity properties (<xref ref-type="bibr" rid="bib4">Barth and Heisenberg, 1997</xref>; <xref ref-type="bibr" rid="bib95">Yilmaz et al., 2019</xref>) provides a possible neural pathway but further analysis is needed to identify the circuit structures that might underpin the generation of RF desired heading. In addition to the desired heading, the current heading of RF is derived from the local compass system anchored to animal’s immediate visual surroundings. This independent compass system may be realised parallel to the global compass system in an similar but independent circuit (<xref ref-type="bibr" rid="bib35">Heinze and Homberg, 2009</xref>; <xref ref-type="bibr" rid="bib5">Beetz et al., 2015</xref>; <xref ref-type="bibr" rid="bib94">Xu et al., 2020</xref>). Our model therefore hypothesises that insects possess different compass systems based on varied sensory information and further that insects possess the capability (via CX-based RAs) to coordinate their influence optimally according to the current context. Since the global compass, the local compass and the desired heading of RF share the same visual pathway (OL-&gt;AOTU-&gt;BU-&gt;CX), distinct input and output patterns along this pathway may be found by future neuroanatomical studies. In addition, in the proposed model, the activation of current heading and desired heading of RF overlap in the EB, and therefore separation of activation profiles representing each output (e.g. following methods in <xref ref-type="bibr" rid="bib74">Seelig and Jayaraman, 2015</xref>) presents another meaningful topic for future neurophysiological research.</p><p>Closed-loop behavioural studies during which the spatial frequency information of views is altered (similar to <xref ref-type="bibr" rid="bib66">Paulk et al., 2015</xref>) coincident with imaging of key brain areas (<xref ref-type="bibr" rid="bib73">Seelig and Jayaraman, 2013</xref>) offers a means to investigate which neural structures make use of what visual information. Complimentary behavioural experiments could verify the distinct VH and RF systems by selectively blocking the proposed neural pathways with impacts on behaviour predicted by <xref ref-type="fig" rid="fig2">Figure 2C</xref> and <xref ref-type="fig" rid="fig4">Figure 4B</xref>, respectively. <xref ref-type="bibr" rid="bib63">Ofstad et al., 2011</xref> report that visual homing abilities are lost for fruit flies with a blocked EB of the CX but not MB, which is predicted by our model if animals have learned target-facing views to which they can later align using their RF guidance system. Analysis of animal’s orientation during learning is thus vital to unpacking precisely how the above results arise.</p><p>With the elemental guidance strategies defined, we propose that their outputs are coordinated through the combined action of the MBs and CX. Specifically, we demonstrate that a pair of ring attractor networks that have similar connectivity patterns of the CX-based head-direction system (<xref ref-type="bibr" rid="bib49">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib86">Turner-Evans et al., 2019</xref>; <xref ref-type="bibr" rid="bib68">Pisokas et al., 2019</xref>), are sufficient for optimally weighting multiple directional cues from the same frame of reference (e.g. VH and PI). The use of a pair of integrating RAs is inspired by the column structure of the FB which has 16 neural columns divided into two groups of 8 neural columns that each represent the entire 360°space. The optimal integration of PI and VH using a ring attractor closely matches the networks theorised to govern optimal directional integration in mammals (<xref ref-type="bibr" rid="bib45">Jeffery et al., 2016</xref>) and supports hypothesis of their conserved use across animals (<xref ref-type="bibr" rid="bib57">Mangan and Yue, 2018</xref>). Optimality is secured either through adapting the shape of the activity profile of the input as is the case for PI which naturally scales with distance, or by using a standardised input activity profile with cross-inhibition of competing cues as is the case for VH in the model. The later schema avoids the need for ever increasing neural activity to maintain relevance.</p><p>To replicate the suite of navigational behaviours described in <xref ref-type="fig" rid="fig1">Figure 1</xref>, our network includes three independent ring attractor networks: the global compass head direction system <xref ref-type="bibr" rid="bib68">Pisokas et al., 2019</xref>; the local compass head direction system (<xref ref-type="bibr" rid="bib74">Seelig and Jayaraman, 2015</xref>; <xref ref-type="bibr" rid="bib49">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib86">Turner-Evans et al., 2019</xref>); and an Off-route integration system (modelled here). We would speculate that it is likely that central place foraging insects also possess a similar integration network for ‘On-Route’ cues (not modelled here) bringing the total number of RAs to four. The utility of RAs for head-direction tracking arises from their properties in converging activity to a signal bump that can easily be shifted by sensory input and is maintained in the absence of stimulation. In addition, RAs also possess the beneficial property that they spontaneously weight competing sensory information stored as bumps of activity in an optimal manner. Thus, there are excellent computational reasons for insects to invest in such neural structures. Yet, it should be clear that the model proposed here represents a proof-of-concept demonstrating that the underlying network architectures already mapped to the CX (directional cues encoded as bumps of activity <xref ref-type="bibr" rid="bib74">Seelig and Jayaraman, 2015</xref>; <xref ref-type="bibr" rid="bib33">Heinze and Homberg, 2007</xref>; various lateral shifting mechanisms (<xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref>; <xref ref-type="bibr" rid="bib27">Green et al., 2017</xref>; <xref ref-type="bibr" rid="bib85">Turner-Evans et al., 2017</xref>); RAs [<xref ref-type="bibr" rid="bib49">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib86">Turner-Evans et al., 2019</xref>; <xref ref-type="bibr" rid="bib68">Pisokas et al., 2019</xref>]) are sufficient to generate adaptive navigation but further studies are required to critique and refine the biological realism of this hypothesis.</p><p>While this assemblage recreates optimal integration of strategies that share a compass system, it does not easily extend to integration of directional cues from other frames of reference (e.g. VH and PI reference the global compass versus RF that references a local compass). Indeed as the CX steering network seeks to minimise the difference between a current and a desired heading, calibrating input signals from different frames of reference would require a similar calibration of their respective compass systems. Rather, the proposed model incorporates a context-dependent non-linear switching mechanism driven by the output of the MB that alternates between strategies: global compass based PI and VH are triggered when the surroundings are unfamiliar, but when in familiar surroundings engage local compass-based RF. In summary, the adaptive behaviour demonstrated is the result of distinct guidance systems that converge in the CX, with their relative weighting defined by the output of the MB. This distributed architecture is reminiscent of mechanisms found in the visual learning of honeybees (<xref ref-type="bibr" rid="bib69">Plath et al., 2017</xref>), and supports the hypothesis that the CX is the navigation coordinator of insects (<xref ref-type="bibr" rid="bib31">Heinze, 2017</xref>; <xref ref-type="bibr" rid="bib41">Honkanen et al., 2019</xref>) but shows how the MB acts as a mediator allowing the CX to generate optimal behaviour according to the context.</p><p>The resultant unified model of insect navigation <xref ref-type="fig" rid="fig1">Figure 1B and C</xref> represents a proof-of-principle framework as to how insects might co-ordinate core navigational behaviours (PI, VH and RF) under standard field manipulations <xref ref-type="fig" rid="fig1">Figure 1A</xref>. Neuroanatomical data has been drawn from across insect classes (see <xref ref-type="table" rid="table2">Table 2</xref>) to ensure neural realism where possible with performance compared to ant navigation behaviour in a single simulated desert ant habitat. The framework can be easily extended to new navigation behaviours observed in other insects from idiothetic PI (<xref ref-type="bibr" rid="bib51">Kim and Dickinson, 2017</xref>) to straight line following <xref ref-type="bibr" rid="bib19">El Jundi et al., 2016</xref> to migrations (<xref ref-type="bibr" rid="bib70">Reppert et al., 2016</xref>) as well as more nuanced strategies that flexibly use directional cues from different sensory modalities (<xref ref-type="bibr" rid="bib91">Wystrach et al., 2013</xref>; <xref ref-type="bibr" rid="bib72">Schwarz et al., 2017</xref>; <xref ref-type="bibr" rid="bib15">Dacke et al., 2019</xref>). A priority of future works should be the investigation of the differences and commonalities in sensory systems, neural structures and ecology of different insect navigators and how they impact behaviour allowing for extension and refinement of the framework for different animals. Complementary stress-testing of models across different environments in both simulation and robotic studies are also required to ensure that model performance generalises across species and habitats and to provide guidance to researchers seeking the sensory, processing and learning circuits underpinning these abilities.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>The details of the main neurons used in the proposed model.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Name</th><th>Function</th><th>Num</th><th>Network</th><th>Brain region</th><th>Neuron in species(e.g.)</th><th>Reference</th></tr></thead><tbody><tr><td>I-TB1</td><td>Global compasscurrent heading</td><td>8</td><td>Ring attractor</td><td align="center" rowspan="12" valign="middle">CX</td><td>TB1 in Schistocerca gregariaand Megalopta genalis</td><td><xref ref-type="bibr" rid="bib34">Heinze and Homberg, 2008</xref>; <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref></td></tr><tr><td>II-TB1</td><td>Local compasscurrent heading</td><td>8</td><td>Ring attractor</td><td>Δ7 in <italic>Drosophila</italic></td><td><xref ref-type="bibr" rid="bib22">Franconville et al., 2018</xref></td></tr><tr><td>S I-TB1</td><td>Copy of shiftedglobal heading</td><td>8</td><td>Ring</td><td>No data</td><td rowspan="3">/</td></tr><tr><td>VH-L</td><td>VH desiredheading left</td><td>8</td><td>Ring</td><td>No data</td></tr><tr><td>VH-R</td><td>VH desiredheading right</td><td>8</td><td>Ring</td><td>No data</td></tr><tr><td>PI-L</td><td>PI desiredheading left</td><td>8</td><td>Ring</td><td>CPU4 in Schistocerca gregariaand Megalopta genalis</td><td><xref ref-type="bibr" rid="bib34">Heinze and Homberg, 2008</xref>; <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref></td></tr><tr><td>PI-R</td><td>PI desiredheading right</td><td>8</td><td>Ring</td><td>P-F3N2v in <italic>Drosophila</italic></td><td><xref ref-type="bibr" rid="bib22">Franconville et al., 2018</xref></td></tr><tr><td>RF-L</td><td>RF desiredheading left</td><td>8</td><td>Ring</td><td>No data</td><td rowspan="4">/</td></tr><tr><td>RF-R</td><td>RF desiredheading right</td><td>8</td><td>Ring</td><td>No data</td></tr><tr><td>RA-L</td><td>Cue integrationleft</td><td>8</td><td>Ring attractor</td><td>No data</td></tr><tr><td>RA-R</td><td>Cue integrationright</td><td>8</td><td>Ring attractor</td><td>No data</td></tr><tr><td>CPU1</td><td>Comparing thecurrent anddesired heading</td><td>16</td><td>Steering circuit</td><td>CPU1 in Schistocerca gregaria and Megalopta genalis PF-LCre in <italic>Drosophila</italic></td><td><xref ref-type="bibr" rid="bib34">Heinze and Homberg, 2008</xref>; <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref><xref ref-type="bibr" rid="bib22">Franconville et al., 2018</xref></td></tr><tr><td>vPN</td><td>visual projection</td><td>81</td><td rowspan="3" valign="middle">Associative learning</td><td align="center" rowspan="3" valign="middle">MB</td><td>MB neurons in <italic>Drosophila</italic></td><td><xref ref-type="bibr" rid="bib2">Aso et al., 2014</xref></td></tr><tr><td>KCs</td><td>Kenyon cells</td><td>4000</td><td>Camponotus</td><td><xref ref-type="bibr" rid="bib18">Ehmer and Gronenberg, 2004</xref></td></tr><tr><td>MBON</td><td>visual novelty</td><td>1</td><td>Apis mellifera</td><td><xref ref-type="bibr" rid="bib71">Rybak and Menzel, 1993</xref></td></tr><tr><td>TUN</td><td>Tuning weightsfrom PI to RA</td><td>1</td><td>/</td><td align="center" rowspan="3" valign="middle">SMP</td><td>No data</td><td rowspan="3">/</td></tr><tr><td>SN1</td><td>Turn on/off theRF output to CPU1</td><td>1</td><td>Switch circuit</td><td>No data</td></tr><tr><td>SN2</td><td>Turn on/off theRA output to CPU1</td><td>1</td><td>Switch circuit</td><td>No data</td></tr></tbody></table></table-wrap></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All source code related to this publication is available for download at <ext-link ext-link-type="uri" xlink:href="https://github.com/XuelongSun/InsectNavigationToolkitModelling">https://github.com/XuelongSun/InsectNavigationToolkitModelling</ext-link> (<xref ref-type="bibr" rid="bib81">Sun et al., 2020</xref> ; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/InsectNavigationToolkitModelling">https://github.com/elifesciences-publications/InsectNavigationToolkitModelling</ext-link>). All simulations and network models are implemented by Python 3.5 and make use of external libraries-<italic>numpy</italic>, <italic>matplotlib</italic>, <italic>scipy</italic>, <italic>PIL</italic> and <italic>cv2</italic>.</p><sec id="s4-1"><title>Simulated 3D world</title><p>The environment used in this study is that provided by <xref ref-type="bibr" rid="bib78">Stone et al., 2018</xref> which is itself adapted from <xref ref-type="bibr" rid="bib3">Baddeley et al., 2012</xref> (see <xref ref-type="fig" rid="fig7">Figure 7C</xref>). It is a virtual ant-like world consisting of randomly generated bushes, trees and tussocks based on triangular patches (for more details see <xref ref-type="bibr" rid="bib3">Baddeley et al., 2012</xref>). Therefore, the data of this simulated world is stored in a matrix with the size of <inline-formula><mml:math id="inf17"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, defining the three dimensional coordinates (x,y,z) of the three vertices of <inline-formula><mml:math id="inf18"><mml:msub><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> (number of patches) triangle patches. Agent movement was constrained to a <inline-formula><mml:math id="inf19"><mml:mrow><mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> training and test area allowing free movement without the requirement of an additional obstacle avoidance mechanism.</p></sec><sec id="s4-2"><title>Image reconstruction</title><p>The agent’s visual input at location <inline-formula><mml:math id="inf20"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with the heading direction <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>θ</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> is simulated from a point 1 cm above from the ground plane with field of view <inline-formula><mml:math id="inf22"><mml:msup><mml:mn>360</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> wide by <inline-formula><mml:math id="inf23"><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> high (centred on the horizon). This panoramic image (<inline-formula><mml:math id="inf24"><mml:mrow><mml:mn>300</mml:mn><mml:mo>×</mml:mo><mml:mn>104</mml:mn></mml:mrow></mml:math></inline-formula>) is then wrapped onto a sky-centred disk as required by the Zernike Moments transformation algorithm used with the size of <inline-formula><mml:math id="inf25"><mml:mrow><mml:mrow><mml:mn>208</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>104</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mn>208</mml:mn></mml:mrow></mml:math></inline-formula> ready for image processing (see <xref ref-type="fig" rid="fig7">Figure 7D</xref> upper).</p></sec><sec id="s4-3"><title>Image processing</title><sec id="s4-3-1"><title>Frequency encoding conceptual overview</title><p>Image compression algorithms such as JPEG encoding <xref ref-type="bibr" rid="bib43">Hudson et al., 2018</xref> have long utilised the fact that a complex signal can be decomposed into a series of trigonometric functions that oscillate at different frequencies. The original signal can then be reconstructed by summing all (for prefect reconstruction) or some (for approximate reconstruction) of the base trigonometric functions. Thus, compression algorithms seek a balance between using the fewest trigonometric functions to encode the scene (for example, by omitting high frequencies that humans struggle to perceive), and the accuracy of the reconstructed signal (often given as an option when converting to JPEG format). <xref ref-type="fig" rid="fig7">Figure 7A</xref> provides a cartoon of the frequency decomposition process for a panoramic view.</p><p>When such transforms are applied to fully panoramic images, or skylines, benefits beyond compression arise. Specifically, discrete transformation algorithms used to extract the frequency information generate a series of information triplets to describe the original function: frequency coefficients describe the frequency of the trigonometric function with associated amplitudes and <italic>phase</italic> values defining the vertical height versus the mean and the lateral position of the waveform respectively (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). For panoramic views, regardless of the rotational angle of the image capturing device (eye or camera) the entire signal will always be visible and hence the amplitudes of the frequency coefficients do not alter with rotation (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). This information has been used for successful place recognition in a series of robot studies (<xref ref-type="bibr" rid="bib65">Pajdla and Hlaváč, 1999</xref>; <xref ref-type="bibr" rid="bib58">Menegatti et al., 2004</xref>; <xref ref-type="bibr" rid="bib76">Stone et al., 2016</xref>). Most recently <xref ref-type="bibr" rid="bib78">Stone et al., 2018</xref> demonstrated that the difference between the amplitudes of the frequency coefficients recorded at two locations increases monotonically with distance producing an error surface suitable for visual homing. This feature of the frequency encoding underlies the visual homing results described in Mushroom bodies as drivers of rotational invariant visual homing.</p><p>In addition, as the <italic>phase</italic> of each coefficient describes how to align the signal this will naturally track any rotation in the panoramic view (<xref ref-type="fig" rid="fig7">Figure 7B</xref>) providing a means to realign with previous headings. The <italic>phase</italic> components of panoramic images have been utilised previously to derive the home direction in a visual homing task (<xref ref-type="bibr" rid="bib80">Stürzl and Mallot, 2006</xref>). This feature of the frequency encoding underlies the route following results described in Route following in the insect brain.</p><p>The image processing field has created an array of algorithms for deriving the frequency content of continuous signals (<xref ref-type="bibr" rid="bib46">Jiang et al., 1996</xref>; <xref ref-type="bibr" rid="bib25">Gonzalez et al., 2004</xref>). To allow exploration of the usefulness of frequency information, and how it could be used by the known neural structures, we adopt the same Zernike Moment algorithm used by <xref ref-type="bibr" rid="bib78">Stone et al., 2018</xref>, but the reader should be clear that there are many alternate and more biologically plausible processes by which insects could derive similar information. It is beyond the scope of this proof of concept study to define precisely how this process might happen in insects but future research possibilities are outlined in the Discussion.</p></sec><sec id="s4-3-2"><title>Zernike Moments encoding</title><p>Zernike Moments (ZM) are defined as the projection of a function onto orthogonal basis polynomials called Zernike polynomials (<xref ref-type="bibr" rid="bib82">Teague, 1980</xref>; <xref ref-type="bibr" rid="bib48">Khotanzad and Hong, 1990</xref>). This set of functions are defined on the unit circle with polar coordinates <inline-formula><mml:math id="inf26"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> shown as:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the order and <inline-formula><mml:math id="inf28"><mml:mi>m</mml:mi></mml:math></inline-formula> is the repetition meeting the condition: <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf30"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is even to ensure the rotational invariant property is met. <inline-formula><mml:math id="inf32"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the radial polynomial defined as:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>!</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For a continuous image function <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the ZM coefficient can be calculated by:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>π</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">𝑑</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">𝑑</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For a digital image, summations can replace the integrals to give the ZM:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>π</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>x</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>y</mml:mi></mml:munder><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="22.5pt">,</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>ZM are extracted from the simulated insect views in wrapped format (<xref ref-type="fig" rid="fig7">Figure 7D</xref>) whose centre is taken to be the origin of the polar coordinates such that all valid pixels lie within the unit circle. For a given image <inline-formula><mml:math id="inf34"><mml:mi>I</mml:mi></mml:math></inline-formula> (P1 in <xref ref-type="fig" rid="fig7">Figure 7D</xref>) and the rotated version of this image <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, (P2 in <xref ref-type="fig" rid="fig7">Figure 7D</xref>), the <bold>amplitude</bold> <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic>phase</italic><inline-formula><mml:math id="inf37"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">∠</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of ZM coefficients of these two images will satisfy:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>e</mml:mi><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd/></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>From which we can see that the <bold>amplitude</bold> of the ZM coefficient remains the same while the <italic>phase</italic> of ZM carries the information regarding the rotation (see <xref ref-type="fig" rid="fig7">Figure 7A and D</xref>). This property is the cornerstone of the visual navigation model where the <bold>amplitudes</bold> encode the features of the view while the <italic>phase</italic> defines the orientation.</p><p>Amplitudes for ZM orders ranging from <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math></inline-formula> were selected as they appeared to cover the majority of information within the image. From <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, we know that <inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, so we limited <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> to reduce the computational cost, which sets the total number of ZM coefficients (<inline-formula><mml:math id="inf42"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) to <inline-formula><mml:math id="inf43"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>16</mml:mn><mml:mo>÷</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>81</mml:mn></mml:mrow></mml:math></inline-formula> which was input to the visual navigation networks. For training the ANN network for RF, in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>, if we set <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, such that <inline-formula><mml:math id="inf45"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> which means that all ZM coefficients will provide the same information when the image is rotated. Further, the difference between the <italic>phase</italic> of ZM coefficients of the current view with those of the memorised view, will inherently provide the angle with which to turn to realign oneself, that is:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>where the order <inline-formula><mml:math id="inf46"><mml:mi>n</mml:mi></mml:math></inline-formula> of this ZM is selected to be <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> manually by comparing the performance with different orders in this specific virtual environment, <inline-formula><mml:math id="inf48"><mml:msub><mml:mi>θ</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> is the current heading of the agent while <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> is the memorised heading direction (desired heading direction).</p></sec></sec><sec id="s4-4"><title>Neural networks</title><p>We use the simple firing rate to model the neurons in the proposed networks, where the output firing rate <inline-formula><mml:math id="inf50"><mml:mi>C</mml:mi></mml:math></inline-formula> is a sigmoid function of the input <inline-formula><mml:math id="inf51"><mml:mi>I</mml:mi></mml:math></inline-formula> if there is no special note. In the following descriptions and formulas, a subscript is used to represent the layers or name of the neuron while the superscript is used to represent the value at a specific time or with a specific index.</p><sec id="s4-4-1"><title>Current headings</title><p>In the proposed model, there are two independent compass systems based on the global and the local cues respectively so named global and local compass correspondingly. These two compass systems have similar neural pathways from OL via AOTU and BU to the CX but ended distinct groupings of TB1 neurons: I-TB1 and II-TB1 in the PB.</p><sec id="s4-4-1-1"><title>Global compass</title><p>The global compass neural network applied in this study is the same as that of <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref>, which has three layers of neurons: TL neurons, CL1 neurons and I-TB1 neurons. The 16 TL neurons respond to simulated polarised light input and are directly modelled as:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the angular preference of the 16 TL-neurons. The 16 CL1-neurons are inhibited by TL-neuron activity which invert the polarisation response:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.0</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The 8 I-TB1 neurons act as a ring attractor creating a sinusoidal encoding of the current heading. Each I-TB1 neuron receives excitation from the CL1 neuron sharing the same directional preference and inhibition from other I-TB1 neurons via mutual connections:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>8</mml:mn></mml:munderover><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf53"><mml:mi>c</mml:mi></mml:math></inline-formula> is a balance factor to modify the strength of the inhibition and the CL1 excitation. Finally, the population coding <inline-formula><mml:math id="inf54"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the heading of global compass of the agent at time <inline-formula><mml:math id="inf55"><mml:mi>t</mml:mi></mml:math></inline-formula>.</p></sec></sec></sec><sec id="s4-5"><title>Local compass</title><p>The local compass is derived from the terrestrial cues through a similar visual pathway as the global compass and also ends in a ring attractor network. As for the global compass, the local compass heading is directly modelled by the population encoding of II-TB1 neurons:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is the angular preference of the II-TB1 neurons and <inline-formula><mml:math id="inf57"><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the <italic>phase</italic> of ZM. Therefore, the firing rate of <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> encodes the heading of the local compass.</p><sec id="s4-5-1"><title>Visual homing</title><p>The neural network of visual homing is an associative network constrained by the anatomical structure of the mushroom body (MB) of the insects. In contrast to <xref ref-type="bibr" rid="bib1">Ardin et al., 2016</xref> where a spiking neural network is implemented to model the MB, we apply a simple version of MB where the average firing rates of neurons are used.</p><p>The visual projection neurons (vPNs) directly receive the <bold>amplitudes</bold> of the ZM coefficients as their firing rates:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mo rspace="22.5pt">,</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf59"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of the vPN neurons which is the same as the total number of ZM <bold>amplitudes</bold> applied and in this study <inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>81</mml:mn></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id="inf61"><mml:msup><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:math></inline-formula> denotes the <inline-formula><mml:math id="inf62"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> <bold>amplitudes</bold> of ZM coefficients.</p><p>The vPNs project into Kenyon cells (KC) through randomly generated binary connections <inline-formula><mml:math id="inf63"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which result in the scenario wherein one KC receives 10 randomly selected vPNs’ activation:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:munderover><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf64"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:math></inline-formula> denotes the total input current of <inline-formula><mml:math id="inf65"><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> KC from the vPN and the KCs are modelled as binary neurons with the same threshold <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mspace width="2em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1em"/><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mspace width="2em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1em"/><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd/></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The MBON neuron sums all the activation of Kenyon cells via plastic connections <inline-formula><mml:math id="inf67"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:munderover><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>An anti-Hebbian learning rule is applied for the plasticity of <inline-formula><mml:math id="inf68"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in a simple way:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>≥</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the learning rate. The learning process will happen only when the reward signal is turned on. The activation of EN <inline-formula><mml:math id="inf70"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the familiarity of the current view and the change of the <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined as:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf72"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is used to track the gradient of the familiarity to guide the agent to the more familiar locations by shifting the I-TB1 neurons’ activation <inline-formula><mml:math id="inf73"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>.<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mi>B</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:mn>7</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mtd><mml:mtd><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mspace width="2em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mn>.7</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The relationship between the <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and the <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> is shown as following:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1em"/><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>O</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">⌊</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>O</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⌋</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-5-2"><title>Path integration</title><p>The PI model implemented is that published by <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref>. The core functionality arises from the CPU4 neurons that integrate the activation of TN2 neurons that encode the speed of the agent and the inverted activation of direction-sensitive I-TB1 neurons. The result is that the population of CPU4 neurons iteratively track the distance and orientation to the nest (a home vector) in a format akin to a series of directionally locked odometers.</p><p>The firing rate of the CPU4 neurons are updated by:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where the rate of the memory accumulation <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0025</mml:mn></mml:mrow></mml:math></inline-formula>; the memory loss <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>; the initial memory charge of CPU4 neurons <inline-formula><mml:math id="inf78"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>The input of the TN2 neurons encoding the speed is calculated by:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mn>2</mml:mn><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">𝒗</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">𝒗</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable><mml:mi/></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf79"><mml:mi mathvariant="bold-italic">𝒗</mml:mi></mml:math></inline-formula> is the velocity (see <xref ref-type="disp-formula" rid="equ39">Equation 39</xref>) of the agent and <inline-formula><mml:math id="inf80"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the preference angle of the TN2 neurons. In this study <inline-formula><mml:math id="inf81"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The activation function applied to TN2 neurons is the rectified linear function given by:<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As CPU4 neurons integrate the speed and direction of the agent, the desired heading of PI can be represented by the population encoding of these neurons, thus:<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-5-3"><title>Route following</title><p>The route following model is based on a simple artificial neural network (ANN) with just one hidden layer. The input layer directly takes the amplitudes of the ZM coefficients as the activation in the same way as that of visual projection neurons in MB network. This is a fully connected neural network with the sigmoid activation function, so the forward propagation is ruled by:<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mn>.7</mml:mn><mml:mspace width="1em"/><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mspace width="1em"/><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf82"><mml:msubsup><mml:mi>Z</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula> denote the input and output of the <inline-formula><mml:math id="inf84"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> neuron in <inline-formula><mml:math id="inf85"><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> layer, thus the input is the same as the MB network <inline-formula><mml:math id="inf86"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and the output of the ANN is consequently the population coding of the RF desired heading, that is:<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For a fast and efficient implementation, the learning method applied here is back propagation with gradient descend. Training data is derived from the amplitudes and the population encoded <italic>phases</italic> of the ZM coefficients of the images reconstructed along a habitual route. As shown in <xref ref-type="disp-formula" rid="equ11">Equation 11</xref> the II-TB1 neurons encode the heading of local compass, therefore, the training pair for the RF network can be defined as <inline-formula><mml:math id="inf87"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. After training, this network will correlate the desired ZM <italic>phase</italic> with the specific ZM amplitudes, and when RF is running, the output of this neural network <inline-formula><mml:math id="inf88"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> will represent the desired heading with respect to the current heading of the local compass represented by the population encoding of II-TB1 neurons.</p></sec><sec id="s4-5-4"><title>Coordination of elemental guidance strategies</title><p>The coordination of the three main navigation strategies PI, VH and RF are realised in distinct stages. Firstly, Off-route strategies (PI and VH) are optimally integrated by weighing according to the certainly of each before a context-dependent switch activates either On-route (RF) or Off-route strategies depending on the current visual novelty.</p></sec></sec><sec id="s4-6"><title>Optimal cue integration</title><p>A ring attractor neural network is used to integrate the cues from the VH and PI guidance systems. As reported in <xref ref-type="bibr" rid="bib39">Hoinville and Wehner, 2018</xref> summation of directional cues represented in vector format leads to optimal angular cue integration which is the same case as real insects. <xref ref-type="bibr" rid="bib57">Mangan and Yue, 2018</xref> gave a biology plausible way to do this kind of computation based on a simple ring attractor neural network. There are two populations of neurons in this network, the first is the integration neurons (IN) which is the output population of the network. Constrained by the number of columns in each hemisphere of the insects CX, we set the number of the IN to be 8, and its firing rate is updated by:<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf89"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the recurrent connections from <inline-formula><mml:math id="inf90"><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> neuron to <inline-formula><mml:math id="inf91"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> neuron, <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the activation function that provides the non-linear property of the neuron:<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf93"><mml:mi>ρ</mml:mi></mml:math></inline-formula> denotes the offset of the function.</p><p>In <xref ref-type="disp-formula" rid="equ26">Equation 26</xref>, <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf95"><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> generally denote the cues that should be integrated. In this study, <inline-formula><mml:math id="inf96"><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf97"><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> represent the desired heading of path integration (<inline-formula><mml:math id="inf98"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and visual homing (<inline-formula><mml:math id="inf99"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). The desired heading of PI is also tuned by the tuning neuron (TUN) in SMP which is stimulated by the MBON of MB (see <xref ref-type="fig" rid="fig3">Figure 3A</xref>) and its activation function is defined by a rectified linear function, that is:<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the scaling factor.</p><p>Thus, the <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> for this ring attractor network can be calculated by:<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>U</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd/></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mn>.7</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The second population of the ring attractor is called the uniform inhibition (UI) neurons modelled by:<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>After arriving at a stable state, the firing rate of the integration neurons in this ring attractor network provides the population encoding of the optimal integrated output <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-7"><title>Context-dependent switch</title><p>The model generates two current/desired headings pairs: the current heading of global compass decoded by <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> with the desired heading optimally integrated by the integration neurons of the ring attractor network <inline-formula><mml:math id="inf105"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the current heading of local compass decoded by II-TB1 neurons <inline-formula><mml:math id="inf106"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> with the desired heading decoded by the output of the RF network <inline-formula><mml:math id="inf107"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. These two pairs of signal both are connected to the steering circuit (see <xref ref-type="fig" rid="fig5">Figure 5A</xref> and Steering circuit) but are turned on/off by two switching neurons (SN1 and SN2) in the SMP (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). SN2 neuron receives the activation from MBON neuron and is modelled as:<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mn>2</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1em"/><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>O</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd><mml:mtd/></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>While SN1 will always fire unless SN2 fires:<disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1em"/><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd><mml:mtd/></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Therefore, the context-depend switch is achieved according to the current visual novelty represented by the activation of MBON.</p><sec id="s4-7-1"><title>Steering circuit</title><p>The steering neurons, that is CPU1 neurons (<inline-formula><mml:math id="inf108"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) receive excitatory input from the desired heading (<inline-formula><mml:math id="inf109"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) and inhibitory input from the current heading (<inline-formula><mml:math id="inf110"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) to generate the turning signal:<disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The turning angle is determined by the difference of the activation summations between left (<inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) and right (<inline-formula><mml:math id="inf112"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) set of CPU1 neurons:<disp-formula id="equ35"><label>(35)</label><mml:math id="m35"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>7</mml:mn></mml:munderover><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow><mml:mn>15</mml:mn></mml:munderover><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>which corresponds to the difference of the length of the subtracted left and right vectors in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. In addition, as it is illustrated in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, another key part of steering circuit is the left/right shifted desired heading, in this paper, this is achieved by the offset connectivity pattern (<inline-formula><mml:math id="inf113"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf114"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) from the desired heading to the steering neurons (<xref ref-type="bibr" rid="bib34">Heinze and Homberg, 2008</xref>; <xref ref-type="bibr" rid="bib77">Stone et al., 2017</xref>):<disp-formula id="equ36"><label>(36)</label><mml:math id="m36"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>-</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable><mml:mi/></mml:mrow></mml:math></disp-formula></p><p>Where the <inline-formula><mml:math id="inf115"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf116"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are:<disp-formula id="equ37"><label>(37)</label><mml:math id="m37"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>0</mml:mn><mml:mo separator="true"> </mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>which defines the connection pattern realising the left/right shifting of the desired headings used throughout our model (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="fig" rid="fig3">Figure 3A</xref>, <xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig5">Figure 5A</xref> and <xref ref-type="fig" rid="fig6">Figure 6A</xref>.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Information provided by frequency encoding in cartoon and simulated ant environments.</title><p>(<bold>A</bold>): A cartoon depiction of a panoramic skyline, it’s decomposition into trigonometric functions, and reconstruction through the summation of low frequency coefficients reflecting standard image compression techniques. (<bold>B</bold>): Following a 90° rotation there is no change in the amplitudes of the frequency coefficients but the <italic>phases</italic> of the frequency coefficients track the change in orientation providing a rotational invariant signal useful for visual homing and rotationally-varying signal useful for route following, respectively. (<bold>C</bold>): The simulated 3D world used for all experiments. The pink area (size: <inline-formula><mml:math id="inf117"><mml:mrow><mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>) is used for model training and testing zone for models allowing obstacle-free movement. (<bold>D</bold>): The frequency encoding (Zernike Moment’s amplitudes and <italic>phase</italic>) of the views sampled from the same location but with different headings (P1 and P2 in (<bold>C</bold>), with <inline-formula><mml:math id="inf118"><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> heading difference) in the simulated world. The first 81 amplitudes are identical while the <italic>phases</italic> have the difference of about <inline-formula><mml:math id="inf119"><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula>.</p><p><supplementary-material id="fig7sdata1"><label>Figure 7—source data 1.</label><caption><title>The matrix of simulated 3D world.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-54026-fig7-data1-v2.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54026-fig7-v2.tif"/></fig><p>The current heading input to the steering circuit is also switched between global and local compass input via the SN1 and SN2 neuron:<disp-formula id="equ38"><label>(38)</label><mml:math id="m38"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>-</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable><mml:mi/></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="s4-8"><title>Detailed neural connectivity of unified model</title><p><xref ref-type="fig" rid="fig6">Figure 6A</xref> shows a complete picture of the proposed model. Specifically, it highlights the final coordination system showing that CX computing the optimal navigation output with the modulation from the MB and SMP. In addition, offset connectivity pattern from the desired heading to the steering circuit that underpin the left/right shifting is clearly shown. <xref ref-type="fig" rid="fig6">Figure 6B and C</xref> shows the network generating the desired heading of RF and VH respectively.</p><p>In addition, <xref ref-type="table" rid="table2">Table 2</xref> provides details of all modelled neural circuits with their function and naming conventions with links to biological evidence for these neural circuits where it exists and the animal that they were observed in.</p></sec><sec id="s4-9"><title>Simulations</title><p><xref ref-type="disp-formula" rid="equ35">Equation 35</xref> gives the turning angle of the agent, thus the instantaneous &quot;velocity&quot; (<inline-formula><mml:math id="inf120"><mml:mi mathvariant="bold-italic">𝒗</mml:mi></mml:math></inline-formula>) at every step can be computed by:<disp-formula id="equ39"><label>(39)</label><mml:math id="m39"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">𝒗</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf121"><mml:msub><mml:mi>S</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula> is the step length with the unit of centimetres. Note that we haven’t defined the time accuracy for every step of the simulations, thus the unit of the velocity in this implementation is <inline-formula><mml:math id="inf122"><mml:mrow><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> rather than <inline-formula><mml:math id="inf123"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>. Then the position of agent <inline-formula><mml:math id="inf124"><mml:msup><mml:mi mathvariant="bold-italic">𝑷</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> in the Cartesian coordinates for the is updated by:<disp-formula id="equ40"><label>(40)</label><mml:math id="m40"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">𝑷</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">𝑷</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">𝒗</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The main parameter settings for all the simulations in this paper can be found in <xref ref-type="table" rid="table1">Table 1</xref>.</p><sec id="s4-9-1"><title>Reproduce visual navigation behaviour</title><p>Inspired by the benchmark study of real ants in <xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref>, we test our model of VH and RF by reproducing the homing behaviours in that study. This is achieved by constructing a habitual route with a similar shape (arc or banana shape) in our simulated 3D world. The position <inline-formula><mml:math id="inf125"><mml:msub><mml:mi mathvariant="bold-italic">𝑷</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> and heading <inline-formula><mml:math id="inf126"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> along that route is manually generated by:<disp-formula id="equ41"><label>(41)</label><mml:math id="m41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mfrac><mml:mi>π</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mn>7</mml:mn><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mtd><mml:mtd/></mml:mtr></mml:mtable><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where the <inline-formula><mml:math id="inf127"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>7</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the radius of the arc and <inline-formula><mml:math id="inf128"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> in this case is the number of the sampling points where view images are reconstructed along the route. The reconstructed views then be wrapped and decomposed by ZM into amplitudes and <italic>phases</italic> are used to train the ANN network of RF and MB network of VH.</p></sec></sec><sec id="s4-10"><title>Visual homing</title><p>After training, 12 agents with different initial headings that were evenly distributed in <inline-formula><mml:math id="inf129"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>360</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> were released at the sideways release point (<inline-formula><mml:math id="inf130"><mml:mrow><mml:mi mathvariant="bold-italic">𝑷</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) for the simulation of VH (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). The headings of the agents at radius 2.5 m from the release point (manually selected to ensure that the all the agents have completed any large initial loop) are taken as the initial headings.</p></sec><sec id="s4-11"><title>Route following</title><p>After training, 2 agents with 0° and 180° are released at the different release points (<inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>) for the simulation of RF (see <xref ref-type="fig" rid="fig4">Figure 4B</xref>) to generate the homing path. And then, we release 12 agents on the route (<inline-formula><mml:math id="inf132"><mml:mrow><mml:mi mathvariant="bold-italic">𝑷</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) with different initial headings that is evenly distributed in <inline-formula><mml:math id="inf133"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>360</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to compare the results with the real ant data in <xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref>. The heading of each agent at the position that is 0.6m from the release point is taken as the initial heading.</p><sec id="s4-11-1"><title>Reproduce the optimal cue integration behaviour</title><p>We evaluated the cue integration model by reproducing the results of <xref ref-type="bibr" rid="bib90">Wystrach et al., 2012</xref> and <xref ref-type="bibr" rid="bib54">Legge et al., 2014</xref>. The ants’ outbound routes in <xref ref-type="bibr" rid="bib92">Wystrach et al., 2015</xref> is bounded by the corridor, so here we simulate the velocity of the agent by:<disp-formula id="equ42"><label>(42)</label><mml:math id="m42"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">𝒗</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="12.5pt">,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where the function <inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> generates a random value from the uniform distribution of <inline-formula><mml:math id="inf135"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, thus the speed of x-axis will be in <inline-formula><mml:math id="inf136"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> and will cancel each other during the forging. The speed of y-axis is constant so it will accumulated and be recorded by the PI model. And <inline-formula><mml:math id="inf137"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the basic speed of the agent and <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the total time for outbound phase determining the length of the outbound route. As for the simulated homing route, we duplicate the outbound route when <inline-formula><mml:math id="inf139"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mrow></mml:math></inline-formula> but with a inverted heading direction. And then the visual navigation network was trained with images sampled along a simulated route (grey curve in <xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p></sec></sec><sec id="s4-12"><title>Tuning PI uncertainty</title><p>The agent in this simulation was allowed to forage to different distances of 0.1m, 1m, 3m or 7m from the nest to accrue different PI states and directional certainties before being translated to a never-before-experienced test site 1.5m from the nest. (RP1 in <xref ref-type="fig" rid="fig3">Figure 3B</xref>). For each trial, we release 20 agents with different initial headings that is evenly distributed in <inline-formula><mml:math id="inf140"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>360</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The headings of every agent at the position that is 0.6m from the start point is taken as the initial headings, and the mean direction and the 95% confidential intervals are calculated. As in the biological experiment, the angle between the directions recommended by the PI and visual navigation systems differed by approximately 130°.</p><p>As the length of the home vector increase (0.1m -&gt; 7m) the activation of PI memory becomes higher (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), and increasingly determines the output of the ring attractor integration. Since the length of the home vector is also encoded in the activation of the PI memory neurons, the ring attractor can extract this information as the strength of the cue. As the visual familiarity is nearly the same in the vicinity of the release point, the strength of visual homing circuit remains constant and has more of an influence as the PI length drops.</p></sec><sec id="s4-13"><title>Tuning visual uncertainty</title><p>The agent in this simulation was allowed to forage up to 1m from the nest to accrue its PI state and directional certainty before being translated to three different release points (RP1, RP2 and RP3 in <xref ref-type="fig" rid="fig3">Figure 3B</xref>). As the distance from nest increases (RP1-&gt;RP2-&gt;RP3) so does the visual uncertainty. For each trial, we release 12 agents with different initial headings that is evenly distributed in <inline-formula><mml:math id="inf141"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>360</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The headings of each agent at the position that is 0.3m from the start point is taken as the initial headings, and the mean direction and the 95% confidential intervals are calculated.</p></sec><sec id="s4-14"><title>Whole model</title><p>The simulated habitual route remains the same as in the simulation of visual navigation (Reproduce visual navigation behaviour) as is the learning procedure. The zero- and full- vector agents are both released at <inline-formula><mml:math id="inf142"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> with the heading 0° and 90°, respectively. The full-vector agent’s PI memory is generated by letting the agent forage along the route from nest to feeder.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 778062, ULTRACEPT and No 691154, STEP2DYNA.</p><p>Thanks to Barbara Webb and Insects Robotics Group at the Univ of Edinburgh, Hadi Maboudi, Alex Cope and Andrew Philippedes for comments on early drafts, and to Antoine Wystrach for provision of data from previous works. Thanks for proof readers Anne and Mike Mangan (Snr). Finally, thanks to our editor and reviewers who helped improve the model and manuscript through their excellent feedback.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Supervision, Funding acquisition, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Data curation, Supervision, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>Data_Model_Simulations_Results.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-54026-code1-v2.zip"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-54026-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All the source code of the implementation and part of the data are uploaded to Github and are available via <ext-link ext-link-type="uri" xlink:href="https://github.com/XuelongSun/InsectNavigationToolkitModelling">https://github.com/XuelongSun/InsectNavigationToolkitModelling</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/InsectNavigationToolkitModelling">https://github.com/elifesciences-publications/InsectNavigationToolkitModelling</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ardin</surname> <given-names>P</given-names></name><name><surname>Peng</surname> <given-names>F</given-names></name><name><surname>Mangan</surname> <given-names>M</given-names></name><name><surname>Lagogiannis</surname> <given-names>K</given-names></name><name><surname>Webb</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using an insect mushroom body circuit to encode route memory in complex natural environments</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004683</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004683</pub-id><pub-id pub-id-type="pmid">26866692</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aso</surname> <given-names>Y</given-names></name><name><surname>Hattori</surname> <given-names>D</given-names></name><name><surname>Yu</surname> <given-names>Y</given-names></name><name><surname>Johnston</surname> <given-names>RM</given-names></name><name><surname>Iyer</surname> <given-names>NA</given-names></name><name><surname>Ngo</surname> <given-names>TT</given-names></name><name><surname>Dionne</surname> <given-names>H</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Axel</surname> <given-names>R</given-names></name><name><surname>Tanimoto</surname> <given-names>H</given-names></name><name><surname>Rubin</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The neuronal architecture of the mushroom body provides a logic for associative learning</article-title><source>eLife</source><volume>3</volume><elocation-id>e04577</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04577</pub-id><pub-id pub-id-type="pmid">25535793</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baddeley</surname> <given-names>B</given-names></name><name><surname>Graham</surname> <given-names>P</given-names></name><name><surname>Husbands</surname> <given-names>P</given-names></name><name><surname>Philippides</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A model of ant route navigation driven by scene familiarity</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002336</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002336</pub-id><pub-id pub-id-type="pmid">22241975</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barth</surname> <given-names>M</given-names></name><name><surname>Heisenberg</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Vision affects mushroom bodies and central complex in <italic>Drosophila melanogaster</italic></article-title><source>Learning &amp; Memory</source><volume>4</volume><fpage>219</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1101/lm.4.2.219</pub-id><pub-id pub-id-type="pmid">10456065</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beetz</surname> <given-names>MJ</given-names></name><name><surname>El Jundi</surname> <given-names>B</given-names></name><name><surname>Heinze</surname> <given-names>S</given-names></name><name><surname>Homberg</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Topographic organization and possible function of the posterior optic tubercles in the brain of the desert locust <italic>Schistocerca gregaria</italic></article-title><source>Journal of Comparative Neurology</source><volume>523</volume><fpage>1589</fpage><lpage>1607</lpage><pub-id pub-id-type="doi">10.1002/cne.23736</pub-id><pub-id pub-id-type="pmid">25557150</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bregy</surname> <given-names>P</given-names></name><name><surname>Sommer</surname> <given-names>S</given-names></name><name><surname>Wehner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Nest-mark orientation versus vector navigation in desert ants</article-title><source>Journal of Experimental Biology</source><volume>211</volume><fpage>1868</fpage><lpage>1873</lpage><pub-id pub-id-type="doi">10.1242/jeb.018036</pub-id><pub-id pub-id-type="pmid">18515716</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cartwright</surname> <given-names>BA</given-names></name><name><surname>Collett</surname> <given-names>TS</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>How honey bees use landmarks to guide their return to a food source</article-title><source>Nature</source><volume>295</volume><fpage>560</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1038/295560a0</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Insect navigation en route to the goal: multiple strategies for the use of landmarks</article-title><source>The Journal of Experimental Biology</source><volume>199</volume><fpage>227</fpage><lpage>235</lpage><pub-id pub-id-type="pmid">9317693</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How navigational guidance systems are combined in a desert ant</article-title><source>Current Biology</source><volume>22</volume><fpage>927</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.03.049</pub-id><pub-id pub-id-type="pmid">22521785</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname> <given-names>M</given-names></name><name><surname>Chittka</surname> <given-names>L</given-names></name><name><surname>Collett</surname> <given-names>TS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Spatial memory in insect navigation</article-title><source>Current Biology</source><volume>23</volume><fpage>R789</fpage><lpage>R800</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.07.020</pub-id><pub-id pub-id-type="pmid">24028962</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname> <given-names>TS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Path integration: how details of the honeybee waggle dance and the foraging strategies of desert ants might help in understanding its mechanisms</article-title><source>The Journal of Experimental Biology</source><volume>222</volume><elocation-id>jeb205187</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.205187</pub-id><pub-id pub-id-type="pmid">31152122</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname> <given-names>M</given-names></name><name><surname>Collett</surname> <given-names>TS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How does the insect central complex use mushroom body output for steering?</article-title><source>Current Biology</source><volume>28</volume><fpage>R733</fpage><lpage>R734</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.05.060</pub-id><pub-id pub-id-type="pmid">29990452</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cope</surname> <given-names>AJ</given-names></name><name><surname>Sabo</surname> <given-names>C</given-names></name><name><surname>Vasilaki</surname> <given-names>E</given-names></name><name><surname>Barron</surname> <given-names>AB</given-names></name><name><surname>Marshall</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A computational model of the integration of landmarks and motion in the insect central complex</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0172325</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0172325</pub-id><pub-id pub-id-type="pmid">28241061</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cruse</surname> <given-names>H</given-names></name><name><surname>Wehner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>No need for a cognitive map: decentralized memory for insect navigation</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002009</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002009</pub-id><pub-id pub-id-type="pmid">21445233</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dacke</surname> <given-names>M</given-names></name><name><surname>Bell</surname> <given-names>ATA</given-names></name><name><surname>Foster</surname> <given-names>JJ</given-names></name><name><surname>Baird</surname> <given-names>EJ</given-names></name><name><surname>Strube-Bloss</surname> <given-names>MF</given-names></name><name><surname>Byrne</surname> <given-names>MJ</given-names></name><name><surname>El Jundi</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Multimodal cue integration in the dung beetle compass</article-title><source>PNAS</source><volume>116</volume><fpage>14248</fpage><lpage>14253</lpage><pub-id pub-id-type="doi">10.1073/pnas.1904308116</pub-id><pub-id pub-id-type="pmid">31235569</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dewar</surname> <given-names>ADM</given-names></name><name><surname>Philippides</surname> <given-names>A</given-names></name><name><surname>Graham</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>What is the relationship between visual environment and the form of ant learning-walks? an in silico investigation of insect navigation</article-title><source>Adaptive Behavior</source><volume>22</volume><fpage>163</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1177/1059712313516132</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehmer</surname> <given-names>B</given-names></name><name><surname>Gronenberg</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Segregation of visual input to the mushroom bodies in the honeybee (Apis mellifera)</article-title><source>The Journal of Comparative Neurology</source><volume>451</volume><fpage>362</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1002/cne.10355</pub-id><pub-id pub-id-type="pmid">12210130</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehmer</surname> <given-names>B</given-names></name><name><surname>Gronenberg</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Mushroom body volumes and visual interneurons in ants: comparison between sexes and castes</article-title><source>The Journal of Comparative Neurology</source><volume>469</volume><fpage>198</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1002/cne.11014</pub-id><pub-id pub-id-type="pmid">14694534</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El Jundi</surname> <given-names>B</given-names></name><name><surname>Foster</surname> <given-names>JJ</given-names></name><name><surname>Khaldy</surname> <given-names>L</given-names></name><name><surname>Byrne</surname> <given-names>MJ</given-names></name><name><surname>Dacke</surname> <given-names>M</given-names></name><name><surname>Baird</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A Snapshot-Based mechanism for celestial orientation</article-title><source>Current Biology</source><volume>26</volume><fpage>1456</fpage><lpage>1462</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.03.030</pub-id><pub-id pub-id-type="pmid">27185557</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname> <given-names>YE</given-names></name><name><surname>Lu</surname> <given-names>J</given-names></name><name><surname>D'Alessandro</surname> <given-names>I</given-names></name><name><surname>Wilson</surname> <given-names>RI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sensorimotor experience remaps visual input to a heading-direction network</article-title><source>Nature</source><volume>576</volume><fpage>121</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1772-4</pub-id><pub-id pub-id-type="pmid">31748749</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleischmann</surname> <given-names>PN</given-names></name><name><surname>Christian</surname> <given-names>M</given-names></name><name><surname>Müller</surname> <given-names>VL</given-names></name><name><surname>Rössler</surname> <given-names>W</given-names></name><name><surname>Wehner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Ontogeny of learning walks and the acquisition of landmark information in desert ants, <italic>Cataglyphis fortis</italic></article-title><source>The Journal of Experimental Biology</source><volume>219</volume><fpage>3137</fpage><lpage>3145</lpage><pub-id pub-id-type="doi">10.1242/jeb.140459</pub-id><pub-id pub-id-type="pmid">27481270</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franconville</surname> <given-names>R</given-names></name><name><surname>Beron</surname> <given-names>C</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Building a functional connectome of the <italic>Drosophila</italic> central complex</article-title><source>eLife</source><volume>7</volume><elocation-id>e37017</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.37017</pub-id><pub-id pub-id-type="pmid">30124430</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukushi</surname> <given-names>T</given-names></name><name><surname>Wehner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Navigation in wood ants Formica japonica: context dependent use of landmarks</article-title><source>Journal of Experimental Biology</source><volume>207</volume><fpage>3431</fpage><lpage>3439</lpage><pub-id pub-id-type="doi">10.1242/jeb.01159</pub-id><pub-id pub-id-type="pmid">15326219</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gkanias</surname> <given-names>E</given-names></name><name><surname>Risse</surname> <given-names>B</given-names></name><name><surname>Mangan</surname> <given-names>M</given-names></name><name><surname>Webb</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>From skylight input to behavioural output: a computational model of the insect polarised light compass</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007123</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007123</pub-id><pub-id pub-id-type="pmid">31318859</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gonzalez</surname> <given-names>RC</given-names></name><name><surname>Woods</surname> <given-names>RE</given-names></name><name><surname>Eddins</surname> <given-names>SL</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Digital image processing using MATLAB</source><publisher-name>Pearson Education India</publisher-name></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname> <given-names>P</given-names></name><name><surname>Philippides</surname> <given-names>A</given-names></name><name><surname>Baddeley</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Animal cognition: multi-modal interactions in ant learning</article-title><source>Current Biology</source><volume>20</volume><fpage>R639</fpage><lpage>R640</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2010.06.018</pub-id><pub-id pub-id-type="pmid">20692612</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname> <given-names>J</given-names></name><name><surname>Adachi</surname> <given-names>A</given-names></name><name><surname>Shah</surname> <given-names>KK</given-names></name><name><surname>Hirokawa</surname> <given-names>JD</given-names></name><name><surname>Magani</surname> <given-names>PS</given-names></name><name><surname>Maimon</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A neural circuit architecture for angular integration in <italic>Drosophila</italic></article-title><source>Nature</source><volume>546</volume><fpage>101</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1038/nature22343</pub-id><pub-id pub-id-type="pmid">28538731</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gronenberg</surname> <given-names>W</given-names></name><name><surname>López-Riquelme</surname> <given-names>GO</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Multisensory convergence in the mushroom bodies of ants and bees</article-title><source>Acta Biologica Hungarica</source><volume>55</volume><fpage>31</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1556/ABiol.55.2004.1-4.5</pub-id><pub-id pub-id-type="pmid">15270216</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanesch</surname> <given-names>U</given-names></name><name><surname>Fischbach</surname> <given-names>K-F</given-names></name><name><surname>Heisenberg</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Neuronal architecture of the central complex in <italic>Drosophila melanogaster</italic></article-title><source>Cell and Tissue Research</source><volume>257</volume><fpage>343</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1007/BF00261838</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname> <given-names>JF</given-names></name><name><surname>Fewell</surname> <given-names>JH</given-names></name><name><surname>Stiller</surname> <given-names>TM</given-names></name><name><surname>Breed</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Effects of experience on use of orientation cues in the giant tropical ant</article-title><source>Animal Behaviour</source><volume>37</volume><fpage>869</fpage><lpage>871</lpage><pub-id pub-id-type="doi">10.1016/0003-3472(89)90076-6</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinze</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Unraveling the neural basis of insect navigation</article-title><source>Current Opinion in Insect Science</source><volume>24</volume><fpage>58</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/j.cois.2017.09.001</pub-id><pub-id pub-id-type="pmid">29208224</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinze</surname> <given-names>S</given-names></name><name><surname>Narendra</surname> <given-names>A</given-names></name><name><surname>Cheung</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Principles of insect path integration</article-title><source>Current Biology</source><volume>28</volume><fpage>R1043</fpage><lpage>R1058</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.04.058</pub-id><pub-id pub-id-type="pmid">30205054</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinze</surname> <given-names>S</given-names></name><name><surname>Homberg</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Maplike representation of celestial E-vector orientations in the brain of an insect</article-title><source>Science</source><volume>315</volume><fpage>995</fpage><lpage>997</lpage><pub-id pub-id-type="doi">10.1126/science.1135531</pub-id><pub-id pub-id-type="pmid">17303756</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinze</surname> <given-names>S</given-names></name><name><surname>Homberg</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neuroarchitecture of the central complex of the desert locust: intrinsic and columnar neurons</article-title><source>The Journal of Comparative Neurology</source><volume>511</volume><fpage>454</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1002/cne.21842</pub-id><pub-id pub-id-type="pmid">18837039</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinze</surname> <given-names>S</given-names></name><name><surname>Homberg</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Linking the input to the output: new sets of neurons complement the polarization vision network in the locust central complex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>4911</fpage><lpage>4921</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0332-09.2009</pub-id><pub-id pub-id-type="pmid">19369560</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinze</surname> <given-names>S</given-names></name><name><surname>Pfeiffer</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Editorial: the insect central Complex-From sensory coding to directing movement</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>12</volume><elocation-id>156</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2018.00156</pub-id><pub-id pub-id-type="pmid">30104965</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heisenberg</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Mushroom body memoir: from maps to models</article-title><source>Nature Reviews Neuroscience</source><volume>4</volume><fpage>266</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1038/nrn1074</pub-id><pub-id pub-id-type="pmid">12671643</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hoinville</surname> <given-names>T</given-names></name><name><surname>Wehner</surname> <given-names>R</given-names></name><name><surname>Cruse</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Learning and retrieval of memory elements in a navigation task</article-title><conf-name>Conference on Biomimetic and Biohybrid Systems</conf-name><fpage>120</fpage><lpage>131</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoinville</surname> <given-names>T</given-names></name><name><surname>Wehner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Optimal multiguidance integration in insect navigation</article-title><source>PNAS</source><volume>115</volume><fpage>2824</fpage><lpage>2829</lpage><pub-id pub-id-type="doi">10.1073/pnas.1721668115</pub-id><pub-id pub-id-type="pmid">29483254</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Homberg</surname> <given-names>U</given-names></name><name><surname>Hofer</surname> <given-names>S</given-names></name><name><surname>Pfeiffer</surname> <given-names>K</given-names></name><name><surname>Gebhardt</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Organization and neural connections of the anterior optic tubercle in the brain of the Locust, Schistocerca gregaria</article-title><source>The Journal of Comparative Neurology</source><volume>462</volume><fpage>415</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1002/cne.10771</pub-id><pub-id pub-id-type="pmid">12811810</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honkanen</surname> <given-names>A</given-names></name><name><surname>Adden</surname> <given-names>A</given-names></name><name><surname>da Silva Freitas</surname> <given-names>J</given-names></name><name><surname>Heinze</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The insect central complex and the neural basis of navigational strategies</article-title><source>The Journal of Experimental Biology</source><volume>222</volume><elocation-id>jeb188854</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.188854</pub-id><pub-id pub-id-type="pmid">30728235</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horridge</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Pattern discrimination by the honeybee: disruption as a cue</article-title><source>Journal of Comparative Physiology A: Sensory, Neural, and Behavioral Physiology</source><volume>181</volume><fpage>267</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1007/s003590050113</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hudson</surname> <given-names>G</given-names></name><name><surname>Léger</surname> <given-names>A</given-names></name><name><surname>Niss</surname> <given-names>B</given-names></name><name><surname>Sebestyén</surname> <given-names>I</given-names></name><name><surname>Vaaben</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>JPEG-1 standard 25 years: past, present, and future reasons for a success</article-title><source>Journal of Electronic Imaging</source><volume>27</volume><elocation-id>040901</elocation-id><pub-id pub-id-type="doi">10.1117/1.JEI.27.4.040901</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>James</surname> <given-names>AC</given-names></name><name><surname>Osorio</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Characterisation of columnar neurons and visual signal processing in the medulla of the locust optic lobe by system identification techniques</article-title><source>Journal of Comparative Physiology A</source><volume>178</volume><fpage>183</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1007/BF00188161</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeffery</surname> <given-names>KJ</given-names></name><name><surname>Page</surname> <given-names>HJ</given-names></name><name><surname>Stringer</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Optimal cue combination and landmark-stability learning in the head direction system</article-title><source>The Journal of Physiology</source><volume>594</volume><fpage>6527</fpage><lpage>6534</lpage><pub-id pub-id-type="doi">10.1113/JP272945</pub-id><pub-id pub-id-type="pmid">27479741</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>H</given-names></name><name><surname>Pogue</surname> <given-names>BW</given-names></name><name><surname>Patterson</surname> <given-names>MS</given-names></name><name><surname>Paulsen</surname> <given-names>KD</given-names></name><name><surname>Osterberg</surname> <given-names>UL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Optical image reconstruction using frequency-domain data: simulations and experiments</article-title><source>Journal of the Optical Society of America A</source><volume>13</volume><fpage>253</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.13.000253</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kakaria</surname> <given-names>KS</given-names></name><name><surname>de Bivort</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Ring attractor dynamics emerge from a spiking model of the entire protocerebral bridge</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>11</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2017.00008</pub-id><pub-id pub-id-type="pmid">28261066</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khotanzad</surname> <given-names>A</given-names></name><name><surname>Hong</surname> <given-names>YH</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Invariant image recognition by Zernike moments</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>12</volume><fpage>489</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1109/34.55109</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>SS</given-names></name><name><surname>Rouault</surname> <given-names>H</given-names></name><name><surname>Druckmann</surname> <given-names>S</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Ring attractor dynamics in the <italic>Drosophila</italic> central brain</article-title><source>Science</source><volume>356</volume><fpage>849</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.1126/science.aal4835</pub-id><pub-id pub-id-type="pmid">28473639</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>SS</given-names></name><name><surname>Hermundstad</surname> <given-names>AM</given-names></name><name><surname>Romani</surname> <given-names>S</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Generation of stable heading representations in diverse visual scenes</article-title><source>Nature</source><volume>576</volume><fpage>126</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1767-1</pub-id><pub-id pub-id-type="pmid">31748750</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>IS</given-names></name><name><surname>Dickinson</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Idiothetic path integration in the fruit fly <italic>Drosophila melanogaster</italic></article-title><source>Current Biology</source><volume>27</volume><fpage>2227</fpage><lpage>2238</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.06.026</pub-id><pub-id pub-id-type="pmid">28736164</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kodzhabashev</surname> <given-names>A</given-names></name><name><surname>Mangan</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Route following without scanning</article-title><conf-name>Conference on Biomimetic and Biohybrid Systems</conf-name><fpage>199</fpage><lpage>210</lpage></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohler</surname> <given-names>M</given-names></name><name><surname>Wehner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Idiosyncratic route-based memories in desert ants, Melophorus bagoti: how do they interact with path-integration vectors?</article-title><source>Neurobiology of Learning and Memory</source><volume>83</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2004.05.011</pub-id><pub-id pub-id-type="pmid">15607683</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Legge</surname> <given-names>EL</given-names></name><name><surname>Wystrach</surname> <given-names>A</given-names></name><name><surname>Spetch</surname> <given-names>ML</given-names></name><name><surname>Cheng</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Combining sky and earth: desert ants (Melophorus bagoti) show weighted integration of celestial and terrestrial cues</article-title><source>Journal of Experimental Biology</source><volume>217</volume><fpage>4159</fpage><lpage>4166</lpage><pub-id pub-id-type="doi">10.1242/jeb.107862</pub-id><pub-id pub-id-type="pmid">25324340</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehrer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Shape perception in the honeybee: symmetry as a global framework</article-title><source>International Journal of Plant Sciences</source><volume>160</volume><fpage>S51</fpage><lpage>S65</lpage><pub-id pub-id-type="doi">10.1086/314216</pub-id><pub-id pub-id-type="pmid">10572022</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mangan</surname> <given-names>M</given-names></name><name><surname>Webb</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spontaneous formation of multiple routes in individual desert ants (Cataglyphis velox)</article-title><source>Behavioral Ecology</source><volume>23</volume><fpage>944</fpage><lpage>954</lpage><pub-id pub-id-type="doi">10.1093/beheco/ars051</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mangan</surname> <given-names>MX</given-names></name><name><surname>Yue</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An analysis of a ring attractor model for cue integration</article-title><conf-name>Conference on Biomimetic and Biohybrid Systems</conf-name><fpage>459</fpage><lpage>470</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menegatti</surname> <given-names>E</given-names></name><name><surname>Maeda</surname> <given-names>T</given-names></name><name><surname>Ishiguro</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Image-based memory for robot navigation using properties of omnidirectional images</article-title><source>Robotics and Autonomous Systems</source><volume>47</volume><fpage>251</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1016/j.robot.2004.03.014</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname> <given-names>J</given-names></name><name><surname>Nawrot</surname> <given-names>M</given-names></name><name><surname>Menzel</surname> <given-names>R</given-names></name><name><surname>Landgraf</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A neural network model for familiarity and context learning during honeybee foraging flights</article-title><source>Biological Cybernetics</source><volume>112</volume><fpage>113</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1007/s00422-017-0732-z</pub-id><pub-id pub-id-type="pmid">28917001</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname> <given-names>M</given-names></name><name><surname>Wehner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Path integration provides a scaffold for landmark learning in desert ants</article-title><source>Current Biology</source><volume>20</volume><fpage>1368</fpage><lpage>1371</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2010.06.035</pub-id><pub-id pub-id-type="pmid">20619653</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narendra</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Homing strategies of the australian desert ant Melophorus bagoti. II. interaction of the path integrator with visual cue information</article-title><source>Journal of Experimental Biology</source><volume>210</volume><fpage>1804</fpage><lpage>1812</lpage><pub-id pub-id-type="doi">10.1242/jeb.02769</pub-id><pub-id pub-id-type="pmid">17488944</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Carroll</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Feature-detecting neurons in dragonflies</article-title><source>Nature</source><volume>362</volume><fpage>541</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1038/362541a0</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ofstad</surname> <given-names>TA</given-names></name><name><surname>Zuker</surname> <given-names>CS</given-names></name><name><surname>Reiser</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual place learning in <italic>Drosophila melanogaster</italic></article-title><source>Nature</source><volume>474</volume><fpage>204</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1038/nature10131</pub-id><pub-id pub-id-type="pmid">21654803</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Omoto</surname> <given-names>JJ</given-names></name><name><surname>Keleş</surname> <given-names>MF</given-names></name><name><surname>Nguyen</surname> <given-names>BM</given-names></name><name><surname>Bolanos</surname> <given-names>C</given-names></name><name><surname>Lovick</surname> <given-names>JK</given-names></name><name><surname>Frye</surname> <given-names>MA</given-names></name><name><surname>Hartenstein</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual input to the <italic>Drosophila</italic> central complex by developmentally and functionally distinct neuronal populations</article-title><source>Current Biology</source><volume>27</volume><fpage>1098</fpage><lpage>1110</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.02.063</pub-id><pub-id pub-id-type="pmid">28366740</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pajdla</surname> <given-names>T</given-names></name><name><surname>Hlaváč</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Zero phase representation of panoramic images for image based localization</article-title><conf-name>International Conference on Computer Analysis of Images and Patterns</conf-name><fpage>550</fpage><lpage>557</lpage></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulk</surname> <given-names>AC</given-names></name><name><surname>Kirszenblat</surname> <given-names>L</given-names></name><name><surname>Zhou</surname> <given-names>Y</given-names></name><name><surname>van Swinderen</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Closed-Loop behavioral control increases coherence in the fly brain</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>10304</fpage><lpage>10315</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0691-15.2015</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname> <given-names>K</given-names></name><name><surname>Homberg</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Organization and functional roles of the central complex in the insect brain</article-title><source>Annual Review of Entomology</source><volume>59</volume><fpage>165</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1146/annurev-ento-011613-162031</pub-id><pub-id pub-id-type="pmid">24160424</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pisokas</surname> <given-names>I</given-names></name><name><surname>Heinze</surname> <given-names>S</given-names></name><name><surname>Webb</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The heading direction circuit of two insect species</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/854521</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plath</surname> <given-names>JA</given-names></name><name><surname>Entler</surname> <given-names>BV</given-names></name><name><surname>Kirkerud</surname> <given-names>NH</given-names></name><name><surname>Schlegel</surname> <given-names>U</given-names></name><name><surname>Galizia</surname> <given-names>CG</given-names></name><name><surname>Barron</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Different roles for honey bee mushroom bodies and central complex in visual learning of colored lights in an aversive conditioning assay</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>11</volume><elocation-id>98</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2017.00098</pub-id><pub-id pub-id-type="pmid">28611605</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reppert</surname> <given-names>SM</given-names></name><name><surname>Guerra</surname> <given-names>PA</given-names></name><name><surname>Merlin</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neurobiology of monarch butterfly migration</article-title><source>Annual Review of Entomology</source><volume>61</volume><fpage>25</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1146/annurev-ento-010814-020855</pub-id><pub-id pub-id-type="pmid">26473314</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rybak</surname> <given-names>J</given-names></name><name><surname>Menzel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Anatomy of the mushroom bodies in the honey bee brain: the neuronal connections of the alpha-lobe</article-title><source>The Journal of Comparative Neurology</source><volume>334</volume><fpage>444</fpage><lpage>465</lpage><pub-id pub-id-type="doi">10.1002/cne.903340309</pub-id><pub-id pub-id-type="pmid">8376627</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname> <given-names>S</given-names></name><name><surname>Mangan</surname> <given-names>M</given-names></name><name><surname>Zeil</surname> <given-names>J</given-names></name><name><surname>Webb</surname> <given-names>B</given-names></name><name><surname>Wystrach</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How ants use vision when homing backward</article-title><source>Current Biology</source><volume>27</volume><fpage>401</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.12.019</pub-id><pub-id pub-id-type="pmid">28111152</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seelig</surname> <given-names>JD</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Feature detection and orientation tuning in the <italic>Drosophila</italic> central complex</article-title><source>Nature</source><volume>503</volume><fpage>262</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nature12601</pub-id><pub-id pub-id-type="pmid">24107996</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seelig</surname> <given-names>JD</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural dynamics for landmark orientation and angular path integration</article-title><source>Nature</source><volume>521</volume><fpage>186</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1038/nature14446</pub-id><pub-id pub-id-type="pmid">25971509</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinbeck</surname> <given-names>F</given-names></name><name><surname>Adden</surname> <given-names>A</given-names></name><name><surname>Graham</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Connecting brain to behaviour: a role for general purpose steering circuits in insect orientation?</article-title><source>The Journal of Experimental Biology</source><volume>223</volume><elocation-id>jeb212332</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.212332</pub-id><pub-id pub-id-type="pmid">32161054</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Stone</surname> <given-names>T</given-names></name><name><surname>Differt</surname> <given-names>D</given-names></name><name><surname>Milford</surname> <given-names>M</given-names></name><name><surname>Webb</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Skyline-based localisation for aggressively manoeuvring robots using uv sensors and spherical harmonics</article-title><conf-name>IEEE International Conference on Robotics and Automation</conf-name><fpage>5615</fpage><lpage>5622</lpage><pub-id pub-id-type="doi">10.1109/ICRA.2016.7487780</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stone</surname> <given-names>T</given-names></name><name><surname>Webb</surname> <given-names>B</given-names></name><name><surname>Adden</surname> <given-names>A</given-names></name><name><surname>Weddig</surname> <given-names>NB</given-names></name><name><surname>Honkanen</surname> <given-names>A</given-names></name><name><surname>Templin</surname> <given-names>R</given-names></name><name><surname>Wcislo</surname> <given-names>W</given-names></name><name><surname>Scimeca</surname> <given-names>L</given-names></name><name><surname>Warrant</surname> <given-names>E</given-names></name><name><surname>Heinze</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An anatomically constrained model for path integration in the bee brain</article-title><source>Current Biology</source><volume>27</volume><fpage>3069</fpage><lpage>3085</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.08.052</pub-id><pub-id pub-id-type="pmid">28988858</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stone</surname> <given-names>T</given-names></name><name><surname>Mangan</surname> <given-names>M</given-names></name><name><surname>Wystrach</surname> <given-names>A</given-names></name><name><surname>Webb</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rotation invariant visual processing for spatial memory in insects</article-title><source>Interface Focus</source><volume>8</volume><elocation-id>20180010</elocation-id><pub-id pub-id-type="doi">10.1098/rsfs.2018.0010</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stürzl</surname> <given-names>W</given-names></name><name><surname>Zeil</surname> <given-names>J</given-names></name><name><surname>Boeddeker</surname> <given-names>N</given-names></name><name><surname>Hemmi</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How wasps acquire and use views for homing</article-title><source>Current Biology</source><volume>26</volume><fpage>470</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.12.052</pub-id><pub-id pub-id-type="pmid">26877083</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stürzl</surname> <given-names>W</given-names></name><name><surname>Mallot</surname> <given-names>HA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Efficient visual homing based on Fourier transformed panoramic images</article-title><source>Robotics and Autonomous Systems</source><volume>54</volume><fpage>300</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.robot.2005.12.001</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sun</surname> <given-names>X</given-names></name><name><surname>Yue</surname> <given-names>S</given-names></name><name><surname>Mangan</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Insect Navigation Toolkit Modelling</data-title><source>Github</source><version designator="6a32700">6a32700</version><ext-link ext-link-type="uri" xlink:href="https://github.com/XuelongSun/InsectNavigationToolkitModelling">https://github.com/XuelongSun/InsectNavigationToolkitModelling</ext-link></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teague</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Image analysis via the general theory of moments*</article-title><source>Journal of the Optical Society of America</source><volume>70</volume><fpage>920</fpage><lpage>930</lpage><pub-id pub-id-type="doi">10.1364/JOSA.70.000920</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Timaeus</surname> <given-names>L</given-names></name><name><surname>Geid</surname> <given-names>L</given-names></name><name><surname>Sancer</surname> <given-names>G</given-names></name><name><surname>Wernet</surname> <given-names>MF</given-names></name><name><surname>Hummel</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Parallel visual pathways with topographic versus non-topographic organization connect the <italic>Drosophila</italic> eyes to the central brain</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.04.11.037333</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Touretzky</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title>Attractor network models of head direction cells</chapter-title><person-group person-group-type="editor"><name><surname>Wiener</surname> <given-names>SI</given-names></name><name><surname>Taube</surname> <given-names>JS</given-names></name></person-group><source>Head Direction Cells and the Neural Mechanisms of Spatial Orientation</source><publisher-name>MIT Press</publisher-name><fpage>411</fpage><lpage>432</lpage></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner-Evans</surname> <given-names>D</given-names></name><name><surname>Wegener</surname> <given-names>S</given-names></name><name><surname>Rouault</surname> <given-names>H</given-names></name><name><surname>Franconville</surname> <given-names>R</given-names></name><name><surname>Wolff</surname> <given-names>T</given-names></name><name><surname>Seelig</surname> <given-names>JD</given-names></name><name><surname>Druckmann</surname> <given-names>S</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Angular velocity integration in a fly heading circuit</article-title><source>eLife</source><volume>6</volume><elocation-id>e23496</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.23496</pub-id><pub-id pub-id-type="pmid">28530551</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Turner-Evans</surname> <given-names>DB</given-names></name><name><surname>Jensen</surname> <given-names>K</given-names></name><name><surname>Ali</surname> <given-names>S</given-names></name><name><surname>Paterson</surname> <given-names>T</given-names></name><name><surname>Sheridan</surname> <given-names>A</given-names></name><name><surname>Ray</surname> <given-names>RP</given-names></name><name><surname>Lauritzen</surname> <given-names>S</given-names></name><name><surname>Bock</surname> <given-names>D</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The neuroanatomical ultrastructure and function of a biological ring attractor</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/847152</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webb</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The internal maps of insects</article-title><source>The Journal of Experimental Biology</source><volume>222</volume><elocation-id>jeb188094</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.188094</pub-id><pub-id pub-id-type="pmid">30728234</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wehner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The architecture of the desert ant’s navigational toolkit</article-title><source>Myrmecological News</source><volume>12</volume><fpage>85</fpage><lpage>96</lpage></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wystrach</surname> <given-names>A</given-names></name><name><surname>Beugnon</surname> <given-names>G</given-names></name><name><surname>Cheng</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Landmarks or panoramas: what do navigating ants attend to for guidance?</article-title><source>Frontiers in Zoology</source><volume>8</volume><elocation-id>21</elocation-id><pub-id pub-id-type="doi">10.1186/1742-9994-8-21</pub-id><pub-id pub-id-type="pmid">21871114</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wystrach</surname> <given-names>A</given-names></name><name><surname>Beugnon</surname> <given-names>G</given-names></name><name><surname>Cheng</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Ants might use different view-matching strategies on and off the route</article-title><source>Journal of Experimental Biology</source><volume>215</volume><fpage>44</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1242/jeb.059584</pub-id><pub-id pub-id-type="pmid">22162852</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wystrach</surname> <given-names>A</given-names></name><name><surname>Mangan</surname> <given-names>M</given-names></name><name><surname>Philippides</surname> <given-names>A</given-names></name><name><surname>Graham</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Snapshots in ants? new interpretations of paradigmatic experiments</article-title><source>Journal of Experimental Biology</source><volume>216</volume><fpage>1766</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1242/jeb.082941</pub-id><pub-id pub-id-type="pmid">23348949</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wystrach</surname> <given-names>A</given-names></name><name><surname>Mangan</surname> <given-names>M</given-names></name><name><surname>Webb</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Optimal cue integration in ants</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>282</volume><elocation-id>20151484</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2015.1484</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wystrach</surname> <given-names>A</given-names></name><name><surname>Lagogiannis</surname> <given-names>K</given-names></name><name><surname>Webb</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Continuous lateral oscillations as a core mechanism for taxis in <italic>Drosophila</italic> larvae</article-title><source>eLife</source><volume>5</volume><elocation-id>e15504</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.15504</pub-id><pub-id pub-id-type="pmid">27751233</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xu</surname> <given-names>CS</given-names></name><name><surname>Januszewski</surname> <given-names>M</given-names></name><name><surname>Lu</surname> <given-names>Z</given-names></name><name><surname>Takemura</surname> <given-names>S-y</given-names></name><name><surname>Hayworth</surname> <given-names>K</given-names></name><name><surname>Huang</surname> <given-names>G</given-names></name><name><surname>Shinomiya</surname> <given-names>K</given-names></name><name><surname>Maitin-Shepard</surname> <given-names>J</given-names></name><name><surname>Ackerman</surname> <given-names>D</given-names></name><name><surname>Berg</surname> <given-names>S</given-names></name><name><surname>Blakely</surname> <given-names>T</given-names></name><name><surname>Bogovic</surname> <given-names>J</given-names></name><name><surname>Clements</surname> <given-names>J</given-names></name><name><surname>Dolafi</surname> <given-names>T</given-names></name><name><surname>Hubbard</surname> <given-names>P</given-names></name><name><surname>Kainmueller</surname> <given-names>D</given-names></name><name><surname>Katz</surname> <given-names>W</given-names></name><name><surname>Kawase</surname> <given-names>T</given-names></name><name><surname>Khairy</surname> <given-names>KA</given-names></name><name><surname>Leavitt</surname> <given-names>L</given-names></name><name><surname>Lindsey</surname> <given-names>L</given-names></name><name><surname>Neubarth</surname> <given-names>N</given-names></name><name><surname>Olbris</surname> <given-names>DJ</given-names></name><name><surname>Otsuna</surname> <given-names>H</given-names></name><name><surname>Troutman</surname> <given-names>ET</given-names></name><name><surname>Umayam</surname> <given-names>L</given-names></name><name><surname>Zhao</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A connectome of the adult <italic>Drosophila</italic> central brain</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.21.911859</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yilmaz</surname> <given-names>A</given-names></name><name><surname>Grübel</surname> <given-names>K</given-names></name><name><surname>Spaethe</surname> <given-names>J</given-names></name><name><surname>Rössler</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distributed plasticity in ant visual pathways following colour learning</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>286</volume><elocation-id>20182813</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2018.2813</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeil</surname> <given-names>J</given-names></name><name><surname>Kelber</surname> <given-names>A</given-names></name><name><surname>Voss</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Structure and function of learning flights in ground-nesting bees and wasps</article-title><source>The Journal of Experimental Biology</source><volume>199</volume><fpage>245</fpage><lpage>252</lpage><pub-id pub-id-type="pmid">9317729</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeil</surname> <given-names>J</given-names></name><name><surname>Hofmann</surname> <given-names>MI</given-names></name><name><surname>Chahl</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Catchment Areas of panoramic snapshots in outdoor scenes</article-title><source>Journal of the Optical Society of America A</source><volume>20</volume><fpage>450</fpage><lpage>469</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.20.000450</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeil</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Visual homing: an insect perspective</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>285</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.12.008</pub-id><pub-id pub-id-type="pmid">22221863</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeil</surname> <given-names>J</given-names></name><name><surname>Fleischmann</surname> <given-names>PN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The learning walks of ants (hymenoptera: formicidae)</article-title><source>Myrmecological News</source><volume>29</volume><fpage>93</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.25849/myrmecol.news_029:093</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54026.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Ramaswami</surname><given-names>Mani</given-names></name><role>Reviewing Editor</role><aff><institution>Trinity College Dublin</institution><country>Ireland</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Heinze</surname><given-names>Stanley</given-names> </name><role>Reviewer</role><aff><institution>Lund University</institution><country>Sweden</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This work builds upon prior models to propose an integrated model for computational strategies used by insects to perform their remarkable navigational feats. This new model accounts several capabilities and the flexibilities that accomplished insect navigators display such as visual homing, which were not as well accounted for earlier. The integrated model is not only an important addition to the literature on the insect central complex, but also particularly valuable because it pins specific computational functions on specific anatomical structures, making predictions that are potentially testable in the near-medium term.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A Decentralised Neural Model Explaining Optimal Integration Of Navigational Strategies in Insects&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by Mani Ramaswami as Reviewing Editor and Michael Eisen as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Stanley Heinze (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This is an original, focussed and timely study on a topic of considerable interest: computational strategies used by insects to perform their remarkable navigational feats. The authors identify shortcomings in existing models -specifically, that they do not account for the entire range of capabilities and the flexibility that the most accomplished of insect navigators display such as visual homing, i.e., the ability of the ant to return to familiar region from novel locations. They then integrate and build upon prior models to successfully fill these gaps. The integrated model is particularly valuable because it pins specific computational functions on specific anatomical structures, most notably the central complex and the mushroom body. It is an important addition to both the literature on the insect central complex, as well as to more theoretical navigational work, in particular as many predictions can be made based on the presented models making it, in principle, testable in the near-medium term. The figures are well made and the writing is compact. Nevertheless, several points need to be addressed before publication.</p><p>Essential revisions:</p><p>1) Accessibility to a broad readership. While the general text is written very well and the content is highly interesting for a life science (in particular insect neuroscience) audience, the Materials and methods section and some aspects of the reasoning behind the model are very technical. Even insect neurobiologists among the reviewers struggled to follow large parts of the methods and had never heard of Zernike moments for instance. The text should be revised to include some more intuitive and broadly accessible language that would allow a biologist to grasp at least the key principles of what is done by those initial analyses of the visual information in the model. A schematic illustration as to what Zernike moments are, maybe combined with some simple examples might help a lot. This is important as the paper is not only directed towards computational biologists, but is highly relevant also for physiologists, anatomists and behavioralists, most of whom would probably fail to grasp the essence of the new principles presented. In similar vein, the authors should ask a mammalian researcher to read the article and provide them with feedback on how accessible they found it. Simple terminology/concepts/structure names in the Abstract/Introduction should not be used until they have been introduced properly, e.g., 'route following', 'visual homing', 'anterior optic tubercle'.</p><p>2) On a similar note, the article builds on a lot of prior modeling literature in the insect navigation field, particularly the work from Barbara Webb and colleagues. Important concepts/algorithmic strategies need to be more fully explained here (with appropriate citations) rather than just being referred to in the prior literature. The Materials and methods section does a good job of this, but the Results section could benefit from more explanation to guide unfamiliar readers.</p><p>3) It is entirely reasonable that the authors combine experimental and modeling work from a range of different insect species to build different pieces of their own model. By and large they are careful to state which is which. However, they could make it clearer which assumptions are based on experimental data and which are based on prior models (i.e., not actual data). As an example, although the mushroom body has been suggested by numerous modeling studies and conceptually driven reviews to be involved in visual navigation, the experimental evidence for this is lacking, and their precise role is far from well-established.</p><p>4) It is excellent that the authors integrate useful components from prior models to construct their integrated model. Although the figures go some way towards clarifying how the different pieces might fit together, it would be useful to make even clearer what is entirely novel here and what is derived/integrated from previous work. In addition, although the authors make a testable case for the involvement of the fan-shaped body in a series of different navigational computations, controlled by the mushroom body, the figures are still somewhat complex and confusing. These should be clarified for the broader readership.</p><p>5) Neuroanatomical correspondence of model details: The paper claims that the model is in most parts biologically constrained and that most elements can be mapped onto known neurons. Where this was not possible (route following) the authors speculated about the possible implementations. While on the levels of neuropil groups this is all quite true, the details, especially in the central complex, are less clear and many of the proposed circuits have no known counterpart in any insect brain to date. This is not saying that those parts of the model are not realistic or interesting, but that the claim that they correspond to existing neurons in the central complex, is slightly misleading. Below series of obvious mix ups of cell types below, which need to be corrected (5.1), but additionally, it should be clearly stated where the model does not (yet) have a solid grounding in biology (see point 5.2). Finally, the speculative route following implementation seems at odds with neurophysiological data from various species and alternative pathways and implementations seem more likely (point 5.3).</p><p>5.1) Subsection “Mushroom Bodies As Drivers of Rotational Invariant Visual Homing”: CPU3 neurons are supposed to be a mirrored TB1 ring attractor network? Is this really what the authors want to say? CPU3 neurons are known in locusts (Heinze and Homberg, 2008), but connect the PB with the FB as columnar cells. If the authors mean CPU4 cells, these neurons are also not forming a ring-network (even though they could receive shifted compass information from TB1 cells by some means). Most simply, would not a parallel set of TB1 cells be optimally suited for this task? There are four TB1 cells for each column in the PB, potentially enough for four parallel ring attractors. These cells are neurochemically distinct and could function independently (see Beetz et al., 2015).</p><p>– There is no known direct connection between the EB and the FB (proposed in Figure 4).</p><p>– There is no direct connection from the OL to the CX (indicated in the legend of Figure 1 as underlying PI).</p><p>– Subsection “Celestial current heading”: CL2 neurons should be CL1 (CL2 correspond to fly P-EN neurons, not E-PG)</p><p>– In the PI section of the Materials and methods, sometimes TN cells are referred to as TN2 cells or just as TN cells. TN2 is one of two types of TN cells (tangential noduli neurons) and was the one primarily used for the standard model of Stone et al., 2017. Please be consistent. Also, the tuning cells of the visual homing circuit are called TN cells. This is very confusing and should be changed.</p><p>5.2) There are no known ring attractors in the FB. The only ring attractor shown experimentally is the one in the EB/PB, which employs recurrent feedback loops with the PB (E-PG/P-EN/P-EG cells; equal to CL1a, CL2, and CL1b) and inhibitory neurons in the PB (TB1 or delta7 cells). While a similar recurrent connection pattern is thinkable in the FB as well, using unknown types of columnar cells, there is no experimental support for that. Pontine cells might also form local connections that could result in a RA, but that is even more speculative. Please clearly state that the numerous RAs required by the model are hypothetical and have not yet any biological correspondence in the form of identified cell types. Also, I suppose not all the neuron rings drawn in the figures are ring attractors. I suggest to make that distinction more clear (the many abbreviations for the different neuron rings do not make this easier to follow either).</p><p>5.3) The authors assume a second compass system in the PB that is fed directly from the OL via the posterior optical tract. There is no evidence for this beyond a single cell type from locusts that connects the accessory medulla (circadian clock) to the POTU, which is also innervated by TB1 neurons. However, there is no connection to the visual part of the OL, and no physiological data exists on the AME-&gt;POTU connection. In contrast, the anterior optic tract via the AOTU has been shown in <italic>Drosophila</italic> to contain many neurons that respond to visual features and they converge on the head direction cells in the EB via a recently resolved mechanism. It seems odd to ignore this known compass pathway and propose another one for which no evidence exists. That said, the authors use the anterior pathway to construct a desired heading via an ANN residing in the AOTU/BU pathway, information that is then used to feed into an EB ring attractor that then connects to additional attractors in the FB. Whereas the EB attractor (in conjunction with the PB) exists, there is no evidence for FB based ring attractors and there is no known direct connection between the EB and the FB. While this all results in a really nice figure, it unfortunately is misleading and based on not enough evidence to show it so prominently (readers might easily take it for factual).</p><p>It is useful to point out that there is an alternative solution for at least the compass problem: There are four individual CL1 cells in each column of the EB in locusts as well as in flies (EPG/PEG cells). While they are identical in their projection patterns, some connect the PB to the EB and others connect the EB to the PB, so that there are in theory enough cells to form two parallel recurrent loops (needed to maintain a head direction signal). One of them could be driven by landmarks, while the other could be driven by global compass cues. Whereas the current idea is that both inputs converge on a single head direction signal (celestial and local cue based), this might not be true, given that local cues have been tested in <italic>Drosophila</italic> and global cues in locusts and some other species. These neurons are neurochemically distinct and most likely play different functional roles.</p><p>Finally with respect to the desired heading, a short term plasticity based, associative mechanism linking the phase of the head direction signal and the local environment was recently demonstrated in <italic>Drosophila</italic> (Fisher at al., 2019 and Kim et al., 2019). The authors state that several of these phases can be stored and retrieved in each respective environment. This sounds very close to what the authors of the current study suggest for routes in ants. The authors should consider these points and revise the proposed circuit identity accordingly.</p><p>6) The overall layout of the model could be further clarified. The authors present many (nicely illustrated) parts of the model, but it is difficult to reconcile some of the partial models with one another and there is no immediate way of seeing how many neurons there are overall, or what their complete connectivity patterns might be. This may be obvious from the code itself, but behavioural biologists, neuroanatomists and physiologists need to be provided more direct intuition for the circuits. The absence of this information hinders independent interpretation and finding alternative solutions for mapping the model onto anatomical neural circuits once newly discovered neurons become available in the future. One possibility is to include (at least in the supplements) a full graphical depiction of the model with all existing neurons and their connections. Maybe using a force directed graph diagram like used by the authors of Stone et al., 2017 for their path integration model results in a model illustration that is intuitively understandable for researchers who think more in terms of anatomy. But even if it turns out to be somewhat messy, it would still be helpful.</p><p>7) The authors' could derive more constraints from the fly physiology literature than they do. As examples, Fisher et al., 2019 and Kim et al., 2019 have relevant findings relating to plasticity in mapping visual stimuli onto a compass representation. Turner-Evans et al., 2017 has a data-driven ring attractor model that is relevant, and Turner-Evans, 2019 features data demonstrating that the fly compass for current heading relies on visual input from the anterior optic tubercle, contrary to the authors' assumption deriving from an anatomical pathway from the posterior optic tubercle to the protocerebral bridge (175-176). On a somewhat related note, the fly heading system does not necessarily show 'bar following' in open loop: the experiments cited (Seelig and Jayaraman, 2015) were performed in closed loop, with the animal controlling bar position.</p><p>8) The authors should also release and include an properly commented code that they used for modelling in the final submission.</p><p>9) Why is the velocity of the simulated ant (Vo = 1cm/s) so much slower than that of the real one (about 50cm/s)? This point must be discussed. Is there any fundamental reason?</p><p>10) What would happen to the simulated ant if an obstacle was placed on the familiar route ? What is the robustness of the Zernike-based moment algorithm to the unpredicted presence of an obstacle that could appear during the homing ? Additional simulations to address this issue could show the robustness of the proposed navigation model. These new simulations could be in line with the well-known experiments proposed by Wehner and Wehner (R. Wehner and S. Wehner, “Insect navigation : use of maps or Ariadne's thread?”).</p><p>11) Subsection “ANN network and Route Following”: would it be possible to plot Crf with respect to angular orientation of the simulated ant in various place (every 10° steps for example).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;A Decentralised Neural Model Explaining Optimal Integration of Navigational Strategies in Insects&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by Mani Ramaswami as Reviewing Editor and Michael Eisen as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Stanley Heinze (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another, expressed enthusiasm and appreciation for the revisions you made to the original submission, but still have some suggestions you should consider for further improvement of this article. The Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>Summary:</p><p>This is an original, focussed and timely study on a topic of considerable interest: computational strategies used by insects to perform their remarkable navigational feats. The authors identify shortcomings in existing models -specifically, that they do not account for the entire range of capabilities and the flexibility that the most accomplished of insect navigators display such as visual homing, i.e., the ability of the ant to return to familiar region from novel locations. They then integrate and build upon prior models to successfully fill these gaps. The integrated model is particularly valuable because it pins specific computational functions on specific anatomical structures, most notably the central complex and the mushroom body. It is an important addition to both the literature on the insect central complex, as well as to more theoretical navigational work, in particular as many predictions can be made based on the presented models making it, in principle, testable in the near-medium term. The figures are well made and the writing is compact, the revisions are extensively, carefully done and, after minor edits, the paper should be ready for publication.</p><p>Larger points for consideration (Optional revisions at the authors’ discretion):</p><p>1) For increased accessibility and readability, please consider doing a little more with the earliest schematics to properly orient the broader readership. As an example, Figure 2A is fairly complicated for a reader who isn't familiar with the insect brain, and the panels at right will not be easy for most people to digest without Stone et al., 2017, open nearby (although the Stone et al. study is a must-read for anyone interested in insect navigation, this may not be the ideal way to get people to read the paper!). Could 'Shifted I-TB1' be unpacked a little by showing how the anatomy of the PB-FB columnar neurons might naturally facilitate the shift (as highlighted in the Stone et al. paper)-perhaps this could be a little breakout box at right. Note that the TB neurons should, in any case, be shown in the PB not the FB.</p><p>2).Similarly, although the vector subtraction plots below are helpful to the informed reader, it is not clear that they would be sufficiently explanatory to a newer reader. Again, it is the authors' decision whether or not to do more here.</p><p>3) A bigger, conceptual point: it is not obvious that one needs as many additional near-independent ring attractors as are invoked in this model, leaving aside the issue that they seem unlikely from an anatomical perspective. It is not clear or convincing that multiple ring attractors are needed to implement the authors' ideas. This potential opportunity for parsimony deserves some exploration, but the authors can decide whether that's something they want to do as part of this paper or not. The one thing that would be good is to make clear where the additional ring attractors reside in the authors' model: if they are speculatively placed in the FB, that should be made clearer in the early schematics (and also made clear that it is speculation at this stage).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54026.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Accessibility to a broad readership. While the general text is written very well and the content is highly interesting for a life science (in particular insect neuroscience) audience, the Materials and methods section and some aspects of the reasoning behind the model are very technical. Even insect neurobiologists among the reviewers struggled to follow large parts of the Materials and methods and had never heard of Zernike moments for instance. The text should be revised to include some more intuitive and broadly accessible language that would allow a biologist to grasp at least the key principles of what is done by those initial analyses of the visual information in the model. A schematic illustration as to what Zernike moments are, maybe combined with some simple examples might help a lot. This is important as the paper is not only directed towards computational biologists, but is highly relevant also for physiologists, anatomists and behavioralists, most of whom would probably fail to grasp the essence of the new principles presented.</p></disp-quote><p>We have added a smoother introduction to results arising from frequency encoded views: specifically in the sections titled Visual Homing and Route Following. In addition, we have added a completely new section to the Materials and methods titled &quot;Frequency Encoding Conceptual Overview&quot; which provides an intuitive description of frequency encoding that prefaces the mathematical description of the Zernike Moment method. This section is now accompanied by an updated Figure 7 which includes both a cartoon depicting frequency encoding followed by the real Zernike Moments encoded of skylines from the simulated world. Finally, we have added a paragraph to the Discussion which looks at how the model could be improved which includes a discussion of more biologically plausible methods for frequency encoding. We believe that this part of our contribution should be much clearer to the non-expert reader now.</p><disp-quote content-type="editor-comment"><p>In similar vein, the authors should ask a mammalian researcher to read the article and provide them with feedback on how accessible they found it.</p></disp-quote><p>We have discussed the presentation of our data to other researchers both with experience in insect and mammalian navigation both personally and via presenting (e.g. student presentation at the recent NeuroMatch conference). These discussions have helped us immensely revise our presentation of challenging concepts – see new introduction to frequency encoding (section &quot;Frequency Encoding Conceptual Overview&quot;) as an example.</p><disp-quote content-type="editor-comment"><p>Simple terminology/concepts/structure names in the Abstract/Introduction should not be used until they have been introduced properly, e.g., 'route following', 'visual homing', 'anterior optic tubercle'.</p></disp-quote><p>The Abstract has been updated with definitions of route-following and visual homing and we have made it clearer that we map these behaviours to specific neural circuits in the insect brains e.g. Mushroom Bodies and Anterior Optic Tubercle. Upon re-reading the text we believe that this issue largely arose from pointing readers to Figure 1 before definitions were made in the Results. Hence, we have updated Figure 1 and Figure 1’s legend to provide a more intuitive introduction to our model and associated brain areas with more clearer labelling and definitions. Similar edits have been made in the Introduction which we believe when taken together addresses this issue.</p><disp-quote content-type="editor-comment"><p>2) On a similar note, the article builds on a lot of prior modeling literature in the insect navigation field, particularly the work from Barbara Webb and colleagues. Important concepts/algorithmic strategies need to be more fully explained here (with appropriate citations) rather than just being referred to in the prior literature. The Materials and methods section does a good job of this, but the Results section could benefit from more explanation to guide unfamiliar readers.</p></disp-quote><p>As suggested we have added a detailed description of the functioning of the steering circuit as outlined by Stone et al., 2017, where it is first used (see section titled &quot;Mushroom Bodies as Drivers of Rotational Invariant Visual Homing&quot;). This is accompanied by an additional panel in Figure 2 depicting the steering circuit function in vector format. We have also described a more detailed explanation of the neurophysiological and functional advances made in Stone et al., 2017, regarding the function of PI (see section titled: &quot;Optimally Integrating Visual Homing and Path Integration&quot;).</p><disp-quote content-type="editor-comment"><p>3) It is entirely reasonable that the authors combine experimental and modeling work from a range of different insect species to build different pieces of their own model. By and large they are careful to state which is which. However, they could make it clearer which assumptions are based on experimental data and which are based on prior models (i.e., not actual data). As an example, although the mushroom body has been suggested by numerous modeling studies and conceptually driven reviews to be involved in visual navigation, the experimental evidence for this is lacking, and their precise role is far from well-established.</p></disp-quote><p>We have added clarification to the text describing our MB model which we believe is the only section based on only prior models. Further we have added clarification of what neural paths are known and those that we speculate through the use of dashed (speculated) and solid (known) connections throughout our figures. In addition, we have added Table 2 which details the neurophysiological studies on which we base our models making it clear which elements are biologically known and those that are hypothesised.</p><disp-quote content-type="editor-comment"><p>4) It is excellent that the authors integrate useful components from prior models to construct their integrated model. Although the figures go some way towards clarifying how the different pieces might fit together, it would be useful to make even clearer what is entirely novel here and what is derived/integrated from previous work.</p></disp-quote><p>As part of our update to all figures we introduced a star labelling of circuits to indicate which elements were derived from previous works, which were completely novel, and those that are a mixture e.g. previous circuit but used in a novel way, adapted, integrated with other systems. See Figure 2 for its first usage.</p><disp-quote content-type="editor-comment"><p>In addition, although the authors make a testable case for the involvement of the fan-shaped body in a series of different navigational computations, controlled by the mushroom body, the figures are still somewhat complex and confusing. These should be clarified for the broader readership.</p></disp-quote><p>Each of the main figures and their legends have been revised and we now believe that they should be much clearer now. In addition, the new added Figure 6 should be helpful to show the neural connections in fan-shape body.</p><disp-quote content-type="editor-comment"><p>5) Neuroanatomical correspondence of model details: The paper claims that the model is in most parts biologically constrained and that most elements can be mapped onto known neurons. Where this was not possible (route following) the authors speculated about the possible implementations. While on the levels of neuropil groups this is all quite true, the details, especially in the central complex, are less clear and many of the proposed circuits have no known counterpart in any insect brain to date. This is not saying that those parts of the model are not realistic or interesting, but that the claim that they correspond to existing neurons in the central complex, is slightly misleading. Below series of obvious mix ups of cell types below, which need to be corrected (5.1), but additionally, it should be clearly stated where the model does not (yet) have a solid grounding in biology (see point 5.2). Finally, the speculative route following implementation seems at odds with neurophysiological data from various species and alternative pathways and implementations seem more likely (point 5.3).</p></disp-quote><p>This feedbacks is very helpful, thanks very much. See below the changes made accordingly.</p><disp-quote content-type="editor-comment"><p>5.1) Subsection “Mushroom Bodies As Drivers of Rotational Invariant Visual Homing”: CPU3 neurons are supposed to be a mirrored TB1 ring attractor network? Is this really what the authors want to say? CPU3 neurons are known in locusts (Heinze and Homberg, 2008), but connect the PB with the FB as columnar cells. If the authors mean CPU4 cells, these neurons are also not forming a ring-network (even though they could receive shifted compass information from TB1 cells by some means). Most simply, would not a parallel set of TB1 cells be optimally suited for this task? There are four TB1 cells for each column in the PB, potentially enough for four parallel ring attractors. These cells are neurochemically distinct and could function independently (see Beetz et al., 2015).</p></disp-quote><p>Thanks for the feedback. We have changed the text (subsection “Mushroom bodies as drivers of rotational invariant visual homing”, fourth paragraph). See also the response to point 5.3.</p><disp-quote content-type="editor-comment"><p>– There is no known direct connection between the EB and the FB (proposed in Figure 4)</p></disp-quote><p>We have amended both the text and figure to show that this connection is included in our hypothesised pathways but also cite Hanesch et al., 1989, who show evidence for such a connection.</p><disp-quote content-type="editor-comment"><p>– There is no direct connection from the OL to the CX (indicated in the legend of Figure 1 as underlying PI).</p></disp-quote><p>We have amended the model pathway accordingly to 'OL-&gt;AUTO-&gt;LAL-&gt;CX'.</p><disp-quote content-type="editor-comment"><p>– Subsection “Celestial current heading”: CL2 neurons should be CL1 (CL2 correspond to fly P-EN neurons, not E-PG)</p></disp-quote><p>Changed labels to 'CL1' as suggested</p><disp-quote content-type="editor-comment"><p>– In the PI section of the Materials and methods, sometimes TN cells are referred to as TN2 cells or just as TN cells. TN2 is one of two types of TN cells (tangential noduli neurons) and was the one primarily used for the standard model of Stone et al., 2017. Please be consistent. Also, the tuning cells of the visual homing circuit are called TN cells. This is very confusing and should be changed.</p></disp-quote><p>We have now changed all TN to be TN2.</p><disp-quote content-type="editor-comment"><p>5.2) There are no known ring attractors in the FB. The only ring attractor shown experimentally is the one in the EB/PB, which employs recurrent feedback loops with the PB (E-PG/P-EN/P-EG cells; equal to CL1a, CL2, and CL1b) and inhibitory neurons in the PB (TB1 or delta7 cells). While a similar recurrent connection pattern is thinkable in the FB as well, using unknown types of columnar cells, there is no experimental support for that. Pontine cells might also form local connections that could result in a RA, but that is even more speculative. Please clearly state that the numerous RAs required by the model are hypothetical and have not yet any biological correspondence in the form of identified cell types. Also, I suppose not all the neuron rings drawn in the figures are ring attractors. I suggest to make that distinction more clear (the many abbreviations for the different neuron rings do not make this easier to follow either).</p></disp-quote><p>Thanks for this feedback. We only propose one new ring attractor in our model which is used to combine PI and VH signals, but on re-reading our text we see that this was not clear. We have added a sentence to the manuscript where you suggested to clarify the difference between ring networks and ring attractor networks. In addition, we have added labels to the figures to clearly indicate where ring attractors are used. Finally, the new Table 2 also provides of description of each circuit element e.g. ring network vs. ring attractor network, and also their biological supports.</p><disp-quote content-type="editor-comment"><p>5.3) The authors assume a second compass system in the PB that is fed directly from the OL via the posterior optical tract. There is no evidence for this beyond a single cell type from locusts that connects the accessory medulla (circadian clock) to the POTU, which is also innervated by TB1 neurons. However, there is no connection to the visual part of the OL, and no physiological data exists on the AME-&gt;POTU connection. In contrast, the anterior optic tract via the AOTU has been shown in <italic>Drosophila</italic> to contain many neurons that respond to visual features and they converge on the head direction cells in the EB via a recently resolved mechanism. It seems odd to ignore this known compass pathway and propose another one for which no evidence exists. That said, the authors use the anterior pathway to construct a desired heading via an ANN residing in the AOTU/BU pathway, information that is then used to feed into an EB ring attractor that then connects to additional attractors in the FB. Whereas the EB attractor (in conjunction with the PB) exists, there is no evidence for FB based ring attractors and there is no known direct connection between the EB and the FB. While this all results in a really nice figure, it unfortunately is misleading and based on not enough evidence to show it so prominently (readers might easily take it for factual).</p><p>It is useful to point out that there is an alternative solution for at least the compass problem: There are four individual CL1 cells in each column of the EB in locusts as well as in flies (EPG/PEG cells). While they are identical in their projection patterns, some connect the PB to the EB and others connect the EB to the PB, so that there are in theory enough cells to form two parallel recurrent loops (needed to maintain a head direction signal). One of them could be driven by landmarks, while the other could be driven by global compass cues. Whereas the current idea is that both inputs converge on a single head direction signal (celestial and local cue based), this might not be true, given that local cues have been tested in <italic>Drosophila</italic> and global cues in locusts and some other species. These neurons are neurochemically distinct and most likely play different functional roles.</p></disp-quote><p>This is very interesting and helpful price of feedback. Thank you. We have amended our model to generate the terrestrial heading (local compass) pathway to OL-&gt;AOTU-&gt;BULB-&gt;EB-&gt;PB to be consistent with the neural data. We have updated our model accordingly which can be seen in Figure 1C, Figure 4A also Figure 6A. Specifically there are four individual TB1/Δ7 cells in each column, which can be used to represent different current-headings. We apply one pathway to generate the Global Compass (celestial heading for VH and PI, I-TB1 neurons), another to generate Local Compass (terrestrial heading for RF, II-TB1 neurons). We have also update the text in the description of the models and in the Discussion.</p><disp-quote content-type="editor-comment"><p>Finally with respect to the desired heading, a short term plasticity based, associative mechanism linking the phase of the head direction signal and the local environment was recently demonstrated in <italic>Drosophila</italic> (Fisher et al., 2019 and Kim et al., 2019). The authors state that several of these phases can be stored and retrieved in each respective environment. This sounds very close to what the authors of the current study suggest for routes in ants. The authors should consider these points and revise the proposed circuit identity accordingly.</p></disp-quote><p>Thank you for this feedback. We have now added these references to our discussion of the possible biological evidence for, and biological pathways for this type of local compass information (see section &quot;Route Following in the Insect Brain&quot;), and adapted our model accordingly. These works have helped us formulate discussions of insects possessing multiple compass systems and the insects’ ability to correlate the views with the orientations, which was very helpful. Finally, we have added some text to the Discussion inspired by these works regarding identifying which groups of neurons in the CX might be encoding for current vs. desired headings.</p><disp-quote content-type="editor-comment"><p>6) The overall layout of the model could be further clarified. The authors present many (nicely illustrated) parts of the model, but it is difficult to reconcile some of the partial models with one another and there is no immediate way of seeing how many neurons there are overall, or what their complete connectivity patterns might be. This may be obvious from the code itself, but behavioural biologists, neuroanatomists and physiologists need to be provided more direct intuition for the circuits. The absence of this information hinders independent interpretation and finding alternative solutions for mapping the model onto anatomical neural circuits once newly discovered neurons become available in the future. One possibility is to include (at least in the supplements) a full graphical depiction of the model with all existing neurons and their connections. Maybe using a force directed graph diagram like used by the authors of Stone et al., 2017, for their path integration model results in a model illustration that is intuitively understandable for researchers who think more in terms of anatomy. But even if it turns out to be somewhat messy, it would still be helpful.</p></disp-quote><p>That’s a really good suggestion. Thanks. We have added both a new force-directed-graph (Figure 6, Materials and methods section) and a table (Table 2, Materials and methods section) that clarifies the type and function of all neurons and their connection that comprise the model.</p><disp-quote content-type="editor-comment"><p>7) The authors' could derive more constraints from the fly physiology literature than they do. As examples, Fisher et al., 2019 and Kim et al., 2019 have relevant findings relating to plasticity in mapping visual stimuli onto a compass representation. Turner-Evans et al., 2017 has a data-driven ring attractor model that is relevant, and Turner-Evans, 2019 features data demonstrating that the fly compass for current heading relies on visual input from the anterior optic tubercle, contrary to the authors' assumption deriving from an anatomical pathway from the posterior optic tubercle to the protocerebral bridge (175-176). On a somewhat related note, the fly heading system does not necessarily show 'bar following' in open loop: the experiments cited (Seelig and Jayaraman, 2015) were performed in closed loop, with the animal controlling bar position.</p></disp-quote><p>Thank you for this feedback. We have changed the neural pathway to generate the terrestrial heading (local compass) as suggested. See response to point 5.3.</p><disp-quote content-type="editor-comment"><p>8) The authors should also release and include an properly commented code that they used for modelling in the final submission.</p></disp-quote><p>Additional comments of the source code, the simulation implementations and a GUI have been uploaded through the full submission and also upload to Github repository: https://github.com/XuelongSun/InsectNavigationToolkitModelling.</p><disp-quote content-type="editor-comment"><p>9) Why is the velocity of the simulated ant (Vo = 1cm/s) so much slower than that of the real one (about 50cm/s)? This point must be discussed. Is there any fundamental reason?</p></disp-quote><p>Apologies there was a typo in our description of the model velocity that stated 1cm/s, but this should be 1cm/step. There is no link between the speed in the simulation and computation. Rather than simulated ant moves in 1cm steps, computes the direction move, takes a step and repeats. We have now corrected this in the text (Materials and methods section).</p><disp-quote content-type="editor-comment"><p>10) What would happen to the simulated ant if an obstacle was placed on the familiar route ? What is the robustness of the Zernike-based moment algorithm to the unpredicted presence of an obstacle that could appear during the homing ? Additional simulations to address this issue could show the robustness of the proposed navigation model. These new simulations could be in line with the well-known experiments proposed by Wehner and Wehner (R. Wehner and S. Wehner, “ Insect navigation : use of maps or Ariadne's thread?”).</p></disp-quote><p>This is a lovely idea and we have started to simulate such scenarios but believe that this is beyond the initial proof of concept nature of this study and better suited to a comparative study between this model and others across environments and with an investigation of parameters which will likely have a large effect on the performance. We have added a sentence to the Discussion raising exactly this idea as a logical next step in evaluating the model.</p><disp-quote content-type="editor-comment"><p>11) Subsection “ANN network and Route Following”: would it be possible to plot Crf with respect to angular orientation of the simulated ant in various place (every 10° steps for example).</p></disp-quote><p>We investigated plotting the data using crf as the reviewer suggested but as the preferred orientation changes with location this was very hard to visualise across locations. We found that this data was more intuitively presented using the quiver plots in the background of Figure 4B.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Larger points for consideration (Optional revisions at the authors’ discretion):</p><p>1) For increased accessibility and readability, please consider doing a little more with the earliest schematics to properly orient the broader readership. As an example, Figure 2A is fairly complicated for a reader who isn't familiar with the insect brain, and the panels at right will not be easy for most people to digest without Stone et al., 2017, open nearby (although the Stone et al. study is a must-read for anyone interested in insect navigation, this may not be the ideal way to get people to read the paper!). Could 'Shifted I-TB1' be unpacked a little by showing how the anatomy of the PB-FB columnar neurons might naturally facilitate the shift (as highlighted in the Stone et al. paper)-perhaps this could be a little breakout box at right. Note that the TB neurons should, in any case, be shown in the PB not the FB.</p></disp-quote><p>Thank you for the feedback. We have now revised Figure 2 to try and address the key issues. Specifically, we have replaced previous vector diagram and VH schematic with a combined schematic that we hope makes it easier to conceptually understand (a) how the steering circuit functions (b) how VH functions through a simple shifted heading input to the steering circuit. On reflection we think that if the readers understand these points then much of what follows should follow, and we think that these schematics should allow that.</p><p>Also, as requested we have relabelled what we previously called &quot;shifted ITB1&quot; neurons to VH as we can only speculate at this stage which neurons would store this signal.</p><p>Regarding adding detail to the shifting circuit. We considered this feedback and agree that it deserves addressing but we prefer to add a discussion of possible shifting mechanism to the text where we have space for some details and to add speculation.</p><disp-quote content-type="editor-comment"><p>2) Similarly, although the vector subtraction plots below are helpful to the informed reader, it is not clear that they would be sufficiently explanatory to a newer reader. Again, it is the authors' decision whether or not to do more here.</p></disp-quote><p>Finally, we have removed the vector plot entirely now as it has been superseded by the new Figure 2B and C.</p><disp-quote content-type="editor-comment"><p>3) A bigger, conceptual point: it is not obvious that one needs as many additional near-independent ring attractors as are invoked in this model, leaving aside the issue that they seem unlikely from an anatomical perspective. It is not clear or convincing that multiple ring attractors are needed to implement the authors' ideas. This potential opportunity for parsimony deserves some exploration, but the authors can decide whether that's something they want to do as part of this paper or not. The one thing that would be good is to make clear where the additional ring attractors reside in the authors' model: if they are speculatively placed in the FB, that should be made clearer in the early schematics (and also made clear that it is speculation at this stage).</p></disp-quote><p>We have added two paragraphs to the Discussion to address these points directly. Specifically, we clarify the benefits of the proposed RAs which may make them more parsimonious than alternate optimal integration networks. Also, we have clarified how many RAs we propose which we feel was confusing in previous edits. Finally, we add a call for future analysis of biological realism indicating that simpler mechanisms cannot be ruled out. We have included a label to the left of Figure 3A that indicates that we propose that the new integrating ring attractors reside in the FB. We have also added this clarification to the figure legend, and in the general discussion in the main text.</p></body></sub-article></article>