<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">56477</article-id><article-id pub-id-type="doi">10.7554/eLife.56477</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Private–public mappings in human prefrontal cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-54201"><name><surname>Bang</surname><given-names>Dan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7446-7090</contrib-id><email>danbang.db@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177269"><name><surname>Ershadmanesh</surname><given-names>Sara</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177268"><name><surname>Nili</surname><given-names>Hamed</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-79903"><name><surname>Fleming</surname><given-names>Stephen M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0233-4891</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Wellcome Centre for Human Neuroimaging, UCL</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>Department of Experimental Psychology, University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>School of Cognitive Sciences, Institute for Research in Fundamental Sciences</institution><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff><aff id="aff4"><label>4</label><institution>Wellcome Centre for Integrative Neuroimaging, Centre for Functional Magnetic Resonance Imaging of the Brain, University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff5"><label>5</label><institution>Max Planck UCL Centre for Computational Psychiatry and Ageing Research, UCL</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff6"><label>6</label><institution>Department of Experimental Psychology, UCL</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>23</day><month>07</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e56477</elocation-id><history><date date-type="received" iso-8601-date="2020-02-28"><day>28</day><month>02</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-07-01"><day>01</day><month>07</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Bang et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Bang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-56477-v1.pdf"/><abstract><p>A core feature of human cognition is an ability to separate private states of mind – what we think or believe – from public actions – what we say or do. This ability is central to successful social interaction – with different social contexts often requiring different mappings between private states and public actions in order to minimise conflict and facilitate communication. Here we investigated how the human brain supports private-public mappings, using an interactive task which required subjects to adapt how they communicated their confidence about a perceptual decision to the social context. Univariate and multivariate analysis of fMRI data revealed that a private-public distinction is reflected in a medial-lateral division of prefrontal cortex – with lateral frontal pole (FPl) supporting the context-dependent mapping from a private sense of confidence to a public report. The concept of private-public mappings provides a promising framework for understanding flexible social behaviour.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>prefrontal cortex</kwd><kwd>context</kwd><kwd>decision-making</kwd><kwd>social cognition</kwd><kwd>cognitive control</kwd><kwd>metacognition</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>213630/Z/18/Z</award-id><principal-award-recipient><name><surname>Bang</surname><given-names>Dan</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>206648/Z/17/Z</award-id><principal-award-recipient><name><surname>Fleming</surname><given-names>Stephen M</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><award-id>206648/Z/17/Z</award-id><principal-award-recipient><name><surname>Fleming</surname><given-names>Stephen M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A distinction between private and public aspects of mental states is reflected in a medial-lateral division of human prefrontal cortex.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A striking aspect of human social behaviour is how often we say things we do not really mean simply because the situation requires it. For example, a politician may have doubts about a policy but, in order to boost voters’ faith in her, defend it with great confidence. Conversely, an employee may be certain that his manager is wrong but, in order to minimise social friction, tentatively present his argument. Central to these scenarios is a distinction between private states of mind – what we think or believe – and public actions – what we say or do. A general ability to separate private and public aspects of mental states is thought to have evolved under the constraints of social communication (<xref ref-type="bibr" rid="bib18">Dennett, 2017</xref>). In having a ‘control buffer’ between private states and public actions, an agent may avoid revealing its current state to competitors (e.g., by suppressing signs of fragility). It also allows an agent to deceive competitors (e.g., by displaying signs of strength) without interfering with processes that are needed for resolving its current state (e.g., taking steps to replenish energy).</p><p>In modern life, where people navigate complex social worlds, this ability to adapt private-public mappings to the context takes on new functions – including minimising conflict and facilitating communication. For example, social norms pertaining to politeness vary between contexts, such that a response which is appropriate in one context may be very inappropriate in another (e.g., commenting on a person’s appearance at home versus at work). Contextual modulation of private-public mappings is, however, particularly challenging: social norms may be arbitrary, social contexts are typically diffuse over time and space, and there is often a tension between the nature of our private states and the public actions required by the context. Indeed, context-inappropriate social behaviour is a common symptom in a variety of neuropsychiatric conditions – such as frontotemporal dementia (<xref ref-type="bibr" rid="bib34">Ibañez and Manes, 2012</xref>), autism (<xref ref-type="bibr" rid="bib30">Happé, 1994</xref>), schizophrenia (<xref ref-type="bibr" rid="bib46">Penn et al., 2002</xref>) and borderline personality disorder (<xref ref-type="bibr" rid="bib37">King-Casas et al., 2008</xref>) – at times with profound effects on quality of life.</p><p>Understanding why and how social function is impaired in these conditions is likely to be aided by a more thorough characterisation of the cognitive and neural mechanisms that underpin contextual modulation of private-public mappings. Here we studied how the brain solves this problem using decision confidence as a model system. Confidence is well-suited for studying private-public mappings because there is often a dissociation between the confidence that we privately feel and that which we publicly communicate (<xref ref-type="bibr" rid="bib1">Aitchison et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Bang et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Hertz et al., 2017</xref>). Further, recent experimental work has provided us with the tools to separately manipulate private and public aspects of confidence (<xref ref-type="bibr" rid="bib9">Bang et al., 2017</xref>; <xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>). In our experiment, subjects were required to communicate their confidence about simple perceptual decisions in different social contexts, with each context requiring a different mapping from private to public confidence. For example, one context required subjects to overstate their confidence in order to maximise reward, whereas another required them to understate their confidence.</p><p>Preliminary evidence suggests that a private-public distinction for confidence is reflected in a medial-lateral division of prefrontal cortex (PFC). First, recent work has indicated that dorsal anterior cingulate cortex (dACC) (<xref ref-type="bibr" rid="bib23">Fleming et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Fleming et al., 2018</xref>) and perigenual anterior cingulate cortex (pgACC) (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>; <xref ref-type="bibr" rid="bib17">De Martino et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Gherman and Philiastides, 2018</xref>; <xref ref-type="bibr" rid="bib39">Lebreton et al., 2015</xref>; <xref ref-type="bibr" rid="bib55">Wittmann et al., 2016</xref>) – both located in the medial wall of PFC – support the formation of an internal (private) sense of confidence. Second, a large body of literature has observed activation of lateral PFC, in particular the lateral frontal pole (FPl), in relation to explicit (public) reports of confidence (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>; <xref ref-type="bibr" rid="bib17">De Martino et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Fleming et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Fleming et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Gherman and Philiastides, 2018</xref>; <xref ref-type="bibr" rid="bib32">Hilgenstock et al., 2014</xref>; <xref ref-type="bibr" rid="bib53">Shekhar and Rahnev, 2018</xref>; <xref ref-type="bibr" rid="bib56">Yokoyama et al., 2010</xref>) – raising the possibility that lateral PFC supports a mapping between private and public confidence. This hypothesis fits with a broader role of lateral PFC in cognitive control functions such as task or set switching (<xref ref-type="bibr" rid="bib5">Badre, 2008</xref>; <xref ref-type="bibr" rid="bib8">Badre and Nee, 2018</xref>). It also fits with the observation that injury to lateral PFC is associated with context-inappropriate social behaviour (<xref ref-type="bibr" rid="bib34">Ibañez and Manes, 2012</xref>). Here, by combining univariate and multivariate analyses of fMRI data, we provide evidence for this division of labour, showing that FPl supports the context-dependent mapping from an internal sense of confidence to a public report.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Experimental manipulation of private-public mappings</title><p>Subjects (<italic>n</italic> = 28) performed a social perceptual decision task, first in a behavioural session and subsequently during fMRI (<xref ref-type="fig" rid="fig1">Figure 1</xref>). On each trial, subjects made a group decision about a visual stimulus with one of four partners. Subjects were told that the partners were created by replaying the responses of four people performing the perceptual task on a separate day but, in reality, the partners were simulated. First, subjects judged whether a field of dots was moving left or right. Next, after being informed about the identity of their partner on the current trial, subjects were asked to report their confidence in the perceptual judgement – an estimate which would enter into the group decision. Subjects were then shown the current partner’s response for that trial. Finally, implementing a common group decision rule (<xref ref-type="bibr" rid="bib9">Bang et al., 2017</xref>), the individual decision made with higher confidence was automatically selected as the group decision, after which feedback about its accuracy was delivered. Subjects were incentivised to help each group achieve as many correct group decisions as possible but, by design, could only affect group decisions through their confidence reports.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental framework for dissociating private and public confidence.</title><p>On each trial, subjects made a perceptual group decision with one of four partners. They first decided whether a random dot motion stimulus was moving left or right. We varied the fraction of coherently moving dots in order to manipulate subjects’ internal sense of confidence in their decision. Subjects were then informed about their partner on the current trial and were asked to submit a report of confidence in their initial decision (discrete scale from 1 to 6). Subjects were then shown the partner’s response, after which the individual decision made with higher confidence was selected as the group decision. Finally, subjects received feedback about choice accuracy, before continuing to the next trial. We engineered the partners to have the same choice accuracy as subjects but to differ in mean confidence. Subjects were incentivised to help each group achieve as many correct decisions as possible: they were told that we would randomly select two trials for each group in each session (4 × 2 × 2 = 16 trials) and pay £1 in bonus for every correct group decision (in reality, all subjects received £10 in bonus). In this design, the strategy for maximising group accuracy (reward) is to match your partner’s mean confidence. The structure of the task differed between the behavioural and fMRI sessions as explained in the main text.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Confidence matching maximises group accuracy and thereby reward.</title><p>The heat map shows expected group accuracy as a function of the mean confidence of two players with the same level of task performance (i.e. sensory noise). The heat map was derived analytically using the sensory noise fitted to an example subject and by assuming maximum entropy confidence distributions (see <xref ref-type="bibr" rid="bib9">Bang et al., 2017</xref>, for details on calculation). The heat map shows that expected group accuracy is highest along the identity line: that is, when a subject’s mean confidence (y-axis) is matched to that of the current partner (x-axis; the four avatars indicate the four partners’ mean confidence as specified in the task). Under our incentive structure, expected reward is proportional to expected group accuracy: the higher the expected group accuracy, the higher the probability that a randomly selected group decision will be correct.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Schematic of study protocol.</title><p>Subjects took part in separate behavioural and fMRI sessions on the same day. The prescan session involved four phases. In phase 1, we calibrated four levels of coherence so as to achieve target levels of choice accuracy (60%, 70%, 80% and 90%). In phases 2–4, we trained subjects on the social task. In phase 2, subjects were paired with the partners in a block-wise manner (each partner is indicated by a unique colour and name). There were four cycles of blocks of 10 trials per partner (e.g., <bold>A–B–C–D–A–B–C–D–A–B–C–D–A–B–C–D</bold>). The context screen was shown before each block of trials but not after a perceptual decision. In phase 3, subjects were paired with the four partners in an interleaved manner, with the current partner’s identity revealed after each perceptual decision. In phase 4, the ‘showdown’ stage was played out in the background. In addition, we introduced a condition where the social context was hidden. The scan session involved four runs, using the same design as in phase 4 of the prescan session. We matched the distribution of conditions (coherence × context) across scan runs in order to facilitate multivariate analysis of the fMRI data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Confidence distributions used to generate four partners who differ in mean confidence.</title><p>The distributions were constructed so as to have maximum entropy for a given level of mean confidence (see <xref ref-type="bibr" rid="bib9">Bang et al., 2017</xref>, for details on calculation).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig1-figsupp3-v1.tif"/></fig></fig-group><p>We varied two features of the task in a factorial (4 × 4) manner. First, we varied the fraction of coherently moving dots (coherence) to manipulate subjects’ internal sense of confidence in a perceptual judgement. In general, the higher the coherence, the higher subjects’ confidence. Second, we engineered the partners (context) to have the same choice accuracy as subjects but to differ in mean confidence. In this case, the strategy for maximising group accuracy (and thereby reward) is to match a partner’s mean confidence (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). If a subject always reports higher confidence than a partner, then they would not benefit from the trials where they were wrong but the partner was correct. In contrast, if a subject always reports lower confidence than a partner, then they would not benefit from the trials where the partner was wrong but they were correct. The design thus allowed us to separate public from private confidence and thereby probe private-public mappings in behaviour and brain activity.</p><p>The aim of the behavioural session was to calibrate levels of coherence so as to achieve target levels of choice accuracy and to train subjects on the social task (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). The training had three phases. First, subjects were paired with the four partners in a block-wise manner. Second, the partners were interleaved, with the identity of the current partner revealed after the perceptual choice. Finally, using the same design as in the fMRI session, the ‘showdown’ stage was played out in the background, with subjects not seeing the partner’s response or the group outcome. This change, which was introduced to minimise inter-trial dependencies in behavioural and neural responses, meant that subjects had to rely on their knowledge (expectations) about the partners learned in the preceding behavioural session. In addition, to establish a baseline for behavioural and neural responses, we added a condition where the partner’s identity was hidden. The fMRI session consisted of four scan runs, with the distribution of conditions matched across runs in order to facilitate multivariate analysis of the fMRI data (i.e. four trials for each coherence x context condition in each scan run). To help subjects keep track of the behaviour of each group, they were informed every 20 trials how often their individual decision had been selected as the group decision for a particular partner. These selection statistics were reset after each scan run.</p></sec><sec id="s2-2"><title>Confidence reports reflect motion coherence and social context</title><p>We first tested whether subjects’ confidence reports in the fMRI session varied as a function of our factorial design (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for analysis of prescan session). As intended, subjects’ confidence reports were influenced by both motion coherence and social context (<xref ref-type="fig" rid="fig2">Figure 2</xref>, ordinal regression; coherence: <italic>t</italic>(27) = 6.95, p&lt;0.001, context: <italic>t</italic>(27) = 4.82, p&lt;0.001, interaction: <italic>t</italic>(27) = −0.03, p=0.975). More specifically, subjects’ reported confidence increased with the level of coherence (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) and the mean confidence of the current partner (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). In other words, the confidence reported for a specific level of coherence depended on the current partner’s mean confidence (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) – demonstrating that subjects flexibly adjusted the mapping from an internal sense of confidence to an explicit report of confidence according to the social context.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Separate influences of motion coherence and social context on confidence reports.</title><p>(<bold>A</bold>) Mean confidence reported for each level of coherence. (<bold>B</bold>) Mean confidence reported when playing with each partner. The question mark indicates a condition where the partner’s identity was hidden. (<bold>C</bold>) Heat map visualising mean confidence in each condition of our factorial design (confidence was z-scored for each subject before averaging across subjects). Warmer colours indicate higher confidence. All data are from the fMRI session. In (<bold>A</bold>) and (<bold>B</bold>), data are represented as group mean ± SEM. Each dot is a subject.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Analysis of confidence reports in prescan session.</title><p>Rows 1–3 indicate phases 2–4 in the behavioural session (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Row 4 indicates the fMRI session and is included for comparison. (<bold>A</bold>) Mean confidence reported for each level of coherence. (<bold>B</bold>) Mean confidence reported when playing with each partner. The question mark indicates a condition in which the partner’s identity was hidden. (<bold>C</bold>) Heat map over mean confidence in each condition of our factorial design (confidence was z-scored for each subject before averaging across subjects). Warmer colours indicate higher confidence. In (<bold>A</bold>) and (<bold>B</bold>), data are represented as group mean ± SEM. Each dot is a subject.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Analysis of confidence reports separated by partner identity.</title><p>Each partner was identified by a unique colour (pink, blue, green or red) and name (Hamed, Max, Sara or Jennifer) – with the name indicating the partner’s gender. The attributes were randomly assigned to the four partner types (low, medium-low, medium-high and high). Visualisation of mean confidence reported for each partner name indicates that there was no modulation by partner identity. As the task was not optimised for studying identity effects (e.g., a subject does not experience the same name in association with different partner types or the same partner type in association with different names), we did not seek to further unpack behaviour in terms of partner identity. Data are represented as group mean ± SEM. Each dot is a subject.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig2-figsupp2-v1.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Encoding of motion coherence and social context in prefrontal cortex</title><p>We focused our fMRI analysis on three regions of interest (ROIs) that have been identified as putative neural substrates of decision confidence across a variety of studies but whose role in the generation of a context-dependent confidence report is unclear (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). First, dACC and pgACC, located in the medial wall of PFC, have been consistently linked to the formation of an internal (private) sense of confidence (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>; <xref ref-type="bibr" rid="bib17">De Martino et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Fleming et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Fleming et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Gherman and Philiastides, 2018</xref>; <xref ref-type="bibr" rid="bib39">Lebreton et al., 2015</xref>; <xref ref-type="bibr" rid="bib55">Wittmann et al., 2016</xref>). For example, a recent fMRI study found that pgACC tracked all variables necessary for the formation of an internal sense of confidence in a novel psychophysical task that isolates decision confidence from its component parts (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>). Second, FPl, a region in human prefrontal cortex with no homologue in the monkey brain (<xref ref-type="bibr" rid="bib42">Neubert et al., 2014</xref>), has consistently been found to track explicit (public) reports of confidence (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>; <xref ref-type="bibr" rid="bib23">Fleming et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Fleming et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Gherman and Philiastides, 2018</xref>; <xref ref-type="bibr" rid="bib32">Hilgenstock et al., 2014</xref>; <xref ref-type="bibr" rid="bib53">Shekhar and Rahnev, 2018</xref>; <xref ref-type="bibr" rid="bib56">Yokoyama et al., 2010</xref>). Of the three ROIs, FPl is of particular interest. We have previously hypothesised that FPl supports the mapping from private to public confidence (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>). First, FPl is not activated by tasks that vary private confidence in the absence of explicit reports (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>). Second, the microstructure (<xref ref-type="bibr" rid="bib2">Allen et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Fleming et al., 2010</xref>) and integrity (<xref ref-type="bibr" rid="bib24">Fleming et al., 2014</xref>) of FPl predicts the degree to which an individual’s confidence reports reflect their task performance – a relationship which may be explained by a role of FPl in maintaining a stable mapping from private to public confidence. However, to date, this hypothesis about the function of FPl has never been directly tested.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Encoding of motion coherence and social context in lateral frontal pole.</title><p>(<bold>A</bold>) Regions of interest (ROIs). (<bold>B</bold>) We modelled neural responses to the context screen, including both linear and quadratic terms for coherence and context as parametric modulators – with the quadratic context term indexing the need for a context-dependent private-public mapping. (<bold>C</bold>) ROI contrast estimates for coherence (K), quadratic coherence (K<sup>2</sup>), context (C) and quadratic context (C<sup>2</sup>). We tested significance (asterisk) by comparing contrast estimates across subjects to zero (p&lt;0.05, one-sample <italic>t</italic>-test). Statistical results are summarised in <xref ref-type="table" rid="table1">Table 1</xref>. Data are represented as group mean ± SEM. (<bold>D</bold>) Visualisation of whole-brain activation for quadratic context in lateral prefrontal cortex (clusters significant at p&lt;0.05, FWE-corrected for multiple comparisons, with a cluster-defining threshold of p&lt;0.001, uncorrected). See Appendix 1 for whole-brain activations in response to context screen and Appendix 2 for whole-brain activations in response to presentation of the motion stimulus. dACC: dorsal anterior cingulate cortex. pgACC: perigenual anterior cingulate cortex. FPl: lateral frontal pole.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Evaluation of quadratic terms.</title><p>The analysis of ROI responses to the context screen shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref> was based on a model (GLM1) that included linear and quadratic terms for coherence (K) and context (C) as parametric modulators. Both the linear and the quadratic terms were derived from our factorial design and theoretically motivated. Nevertheless, we ran an independent set of analyses to validate the inclusion of the quadratic terms in our model. To this end, we extracted ROI activity estimates under GLM3 – originally estimated for RSA – which modelled neural responses to the context screen separately for each condition of our factorial design (4 × 4 = 16). We applied a full model (linear and quadratic) and a reduced model (only linear) to these activity estimates within a regression framework and compared their goodness-of-fit (here defined as adjusted R<sup>2</sup> which controls for the number of model predictors). This approach revealed that the goodness-of-fit was higher for the full than the reduced model in FPl and that this difference was higher for FPl than the other ROIs. (<bold>A</bold>) Heat map visualising mean ROI activity estimates for each condition of our factorial design under GLM3 (activity estimates were z-scored for each subject before averaging across subjects). (<bold>B</bold>) Mixed-effects analysis of ROI activity estimates. Plots show (<italic>left</italic>) fixed effects under full model and (<italic>right</italic>) difference in adjusted R<sup>2</sup> between full and reduced model under a linear mixed-effects regression model (both fixed and random effects for each subject). (<bold>C</bold>) Group-average analysis of ROI activity estimates. Plots show (<italic>left</italic>) fixed effects under full model and (<italic>right</italic>) difference in adjusted R<sup>2</sup> between full and reduced model under a linear regression analysis of the mean ROI activity estimates shown in panel A.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig3-figsupp1-v1.tif"/></fig></fig-group><p>As an initial assessment of the contribution of the three ROIs to a context-dependent mapping from private to public confidence, we first estimated a general linear model (GLM1) of neural activity locked to the screen that revealed the current partner (context screen; <xref ref-type="fig" rid="fig3">Figure 3B</xref>). We selected the context screen as our time window of interest for two reasons. First, the context screen is the first point in a trial that information about the current partner is revealed – any context-related regressors would have little meaning if assigned to timepoints earlier on in a trial. Second, during the presentation of the context screen, subjects have all the information needed to internally decide on a context-dependent confidence report, but the neural response will not be confounded by the engagement of motor processes needed to select a confidence report – a motor plan can only be prepared after the randomised initial location of the confidence marker is revealed. Building on our factorial design, we included linear and quadratic terms for coherence and context as parametric modulators of the neural response to the context screen. In this model, the quadratic context term captures the intuition that larger shifts in the mapping from private to public confidence are required when playing with both low-confidence and high-confidence partners (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), whereas the quadratic coherence term controls for potential non-linear influences of an internal sense of confidence on the neural response (<xref ref-type="bibr" rid="bib41">Mazor et al., 2020</xref>).</p><p>Under this model, we would expect activity in neural areas which track private confidence to vary as a function of motion coherence, whereas neural areas which support a mapping from private to public confidence should also track information about the social context. Consistent with a role in private-public mappings, FPl tracked both motion coherence and social context (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; see statistical results in <xref ref-type="table" rid="table1">Table 1</xref>). In particular, FPl activity was higher for higher levels of coherence and for both low-confidence and high-confidence partners – contexts that involved a greater need for the employment of a context-dependent private-public mapping – as indexed by the quadratic context term. As for the ROIs hypothesised to support the formation of an internal sense of confidence, only pgACC tracked motion coherence (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). While an encoding of motion coherence on its own is insufficient to establish an area as a neural substrate for private confidence, this result is consistent with the previous finding that pgACC tracked a combination of variables that underpinned changes in private confidence (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Encoding of motion coherence and social context in lateral frontal pole.</title><p>Table shows statistical results for the analysis of ROI responses to the context screen shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref>. The model (GLM1) included separate condition regressors for trials where the context was signalled and trials where the context was hidden. The condition regressor for signalled-context trials was parametrically modulated by linear and quadratic terms for coherence (K and K<sup>2</sup>) and context (C and C<sup>2</sup>). In addition to the contrast estimates for these parametric modulators, the table also shows the contrast between signalled-context and hidden-context trials. Statistical testing was performing by comparing contrast estimates across subjects to zero using a one-sample <italic>t</italic>-test. dACC: dorsal anterior cingulate cortex. pgACC: perigenual anterior cingulate cortex. FPl: lateral frontal pole.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">ROI</th><th align="left" valign="top">Contrast</th><th valign="top">Mean</th><th valign="top">SEM</th><th valign="top"><italic>t</italic></th><th valign="top"><italic>P</italic></th></tr></thead><tbody><tr><td rowspan="5" valign="top">dACC</td><td align="left" valign="top">K</td><td valign="top">0.0646</td><td valign="top">0.1748</td><td valign="top">0.3694</td><td valign="top">0.7147</td></tr><tr><td align="left" valign="top">K<sup>2</sup></td><td valign="top">0.0293</td><td valign="top">0.1492</td><td valign="top">0.1966</td><td valign="top">0.8456</td></tr><tr><td align="left" valign="top">C</td><td valign="top">-0.1930</td><td valign="top">0.1503</td><td valign="top">-1.2840</td><td valign="top">0.2101</td></tr><tr><td align="left" valign="top">C<sup>2</sup></td><td valign="top">0.3448</td><td valign="top">0.1736</td><td valign="top">1.9857</td><td valign="top">0.0573</td></tr><tr><td align="left" valign="top">Signalled vs. hidden</td><td valign="top">0.6045</td><td valign="top">0.3685</td><td valign="top">1.6402</td><td valign="top">0.1126</td></tr><tr><td rowspan="5" valign="top">pgACC</td><td align="left" valign="top">K</td><td valign="top">0.4427</td><td valign="top">0.1616</td><td valign="top">2.7389</td><td valign="top">0.0108</td></tr><tr><td align="left" valign="top">K<sup>2</sup></td><td valign="top">0.0008</td><td valign="top">0.1839</td><td valign="top">0.0043</td><td valign="top">0.9966</td></tr><tr><td align="left" valign="top">C</td><td valign="top">0.2469</td><td valign="top">0.1994</td><td valign="top">1.2384</td><td valign="top">0.2262</td></tr><tr><td align="left" valign="top">C<sup>2</sup></td><td valign="top">-0.0878</td><td valign="top">0.2127</td><td valign="top">-0.4126</td><td valign="top">0.6831</td></tr><tr><td align="left" valign="top">Signalled vs. hidden</td><td valign="top">0.8477</td><td valign="top">0.4412</td><td valign="top">1.9213</td><td valign="top">0.0653</td></tr><tr><td rowspan="5" valign="top">FPl</td><td align="left" valign="top">K</td><td valign="top">0.6132</td><td valign="top">0.1496</td><td valign="top">4.0981</td><td valign="top">0.0003</td></tr><tr><td align="left" valign="top">K<sup>2</sup></td><td valign="top">-0.1349</td><td valign="top">0.2314</td><td valign="top">-0.5830</td><td valign="top">0.5647</td></tr><tr><td align="left" valign="top">C</td><td valign="top">-0.1172</td><td valign="top">0.2118</td><td valign="top">-0.5535</td><td valign="top">0.5845</td></tr><tr><td align="left" valign="top">C<sup>2</sup></td><td valign="top">0.9070</td><td valign="top">0.1941</td><td valign="top">4.6741</td><td valign="top">0.0001</td></tr><tr><td align="left" valign="top">Signalled vs. hidden</td><td valign="top">1.2872</td><td valign="top">0.5155</td><td valign="top">2.4967</td><td valign="top">0.0189</td></tr></tbody></table></table-wrap><p>Finally, we ran an exploratory whole-brain analysis using the same GLM as in the ROI analysis (see Appendix 1). Here we focus on context-related effects: while neural encoding of an internal sense of confidence has been investigated by previous research, our study is the first to manipulate the contextual requirements on the mapping of this internal sense onto explicit confidence reports. We observed activations in relation to the quadratic context term in neural areas that are typically implicated in studies of social cognition – including bilateral temporoparietal junction (<xref ref-type="bibr" rid="bib26">Frith and Frith, 2012</xref>; <xref ref-type="bibr" rid="bib51">Saxe, 2006</xref>) – and cognitive control – including a region anterior to our dACC ROI and in right dorsolateral PFC (<xref ref-type="bibr" rid="bib19">Duncan, 2010</xref>). Notably, and consistent with our ROI analysis, the quadratic context term also revealed significant clusters in bilateral anterior-lateral PFC that overlapped with our FPl ROI (<xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p></sec><sec id="s2-4"><title>Encoding of trial-by-trial confidence in prefrontal cortex</title><p>The preceding analysis of neural activity utilised our experimental dissociation of private and public confidence and did not directly incorporate subjects’ behaviour. In order to further probe the contribution of our ROIs to a context-dependent mapping from private to public confidence, we next used subjects’ confidence reports to unpack ROI response profiles at a trial-by-trial level. As shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, subjects’ confidence reports reflect factors relating both to the perceptual decision and the social context. A simple correlation between reported confidence and ROI activity would therefore not reveal whether the relationship was driven by private or public aspects of confidence, or both. To separate the contribution of the perceptual decision and the social context, we leveraged a previously established model-based approach (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>) to estimate the confidence that subjects would have reported on a given trial had there been no contextual modulation – an estimate that could then be compared to the confidence that subjects actually reported.</p><p>Our model-based estimate of private confidence is obtained by (1) fitting an ordinal regression model to the behavioural session in order to characterise the influence of motion coherence, choice reaction time and each social context on a subject’s confidence reports and (2) applying the fitted model to data from the fMRI session while setting the influence of each social context to zero (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Importantly, this model-based estimate provides a more direct proxy of private confidence than motion coherence alone by also taking into account the time taken to make a decision – a factor which has been shown to affect private confidence over and above the strength of the perceptual evidence (<xref ref-type="bibr" rid="bib36">Kiani et al., 2014</xref>). Indeed, our model-based approach indicated that private confidence was comparable for fast decisions made in response to low-coherence stimuli and slow decisions in response to high-coherence stimuli (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Encoding of private and reported confidence in prefrontal cortex.</title><p>(<bold>A</bold>) Model-based estimate of private confidence. We fitted an ordinal regression model to a subject’s confidence reports from the behavioural session (we used the data from the final phase as this phase had the same task design as the fMRI session). The model has a set of weights, which parameterise the influence of the perceptual decision (coherence and choice reaction time) and social context (one term for each partner), and a set of thresholds, which parameterise report biases. We used the fitted model to predict the confidence that a subject would have reported in the fMRI session had there been no contextual modulation – by applying the model to a subject’s data while setting the context weights to zero. This prediction is a probability distribution over reports (e.g., a report of ‘1’ has a 10% probability, ‘2’ has 20% probability and so on). We used the expectation under this distribution as our estimate of private confidence. (<bold>B</bold>) GLM analysis of the effects of private confidence and empirically observed confidence reports on ROI activity time courses. Vertical dashed line indicates the onset of the context screen - the context screen, which is presented for 1 s, is shown .5 s after the submission of the perceptual decision and is immediately followed by the confidence scale. Analyses were performed on trials in which the context was explicitly signalled. We tested significance (coloured square) for each time point by comparing coefficients across subjects to zero (p&lt;0.05, one-sample <italic>t</italic>-test).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Model-based estimate of private confidence varies with both motion coherence and choice reaction time.</title><p>For each subject, we used the model fitted to their data (see <xref ref-type="fig" rid="fig4">Figure 4A</xref>) to estimate private confidence (y-axis) for each level of motion coherence (lines) and for 10 choice reaction times specified using quantiles (x-axis). We then averaged the model-based estimate of private confidence across subjects for visualisation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Cross-validation accuracy for confidence model.</title><p>We used a leave-one-trial-out procedure to assess cross-validation accuracy within the data used to fit the confidence model (final phase of the behavioural session). For each subject, we iteratively fitted the model to all trials but one and computed the negative log likelihood of the report observed on the left-out trial under the confidence model (where the probability distribution over reports depends on the fitted model) and a null model (where the probability distribution over reports is uniform). We then summed the cross-validated negative log-likelihoods across all trials and computed the difference between the confidence model and the null model (y-axis) – with a positive value indicating higher cross-validation accuracy under the confidence model than the null model. In support of our approach, the difference between the confidence model and the null model was positive in all subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig4-figsupp2-v1.tif"/></fig></fig-group><p>We related both the model-based estimate of private confidence and the empirically observed confidence reports to ROI activity time courses within a regression framework (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Our analysis showed that all three ROIs encoded the model-based estimate of private confidence (pink line). The encoding profiles peaked around 2 s after the onset of the context screen, which, given the slow dynamics of the fMRI signal, is in line with the model-based estimate of private confidence being associated with the earlier perceptual decision rather than the social context. Of the three ROIs, only dACC encoded the empirically observed confidence reports (cyan line). Consistent with a sequential computation of the private and public aspects of confidence, the dACC encoding profile of reported confidence peaked 5–6 s after the onset of the context screen. Given the inclusion of the model-based estimate of private confidence in this analysis, an effect of reported confidence is likely to be driven by the social context. Taken together, these results indicate that dACC tracks the input and the output of a private-public mapping.</p><p>We next reasoned that, if FPl provides the context-dependent private-public mapping that is used to transform a private state (input) into a public report (output), then connectivity between FPl and dACC/pgACC should vary with the contextual requirements of the task. In particular, connectivity should be modulated by the relationship between ‘what I would have said’ had there been no contextual modulation and ‘what I actually said’. To test this prediction, we conducted a psychophysiological interaction analysis that quantified connectivity between FPl and dACC/pgACC as a function of the difference between the model-based estimate of private confidence and the empirically observed confidence reports. We included both the individual variables as well as their interaction to allow for differences in the impact of both understating (i.e. private &gt; reported) and overstating (i.e. private &lt; reported) confidence on connectivity. Our analysis revealed a close coupling between FPl and dACC (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). First, around the onset of the context screen, there was a transient increase in FPl-dACC connectivity associated with the model-based estimate of private confidence (pink line). Second, 6–8 s after the onset of the context screen, FPl-dACC connectivity varied with not only the model-based estimate of private confidence but also the empirically observed confidence reports (cyan line) and their interaction (green line). Visualisation of these effects showed that FPl-dACC connectivity was (1) higher for larger shifts in the mapping from private to reported confidence (off-diagonal elements are warmer than diagonal elements in <xref ref-type="fig" rid="fig5">Figure 5B</xref>) and (2) highest when confidence was understated rather than overstated (bottom-right elements are warmer than top-left elements in <xref ref-type="fig" rid="fig5">Figure 5B</xref>). Taken together, these results suggest that dACC integrates contextualised signals from FPl in order to guide trial-by-trial behaviour.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Functional connectivity between medial and lateral prefrontal context varies with contextual requirements of task.</title><p>(<bold>A</bold>) Psychophysiological interaction analysis of ROI activity time courses. Traces are coefficients from a GLM in which we predicted dACC/pgACC activity from the interaction between FPl activity and (1) the model-based estimate of private confidence (pink), (2) the empirically observed confidence reports (cyan) and (3) the interaction between private and reported confidence (green) – while controlling for the main effect of each term. Vertical dashed line indicates the onset of the context screen - the context screen, which is presented for 1 s, is shown .5 s after the submission of the perceptual decision and is immediately followed by the confidence scale. Analyses were performed on trials in which the context was explicitly signalled. We tested significance (coloured square) for each time point by comparing coefficients across subjects to zero (p&lt;0.05, one-sample <italic>t</italic>-test). (<bold>B</bold>) Visualisation of FPl-dACC connectivity. Hotter colours indicate greater FPl-dACC connectivity as a function of variation (in <italic>z</italic>-score units) in private confidence (x-axis) and reported confidence (y-axis). FPl-dACC connectivity was estimated using group-level coefficients averaged across a time window from 6 s to 8 s (box in panel A).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig5-v1.tif"/></fig></sec><sec id="s2-5"><title>Representation of task space in lateral frontal pole</title><p>Finally, we reasoned that, if FPl indeed orchestrates the context-dependent mapping from private to public confidence, then this area should also carry detailed information about the different social situations engendered by our task. In computational terms, our task comprises 16 states (social situations), with each state corresponding to a combination of coherence and context (e.g., the state on the current trial may be ‘low coherence + high-confidence partner’, whereas on the next trial a new combination of coherence and context will be encountered). By design, each of these 16 states requires subtly different behavioural responses in order to maximise reward – a relationship reflected by subjects’ confidence reports (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). If FPl supports this contextual modulation of behavioural responses, then it should represent the different states of the task as distinct.</p><p>We tested this prediction using representational similarity analysis (RSA) and a metric known as the exemplar discriminability index (EDI) (<xref ref-type="bibr" rid="bib44">Nili et al., 2020</xref>). Like other multivariate methods, RSA considers the pattern of activity across voxels within an ROI rather than the mean activity across voxels. The EDI metric asks whether a multivariate pattern is more stable across scan runs <italic>within</italic> conditions than <italic>between</italic> conditions – the intuition being that higher within-condition stability shows that an ROI represents the conditions as distinct (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). We obtained the condition-specific multivariate patterns by modelling the neural response to the context screen separately for each condition of our factorial design (GLM3). In support of a role in orchestrating private-public mappings, FPl – unlike dACC or pgACC – carried a representation of the full task space as well as the sub-spaces of coherence and context (<xref ref-type="fig" rid="fig6">Figure 6B</xref>; see statistical results in <xref ref-type="table" rid="table2">Table 2</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Task space representation in lateral frontal pole.</title><p>(<bold>A</bold>) A split-data representational dissimilarity matrix (sdRDM) is constructed by (1) computing the Mahalanobis distance between the voxel activity pattern in scan run <italic>i</italic> and the voxel activity pattern averaged across scan runs <italic>j</italic> ≠ <italic>i</italic> for every pair of conditions and (2) averaging the sdRDMs across scan runs. The exemplar discriminability index (EDI) is computed as the average dissimilarity across off-diagonal elements (blue) minus the average dissimilarity across diagonal elements (green). A positive EDI indicates that the voxel activity pattern within an ROI is more stable within conditions than between conditions and therefore that the ROI discriminates between the conditions. Simulated data are presented to aid visualisation. (<bold>B</bold>) ROI EDIs for the full task space (KxC) as well as the sub-spaces of coherence (K) and context (<bold>C</bold>). We tested significance (asterisk) by comparing EDIs across subjects to zero (p&lt;0.05, one-tailed sign-rank test). Statistical results are summarised in <xref ref-type="table" rid="table2">Table 2</xref>. Data are represented as group mean ± SEM. See Appendix 3 for whole-brain EDI searchlight analysis. dACC: dorsal anterior cingulate cortex. pgACC: perigenual anterior cingulate cortex. FPl: lateral frontal pole.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56477-fig6-v1.tif"/></fig><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Task space representation in lateral frontal pole.</title><p>Table shows statistical results for the analysis of ROI task space representations shown in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. Condition-specific multivariate patterns were obtained by modelling the neural response to the context screen separately for each condition of our factorial design (GLM3; only signalled-context trials). ROI EDIs were then computed separately for the full task space (KxC) as well as the sub-spaces of coherence (K) and context (C). Statistical testing was performed by comparing EDIs across subjects to zero using a one-tailed sign-rank test. dACC: dorsal anterior cingulate cortex. pgACC: perigenual anterior cingulate cortex. FPl: lateral frontal pole.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">ROI</th><th valign="top">Space</th><th valign="top">Mean</th><th valign="top">SEM</th><th valign="top"><italic>P</italic></th></tr></thead><tbody><tr><td rowspan="3" valign="top">dACC</td><td valign="top">K</td><td valign="top">-0.0249</td><td valign="top">0.0436</td><td valign="top">0.7772</td></tr><tr><td valign="top">C</td><td valign="top">0.0577</td><td valign="top">0.0379</td><td valign="top">0.1073</td></tr><tr><td valign="top">KxC</td><td valign="top">0.0353</td><td valign="top">0.0499</td><td valign="top">0.1397</td></tr><tr><td rowspan="3" valign="top">pgACC</td><td valign="top">K</td><td valign="top">0.1033</td><td valign="top">0.0749</td><td valign="top">0.1073</td></tr><tr><td valign="top">C</td><td valign="top">0.0646</td><td valign="top">0.0678</td><td valign="top">0.2029</td></tr><tr><td valign="top">KxC</td><td valign="top">0.0567</td><td valign="top">0.0939</td><td valign="top">0.2437</td></tr><tr><td rowspan="3" valign="top">FPl</td><td valign="top">K</td><td valign="top">0.1378</td><td valign="top">0.0854</td><td valign="top">0.0334</td></tr><tr><td valign="top">C</td><td valign="top">0.1746</td><td valign="top">0.0768</td><td valign="top">0.0258</td></tr><tr><td valign="top">KxC</td><td valign="top">0.2624</td><td valign="top">0.1062</td><td valign="top">0.0063</td></tr></tbody></table></table-wrap></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Contextual modulation of social behaviour – a core part of healthy social function – is facilitated by the ability to map private states onto public actions in a flexible manner. Here we studied how the brain supports such private-public mappings, by using a social perceptual decision task that required subjects to use learned context-dependent mappings from private to public confidence in order to maximise reward. Combining univariate and multivariate analyses of fMRI data, we found that a private-public distinction is reflected in a medial-lateral division of prefrontal cortex. In line with recent studies (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>; <xref ref-type="bibr" rid="bib25">Fleming et al., 2018</xref>), we found that dACC and pgACC tracked aspects of private confidence – as estimated by a model-based approach that controlled for the impact of the social context on subjects’ confidence reports. Further, we found that dACC tracked not only private confidence – the ‘input’ to a private-public mapping – but also the confidence that subjects actually reported under the contextual requirements of our task – the ‘output’. By contrast, FPl, which was hypothesised to govern the mapping from private to public confidence itself, tracked the need for a context-dependent private-public mapping in our task and carried a high-dimensional representation of the different social situations induced by our task. Finally, and in support of a role for FPl in orchestrating the mapping from private to public confidence, FPl-dACC connectivity varied with the contextual requirements of our task – with an increase in connectivity when a larger shift in the mapping from private to public confidence was required.</p><p>More broadly, our results are in line with a role of lateral PFC in cognitive control, defined as the ability to use task representations to guide thought and behaviour (<xref ref-type="bibr" rid="bib5">Badre, 2008</xref>; <xref ref-type="bibr" rid="bib8">Badre and Nee, 2018</xref>). In a typical study on cognitive control, the appropriate stimulus-response mapping on a particular trial depends on a contextual cue (e.g., respond ‘1’ when stimulus is blue in context A and ‘2’ in context B). The complexity of the control problem is then increased by introducing additional hierarchical rules (e.g., another cue indicating whether to respond to the colour or size of the stimulus). Both functional neuroimaging and patient studies indicate that lateral PFC is required when control problems increase in complexity – for example, by providing a model of the current task abstracted across individual episodes or acquired through instruction (<xref ref-type="bibr" rid="bib7">Badre and D’Esposito, 2007</xref>; <xref ref-type="bibr" rid="bib6">Badre et al., 2009</xref>). Our results suggest that similar neural and cognitive mechanisms are recruited to solve the private-public mapping problem inherent to social interaction. One open question is, however, whether private-public mappings involve additional computations beyond those invoked by typical cognitive control paradigms, given that the ‘stimulus’ requiring a different response involves an intervening subjective (metacognitive) state.</p><p>An alternative explanation of our results is that a division of labour between medial and lateral PFC reflects the engagement of a serial-stage process – a perceptual decision followed by a confidence report – rather than the resolution of a private-public mapping. This explanation is, however, not supported by the data. The quadratic context term – encoded by FPl – compares conditions that are matched in general task characteristics and only differ in the contextual requirements on a private-public mapping. In particular, both ‘inlying’ contexts (medium-low and medium-high confidence) and ‘outlying’ contexts (low and high confidence) require sequential preparation of a private state and a public action – the difference between these two types of context is that the latter requires a larger shift in the mapping from private to public confidence. Further, we found that FPl activity was higher on trials where the context was directly signalled than on trials where the context was hidden from subjects (<xref ref-type="table" rid="table1">Table 1</xref>). Again, these two types of trial are matched in general task characteristics and only differ in the availability of a context-dependent private-public mapping. We highlight that these ‘matched’ comparisons also rule out the alternative explanation that the FPl activations reflect a distinction between implicit and explicit processing (<xref ref-type="bibr" rid="bib52">Shea and Frith, 2016</xref>).</p><p>Our results may prompt a re-evaluation of the contribution of lateral PFC to metacognition – the ability to monitor and evaluate our ongoing thought and behaviour. Several studies have shown that lateral PFC tracks explicit reports of confidence (<xref ref-type="bibr" rid="bib17">De Martino et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Fleming et al., 2012</xref>), and that the microstructure of lateral PFC predicts the degree to which an individual’s confidence reports reflect their objective performance (<xref ref-type="bibr" rid="bib2">Allen et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Fleming et al., 2010</xref>). These results are often taken to show that lateral PFC underpins the computation of an internal sense of confidence, or the ‘read-out’ of such a variable from circuits involved in decision-making. Our study supports and refines this latter hypothesis, by showing that FPl may contextualise an internal sense of confidence for explicit report in accordance with task requirements. On this account, a relationship between anterior prefrontal structure or function and individual differences in metacognition may not reflect a contribution to insight per se but, rather, the ability to maintain a stable private-public mapping. More broadly, our results fit with a hypothesis that variation in metacognitive biases (e.g., overconfidence) reflect social rather than cognitive factors (<xref ref-type="bibr" rid="bib9">Bang et al., 2017</xref>) – a hypothesis supported by evidence that a private sense of confidence may in fact be computed in a statistically optimal manner (<xref ref-type="bibr" rid="bib1">Aitchison et al., 2015</xref>; <xref ref-type="bibr" rid="bib16">de Gardelle and Mamassian, 2014</xref>; <xref ref-type="bibr" rid="bib50">Sanders et al., 2016</xref>).</p><p>Recent years has seen an interest in how the brain encodes structured representations of the world for efficient learning and decision-making (<xref ref-type="bibr" rid="bib12">Behrens et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Niv, 2019</xref>; <xref ref-type="bibr" rid="bib49">Radulescu et al., 2019</xref>). In these scenarios, context may be particularly important for adaptive behaviour. In psychology, context is defined as the set of circumstances surrounding an event (e.g., we may hear a ringing phone in different environments such as our home or a friend’s house) (<xref ref-type="bibr" rid="bib27">Gershman, 2017</xref>; <xref ref-type="bibr" rid="bib40">Maren et al., 2013</xref>). Representing context is useful because it may carry information about what to expect or what to do (e.g., we should answer a ringing phone at home but not at a friend’s house) (<xref ref-type="bibr" rid="bib27">Gershman, 2017</xref>; <xref ref-type="bibr" rid="bib40">Maren et al., 2013</xref>). Context may also hold structural information that can be generalised across events (e.g., many behaviours that are acceptable at home are not acceptable when visiting others) (<xref ref-type="bibr" rid="bib27">Gershman, 2017</xref>). There is evidence that such structural knowledge is encoded in the medial temporal lobe and ventral PFC (<xref ref-type="bibr" rid="bib12">Behrens et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Niv, 2019</xref>). Our results complement this work by identifying representations in FPl that may support a mapping between private states and public actions that are appropriate for the current context.</p><p>Our study highlights several questions for future research. First, how are context-dependent private-public mappings learned? Model-free reinforcement learning (e.g., Q-learning) (<xref ref-type="bibr" rid="bib54">Sutton and Barto, 1998</xref>) may be able to learn the value of taking a public action given a private state and a social context but is likely to be too memory intensive (e.g., storing Q-values for every state-action pair in every context) and too slow (e.g., having to experience every state-action pair in every context) without model-based components (e.g., using a model of the world to generalise across state-action pairs or across contexts based on their similarity) (<xref ref-type="bibr" rid="bib27">Gershman, 2017</xref>). Second, what happens when there is uncertainty about the current context? In our task, the context was either explicitly signalled or fully unknown. However, in everyday life, context typically falls into neither of these categories – it is partially observable and can be inferred from available cues. At a neural level, inference on context is likely to require the involvement of additional areas such as the hippocampus which has been hypothesised to support inference and generalisation based on structural knowledge (<xref ref-type="bibr" rid="bib20">Eichenbaum, 2017a</xref>; <xref ref-type="bibr" rid="bib21">Eichenbaum, 2017b</xref>). Finally, what is the nature of the PFC task representation identified by our study? In our task, each social context is associated with a distinct private-public mapping. It is therefore hard to tell whether PFC discriminates between social contexts or a more abstract construct such as behavioural policies. This question could be addressed by using a version of our task in which seemingly distinct social contexts require subjects to adopt the same private-public mapping.</p><p>Here we focused on confidence as a canonical computation that is often the target of private-public mappings, but our approach may be adapted to other internal states in order to further elucidate the neural basis of flexible social behaviour. We achieved contextual shaping of social behaviour through a group decision rule under which subjects had to adapt the mapping from private to public confidence according to the social context in order to maximise reward. However, not all domains involving dissociations between private and public aspects of mental states – such as emotions and preferences – can be readily embedded within a decision task. Nevertheless, incentive structures may be imposed onto the mapping between a private state and a public action in a manner that mimics social life. For example, subjects may be rewarded for understating experimentally controlled feelings of pain in one context but rewarded for overstating them in another one. Similarly, subjects may be rewarded for understating their preferences for consumer items – whose valuation can be established experimentally – in one context but rewarded for overstating them in another one. An open question is the extent to which the PFC activations observed in our study will generalise across domains.</p><p>The current conceptualisation of private-public mappings may offer insight into the multiple routes to social dysfunction across a range of neuropsychiatric conditions. Broadly, contextual modulation of social behaviour requires at least three distinct sets of computation – (1) context inference, (2) action selection and (3) learning from outcomes – each presumably supported by distinct neural substrates. In future, it may be possible to distinguish different aspects of social dysfunction along each of these dimensions. For example, context-inappropriate social behaviour may arise because an individual cannot identify the current context (e.g., due to hippocampal damage), cannot inhibit prepotent but context-inappropriate actions (e.g., due to prefrontal damage), or does not make correct inferences from past experience (e.g., due to disturbance in neuromodulatory systems supporting learning). Our study provides a starting point for developing such computational-level characterisations of social dysfunction.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subject details</title><p>Twenty-eight adults (14 females; mean age ± SD, 26.36 ± 7.00) took part in the study. Sample size was determined based on common sample sizes in the field and in order to balance power and resource constraints (<xref ref-type="bibr" rid="bib48">Poldrack, 2019</xref>; <xref ref-type="bibr" rid="bib47">Poldrack et al., 2017</xref>). Each subject received a flat rate for participation (£45, £10/hour) and a performance-based bonus (see below). All subjects provided informed consent including consent to publish and sharing of anonymised data. The study was approved by the Ethics Committee of University College London (8231/001).</p></sec><sec id="s4-2"><title>Experimental details</title><sec id="s4-2-1"><title>Task and procedure</title><p>Subjects performed a social perceptual decision task in separate prescan and scan sessions (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). We made modifications to the task within and between these sessions. Here we first describe the full task, before detailing the changes made.</p><p>Each trial began with the presentation of a fixation cross at the centre of a circular aperture. After a uniformly sampled delay (prescan:. 5–1 s; scan: 1–4 s), subjects viewed a field of moving dots inside the aperture (.4 s). Once the stimulus terminated, subjects had to press one of two buttons to indicate whether the average direction of dot motion was left or right. Once a choice had been registered, the fixation cross turned grey (.5 s). Subjects were then presented with a screen informing them about their partner on the current trial (1 s). There were four partners in total. Each partner was indicated by a unique colour and name (randomised across subjects). Subjects were told that the partners were created by replaying the responses of people performing the perceptual task on another day but, in reality, all partners were simulated.</p><p>Next, subjects had to indicate their confidence in the perceptual decision, by moving a marker along a scale from 1 to 6 in steps of 1. The marker started randomly in one of the six locations on the scale and was controlled by button press. Once a response had been registered, the marker turned grey (.5 s). Subjects were then presented with the partner’s response on the corresponding trial (1 s), and the individual decision made with higher confidence was selected by the computer as the group decision, highlighted by a yellow triangle (2 s). Finally, subjects received feedback about the accuracy of the group decision (2 s), before continuing to the next trial. The feedback was indicated by replacing the yellow triangle with a green plus sign if the group decision was correct and a red cross if incorrect. Subjects were incentivised to help each group achieve as many correct decisions as possible: they were told that we would randomly select two trials for each group in each session (4 × 2 × 2 = 16 trials) and pay £1 in bonus for every correct group decision (in reality, all subjects received £10 in bonus).</p><p>We varied two features of the task in a factorial (4 × 4) manner. First, we varied the fraction of coherently moving dots (coherence) to manipulate subjects’ internal sense of confidence in a perceptual judgement (see stimulus calibration for specification of coherence levels). Second, we specified the partners (context) such that they had the same choice accuracy as subjects but differed in mean confidence (see simulation of partners).</p><p>The behavioural session involved four phases. In phase 1, we calibrated the coherence levels so as to achieve target levels of choice accuracy (see stimulus calibration). In phases 2–4, we trained subjects on the social task. In phase 2, subjects were paired with the four partners in a block-wise manner. In particular, there were 4 cycles of blocks of 10 trials per partner (e.g., A-B-C-D-A-B-C-D-A-B-C-D-A-B-C-D; 4 × 4 × 10 = 160 trials). The identity of the current partner was shown before each block of 10 trials. In phase 3, subjects were paired with the four partners in an interleaved manner, with the identity of the current partner only revealed after a perceptual decision had been made (4 × 40 = 160 trials). In phase 4, the group decision and the group outcome, was played out in the background, with the next trial starting after subjects had indicated their confidence. In addition, we introduced a condition where the social context was hidden (5 × 40 = 200 trials). In each phase, the coherence levels were counterbalanced across trials within a social context, so that each coherence level was experienced the same number of times for each partner. To help subjects keep track of the behaviour of each group, they were informed every 40 trials how often their individual decision had been selected as the group decision for each partner (15 s). The selection statistics were reset after each phase.</p><p>The scan session involved four scan runs, using the same task design as in phase 4 of the prescan session. We matched the distribution of conditions (coherence × context) across scan runs in order to facilitate multivariate analysis of the fMRI data, with four repetitions per condition (4 × 4 × 5 = 80 trials per scan run). The screen informing subjects about how often their individual decision had been selected as the group decision for each partner was shown every 20 trials. The selection statistics were reset after each scan run.</p></sec><sec id="s4-2-2"><title>Random dot kinematograms</title><p>Subjects viewed random dot kinematograms (RDKs) contained in a circular white aperture (7 degrees in diameter). Each RDK was made up of three independent sets of dots (each dot was 0.12 degrees in diameter) shown in consecutive frames. Each set of dots were shown for one frame (about 16ms) and then replotted again three frames later (about 50ms). Each time a set of dots was replotted, a subset of the dots, determined by the motion coherence, <inline-formula><mml:math id="inf1"><mml:mi>k</mml:mi></mml:math></inline-formula>, was displaced in the direction of motion at a speed of 5 degrees s<sup>−1</sup>, whereas the rest of the dots were displaced at random locations within the aperture. The motion direction was to the left or the right along the horizontal meridian. The dot density was fixed at 30 dots degrees<sup>−2</sup> s<sup>−1</sup>. To help subjects maintain fixation, a circular region (0.7 degrees in diameter) at the centre of the aperture was kept free of dots. A set of coherence levels, <inline-formula><mml:math id="inf2"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>, was identified for each subject in a separate stimulus calibration session.</p></sec><sec id="s4-2-3"><title>Stimulus calibration</title><p>Subjects performed the perceptual part of the task in two blocks. In block 1, we deployed a set of prespecified coherence levels, <inline-formula><mml:math id="inf3"><mml:mi mathvariant="bold">K</mml:mi><mml:mo>:</mml:mo> <mml:mi/><mml:mo>{</mml:mo><mml:mo>.</mml:mo><mml:mn>03</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mn>06</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mn>12</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mn>24</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mn>48</mml:mn><mml:mo>}</mml:mo></mml:math></inline-formula>. Each coherence level was used 20 times for each direction (5 x 20 x 2 = 200 trials). We then fitted a simple signal detection theory model with a single noise parameter, <inline-formula><mml:math id="inf4"><mml:mi>σ</mml:mi></mml:math></inline-formula>, governing the statistical relationship between coherence and choice (see simulation of partners for details on model). We selected the noise parameter which minimised the sum of squared errors between predicted and observed choice accuracy across coherence levels. In block 2, we used the fitted noise parameter to select a set of coherence levels associated with the target choice accuracies 60%, 70%, 80% and 90%, <inline-formula><mml:math id="inf5"><mml:mi mathvariant="bold">K</mml:mi><mml:mo>:</mml:mo> <mml:mi/><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>.</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>.</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>.</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>.</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula>. Each coherence level was then used 25 times for each direction (4 x 25 x 2 = 200 trials). We repeated the fitting procedure and selected a final set of coherence levels for the main task (the final fitted noise parameter was in turn used to simulate the partners).</p></sec><sec id="s4-2-4"><title>Simulation of partners</title><p>We used a signal detection theory model to simulate the partners’ choices and confidence reports in phases 2 and 3 of the prescan session (<xref ref-type="bibr" rid="bib9">Bang et al., 2017</xref>). In this model, an agent receives noisy sensory evidence, <inline-formula><mml:math id="inf6"><mml:mi>x</mml:mi></mml:math></inline-formula>, sampled from a Gaussian distribution, <inline-formula><mml:math id="inf7"><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:math></inline-formula>), and makes a choice by comparing the sensory evidence to zero, choosing left if <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and right if <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The mean of the Gaussian distribution is given by coherence, <inline-formula><mml:math id="inf10"><mml:mi>k</mml:mi></mml:math></inline-formula>, and direction, <inline-formula><mml:math id="inf11"><mml:mi>d</mml:mi></mml:math></inline-formula>, with <inline-formula><mml:math id="inf12"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> indicating left and <inline-formula><mml:math id="inf13"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> indicating right. The standard deviation, <inline-formula><mml:math id="inf14"><mml:mi>σ</mml:mi></mml:math></inline-formula>, is the level of sensory noise – specified by fitting the model to a subject’s choices in the stimulus calibration session (see stimulus calibration). The agent computes an internal estimate of decision confidence, <inline-formula><mml:math id="inf15"><mml:mi>z</mml:mi></mml:math></inline-formula>, using the absolute value of the evidence strength, <inline-formula><mml:math id="inf16"><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo></mml:math></inline-formula> – a quantity which is monotonically related to the probability that the perceptual choice is correct given the sensory evidence and the level of sensory noise. Finally, the agent maps this internal estimate onto a confidence report, <inline-formula><mml:math id="inf17"><mml:mi>r</mml:mi></mml:math></inline-formula>, by applying a set of thresholds, <inline-formula><mml:math id="inf18"><mml:mi>r</mml:mi> <mml:mi/><mml:mo>=</mml:mo> <mml:mi/><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. Precise control over the number of times that the agent selects a particular confidence report is achieved by first simulating a vector of <inline-formula><mml:math id="inf19"><mml:mi>x</mml:mi></mml:math></inline-formula>’s – using the known sequence of stimuli in the task – and then setting the thresholds in <inline-formula><mml:math id="inf20"><mml:mi>z</mml:mi></mml:math></inline-formula>-space so as to achieve a desired confidence distribution. In this way, we created partners with low, medium-low, medium-high and high mean confidence (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref> for confidence distributions). We simulated phases 2 and 3 separately, so that a particular partner had the same confidence distribution in each phase. In addition, we simulated <inline-formula><mml:math id="inf21"><mml:mi>x</mml:mi></mml:math></inline-formula>’s for each partner in each session under the constraint that their choice accuracy was within 1% of the target choice accuracy for each coherence level (see stimulus calibration). In phase 4, and in the scan session, we did not simulate responses. Instead, to calculate how often a subject’s individual decision had been selected as the group decision, we first computed for each trial the probability of a subject’s decision being selected given their confidence report and the partner’s confidence distribution and then averaged these probabilities across trials.</p></sec></sec><sec id="s4-3"><title>Behavioural analysis</title><sec id="s4-3-1"><title>Regression analysis</title><p>We used ordinal regression (probit) to analyse subjects’ trial-by-trial confidence reports. The model included (contrast-coded) coherence and context as predictors of interest and (log-transformed) choice reaction time, choice, motion direction and marker starting position as predictors of no interest. We z-scored all variables before analysis. We excluded trials in which the partner’s identity was hidden. We performed a separate regression for each subject. We tested the group-level significance of a predictor by comparing the coefficients across subjects to 0 (p&lt;0.05, one-sample <italic>t</italic>-test).</p></sec><sec id="s4-3-2"><title>Confidence model</title><p>We used a previously established approach (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>) to construct a model of private confidence for fMRI analysis. We fitted an ordinal regression model to a subject’s confidence reports in the final phase of the behavioural session using six predictors: (1) z-scored, contrast-coded coherence, (2) z-scored, log-transformed choice reaction time and (3-6) a dummy variable for each explicitly signalled context. We then applied the fitted model to a subject’s data in the fMRI session, while setting the fitted context coefficients to zero. This approach yields an out-of-sample prediction about the level of confidence that a subject would have reported on a given trial in the absence of contextual (i.e. partner-specific) modulation. The prediction takes the form of a probability distribution over possible responses (e.g., a report of ‘1’ has a 10% probability, ‘2’ has a 20% probability and so on). We used the expectation under this distribution as our estimate of private confidence.</p></sec></sec><sec id="s4-4"><title>fMRI analysis</title><sec id="s4-4-1"><title>Acquisition</title><p>MRI data were acquired on a 3T Siemens Prisma scanner with a 64-channel head coil. T1-weighted structural images were acquired using a 3D MPRAGE sequence: 1 × 1 × 1 mm resolution voxels; 176 sagittal slices, 256 × 224 matrix; TR = 2530 ms; TE = 3.34 ms; TI = 1100 ms. BOLD T2<sup>*</sup>-weighted functional images were acquired using a gradient-echo EPI pulse sequence: 3 × 3 × 3 mm resolution voxels; 48 transverse slices, 64 × 74 matrix; TR = 3.36; TE = 30 ms; slice tilt = 0 degrees, slice thickness = 2 mm; inter-slice gap = 1 mm; ascending slice order. Field maps were acquired using a double-echo FLASH (gradient echo) sequence: TE1 = 10 ms; TE2 = 12.46 ms; 64 slices were acquired with 2 mm slice thickness and a 1 mm gap; in-plane field of view is 192 × 192 mm<sup>2</sup> with 3 × 3 mm<sup>2</sup> resolution.</p></sec><sec id="s4-4-2"><title>Preprocessing</title><p>MRI data were pre-processed using SPM12. The first 4 volumes of each functional run were discarded to allow for T1 equilibration. Functional images were slice-time corrected, realigned and unwarped using the field maps (<xref ref-type="bibr" rid="bib3">Andersson et al., 2001</xref>). Structural T1-weighted images were co-registered to the mean functional image of each subject using the iterative mutual-information algorithm. Each subject’s structural image was segmented into grey matter, white matter and cerebral spinal fluid using a nonlinear deformation field to map it onto a template tissue probability map (<xref ref-type="bibr" rid="bib4">Ashburner and Friston, 2005</xref>). These deformations were applied to structural and functional images to create new images spatially normalised to the Montreal Neurological Institute (MNI) space and interpolated to 2 × 2 × 2 mm voxels. Normalized images were spatially smoothed using a Gaussian kernel with full-width half-maximum of 8 mm. The motion correction parameters estimated from the realignment procedure and their first temporal derivatives – 12 ‘motion’ regressors in total – were included as confounds in the first-level analysis for each subject.</p></sec><sec id="s4-4-3"><title>Physiological monitoring</title><p>Peripheral measurements of a subject’s pulse and breathing were made together with scanner slice synchronisation pulses using a Spike2 data acquisition system (Cambridge Electronic Design Limited, Cambridge UK). The cardiac pulse signal was measured using an MRI compatible pulse oximeter (Model 8600 F0, Nonin Medical, Inc Plymouth, MN) attached to a subject’s finger. The respiratory signal, thoracic movement, was monitored using a pneumatic belt positioned around the abdomen close to the diaphragm. A physiological noise model was constructed to account for artifacts related to cardiac and respiratory phase and changes in respiratory volume using an in-house MATLAB toolbox (<xref ref-type="bibr" rid="bib33">Hutton et al., 2011</xref>). Models for cardiac and respiratory phase and their aliased harmonics were based on RETROICOR (<xref ref-type="bibr" rid="bib29">Glover et al., 2000</xref>) and a similar, earlier method (<xref ref-type="bibr" rid="bib35">Josephs et al., 1997</xref>). Basis sets of sine and cosine Fourier series components extending to the 3rd harmonic were used to model physiological fluctuations. Additional terms were included to model changes in respiratory volume (<xref ref-type="bibr" rid="bib13">Birn et al., 2006</xref>; <xref ref-type="bibr" rid="bib14">Birn et al., 2008</xref>) and heart rate (<xref ref-type="bibr" rid="bib15">Chang and Glover, 2009</xref>). This procedure yielded a total of 14 ‘biophysical’ regressors that were sampled at a reference slice in each image volume. The regressors were included as confounds in the first-level analysis for each subject.</p></sec><sec id="s4-4-4"><title>Regions of interest</title><p>We focused on three a priori ROIs highlighted by previous research on decision confidence. The dACC mask was an 8 mm sphere around the peak coordinates (MNI coordinates [<italic>x y z</italic>] = [0 17 46]) identified by <xref ref-type="bibr" rid="bib23">Fleming et al., 2012</xref>. The pgACC mask was defined using the coherence x distance second-level <italic>t</italic>-map from <xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>. The FPl mask was defined using the right-hemisphere atlas developed by <xref ref-type="bibr" rid="bib42">Neubert et al., 2014</xref> and mirrored to the left hemisphere to create a bilateral mask.</p></sec><sec id="s4-4-5"><title>Univariate analysis</title><p>Univariate analysis of fMRI data was performed using SPM12. Our main analysis was based on an event-related GLM (GLM1) of the neural response to the context screen. This model included three condition regressors. First, the context screen when the partner was signalled (signalled, 1 s boxcar). Second, the context screen when the partner was hidden (hidden, 1 s boxcar). Third, the update screen informing subjects how often their individual decision had been selected as the group decision for each partner (update, 15 s boxcar). We parametrically modulated the signalled condition regressor using our task factors: (1) K, contrast-coded coherence, {−1.5,−0.5,.5,1.5}; (2) K<sup>2</sup>, contrast-coded coherence squared; (3) C, contrast-coded context, {−1.5,−0.5,.5,1.5}; and (4) C<sup>2</sup>, contrast-coded context squared. For comparability with earlier studies on decision confidence, we also estimated an event-related GLM (GLM2) of the neural response to the presentation of the motion stimulus. This modelled included one condition regressor – a boxcar lasting the duration of the stimulus (.4 s) – parametrically modulated by linear and quadratic coherence terms as defined above.</p><p>Parametric modulators were not orthogonalized. We excluded trials in which subjects’ choice reaction times were 2.5 SD below or above their grand mean reaction time within a scan run (0–2 trials per subject per run). In addition to the condition regressors, we added motion and biophysical parameters as additional ‘nuisance’ regressors. Regressors were convolved with a canonical hemodynamic response function. Regressors were modelled separately for each scan run, and constants were included to account for differences in mean activation between runs and scanner drifts. A high-pass filter (128 s cutoff) was applied to remove low-frequency drifts. Whole-brain statistical testing was performed by applying one-sample <italic>t</italic>-tests against 0 to the first-level contrast images. We report clusters significant at p&lt;0.05, FWE-corrected for multiple comparisons, with a cluster-defining threshold of p&lt;0.001, uncorrected. For the ROI analysis, we extracted mean contrast estimates within the ROI masks from first-level contrast images and assessed group-level significance by applying one-sample <italic>t</italic>-tests against zero (p&lt;0.05) to the extracted contrast estimates.</p></sec><sec id="s4-4-6"><title>Activity time course analysis</title><p>We used activity time courses to study the neural encoding of trial-by-trial confidence and assess functional coupling between ROIs. We transformed each ROI mask from MNI to native space and extracted preprocessed BOLD time courses as the average of voxels within each mask. For each scan run, we regressed out variation due to head motion, applied a high-pass filter (128 s cut-off) to remove low-frequency drifts, and oversampled the BOLD time course by a factor of ~23 (time resolution of. 144 s). For each trial, we extracted activity estimates in a 12 s window time-locked to our event of interest (i.e. from 2 s prior to the onset of the context screen to 10 s after its onset). We excluded trials in which subjects’ choice reaction times were 2.5 SD below or above their grand mean reaction time across trials (1–13 trials per subject). We applied a linear regression to each time point and then, by concatenating beta-weights across time points, generated a time course for each predictor of the regression model. We performed a separate analysis for each subject. We tested the group-level significance of a time point by comparing the beta-weights across subjects to 0 (p&lt;0.05, one-sample <italic>t</italic>-test).</p></sec><sec id="s4-4-7"><title>Multivariate analysis</title><p>Representational similarity analysis (RSA) of fMRI data was performed using SPM12 and the RSA toolbox (<xref ref-type="bibr" rid="bib43">Nili et al., 2014</xref>). To estimate voxel activity patterns, we constructed an event-related GLM with a condition regressor locked to the context screen (1 s boxcar) for each coherence × (signalled) context condition (4 × 4 = 16 regressors per scan run). The GLM for ROI analysis (GLM3) was based on unsmoothed data, whereas the GLM for searchlight analysis (GLM4) was based on smoothed data. As in GLM1, we included regressors for hidden context and update screen and motion and biophysiological regressors as additional ‘nuisance’ variables. Regressors were convolved with a canonical hemodynamic response function. Regressors were modelled separately for each scan run, and constants were included to account for differences in mean activation between runs and scanner drifts. A high-pass filter (128 s cutoff) was applied to remove low-frequency drifts.</p><p>The exemplar discriminability index (EDI) is a test of whether an ROI carries information about a set of conditions (<xref ref-type="bibr" rid="bib44">Nili et al., 2020</xref>) – in our case the 16 coherence × context conditions that make up the full task space. The hypothesis is that, if a neural area represents the conditions as distinct, then its voxel activity pattern should be more stable across scan runs <italic>within</italic> conditions than <italic>between</italic> conditions. This hypothesis is tested by first constructing a split-data representational dissimilarity matrix (sdRDM) for each scan run that contains the Mahalanobis distance (i.e. multivariate noise normalisation and Euclidean distance) between the voxel activity pattern in scan run <italic>i</italic> and the voxel activity pattern averaged across scan runs <italic>j</italic> ≠ <italic>i</italic> for all condition pairs and then averaging the run-specific sdRDMs across scan runs. An EDI metric is then be computed as the average dissimilarity across the off-diagonal elements minus the average dissimilarity across the diagonal elements (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). A positive EDI shows that the voxel activity pattern in an area is more stable within conditions than between conditions and therefore that the area carries information about the conditions.</p><p>ROI analysis was performed by computing the EDI metric from voxel activity estimates under the first-level model within our ROI masks. As the EDI should theoretically not be below zero, we assessed group-level significance for ROI analysis using a one-tailed sign-rank test (p&lt;0.05). Searchlight analysis – as executed by the RSA toolbox – was performed by computing the EDI metric for a spherical cluster of voxels centred at each voxel within each subject’s first-level map. Consistent with previous RSA studies (<xref ref-type="bibr" rid="bib43">Nili et al., 2014</xref>), the diameter of the sphere was 15 mm (around 100 voxels). The sphere was adapted in shape if it was near the edges of a whole-brain group-level mask – the mask was defined as the intersection of the whole-brain masks obtained for each subject during estimation of the first-level model. Whole-brain statistical testing was performed by applying one-sample <italic>t</italic>-tests against 0 to the first-level EDI maps. We report clusters significant at p&lt;0.05, FWE-corrected for multiple comparisons, with a cluster-defining threshold of p&lt;0.001, uncorrected. We note that the searchlight analysis is not directly comparable to the ROI analysis but, rather, complements it. First, the FPl ROI analysis is more sensitive to subtle pattern information as the FPl ROI contains around 10 times more voxels than the searchlight sphere. Second, our FPl ROI is bilateral and thus contains a substantial proportion of non-contiguous voxels. By contrast, the searchlight sphere is restricted to contiguous voxels within a relatively restricted region and can therefore only identify localised pattern information. Finally, our FPl ROI is anatomically defined and thus has a shape which cannot be captured by a searchlight sphere.</p><p>We note that the number of trials per condition per scan run (i.e. 4) is compatible with previous studies employing RSA (<xref ref-type="bibr" rid="bib38">Kriegeskorte et al., 2008</xref>). Constraints on per-subject scanning time means that there is an inherent trade-off between the ability to estimate a neural pattern within a condition (the estimate improves with repetitions of a condition) and the ability to estimate neural pattern dissimilarities between conditions (the estimate improves with the number of conditions). We adopted a condition-rich design as our goal was to test whether an ROI treats the different conditions (i.e. task states) as distinct and not to characterise the ROI pattern for any particular condition. We note that our analysis of the sub-spaces of coherence and context were based on 16 trials per condition per run and not four trials per condition per run as in the analysis of the full task space.</p></sec></sec><sec id="s4-5"><title>Data and code availablity</title><p>Data and code for reproducing figures as well as associated analyses are available on GitHub: <xref ref-type="bibr" rid="bib10">Bang, 2020</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/danbang/article-private-public">https://github.com/danbang/article-private-public</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/article-private-public">https://github.com/elifesciences-publications/article-private-public</ext-link>). Whole-brain group-level statistical maps are available on NeuroVault: <ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/6782/">https://neurovault.org/collections/6782/</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The Wellcome Centre for Human Neuroimaging is supported by core funding from Wellcome (203147/Z/16/Z). DB is supported by a Sir Henry Wellcome Postdoctoral Fellowship funded by Wellcome (213630/Z/18/Z). SMF is supported by a Sir Henry Dale Fellowship jointly funded by Wellcome and the Royal Society (206648/Z/17/Z).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All subjects provided informed consent including consent to publish and sharing of anonymised data. The study was approved by the Ethics Committee of University College London (8231/001).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-56477-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data and code for reproducing figures as well as associated analyses are available on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/danbang/article-private-public">https://github.com/danbang/article-private-public</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/article-private-public">https://github.com/elifesciences-publications/article-private-public</ext-link>). Whole-brain group-level statistical maps are available on NeuroVault: <ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/6782/">https://neurovault.org/collections/6782/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Ershadmanesh</surname><given-names>S</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Fleming</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Private-public mappings in human prefrontal cortex</data-title><source>NeuroVault</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="https://neurovault.org/collections/6782/">6782</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitchison</surname> <given-names>L</given-names></name><name><surname>Bang</surname> <given-names>D</given-names></name><name><surname>Bahrami</surname> <given-names>B</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Doubly bayesian analysis of confidence in perceptual Decision-Making</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004519</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004519</pub-id><pub-id pub-id-type="pmid">26517475</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname> <given-names>M</given-names></name><name><surname>Glen</surname> <given-names>JC</given-names></name><name><surname>Müllensiefen</surname> <given-names>D</given-names></name><name><surname>Schwarzkopf</surname> <given-names>DS</given-names></name><name><surname>Fardo</surname> <given-names>F</given-names></name><name><surname>Frank</surname> <given-names>D</given-names></name><name><surname>Callaghan</surname> <given-names>MF</given-names></name><name><surname>Rees</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Metacognitive ability correlates with hippocampal and prefrontal microstructure</article-title><source>NeuroImage</source><volume>149</volume><fpage>415</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.02.008</pub-id><pub-id pub-id-type="pmid">28179164</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname> <given-names>JL</given-names></name><name><surname>Hutton</surname> <given-names>C</given-names></name><name><surname>Ashburner</surname> <given-names>J</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Modeling geometric deformations in EPI time series</article-title><source>NeuroImage</source><volume>13</volume><fpage>903</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0746</pub-id><pub-id pub-id-type="pmid">11304086</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname> <given-names>J</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Unified segmentation</article-title><source>NeuroImage</source><volume>26</volume><fpage>839</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.02.018</pub-id><pub-id pub-id-type="pmid">15955494</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cognitive control, hierarchy, and the rostro-caudal organization of the frontal lobes</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>193</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.02.004</pub-id><pub-id pub-id-type="pmid">18403252</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname> <given-names>D</given-names></name><name><surname>Hoffman</surname> <given-names>J</given-names></name><name><surname>Cooney</surname> <given-names>JW</given-names></name><name><surname>D'Esposito</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Hierarchical cognitive control deficits following damage to the human frontal lobe</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>515</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/nn.2277</pub-id><pub-id pub-id-type="pmid">19252496</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname> <given-names>D</given-names></name><name><surname>D’Esposito</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>FMRI evidence for a hierarchical organization of the prefrontal cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>19</volume><fpage>2082</fpage><lpage>2099</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.12.2082</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname> <given-names>D</given-names></name><name><surname>Nee</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Frontal cortex and the hierarchical control of behavior</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>170</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.11.005</pub-id><pub-id pub-id-type="pmid">29229206</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bang</surname> <given-names>D</given-names></name><name><surname>Aitchison</surname> <given-names>L</given-names></name><name><surname>Moran</surname> <given-names>R</given-names></name><name><surname>Herce Castanon</surname> <given-names>S</given-names></name><name><surname>Rafiee</surname> <given-names>B</given-names></name><name><surname>Mahmoodi</surname> <given-names>A</given-names></name><name><surname>Lau</surname> <given-names>JYF</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Bahrami</surname> <given-names>B</given-names></name><name><surname>Summerfield</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Confidence matching in group decision-making</article-title><source>Nature Human Behaviour</source><volume>1</volume><elocation-id>0117</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-017-0117</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bang</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>article-private-public</data-title><source>GitHub</source><version designator="54">54</version><ext-link ext-link-type="uri" xlink:href="https://github.com/danbang/article-private-public">https://github.com/danbang/article-private-public</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bang</surname> <given-names>D</given-names></name><name><surname>Fleming</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinct encoding of decision confidence in human medial prefrontal cortex</article-title><source>PNAS</source><volume>115</volume><fpage>6082</fpage><lpage>6087</lpage><pub-id pub-id-type="doi">10.1073/pnas.1800795115</pub-id><pub-id pub-id-type="pmid">29784814</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Behrens</surname> <given-names>TEJ</given-names></name><name><surname>Muller</surname> <given-names>TH</given-names></name><name><surname>Whittington</surname> <given-names>JCR</given-names></name><name><surname>Mark</surname> <given-names>S</given-names></name><name><surname>Baram</surname> <given-names>AB</given-names></name><name><surname>Stachenfeld</surname> <given-names>KL</given-names></name><name><surname>Kurth-Nelson</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What is a cognitive map? organising knowledge for flexible behaviour</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/365593</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Birn</surname> <given-names>RM</given-names></name><name><surname>Diamond</surname> <given-names>JB</given-names></name><name><surname>Smith</surname> <given-names>MA</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Separating respiratory-variation-related fluctuations from neuronal-activity-related fluctuations in fMRI</article-title><source>NeuroImage</source><volume>31</volume><fpage>1536</fpage><lpage>1548</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.02.048</pub-id><pub-id pub-id-type="pmid">16632379</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Birn</surname> <given-names>RM</given-names></name><name><surname>Smith</surname> <given-names>MA</given-names></name><name><surname>Jones</surname> <given-names>TB</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The respiration response function: the temporal dynamics of fMRI signal fluctuations related to changes in respiration</article-title><source>NeuroImage</source><volume>40</volume><fpage>644</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.11.059</pub-id><pub-id pub-id-type="pmid">18234517</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>C</given-names></name><name><surname>Glover</surname> <given-names>GH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Effects of model-based physiological noise correction on default mode network anti-correlations and correlations</article-title><source>NeuroImage</source><volume>47</volume><fpage>1448</fpage><lpage>1459</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.05.012</pub-id><pub-id pub-id-type="pmid">19446646</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Gardelle</surname> <given-names>V</given-names></name><name><surname>Mamassian</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Does confidence use a common currency across two visual tasks?</article-title><source>Psychological Science</source><volume>25</volume><fpage>1286</fpage><lpage>1288</lpage><pub-id pub-id-type="doi">10.1177/0956797614528956</pub-id><pub-id pub-id-type="pmid">24699845</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Martino</surname> <given-names>B</given-names></name><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>Garrett</surname> <given-names>N</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Confidence in value-based choice</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>105</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1038/nn.3279</pub-id><pub-id pub-id-type="pmid">23222911</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dennett</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>From Bacteria to Bach and Back: The Evolution of Minds</source><publisher-loc>New York</publisher-loc><publisher-name>W. W. Norton</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>172</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.004</pub-id><pub-id pub-id-type="pmid">20171926</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>Memory: organization and control</article-title><source>Annual Review of Psychology</source><volume>68</volume><fpage>19</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010416-044131</pub-id><pub-id pub-id-type="pmid">27687117</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>Prefrontal-hippocampal interactions in episodic memory</article-title><source>Nature Reviews Neuroscience</source><volume>18</volume><fpage>547</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.74</pub-id><pub-id pub-id-type="pmid">28655882</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>Weil</surname> <given-names>RS</given-names></name><name><surname>Nagy</surname> <given-names>Z</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Rees</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Relating introspective accuracy to individual differences in brain structure</article-title><source>Science</source><volume>329</volume><fpage>1541</fpage><lpage>1543</lpage><pub-id pub-id-type="doi">10.1126/science.1191883</pub-id><pub-id pub-id-type="pmid">20847276</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>Huijgen</surname> <given-names>J</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Prefrontal contributions to metacognition in perceptual decision making</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>6117</fpage><lpage>6125</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6489-11.2012</pub-id><pub-id pub-id-type="pmid">22553018</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>Ryu</surname> <given-names>J</given-names></name><name><surname>Golfinos</surname> <given-names>JG</given-names></name><name><surname>Blackmon</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Domain-specific impairment in metacognitive accuracy following anterior prefrontal lesions</article-title><source>Brain</source><volume>137</volume><fpage>2811</fpage><lpage>2822</lpage><pub-id pub-id-type="doi">10.1093/brain/awu221</pub-id><pub-id pub-id-type="pmid">25100039</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>van der Putten</surname> <given-names>EJ</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural mediators of changes of mind about perceptual decisions</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>617</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0104-6</pub-id><pub-id pub-id-type="pmid">29531361</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frith</surname> <given-names>CD</given-names></name><name><surname>Frith</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mechanisms of social cognition</article-title><source>Annual Review of Psychology</source><volume>63</volume><fpage>287</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-120710-100449</pub-id><pub-id pub-id-type="pmid">21838544</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Context-dependent learning and causal structure</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>24</volume><fpage>557</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.3758/s13423-016-1110-x</pub-id><pub-id pub-id-type="pmid">27418259</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gherman</surname> <given-names>S</given-names></name><name><surname>Philiastides</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Human VMPFC encodes early signatures of confidence in perceptual decisions</article-title><source>eLife</source><volume>7</volume><elocation-id>e38293</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38293</pub-id><pub-id pub-id-type="pmid">30247123</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glover</surname> <given-names>GH</given-names></name><name><surname>Li</surname> <given-names>TQ</given-names></name><name><surname>Ress</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Image-based method for retrospective correction of physiological motion effects in fMRI: retroicor</article-title><source>Magnetic Resonance in Medicine</source><volume>44</volume><fpage>162</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1002/1522-2594(200007)44:1&lt;162::AID-MRM23&gt;3.0.CO;2-E</pub-id><pub-id pub-id-type="pmid">10893535</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Happé</surname> <given-names>FG</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>An advanced test of theory of mind: understanding of story characters' thoughts and feelings by able autistic, mentally handicapped, and normal children and adults</article-title><source>Journal of Autism and Developmental Disorders</source><volume>24</volume><fpage>129</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1007/BF02172093</pub-id><pub-id pub-id-type="pmid">8040158</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertz</surname> <given-names>U</given-names></name><name><surname>Palminteri</surname> <given-names>S</given-names></name><name><surname>Brunetti</surname> <given-names>S</given-names></name><name><surname>Olesen</surname> <given-names>C</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name><name><surname>Bahrami</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural computations underpinning the strategic management of influence in advice giving</article-title><source>Nature Communications</source><volume>8</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41467-017-02314-5</pub-id><pub-id pub-id-type="pmid">29259152</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hilgenstock</surname> <given-names>R</given-names></name><name><surname>Weiss</surname> <given-names>T</given-names></name><name><surname>Witte</surname> <given-names>OW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>You'd better think twice: post-decision perceptual confidence</article-title><source>NeuroImage</source><volume>99</volume><fpage>323</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.05.049</pub-id><pub-id pub-id-type="pmid">24862076</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutton</surname> <given-names>C</given-names></name><name><surname>Josephs</surname> <given-names>O</given-names></name><name><surname>Stadler</surname> <given-names>J</given-names></name><name><surname>Featherstone</surname> <given-names>E</given-names></name><name><surname>Reid</surname> <given-names>A</given-names></name><name><surname>Speck</surname> <given-names>O</given-names></name><name><surname>Bernarding</surname> <given-names>J</given-names></name><name><surname>Weiskopf</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The impact of physiological noise correction on fMRI at 7 T</article-title><source>NeuroImage</source><volume>57</volume><fpage>101</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.04.018</pub-id><pub-id pub-id-type="pmid">21515386</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ibañez</surname> <given-names>A</given-names></name><name><surname>Manes</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Contextual social cognition and the behavioral variant of frontotemporal dementia</article-title><source>Neurology</source><volume>78</volume><fpage>1354</fpage><lpage>1362</lpage><pub-id pub-id-type="doi">10.1212/WNL.0b013e3182518375</pub-id><pub-id pub-id-type="pmid">22529204</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Josephs</surname> <given-names>O</given-names></name><name><surname>Howseman</surname> <given-names>AM</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Physiological noise modelling for multi-slice EPI fMRI using SPM</article-title><conf-name>Proceedings of the 5th Annual Meeting of ISMRM</conf-name></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Corthell</surname> <given-names>L</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Choice certainty is informed by both evidence and decision time</article-title><source>Neuron</source><volume>84</volume><fpage>1329</fpage><lpage>1342</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.015</pub-id><pub-id pub-id-type="pmid">25521381</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King-Casas</surname> <given-names>B</given-names></name><name><surname>Sharp</surname> <given-names>C</given-names></name><name><surname>Lomax-Bream</surname> <given-names>L</given-names></name><name><surname>Lohrenz</surname> <given-names>T</given-names></name><name><surname>Fonagy</surname> <given-names>P</given-names></name><name><surname>Montague</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The rupture and repair of cooperation in borderline personality disorder</article-title><source>Science</source><volume>321</volume><fpage>806</fpage><lpage>810</lpage><pub-id pub-id-type="doi">10.1126/science.1156902</pub-id><pub-id pub-id-type="pmid">18687957</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Ruff</surname> <given-names>DA</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Bodurka</surname> <given-names>J</given-names></name><name><surname>Esteky</surname> <given-names>H</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebreton</surname> <given-names>M</given-names></name><name><surname>Abitbol</surname> <given-names>R</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Pessiglione</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automatic integration of confidence in the brain valuation signal</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1159</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1038/nn.4064</pub-id><pub-id pub-id-type="pmid">26192748</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maren</surname> <given-names>S</given-names></name><name><surname>Phan</surname> <given-names>KL</given-names></name><name><surname>Liberzon</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The contextual brain: implications for fear conditioning, extinction and psychopathology</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>417</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1038/nrn3492</pub-id><pub-id pub-id-type="pmid">23635870</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazor</surname> <given-names>M</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Fleming</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Distinct neural contributions to metacognition for detecting, but not discriminating visual stimuli</article-title><source>eLife</source><volume>9</volume><elocation-id>e53900</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.53900</pub-id><pub-id pub-id-type="pmid">32310086</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neubert</surname> <given-names>FX</given-names></name><name><surname>Mars</surname> <given-names>RB</given-names></name><name><surname>Thomas</surname> <given-names>AG</given-names></name><name><surname>Sallet</surname> <given-names>J</given-names></name><name><surname>Rushworth</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Comparison of human ventral frontal cortex Areas for cognitive control and language with Areas in monkey frontal cortex</article-title><source>Neuron</source><volume>81</volume><fpage>700</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.012</pub-id><pub-id pub-id-type="pmid">24485097</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Wingfield</surname> <given-names>C</given-names></name><name><surname>Walther</surname> <given-names>A</given-names></name><name><surname>Su</surname> <given-names>L</given-names></name><name><surname>Marslen-Wilson</surname> <given-names>W</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A toolbox for representational similarity analysis</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Walther</surname> <given-names>A</given-names></name><name><surname>Alink</surname> <given-names>A</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Inferring exemplar discriminability in brain representations</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0232551</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0232551</pub-id><pub-id pub-id-type="pmid">32520962</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning task-state representations</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1544</fpage><lpage>1553</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0470-8</pub-id><pub-id pub-id-type="pmid">31551597</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penn</surname> <given-names>DL</given-names></name><name><surname>Ritchie</surname> <given-names>M</given-names></name><name><surname>Francis</surname> <given-names>J</given-names></name><name><surname>Combs</surname> <given-names>D</given-names></name><name><surname>Martin</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Social perception in schizophrenia: the role of context</article-title><source>Psychiatry Research</source><volume>109</volume><fpage>149</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/S0165-1781(02)00004-5</pub-id><pub-id pub-id-type="pmid">11927140</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname> <given-names>RA</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name><name><surname>Durnez</surname> <given-names>J</given-names></name><name><surname>Gorgolewski</surname> <given-names>KJ</given-names></name><name><surname>Matthews</surname> <given-names>PM</given-names></name><name><surname>Munafò</surname> <given-names>MR</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name><name><surname>Poline</surname> <given-names>JB</given-names></name><name><surname>Vul</surname> <given-names>E</given-names></name><name><surname>Yarkoni</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Scanning the horizon: towards transparent and reproducible neuroimaging research</article-title><source>Nature Reviews Neuroscience</source><volume>18</volume><fpage>115</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.167</pub-id><pub-id pub-id-type="pmid">28053326</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The costs of reproducibility</article-title><source>Neuron</source><volume>101</volume><fpage>11</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.11.030</pub-id><pub-id pub-id-type="pmid">30605654</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radulescu</surname> <given-names>A</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Ballard</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Holistic reinforcement learning: the role of structure and attention</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>278</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.01.010</pub-id><pub-id pub-id-type="pmid">30824227</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanders</surname> <given-names>JI</given-names></name><name><surname>Hangya</surname> <given-names>B</given-names></name><name><surname>Kepecs</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Signatures of a statistical computation in the human sense of confidence</article-title><source>Neuron</source><volume>90</volume><fpage>499</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.025</pub-id><pub-id pub-id-type="pmid">27151640</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Uniquely human social cognition</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>235</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.03.001</pub-id><pub-id pub-id-type="pmid">16546372</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shea</surname> <given-names>N</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dual-process theories and consciousness: the case for 'Type Zero' cognition</article-title><source>Neuroscience of Consciousness</source><volume>2016</volume><elocation-id>niw005</elocation-id><pub-id pub-id-type="doi">10.1093/nc/niw005</pub-id><pub-id pub-id-type="pmid">30109126</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shekhar</surname> <given-names>M</given-names></name><name><surname>Rahnev</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinguishing the roles of dorsolateral and anterior PFC in visual metacognition</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>5078</fpage><lpage>5087</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3484-17.2018</pub-id><pub-id pub-id-type="pmid">29720553</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Reinforcement Learning: An Introduction</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wittmann</surname> <given-names>MK</given-names></name><name><surname>Kolling</surname> <given-names>N</given-names></name><name><surname>Faber</surname> <given-names>NS</given-names></name><name><surname>Scholl</surname> <given-names>J</given-names></name><name><surname>Nelissen</surname> <given-names>N</given-names></name><name><surname>Rushworth</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Self-Other mergence in the frontal cortex during cooperation and competition</article-title><source>Neuron</source><volume>91</volume><fpage>482</fpage><lpage>493</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.06.022</pub-id><pub-id pub-id-type="pmid">27477020</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yokoyama</surname> <given-names>O</given-names></name><name><surname>Miura</surname> <given-names>N</given-names></name><name><surname>Watanabe</surname> <given-names>J</given-names></name><name><surname>Takemoto</surname> <given-names>A</given-names></name><name><surname>Uchida</surname> <given-names>S</given-names></name><name><surname>Sugiura</surname> <given-names>M</given-names></name><name><surname>Horie</surname> <given-names>K</given-names></name><name><surname>Sato</surname> <given-names>S</given-names></name><name><surname>Kawashima</surname> <given-names>R</given-names></name><name><surname>Nakamura</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Right frontopolar cortex activity correlates with reliability of retrospective rating of confidence in short-term recognition memory performance</article-title><source>Neuroscience Research</source><volume>68</volume><fpage>199</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.neures.2010.07.2041</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><p>Whole-brain activations in response to context screen. The model (GLM1) included separate ‘condition’ regressors for trials where the context was signalled and trials where the context was hidden. The condition regressor for signalled-context trials was parametrically modulated by linear and quadratic terms for coherence (K and K<sup>2</sup>) and context (C and C<sup>2</sup>). In addition to the contrast estimates for these parametric modulators, the table also shows the contrast between signalled-context and hidden-context trials. Whole-brain statistical testing was performed by comparing contrast images across subjects to zero using one-sample <italic>t</italic>-tests. All reported activations are significant at p&lt;0.05, FWE-corrected for multiple comparisons, with a cluster-defining threshold of p&lt;0.001, uncorrected. Only clusters with a minimum size of 100 voxels are reported. K: coherence. C: context. FWE: familywise error. MNI: Montreal Neurological Institute. L: left. R: right. TPJ: temporoparietal junction. dmPFC: dorsomedial prefrontal cortex. FPl: lateral frontal pole.</p><p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th valign="top">Contrast</th><th valign="top">Label</th><th valign="top">voxels at <break/>p&lt;0.001</th><th valign="top">Peak <break/><italic>z</italic>-score</th><th valign="top"><italic>P</italic> (cluster <break/>FWE <break/>corrected)</th><th colspan="4" valign="top">Peak Voxel MNI coordinates</th></tr></thead><tbody><tr><td rowspan="7" valign="top">K: positive</td><td valign="top">striatum</td><td valign="top">1021</td><td valign="top">5.48</td><td valign="top">&lt;0.001</td><td valign="top">-20</td><td valign="top">8</td><td valign="top">10</td><td valign="top">L</td></tr><tr><td valign="top">striatum</td><td valign="top">852</td><td valign="top">5.18</td><td valign="top">&lt;0.001</td><td valign="top">18</td><td valign="top">14</td><td valign="top">0</td><td valign="top">R</td></tr><tr><td valign="top">posterior cingulate</td><td valign="top">419</td><td valign="top">4.49</td><td valign="top">&lt;0.001</td><td valign="top">-4</td><td valign="top">-34</td><td valign="top">32</td><td valign="top">LR</td></tr><tr><td valign="top">cerebellum</td><td valign="top">479</td><td valign="top">4.45</td><td valign="top">&lt;0.001</td><td valign="top">-28</td><td valign="top">-62</td><td valign="top">-40</td><td valign="top">L</td></tr><tr><td valign="top">cerebellum</td><td valign="top">281</td><td valign="top">4.41</td><td valign="top">0.003</td><td valign="top">22</td><td valign="top">-72</td><td valign="top">-28</td><td valign="top">R</td></tr><tr><td valign="top">parietal</td><td valign="top">196</td><td valign="top">4.30</td><td valign="top">0.019</td><td valign="top">-36</td><td valign="top">-60</td><td valign="top">62</td><td valign="top">L</td></tr><tr><td valign="top">parietal</td><td valign="top">462</td><td valign="top">4.11</td><td valign="top">&lt;0.001</td><td valign="top">42</td><td valign="top">-52</td><td valign="top">44</td><td valign="top">R</td></tr><tr><td valign="top">K: negative</td><td colspan="8" valign="top"/></tr><tr><td valign="top">K<sup>2</sup>: positive</td><td colspan="8" valign="top"/></tr><tr><td valign="top">K<sup>2</sup>: negative</td><td colspan="8" valign="top"/></tr><tr><td valign="top">C: positive</td><td valign="top">occipital</td><td valign="top">222</td><td valign="top">4.23</td><td valign="top">0.015</td><td valign="top">20</td><td valign="top">-80</td><td valign="top">-10</td><td valign="top">R</td></tr><tr><td valign="top">C: negative</td><td valign="top">occipital</td><td valign="top">277</td><td valign="top">3.71</td><td valign="top">0.005</td><td valign="top">-6</td><td valign="top">-84</td><td valign="top">-14</td><td valign="top">L</td></tr><tr><td rowspan="8" valign="top">C<sup>2</sup>: positive</td><td valign="top">parietal (incl. TPJ)</td><td valign="top">1668</td><td valign="top">5.57</td><td valign="top">&lt;0.001</td><td valign="top">-40</td><td valign="top">-58</td><td valign="top">36</td><td valign="top">L</td></tr><tr><td valign="top">parietal (incl. TPJ)</td><td valign="top">1656</td><td valign="top">4.82</td><td valign="top">&lt;0.001</td><td valign="top">50</td><td valign="top">-42</td><td valign="top">44</td><td valign="top">R</td></tr><tr><td valign="top">lateral frontal (incl. FPl)</td><td valign="top">465</td><td valign="top">4.49</td><td valign="top">&lt;0.001</td><td valign="top">-38</td><td valign="top">56</td><td valign="top">8</td><td valign="top">L</td></tr><tr><td valign="top">posterior cingulate</td><td valign="top">388</td><td valign="top">4.44</td><td valign="top">0.001</td><td valign="top">-2</td><td valign="top">-24</td><td valign="top">34</td><td valign="top">LR</td></tr><tr><td valign="top">dorsal medial frontal (incl. dmPFC)</td><td valign="top">692</td><td valign="top">4.29</td><td valign="top">&lt;0.001</td><td valign="top">4</td><td valign="top">36</td><td valign="top">32</td><td valign="top">LR</td></tr><tr><td valign="top">dorsolateral frontal</td><td valign="top">330</td><td valign="top">4.25</td><td valign="top">0.002</td><td valign="top">36</td><td valign="top">32</td><td valign="top">40</td><td valign="top">R</td></tr><tr><td valign="top">lateral frontal (incl. FPl)</td><td valign="top">598</td><td valign="top">4.05</td><td valign="top">&lt;0.001</td><td valign="top">26</td><td valign="top">60</td><td valign="top">-8</td><td valign="top">R</td></tr><tr><td valign="top">precuneus</td><td valign="top">366</td><td valign="top">3.90</td><td valign="top">0.001</td><td valign="top">10</td><td valign="top">-70</td><td valign="top">44</td><td valign="top">R</td></tr><tr><td valign="top">C<sup>2</sup>: negative</td><td valign="top">amygdala</td><td valign="top">196</td><td valign="top">4.54</td><td valign="top">0.029</td><td valign="top">18</td><td valign="top">-8</td><td valign="top">-20</td><td valign="top">R</td></tr><tr><td rowspan="4" valign="top">signalled &gt; hidden</td><td valign="top">dorsal medial frontal (incl. dmPFC)</td><td valign="top">341</td><td valign="top">4.67</td><td valign="top">.001</td><td valign="top">2</td><td valign="top">32</td><td valign="top">32</td><td valign="top">LR</td></tr><tr><td valign="top">posterior cingulate</td><td valign="top">1020</td><td valign="top">4.64</td><td valign="top">&lt;0.001</td><td valign="top">0</td><td valign="top">-14</td><td valign="top">32</td><td valign="top">LR</td></tr><tr><td valign="top">lateral frontal (incl. FPl)</td><td valign="top">181</td><td valign="top">4.05</td><td valign="top">.035</td><td valign="top">38</td><td valign="top">52</td><td valign="top">14</td><td valign="top">R</td></tr><tr><td valign="top">anterior frontal</td><td valign="top">283</td><td valign="top">3.95</td><td valign="top">.004</td><td valign="top">6</td><td valign="top">56</td><td valign="top">0</td><td valign="top">LR</td></tr><tr><td rowspan="4" valign="top">signalled &lt; hidden</td><td valign="top">temporal lobe</td><td valign="top">519</td><td valign="top">4.59</td><td valign="top">&lt;0.001</td><td valign="top">-68</td><td valign="top">-42</td><td valign="top">16</td><td valign="top">L</td></tr><tr><td valign="top">anterior temporal lobe</td><td valign="top">264</td><td valign="top">4.17</td><td valign="top">.006</td><td valign="top">-52</td><td valign="top">-2</td><td valign="top">-14</td><td valign="top">L</td></tr><tr><td valign="top">temporal lobe</td><td valign="top">584</td><td valign="top">4.12</td><td valign="top">&lt;0.001</td><td valign="top">60</td><td valign="top">-32</td><td valign="top">2</td><td valign="top">R</td></tr><tr><td valign="top">anterior temporal lobe</td><td valign="top">220</td><td valign="top">3.87</td><td valign="top">.015</td><td valign="top">52</td><td valign="top">-12</td><td valign="top">-20</td><td valign="top">R</td></tr></tbody></table></table-wrap></p></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><p>Whole-brain activations in response to stimulus presentation. The model (GLM2) included a ‘condition’ regressor for stimulus presentation. The condition regressor was parametrically modulated by linear and quadratic coherence (K and K<sup>2</sup>). Whole-brain statistical testing was performed by comparing contrast images across subjects to zero using one-sample <italic>t</italic>-tests. All reported activations are significant at p&lt;0.05, FWE-corrected for multiple comparisons, with a cluster-defining threshold of p&lt;0.001, uncorrected. Only clusters with a minimum size of 100 voxels are reported. K: coherence. FWE: familywise error. MNI: Montreal Neurological Institute. L: left. R: right. IPS: intraparietal sulcus.</p><p><table-wrap id="inlinetable2" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th valign="top">Contrast</th><th valign="top">Label</th><th valign="top">voxels <break/>at p&lt;0.001</th><th valign="top">Peak <break/><italic>z</italic>-score</th><th valign="top"><italic>P</italic> (cluster <break/>FWE corrected)</th><th colspan="4" valign="top">Peak Voxel MNI coordinates</th></tr></thead><tbody><tr><td rowspan="4" valign="top">K: positive</td><td valign="top">striatum</td><td valign="top">241</td><td valign="top">4.41</td><td valign="top">.006</td><td valign="top">-20</td><td valign="top">2</td><td valign="top">12</td><td valign="top">L</td></tr><tr><td valign="top">striatum</td><td valign="top">160</td><td valign="top">4.31</td><td valign="top">.042</td><td valign="top">22</td><td valign="top">-2</td><td valign="top">0</td><td valign="top">R</td></tr><tr><td valign="top">parietal (incl. IPS)</td><td valign="top">821</td><td valign="top">4.25</td><td valign="top">&lt;0.001</td><td valign="top">40</td><td valign="top">-52</td><td valign="top">38</td><td valign="top">R</td></tr><tr><td valign="top">parietal (incl. IPS)</td><td valign="top">506</td><td valign="top">4.08</td><td valign="top">&lt;0.001</td><td valign="top">-42</td><td valign="top">-56</td><td valign="top">46</td><td valign="top">L</td></tr><tr><td valign="top">K: negative</td><td colspan="8" valign="top"/></tr><tr><td valign="top">K<sup>2</sup>: positive</td><td colspan="8" valign="top"/></tr><tr><td valign="top">C<sup>2</sup>: negative</td><td colspan="8" valign="top"/></tr></tbody></table></table-wrap></p></boxed-text></app><app id="appendix-3"><title>Appendix 3</title><boxed-text><p>Whole-brain activations in EDI searchlight analysis. Condition-specific multivariate patterns were obtained by modelling the neural response to the context screen separately for each condition of our factorial design (GLM4; only signalled-context trials). An EDI for the full task space (KxC) was then computed for a spherical cluster of voxels centred at each voxel within each subject using a searchlight procedure. Whole-brain statistical testing was performed by comparing EDI images across subjects to zero using one-sample <italic>t</italic>-tests. All reported activations are significant at p&lt;0.05, FWE-corrected for multiple comparisons, with a cluster-defining threshold of p&lt;0.001, uncorrected. Only clusters with a minimum size of 100 voxels are reported. FWE: familywise error. KxC: coherence x context. MNI: Montreal Neurological Institute. L: left. R: right.</p><p><table-wrap id="inlinetable3" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th valign="top">Space</th><th valign="top">Label</th><th valign="top">voxels at p&lt;0.001</th><th valign="top">Peak <italic>z</italic>-score</th><th valign="top"><italic>P</italic> (cluster <break/>FWE corrected)</th><th colspan="4" valign="top">Peak Voxel MNI coordinates</th></tr></thead><tbody><tr><td rowspan="4" valign="top">KxC</td><td valign="top">visual cortex</td><td valign="top">12170</td><td valign="top">7.01</td><td valign="top">&lt;0.001</td><td valign="top">-18</td><td valign="top">-86</td><td valign="top">-14</td><td valign="top">LR</td></tr><tr><td valign="top">posterior parietal</td><td valign="top">791</td><td valign="top">5.12</td><td valign="top">&lt;0.001</td><td valign="top">-42</td><td valign="top">-48</td><td valign="top">36</td><td valign="top">R</td></tr><tr><td valign="top">visual cortex</td><td valign="top">141</td><td valign="top">5.10</td><td valign="top">&lt;0.001</td><td valign="top">46</td><td valign="top">-52</td><td valign="top">-6</td><td valign="top">R</td></tr><tr><td valign="top">parietal</td><td valign="top">170</td><td valign="top">4.29</td><td valign="top">&lt;0.001</td><td valign="top">-24</td><td valign="top">-24</td><td valign="top">78</td><td valign="top">L</td></tr></tbody></table></table-wrap></p></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56477.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Chang</surname><given-names>Steve WC</given-names></name><role>Reviewer</role><aff><institution>Yale University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>How individuals convert private mental states into context-dependent public reports is an important but open question. This study tackles this issue in an elegant and novel way, using functional imaging and perceptual reports as an example. The results show that lateral frontal pole supports the mapping of private information to public reports, providing novel insights into the brain areas supporting social behavior.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Private-public mappings in human prefrontal cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Michael Frank as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Steve W C Chang (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission. In recognition of the fact that revisions may take longer than the two months we typically allow, until the research enterprise restarts in full, we will give authors as much time as they need to submit revised manuscripts.</p><p>Summary:</p><p>This study examines context-dependent mapping from &quot;private&quot; to &quot;public&quot; confidence in the human brain using combined univariate and multivariate approaches to fMRI data. The authors used a social perceptual decision task with 4x4 factorial design, which enabled the separation of an internal sense of confidence from an explicit public report. The authors found that subjects considered the social context for making their public confidence ratings. Imaging data was analyzed in medial-lateral divisions of the prefrontal cortex, motivated by previous studies. Univariate fMRI analyses show that pgACC and dACC tracked the formation of an internal sense of confidence, while FPl is involved in mapping it to a public report that reflects the contextual requirement. Finally, multivariate analysis using RSA further support the idea that FPI is involved in contextualizing private confidence by carrying a representation of different state conditions in the task space.</p><p>How the brain translates private information into the public domain is an important question, and all reviewers agreed that this study makes an important contribution to this topic. There were also a number of concerns that the authors should address in a revised version of this manuscript.</p><p>Essential revisions:</p><p>1) There were multiple comments regarding the analyses and results surrounding Figures 3 and 4. These need to be clarified.</p><p>1.1) The choice of modeling BOLD for Figure 3 only when the player was revealed seems arbitrary. Other studies looking at &quot;internal&quot; confidence ratings have typically looked at neural signals locked on the type I response onset. Please report the results of modeling BOLD at the response onset of the private and public decision with the same parametric regressors.</p><p>1.2) Relatedly, why does pgACC and dACC track the formation of an internal sense of confidence (although each showed the response profile of encoding different measures of it) only after being presented with the social context, but not at the time of making an individual decision (no effects of private confidence seemed to be present before the revelation of the context in Figure 4B)? Is this what the authors expected or is this driven by not having any task marker associated with the time when subjects internally decided on a response?</p><p>1.3) The authors report areas that track subjective confidence as well as those that may be involved in private-to-public mapping, but it is unclear which (if any) areas tracks the variable that is ultimately being reported? Figure 4 suggests that dACC activity would be a good candidate. Does dACC correlate with publicly reported confidence (more so than subjective confidence) if an analytical approach akin to Figure 3 is used?</p><p>1.4) For all parametric fMRI analyses in Figure 3, please compare the goodness of fit between the linear and quadratic models, to assess the necessity of adding a quadratic expansion. For the conclusions to be warranted, a quadratic term should improve goodness of fit in FPI but not other regions.</p><p>1.5) &quot;an indicator of the degree of behavioral deviation from a default policy&quot; – If this statement were true, shouldn't the connectivity between FPl and dACC correlate with the absolute value of the difference between private and public confidence, reflecting the same rationale for using the quadratic context term as parametric regressor?</p><p>1.6) There were no explanations for why there might be two significant time periods of PPI effects in Figure 4C. There are to two independent peaks, one right after the context revelation and the other, a more pronounced peak on average, occurring from 6 sec and 8 sec. Please explain these patterns, which are also seen even for the FPI-pgACC panel.</p><p>2) Reviewers wondered if the contrast between internal and explicit confidence is contaminated by the choice of time-locking event. That is, are the results really specific to private-to-public mapping, or do they rather reflect the sequential nature of this task, and comparable findings would be observed in a number of processes involving two different states? This question has implications for the novelty of the current study. That is, if the results are not specific for private-public mapping, as opposed to a serial stage, implicit-explicit, etc. type of processes, then the novelty of the neural data would decrease profoundly (e.g., the idea of internal processing to be more medially and external processing to be more laterally localized in the PFC has been around for quite some time).</p><p>3) Confidence model.</p><p>3.1) It would be important to add a description of the model leading to confidence estimates irrespective of context, as it seems rather mysterious in the present form. Regarding the analysis strategy, please compare this modeling approach with a simpler strategy, e.g., applying mixed-effects ordinal regression on confidence with perceptual evidence and context as fixed effects.</p><p>3.2) In order to make sure that the confidence model and its out-of-sample prediction of private confidence estimates are reliable, it would be necessary to validate the accuracy of the model within the data it fitted to. Please report cross-validation accuracy within the data from the behavioral session.</p><p>3.3) What is the correlation between motion coherence and the model-derived estimates of private confidence? It was unclear why dACC did not encode the linear term for coherence as in Figure 3C but did track the model-derived estimates of private confidence, although both are the indirect measures of private confidence. Please explain what each of these measures differentially represent with regards to the subject's private state. Also, it was not clear why dACC would only encode one of private confidence measures.</p><p>4) Reviewers were concerned that because the current study addresses a novel question with a novel approach and there was no pre-registration, that there was quite a bit of analytical freedom, for instance in the selection of ROIs, which might have biased results. To help mitigate these concerns, it would be important to (a) clearly state the a priori rational that went into the selection of ROIs, (b) fully report the results of the analyses regarding different time points (essential revision 1.1), and (c) show that the results of the ROI analyses converge with the whole-brain results. In this regard, please move the whole-brain univariate results to the main text and discuss it more thoroughly. For the RSA analysis, please conduct a searchlight to provide whole-brain results that can be compared with the ROI findings as well.</p><p>5) The manuscript does not contain any statistical results in the main text to support the claims. Some statistical thresholds are reported in the figure legends, but some figures (for instance Figures 4 and 5) do not contain any statistical information. Please include all statistical tests, t, f, and exact p-values in the main text.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56477.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) There were multiple comments regarding the analyses and results surrounding Figure 3 and 4. These need to be clarified.</p><p>1.1) The choice of modeling BOLD for Figure 3 only when the player was revealed seems arbitrary. Other studies looking at &quot;internal&quot; confidence ratings have typically looked at neural signals locked on the type I response onset. Please report the results of modeling BOLD at the response onset of the private and public decision with the same parametric regressors.</p></disp-quote><p>We thank the reviewers for this prompt to clarify our key analyses. The focus of the current study is the generation of a context-dependent confidence report – a process that requires the combination of an internal sense of confidence with knowledge about the current context. We selected the context screen as our time window of interest for two reasons. First, the context screen is the first point in a trial that information about the current partner is revealed – any context-related regressors would have little meaning if assigned to timepoints earlier on in a trial. Second, during the presentation of the context screen, subjects have all the information needed to internally decide on a context-dependent confidence report, but the neural response will not be confounded by the engagement of motor processes needed to select a confidence report – a motor plan can only be prepared after the randomised initial location of the confidence marker is revealed. We now carefully describe the reasoning behind the selection of our time window of interest in the main text (subsection “Encoding of motion coherence and social context in prefrontal cortex”).</p><p>Nonetheless, we appreciate that any inference on timing here is going to be imprecise due to the lag in the BOLD signal and the short, fixed intervals between the context screen and the preceding perceptual decision (0.5s) and subsequent confidence scale (presented immediately after the context screen). Thus, in line with the reviewers’ suggestion, we now also perform a whole-brain analysis of the BOLD response to the presentation of the motion stimulus using linear and quadratic coherence as parametric modulators (Appendix 2 – statistical maps uploaded to our NeuroVault repository). Note that, as described above, it does not make sense to include regressors related to the context at this timepoint, as the information has not yet been revealed. Further, we reiterate the short, fixed intervals between the context screen and (1) the preceding perceptual decision and (2) the subsequent confidence scale in the figures that display ROI activity time courses (Figure 4 and 5) - thus encouraging the reader to independently evaluate the temporal profile of our results.</p><disp-quote content-type="editor-comment"><p>1.2) Relatedly, why does pgACC and dACC track the formation of an internal sense of confidence (although each showed the response profile of encoding different measures of it) only after being presented with the social context, but not at the time of making an individual decision (no effects of private confidence seemed to be present before the revelation of the context in Figure 4B)? Is this what the authors expected or is this driven by not having any task marker associated with the time when subjects internally decided on a response?</p></disp-quote><p>We thank the reviewers for prompting further clarity on this analysis. We regret that we did not provide sufficient detail in the main text for appreciating (a) the difference between the factorial analysis of neural activity (Figure 3) and the model-based analysis of neural encoding (old Figure 4) and (b) the temporal features of the encoding profiles revealed by our model-based approach. In response to this point – and essential revisions 1.3, 1.6 and 3 – we have substantially revised the section on neural encoding (subsection “Encoding of trial-by-trial confidence in prefrontal cortex”) – including the description of the confidence model – and the associated figure (now divided into Figure 4 and Figure 5).</p><p>There are two aspects to point 1.2: (a) possible reasons why the factorial and model-based analyses generate different (but in fact complementary) results and (b) temporal features of the neural encoding of the model-based estimate of private confidence. As for issue (a), our model-based estimate of private confidence is not directly equivalent to the coherence terms in our factorial analysis. Instead, the model-based estimate takes into account not only the influence of motion coherence on subjective experience but also the time taken to make a decision – a factor known to affect private confidence over and above the perceptual evidence itself (Kiani, Corthell and Shadlen, Neuron, 2014). Indeed, our model-based approach indicates that private confidence is comparable for fast decisions in response to low-coherence stimuli and slow decisions in response to high-coherence stimuli (now illustrated in Figure 4—figure supplement 1). As for issue (b), peak neural encoding of the model-based estimate of private confidence appears around 2 seconds after the onset of the context screen – a result which, given the canonical lag in the BOLD signal and the trial timings detailed in response to essential revision 1.1, is consistent with the model-based estimate relating to the perceptual decision rather than the social context. We now explain these relationships carefully in the main text.</p><disp-quote content-type="editor-comment"><p>1.3) The authors report areas that track subjective confidence as well as those that may be involved in private-to-public mapping, but it is unclear which (if any) areas tracks the variable that is ultimately being reported? Figure 4 suggests that dACC activity would be a good candidate. Does dACC correlate with publicly reported confidence (more so than subjective confidence) if an analytical approach akin to Figure 3 is used?</p></disp-quote><p>We recognise that this result may not have been apparent from the section on neural encoding and we have – as described in response to essential revision 1.2 – now revised it significantly. Critically, our analysis of neural encoding includes not only the model-based estimate of private confidence but also the empirically observed confidence reports (pink and cyan lines in Figure 4). Of the three ROIs, only dACC encoded both variables, indicating that dACC indeed covaries with the variable that is ultimately being reported by subjects. We interpret this analysis as indicating that dACC encodes elements of both the “input” and the “output” of a private-public mapping – with our other analyses indicating that FPl provides the relevant signals needed to adjust the mapping itself. We have also included a new opening paragraph of the Discussion to ensure that the findings regarding the three ROIs are clearly summarised.</p><disp-quote content-type="editor-comment"><p>1.4) For all parametric fMRI analyses in Figure 3, please compare the goodness of fit between the linear and quadratic models, to assess the necessity of adding a quadratic expansion. For the conclusions to be warranted, a quadratic term should improve goodness of fit in FPI but not other regions.</p></disp-quote><p>We thank the reviewers for this suggestion. We have added Figure 3—figure supplement 1 where we show – in an independent set of analyses – that (1) a full model (both linear and quadratic terms) provides a better fit to FPl activity estimates than a reduced model (linear terms only) and (2) that the difference in goodness-of-fit between the full and the reduced model is higher in FPl than in other ROIs.</p><disp-quote content-type="editor-comment"><p>1.5) &quot;an indicator of the degree of behavioral deviation from a default policy&quot; – If this statement were true, shouldn't the connectivity between FPl and dACC correlate with the absolute value of the difference between private and public confidence, reflecting the same rationale for using the quadratic context term as parametric regressor?</p></disp-quote><p>This is a very useful suggestion, and we have now revised our connectivity analysis to directly address this question. More specifically, we now include separate terms for the model-based estimate of private confidence and the empirically observed confidence reports as well as their interaction in our analysis. The advantage of this revised design matrix over using the absolute difference is that the three terms together allow us to capture signed differences in the coding of understatements (private &gt; reported) and overstatements of confidence (private &lt; reported). In support of our original interpretation, this analysis shows that FPl-dACC connectivity is (1) higher for larger shifts in the mapping from private to reported confidence and (2) highest when a subject understated rather than overstated their confidence. We now present this analysis in the main text (subsection “Encoding of trial-by-trial confidence in prefrontal cortex”) and have revised the figure (now Figure 5) accordingly.</p><disp-quote content-type="editor-comment"><p>1.6) There were no explanations for why there might be two significant time periods of PPI effects in Figure 4C. There are to two independent peaks, one right after the context revelation and the other, a more pronounced peak on average, occurring from 6 sec and 8 sec. Please explain these patterns, which are also seen even for the FPI-pgACC panel.</p></disp-quote><p>As described in response to essential revision 1.5, we have revised our connectivity analysis (now shown in Figure 5) in a way that allows us to better understand the underlying drivers of this pattern. First, around the onset of the context screen, there is a transient increase in FPl-dACC connectivity driven by the model-based estimate of private confidence (pink line). Second, 5-7s after the onset of the context screen, FPl-dACC connectivity varies as a function of not only the model-based estimate of private confidence but also the empirically observed confidence reports (cyan line) as well as their interaction (green line). In other words, the “double bump” observed in the previous Figure 4C reflected two distinct process – an early process relating to the perceptual decision and a later process relating to the social context. We now comment on both of these aspects of FPl-dACC connectivity in the main text (subsection “Encoding of trial-by-trial confidence in prefrontal cortex”).</p><disp-quote content-type="editor-comment"><p>2) Reviewers wondered if the contrast between internal and explicit confidence is contaminated by the choice of time-locking event. That is, are the results really specific to private-to-public mapping, or do they rather reflect the sequential nature of this task, and comparable findings would be observed in a number of processes involving two different states? This question has implications for the novelty of the current study. That is, if the results are not specific for private-public mapping, as opposed to a serial stage, implicit-explicit, etc. type of processes, then the novelty of the neural data would decrease profoundly (e.g., the idea of internal processing to be more medially and external processing to be more laterally localized in the PFC has been around for quite some time).</p></disp-quote><p>We are glad for the opportunity to expand on this issue. As we explain in response to essential revision 1.1, the choice of our time window of interest is motivated directly by the design: this is the first timepoint within a trial that context information is available. We also think there are several reasons for why the results cannot be accommodated by more generic “serial-stage” or “implicit-explicit” accounts. In a new paragraph in the Discussion (third paragraph), we provide several lines of evidence in support of a private-public mapping interpretation. In short, we note that the quadratic context term – encoded by FPl in our factorial analysis of neural activity – compares conditions that are matched in general task characteristics and only differ in the contextual requirements on a private-public mapping. Specifically, both the “inlying” contexts (medium-low and medium-high confidence) and the “outlying” contexts (low and high confidence) involve sequential preparation of a private state and a public action – the difference between these two types of context is that the latter requires a larger shift in the mapping from private to public confidence. Furthermore, we note that FPl activity was higher on the trials where the context was signalled than on the trials in which the context was hidden from subjects (now shown in Table 1). Again, these two types of trial are matched in general task characteristics and only differ in the availability of a context-dependent private-public mapping. More broadly, we believe that any task that involves explicit reports – regardless of whether the task itself involves social interaction – must to an extent invoke private-public mappings – and our results thus provide a potential explanation of why lateral PFC is often activated by such tasks.</p><disp-quote content-type="editor-comment"><p>3) Confidence model.</p><p>3.1) It would be important to add a description of the model leading to confidence estimates irrespective of context, as it seems rather mysterious in the present form. Regarding the analysis strategy, please compare this modeling approach with a simpler strategy, e.g., applying mixed-effects ordinal regression on confidence with perceptual evidence and context as fixed effects.</p></disp-quote><p>We thank the reviewers for prompting more clarity here. As described in response to essential revision 1.2, we have substantially revised the section on neural encoding (subsection “Encoding of trial-by-trial confidence in prefrontal cortex”) – including the description of the confidence model in the main text and its associated illustration in Figure 4A. Our approach in fact very similar to the ordinal regression approach suggested by the reviewers: we (1) fit an ordinal regression model to data from the behavioural session to estimate the fixed effects of motion coherence, choice reaction time and each social context and then (2) apply the fitted model to behavioural data from the fMRI session while setting the estimated fixed effects of social context to zero. This procedure is performed separately for each subject in order to take into account individual variation in how people generally report their confidence (e.g., some people report higher confidence regardless of the task at hand; Ais et al., Cognition, 2016) – an aspect that is modelled by the “thresholds” inherent to an ordinal regression model.</p><disp-quote content-type="editor-comment"><p>3.2) In order to make sure that the confidence model and its out-of-sample prediction of private confidence estimates are reliable, it would be necessary to validate the accuracy of the model within the data it fitted to. Please report cross-validation accuracy within the data from the behavioral session.</p></disp-quote><p>We have included a supplementary figure (Figure 4—figure supplement 2) that shows cross-validation accuracy within the behavioural session for each subject. In short, on each iteration of a leave-one-trial-out procedure, we fit the model to all trials but one and then compute the negative log likelihood of the report observed on the left-out trial under the confidence model (where the probability distribution over reports depends on the fitted model) and a null model (where the probability distribution over reports is uniform). We then sum the cross-validated negative log-likelihoods across all trials and compute the difference between the confidence model and the null model – with a positive value indicating higher cross-validation accuracy under the confidence model than the null model. In support of our model-based approach, the confidence model outperformed the null model in all subjects.</p><disp-quote content-type="editor-comment"><p>3.3) What is the correlation between motion coherence and the model-derived estimates of private confidence? It was unclear why dACC did not encode the linear term for coherence as in Figure 3C but did track the model-derived estimates of private confidence, although both are the indirect measures of private confidence. Please explain what each of these measures differentially represent with regards to the subject's private state. Also, it was not clear why dACC would only encode one of private confidence measures.</p></disp-quote><p>As described in response to essential revision 1.2, we now explain the subtle but important differences between our factorial analysis of neural activity and the model-based analysis of neural encoding. More specifically, our model-based estimate of private confidence takes into account not only the impact of motion coherence on subjective experience but also the time taken to make a decision – a factor which has been shown to affect private confidence over and above the perceptual evidence itself. In addition to revising the main text, we have also added a supplementary figure that illustrates how the model-based estimate of private confidence varies with motion coherence and choice reaction time – for example, private confidence is comparable for fast decisions in response to low-coherence stimuli and slow decisions in response to high-coherence stimuli (Figure 4—figure supplement 1).</p><disp-quote content-type="editor-comment"><p>4) Reviewers were concerned that because the current study addresses a novel question with a novel approach and there was no pre-registration, that there was quite a bit of analytical freedom, for instance in the selection of ROIs, which might have biased results. To help mitigate these concerns, it would be important to (a) clearly state the a priori rational that went into the selection of ROIs, (b) fully report the results of the analyses regarding different time points (essential revision 1.1), and (c) show that the results of the ROI analyses converge with the whole-brain results. In this regard, please move the whole-brain univariate results to the main text and discuss it more thoroughly. For the RSA analysis, please conduct a searchlight to provide whole-brain results that can be compared with the ROI findings as well.</p></disp-quote><p>We appreciate in hindsight that formal preregistration would have been useful – especially given the specificity of our hypotheses. To provide more context for our study, and address these concerns, we have now revised the paper as follows:</p><p>a) ROI selection</p><p>We now explain the a priori reasoning behind ROI selection in a separate paragraph in the first section of the Results reporting fMRI analyses. We highlight that the three ROIs have been identified as putative neural substrates for decision confidence across a variety of studies but that their role in the generation of context-dependent confidence report is unclear. We directly hypothesised in the Discussion section of a previous paper (Bang and Fleming, PNAS, 2018) that the lateral frontopolar cortex may support a mapping from private to public confidence – a hypothesis that we suggested could be tested by manipulating the contextual requirements on a private-public mapping. Our study was also reviewed internally in January 2018 at the Wellcome Centre for Human Neuroimaging in the Centre’s “project presentation” sessions. The lateral frontopolar cortex was explicitly identified as a key ROI for mediating private-public mappings in this presentation.</p><p>b) Time window of interest</p><p>As described in response to essential revision 1.1, we now explain the reasoning behind the selection of our time window of interest.</p><p>c) Univariate whole-brain analysis</p><p>We now report the univariate whole-brain analysis of the BOLD response to the context screen in the main text (subsection “Encoding of motion coherence and social context in prefrontal cortex”).</p><p>d) Multivariate whole-brain analysis</p><p>We have now conducted a multivariate whole-brain analysis (Appendix 3 – statistical map uploaded to our NeuroVault repository). The analysis (now detailed in the Materials and methods subsection “Multivariate analysis”) is based on a first-level model using the same design matrix as our multivariate ROI analyses – modelling the whole-brain response to the context screen separately for each condition of our factorial design – but using smoothed data as we no longer extract pattern information from anatomically defined ROIs. In brief, we computed the EDI metric for the full task space (coherence x context) for a spherical cluster of voxels centred at each voxel within a subject. Consistent with previous RSA studies (e.g., Nili et al., PLOS Computational Biology, 2014), the diameter of the sphere was 15mm (around 100 voxels). The sphere was adapted in shape if it was near the edges of a whole-brain group-level mask – a mask defined as the intersection of the whole-brain masks obtained for each subject during estimation of the first-level model. Finally, whole-brain statistical testing was performed by applying one-sample t-tests against 0 to the first-level EDI maps while correcting for multiple comparisons within the SPM framework. The searchlight analysis was implemented using the RSA toolbox (Nili et al., PLOS Computational Biology, 2014) for SPM.</p><p>The searchlight analysis identified significant clusters in visual areas – a result which may be explained by the task conditions differing in visual appearance (i.e. motion coherence + colour of the avatar that indicates the current partner). We highlight that the searchlight analysis is not directly comparable to the ROI analysis but, rather, complements it. For example, the FPl ROI analysis is more sensitive to subtle pattern information as the FPl ROI contains around 10 times more voxels than the searchlight sphere. Further, our FPl ROI is bilateral and thus contains a substantial proportion of non-contiguous voxels. By contrast, the searchlight sphere is restricted to contiguous voxels within a relatively restricted region and can therefore only identify localised pattern information. In addition, our FPl ROI is anatomically defined and thus has a shape which cannot be captured by a searchlight sphere. We now explain these differences in the Materials and methods.</p><disp-quote content-type="editor-comment"><p>5) The manuscript does not contain any statistical results in the main text to support the claims. Some statistical thresholds are reported in the figure legends, but some figures (for instance Figures 4 and 5) do not contain any statistical information. Please include all statistical tests, t, f, and exact p-values in the main text.</p></disp-quote><p>We now provide information about statistical testing in all figure legends. We have also added tables providing statistical information for both the univariate (Table 1) and multivariate (Table 2) ROI analyses.</p></body></sub-article></article>