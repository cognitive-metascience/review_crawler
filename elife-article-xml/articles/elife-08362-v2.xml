<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">08362</article-id><article-id pub-id-type="doi">10.7554/eLife.08362</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A principle of economy predicts the functional architecture of grid cells</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-33582"><name><surname>Wei</surname><given-names>Xue-Xin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-33583"><name><surname>Prentice</surname><given-names>Jason</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-15603"><name><surname>Balasubramanian</surname><given-names>Vijay</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="other" rid="par-6"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Psychology</institution>, <institution>University of Pennsylvania</institution>, <addr-line><named-content content-type="city">Philadelphia</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Princeton Neuroscience Institute</institution>, <institution>Princeton University</institution>, <addr-line><named-content content-type="city">Princeton</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Physics</institution>, <institution>University of Pennsylvania</institution>, <addr-line><named-content content-type="city">Philadelphia</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Neuroscience</institution>, <institution>University of Pennsylvania</institution>, <addr-line><named-content content-type="city">Philadelphia</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-14764"><name><surname>Skinner</surname><given-names>Frances K</given-names></name><role>Reviewing editor</role><aff><institution>University Health Network</institution>, <country>Canada</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>vijay@physics.upenn.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>03</day><month>09</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>4</volume><elocation-id>e08362</elocation-id><history><date date-type="received"><day>27</day><month>04</month><year>2015</year></date><date date-type="accepted"><day>01</day><month>09</month><year>2015</year></date></history><permissions><copyright-statement>© 2015, Wei et al</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Wei et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-08362-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.08362.001</object-id><p>Grid cells in the brain respond when an animal occupies a periodic lattice of ‘grid fields’ during navigation. Grids are organized in modules with different periodicity. We propose that the grid system implements a hierarchical code for space that economizes the number of neurons required to encode location with a given resolution across a range equal to the largest period. This theory predicts that (i) grid fields should lie on a triangular lattice, (ii) grid scales should follow a geometric progression, (iii) the ratio between adjacent grid scales should be √e for idealized neurons, and lie between 1.4 and 1.7 for realistic neurons, (iv) the scale ratio should vary modestly within and between animals. These results explain the measured grid structure in rodents. We also predict optimal organization in one and three dimensions, the number of modules, and, with added assumptions, the ratio between grid periods and field widths.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.08362.001">http://dx.doi.org/10.7554/eLife.08362.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.08362.002</object-id><title>eLife digest</title><p>In the 1930s, neuroscientists studying how rodents find their way through a maze proposed that the animals could construct an internal map of the maze inside their heads. The map was thought to enable the animals to navigate between familiar locations and also to identify shortcuts and alternative routes whenever familiar ones were blocked.</p><p>In the 1960s, recordings of electrical activity in the rat brain provided the first clues as to which nerve cells form this spatial map. In a region of the brain called the hippocampus, nerve cells called ‘place cells’ are active whenever the rat finds itself in a specific location. However, place cells alone are not able to support all types of navigation. Some spatial tasks also require cells in a region of the brain called the medial entorhinal cortex (MEC), which supplies most of the information that the hippocampus receives.</p><p>Cells in the MEC called ‘grid cells’ represent two-dimensional space as a repeating grid of triangles. A given grid cell is activated if the animal is located at a particular distance and angle away from the center of any of these triangles. The size of the triangles in these grids varies systematically throughout the MEC. Individual grid cells at one end of the structure encode space in finer detail than grid cells at the opposite end.</p><p>Wei et al. have now used mathematical modeling to explore how grid cells are organized. The model assumes that the brain seeks to encode space at whatever resolution an animal requires using as few nerve cells as possible. The model successfully reproduces several known features of grid cells, including the triangular shape of the grid, and the fact that the size of the triangles increases in steps of a specific size across the MEC.</p><p>In addition to providing a mathematical basis for the way that grid cells are organized in the brain, the model makes a number of testable predictions. These include predictions of the number of grid cells in the rat brain, as well as the pattern that grid cells adopt in three-dimensions: a question that is currently being studied in bats. Wei et al.'s findings suggest that the code used by the grid to represent space is an analog of a decimal number system—except that space is not subdivided by factors of 10 to form decimal ‘digits’, but by a quantity related to a famous constant in the field of mathematics called Euler's number.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.08362.002">http://dx.doi.org/10.7554/eLife.08362.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>grid cell</kwd><kwd>spatial cognition</kwd><kwd>efficient coding</kwd><kwd>theoretical neuroscience</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation (NSF)</institution></institution-wrap></funding-source><award-id>PHY-1058202</award-id><principal-award-recipient><name><surname>Wei</surname><given-names>Xue-Xin</given-names></name><name><surname>Prentice</surname><given-names>Jason</given-names></name><name><surname>Balasubramanian</surname><given-names>Vijay</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>PSL Research University Paris</institution></institution-wrap></funding-source><award-id>Fondation Pierre-Gilles de Gennes</award-id><principal-award-recipient><name><surname>Balasubramanian</surname><given-names>Vijay</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>The Starr Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Prentice</surname><given-names>Jason</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation (NSF)</institution></institution-wrap></funding-source><award-id>PHY-1066293</award-id><principal-award-recipient><name><surname>Balasubramanian</surname><given-names>Vijay</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation (NSF)</institution></institution-wrap></funding-source><award-id>EF-0928048</award-id><principal-award-recipient><name><surname>Wei</surname><given-names>Xue-Xin</given-names></name><name><surname>Prentice</surname><given-names>Jason</given-names></name><name><surname>Balasubramanian</surname><given-names>Vijay</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation (NSF)</institution></institution-wrap></funding-source><award-id>PHY-1125915</award-id><principal-award-recipient><name><surname>Balasubramanian</surname><given-names>Vijay</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.3</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Mathematical modeling suggests that grid cells in the rodent brain use fundamental principles of number theory to maximize the efficiency of spatial mapping, enabling animals to accurately encode their location with as few neurons as possible.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>How does the brain represent space? <xref ref-type="bibr" rid="bib57">Tolman (1948)</xref> suggested that the brain must have an explicit neural representation of physical space, a <italic>cognitive map</italic>, that supports higher brain functions such as navigation and path planning. The discovery of place cells in the rat hippocampus (<xref ref-type="bibr" rid="bib42">O'Keefe, 1976</xref>; <xref ref-type="bibr" rid="bib41">O'Keefe and Nadel, 1978</xref>) suggested one potential locus for this map. Place cells have spatially localized firing fields which reorganize dramatically when the environment changes (<xref ref-type="bibr" rid="bib34">Leutgeb et al., 2005</xref>). Another potential locus for the cognitive map of space has been uncovered in the main input to hippocampus, a structure known as the medial entorhinal cortex (MEC) (<xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="bibr" rid="bib23">Fyhn et al., 2004</xref>; <xref ref-type="bibr" rid="bib27">Hafting et al., 2005</xref>). When rats freely explore a two-dimensional open environment, individual ‘grid cells’ in the MEC display spatial firing fields that form a periodic triangular grid which tiles space (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). It is believed that grid fields provide relatively rigid coordinates on space based partly on self-motion and partly on environmental cues (<xref ref-type="bibr" rid="bib39">Moser et al., 2008</xref>). The scale of grid fields varies systematically along the dorso–ventral axis of the MEC (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) (<xref ref-type="bibr" rid="bib27">Hafting et al., 2005</xref>; <xref ref-type="bibr" rid="bib4">Barry et al., 2007</xref>; <xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>). Recently, it was shown that grid cells are organized in discrete modules within which cells share the same orientation and periodicity but vary randomly in phase (<xref ref-type="bibr" rid="bib4">Barry et al., 2007</xref>; <xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.08362.003</object-id><label>Figure 1.</label><caption><title>Representing place in the grid system.</title><p>(<bold>A</bold>) Grid cells (small triangles) in the medial entorhinal cortex (MEC) respond when the animal is in a triangular lattice of physical locations (red circles) (<xref ref-type="bibr" rid="bib23">Fyhn et al., 2004</xref>; <xref ref-type="bibr" rid="bib27">Hafting et al., 2005</xref>). The scale of periodicity (the ‘grid scale’, <italic>λ</italic><sub><italic>i</italic></sub>) and the size of the regions evoking a response above a noise threshold (the ‘grid field width’, <italic>l</italic><sub><italic>i</italic></sub>) vary modularly along the dorso-ventral axis of the MEC (<xref ref-type="bibr" rid="bib27">Hafting et al., 2005</xref>). Grid cells within a module vary in the phase of their spatial response, but share the same period and grid orientation (in two dimensions) (<xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>). (<bold>B</bold>) A simplified binary grid scheme for encoding location along a linear track. At each scale (<italic>λ</italic><sub><italic>i</italic></sub>) there are two grid cells (red vs blue firing fields). The periodicity and grid field widths are halved at each successive scale. (<bold>C</bold>) The binary scheme in (<bold>B</bold>) is ambiguous if the grid field width at scale <italic>i</italic> exceeds the grid periodicity at scale <italic>i</italic> + 1. For example, if the grid fields marked in red respond at scales <italic>i</italic> and <italic>i</italic> + 1, the animal might be in either of the two marked locations. (<bold>D</bold>) The grid system is composed of discrete modules, each of which contains neurons with periodic tuning curves, and varying phase, in space. (<bold>E</bold>) For a simple winner-take-all decoder of the grids in panel <bold>D</bold>, decoded position will be ambiguous unless <italic>l</italic><sub><italic>i</italic></sub> ≤ <italic>λ</italic><sub><italic>i</italic> + 1</sub>, analogously to panel <bold>C</bold> (see text). Variants of this limitation occur in other decoding schemes.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.08362.003">http://dx.doi.org/10.7554/eLife.08362.003</ext-link></p></caption><graphic xlink:href="elife-08362-fig1-v2.tif"/></fig></p><p>How does the grid system represent spatial location and what function does the modular variation in grid scale serve? Here, we propose that the grid system provides a hierarchical representation of space where fine grids provide precise location and coarse grids resolve ambiguity, and that the grids are organized to minimize the number of neurons required to achieve the behaviorally necessary spatial resolution across a spatial range equal in size to the period of the largest grid module. Our analyses thus assume that there is a behaviorally defined maximum range over which a fixed grid represents locations. Our hypotheses, together with general assumptions about tuning curve shape and decoding mechanism, explain the triangular lattice structure of two-dimensional grid cell firing maps and predict a geometric progression of grid scales. Crucially, the theory further predicts that the ratio of adjacent grid scales will be modestly variable within and between animals with a mean in the range 1.4–1.7 depending on the assumed decoding mechanism used by the brain. With additional assumptions the theory also predicts that the ratio between grid scale and individual grid field widths should lie in the same range. These predictions naturally explain the structural parameters of grid cell modules measured in rodents (<xref ref-type="bibr" rid="bib4">Barry et al., 2007</xref>; <xref ref-type="bibr" rid="bib25">Giocomo et al., 2011a</xref>; <xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>). Our results follow from general principles, and thus, we expect similar organization of the grid system in other species. The theory makes further predictions including: (a) the number of grid scales necessary to support navigation over typical behavioral distances (i.e., a logarithmic relation between number of modules and navigational range), (b) possible deficits in spatial behavior that will obtain upon inactivating specific grid modules, (c) the structure of one- and three-dimensional grids that will be relevant to navigation in, for example, bats (<xref ref-type="bibr" rid="bib61">Yartsev et al., 2011</xref>), (d) an estimate of the number of grid cells we expect in the mEC. Remarkably, in a simple decoding scheme, the scale ratio in an <italic>n</italic>-dimensional environment is predicted to be close to <inline-formula><mml:math id="inf1"><mml:mrow><mml:mroot><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mroot></mml:mrow></mml:math></inline-formula>.</p><p>As we will explain, our results and their apparent experimental confirmation in <xref ref-type="bibr" rid="bib52">Stensola et al. (2012)</xref>, suggest that the grid system implements a two-dimensional neural analog of a base-b number system. This provides an intuitive and powerful metaphor for interpreting the representation of space in the entorhinal cortex.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>The set-up</title><p>The key features of the grid system in the MEC are schematized in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. Grid cells are organized in modules, and cells within a module share a common lattice organization of their firing fields (<xref ref-type="bibr" rid="bib4">Barry et al., 2007</xref>; <xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>). These lattices have periods <italic>λ</italic><sub>1</sub> &gt; <italic>λ</italic><sub>2</sub> &gt;⋯<italic>λ</italic><sub><italic>m</italic></sub>, measured as the distance between nearest neighbor firing fields. It will prove convenient to define ‘scale factors’ <italic>r</italic><sub><italic>i</italic></sub> = <italic>λ</italic><sub><italic>i</italic></sub>/<italic>λ</italic><sub><italic>i</italic>+1</sub> relating the periods of adjacent scales. In each module, the grid firing fields (i.e., the connected spatial regions that evoke firing) are compact (with a diameter denoted <italic>l</italic><sub><italic>i</italic></sub>) after thresholding for activity above the noise level (see, e.g., <xref ref-type="bibr" rid="bib27">Hafting et al., 2005</xref>). Within any module, grid cells have a variety of spatial phases so that at least one cell will respond at any physical location (<xref ref-type="fig" rid="fig1">Figure 1B,D</xref>). Grid modules with smaller field widths <italic>l</italic><sub><italic>i</italic></sub> provide more local spatial information than those with larger scales. However, this increased spatial precision comes at a cost: the correspondingly smaller periodicity <italic>λ</italic><sub><italic>i</italic></sub> of these modules leads to increased ambiguity since there are more grid periods within a given spatial region (e.g., see scale 3 in the schematic one-dimensional grid in <xref ref-type="fig" rid="fig1">Figure 1B,D</xref>). By contrast, modules with large periods and field widths have less spatial precision, but also less ambiguity (e.g., in scale 1 in <xref ref-type="fig" rid="fig1">Figure 1B</xref> the red cell has only one firing field in the environment and hence no ambiguity).</p><p>We propose that the entorhinal cortex exploits this trade-off to implement a hierarchical representation of space where large scales resolve ambiguity and small scales provide precision. Consistently with existing data for one- and two-dimensional grids (<xref ref-type="bibr" rid="bib4">Barry et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Brun et al., 2008</xref>; <xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>), we will take the largest grid period <italic>λ</italic><sub>1</sub> to be comparable to the range over which space is represented unambiguously by a fixed grid without remapping (<xref ref-type="bibr" rid="bib24">Fyhn et al., 2007</xref>). (An alternative view, that the range might greatly exceed the largest period, is addressed in the ‘Discussion’.) The spatial resolution of such a grid can be measured by comparing the range of spatial representation set by the largest period <italic>λ</italic><sub>1</sub> to the precision (related to the smallest grid field width <italic>l</italic><sub><italic>m</italic></sub>) to quantify how many distinct spatial ‘bins’ can be resolved. We will assume that the required resolution is set by the animal's behavioral requirements.</p></sec><sec id="s2-2"><title>Intuitions from a simplified model</title><p>What are the advantages of a multi-scale, hierarchical representation of physical location? Consider an animal living in an 8 m linear track and requiring spatial precision of 1 m to support its behavior. To develop intuition, consider a simple model where location is represented in the animal's brain by reliable neurons with rectangular firing fields (e.g., <xref ref-type="fig" rid="fig1">Figure 1B</xref>). The animal could achieve the required resolution in a <italic>place coding</italic> scheme by having eight neurons tuned to respond when the animal is in 1 m wide, non-overlapping regions (see [<xref ref-type="bibr" rid="bib21">Fiete et al., 2008</xref>] for a related comparison between grid and place cells). Consider an alternative, the idealized <italic>grid coding</italic> scheme in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. Here, the two neurons at the largest scale (<italic>λ</italic><sub>1</sub>) have 4 m wide tuning curves so that their responses just indicate the left and right halves of the track. The pairs of neurons at the next two scales have grid field widths of 2 m and 1 m respectively, and proportionally shorter periodicities as well. These pairs successively localize the animal into 2 m and 1 m bins. All told only six neurons are required, less than in the place coding scheme. This suggests that grid schemes that integrate multiple scales of representation can encode space more efficiently, that is, with fewer neural resources. In the sensory periphery, there is evidence of selection for more efficient circuit architectures (e.g., <xref ref-type="bibr" rid="bib47">Simoncelli and Olshausen, 2001</xref>). If similar selection operates in cortex, the experimentally measured grid architecture should be predicted by maximizing the efficiency of the grid system given a behaviorally determined range and resolution. Thus, we seek to predict the key structural parameters of the grid system—the ratios <italic>r</italic><sub><italic>i</italic></sub> = <italic>λ</italic><sub><italic>i</italic></sub>/<italic>λ</italic><sub><italic>i</italic>+1</sub> relating adjacent scales (which need not be equal).</p><p>The need to avoid spatial ambiguity constrains the ratios <italic>r</italic><sub><italic>i</italic></sub>. Again in our simple model, consider <xref ref-type="fig" rid="fig1">Figure 1C</xref> where the cells with the grid fields marked in red respond at scales <italic>i</italic> and <italic>i</italic> + 1. Then the animal might be in either of the two marked locations. Avoiding ambiguity requires that <italic>λ</italic><sub><italic>i</italic>+1</sub>, the period at scale <italic>i</italic> + 1, must exceed <italic>l</italic><sub><italic>i</italic></sub>, the grid field width at scale <italic>i</italic>. Variants of this condition will recur in the more realistic models that we will consider. Theoretically, one could resolve the ambiguity in <xref ref-type="fig" rid="fig1">Figure 1C</xref> by combining the responses of more grid modules, provided they have mutually incommensurate periods (<xref ref-type="bibr" rid="bib21">Fiete et al., 2008</xref>; <xref ref-type="bibr" rid="bib51">Sreenivasan and Fiete, 2011</xref>). However, anatomical evidence suggests that contiguous subsets of the mEC along the dorso–ventral axis project topographically to the hippocampus (<xref ref-type="bibr" rid="bib59">Van Strien et al., 2009</xref>). While there is evidence that hippocampal place cells are not formed and maintained by grid cell inputs alone (<xref ref-type="bibr" rid="bib11">Bush et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Sasaki et al., 2015</xref>), for each of these restricted projections to represent a well-defined spatial map, ambiguities like the one in <xref ref-type="fig" rid="fig1">Figure 1C</xref> should be resolved at each scale. The hierarchical position encoding schemes that we consider below embody this observation by seeking to reduce position ambiguity at each scale, given the responses at larger scales.</p></sec><sec id="s2-3"><title>Efficient grid coding in one dimension</title><p>How should the grid system be organized to minimize the resources required to represent location unambiguously with a given resolution? Consider a one-dimensional grid system that develops when an animal runs on a linear track. As described above, the <italic>i</italic>th module is characterized by a period <italic>λ</italic><sub><italic>i</italic></sub>, while the ratio of adjacent periods is <italic>r</italic><sub><italic>i</italic></sub> = <italic>λ</italic><sub><italic>i</italic></sub>/<italic>λ</italic><sub><italic>i</italic>+1</sub>. Within any module, grid cells have periodic, bumpy response fields with a variety of spatial phases so that at least one cell responds at any physical location (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). If <italic>d</italic> cells respond above the noise threshold at each point, the number of grid cells <italic>n</italic><sub><italic>i</italic></sub> in module <italic>i</italic> will be <italic>n</italic><sub><italic>i</italic></sub> = <italic>dλ</italic><sub><italic>i</italic></sub>/<italic>l</italic><sub><italic>i</italic></sub>. We will take <italic>d</italic>, the <italic>coverage factor</italic>, to be the same in each module. In terms of these parameters, the total number of grid cells is <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <italic>m</italic> is the number of grid modules. How should such a grid be organized to minimize the number of grid cells required to achieve a given spatial resolution? The answer might depend on how the brain decodes the grid system. Hence, we will consider decoding methods at extremes of decoding complexity and show that they give similar answers for the optimal grid.</p><sec id="s2-3-1"><title>Winner-take-all decoder</title><p>First imagine a decoder which considers the animal as localized within the grid fields of the most responsive cell in each module (<xref ref-type="bibr" rid="bib12">Coultrip et al., 1992</xref>; <xref ref-type="bibr" rid="bib35">Maass, 2000</xref>). A simple ‘winner-take-all’ (WTA) scheme of this kind can be easily implemented by neural circuits where lateral inhibition causes the influence of the most responsive cell to dominate. A maximally conservative decoder ignoring all information from other cells and from the shape of the tuning curve (illustrated in <xref ref-type="fig" rid="fig1">Figure 1E</xref>) could then take uncertainty in spatial location to be equal to <italic>l</italic><sub><italic>i</italic></sub>. The smallest interval that can be resolved in this way will be <italic>l</italic><sub><italic>m</italic></sub>. We therefore quantify the resolution of the grid system (the number of spatial bins that can be resolved) as the ratio of the largest to the smallest scale, <italic>R</italic><sub>1</sub> = <italic>λ</italic><sub>1</sub>/<italic>l</italic><sub><italic>m</italic></sub>, which we assume to be large and fixed by the animal's behavior. In terms of scale factors <italic>r</italic><sub><italic>i</italic></sub> = <italic>λ</italic><sub><italic>i</italic></sub>/<italic>λ</italic><sub><italic>i</italic>+1</sub>, we can write the resolution as <inline-formula><mml:math id="inf5"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where we also defined <italic>r</italic><sub><italic>m</italic></sub> = <italic>λ</italic><sub><italic>m</italic></sub>/<italic>l</italic><sub><italic>m</italic></sub>. As in our simplified model above, unambiguous decoding requires that <italic>l</italic><sub><italic>i</italic></sub> ≤ <italic>λ</italic><sub><italic>i</italic>+1</sub> (<xref ref-type="fig" rid="fig1">Figure 1C,E</xref>), or, equivalently, <inline-formula><mml:math id="inf6"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>≥</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. To minimize <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mtext>  </mml:mtext><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>, all the <inline-formula><mml:math id="inf8"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> should be as small as possible; so this fixes <inline-formula><mml:math id="inf9"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Thus, we are reduced to minimizing the sum <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:mtext> </mml:mtext><mml:mo>∑</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> over the parameters <italic>r</italic><sub><italic>i</italic></sub>, while fixing the product <inline-formula><mml:math id="inf11"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∏</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula>. Because this problem is symmetric under permutation of the indices <italic>i</italic>, the optimal <italic>r</italic><sub><italic>i</italic></sub> turn out to all be equal, allowing us to set <italic>r</italic><sub><italic>i</italic></sub> = <italic>r</italic> (Optimizing the grid system: winner-take-all decoder, ‘Materials and methods’). This is our first prediction: (1) the ratios between adjacent periods will be constant. The constraint on resolution then gives <italic>m</italic> = log<sub><italic>r</italic></sub><italic>R</italic><sub>1</sub>, so that we seek to minimize <italic>N</italic> (<italic>r</italic>) = <italic>d r</italic> log<sub><italic>r</italic></sub> <italic>R</italic><sub>1</sub> with respect to <italic>r</italic>: the solution is <italic>r</italic> = <italic>e</italic> (Optimizing the grid system: winner-take-all decoder, ‘Materials and methods’, and panel B of Figure 5 in Optimizing the grid system: probabilistic decoder, ‘Materials and methods’). This gives a second prediction: (2) the ratio of adjacent grid periods should be close to <italic>r</italic> = <italic>e</italic>. Therefore, for each scale <italic>i</italic>, <italic>λ</italic><sub><italic>i</italic></sub> = <italic>e λ</italic><sub><italic>i</italic> + 1</sub> and <italic>λ</italic><sub><italic>i</italic></sub> = <italic>el</italic><sub><italic>i</italic></sub>. This gives a third prediction: (3) the ratio of the grid period and the grid field width will be constant across modules and be close to the scale ratio.</p><p>More generally, in winner-take-all decoding schemes, the local uncertainty in the animal's location in grid module <italic>i</italic> will be proportional to the grid field width <italic>l</italic><sub><italic>i</italic></sub>. The proportionality constant will be a function <italic>f</italic>(<italic>d</italic>) of the coverage factor <italic>d</italic> that depends on the tuning curve shape and neural variability. Thus, the uncertainty will be <italic>f</italic>(<italic>d</italic>)<italic>l</italic><sub><italic>i</italic></sub>. Unambiguous decoding at each scale requires that <italic>λ</italic><sub><italic>i</italic> + 1</sub> ≥ <italic>f</italic>(<italic>d</italic>)<italic>l</italic><sub><italic>i</italic></sub>. The smallest interval that can be resolved in this way will be <italic>f</italic>(<italic>d</italic>)<italic>l</italic><sub><italic>m</italic></sub>, and this sets the positional accuracy of the decoding scheme. Finally, we require that <italic>λ</italic><sub>1</sub> &gt; <italic>L</italic>, where <italic>L</italic> is a scale big enough to ensure that the grid code resolves positions over a sufficiently large range. Behavioral requirements fix the required positional accuracy and range. The optimal grid satisfying these constraints is derived in Optimizing the grid system: winner-take-all decoder, ‘Materials and methods’. Again, the adjacent modules are organized in a geometric progression and the ratio between adjacent periods is predicted to be <italic>e</italic>. However, the ratio between the grid period and grid field width in each module depends on the specific model through the function <italic>f</italic>(<italic>d</italic>). Thus, within winner-take-all decoding schemes, the constancy of the scale ratio, the value of the scale ratio, and the constancy of the ratio of grid period to field width are parameter-free predictions, and therefore furnish tests of theory. If the tests succeed, <italic>f</italic>(<italic>d</italic>) can be matched to data to constrain possible mechanisms used by the brain to decode the grid system.</p></sec><sec id="s2-3-2"><title>Probabilistic decoder</title><p>What do we predict for a more general, and more complex, decoding scheme that optimally pools all the information available in the responses of noisy neurons within and between modules? Statistically, the best we can do is to use all these responses, which may individually be noisy, to find a probability distribution over physical locations that can then inform subsequent behavioral decisions (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Thus, the population response at each scale <italic>i</italic> gives rise to a likelihood function over location <italic>P</italic>(<italic>x</italic>|<italic>i</italic>), which will have the same periodicity <italic>λ</italic><sub><italic>i</italic></sub> as the individual grid cells' firing rates (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). This likelihood explicitly captures the uncertainty in location given the tuning and noise characteristics of the neural population in the module <italic>i</italic>. Because there are at least scores of neurons in each grid module (<xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>) <italic>P</italic>(<italic>x</italic>|<italic>i</italic>) can be approximated as a periodic sum of Gaussians without making restrictive assumptions about the shapes of the tuning curves of individual grid cells, or about the precision of their periodicity, so long as the variability of individual neurons is weakly correlated and homogeneous. For example, even though individual grid cells can have somewhat different firing rates in each of their firing fields, this spatial heterogeneity will be smoothed in the posterior over the full population of cells, leading to much more accurate periodicity. In other words, individual grid cells show both spiking noise and ‘noise’ due to heterogeneity and imperfect periodicity of the firing rate maps. Both these forms of variability are smoothed by averaging over the population, provided, as we will assume, that there are enough cells and noise is not too correlated between cells.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.08362.005</object-id><label>Figure 2.</label><caption><title>Trade-off between precision and ambiguity in the probabilistic decoder.</title><p>(<bold>A</bold>) The probability of position <italic>x</italic> given the responses of all grid cells at scales larger than module <italic>i</italic> is described by the distribution <italic>Q</italic><sub><italic>i</italic>−1</sub>(<italic>x</italic>) (black curve), and the uncertainty in position is given by the standard deviation <italic>δ</italic><sub><italic>i</italic>−1</sub>. The probability of position given just the responses in module <italic>i</italic> will be a periodic function <italic>P</italic><sub><italic>i</italic></sub>(<italic>x</italic>) (green curve). (<bold>B</bold>) The probability distribution over position <italic>x</italic> after combining module <italic>i</italic> with all larger scales is <italic>Q</italic><sub><italic>i</italic></sub>(<italic>x</italic>) ∼ <italic>P</italic><sub><italic>i</italic></sub>(<italic>x</italic>)<italic>Q</italic><sub><italic>i</italic>−1</sub>(<italic>x</italic>) and has reduced uncertainty <italic>δ</italic><sub><italic>i</italic></sub>. (<bold>C</bold>) Precision can be improved by increasing the scale factor, thereby narrowing the peaks of <italic>P</italic><sub><italic>i</italic></sub>(<italic>x</italic>). However, the periodicity shrinks as well, increasing ambiguity. (<bold>D</bold>) The distribution over position <italic>Q</italic><sub><italic>i</italic></sub>(<italic>x</italic>) from combining the modules shown in <bold>C</bold>. Ambiguity from the secondary peaks leads to an overall uncertainty <italic>δ</italic><sub><italic>i</italic></sub> larger than in <bold>B</bold>, despite the improved precision from the narrower central peak.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.08362.005">http://dx.doi.org/10.7554/eLife.08362.005</ext-link></p></caption><graphic xlink:href="elife-08362-fig2-v2.tif"/></fig></p><p>The standard deviations of the peaks in <italic>P</italic>(<italic>x</italic>|<italic>i</italic>), which we call <italic>σ</italic><sub><italic>i</italic></sub>, depend on the tuning curve shape and response noise of individual grid cells, and will decrease as the coverage factor <italic>d</italic> increases. To have even coverage of space, the number of grid phases, and thus grid cells in a module, must be uniformly distributed so that equally reliable posterior distributions can be formed at each point in the unit cell of the module response. This requires that the number of cells (and phases) in the module should be proportional to the ratio <inline-formula><mml:math id="inf20"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Summing over modules, the total number of grid cells will be <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>N</mml:mi><mml:mo>∝</mml:mo><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. The composite posterior given all <italic>m</italic> scales and a uniform prior over positions, <italic>Q</italic><sub><italic>m</italic></sub>(<italic>x</italic>), will be given by the product <inline-formula><mml:math id="inf22"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:msubsup><mml:mtext>Π</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, assuming independent response noise across scales (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The animal's overall uncertainty about its position depends on the standard deviation <italic>δ</italic><sub><italic>m</italic></sub> of the composite posterior distribution <italic>Q</italic><sub><italic>m</italic></sub>(<italic>x</italic>). Setting <italic>δ</italic><sub>0</sub> to be the uncertainty in location without using any grid responses at all, we can quantify resolution as <italic>R</italic> = <italic>δ</italic><sub>0</sub>/<italic>δ</italic><sub><italic>m</italic></sub>.</p><p>In this framework, there is a precision-ambiguity trade-off controlled by the scale factors <italic>r</italic><sub><italic>i</italic></sub>. The larger these ratios, the more rapidly grid field widths shrink in successive modules, thus increasing precision and reducing the number of modules, and hence grid cells, required to achieve a given resolution. However, if the periods of adjacent scales shrink too quickly, the composite posterior <italic>Q</italic><sub><italic>i</italic></sub>(<italic>x</italic>) will develop prominent side-lobes (<xref ref-type="fig" rid="fig2">Figure 2C,D</xref>) making decoding ambiguous, as reflected in a large standard deviation <italic>δ</italic><sub><italic>i</italic></sub> of the composite posterior distribution (<xref ref-type="fig" rid="fig2">Figure 2B,D</xref>). This ambiguity could be avoided by shrinking the width of <italic>Q</italic><sub><italic>i</italic>−1</sub>(<italic>x</italic>)—however, this would require increasing the number of neurons <italic>n</italic><sub>1</sub>,⋯<italic>n</italic><sub><italic>i</italic>−1</sub> in the modules 1,⋯<italic>i</italic> − 1. Ambiguity can also be avoided by having a smaller scale ratio (so that the side lobes of the posterior <italic>P</italic>(<italic>x</italic>|<italic>i</italic>) of module <italic>i</italic> do not penetrate the central lobe of the composite posterior <italic>Q</italic><sub><italic>i</italic>−1</sub>(<italic>x</italic>) of modules 1,⋯<italic>i</italic>−1. But reducing the scale ratios to reduce ambiguity increases the number of modules necessary to achieve the required resolution, and hence increases the number of grid cells. This sets up a trade-off—increasing the scale ratios reduces the number of modules to achieve a fixed resolution but requires more neurons in each module; reducing the scale ratios permits the use of fewer grid cells in each module, but increases the number of required modules. Optimizing this trade-off (analytical and numerical details in 'Materials and methods' and Figure 5) predicts: (1) a constant scale ratio between the periods of each grid module, and (2) an optimal ratio ≈2.3, slightly smaller than, but close to the winner-take-all value, <italic>e</italic>.</p><p>Why is the predicted scale factor based on the probabilistic decoder somewhat smaller than the prediction based on the winner-take-all analysis? In the probabilistic analysis, when the likelihood is combined across modules, there will be side lobes arising from the periodic peaks of the likelihood derived from module <italic>i</italic> multiplying the tails of the Gaussian arising from the previous modules. These side lobes increase location ambiguity (measured by the standard deviation <italic>δ</italic><sub><italic>i</italic></sub> of the overall likelihood). Reducing the scale factor reduces the height of side lobes because the secondary peaks from module <italic>i</italic> move further into the tails of the Gaussian derived from the previous modules. Thus, conceptually, the optimal probabilistic scale factor is smaller than the winner-take-all case in order to suppress side lobes that arise in the combined likelihood across modules (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Such side lobes were absent in the winner-take-all analysis, which thus permits a more aggressive (larger) scale ratio that improves precision, without being penalized by increased ambiguity. The theory also predicts a fixed ratio between grid period <italic>λ</italic><sub><italic>i</italic></sub> and posterior likelihood width <italic>σ</italic><sub><italic>i</italic></sub>. However, the relationship between <italic>σ</italic><sub><italic>i</italic></sub> and the more readily measurable grid field width <italic>l</italic><sub><italic>i</italic></sub> depends on a variety of parameters including the tuning curve shape, noise level, and neuron density.</p></sec></sec><sec id="s2-4"><title>General grid coding in two dimensions</title><p>How do these results extend to two dimensions? Let <italic>λ</italic><sub><italic>i</italic></sub> be the distance between nearest neighbor peaks of grid fields of width <italic>l</italic><sub><italic>i</italic></sub> (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Assume in addition that a given cell responds on a lattice whose vertices are located at the points <italic>λ</italic><sub><italic>i</italic></sub> (<italic>n</italic><bold>u</bold> + <italic>m</italic><bold>v</bold>), where <italic>n</italic>, <italic>m</italic> are integers and <bold>u</bold>, <bold>v</bold> are linearly independent vectors generating the lattice (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). We may take <bold>u</bold> to have unit length (|<bold>u</bold>| = 1) without loss of generality, however |<bold>v</bold>| ≠ 1 in general. It will prove convenient to denote the components of <bold>v</bold> parallel and perpendicular to <bold>u</bold> by <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>∥</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <italic>v</italic><sub>⊥</sub>, respectively (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The two numbers <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>∥</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>v</mml:mi><mml:mo>⊥</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> quantify the geometry of the grid and are additional parameters that we may optimize over: this is a primary difference from the one-dimensional case. We will assume that <inline-formula><mml:math id="inf25"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>∥</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <italic>v</italic><sub>⊥</sub> are independent of scale; this still allows for relative rotation between grids at different scales. At each scale, grid cells have different phases so that at least one cell responds at each physical location. The minimal number of phases required to cover space is computed by dividing the area of the unit cell of the grid (<inline-formula><mml:math id="inf26"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mo>⊥</mml:mo></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula>) by the area of the grid field. As in the one-dimensional case, we define a coverage factor <italic>d</italic> as the number of neurons covering each point in space, giving for the total number of neurons <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mo>⊥</mml:mo></mml:msub><mml:mo>|</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>.<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.08362.006</object-id><label>Figure 3.</label><caption><title>Optimizing two-dimensional grids.</title><p>(<bold>A</bold>) A general two-dimensional lattice is parameterized by two vectors <bold>u</bold> and <bold>v</bold> and a periodicity parameter <italic>λ</italic><sub><italic>i</italic></sub>. Take <bold>u</bold> to be a unit vector, so that the spacing between peaks along the <bold>u</bold> direction is <italic>λ</italic><sub><italic>i</italic></sub>, and denote the two components of <bold>v</bold> by <inline-formula><mml:math id="inf28"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>∥</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, <italic>v</italic><sub>⊥</sub>. The blue-bordered region is a fundamental domain of the lattice, the largest spatial region that can be unambiguously represented. (<bold>B</bold>) The two-dimensional analog of the ambiguity in <xref ref-type="fig" rid="fig1">Figure 1C,E</xref> for the winner-take-all decoder. If the grid fields in scale <italic>i</italic> are too close to each other relative to the size of the grid field of scale <italic>i</italic> − 1 (i.e., <italic>l</italic><sub><italic>i</italic> − 1</sub>), the animal might be in one of several locations. (<bold>C</bold>) The optimal ratio <italic>r</italic> between adjacent scales in a hierarchical grid system in two dimensions for a winner-take-all decoding model (blue curve, WTA) and a probabilistic decoder (red curve). <italic>N</italic><sub><italic>r</italic></sub> is the number of neurons required to represent space with resolution <italic>R</italic> given a scaling ratio <italic>r</italic>, and <italic>N</italic><sub>min</sub> is the number of neurons required at the optimum. In both decoding models, the ratio <italic>N</italic><sub><italic>r</italic></sub>/<italic>N</italic><sub>min</sub> is independent of resolution, <italic>R</italic>. For the winner-take-all model, <italic>N</italic><sub><italic>r</italic></sub> is derived analytically, while the curve for the probabilistic model is derived numerically (details in Optimizing the grid system: winner-take-all decoder and Optimizing the grid system: probabilistic decoder, ‘Materials and methods’). The winner-take-all model predicts <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mi>e</mml:mi></mml:msqrt><mml:mo>≈</mml:mo><mml:mn>1.65</mml:mn></mml:mrow></mml:math></inline-formula>, while the probabilistic decoder predicts <italic>r</italic> ≈ 1.44. The minima of the two curves lie within each others' shallow basins, predicting that some variability of adjacent scale ratios is tolerable within and between animals. The green and blue bars represent a standard deviation of the scale ratios of the period ratios between modules measured in <xref ref-type="bibr" rid="bib4">Barry et al. (2007)</xref>; <xref ref-type="bibr" rid="bib52">Stensola et al. (2012)</xref>. (<bold>D</bold>) Contour plot of normalized neuron number <italic>N</italic>/<italic>N</italic><sub>min</sub> in the probabilistic decoder, as a function of the grid geometry parameters <inline-formula><mml:math id="inf30"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>⊥</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mo>∥</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> after minimizing over the scale factors for fixed resolution <italic>R</italic>. As in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, the normalized neuron number is independent of <italic>R</italic>. The spacing between contours is 0.01, and the asterisk labels the minimum at <inline-formula><mml:math id="inf31"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>∥</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mtext>  </mml:mtext><mml:msub><mml:mi>v</mml:mi><mml:mo>⊥</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>; this corresponds to the triangular lattice.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.08362.006">http://dx.doi.org/10.7554/eLife.08362.006</ext-link></p></caption><graphic xlink:href="elife-08362-fig3-v2.tif"/></fig></p><p>As before, consider a situation where grid fields thresholded for noise lie completely within compact regions and assume a simple decoder which selects the most activated cell and does not take tuning curve shape into account (<xref ref-type="bibr" rid="bib12">Coultrip et al., 1992</xref>; <xref ref-type="bibr" rid="bib35">Maass, 2000</xref>; <xref ref-type="bibr" rid="bib1">de Almeida et al., 2009</xref>). In such a model, each scale <italic>i</italic> simply serves to localize the animal within a circle of diameter <italic>l</italic><sub><italic>i</italic></sub>. The spatial resolution is summarized by the square of the ratio of the largest scale <italic>λ</italic><sub>1</sub> to the smallest scale <italic>l</italic><sub><italic>m</italic></sub>: <italic>R</italic><sub>2</sub> = (<italic>λ</italic><sub>1</sub>/<italic>l</italic><sub><italic>m</italic></sub>)<sup>2</sup>. In terms of the scale factors <inline-formula><mml:math id="inf32"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, we write <inline-formula><mml:math id="inf33"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, where we also define <inline-formula><mml:math id="inf34"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. To decode the position of an animal unambiguously, each cell at scale <italic>i</italic> + 1 should have at most one grid field within a region of diameter <italic>l</italic><sub><italic>i</italic></sub>. We therefore require that the shortest lattice vector of the grid at scale <italic>i</italic> has a length greater than <italic>l</italic><sub><italic>i</italic> − 1</sub>, in order to avoid ambiguity (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). We wish to minimize <italic>N</italic>, which will be convenient to express as <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mo>⊥</mml:mo></mml:msub><mml:mo>|</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>. There are two kinds of contributions here to the number of neurons—the factors <inline-formula><mml:math id="inf36"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> are constrained by the overall resolution of the grid, while, as we will see, the combination |<italic>v</italic><sub>⊥</sub>|(<italic>λ</italic><sub><italic>i</italic> + 1</sub>/<italic>l</italic><sub><italic>i</italic></sub>)<sup>2</sup> measures a packing density of discs placed on the grid lattice. This suggests that we should separate the minimization of neuron number into first optimizing the lattice and then optimizing ratios. After doing so, we can check that the result is the global optimum.</p><p>To obtain the optimal lattice geometry, we can ignore the resolution constraint, as it depends only on the scale factors and not the grid geometry. We may then exploit an equivalence between our optimization problem and the optimal circle-packing problem. To see this connection, consider placing disks of diameter <italic>l</italic><sub><italic>i</italic></sub> on each vertex of the grid at scale <italic>i</italic> + 1. In order to avoid ambiguity, all points of the grid <italic>i</italic> + 1 must be separated by at least <italic>l</italic><sub><italic>i</italic></sub>: equivalently, the disks must not overlap. The density of disks is proportional to <inline-formula><mml:math id="inf37"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mo>⊥</mml:mo></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which is proportional to the reciprocal of each term in <italic>N</italic>. Therefore, <italic>minimizing</italic> neuron number amounts to <italic>maximizing</italic> the packing density; and the no-ambiguity constraint requires that the disks do not overlap. This is the optimal circle packing problem, and its solution in two dimensions is known to be the triangular lattice (<xref ref-type="bibr" rid="bib56">Thue, 1892</xref>), so <inline-formula><mml:math id="inf38"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>∥</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf39"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>⊥</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. Furthermore, the grid spacing should be as small as allowed by the no-ambiguity constraint, giving <inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>We have now reduced the problem to minimizing <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext> </mml:mtext><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>, over the scale factors <inline-formula><mml:math id="inf42"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, while fixing the resolution <italic>R</italic><sub>2</sub>. This optimization problem is mathematically the same as in one dimension if we formally set <inline-formula><mml:math id="inf43"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≡</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. This gives the optimal ratio <inline-formula><mml:math id="inf44"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> for all <italic>i</italic> (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). We conclude that in two dimensions, the optimal ratio of neighboring grid periodicities is <inline-formula><mml:math id="inf45"><mml:mrow><mml:msqrt><mml:mi>e</mml:mi></mml:msqrt><mml:mo>≈</mml:mo><mml:mn>1.65</mml:mn></mml:mrow></mml:math></inline-formula> for the simple winner-take-all decoding model, and the optimal lattice is triangular.</p><p>The optimal probabilistic decoding model from above can also be extended to two dimensions with the posterior distributions <italic>P</italic>(<italic>x</italic>|<italic>i</italic>) becoming sums of Gaussians with peaks on the two-dimensional lattice. In analogy with the one-dimensional case, we then derive a formula for the resolution <italic>R</italic><sub>2</sub> = <italic>λ</italic><sub>1</sub>/<italic>δ</italic><sub><italic>m</italic></sub> in terms of the standard deviation <italic>δ</italic><sub><italic>m</italic></sub> of the posterior given all scales. The quantity <italic>δ</italic><sub><italic>m</italic></sub> may be explicitly calculated as a function of the scale factors <inline-formula><mml:math id="inf46"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the geometric factors <inline-formula><mml:math id="inf47"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>∥</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>v</mml:mi><mml:mo>⊥</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>, and the minimization of neuron number may then be carried out numerically (Optimizing the grid system: probabilistic decoder, ‘Materials and methods’). In this approach, the optimal scale factor turns out to be <inline-formula><mml:math id="inf48"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mn>1.44</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), and the optimal lattice is again triangular (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). Attractor network models of grid formation readily produce triangular lattices (<xref ref-type="bibr" rid="bib10">Burak and Fiete, 2009</xref>); our analysis suggests that this architecture is functionally beneficial in reducing the required number of neurons.</p><p>Even though our two decoding strategies lie at extremes of complexity (one relying just on the most active cell at each scale and another optimally pooling information in the grid population) their respective ‘optimal intervals’ substantially overlap (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; see Figure 5B in 'Materials and methods' for the one-dimensional case). This indicates that our proposal is robust to variations in grid field shape and to the precise decoding algorithm (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). The scaling ratio <italic>r</italic> may lie anywhere within a basin around the optimum at the cost of a small number of additional neurons. Such considerations also suggest that these coding schemes have the capacity to tolerate developmental noise: different animals could develop grid systems with slightly different scaling ratios, without suffering a large loss in efficiency. In two dimensions, the required neuron number will be no more than 5% of the minimum if the scale factor is within the range (1.43, 1.96) for the winner-take-all model and the range (1.28, 1.66) for the probabilistic model. These ‘optimal intervals’ are narrower than in the one-dimensional case and have substantial overlap.</p><p>In summary, for 2-d case, the theory predicts that (1) the ratios between adjacent scales should be a constant; (2) the optimal scaling constant is <inline-formula><mml:math id="inf49"><mml:mrow><mml:msqrt><mml:mi>e</mml:mi></mml:msqrt><mml:mo>≈</mml:mo><mml:mn>1.65</mml:mn></mml:mrow></mml:math></inline-formula> in a simple WTA decoding model, and it is ≈1.44 in a probabilistic decoding model; (3) the predictions for the optimal grid field width depends on the specific decoding method, (4) The grid lattice should be a triangular lattice.</p></sec><sec id="s2-5"><title>Comparison to experiment</title><p>Our predictions agree with experiment (<xref ref-type="bibr" rid="bib4">Barry et al., 2007</xref>; <xref ref-type="bibr" rid="bib25">Giocomo et al., 2011a</xref>; <xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>) (see Reanalysis of grid data from previous studies, ‘Materials and methods’ for details of the data re-analysis). Specifically, <xref ref-type="bibr" rid="bib4">Barry et al. (2007)</xref> (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) reported the grid periodicities measured at three locations along the dorso–ventral axis of the MEC in rats and found ratios of ∼1, ∼1.7 and ∼2.5 ≈ 1.6 × 1.6 relative to the smallest period (<xref ref-type="bibr" rid="bib4">Barry et al., 2007</xref>). The ratios of adjacent scales reported in <xref ref-type="bibr" rid="bib4">Barry et al. (2007)</xref> had a mean of 1.64 ± 0.09 (mean ± std. dev., <italic>n</italic> = 6), which almost precisely matches the mean scale factor of <inline-formula><mml:math id="inf50"><mml:mrow><mml:msqrt><mml:mi>e</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> predicted from the winner-take-all decoding model, and is also consistent with the probabilistic decoding model. In another study (<xref ref-type="bibr" rid="bib31">Krupic et al., 2012</xref>), the scale ratio between the two smaller grid scales, measured by the ratio between the grid frequencies, is reported to be ∼1.57 in one animal. Recent analysis based on a larger data set (<xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>) confirms the geometric progression of the grid scales in individual animals over four modules. The mean ratio between adjacent scales is 1.42 ± 0.17 (mean ± std. dev., <italic>n</italic> = 24) in that data set, accompanied by modest variability within and between animals. These measurements again match both our models (<xref ref-type="fig" rid="fig4">Figure 4A</xref>).<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.08362.007</object-id><label>Figure 4.</label><caption><title>Comparison with experiment.</title><p>(<bold>A</bold>) Our models predict grid scaling ratios that are consistent with experiment. ‘WTA’ (winner-take-all) and ‘probabilistic’ represent predictions from two decoding models; the dot is the scaling ratio minimizing the number of neurons, and the bars represent the interval within which the neuron number will be no more than 5% higher than the minimum. For the experimental data, the dot represents the mean measured scale ratio, and the error bars represent ± one standard deviation. Data were replotted from <xref ref-type="bibr" rid="bib4">Barry et al. (2007)</xref>; <xref ref-type="bibr" rid="bib52">Stensola et al. (2012)</xref>. The dashed red line shows a consensus value running through the two theoretical predictions and the two experimental datasets. (<bold>B</bold>) The mean ratio between grid periodicity (<italic>λ</italic><sub><italic>i</italic></sub>) and the diameter of grid fields (<italic>l</italic><sub><italic>i</italic></sub>) in mice (data from <xref ref-type="bibr" rid="bib25">Giocomo et al., 2011a</xref>). Error bars indicate ± one S.E.M. For both wild-type mice and HCN knockouts (which have larger grid periodicities), the ratio is consistent with <inline-formula><mml:math id="inf51"><mml:mrow><mml:msqrt><mml:mi>e</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> (dashed red line). (<bold>C</bold>) The response lattice of grid cells in rats forms an equilateral triangular lattice with 60° angles between adjacent lattice edges (replotted from <xref ref-type="bibr" rid="bib27">Hafting et al., 2005</xref>, <italic>n</italic> = 45 neurons from six rats). Dots represent outliers, as reported in <xref ref-type="bibr" rid="bib27">Hafting et al. (2005)</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.08362.007">http://dx.doi.org/10.7554/eLife.08362.007</ext-link></p></caption><graphic xlink:href="elife-08362-fig4-v2.tif"/></fig></p><p>The optimal grid was triangular in both of our models, again matching measurements (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) (<xref ref-type="bibr" rid="bib27">Hafting et al., 2005</xref>; <xref ref-type="bibr" rid="bib39">Moser et al., 2008</xref>; <xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>). However, the minimum in <xref ref-type="fig" rid="fig3">Figure 3D</xref> is relatively shallow—the contour lines indicating equally efficient grids are widely spaced near the minimum. This leads us hypothesize that the measured grid geometries will be modestly variable around the triangular lattice, as reported in <xref ref-type="bibr" rid="bib52">Stensola et al. (2012)</xref>.</p><p>A recent study measured the ratio between grid periodicity and grid field size to be 1.63 ± 0.035 (mean ± S.E.M., <italic>n</italic> = 48) in wild-type mice (<xref ref-type="bibr" rid="bib25">Giocomo et al., 2011a</xref>). This ratio was unchanged, 1.66 ± 0.03 (mean ± S.E.M., <italic>n</italic> = 86), in HCN1 knockout strains whose absolute grid periodicities increased relative to the wild type (<xref ref-type="bibr" rid="bib25">Giocomo et al., 2011a</xref>). Such measurements are consistent with the prediction of the simple winner-take-all model, which predicts a ratio between grid period and grid field width of <inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mi>e</mml:mi></mml:msqrt><mml:mo>≈</mml:mo><mml:mn>1.65</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have shown that a grid system with a discrete set of periodicities, as found in the entorhinal cortex, should use a common scale factor <italic>r</italic> between modules to represent spatial location with the fewest neurons. In other words, the periods of grid modules should be organized in a geometric progression. In one dimension, this organization may be thought of intuitively as implementing a neural analog of a base-<italic>b</italic> number system. Roughly, the largest scale localizes the animal into a coarse region of the environment and finer scales successively subdivide the region into <italic>b</italic> ‘bins’. For example, suppose that the largest scale has one firing field in the environment and that <italic>b</italic> = 2, so that subsequent scales subdivide this firing field into halves (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Then, keeping track of which half the animal occupies at each scale gives a binary encoding of location. This is just like a binary number system being used to encode a number representing the location. Our problem of minimizing neuron number while fixing resolution is analogous to minimizing the product of the number of digits and the number of decimal places (which we can term <italic>complexity</italic>) needed to represent a given range <italic>R</italic> of integers in a base-<italic>b</italic> number system. The complexity is approximately <italic>C</italic> ∼ <italic>b</italic> log<sub><italic>b</italic></sub> <italic>R</italic>. What ‘base’ minimizes the complexity of the representation? We can compute this by evaluating the extremum <inline-formula><mml:math id="inf53"><mml:mrow><mml:mo>∂</mml:mo><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and find that the optimum is at <italic>b</italic> = <italic>e</italic> (details in Optimizing a ‘base-b’ representation of one-dimensional space, ‘Materials and methods’). Our full theory is a generalization of this simple fixed-base representational scheme for numbers to noisy neurons encoding two-dimensional location. It is remarkable that natural selection seems to have reached such efficient solutions for encoding location.</p><p>Our theory quantitatively predicted the ratios of adjacent scales within the variability tolerated by the models and by the data (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Further tests of our theory are possible. For example, a direct generalization of our reasoning says that in n-dimensions the optimal ratio between grid scales for winner-take-all decoding is <inline-formula><mml:math id="inf54"><mml:mrow><mml:mroot><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mroot></mml:mrow></mml:math></inline-formula> (as compared to <inline-formula><mml:math id="inf55"><mml:mrow><mml:msqrt><mml:mi>e</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> in two dimensions). The three-dimensional case is possibly relevant to the grid system in, for example, bats (<xref ref-type="bibr" rid="bib61">Yartsev et al., 2011</xref>; <xref ref-type="bibr" rid="bib60">Yartsev and Ulanovsky, 2013</xref>). Robustly, for any given decoding scheme, our theory would predict a smaller scaling ratio for 3d grids than for 2d grids. The packing density argument given above for two-dimensional lattice structure, when generalized to three dimensions, would predict a face center cubic lattice or hexagonal close packing, which share the highest packing density. Bats are known to have 2d grids when crawling on surfaces (<xref ref-type="bibr" rid="bib61">Yartsev et al., 2011</xref>) and if they also have a 3d grid system when flying, similar to their place cell system (<xref ref-type="bibr" rid="bib60">Yartsev and Ulanovsky, 2013</xref>), our predictions for three-dimensional grids can be directly tested. In general, the theory can be tested by comprehensive population recordings of grid cells along the dorso–ventral axis for animals moving in one-, two-, and three-dimensional environments.</p><p>Our theory also predicts a logarithmic relationship between the natural behavioral range and the number of grid modules. To estimate the number of modules, <italic>m</italic>, required for a given resolution <italic>R</italic><sub>2</sub> via the approximate relationship <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mtext>log</mml:mtext><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:mtext>log</mml:mtext><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. Assuming that the animal must be able to represent an environment of area ∼(10 m)<sup>2</sup> (e.g., <xref ref-type="bibr" rid="bib13">Davis et al., 1948</xref>), with a positional accuracy on the scale of the rat's body size, ∼(10 cm)<sup>2</sup>, we get a resolution of <italic>R</italic><sub>2</sub> ∼ 10<sup>4</sup>. Together with the predicted two-dimensional scale factor <inline-formula><mml:math id="inf57"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, this gives <italic>m</italic> ≈ 10 as an order-of-magnitude estimate. Indeed, in <xref ref-type="bibr" rid="bib52">Stensola et al. (2012)</xref>, 4–5 modules were discovered in recordings spanning up to 50% of the dorsoventral extent of MEC; extrapolation gives a total module number consistent with our estimate.</p><p>How many grid cells do we predict in total? Consider the simplest case where grid cells are independent encoders of position in two dimensions. Our likelihood analysis (details in Optimizing the grid system: probabilistic decoder, ‘Materials and methods’) gives the number of neurons as <italic>N</italic> = <italic>mc</italic>(<italic>λ</italic>/<italic>σ</italic>)<sup>2</sup>, where <italic>m</italic> is the number of modules and <italic>c</italic> is constant. In detail, <italic>c</italic> is determined by factors like the tuning curve shape of individual neurons and their firing rates, but broadly what matters is the typical number of spikes <italic>K</italic> that a neuron emits during a sampling time, because this will control the precision with which location can be inferred from a single cell's response. General considerations (<xref ref-type="bibr" rid="bib14">Dayan and Abbott, 2001</xref>) indicate that <italic>c</italic> will be proportional to 1/<italic>K</italic>. We can estimate that if a rat runs at ∼50 cm/s and covers ∼1 cm in a sampling time, then a grid cell firing at 10 Hz (<xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>) gives <italic>K</italic> ∼ 1/5. Using our prediction that the number of modules will be ∼10 and that <italic>λ</italic>/<italic>σ</italic> ≈ 5.3 in the optimal grid (see Optimizing the grid system: probabilistic decoder, ‘Materials and methods’), we get <italic>N</italic><sub>est</sub> ≈ 1400. This estimate assumed independent neurons and that the decoder of the grid system will efficiently use all the information in every grid cell's response. This is unlikely to be the case. Given homogeneous noise correlations within a grid module, which will arise naturally if grid cells are formed by an attractor mechanism, the required number of neurons could be an order of magnitude higher (<xref ref-type="bibr" rid="bib50">Sompolinsky et al., 2001</xref>; <xref ref-type="bibr" rid="bib2">Averbeck et al., 2006</xref>). (Noise correlation between grid cells was investigated in <xref ref-type="bibr" rid="bib38">Mathis et al. (2013)</xref>; <xref ref-type="bibr" rid="bib19">Dunn et al. (2015)</xref>—they found positive correlation between aligned grids of similar periods and some evidence for weak negative correlation for grids differing in phase.) Thus, in round numbers, we estimate that our theory requires something in the range of ∼1400–14000 grid cells.</p><p>Are there so many grid cells in the MEC? In fact, we need this number of grid cells separately in layer II and layer III of the MEC since these regions likely maintain separate grid codes. (To see this, recall that layers II and III project largely to the dentate gyrus and CA1, respectively [<xref ref-type="bibr" rid="bib53">Steward and Scoville, 1976</xref>; <xref ref-type="bibr" rid="bib18">Dolorfo and Amaral, 1998</xref>], while the place map in CA1 survives lesions of the dentate input to CA1 via CA3 [<xref ref-type="bibr" rid="bib8">Brun et al., 2002</xref>].) Physiological studies (<xref ref-type="bibr" rid="bib45">Sargolini et al., 2006</xref>) have shown that only about 10% of the cells in MEC are layer II grid cells and another 10% are layer III grid cells. Cells that have weak responsiveness during spatial tasks are probably undersampled in such experiments and so the real proportion of grid cells is likely to be somewhat smaller. Other studies (<xref ref-type="bibr" rid="bib40">Mulders et al., 1997</xref>) have shown that MEC has ∼10<sup>5</sup> neurons. Thus, we can estimate that layer II and layer III each contain something in the range of 5000–10000 grid cells. This is well within the predicted theoretical range.</p><p>Our analysis assumed that the grid code is hierarchical, with large grids resolving the spatial ambiguity created by the multiple firing fields of the small grids that deliver precision of location. Recall that place cells are thought to provide one readout of the grid system. Anatomical evidence (<xref ref-type="bibr" rid="bib59">Van Strien et al., 2009</xref>) shows that the projections from the mEC to the hippocampus are restricted along the dorso-ventral axis, so that a given place cell receives input from perhaps a quarter of the mEC. The data of <xref ref-type="bibr" rid="bib52">Stensola et al. (2012)</xref> show additionally that the dorsal mEC is impoverished in large grid modules. If place cells were formed from grids via summation as in the model of (<xref ref-type="bibr" rid="bib49">Solstad et al., 2006</xref>), the anatomy (<xref ref-type="bibr" rid="bib59">Van Strien et al., 2009</xref>) and the hierarchical view of location coding that we have proposed would together predict that dorsal place cells should be revealed to have multiple place fields in large environments because their spatial ambiguities will not be fully resolved at larger scales. Preliminary evidence for such a multiplicity of dorsal place fields appears in <xref ref-type="bibr" rid="bib20">Fenton et al. (2008)</xref>; <xref ref-type="bibr" rid="bib44">Rich et al. (2014)</xref>. However, a naive model where place cells are sums of grid cells would also suggest that the multiple place fields would be arranged in an orderly, possibly periodic, manner. To the contrary, the data (<xref ref-type="bibr" rid="bib20">Fenton et al., 2008</xref>; <xref ref-type="bibr" rid="bib44">Rich et al., 2014</xref>) show that the multiple place fields of dorsal hippocampal cells are organized in a disorderly fashion. On the other hand, real grid fields show significant variability in period, orientation, and ellipticity even within a module (<xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>)—this variability would disorder any linearly summed place fields, changing the prediction of the naive model. We have not attempted to investigate this in detail because there is also significant evidence (summarized in <xref ref-type="bibr" rid="bib11">Bush et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Sasaki et al., 2015</xref>) that place cells are not formed and maintained via simple summation of grid cells alone, although they are influenced by them. It would be interesting for future work to integrate the accumulating information about the complex interplay between the hippocampus and the mEC to better understand the consequences of hierarchical grid organization for the hippocampal place system.</p><p>We assumed that the largest scales of grid modules should be roughly comparable to the behavioral range of the animal. This is consistent with the existing data on grid modules (<xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>) and with measurements in the largest environments tested so far (<xref ref-type="bibr" rid="bib9">Brun et al., 2008</xref>) (periods at least as large as 10 m in an 18 m track). To accommodate very large environments, grids could either increase their scale (as reported at least transiently in <xref ref-type="bibr" rid="bib4">Barry et al., 2007</xref>; <xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>) or could segment the environment into large sections (<xref ref-type="bibr" rid="bib16">Derdikman et al., 2009</xref>; <xref ref-type="bibr" rid="bib15">Derdikman and Moser, 2010</xref>) across which remapping occurs (<xref ref-type="bibr" rid="bib24">Fyhn et al., 2007</xref>). These predictions can be tested in detail by exploring spatial coding in natural environments of behaviorally appropriate size and complexity. In fact, ethological studies have indicated a typical homing rate of a few tens of meters for rats with significant variation between strains (<xref ref-type="bibr" rid="bib13">Davis et al., 1948</xref>; <xref ref-type="bibr" rid="bib22">Fitch, 1948</xref>; <xref ref-type="bibr" rid="bib54">Stickel and Stickel, 1949</xref>; <xref ref-type="bibr" rid="bib48">Slade and Swihart, 1983</xref>; <xref ref-type="bibr" rid="bib6">Braun, 1985</xref>). Our theory predicts that the period of the largest grid module and the number of modules will be correlated with homing range.</p><p>In our theory, we took the coverage factor <italic>d</italic> (the number of grid fields overlapping a given point in space) to be the same for each module. In fact, experimental measurements have not yet established whether this parameter is constant or varies between modules. How would a varying <italic>d</italic> affect our results? The answer depends on the dimensionality of the grid. In two dimensions, if neurons have weakly correlated noise, modular variation of the coverage factor does not affect the optimal grid at all. This is because the coverage factor cancels out of all relevant formulae, a coincidence of two dimensions (see Optimizing the grid system: probabilistic decoder, ‘Materials and methods’, and p. 112 of <xref ref-type="bibr" rid="bib14">Dayan and Abbott, 2001</xref>). In one and three dimensions, variation of <italic>d</italic> between modules will have an effect on the optimal ratios between the variable modules. Thus, if the coverage factor is found to vary between grid modules for animals navigating one and three dimensions, our theory can be tested by comparing its predictions for the corresponding variations in grid scale factors. Similarly, even in two dimensions, if noise is correlated between grid cells, then variability in <italic>d</italic> can affect our predicted scale factor. This provides another avenue for testing our theory.</p><p>The simple winner-take-all model assuming compact grid fields predicted a ratio of field width to grid period that matched measurements in both wild-type and HCN1 knockout mice (<xref ref-type="bibr" rid="bib25">Giocomo et al., 2011a</xref>). Since the predicted grid field width is model dependent, the match with the simple WTA prediction might be providing a hint concerning the method the brain uses to read the grid code. Additional data on this ratio parameter drawn from multiple grid modules may serve to distinguish and select between potential decoding models for the grid system. The probabilistic model did not make a direct prediction about grid field width; it instead worked with the standard deviation σ<sub>i</sub> of the posterior <italic>P</italic>(<italic>x</italic>|<italic>i</italic>). This parameter is predicted to be <italic>σ</italic><sub><italic>i</italic></sub> = 0.19<italic>λ</italic><sub><italic>i</italic></sub> in two dimensions (see Optimizing the grid system: probabilistic decoder, ‘Materials and methods’). This prediction could be tested behaviorally by comparing discrimination thresholds for location to the period of the smallest module. The standard deviation <italic>σ</italic><sub><italic>i</italic></sub> can also be related to the noise, neural density and tuning curve shape in each module (<xref ref-type="bibr" rid="bib14">Dayan and Abbott, 2001</xref>).</p><p>Previous work by <xref ref-type="bibr" rid="bib21">Fiete et al. (2008)</xref> proposed that the grid system is organized to represent very large ranges in space by exploiting the incommensurability (i.e., lack of common rational factors) of different grid periods. As originally proposed, the grid scales in this scheme were not hierarchically organized (as we now know they are <xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>) but were of similar magnitude, and hence it was particularly important to suggest a scheme where a large spatial range could be represented using grids with small and similar periods. Using all the scales together (<xref ref-type="bibr" rid="bib21">Fiete et al., 2008</xref>) argued that it is easy to generate ranges of representation that are much larger than necessary for behavior, and Sreenivasan and Fiete argued that the excess capacity could be used for error correction over distances relevant for behavior (<xref ref-type="bibr" rid="bib51">Sreenivasan and Fiete, 2011</xref>). However, recent experiments tell us that there is a hierarchy of scales (<xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>) which should make the representation of behaviorally plausible range of 20–100 m easily accessible in the alternative hierarchical coding scheme that we have proposed. Nevertheless, we have checked that a grid coding scheme with the optimal scale ratio predicted by our theory can represent space over ranges larger than the largest grid period (‘Range of location coding in a grid system’, <xref ref-type="app" rid="app1">Appendix 1</xref>). However, to achieve this larger range, the number of neurons in each module will have to increase relative to the minimum in order to shrink the widths of the peaks in the likelihood function over position. It could be that animals sometimes exploit this excess capacity either for error correction or to avoid remapping over a range larger than the period of the largest grid. That said, experiments do tell us that remapping occurs readily over relatively small (meter length) scales at least for dorsal (small scale) place cells and grid cells (<xref ref-type="bibr" rid="bib24">Fyhn et al., 2007</xref>) in tasks that involve spatial cues.</p><p>Our hierarchical grid scheme makes distinctive predictions relative to a non-hierarchical model for the effects of selective lesions of grid modules in the context of specific models where grid cells sum to make place cells (details in ‘Predictions for the effects of lesions and for place cell activity’, <xref ref-type="app" rid="app1">Appendix 1</xref>). In such a simple grid to place cell transformation, lesioning the modules with small periods will expand place field widths, while lesioning modules with large periods will lead to increased firing at locations outside the main place field, at scales set by the missing module. Similar effects are predicted for any simple decoder of a lesioned hierarchical grid system that has no other location related inputs—that is, animals with lesions to fine grid modules will show less precision in spatial behavior, while animals with lesions to large grid modules will confound well-separated locations. In contrast, in a non-hierarchical grid scheme with similar but incommensurate periods, lesions of any module lead to the appearance of multiple place fields at many scales for each place cell. Recent studies which ablated a large fraction of the mEC at all depths showed an increase in place field widths (<xref ref-type="bibr" rid="bib28">Hales et al., 2014</xref>), as did the more focal lesions of <xref ref-type="bibr" rid="bib43">Ormond and McNaughton (2015)</xref> along the dorso–ventral axis of the mEC. However, there are multiple challenges in interpreting these experiments. First, the data of <xref ref-type="bibr" rid="bib52">Stensola et al. (2012)</xref> shows that there are modules with both small and large periods at every depth along the mEC—the dorsal mEC is simply enriched in modules with large periods. So <xref ref-type="bibr" rid="bib28">Hales et al. (2014)</xref>; <xref ref-type="bibr" rid="bib43">Ormond and McNaughton (2015)</xref> are both removing modules that have both small and large periods. A simple linear transformation from a hierarchical grid to place cells would predict that removing large periods increases the number of place fields, but <xref ref-type="bibr" rid="bib28">Hales et al. (2014)</xref> did not look for this effect while in <xref ref-type="bibr" rid="bib43">Ormond and McNaughton (2015)</xref> the reported number of place fields decreases after lesions (including complete dirsruption of place fields of some cells). The underlying difficulty in interpretation is that while place cells might be summing up grid cells, there is evidence that they can be formed and maintained through mechanisms that may not critically involve the mEC at all (<xref ref-type="bibr" rid="bib11">Bush et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Sasaki et al., 2015</xref>). Thus, despite the interpretation given in <xref ref-type="bibr" rid="bib32">Kubie and Fox (2015)</xref>; <xref ref-type="bibr" rid="bib43">Ormond and McNaughton (2015)</xref> in favor of the partial validity of a linearly summed grid to place model, it is difficult for theory to make a definitive prediction for experiments until the inter-relation of the mEC and hippocampus is better understood.</p><p><xref ref-type="bibr" rid="bib36">Mathis et al. (2012a)</xref> and <xref ref-type="bibr" rid="bib37">Mathis et al. (2012b)</xref> studied the resolution and representational capacity of grid codes vs place codes. They found that grid codes have exponentially greater capacity to represent locations than place codes with the same number of neurons. Furthermore, <xref ref-type="bibr" rid="bib36">Mathis et al. (2012a)</xref> predicted that in one dimension a geometric progression of grids that is self-similar at each scale minimizes the asymptotic error in recovering an animal's location given a fixed number of neurons. To arrive at these results the authors formulated a population coding model where independent Poisson neurons have periodic one-dimensional tuning curves. The responses of these model neurons were used to construct a maximum likelihood estimator of position, whose asymptotic estimation error was bounded in terms of the Fisher information—thus the resolution of the grid was defined in terms of the Fisher information of the neural population (which can, however, dramatically overestimate coding precision for neurons with multimodal tuning curves [<xref ref-type="bibr" rid="bib5">Bethge et al., 2002</xref>]). Specializing to a grid system organized in a fixed number of modules, <xref ref-type="bibr" rid="bib36">Mathis et al. (2012a)</xref> found an expression for the Fisher information that depended on the periods, populations, and tuning curve shapes in each module. Finally, the authors imposed a constraint that the scale ratio had to exceed some fixed value determined by a ‘safety factor’ (dependent on tuning curve shape and neural variability), in order reduce ambiguity in decoding position. With this formulation and assumptions, optimizing the Fisher information predicts geometric scaling of the grid in a regime where the scale factor is sufficiently large. The Fisher information approximation to position error in <xref ref-type="bibr" rid="bib36">Mathis et al. (2012a)</xref> is only valid over a certain range of parameters. An ambiguity-avoidance constraint keeps the analysis within this range, but introduces two challenges for an optimization procedure: (i) the optimum depends on the details of the constraint, which was somewhat arbitrarily chosen and was dependent on the variability and tuning curve shape of grid cells, and (ii) the optimum turns out to saturate the constraint, so that for some choices of constraint the procedure is pushed right to the edge of where the Fisher information is a valid approximation at all, causing difficulties for the self-consistency of the procedure.</p><p>Because of these limits on the Fisher information approximation, <xref ref-type="bibr" rid="bib36">Mathis et al. (2012a)</xref> also measured decoding error directly through numerical studies. But here a complete optimization was not possible because there are too many inter-related parameters, a limitation of any numerical work. The authors then analyzed the dependence of the decoding error on the grid scale factor and found that, in their theory, the optimal scale factor depends on ‘the number of neurons per module and peak firing rate’ and, relatedly, on the ‘tolerable level of error’ during decoding (<xref ref-type="bibr" rid="bib36">Mathis et al., 2012a</xref>). Note that decoding error was also studied in <xref ref-type="bibr" rid="bib58">Towse et al. (2014)</xref> and those authors reported that the results did not depend strongly on the precise organization of scales across modules.</p><p>In contrast to <xref ref-type="bibr" rid="bib36">Mathis et al. (2012a)</xref>, we estimated decoding error directly by working with approximated forms of the likelihood function over position rather than by approximating decoding error in terms of the Fisher information. Conceptually, we can think of the winner-take-all analysis as effectively approximating the likelihood in terms of periodic boxcar functions; for the probabilistic analysis, we treat the likelihood as a periodic sum-of-Gaussians. Since at least scores of cells are being combined within modules, the Gaussian approximation to local likelihood peaks is valid, allowing us to circumvent detailed analysis of tuning curves and variability of individual neurons. These approximations allow analytical treatment of the optimization problem over a much wider parameter range without requiring arbitrary hand-imposed constraints. Our formulation of grid resolution then simply estimates the number of distinct regions that a fixed range can be divided into. We then fix this resolution as being behaviorally determined and minimize the number of required neurons while allowing the periods of the modules, and, crucially, the number of modules, to vary to achieve the minimum.</p><p>All told, our simpler, and more intuitive, formulation of grid coding embodies very general considerations trading off precision and ambiguity with a sufficiently dense population of grid cells. The simplicity and generality of our setting allows us to make predictions for structural parameters of the grid system in different dimensions. These predictions—scaling ratios in 1, 2, and 3 dimensions; the ratio of grid period to grid field width; the number of expected modules; the shape of the optimal grid lattice; an estimate of the total expected number of grid cells—can be directly tested in experiments.</p><p>There is a long history in the study of sensory coding, especially vision, of identifying efficiency principles underlying neural circuits and codes starting with <xref ref-type="bibr" rid="bib3">Barlow (1961)</xref>. Our results constitute evidence that such principles might also operate in the organization of cognitive circuits processing non-sensory variables. Furthermore, the existence of an efficiency argument for grid organization of spatial coding suggests that grid systems may be universal amongst the vertebrates, and not just a rodent specialization. In fact, there is evidence that humans (<xref ref-type="bibr" rid="bib17">Doeller et al., 2010</xref>; <xref ref-type="bibr" rid="bib29">Jacobs et al., 2013</xref>) and other primates (<xref ref-type="bibr" rid="bib30">Killian et al., 2012</xref>) also have grid systems. We expect that our predicted scaling of the grid modules also holds in humans and other primates.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Optimizing a ‘base-b’ representation of one-dimensional space</title><p>Suppose that we want to resolve location with a precision <italic>l</italic> in a track of length <italic>L</italic>. In terms of the resolution <italic>R</italic> = <italic>L</italic>/<italic>l</italic>, we argued in the ‘Discussion’ that a ‘base-b’ hierarchical neural coding scheme will roughly require <italic>N</italic> = <italic>b</italic> log<sub><italic>b</italic></sub> <italic>R</italic> neurons. To derive the optimal base (i.e., the base that minimizes the number of the neurons), we evaluate the extremum <inline-formula><mml:math id="inf58"><mml:mrow><mml:mo>∂</mml:mo><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mo>∂</mml:mo><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mtext>  </mml:mtext><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mi>R</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mtext>  ln </mml:mtext><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mtext>ln </mml:mtext><mml:mi>R</mml:mi><mml:mtext>  </mml:mtext><mml:mfrac><mml:mrow><mml:mtext>ln </mml:mtext><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>ln </mml:mtext><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Setting <inline-formula><mml:math id="inf59"><mml:mrow><mml:mo>∂</mml:mo><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> gives ln<italic>b</italic> − 1 = 0. Therefore, the number of neurons is extremized when <italic>b</italic> = <italic>e</italic>. It is easy to check that this is a minimum. Of course, the base of a number system is usually taken to be an integer, so the argument should be taken as motivating the more detailed treatment of neural representations of space above. Neurons are of course not constrained to organize the periodicity of their tuning curves in integer ratios.</p></sec><sec id="s4-2"><title>Optimizing the grid system: winner-take-all decoder</title><sec id="s4-2-1"><title>Deriving the optimal grid</title><p>We have seen that, for a winner-take-all decoder, the problem of deriving the optimal ratios of adjacent grid scales in one dimension is equivalent to minimizing the sum of a set of numbers (<inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) while fixing the product (<inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) to take the value <italic>R</italic>. Mathematically, it is equivalent to minimize <italic>N</italic> while fixing ln<italic>R</italic><sub>1</sub>. When <italic>N</italic> is large, we can treat it as a continuous variable and use the method of Lagrange multipliers as follows. First, we construct the auxiliary function <italic>H</italic>(<italic>r</italic><sub>1</sub>⋯<italic>r</italic><italic><sub>m</sub></italic>, <italic>β</italic>) = <italic>N</italic> − <italic>β</italic> (ln <italic>R</italic><sub>1</sub> − ln <italic>R</italic>) and then extremize <italic>H</italic> with respect to each <italic>r</italic><sub><italic>i</italic></sub> and <italic>β</italic>. Extremizing with respect to <italic>r</italic><sub><italic>i</italic></sub> gives<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mi>β</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mtext>     </mml:mtext><mml:mo>⇒</mml:mo><mml:mtext>     </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>β</mml:mi><mml:mi>d</mml:mi></mml:mfrac><mml:mo>≡</mml:mo><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Next, extremizing with respect to <italic>β</italic> to implement the constraint on the resolution gives<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mtext>ln </mml:mtext><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mtext>ln </mml:mtext><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mtext> ln </mml:mtext><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mtext>ln </mml:mtext><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mtext>     </mml:mtext><mml:mo>⇒</mml:mo><mml:mtext>     </mml:mtext><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Having thus implemented the constraint that ln<italic>R</italic><sub>1</sub> = ln<italic>R</italic>, it follows that <italic>H</italic> = <italic>N</italic> = <italic>dmR</italic><sup>1/<italic>m</italic></sup>. Alternatively, solving for <italic>m</italic> in terms of <italic>r</italic>, we can write <italic>H</italic> = <italic>d</italic> <italic>r</italic> (ln <italic>R</italic>)/ln <italic>r</italic>) = <italic>d r log</italic><sub><italic>r</italic></sub> <italic>R</italic>. It remains to minimize the number of cells <italic>N</italic> with respect to <italic>r</italic>,<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mtext>  ln </mml:mtext><mml:mi>R</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mtext>ln </mml:mtext><mml:mi>r</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mtext>ln </mml:mtext><mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mtext>     </mml:mtext><mml:mo>⇒</mml:mo><mml:mtext>     ln </mml:mtext><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This is in turn implies our result<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>for the optimal ratio between adjacent scales in a hierarchical, grid coding scheme for position in one dimension, using a winner-take-all decoder. In this argument, we employed the sleight of hand that <italic>N</italic> and <italic>m</italic> can be treated as continuous variables, which is approximately valid when <italic>N</italic> is large. This condition obtains if the required resolution <italic>R</italic> is large. A more careful argument is given below that preserves the integer character of <italic>N</italic> and <italic>m</italic>.</p><sec id="s4-2-1-1"><title>Integer <italic>N</italic> and <italic>m</italic></title><p>Above we used Lagrange multipliers to enforce the constraint on resolution and to bound the scale ratios to avoid ambiguity while minimizing the number of neurons required by a winner-take-all decoding model of grid systems. Here, we will carry out this minimization while recognizing that the number of neurons is an integer. First, consider the arithmetic mean–geometric mean inequality which states that, for a set of non-negative real numbers, <italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>,…, <italic>x</italic><sub><italic>m</italic></sub>, the following holds:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>m</mml:mi><mml:mo>≥</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>with equality if and only if all the <italic>x</italic><sub><italic>i</italic></sub>'s are equal. Applying this inequality, it is easy to see that to minimize <inline-formula><mml:math id="inf62"><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, all of the <italic>r</italic><sub><italic>i</italic></sub> should be equal. We denote this common value as <italic>r</italic>, and we can write <italic>r</italic> = <italic>R</italic><sup>1/<italic>m</italic></sup>.</p><p>Therefore, we have<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mtext>  </mml:mtext><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Suppose <italic>R</italic> = <italic>e</italic><sup><italic>z</italic> + <italic>ϵ</italic></sup>, where <italic>z</italic> is an integer, and <italic>ϵ</italic> ∈ [0, 1). By taking the first derivative of N with respect to m, and setting it to zero, we find that <italic>N</italic> is minimized when <italic>m</italic> = <italic>z</italic> + <italic>ϵ</italic>. However, since <italic>m</italic> is an integer the minimum will be achieved either at <italic>m</italic> = <italic>z</italic> or <italic>m</italic> = <italic>z</italic> + 1. (Here, we used the fact <italic>mR</italic><sup>1/<italic>m</italic></sup> is monotonically increasing between 0 and <italic>z</italic> + <italic>ϵ</italic> and is monotonically decreasing between <italic>z</italic> + <italic>ϵ</italic> and ∞.) Thus, minimizing N requires either<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">ϵ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>z</mml:mi></mml:mfrac></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">ϵ</mml:mi></mml:mrow><mml:mi>z</mml:mi></mml:mfrac></mml:mrow></mml:msup><mml:mtext>     or     </mml:mtext><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">ϵ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In either case, when <italic>z</italic> is large (and therefore <italic>R</italic>, <italic>N</italic> and <italic>m</italic> are large), <italic>r</italic> → <italic>e</italic>. This shows that when the resolution <italic>R</italic> is sufficiently large, the total number of neurons <italic>N</italic> is minimized when <italic>r</italic><sub><italic>i</italic></sub> ≈ <italic>e</italic> for all <italic>i</italic>.</p></sec></sec><sec id="s4-2-2"><title>Optimal winner-take-all grids: general formulation</title><p>As described in the above, we wish to choose the grid system parameters {<italic>λ</italic><sub><italic>i</italic></sub>, <italic>l</italic><sub><italic>i</italic></sub>}, 1 ≤ <italic>i</italic> ≤ <italic>m</italic>, as well as the number of scales <italic>m</italic>, to minimize neuron number:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>d</italic> is the fixed coverage factor in each module, while constraining the positional accuracy of the grid system and the range of representation. We can take the positional accuracy to be proportional to the grid field width of the smallest module. This gives<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To give a sufficiently large range of representation in our hierarchical scheme we will require that<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>≥</mml:mo><mml:mi>L</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Following the main text, to eliminate ambiguity at each scale we need that<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>c</italic><sub>2</sub> depends on the tuning curve shape and coverage factor (written as <italic>f</italic>(<italic>d</italic>) above).</p><p>We will first fix <italic>m</italic> and solve for the remaining parameters, then optimize over <italic>m</italic> in a subsequent step. Optimization problems subject to inequality constraints may be solved by the method of Karush-Kuhn-Tucker (KKT) conditions (<xref ref-type="bibr" rid="bib33">Kuhn and Tucker, 1951</xref>). We first form the Lagrange function,<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mi mathvariant="normal">ℒ</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The KKT conditions include that the gradient of <inline-formula><mml:math id="inf63"><mml:mi mathvariant="normal">ℒ</mml:mi></mml:math></inline-formula> with respect to {<italic>λ</italic><sub><italic>i</italic></sub>,...,<italic>l</italic><sub><italic>i</italic></sub>} vanish,<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="normal">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>α</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="normal">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mtext>    </mml:mtext><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="normal">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>together with the ‘complementary slackness’ conditions,<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:math></disp-formula></p><p>From <xref ref-type="disp-formula" rid="equ15 equ16">Equations 15, 16</xref>, we obtain:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:mfrac><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>It follows that <italic>β</italic><sub><italic>i</italic></sub> ≠ 0, and so the complementary slackness conditions give:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Substituting this result into <xref ref-type="disp-formula" rid="equ19">Equation 19</xref> yields,<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≡</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>that is, the scale factor <italic>r</italic> is the same for all modules. Once we obtain a value for <italic>r</italic>, <xref ref-type="disp-formula" rid="equ20">Equations 20</xref>–<xref ref-type="disp-formula" rid="equ22">22</xref> yield values for all <italic>λ</italic><sub><italic>i</italic></sub> and <italic>l</italic><sub><italic>i</italic></sub>. Since the resolution constraint may now be rewritten,<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mtext> </mml:mtext><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>we have <italic>m</italic> = ln (<italic>c</italic><sub>1</sub><italic>L</italic>/<italic>A</italic>)/ln<italic>r</italic>. Therefore, <italic>r</italic> determines <italic>m</italic> and so minimizing <italic>N</italic> over <italic>m</italic> is equivalent to minimizing over <italic>r</italic>. Expressing <italic>N</italic> entirely in terms of <italic>r</italic> gives,<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mtext> ln</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>L</mml:mi><mml:mo>/</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mtext>ln </mml:mtext><mml:mi>r</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Optimizing with respect to <italic>r</italic> gives the result <italic>r</italic> = <italic>e</italic>, independent of <italic>d</italic>, <italic>c</italic><sub>1</sub>, <italic>c</italic><sub>2</sub>, <italic>L</italic>, and <italic>R</italic>.</p></sec></sec><sec id="s4-3"><title>Optimizing the grid system: probabilistic decoder</title><p>Consider a probabilistic decoder of the grid system that pools all the information available in the population of neurons in each module by forming the posterior distribution over position given the neural activity. In this general setting, we assume that the firing of different grid cells is weakly correlated, that noise is homogeneous, and that the tuning curves in each module <italic>i</italic> provide dense, uniform, coverage of the interval <italic>λ</italic><sub><italic>i</italic></sub>. With these assumptions, we will first consider the one-dimensional case, and then analyze the two-dimensional case by analogy.</p><sec id="s4-3-1"><title>One-dimensional grids</title><p>With the above assumptions, the likelihood of the animal's position, given the activity of grid cells in module <italic>i</italic>, <italic>P</italic>(<italic>x</italic>|<italic>i</italic>), can be approximated as a series of Gaussian bumps of standard deviation <italic>σ</italic><sub><italic>i</italic></sub> spaced at the period <italic>λ</italic><sub><italic>i</italic></sub> (<xref ref-type="bibr" rid="bib14">Dayan and Abbott, 2001</xref>). As defined in 'Results', the number of cells (<italic>n</italic><sub><italic>i</italic></sub>) in the <italic>i</italic>th module, is expressed in terms of the period (<italic>λ</italic><sub><italic>i</italic></sub>), the grid field width (<italic>l</italic><sub><italic>i</italic></sub>) and a ‘coverage factor’ <italic>d</italic> representing the cell density as <italic>n</italic><sub><italic>i</italic></sub> = <italic>dλ</italic><sub><italic>i</italic></sub>/<italic>l</italic><sub><italic>i</italic></sub>. The coverage factor <italic>d</italic> will control the relation between the grid field width <italic>l</italic><sub><italic>i</italic></sub> and the standard deviation <italic>σ</italic><sub><italic>i</italic></sub> of the local peaks in the likelihood function of location. If <italic>d</italic> is larger, <italic>σ</italic><sub><italic>i</italic></sub> will be narrower since we can accumulate evidence from a denser population of neurons. The ratio <inline-formula><mml:math id="inf64"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> in general will be a monotonic function of the coverage factor <italic>d</italic>, which we will write as <inline-formula><mml:math id="inf65"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In the special case where the grid cells have independent noise <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>, so that <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∝</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>—that is, the precision increases as the inverse square root of the cell density, as expected because the relevant parameter is the number of cells within one grid field rather than the total number of cells. Note that this does <italic>not</italic> imply an inverse square root relation between the <italic>number</italic> of cells <italic>n</italic><sub><italic>i</italic></sub> and <italic>σ</italic><sub><italic>i</italic></sub>, because <italic>n</italic><sub><italic>i</italic></sub> is also proportional to the period <italic>λ</italic><sub><italic>i</italic></sub>, and in our formulation the density <italic>d</italic> is fixed while <italic>λ</italic><sub><italic>i</italic></sub> can be varied. Note also that if the neurons have correlated noise, <italic>g</italic>(<italic>d</italic>) may scale substantially slower than <inline-formula><mml:math id="inf68"><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib7">Britten et al., 1992</xref>; <xref ref-type="bibr" rid="bib62">Zohary et al., 1994</xref>; <xref ref-type="bibr" rid="bib50">Sompolinsky et al., 2001</xref>). Putting all of these statements together, we have, in general, <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Assuming that the coverage factor <italic>d</italic> is the same across modules, we can simplify the notation and write <inline-formula><mml:math id="inf70"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <italic>c</italic> = <italic>d</italic>/<italic>g</italic>(<italic>d</italic>) is a constant. (Again, for independent noise <italic>σ</italic><sub><italic>i</italic></sub> ∝ 1/<italic>d</italic> as expected—see above—and this does <italic>not</italic> imply a similar relationship to the number of cells <italic>n</italic><sub><italic>i</italic></sub> as one might have naively assumed.) In sum, we can write the total number of cells in a grid system with <italic>m</italic> modules as <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p><p>The likelihood of position derived from each module can be combined to give an overall probability distribution over location. Let <italic>Q</italic><sub><italic>i</italic></sub>(<italic>x</italic>) be the likelihood obtained by combining modules 1 (the largest period) through <italic>i</italic>. Assuming that the different modules have independent noise, we can compute <italic>Q</italic><sub><italic>i</italic></sub>(<italic>x</italic>) from the module likelihoods as <inline-formula><mml:math id="inf72"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We will take the prior probability over locations be uniform here so that this combined likelihood is equivalent to the Bayesian posterior distribution over location. The likelihoods from different scales have different periodicities, so multiplying them against each other will tend to suppress all peaks except the central one, which is aligned across scales. We may thus approximate <italic>Q</italic><sub><italic>i</italic></sub>(<italic>x</italic>) by single Gaussians whose standard deviations we will denote as <italic>δ</italic><sub><italic>i</italic></sub>. (The validity of this approximation is taken up in further detail below.)</p><p>Since <italic>Q</italic><sub><italic>i</italic></sub>(<italic>x</italic>) ∝ <italic>Q</italic><sub><italic>i</italic>−1</sub>(<italic>x</italic>)<italic>P</italic>(<italic>x</italic>|<italic>i</italic>), <italic>δ</italic><sub><italic>i</italic></sub> is determined by <italic>δ</italic><sub><italic>i</italic>−1</sub>, <italic>λ</italic><sub><italic>i</italic></sub> and <italic>σ</italic><sub><italic>i</italic></sub>. These all have dimensions of length. Dimensional analysis (<xref ref-type="bibr" rid="bib55">Rayleigh, 1896</xref>) therefore says that, without loss of generality, the ratio <italic>δ</italic><sub><italic>i</italic></sub>/<italic>δ</italic><sub><italic>i</italic>−1</sub> can be written as a dimensionless function of any two cross-ratios of these parameters. It will prove useful to use this freedom to write <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The standard error in decoding the animal's position after combining information from all the grid modules will be proportional to <italic>δ</italic><sub><italic>m</italic></sub>, the standard deviation of <italic>Q</italic><sub><italic>m</italic></sub>. We can iterate our expression for <italic>δ</italic><sub><italic>i</italic></sub> in terms of <italic>δ</italic><sub><italic>i</italic>−1</sub> to write <inline-formula><mml:math id="inf74"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <italic>δ</italic><sub>0</sub> is the uncertainty in location without using any grid responses at all. (We are abbreviating <italic>ρ</italic><sub><italic>i</italic></sub> = <italic>ρ</italic>(<italic>λ</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub>, <italic>σ</italic><sub><italic>i</italic></sub>/<italic>δ</italic><sub><italic>i</italic>−1</sub>)). In the present probabilistic context, we can view <italic>δ</italic><sub>0</sub> as the standard deviation of the a priori distribution over position before the grid system is consulted, but it will turn out that the precise value or meaning of <italic>δ</italic><sub>0</sub> is unimportant. We assume a behavioral requirement that fixes <italic>δ</italic><sub><italic>m</italic></sub> and thus the resolution of the grid, and that <italic>δ</italic><sub>0</sub> is likewise fixed by the behavioral range. Thus, there is a constraint on the product <inline-formula><mml:math id="inf75"><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∏</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>.</p><p>Putting everything together, we wish to minimize <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> subject to the constraint that <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <italic>ρ</italic><sub><italic>i</italic></sub> is a function of <italic>λ</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub> and <italic>σ</italic><sub><italic>i</italic></sub>/<italic>δ</italic><sub><italic>i</italic> − 1</sub>. Given the formula for <italic>ρ</italic><sub><italic>i</italic></sub> derived in the next section, this can be carried out numerically. To understand the optimum, it is helpful to observe that the problem has a symmetry under permutations of <italic>i</italic>. So we can guess that in the optimum all the <italic>λ</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub>, <italic>σ</italic><sub><italic>i</italic></sub>/<italic>δ</italic><sub><italic>i</italic> − 1</sub> and <italic>ρ</italic><sub><italic>i</italic></sub> will be equal to a fixed <italic>λ</italic>/<italic>σ</italic>, <italic>σ</italic>/<italic>δ</italic>, and <italic>ρ</italic>. We can look for a solution with this symmetry and then check that it is an optimum. First, using the symmetry, we write <italic>N</italic> = <italic>cm</italic>(<italic>λ</italic>/<italic>σ</italic>) and <italic>R</italic> = <italic>ρ</italic><sup><italic>m</italic></sup>. It follows that <italic>N</italic> = <italic>c</italic>(1/ln<italic>ρ</italic>)(<italic>λ</italic>/<italic>σ</italic>) and we want to minimize it with respect to <italic>λ</italic>/<italic>σ</italic> and <italic>σ</italic>/<italic>δ</italic>. Now, <italic>ρ</italic>(<italic>λ</italic>/<italic>σ</italic>, <italic>σ</italic>/<italic>δ</italic>) is a complicated function of its arguments (<xref ref-type="disp-formula" rid="equ30">Equation 30</xref>) which has a maximum value as a function of <italic>σ</italic>/<italic>δ</italic> for any fixed <italic>λ</italic>/<italic>σ</italic>. To minimize <italic>N</italic> at fixed <italic>λ</italic>/<italic>σ</italic>, we should maximize <italic>ρ</italic> with respect to <italic>σ</italic>/<italic>δ</italic> (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Given this <italic>ρ</italic><sub><italic>max</italic></sub>, we can minimize <italic>N</italic> = <italic>c</italic>(<italic>λ</italic>/<italic>σ</italic>)/ln <italic>ρ</italic><sub><italic>max</italic></sub>(<italic>λ</italic>/<italic>σ</italic>) with respect to <italic>λ</italic>/<italic>σ</italic>, and then plug back in to find the optimal <italic>ρ</italic>. It turns out to be <inline-formula><mml:math id="inf78"><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mtext>*</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>.</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.08362.004</object-id><label>Figure 5.</label><caption><title>Optimizing the one-dimensional grid system.</title><p>(<bold>A</bold>) <inline-formula><mml:math id="inf12"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:msub><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo>/</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>λ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mi>σ</mml:mi><mml:mi>δ</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the scale factor after optimizing <italic>N</italic> over <italic>σ</italic>/<italic>δ</italic>. The values <italic>r</italic><sup>*</sup> and <italic>λ</italic><sup>*</sup> are the values chosen by the complete optimization procedure. (<bold>B</bold>) The optimal ratio <italic>r</italic> between adjacent scales in a hierarchical grid system in one dimension for a simple winner-take-all decoding model (blue, WTA) and a probabilistic decoder (red). Here, <italic>N</italic><sub><italic>r</italic></sub> is the number of neurons required to represent space with resolution <italic>R</italic> given a scaling ratio <italic>r</italic>, and <italic>N</italic><sub>min</sub> is the number of neurons required at the optimum. In both models, the ratio <italic>N</italic><sub><italic>r</italic></sub>/<italic>N</italic><sub>min</sub> is independent of resolution, <italic>R</italic>. For the winner-take-all model, <italic>N</italic><sub><italic>r</italic></sub> ∝ <italic>r</italic>/ln<italic>r</italic>, while the curve for the probabilistic model is derived numerically (mathematical details in Optimizing the grid system: probabilistic decoder, ‘Materials and methods’). The winner-take-all model predicts <italic>r</italic> = <italic>e</italic> ≈ 2.7, while the probabilistic decoder predicts <italic>r</italic> ≈ 2.3. The minima of the two curves lie within each others' shallow basins.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.08362.004">http://dx.doi.org/10.7554/eLife.08362.004</ext-link></p></caption><graphic xlink:href="elife-08362-fig5-v2.tif"/></fig></p><p>In fact, <italic>ρ</italic> is equal to the scale factor of the grid: <italic>ρ</italic><sub><italic>i</italic></sub> = <italic>r</italic><sub><italic>i</italic></sub> = <italic>λ</italic><sub><italic>i</italic></sub>/<italic>λ</italic><sub><italic>i</italic>+1</sub>. To see this, we have to express <italic>ρ</italic><sub><italic>i</italic></sub> in terms of the parameters <italic>λ</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub> and <italic>σ</italic><sub><italic>i</italic></sub>/<italic>δ</italic><sub><italic>i</italic>−1</sub>: <inline-formula><mml:math id="inf79"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Since the factors <italic>σ</italic><sub><italic>i</italic></sub>/<italic>δ</italic><sub><italic>i</italic> − 1</sub> and <italic>λ</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub> are independent of <italic>i</italic>, they cancel in the product and we are left with <italic>ρ</italic><sub><italic>i</italic></sub> = <italic>λ</italic><sub><italic>i</italic></sub>/<italic>λ</italic><sub><italic>i</italic> + 1</sub>.</p><p>Thus, the probabilistic decoder predicts an optimal scale factor <italic>r</italic><sup>*</sup> = 2.3 in one dimension. This is similar to, but somewhat different than, the winner-take-all result <italic>r</italic><sup>*</sup> = <italic>e</italic> = 2.7 (<xref ref-type="fig" rid="fig5">Figure 5</xref>). At a technical level, the difference arises because the function <italic>ρ</italic><sub><italic>max</italic></sub>(<italic>λ</italic>/<italic>σ</italic>) is effectively <inline-formula><mml:math id="inf80"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> in the winner-take-all analysis, but in the probabilistic case, it is more nearly a linear function with a positive offset <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>≈</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>λ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Conceptually, the optimal probabilistic scale factor is smaller in order to suppress side lobes that can arise in the combined likelihood across modules (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Such side lobes were absent in the winner-take-all analysis. The optimization also predicts <italic>λ</italic><sup>*</sup> = 9.1<italic>σ</italic>. This relation between the period and standard deviation at each scale could be converted into a relation between grid period and grid field width given specific measurements of tuning curves, noise levels, and cell density in each module. For example, if neurons within a module have independent noise, then general population coding considerations (<xref ref-type="bibr" rid="bib14">Dayan and Abbott, 2001</xref>) show that <italic>σ</italic> = <italic>βd</italic><sup>−1/2</sup><italic>l</italic>, where <italic>l</italic> is a measure of grid field width, <italic>d</italic> is the density of neurons in a module, and <italic>β</italic> is a dimensionless number that depends on noise (given the integration time) and tuning curve shape.and tuning curve shape.</p></sec><sec id="s4-3-2"><title>Two-dimensional grids</title><p>A similar probabilistic analysis can be carried out for two-dimensional grid fields. The posteriors <italic>P</italic>(<italic>x</italic>|<italic>i</italic>) become two-dimensional sums-of-Gaussians, with the centers of the Gaussians laid out on the vertices of the grid. <italic>Q</italic><sub><italic>i</italic></sub>(<italic>x</italic>) is then similarly approximated by a two-dimensional Gaussian. Generalizing from the one-dimensional case, the number of cells in module <italic>i</italic> is given by <italic>n</italic><sub><italic>i</italic></sub> = <italic>d</italic>(<italic>λ</italic><sub><italic>i</italic></sub>/<italic>l</italic><sub><italic>i</italic></sub>)<sup>2</sup>, where <italic>d</italic> is density of grid fields. As in one dimension, increasing the density <italic>d</italic> will decrease the standard deviation <italic>σ</italic><sub><italic>i</italic></sub> of the local bumps in the posterior <italic>P</italic>(<italic>x</italic>|<italic>i</italic>)—that is, <italic>l</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub> = <italic>g</italic>(<italic>d</italic>), where <italic>g</italic> is an increasing function of <italic>d</italic>. In the special case where the neurons have independent noise, <italic>g</italic>(<italic>d</italic>) ∝ <italic>d</italic> so that the precision measured by the standard deviation <italic>σ</italic><sub><italic>i</italic></sub> decreases as the inverse square root of <italic>d</italic>. Putting all of these statements together, we have, in general, <inline-formula><mml:math id="inf82"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. In the special case where noise is independent so that <italic>g</italic>(<italic>d</italic>) ∝ <italic>d</italic>, the density <italic>d</italic> cancels out in this expression, and in this case, or when the density <italic>d</italic> is the same across modules, we can write <inline-formula><mml:math id="inf83"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, where <italic>c</italic> is just a constant. Redoing the optimization analysis from the one-dimensional case, the form of the function <italic>ρ</italic> changes (Calculating <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>λ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mi>σ</mml:mi><mml:mi>δ</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, ‘Materials and methods’), but the logic of the above derivation is otherwise unaltered. In the optimal grid, we find that <italic>λ</italic><sup>*</sup> ≈ 5.3<italic>σ</italic> (or equivalently <italic>σ</italic> ≈ 0.19<italic>λ</italic><sup>*</sup>).</p></sec><sec id="s4-3-3"><title>Calculating <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>λ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mi>σ</mml:mi><mml:mi>δ</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></title><p>Above, we argued that the function <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>λ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mtext>  </mml:mtext><mml:mfrac><mml:mi>σ</mml:mi><mml:mi>δ</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be computed by approximating the posterior distribution of the animal's position given the activity in module <italic>i</italic>, <italic>P</italic>(<italic>x</italic>|<italic>i</italic>), as a periodic sum-of-Gaussians:<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mtext> </mml:mtext><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mtext> </mml:mtext><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mtext>  </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>K</italic> is assumed large. We further approximate the posterior given the activity of <italic>all</italic> modules coarser than <italic>λ</italic><sub><italic>i</italic></sub> by a Gaussian with standard deviation <italic>δ</italic><sub><italic>i</italic>−1</sub>:<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>(We are assuming here that the animal is really located at <italic>x</italic> = 0 and that the distributions <italic>P</italic>(<italic>x</italic>|<italic>i</italic>) for each <italic>i</italic> have one peak at this location.) Assuming noise independence across scales, it then follows that <inline-formula><mml:math id="inf87"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mtext> </mml:mtext><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mtext> d</mml:mtext><mml:mi>x</mml:mi><mml:mtext> </mml:mtext><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mtext> </mml:mtext><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Then <italic>ρ</italic>(<italic>λ</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub>, <italic>σ</italic><sub><italic>i</italic></sub>/<italic>δ</italic><sub><italic>i</italic> − 1</sub>) is given by <italic>δ</italic><sub><italic>i</italic> − 1</sub>/<italic>δ</italic><sub><italic>i</italic></sub>, where <italic>δ</italic><sub><italic>i</italic></sub> is the standard deviation of <italic>Q</italic><sub><italic>i</italic></sub>. We therefore must calculate <italic>Q</italic><sub><italic>i</italic></sub>(<italic>x</italic>) and its variance in order to obtain <italic>ρ</italic>. After some algebraic manipulation, we find,<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mtext>Σ</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mtext>Σ</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf88"><mml:mrow><mml:msup><mml:mtext>Σ</mml:mtext><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf89"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mtext>Σ</mml:mtext><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>, and<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p><italic>Z</italic> is a normalization factor enforcing <inline-formula><mml:math id="inf90"><mml:mrow><mml:mtext> </mml:mtext><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. <italic>Q</italic><sub><italic>i</italic></sub> is thus a mixture-of-Gaussians, seemingly contradicting our approximation that all the <italic>Q</italic> are Gaussian. However, if the secondary peaks of <italic>P</italic>(<italic>x</italic>|<italic>i</italic>) are well into the tails of <italic>Q</italic><sub><italic>i</italic>−1</sub>(<italic>x</italic>), then they will be suppressed (quantitatively, if <inline-formula><mml:math id="inf91"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>≫</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, then <inline-formula><mml:math id="inf92"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>≪</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> for |<italic>n</italic>| ≥ 1), so that our assumed Gaussian form for <italic>Q</italic> holds to a good approximation. In particular, at the values of <italic>λ</italic>, <italic>σ</italic> and <italic>δ</italic> selected by the optimization procedure described above, <italic>π</italic><sub>1</sub> = 1.3 × 10<sup>−3</sup><italic>π</italic><sub>0</sub>. So our approximation is self-consistent.</p><p>Next, we find the variance <inline-formula><mml:math id="inf93"><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ33"><mml:math id="m33"><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ34"><mml:math id="m34"><mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>n</mml:mi></mml:munder><mml:mtext> </mml:mtext><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mtext>Σ</mml:mtext><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ35"><mml:math id="m35"><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mtext>Σ</mml:mtext><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mtext>Σ</mml:mtext><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>n</mml:mi></mml:munder><mml:mtext> </mml:mtext><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mtext>Σ</mml:mtext><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>n</mml:mi></mml:munder><mml:mtext> </mml:mtext><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We can finally read off <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext>  </mml:mtext><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the ratio <italic>δ</italic><sub><italic>i</italic>−1</sub>/<italic>δ</italic><sub><italic>i</italic></sub>:<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>n</mml:mi></mml:munder><mml:mtext> </mml:mtext><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the calculations reported in the text, we took <italic>K</italic> = 500.</p><p>We explained above that we should maximize <italic>ρ</italic> over <inline-formula><mml:math id="inf95"><mml:mrow><mml:mfrac><mml:mi>σ</mml:mi><mml:mi>δ</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>, while sholding <inline-formula><mml:math id="inf96"><mml:mrow><mml:mfrac><mml:mi>λ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> fixed. The first factor in <xref ref-type="disp-formula" rid="equ30">Equation 30</xref> increases monotonically with decreasing <inline-formula><mml:math id="inf97"><mml:mrow><mml:mfrac><mml:mi>σ</mml:mi><mml:mi>δ</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>; however, <inline-formula><mml:math id="inf98"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>n</mml:mi></mml:munder><mml:mtext> </mml:mtext><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> also increases and this has the effect of reducing <italic>ρ</italic>. The optimal <inline-formula><mml:math id="inf99"><mml:mrow><mml:mfrac><mml:mi>σ</mml:mi><mml:mi>δ</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> is thus controlled by a trade-off between these factors. The first factor is related to the increasing precision given by narrowing the central peak of <italic>P</italic>(<italic>x</italic>|<italic>i</italic>), while the second factor describes the ambiguity from multiple peaks.</p><sec id="s4-3-3-1"><title>Generalization to two-dimensional grids</title><p>The derivation can be repeated in the two-dimensional case. We take <italic>P</italic>(<italic>x</italic>|<italic>i</italic>) to be a sum-of-Gaussians with peaks centered on the vertices of a regular lattice generated by the vectors <inline-formula><mml:math id="inf100"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mtext>  </mml:mtext><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We also define <inline-formula><mml:math id="inf101"><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>≡</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The factor of 1/2 ensures that the variance so defined is measured as an average over the two dimensions of space. The derivation is otherwise parallel to the above, and the result is,<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf102"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>|</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p></sec></sec></sec><sec id="s4-4"><title>Reanalysis of grid data from previous studies</title><p>We reanalyzed the data from <xref ref-type="bibr" rid="bib4">Barry et al. (2007)</xref> and <xref ref-type="bibr" rid="bib52">Stensola et al. (2012)</xref> in order to get the mean and the variance of the ratio of adjacent grid scales. For <xref ref-type="bibr" rid="bib4">Barry et al. (2007)</xref>, we first read the raw data from Figure 3B of their paper using the software GraphClick, which allows retrieval of the original (x,y)-coordinates from the image. This gave the scales of grid cells recorded from six different rats. For each animal, we grouped the grids that had similar periodicities (i.e., differed by less than 20%) and calculated the mean periodicity for each group. We defined this mean periodicity as the scale of each group. For four out of six rats, there were two scales in the data. For one out six rats, there were three grid scales. For the remaining rat, only one scale was obtained as only one cell was recorded from that rat. We excluded this rat from further analysis. We then calculated the ratio between adjacent grid scales, resulting in 6 ratios from five rats. The mean and variance of the ratio were 1.64 and 0.09, respectively (<italic>n</italic> = 6).</p><p>For <xref ref-type="bibr" rid="bib52">Stensola et al. (2012)</xref>, we first read in the data using GraphClick from Figure 5D of their paper. This gave the scale ratios between different grids for 16 different rats. We then pooled all the ratios together and calculated the mean and variance. The mean and variance of the ratio were 1.42 and 0.17, respectively (<italic>n</italic> = 24).</p><p><xref ref-type="bibr" rid="bib25">Giocomo et al. (2011a)</xref> reported the ratios between the grid period and the <italic>radius</italic> of grid field (measured as the radius of the circle around the center field of the autocorrelation map of the grid cells) to be 3.26 ± 0.07 and 3.32 ± 0.06 for Wild-type and HCN KO mice, respectively. We halved these measurements to the ratios between grid period and the <italic>diameter</italic> of the grid field to facilitate the comparison to our theoretical predictions. The results are plotted in a bar graph (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><p>Finally, in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, we replotted Figure 1C from <xref ref-type="bibr" rid="bib27">Hafting et al. (2005)</xref> by reading in the data using GraphClick and then translating that information back into a plot.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>NSF grants PHY-1058202, EF-0928048, PHY-1066293, and PHY11-25915 supported this work, which was completed at the Aspen Center for Physics and the Kavli Institute for Theoretical Physics. VB was also supported by the Fondation Pierre Gilles de Gennes. JP was supported by the C.V. Starr Foundation. XW conceived of the project and developed the winner-take-all framework with VB. JSP developed the probabilistic framework and two-dimensional grid optimization. VB and XW carried out simulated lesion studies. XW, JSP, and VB wrote the article.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>X-XW, Contributed to the conception and design of the theory, to the analysis and interpretation of data, and to the writing of the article, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>JP, Contributed to the conception and design of the theory, to the analysis and interpretation of data, and to the writing of the article, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>VB, Contributed to the conception and design of the theory, to the analysis and interpretation of data, and to the writing of the article, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year>2006</year><article-title>Neural correlations, population coding and computation</article-title><source>Nature Reviews Neuroscience</source><volume>7</volume><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn1888</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><year>1961</year><article-title>Possible principles underlying the transformation of sensory messages</article-title><source>Sensory Communication</source><fpage>217</fpage><lpage>234</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Hayman</surname><given-names>R</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><year>2007</year><article-title>Experience-dependent rescaling of entorhinal grids</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>682</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.1038/nn1905</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Rotermund</surname><given-names>D</given-names></name><name><surname>Pawelzik</surname><given-names>K</given-names></name></person-group><year>2002</year><article-title>Optimal short-term population coding: when fisher information fails</article-title><source>Neural Computation</source><volume>14</volume><fpage>2317</fpage><lpage>2351</lpage><pub-id pub-id-type="doi">10.1162/08997660260293247</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braun</surname><given-names>SE</given-names></name></person-group><year>1985</year><article-title>Home range and activity patterns of the giant kangaroo rat, dipodomys ingens</article-title><source>Journal of Mammalogy</source><volume>6</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.2307/1380950</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year>1992</year><article-title>The analysis of visual motion: a comparison of neuronal and psychophysical performance</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>4745</fpage><lpage>4765</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brun</surname><given-names>VH</given-names></name><name><surname>Otnæss</surname><given-names>MK</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Steffenach</surname><given-names>HA</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year>2002</year><article-title>Place cells and place recognition maintained by direct entorhinal-hippocampal circuitry</article-title><source>Science</source><volume>296</volume><fpage>2243</fpage><lpage>2246</lpage><pub-id pub-id-type="doi">10.1126/science.1071089</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brun</surname><given-names>VH</given-names></name><name><surname>Solstad</surname><given-names>T</given-names></name><name><surname>Kjelstrup</surname><given-names>KB</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year>2008</year><article-title>Progressive increase in grid scale from dorsal to ventral medial entorhinal cortex</article-title><source>Hippocampus</source><volume>18</volume><fpage>1200</fpage><lpage>1212</lpage><pub-id pub-id-type="doi">10.1002/hipo.20504</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year>2009</year><article-title>Accurate path integration in continuous attractor network models of grid cells</article-title><source>PLOS Computational Biology</source><volume>5</volume><fpage>e1000291</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000291</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bush</surname><given-names>D</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><year>2014</year><article-title>What do grid cells contribute to place cell firing?</article-title><source>Trends in Neurosciences</source><volume>37</volume><fpage>136</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2013.12.003</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coultrip</surname><given-names>R</given-names></name><name><surname>Granger</surname><given-names>R</given-names></name><name><surname>Lynch</surname><given-names>G</given-names></name></person-group><year>1992</year><article-title>A cortical model of winner-take-all competition via lateral inhibition</article-title><source>Neural Networks</source><volume>5</volume><fpage>47</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(05)80006-1</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>DE</given-names></name><name><surname>Emlen</surname><given-names>JT</given-names></name><name><surname>Stokes</surname><given-names>AW</given-names></name></person-group><year>1948</year><article-title>Studies on home range in the brown rat</article-title><source>Journal of Mammalogy</source><volume>29</volume><fpage>207</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.2307/1375387</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year>2001</year><source>Theoretical neuroscience</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Almeida</surname><given-names>L</given-names></name><name><surname>Idiart</surname><given-names>M</given-names></name><name><surname>Lisman</surname><given-names>JE</given-names></name></person-group><year>2009</year><article-title>The input–output transformation of the hippocampal granule cells: from grid cells to place fields</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>7504</fpage><lpage>7512</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6048-08.2009</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derdikman</surname><given-names>D</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year>2010</year><article-title>A manifold of spatial maps in the brain</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>561</fpage><lpage>569</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.09.004</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derdikman</surname><given-names>D</given-names></name><name><surname>Whitlock</surname><given-names>JR</given-names></name><name><surname>Tsao</surname><given-names>A</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year>2009</year><article-title>Fragmentation of grid cell maps in a multicompartment environment</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1325</fpage><lpage>1332</lpage><pub-id pub-id-type="doi">10.1038/nn.2396</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doeller</surname><given-names>CF</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><year>2010</year><article-title>Evidence for grid cells in a human memory network</article-title><source>Nature</source><volume>463</volume><fpage>657</fpage><lpage>661</lpage><pub-id pub-id-type="doi">10.1038/nature08704</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolorfo</surname><given-names>CL</given-names></name><name><surname>Amaral</surname><given-names>DG</given-names></name></person-group><year>1998</year><article-title>Entorhinal cortex of the rat: topographic organization of the cells of origin of the perforant path projection to the dentate gyrus</article-title><source>Journal of Comparative Neurology</source><volume>398</volume><fpage>25</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(19980817)398:1&lt;25::AID-CNE3&gt;3.0.CO;2-B</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>B</given-names></name><name><surname>Mørreaunet</surname><given-names>M</given-names></name><name><surname>Roudi</surname><given-names>Y</given-names></name></person-group><year>2015</year><article-title>Correlations and functional connections in a population of grid cells</article-title><source>PLOS Computational Biology</source><volume>11</volume><fpage>e1004052</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004052</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fenton</surname><given-names>AA</given-names></name><name><surname>Kao</surname><given-names>HY</given-names></name><name><surname>Neymotin</surname><given-names>SA</given-names></name><name><surname>Olypher</surname><given-names>A</given-names></name><name><surname>Vayntrub</surname><given-names>Y</given-names></name><name><surname>Lytton</surname><given-names>WW</given-names></name><name><surname>Ludvig</surname><given-names>N</given-names></name></person-group><year>2008</year><article-title>Unmasking the ca1 ensemble place code by exposures to small and large environments: more place cells and multiple, irregularly arranged, and expanded place fields in the larger space</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>11250</fpage><lpage>11262</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2862-08.2008</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiete</surname><given-names>IR</given-names></name><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Brookings</surname><given-names>T</given-names></name></person-group><year>2008</year><article-title>What grid cells convey about rat location</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>6858</fpage><lpage>6871</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5684-07.2008</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitch</surname><given-names>HS</given-names></name></person-group><year>1948</year><article-title>Habits and economic relationships of the tulare kangaroo rat</article-title><source>Journal of Mammalogy</source><volume>29</volume><fpage>5</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.2307/1375277</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Treves</surname><given-names>A</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year>2007</year><article-title>Hippocampal remapping and grid realignment in entorhinal cortex</article-title><source>Nature</source><volume>446</volume><fpage>190</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1038/nature05601</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year>2004</year><article-title>Spatial representation in the entorhinal cortex</article-title><source>Science</source><volume>305</volume><fpage>1258</fpage><lpage>1264</lpage><pub-id pub-id-type="doi">10.1126/science.1099901</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giocomo</surname><given-names>LM</given-names></name><name><surname>Hussaini</surname><given-names>SA</given-names></name><name><surname>Zheng</surname><given-names>F</given-names></name><name><surname>Kandel</surname><given-names>ER</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year>2011a</year><article-title>Grid cells use hcn1 channels for spatial scaling</article-title><source>Cell</source><volume>147</volume><fpage>1159</fpage><lpage>1170</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2011.08.051</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giocomo</surname><given-names>LM</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year>2011b</year><article-title>Computational models of grid cells</article-title><source>Neuron</source><volume>71</volume><fpage>589</fpage><lpage>603</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.07.023</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year>2005</year><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><volume>436</volume><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1038/nature03721</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hales</surname><given-names>JB</given-names></name><name><surname>Schlesinger</surname><given-names>MI</given-names></name><name><surname>Leutgeb</surname><given-names>JK</given-names></name><name><surname>Squire</surname><given-names>LR</given-names></name><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Clark</surname><given-names>RE</given-names></name></person-group><year>2014</year><article-title>Medial entorhinal cortex lesions only partially disrupt hippocampal place cells and hippocampus-dependent memory</article-title><source>Cell Reports</source><volume>9</volume><fpage>893</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2014.10.009</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>J</given-names></name><name><surname>Weidemann</surname><given-names>CT</given-names></name><name><surname>Miller</surname><given-names>JF</given-names></name><name><surname>Solway</surname><given-names>A</given-names></name><name><surname>Burke</surname><given-names>JF</given-names></name><name><surname>Wei</surname><given-names>XX</given-names></name><name><surname>Suthana</surname><given-names>N</given-names></name><name><surname>Sperling</surname><given-names>MR</given-names></name><name><surname>Sharan</surname><given-names>AD</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><year>2013</year><article-title>Direct recordings of grid-like neuronal activity in human spatial navigation</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1188</fpage><lpage>1190</lpage><pub-id pub-id-type="doi">10.1038/nn.3466</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Killian</surname><given-names>NJ</given-names></name><name><surname>Jutras</surname><given-names>MJ</given-names></name><name><surname>Buffalo</surname><given-names>EA</given-names></name></person-group><year>2012</year><article-title>A map of visual space in the primate entorhinal cortex</article-title><source>Nature</source><volume>491</volume><fpage>761</fpage><lpage>764</lpage><pub-id pub-id-type="doi">10.1038/nature11587</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krupic</surname><given-names>J</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name><name><surname>OKeefe</surname><given-names>J</given-names></name></person-group><year>2012</year><article-title>Neural representations of location composed of spatially periodic bands</article-title><source>Science</source><volume>337</volume><fpage>853</fpage><lpage>857</lpage><pub-id pub-id-type="doi">10.1126/science.1222403</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubie</surname><given-names>JL</given-names></name><name><surname>Fox</surname><given-names>SE</given-names></name></person-group><year>2015</year><article-title>Do the spatial frequencies of grid cells mold the firing fields of place cells?</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>112</volume><fpage>3860</fpage><lpage>3861</lpage><pub-id pub-id-type="doi">10.1073/pnas.1503155112</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kuhn</surname><given-names>HW</given-names></name><name><surname>Tucker</surname><given-names>AW</given-names></name></person-group><year>1951</year><source>Nonlinear programming. In proceedings of the second Berkeley symposium on mathematical statistics and probability</source><volume>Volume 5</volume><publisher-name>University of California Press</publisher-name><publisher-loc>Berkeley, California</publisher-loc></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Leutgeb</surname><given-names>JK</given-names></name><name><surname>Barnes</surname><given-names>CA</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year>2005</year><article-title>Independent codes for spatial and episodic memory in hippocampal neuronal ensembles</article-title><source>Science</source><volume>309</volume><fpage>619</fpage><lpage>623</lpage><pub-id pub-id-type="doi">10.1126/science.1114037</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year>2000</year><article-title>On the computational power of winner-take-all</article-title><source>Neural Computation</source><volume>12</volume><fpage>2519</fpage><lpage>2535</lpage><pub-id pub-id-type="doi">10.1162/089976600300014827</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Herz</surname><given-names>AV</given-names></name><name><surname>Stemmler</surname><given-names>M</given-names></name></person-group><year>2012a</year><article-title>Optimal population codes for space: grid cells outperform place cells</article-title><source>Neural Computation</source><volume>24</volume><fpage>2280</fpage><lpage>2317</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00319</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Herz</surname><given-names>AV</given-names></name><name><surname>Stemmler</surname><given-names>MB</given-names></name></person-group><year>2012b</year><article-title>Resolution of nested neuronal representations can be exponential in the number of neurons</article-title><source>Physical Review Letters</source><volume>109</volume><fpage>018103</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.109.018103</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Herz</surname><given-names>AV</given-names></name><name><surname>Stemmler</surname><given-names>MB</given-names></name></person-group><year>2013</year><article-title>Multiscale codes in the nervous system: the problem of noise correlations and the ambiguity of periodic scales</article-title><source>Physical Review E</source><volume>88</volume><fpage>022713</fpage><pub-id pub-id-type="doi">10.1103/PhysRevE.88.022713</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Kropff</surname><given-names>E</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year>2008</year><article-title>Place cells, grid cells, and the brain's spatial representation system</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>69</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.31.061307.090723</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mulders</surname><given-names>WH</given-names></name><name><surname>West</surname><given-names>MJ</given-names></name><name><surname>Slomianka</surname><given-names>L</given-names></name></person-group><year>1997</year><article-title>Neuron numbers in the presubiculum, parasubiculum, and entorhinal area of the rat</article-title><source>Journal of Comparative Neurology</source><volume>385</volume><fpage>83</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(19970818)385:1&lt;83::AID-CNE5&gt;3.0.CO;2-8</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname><given-names>J</given-names></name></person-group><year>1976</year><article-title>Place units in the hippocampus of the freely moving rat</article-title><source>Experimental Neurology</source><volume>51</volume><fpage>78</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1016/0014-4886(76)90055-8</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O'Keefe</surname><given-names>J</given-names></name><name><surname>Nadel</surname><given-names>L</given-names></name></person-group><year>1978</year><source>The hippocampus as a cognitive map</source><publisher-name>Clarendon Press</publisher-name><publisher-loc>Oxford</publisher-loc></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ormond</surname><given-names>J</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year>2015</year><article-title>Place field expansion after focal mec inactivations is consistent with loss of fourier components and path integrator gain reduction</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>112</volume><fpage>4116</fpage><lpage>4121</lpage><pub-id pub-id-type="doi">10.1073/pnas.1421963112</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rich</surname><given-names>PD</given-names></name><name><surname>Liaw</surname><given-names>HP</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name></person-group><year>2014</year><article-title>Large environments reveal the statistical structure governing hippocampal representations</article-title><source>Science</source><volume>345</volume><fpage>814</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1126/science.1255635</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sargolini</surname><given-names>F</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year>2006</year><article-title>Conjunctive representation of position, direction, and velocity in entorhinal cortex</article-title><source>Science</source><volume>312</volume><fpage>758</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1126/science.1125572</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sasaki</surname><given-names>T</given-names></name><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Leutgeb</surname><given-names>JK</given-names></name></person-group><year>2015</year><article-title>Spatial and memory circuits in the medial entorhinal cortex</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>16</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.10.008</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year>2001</year><article-title>Natural image statistics and neural representation</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slade</surname><given-names>NA</given-names></name><name><surname>Swihart</surname><given-names>RK</given-names></name></person-group><year>1983</year><article-title>Home range indices for the hispid cotton rat (sigmodon hispidus) in northeastern kansas</article-title><source>Journal of Mammalogy</source><volume>64</volume><fpage>580</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.2307/1380513</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solstad</surname><given-names>T</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Einevoll</surname><given-names>GT</given-names></name></person-group><year>2006</year><article-title>From grid cells to place cells: a mathematical model</article-title><source>Hippocampus</source><volume>16</volume><fpage>1026</fpage><lpage>1031</lpage><pub-id pub-id-type="doi">10.1002/hipo.20244</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Yoon</surname><given-names>H</given-names></name><name><surname>Kang</surname><given-names>K</given-names></name><name><surname>Shamir</surname><given-names>M</given-names></name></person-group><year>2001</year><article-title>Population coding in neuronal systems with correlated noise</article-title><source>Physical Review E</source><volume>64</volume><fpage>051904</fpage><pub-id pub-id-type="doi">10.1103/PhysRevE.64.051904</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sreenivasan</surname><given-names>S</given-names></name><name><surname>Fiete</surname><given-names>I</given-names></name></person-group><year>2011</year><article-title>Grid cells generate an analog error-correcting code for singularly precise neural computation</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1330</fpage><lpage>1337</lpage><pub-id pub-id-type="doi">10.1038/nn.2901</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stensola</surname><given-names>H</given-names></name><name><surname>Stensola</surname><given-names>T</given-names></name><name><surname>Solstad</surname><given-names>T</given-names></name><name><surname>Frøland</surname><given-names>K</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year>2012</year><article-title>The entorhinal grid map is discretized</article-title><source>Nature</source><volume>492</volume><fpage>72</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1038/nature11649</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steward</surname><given-names>O</given-names></name><name><surname>Scoville</surname><given-names>SA</given-names></name></person-group><year>1976</year><article-title>Cells of origin of entorhinal cortical afferents to the hippocampus and fascia dentata of the rat</article-title><source>Journal of Comparative Neurology</source><volume>169</volume><fpage>347</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1002/cne.901690306</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stickel</surname><given-names>LF</given-names></name><name><surname>Stickel</surname><given-names>WH</given-names></name></person-group><year>1949</year><article-title>A sigmodon and baiomys population in ungrazed and unburned texas prairie</article-title><source>Journal of Mammalogy</source><fpage>141</fpage><lpage>150</lpage><pub-id pub-id-type="doi">10.2307/1375262</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rayleigh</surname><given-names>JWS</given-names></name></person-group><year>1896</year><source>The theory of sound</source><volume>Volume 2</volume><publisher-name>Macmillan</publisher-name><publisher-loc>London</publisher-loc></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thue</surname><given-names>A</given-names></name></person-group><year>1892</year><article-title>Om nogle geometrisk-taltheoretiske theoremer</article-title><source>Forhandlinger ved de Skandinaviske Naturforskeres</source><fpage>352</fpage><lpage>353</lpage></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name></person-group><year>1948</year><article-title>Cognitive maps in rats and men</article-title><source>Psychological Review</source><volume>55</volume><fpage>189</fpage><pub-id pub-id-type="doi">10.1037/h0061626</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Towse</surname><given-names>BW</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Bush</surname><given-names>D</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><year>2014</year><article-title>Optimal configurations of spatial scale for grid cell firing under noise and uncertainty</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>369</volume><fpage>20130290</fpage><pub-id pub-id-type="doi">10.1098/rstb.2013.0290</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Strien</surname><given-names>NM</given-names></name><name><surname>Cappaert</surname><given-names>NL</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name></person-group><year>2009ssss</year><article-title>The anatomy of memory: an interactive overview of the parahippocampal–hippocampal network</article-title><source>Nature Reviews Neuroscience</source><volume>10</volume><fpage>272</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.1038/nrn2614</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yartsev</surname><given-names>MM</given-names></name><name><surname>Ulanovsky</surname><given-names>N</given-names></name></person-group><year>2013</year><article-title>Representation of three-dimensional space in the hippocampus of flying bats</article-title><source>Science</source><volume>340</volume><fpage>367</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1126/science.1235338</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yartsev</surname><given-names>MM</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Ulanovsky</surname><given-names>N</given-names></name></person-group><year>2011</year><article-title>Grid cells without theta oscillations in the entorhinal cortex of bats</article-title><source>Nature</source><volume>479</volume><fpage>103</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1038/nature10583</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zohary</surname><given-names>E</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year>1994</year><article-title>Correlated neuronal discharge rate and its implications for psychophysical performance</article-title><source>Nature</source><volume>370</volume><fpage>140</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1038/370140a0</pub-id></element-citation></ref></ref-list><app-group><app id="app1"><title>Appendix 1</title><boxed-text><sec id="s6" sec-type="appendix"><title>Range of location coding in a grid system</title><p>The main text describes hierarchical grid coding schemes where the larger periods resolve ambiguity and smaller periods give precision in location coding. We took the largest grid period to be comparable to the behavioral range. In fact, if the periods <italic>λ</italic><sub><italic>i</italic></sub> of the different modules are incommensurate with each other (i.e., they do not share common integer factors), it should be possible to resolve location over ranges larger than the largest grid period (<xref ref-type="bibr" rid="bib21">Fiete et al., 2008</xref>; <xref ref-type="bibr" rid="bib51">Sreenivasan and Fiete, 2011</xref>). The grid schemes that we predict share this virtue since they predict scale ratios that are not simple rational numbers. However, the precise maximum range will also depend on the widths of the grid fields <italic>l</italic><sub><italic>i</italic></sub> relative to the period and on the number of grid cells <italic>n</italic><sub><italic>i</italic></sub> in each module. In the probabilistic decoding scheme described in the main text, these parameters determine the standard deviation <italic>σ</italic><sub><italic>i</italic></sub> of the periodic peaks in the likelihood of position given the activity in module <italic>i</italic>. The full range of unambiguous location representation depends on the ratios <italic>λ</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub>. Increasing this ratio will tend to increase the range of unambiguous representation, but at the cost of increasing the number of cells in each module.</p><p>To illustrate, consider a one-dimensional grid system with four modules with a ratio of 2.7 between adjacent scales (this is close to the optimal ratio predicted by our analysis). Suppose the animal's true location is at 0. We can calculate the overall probability of the animal's location by multiplying together the likelihood functions resulting from activity in each individual module (see main text for details). We will examine the extent to which location can be decoded unambiguously over a range (−3<italic>λ</italic><sub><italic>max</italic></sub>, 3<italic>λ</italic><sub><italic>max</italic></sub>) where <italic>λ</italic><sub>max</sub> is the larges period. When <italic>λ</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub> is close to the value of 9.1 predicted by the probabilistic analysis in Optimizing the grid system: probabilistic decoder, ‘Materials and methods’, the overall likelihood shows substantial ambiguity over this range because of secondary peaks in the likelihood distribution (<xref ref-type="fig" rid="fig6">Appendix figure 1A</xref>). As <italic>λ</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub> increases (requiring more neurons in each module), these secondary peaks decrease in amplitude. In <xref ref-type="fig" rid="fig6">Appendix figure 1B</xref>, we show that when <italic>λ</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub> = 30, the 4-module grid system can represent location at least within the range (−3<italic>λ</italic><sub><italic>max</italic></sub>, 3<italic>λ</italic><sub><italic>max</italic></sub>).<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.08362.008</object-id><label>Appendix figure 1.</label><caption><title>Encoding range can exceed the period of the largest grid module at a cost in the number of neurons.</title><p>Assume that the animal is located is at 0. (<bold>A</bold>) Top, the likelihood resulting from the largest grid module, where the standard deviation of the Gaussian peaks is <inline-formula><mml:math id="inf13"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>9.1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> of the grid period (<italic>λ</italic><sub><italic>max</italic></sub> = 1000). Bottom, the inferred distribution over location after pooling over 4-grid modules related by a scale factor of 2.7. As shown, this 4-module grid system shows ambiguities in location coding outside the range [<italic>λ</italic><sub><italic>max</italic></sub>, <italic>λ</italic><sub><italic>max</italic></sub>]. (<bold>B</bold>) Top, the likelihood resulting from the largest grid module, where the standard deviation of the Gaussian peaks is <inline-formula><mml:math id="inf14"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>30</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> of the grid period (<italic>λ</italic><sub><italic>max</italic></sub> = 1000). Bottom, the inferred distribution over location after pooling over four grid modules related by a scale factor of 2.7. As shown, this 4-module grid system provides a good representation over a range of at least [−3000, 3000] = [−3<italic>λ</italic><sub><italic>max</italic></sub>, 3<italic>λ</italic><sub><italic>max</italic></sub>].</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.08362.008">http://dx.doi.org/10.7554/eLife.08362.008</ext-link></p></caption><graphic xlink:href="elife-08362-app1-fig1-v2.tif"/></fig></p><p>If there is a biological limitation to the largest period possible in a grid system, and if the organism must represent very large ranges without grid remapping, it may prove beneficial to add neurons to expand range. Analyzing this trade-off requires knowledge of the range, biophysical limits on grid periods, and the degree of ambiguity (the maximum heights of secondary peaks in the probability of position) that can be behaviorally tolerated. This information is not currently available for any species, and so we do not attempt the analysis.</p><sec id="s6-1"><title>Predictions for the effects of lesions and for place cell activity</title><p>In the grid coding scheme that we propose there is a hierarchy of grid periods governed by a geometric progression. The alternative schemes of <xref ref-type="bibr" rid="bib21">Fiete et al. (2008)</xref>; <xref ref-type="bibr" rid="bib51">Sreenivasan and Fiete (2011)</xref> are designed to produce a large range of representation from grids with <italic>similar</italic> periods. These two alternatives make very different predictions for the effects of lesions in the entorhinal cortex on location coding. In a hierarchical scheme, losing a grid module produces location ambiguities that increase in size with the period of the missing module. In the alternative scheme of <xref ref-type="bibr" rid="bib21">Fiete et al. (2008)</xref>; <xref ref-type="bibr" rid="bib51">Sreenivasan and Fiete (2011)</xref> lesions of a module produce periodic ambiguities that are sporadically tied to the missing period. An illustrative example is shown in <xref ref-type="fig" rid="fig7">Appendix figure 2</xref>.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.08362.009</object-id><label>Appendix figure 2.</label><caption><title>The effect of lesioning grid modules on the distribution over location for hierarchical vs non-hierarchical grid schemes.</title><p>For the hierarchical scheme, we assume that four one-dimensional grid modules are related by a scale factor <italic>r</italic> (<italic>r</italic> = 2.7), that is, <inline-formula><mml:math id="inf2"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>2.7</mml:mn></mml:mrow></mml:math></inline-formula>, <italic>i</italic> = 1, 2, 3, and the ratio <inline-formula><mml:math id="inf3"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>9.1</mml:mn></mml:mrow></mml:math></inline-formula>, i = 1, 2, 3, 4. We assume that the animal is at <italic>x</italic> = 0 and construct the probability distribution over location given the activity in each grid module as described in Optimizing the grid system: probabilistic decoder, ‘Materials and methods’. For the non-hierarchical scheme, we again assume four grid modules and set the periods of the four modules to be 1/105 (fourth), 1/70 (third), 1/42 (second), 1/30 (first) of the whole range, respectively. We set the width of the composite likelihood after combining all four modules to be 1/210 of the range [−5000, 5000].</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.08362.009">http://dx.doi.org/10.7554/eLife.08362.009</ext-link></p></caption><graphic xlink:href="elife-08362-app1-fig2-v2.tif"/></fig></p><p>The grid cell representation of space in the entorhinal cortex is related in a complex manner to the hippocampal place cell representation (<xref ref-type="bibr" rid="bib11">Bush et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Sasaki et al., 2015</xref>). Simplistic models of this transformation assume that grid cells are pooled in the hippocampus and that some form of synaptic plasticity selects inputs with the same spatial phase (<xref ref-type="bibr" rid="bib49">Solstad et al., 2006</xref>). In the context of such a model (which does not reflect many aspects of the known physiology), our grid scheme makes specific predictions for the effects of module lesions on place fields.</p><p>We use a firing rate model for both place cells and grid cells. The 1-d grid cell firing rate is modeled as a periodic sum of truncated Gaussians (a full Gaussian mixture model gives similar results but the truncated model is easier to handle numerically). We will consider four grid modules with module periods <italic>λ</italic><sub><italic>i</italic></sub>, Gaussian standard deviations <italic>σ</italic><sub><italic>i</italic></sub> of the bump of the grid cell tuning curve, and ratios <italic>λ</italic><sub><italic>i</italic></sub>/<italic>σ</italic><sub><italic>i</italic></sub> = 9.1. The grid periods follow a scaling <italic>λ</italic><sub><italic>i</italic></sub>/<italic>λ</italic><sub><italic>i</italic> + 1</sub> = 2.7, and we examine place coding over the range set by the biggest period <italic>λ</italic><sub>1</sub>.</p><p>The place cell response is modeled via linear pooling of grid cells with the same phase followed by a threshold and an exponential nonlinearity:<disp-formula id="equ36"><mml:math id="m36"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mo>∗</mml:mo><mml:mi>m</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here, <italic>g</italic><sub><italic>i</italic></sub>(<italic>x</italic>) is the grid cell firing rate, <italic>c</italic> = 0.3 sets the threshold and <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle></mml:mrow><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:msubsup><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the maximum activation. This is a simplified description of the essential features of many models of the grid-place transformation (see, e.g., [<xref ref-type="bibr" rid="bib49">Solstad et al., 2006</xref>; <xref ref-type="bibr" rid="bib1">de Almeida et al., 2009</xref>] and the review [<xref ref-type="bibr" rid="bib26">Giocomo et al., 2011b</xref>]). To model the effect of lesioning grid module <italic>i</italic>, we set the <italic>g</italic><sub><italic>i</italic></sub>(<italic>x</italic>) = 0. The results are shown in the <xref ref-type="fig" rid="fig8">Appendix figure 3</xref>. Qualitatively, lesioning the smallest grid module increases the place cell width, while lesioning the largest grid module leads to increased firing in locations outside the main place fields. In general, lesioning different grid modules along the hierarchy leads to different effects on the place field. This is a testable prediction in future experiments. Note that lesions of dorsal-ventral bands are not a direct test—multiple grid modules co-exist in each location along the dorsal-ventral axis (<xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref>).<fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.08362.010</object-id><label>Appendix figure 3.</label><caption><title>The effect of lesioning individual grid modules on place cell activity in a simple grid-place transformation model.</title><p>Lesioning different modules leads to qualitatively different effects on the place cell response in the hierarchical coding scheme we proposed, as compared to a non-hierarchical scheme. See ‘Predictions for the effects of lesions and for place cell activity’, <xref ref-type="app" rid="app1">Appendix 1</xref> for details.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.08362.010">http://dx.doi.org/10.7554/eLife.08362.010</ext-link></p></caption><graphic xlink:href="elife-08362-app1-fig3-v2.tif"/></fig></p><p>For comparison purposes, we also simulated a non-hierarchical model where grid periods are similar but incommensurate. In this model, the place cell response is<disp-formula id="equ32"><mml:math id="m32"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:munderover><mml:mtext> </mml:mtext><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mtext>∗</mml:mtext><mml:mrow><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mn>0</mml:mn><mml:mo stretchy="true">~</mml:mo></mml:mover></mml:mrow><mml:mn>.35</mml:mn></mml:mrow></mml:math></inline-formula> is a threshold, <inline-formula><mml:math id="inf17"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle></mml:mrow><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:msubsup><mml:mtext> </mml:mtext><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf18"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the grid cell firing rate again modeled as a sum of truncated Gaussians. In each module, we took the standard deviation of the Gaussians to be 1/210 of the whole range. The periods of the grids in the four modules were 1/105 (forth), 1/70 (third), 1/42 (second), 1/30 (first) of the whole range respectively. Again, to model the effect of lesioning grid module <italic>i</italic>, we set the <inline-formula><mml:math id="inf19"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. In this grid scheme, lesioning any grid module leads to qualitatively similar effects on the place cell activity, as they all lead to the emergence of several place fields (<xref ref-type="fig" rid="fig8">Appendix figure 3</xref>). This is in contrast with the hierarchical scheme, in which lesioning the largest scale leads to an expansion of place fields rather than an increase in the number of fields.</p></sec></sec></boxed-text></app></app-group></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.08362.011</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Skinner</surname><given-names>Frances K</given-names></name><role>Reviewing editor</role><aff><institution>University Health Network</institution>, <country>Canada</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for submitting your work entitled “A principle of economy predicts the functional architecture of grid cells” for peer review at <italic>eLife</italic>. Your submission has been favorably evaluated by Timothy Behrens (Senior Editor), a Reviewing Editor, and three reviewers.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>In this study, the authors present a simpler, more elegant, and more intuitive presentation (relative to other studies) of coding optimality principles/constraints leading to grid cell formation. While there was agreement about this, two main aspects came forth that should be addressed in a revised submission:</p><p>1) Be clear and specific about how this work differs from other studies (e.g. Mathis, Herz et al.). That is, the analysis in the current paper is simpler and does not rely on Fisher information, but rather on quite straightforward assumptions related to the nature of optimal coding.</p><p>2) Be clear about how the novel analyses presented (as different from other studies) allow a closer link with experimental studies.</p><p>More details of revisions are given below.</p><p><italic>Reviewer #1:</italic></p><p>The authors present a theoretical analysis of the ideal properties that a “grid coding” system for spatial location should exhibit, by assessing the conditions which allow location to be encoded with maximum precision across a specific spatial range using the minimum number of grid cells. Unlike several previous theoretical studies (most notably, <xref ref-type="bibr" rid="bib21">Fiete et al., 2008</xref>), the authors assume that this spatial range is equal to the scale of the largest grid module (i.e. ∼10m), rather than exploiting the combinatorial properties of the grid cell code to encode location over a range equal to the lowest common multiple of all grid scales. Thus, although a similar topic has been addressed in various guises by several previous publications (i.e. <xref ref-type="bibr" rid="bib21">Fiete et al., 2008</xref>; Mathis et al., 2012, 2013; <xref ref-type="bibr" rid="bib58">Towse et al., 2014</xref>), there is some novelty to the current work. The manuscript is well written, thorough, and makes some interesting and specific predictions (such as the optimal ratio between grid field size and grid module scale) that have not – to the best of my knowledge – been described elsewhere. Concerns to be addressed are described below.</p><p>Specific comments:</p><p>In the subsection “Intuitions from a simplified model”, the current body of experimental data in this field simply does not support the authors’ repeated assertion that “[…] anatomical and functional evidence suggests that place cells selectively read out contiguous subsets of entorhinal grid modules along the dorsoventral axis (Van Strien, Cappaert and Witter, 2009; Solstad, Moser and Einevoll, 2006)”. First, citations Van Strien, Cappaert and Witter, 2009 and Solstad, Moser and Einevoll, 2006 are an anatomical review and a theoretical paper, respectively – neither of which can reasonably be described as providing “functional evidence”. Second, several groups have recently published review papers summarising a wide body of functional evidence that directly contradicts the hypothesis that place cells ‘selectively read-out’ a subset of grid cell inputs (i.e. <xref ref-type="bibr" rid="bib11">Bush et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Sasaki et al., 2015</xref>). The authors should edit the text here, and at several other junctures throughout the manuscript (listed below) to address this point. The hypothesis that place cells represent a read-out of the grid cell system has no bearing on the theoretical work presented, and only serves to misrepresent our current understanding of the grid cell system.</p><p>In the Discussion, the authors state that: “Together with the anatomy (Van Strien, Cappaert and Witter, 2009), the hierarchical view of location coding that we have proposed then predicts that dorsal place cells should be revealed to have multiple place fields in large environments because their spatial ambiguities will not be fully resolved at larger scales. Preliminary evidence for this prediction has appeared in <xref ref-type="bibr" rid="bib20">Fenton et al., 2008</xref>; Rich, Liaw and Lee, 2014.” However, those studies show no systematic relationship between the locations of dorsal place cell's multiple firing fields in large environments, which directly contradicts the predictions of a grid cell to place cell model (see, for example, <xref ref-type="fig" rid="fig8">Appendix–figure 3</xref> and Appendix–figure 4 in this manuscript). The authors should note this caveat– along with the other experimental data showing that a grid to place cell model is overly simplistic (see above) – or remove the corresponding piece of text.</p><p>In the Discussion, the authors “[…] predict that lesioning the modules with small periods will expand place field widths, while lesioning modules with large periods will lead to increased firing at locations outside the main place field, at scales set by the missing module. Our prediction is supported by a recent study demonstrating effects of lesions including dorsal mEC on place field widths in small environments (<xref ref-type="bibr" rid="bib28">Hales et al., 2014</xref>)”. This is misleading for several reasons. First, as described above, this statement neglects to mention the wider body of evidence contradicting the hypothesis that grid cell inputs solely generate place cell firing fields. Second, the aim of the cited study (<xref ref-type="bibr" rid="bib28">Hales et al., 2014</xref>) was to examine the effect of eliminating all grid cell inputs to place cells, and ∼85% of the total mEC volume was ablated, including 94.6% of layer II and 83.5% of layer III. Are the authors suggesting that the observed effects on place cell firing are a result of remaining grid cell inputs from modules with a large or small period? Third, the more specific prediction is that lesioning grid modules with large periods will lead to the appearance of additional place fields at periodic, grid-like locations in two dimensions, but that analysis is not made or discussed in <xref ref-type="bibr" rid="bib28">Hales et al. (2014)</xref>. Fourth, more recent experimental evidence indicates that focal inactivations of dorsal or ventral mEC each produce place field expansion, and neither generated increased firing at locations outside the main place field – in fact, that data showed a trend towards a decrease in the number of firing fields exhibited by each place cell following focal inactivations (Figure S7 in Ormond et al., 2015). Each of those results also contradict the predictions of a grid cell to place cell model, despite the strange interpretation of the data made in that paper. Hence, the authors should include a citation to that paper and edit the text accordingly.</p><p>In the Appendix, the authors again suggest that “the grid cell representation of space in the entorhinal cortex is […] transformed in the hippocampus into the place cell representation.” They should edit this text to more accurately represent the current understanding of the relationship between grid and place cell firing patterns.</p><p>Abstract and Introduction: Given that the principal difference between the analysis in this manuscript and that presented in several previous publications (i.e. <xref ref-type="bibr" rid="bib21">Fiete et al., 2008</xref>; <xref ref-type="bibr" rid="bib58">Towse et al., 2014</xref>) is that the authors assume a spatial range equal to the size of the largest grid module, this should be more explicitly stated in the Abstract and Introduction. This would make the novelty of this manuscript more apparent. For example, the Abstract should be edited to read: “We propose that the grid system implements a hierarchical code for spatial location that economizes the number of neurons required to encode location with a given resolution across a spatial range equal in size to the period of the largest grid module” or similar. Likewise, the Introduction should be edited to read: “minimize the number of neurons required to achieve the behaviourally necessary spatial resolution across a spatial range equal in size to the period of the largest grid module” or similar.</p><p>In the Introduction, it is not clear to me what the authors mean by the following statement: “Consistent with studies of grid cell and place cell remapping, our analyses assume that there is a behaviorally defined maximum range over which a fixed grid represents locations (<xref ref-type="bibr" rid="bib24">Fyhn et al., 2007</xref>).” I fail to see the relevance of remapping to the behavioural range of an animal. Could the authors explain their rationale here please?</p><p>In the Introduction, when the authors state “three dimensional grids that will be relevant to navigation in, e.g., bats”, they should include a reference to <xref ref-type="bibr" rid="bib61">Yartsev et al., 2011</xref>, which demonstrates that bats do have grid cell responses, even though they have only been recorded in two dimensional environments so far.</p><p>In the subsection “Intuitions from a simplified model”, when the authors stress that “the animal could achieve the required resolution in a place coding scheme […]”, they must incorporate a reference to <xref ref-type="bibr" rid="bib21">Fiete et al., 2008</xref>, which makes a very similar comparison of “place coding” and “grid coding” schemes.</p><p><xref ref-type="fig" rid="fig2">Figure 2E</xref> is cited before <xref ref-type="fig" rid="fig2">Figures 2A-D</xref> in the main text, which is confusing (subsection “Winner-Take All Decoder”). Similarly, in subsection “General grid coding in two dimensions”, <xref ref-type="fig" rid="fig3">Figure 3A</xref> is not referred to in the text at all, and <xref ref-type="fig" rid="fig3">Figures 3B and 3C</xref> are cited before <xref ref-type="fig" rid="fig2">Figure 2F</xref>, which is not ideal. It would be preferable if the authors placed all figures pertaining to the 2D case in <xref ref-type="fig" rid="fig3">Figure 3</xref> (i.e. move <xref ref-type="fig" rid="fig2">Figure 2F</xref> into <xref ref-type="fig" rid="fig3">Figure 3</xref>) and moved <xref ref-type="fig" rid="fig2">Figure 2E</xref> to match the flow of the text (i.e. before <xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><p>In the Discussion, the authors state: “Given homogeneous positive noise correlations within a grid module, which will arise naturally if grid cells are formed by an attractor mechanism, the required number of neurons could be an order of magnitude higher (<xref ref-type="bibr" rid="bib50">Sompolinsky et al., 2001</xref>; Averbeck, Latham and Pouget, 2006)”. It has recently been demonstrated that positive noise correlations appear to be largely absent in the rodent grid cell system, and the authors may wish to note this point and cite the corresponding paper (<xref ref-type="bibr" rid="bib38">Mathis et al., 2013</xref>) here.</p><p>In the Discussion, “the answer depends on the dimension of the grid” should be “the answer depends on the dimensionality of the grid”.</p><p>Again, in the Discussion, the authors mention that they “have checked that the optimal grid scheme predicted by our theory, if decoded in the fashion of (<xref ref-type="bibr" rid="bib21">Fiete et al., 2008</xref>), can represent space over ranges longer than the largest scale”, but do not mention (in the main text) whether it could or not. They should incorporate a brief description of the outcome of those simulations in this section of text, for clarity.</p><p><italic>Reviewer #2:</italic></p><p>The paper by Wei et al. uses an optimality argument to explain the empirically observed geometric progression of the spacing of grid modules and the ratio of this geometric progression. The paper is nicely written and the arguments are clear. I am however not fully convinced about the potential influence of this paper based on the following reasons.</p><p>1) The major assumption of this paper is the ambiguity of the grid cell firing, that is, from the firing of one grid cell, it is not possible to infer in which of the many vertices of the grid one is located. This assumption, however, does not take into account the fact that the peak firing rate of a grid cell at its fields significantly vary. In other words, the translational symmetry is about the positions at which peak firing occurs, not that each field is identical to the other in terms of firing rate. In my view, this experimental fact fundamentally affects the argument offered here.</p><p>2) The minimum of the cost function versus ratio of the spacing of successive modules is very wide, raising the question whether one can really say anything meaningful about the value that that ratio should take. It should not escape our attention that in <xref ref-type="fig" rid="fig2">Figure 2E and F</xref>, the authors plot the cost function versus the logarithm of the ratio between successive modules, which gives the impression of a narrower minimum (though still wide). Even with this, the authors’ prediction is stated to “[…] robustly lie in the rage 1.4-1.7[…]”. This is a 20% range.</p><p>3) (a) The idea of using optimality for predicting the ratio of grid spacing of the modules has been already employed by Mathis et al., Neural Comp 2012. There are differences between the two works, e.g. Mathis et al. maximize the resolution given a fixed number of neurons while Wei et al. minimize the number of neurons given the resolution and Mathis et al. only focus on the one dimensional case. Despite the differences, it is not clear what is the major conceptual advancement. As far as I can say, the argument of Mathis et al. can be easily extended to 2D to produce a geometric progression.</p><p>(b) It is true that, as stated in the in the conclusion, in the work of Mathis et al. the optimal ratio depends on “the number of neurons per module and peak firing rate”. But the prediction of the optimal ratio here also varies over a wide range, depending on the assumption on the decoding scheme (and probably the shape of the tuning curves, assumption on the correlation between neurons etc.).</p><p><italic>Reviewer #3:</italic></p><p>This excellent paper uses a very simple principle for demonstrating that coding of grid cells is better than coding of place cells, and generates some postdictions following this simple principle. The basic idea is that grid cells act as a kind of “Base-b” representation of space, and it is shown that the representation is optimal when the base chosen is base e (2.71828…). From that, various postdictions follow (which conform nicely with known experiments). Specifically, grid cell modules have a constant scale ratio, which should be √<italic>e</italic> in the simplest model, and closer to the real experimental value (1.4) in a probabilistic model of the cells coding. Furthermore, there should be a certain optimal ratio between the grid field width and the spacing between grid points.</p><p>The paper interacts nicely the papers of the group of Andreas Herz, which deal with similar issues using Fisher information. I have no major concerns, as I think the paper is well written, deals with an important subject, looks sound mathematically, and has a nice treatment of relation to experimental data.</p><p>The only issue I would like to be dealt with is to make the Discussion more clear as to the relation between this paper and the papers from the Herz group (including the relevant recent one from 2015). Specifically, they have a treatment of the issue of grid cell coding through Fisher information, and it could be of value to connect the work performed here to their line of thought, at least minimally by adding some discussion to the paper (elaborating on the existing paragraph).</p><p>Another small question I am curious about is whether the winner-take-all decoder could be seen as a limit-case of the probabilistic decoder. But if that is the case, I do not completely understand the “leap” from <italic>e</italic> to 2.4.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.08362.012</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>In this study, the authors present a simpler, more elegant, and more intuitive presentation (relative to other studies) of coding optimality principles/constraints leading to grid cell formation. While there was agreement about this, two main aspects came forth that should be addressed in a revised submission</italic>.</p><p><italic>1) Be clear and specific about how this work differs from other studies (e.g., Mathis, Herz et al.). That is, the analysis in the current paper is simpler and does not rely on Fisher information, but rather on quite straightforward assumptions related to the nature of optimal coding</italic>.</p><p>To respond to this recommendation, we have expanded our discussion of what sets our work apart from others, especially Mathis et al. The comments of the referees have been helpful in this regard. We have attempted to be clear and specific that Mathis et al. explored grid coding in one dimension using Fisher information and numerical simulation to explore decoding error. They found that the set of periods that maximizes the Fisher information is approximated by a geometric series in a regime of large-scale ratios. By contrast, we rely on a simpler formulation of optimal coding with straightforward assumptions about tradeoffs between ambiguity and resolution in a hierarchical grid. We take a simpler definition of the resolution (as the largest scale divided by the smallest scale the system can discriminate), assume that the grid encodes location with a restricted range, and then seek to minimize the resources (number of neurons) required to achieve a given resolution within this range. Our simpler formulation allows us to extend our analysis to any number of dimensions, and to predict the values of structural parameters of the grid such as the ratio between periods, the grid geometry etc. We have added substantially to the Discussion to address these points.</p><p>In more detail, the Fisher information approximation to position error in Mathis et al. is only valid over a certain range of parameters. They introduce a no-ambiguity constraint to keep them within this range, but this creates two challenges for an optimization procedure: (1) The optimum depends on the details of the constraint, which was somewhat arbitrarily chosen and dependent on the variability, the tuning curve shape of grid cells and a “tolerable error level”, and (2) The optimum turns out to saturate the constraint, so for some choices of the constraint the procedure is pushed right to the edge of where the Fisher information is a valid approximation at all, causing difficulties for the self-consistency of the procedure. Because of these limits on the Fisher information approximation, Mathis et al. proceed to measure decoding error directly through their numerical studies. But here a complete optimization is not possible because there are too many inter-related parameters. This last point is a limitation of any numerical study. In contrast, we estimated decoding error directly by working with approximated forms of the posteriors rather than by approximating decoding error in terms of the Fisher information. For the winner-take-all analysis, we effectively approximate posteriors as periodic boxcar functions, for the probabilistic analysis as periodic sums-of-Gaussians. These choices allow analytical treatment of the optimization problem over a much wider parameter range without requiring arbitrary hand-imposed constraints.</p><p>While going through the paper of Mathis and collaborators carefully for the purpose of this revision, we have also developed some concerns about their analysis. First, with the assumptions as formulated in their paper, and the scores of neurons that are known to exist in each grid module, the scale ratios would generically be predicted to be much larger than 1 (please see the detailed response to Reviewer 2). This is in tension with data. Even ignoring this point, optimizing the Fisher information generally predicts a hierarchy of scale ratios, and only predicts geometric scaling if that scale is significantly greater than 1. Experimentally the scale ratio is ∼1.5. Thus, it seems that optimizing Fisher information does not predict geometric scaling in the regime of relevance to experiment. What is more, while Mathis et al. can only predict a geometric scaling if the scale is large, their <xref ref-type="fig" rid="fig5">Figure 5B</xref> illustrates that for large scale ratios the Fisher information does a poor job in approximating the decoding error. So this means that their prediction of a geometric series of periods, even in one dimension, has a limited range of validity. The optimal one-dimensional grid in our work is perched near the edge of the estimated range where their analysis appears to be valid.</p><p>We have described these last points in the detailed response to the referees. However, pointing out limitations of Mathis et al. is not our goal in this paper. Hence, we have simply added the phrase “in a regime where the scale factor is sufficiently large” in our description of their prediction of geometric scaling in one dimension, without further comment on the limitation of this prediction.</p><p><italic>2) Be clear about how the novel analyses presented (as different from other studies) allow a closer link with experimental studies</italic>.</p><p>We have expanded our discussion of the link with experiments. Specifically, we have pointed out that, as distinct from other studies, our approach allows us to predict that: (a) grid fields should lie on a triangular latice, (b) grid periods should follow a geometric projection, (c) the ratio between grid scales should be <italic>e</italic><sup><italic>1/2</italic></sup> for idealized neurons, liying between 1.4 and 1.7 for realistic neurons, (d) the scale ratio should vary modestly within and between animals, (e) the optimal scale ratio in one and three dimensions. With some additional assumptions we also predict: (i) the number of grid modules should be ∼10, and (ii) the ratio between grid periods and field widths. Finally, we examine possible deficits in spatial behavior that will obtain upon inactivating grid modules in the context of specific models of grid cells readout. Most of this material is in the Abstract, Introduction, Comparison to Experiment and Discussion sections.</p><p>In the original submission, we used a simple model of linear summation of grid cells to make place cells to investigate the effects of grid module inactivation in a hierarchical grid system like the one we study. Reviewer 1 pointed out that: (a) the idea that place cells are a read-out of grid cells has no direct bearing on our theoretical work, (b) recent experimental work very strongly suggests that while the hippocampal place system is certainly affected by the grid system, place cells are not a “read-out” of the grid system in any simple sense of that term. We agree entirely and have edited the text in detail to reflect these points. The changes are in Results, Discussion and Appendix.</p><p>Details of the changes are described below.</p><p>Reviewer #1:</p><p><italic>[…] In the subsection “Intuitions from a simplified model”, the current body of experimental data in this field simply does not support the authors’ repeated assertion that “[…] anatomical and functional evidence suggests that place cells selectively read out contiguous subsets of entorhinal grid modules along the dorsoventral axis (Van Strien, Cappaert and Witter, 2009; Solstad, Moser and Einevoll, 2006)”. First, citations Van Strien, Cappaert and Witter, 2009 and Solstad, Moser and Einevoll, 2006 are an anatomical review and a theoretical paper, respectively – neither of which can reasonably be described as providing “functional evidence”. Second, several groups have recently published review papers summarising a wide body of functional evidence that directly contradicts the hypothesis that place cells ‘selectively read-out’ a subset of grid cell inputs (i.e.</italic> <xref ref-type="bibr" rid="bib11"><italic>Bush et al., 2014</italic></xref><italic>;</italic> <xref ref-type="bibr" rid="bib46"><italic>Sasaki et al., 2015</italic></xref><italic>). The authors should edit the text here, and at several other junctures throughout the manuscript (listed below) to address this point. The hypothesis that place cells represent a read-out of the grid cell system has no bearing on the theoretical work presented, and only serves to misrepresent our current understanding of the grid cell system</italic>.</p><p>We agree that, as written, our paper suggests an understanding of the relation between grid and place cells that is both overly definitive, and one which is challenged by recent findings (e.g. that place cells are active before grid cells, and that place fields survive sustained inactivation of grid cells). The material that the referee is commenting on arose from multiple discussions with audiences of seminars and readers of our manuscript. We were repeatedly asked (and are still asked during talks) how our view of a hierarchical grid code would affect readout of the grid system, perhaps via place cells, as compared to a non-hierarchical grid system of the sort proposed by Burak and Fiete. We decided that a concrete way of addressing these questions would be to pick a simple model relating grid and place cells (e.g. the Fourier summation setup of Solstad et al., and explicitly show that different grid schemes can have different effects).</p><p>However, we fully agree with the reviewer that: (a) the idea that place cells are a read-out of grid cells has no direct bearing on our theoretical work, (b) recent experimental work very strongly suggests that while the hippocampal place system is certainly affected by the grid system, place cells are not a “read-out” of the grid system in any simple sense of that term. The two reviews cited above (<xref ref-type="bibr" rid="bib11">Bush et al., 2014</xref> and <xref ref-type="bibr" rid="bib46">Sasaki et al., 2015</xref>) make the latter point very clearly and effectively.</p><p>As we see it there are two options: (1) we could completely remove any mention of a readout via place cells or otherwise and simply discuss the architecture of the grid system; (2) we could be clear that we are going to look at linear summation models of grid cell readout as a toy model, not because we think they accurately represent the readout, but to show that different assumptions about the grid architecture can lead to different specific effects for manipulations like lesions. Of course, in order to make specific predictions for how lesions in the grid system would affect place cells, we would need detailed knowledge about the precise relationship between these systems, and while there are many hints of a complex relationship, the details remain unclear.</p><p>We decided to go with option (2), because we are repeatedly asked, “I know that the relation between place cells and grid cells is complicated, but can you tell me what would happen if you imagine, for purposes of argument, a simple linear summation readout and then remove some modules?” So we think that including this material (which is mostly in the Appendix and Discussion), with appropriate nuance and caveats, might be helpful to some readers. We have revised to try to achieve this goal, but are open to the idea of simply leaving this material out.</p><p>We have made a series of changes starting with the remark that the referee mentions (in the subsection “Intuitions from a simplified model”) and continuing throughout the paper (please also see our responses to the comments below). Also, a minor point – as the referee says, references to Van Strien, Cappaert and Witter, 2009, and Solstad, Moser and Einevoll, 2006 are an anatomical review and a theory paper, and we have modified the citation accordingly.</p><p><italic>In the Discussion, the authors state that: “Together with the anatomy (Van Strien, Cappaert and Witter, 2009), the hierarchical view of location coding that we have proposed then predicts that dorsal place cells should be revealed to have multiple place fields in large environments because their spatial ambiguities will not be fully resolved at larger scales. Preliminary evidence for this prediction has appeared in</italic> <xref ref-type="bibr" rid="bib20"><italic>Fenton et al., 2008</italic></xref><italic>; Rich, Liaw and Lee, 2014.” However, those studies show no systematic relationship between the locations of dorsal place cell's multiple firing fields in large environments, which directly contradicts the predictions of a grid cell to place cell model (see, for example,</italic> <xref ref-type="fig" rid="fig8"><italic>Appendix–figure 3</italic></xref> <italic>and Appendix–figure 4 in this manuscript). The authors should note this caveat– along with the other experimental data showing that a grid to place cell model is overly simplistic (see above) – or remove the corresponding piece of text</italic>.</p><p>The studies of <xref ref-type="bibr" rid="bib20">Fenton et al., 2008</xref>, and <xref ref-type="bibr" rid="bib44">Rich et al., 2014</xref>, find that dorsal place cells often have multiple place fields. Fenton et al. claim that 85% of dorsal CA1 place cells have multiple place fields, and the majority of cells in Rich et al. have multiple place fields in a 48m track, some having dozens of place fields. However, these studies show that the locations of these place fields are sporadic, perhaps even random according to some distribution. This disordered structure may be in tension with a simplistic summation view of the grid-to-place cell transformation. On the other hand, it should be noted that the data of Stensola et al. shows that there is significant variability of the period, orientation and ellipticity within each module. As part of a different collaboration, one of us (VB) has been investigating the consequences of this variability for spatial coverage – it seems to produce significant differences in the relative phase of grid cells between unit cells of the grid lattice. This variability can change the prediction of regularity in the locations of multiple place fields in a naive summation model. That said, investigating this properly lies outside the scope of the present paper. Thus we have contented ourselves with adding appropriate nuance and caveats as follows: (a) indicate that a naive summation model of place cells along with the anatomy of mEC-hippocampal projections predicts multiple place fields for a single dorsal place cell, as seen in experiments, (b) a naive model of this kind also predicts a orderly distribution of place fields which is not seen, (c) however, the variability in the grids even within a module likely interferes with the predicted order, (d) and in any case there is significant evidence (see Bush et al., and Sasaki et al. for a summary) that place cells are not formed and maintained by grid cells alone. The changes have been added to the sixth paragraph of the Discussion.</p><p><italic>In the Discussion, the authors “[…] predict that lesioning the modules with small periods will expand place field widths, while lesioning modules with large periods will lead to increased firing at locations outside the main place field, at scales set by the missing module. Our prediction is supported by a recent study demonstrating effects of lesions including dorsal mEC on place field widths in small environments (</italic><xref ref-type="bibr" rid="bib28"><italic>Hales et al., 2014</italic></xref><italic>)”. This is misleading for several reasons. First, as described above, this statement neglects to mention the wider body of evidence contradicting the hypothesis that grid cell inputs solely generate place cell firing fields. Second, the aim of the cited study (</italic><xref ref-type="bibr" rid="bib28"><italic>Hales et al., 2014</italic></xref><italic>) was to examine the effect of eliminating all grid cell inputs to place cells, and ∼85% of the total mEC volume was ablated, including 94.6% of layer II and 83.5% of layer III. Are the authors suggesting that the observed effects on place cell firing are a result of remaining grid cell inputs from modules with a large or small period? Third, the more specific prediction is that lesioning grid modules with large periods will lead to the appearance of additional place fields at periodic, grid-like locations in two dimensions, but that analysis is not made or discussed in</italic> <xref ref-type="bibr" rid="bib28"><italic>Hales et al. (2014)</italic></xref><italic>. Fourth, more recent experimental evidence indicates that focal inactivations of dorsal or ventral mEC each produce place field expansion, and neither generated increased firing at locations outside the main place field – in fact, that data showed a trend towards a decrease in the number of firing fields exhibited by each place cell following focal inactivations (Figure S7 in Ormond et al., 2015). Each of those results also contradict the predictions of a grid cell to place cell model, despite the strange interpretation of the data made in that paper. Hence, the authors should include a citation to that paper and edit the text accordingly</italic>.</p><p>We agree entirely that there is a substantial body of evidence that grid cells do not solely generate place cells, although they do influence some of the functional properties of the hippocampus. Further, Hales et al. were indeed attempting to examine the effects of eliminating all the grid inputs and found that the substantial ablation they performed led to fewer, smaller and less stable place fields. These ablations were not specific to a given module, but we would expect in a hierarchical grid code that elimination of many contributions with small periods would decrease the precision of spatial coding that exploits grid cell responses. Of course elimination of large periods should lead to ambiguities in large environments and Hales et al. did not test for or discuss this – they are working with small 1m x 1m environments so we might not expect to see many ambiguities.</p><p>There is also the paper of Ormond et al., which discussed focal inactivations. We find the results in this paper difficult to interpret also. For starters, the inactivations are focal along the dorso-ventral axis of the mEC, but the data in Stensola et al. seem to indicate that that many grid modules are present at every mEC depth, with a dorsal enrichment of small periods. So the focal inactivations of Ormond et al. would seem to still be inactivating modules with multiple periods. Thus, even in a naive summation model, we would expect expansion of place fields for all of these inactivations with larger expansion for dorsal fields. This is because dorsal lesions would get rid of more cells in the smallest modules, and would lead to a greater broadening. Ventral lesions would still remove some cells with small periods, and so the broadening effect should be smaller as Ormond et al. appear to see.</p><p>The data in the supplement of Ormond et al. seem to indicate, on the one hand, a decrease in the number of distinct place fields associated to place cells (in tension with a grid to place model), but on the other hand they seem to show an increase in the “out-of-field” firing rate. What is more, these figures likely include various cases where the place cells went from having one field to having none (i.e. the place field simply disappeared) – it is hard to be sure, because the information was not provided as far as we can tell, and thus it is hard to evaluate whether this is in fact inconsistent with a summation model. One could also imagine that the general increase in the size and noisiness of place fields might lead to a decrease in the number of place fields that can be accommodated in the 7m linear track that they were working with. In particular, the decrease in theta power makes their measurements much more prone to noise. Finally, Ormond et al. are recording dorsally in CA1 and CA3. If the anatomy of Witter et al. that we cite is accurately indicative of functional connectivity, then ventral inactivation in mEC would have less clear effects on these dorsal hippocampal recordings. That makes it still more problematic to interpret what is going on. We also note that none of the individual examples depicted in Figure S5 of that paper show a decrease in ambiguity after lesions. So we conclude from Ormond et al. that there is no clear evidence for a decrease in ambiguity following lesion, but it is similarly debatable whether there is any indication of an increase in ambiguity.</p><p>Given the complex, and, in our view, difficult to interpret, experimental situation we have edited as follows. First, we have modified our remarks to make it clear that we regard a linear summation model of place cells as simplistic in view of recent experimental developments reviewed in Bush et al. and Sasaki et al. Second, we are more clear that we are simply seeking to illustrate that different grid schemes can have different effects on specific readouts. Finally we refer to Hales et al. and Ormond et al. in a nuanced way, clarifying that these experiments do not lesion individual modules and thus do not constitute specific evidence for the results of such lesions. These changes have been added to the Discussion section.</p><p><italic>In the Appendix, the authors again suggest that “the grid cell representation of space in the entorhinal cortex is […] transformed in the hippocampus into the place cell representation.” They should edit this text to more accurately represent the current understanding of the relationship between grid and place cell firing patterns</italic>.</p><p>We have edited this text to be more nuanced and accurate about the current state of understanding of the relationship between the mEC and the hippocampus (see Section F of the Appendix). Specifically we say: “The grid cell representation of space in the entorhinal cortex is related in a complex manner to the hippocampal place cell representation (<xref ref-type="bibr" rid="bib11">Bush et al., 2014</xref>, <xref ref-type="bibr" rid="bib46">Sasaki et al., 2015</xref>). […] In the context of such a model (which does not reflect many aspects of the known physiology), our grid scheme makes specific predictions for the effects of module lesions on place fields.”</p><p>The papers mentioned above are now cited in the main text, and again in the Appendix.</p><p><italic>Abstract and Introduction: Given that the principal difference between the analysis in this manuscript and that presented in several previous publications (i.e.</italic> <xref ref-type="bibr" rid="bib21"><italic>Fiete et al., 2008</italic></xref><italic>;</italic> <xref ref-type="bibr" rid="bib58"><italic>Towse et al., 2014</italic></xref><italic>) is that the authors assume a spatial range equal to the size of the largest grid module, this should be more explicitly stated in the Abstract and Introduction. This would make the novelty of this manuscript more apparent. For example, the Abstract should be edited to read: “We propose that the grid system implements a hierarchical code for spatial location that economizes the number of neurons required to encode location with a given resolution across a spatial range equal in size to the period of the largest grid module” or similar. Likewise, the Introduction should be edited to read: “minimize the number of neurons required to achieve the behaviourally necessary spatial resolution across a spatial range equal in size to the period of the largest grid module” or similar</italic>.</p><p>Thank you for this suggestion. We have made this change, and it helps to clarify the differences between the frameworks.</p><p>We have also added a reference to Towse et al. (please see the Discussion section) as part of our analysis of the relation of our study to previous work: “Note that decoding error was also studied in Towse et al. and those authors reported that the results did not depend strongly on the precise organization of scales across modules.”</p><p><italic>In the Introduction, it is not clear to me what the authors mean by the following statement: “Consistent with studies of grid cell and place cell remapping, our analyses assume that there is a behaviorally defined maximum range over which a fixed grid represents locations (</italic><xref ref-type="bibr" rid="bib24"><italic>Fyhn et al., 2007</italic></xref><italic>).” I fail to see the relevance of remapping to the behavioural range of an animal. Could the authors explain their rationale here please?</italic></p><p>We intended to say that assuming that the grid code can only represent location up to some maximum range without additional information, it would be necessary that a new grid should be loaded upon reaching the edge of the representational range. The ability of grids to remap in new environments suggests that this should be possible. In effect, we were trying to suggest that an animal could “stitch together” a representation of a very large environment by remapping its grids between segments. To keep things simple we have removed the phrase “Consistent with studies of grid cell and place cell remapping”.</p><p><italic>In the Introduction, when the authors state “three dimensional grids that will be relevant to navigation in, e.g., bats”, they should include a reference to</italic> <xref ref-type="bibr" rid="bib61"><italic>Yartsev et al., 2011</italic></xref><italic>, which demonstrates that bats do have grid cell responses, even though they have only been recorded in two dimensional environments so far</italic>.</p><p>We have added the citation.</p><p><italic>In the subsection “Intuitions from a simplified model”, when the authors stress that “the animal could achieve the required resolution in a place coding scheme […]”, they must incorporate a reference to</italic> <xref ref-type="bibr" rid="bib21"><italic>Fiete et al., 2008</italic></xref><italic>, which makes a very similar comparison of “place coding” and “grid coding” schemes</italic>.</p><p>We have added this citation. Thank you for the suggestion.</p><p><xref ref-type="fig" rid="fig2"><italic>Figure 2E</italic></xref> <italic>is cited before</italic> <xref ref-type="fig" rid="fig2"><italic>Figures 2A-D</italic></xref> <italic>in the main text, which is confusing (subsection “Winner-Take All Decoder”). Similarly, in subsection “General grid coding in two dimensions”,</italic> <xref ref-type="fig" rid="fig3"><italic>Figure 3A</italic></xref> <italic>is not referred to in the text at all, and</italic> <xref ref-type="fig" rid="fig3"><italic>Figures 3B and 3C</italic></xref> <italic>are cited before</italic> <xref ref-type="fig" rid="fig2"><italic>Figure 2F</italic></xref><italic>, which is not ideal. It would be preferable if the authors placed all figures pertaining to the 2D case in</italic> <xref ref-type="fig" rid="fig3"><italic>Figure 3</italic></xref> <italic>(i.e. move</italic> <xref ref-type="fig" rid="fig2"><italic>Figure 2F</italic></xref> <italic>into</italic> <xref ref-type="fig" rid="fig3"><italic>Figure 3</italic></xref><italic>) and moved</italic> <xref ref-type="fig" rid="fig2"><italic>Figure 2E</italic></xref> <italic>to match the flow of the text (i.e. before</italic> <xref ref-type="fig" rid="fig2"><italic>Figure 2A</italic></xref><italic>)</italic>.</p><p>Thank you for these suggestions. In order to respect the flow of the text we have reorganized as follows. We moved <xref ref-type="fig" rid="fig2">Figure 2E</xref> (the optimization curve in 1D) to be a panel of <xref ref-type="fig" rid="fig6">Appendix–figure 1</xref> where other material on the optimization in one dimension is gathered. We removed <xref ref-type="fig" rid="fig3">Figure 3A</xref> (which was not referred to) and we moved <xref ref-type="fig" rid="fig2">Figure 2F</xref> into <xref ref-type="fig" rid="fig3">Figure 3</xref>. Now <xref ref-type="fig" rid="fig2">Figure 2</xref> is focused on illustrating the precision-ambiguity tradeoff in the setting of probabilistic decoding. <xref ref-type="fig" rid="fig3">Figure 3</xref> is focused on the two dimensional optimization. We hope that this helps with clarity.</p><p><italic>In the Discussion, the authors state: “Given homogeneous positive noise correlations within a grid module, which will arise naturally if grid cells are formed by an attractor mechanism, the required number of neurons could be an order of magnitude higher (</italic><xref ref-type="bibr" rid="bib50"><italic>Sompolinsky et al., 2001</italic></xref><italic>; Averbeck, Latham and Pouget, 2006)”. It has recently been demonstrated that positive noise correlations appear to be largely absent in the rodent grid cell system, and the authors may wish to note this point and cite the corresponding paper (</italic><xref ref-type="bibr" rid="bib38"><italic>Mathis et al., 2013</italic></xref><italic>) here</italic>.</p><p>Thank you for this suggestion. We have added this citation. However, the authors of the paper seem to explicitly say that they did find noise correlations. More specifically, their abstract says: “We analyze the noise correlations between pairs of grid code neurons in behaving rodents. We find that if the grids of the two neurons align and have the same length scale, the noise correlations between the neurons can reach 0.8. For increasing mismatches between the grids of the two neurons, the noise correlations fall rapidly.” This is apparently also the message that they derive from their Figure 9. Meanwhile Dunn, Morreaunet and Roudi (2015) also report positive noise correlations for grids with similar phases, and vanishing or sometimes negative correlations for grids with very different phases. Since most grid cells differ in their mutual phase or period, we take the referee’s point. Since the presence or absence of noise correlations is not a main point of our paper, we have simply indicated (Discussion, fourth paragraph) that Mathis et al. and Dunn et al. investigated noise correlations between grid cells and found positive correlations for aligned grids (i.e. similar phase) of the same scale and weak correlations otherwise.</p><p><italic>In the Discussion, “the answer depends on the dimension of the grid” should be “the answer depends on the dimensionality of the grid”</italic>.</p><p>We have made this change.</p><p><italic>Again, in the Discussion, the authors mention that they “have checked that the optimal grid scheme predicted by our theory, if decoded in the fashion of (</italic><xref ref-type="bibr" rid="bib21"><italic>Fiete et al., 2008</italic></xref><italic>), can represent space over ranges longer than the largest scale”, but do not mention (in the main text) whether it could or not. They should incorporate a brief description of the outcome of those simulations in this section of text, for clarity</italic>.</p><p>We were trying to indicate the following. The optimization analysis predicts a particular scale ratio and enough neurons in each module to ensure that the likelihood function over position in each module has peak widths that are a certain fraction of the period. The range of representation can be extended by shrinking the widths of the likelihood ratio peaks. This requires increasing the number of neurons in each module beyond the minimum required for the spatial range that we started with. We have edited this text to say: “Nevertheless, we have checked that a grid coding scheme with the optimal scale ratio predicted by our theory can represent space over ranges larger than the largest grid period (Appendix, Section E). However, to achieve this larger range, the number of neurons in each module will have to increase relative to the minimum in order to shrink the widths of the peaks in the likelihood function over position.” The edited text is in the Discussion.</p><p>Reviewer #2:</p><p><italic>1) The major assumption of this paper is the ambiguity of the grid cell firing, that is, from the firing of one grid cell, it is not possible to infer in which of the many vertices of the grid one is located. This assumption, however, does not take into account the fact that the peak firing rate of a grid cell at its fields significantly vary. In other words, the translational symmetry is about the positions at which peak firing occurs, not that each field is identical to the other in terms of firing rate. In my view, this experimental fact fundamentally affects the argument offered here</italic>.</p><p>Consider the probabilistic decoder. In this case, <italic>P(x|i)</italic> can be approximated as a periodic sum of Gaussians without making restrictive assumptions about the shapes of the tuning curves of individual grid cells, or about the precision of their periodicity, so long as, on average, the variability of individual neurons is weakly correlated and homogeneous.</p><p>For example, even though individual grid cells can have somewhat different firing rates in each of their firing fields, this spatial heterogeneity will be smoothed in the posterior over the full population of cells, leading to much more accurate periodicity. In other words, individual grid cells show both spiking noise and “noise” due to heterogeneity and imperfect periodicity of the firing rate maps. Both these forms of variability are smoothed out by averaging over the population, provided there are enough cells and noise is homogeneous and not too correlated – we assume this. The first paragraph in the “Probabilistic Decoder” subsection makes these points.</p><p>Even if the experimentally-measured heterogeneity is too strong to be completely neglected, we still feel that our framework provides value in studying the grid system. Developing a theoretical framework that solves the simpler case of perfect periodicity is a natural starting point for studying the more complex, realistic case. Experimental details that deviate from our idealized assumptions may be added, and our calculations modified to see how these complications modify our predicted optimality conditions. We think this is an exciting avenue for future work building on the results and framework we have reported here.</p><p><italic>2) The minimum of the cost function versus ratio of the spacing of successive modules is very wide, raising the question whether one can really say anything meaningful about the value that that ratio should take. It should not escape our attention that in</italic> <xref ref-type="fig" rid="fig2"><italic>Figure 2E and F</italic></xref><italic>, the authors plot the cost function versus the logarithm of the ratio between successive modules, which gives the impression of a narrower minimum (though still wide). Even with this, the authors’ prediction is stated to “[…] robustly lie in the rage 1.4-1.7[…]”. This is a 20% range</italic>.</p><p>Please note that our text goes to some pains to point out that the minima of the cost functions are not extremely sharp (in the subsection “General grid coding in two dimensions”). In our view this is a virtue, not a problem, because it means that a degree of variability in the grid parameters can be tolerated. Please also note that, as we say, the predictions of the simple winner-take-all and probabilistic models lie within the “overlapping shallow basins” of the two models. Given that these two models lie at extremes of decoding complexity, this adds to our confidence that over a wide range of assumptions the optimal grids will lie within a similar range. Similar considerations apply to both the one dimensional and two dimensional grids.</p><p>As we also state, the relative shallowness of the minima lead us to expect that the parameters of the grid should be somewhat variable between between cells within a module, and between individuals. Indeed, the experimental measurements are variable in this way. It is difficult to formulate a theory of precisely how much variability, and associated cost in the number of neurons, is acceptable to the animal. In this situation, the sensible prediction to make is that the grid periods ratios will be localized around a certain value, and to ask what deviation in cost relative to the optimum is implied by the experimentally determined spread of these ratios (we find a ∼5% deviation in cost). This is what we have done in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><p>We can further illustrate these points by considering an additional kind of variability in the experimentally measured grids. It has been noted that grids can have an ellipticity – i.e. they can be “squished”. Our analysis of grid geometries in two dimensions showed that the triangular grid is optimal, but geometries close to the triangular one will do well also (see the contour plot in <xref ref-type="fig" rid="fig3">Figure 3D</xref>). How does the range of ellipticity in the experiments compare to the tolerable extent predicted by the theory?</p><p>To address this point, we can re-examine the contour plot in <xref ref-type="fig" rid="fig3">Figure 3D</xref> which shows <italic>N/N</italic><sub><italic>min</italic></sub> (number of neurons/minimum number of neurons) as a function of the array geometry after minimizing over the scale factors between modules for a fixed resolution <italic>R</italic>. The plateau around the triangular array geometry (the point in the middle of the plot) shows that a range of ellipticities will be similarly efficient. To show this range explicitly in a different way, we can keep <italic>N</italic> fixed, and plot the logarithm of the resolution (as defined in the main text), normalized to its maximum, which is achieved at the optimal triangular grid – we will call this the “relative efficiency”. The array geometry is parametrized in terms of two variables, v parallel and v perpendicular as described in the main text, with the triangular lattice being given by <italic>v∥</italic>= 1/2, <italic>v⊥</italic>= √3/2. The plot below shows the relative efficiency as a function of <italic>v∥</italic> for <italic>v⊥</italic>= 1/2 – there is a plateau surrounding the triangular lattice parameters, with a sharp decline in efficiency on either side. Marked on the figure is a range of ellipticities 1.0-1.4 that is wider than the range reported in <xref ref-type="bibr" rid="bib52">Stensola et al., 2012</xref> (the largest ellipticity there was 1.26, albeit with a small sample). Satisfyingly, the ellipticities reported by Stensola et al. will all be closely arranged along the plateau, as our theory would predict.<fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.08362.013</object-id><label>Author response image 1.</label><caption><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.08362.013">http://dx.doi.org/10.7554/eLife.08362.013</ext-link></p></caption><graphic xlink:href="elife-08362-resp-fig1-v2.tif"/></fig></p><p>We have not included this analysis (Author response image 1) in the paper because it is not comprehensive and we hope to include a more detailed version in work that is ongoing. But we hope that the result helps to answer the referee’s question.</p><p><italic>3) (a) The idea of using optimality for predicting the ratio of grid spacing of the modules has been already employed by Mathis et al., Neural Comp 2012. There are differences between the two works, e.g. Mathis et al. maximize the resolution given a fixed number of neurons while Wei et al. minimize the number of neurons given the resolution and Mathis et al. only focus on the one dimensional case. Despite the differences, it is not clear what is the major conceptual advancement. As far as I can say, the argument of Mathis et al. can be easily extended to 2D to produce a geometric progression</italic>.</p><p>First, we would like to be clear that, contrary to the assertion here, Mathis et al. <italic>did not claim to predict a value for the ratio of grid spacings in grid modules in any dimensions, and did not attempt to say anything about the optimal period ratio and grid shape in two dimensions.</italic> They formulated the Fisher information for decoding position from populations of periodic, one dimensional tuning curves and found that under some conditions the set of periods that maximizes the Fisher information approximates a geometric series. However, as discussed in detail below, their derivation, as they present it: (1) generically implies either unrealistically large period ratios or unreasonably small numbers of cells in each module, both of which disagree with experiment, and (2) in general does not predict a constant scale ratio <italic>r</italic> unless <italic>r &gt;&gt; 1</italic>, which it is not in the data. Incidentally, the derivations in Mathis et al. also have minor mathematical errors which are fixed in the discussion below.</p><p>Mathis et al. write an expression for the Fisher Information of the <italic>i</italic>th module which takes the form (Equation 3.22):</p><p>J<sub>i</sub> = C<sub>1</sub> (M<sub>i</sub>/λ<sub>i</sub>)<sup>2</sup> (1)</p><p>where C<sub>1</sub> is a constant, M<sub>i</sub> is the number of cells in module i, and λ<sub>i</sub> is the period of module i. The sum of M<sub>i</sub> over <italic>L</italic> modules is <italic>N</italic>, the number of cells in the grid system. The total Fisher Information J is the sum of the J<sub>i</sub>. In any treatment of the grid system we must understand how information from different modules is integrated to eliminate the ambiguity in position left by the responses in a single module. Mathis et al. resolve this ambiguity by a hard constraint (which is reminiscent of our Winner-Take-All model) by setting (Equation 3.24 and the text below it):</p><p>λ<sub>i+1</sub> = D(ε)/(J<sub>i</sub>)<sup>1/2</sup> = (D(ε)/M<sub>i</sub> C<sub>1</sub>) λ<sub>i</sub> (2)</p><p>Here D(ε) is a “safety factor” that depends on noise and the tuning curve shapes. They arrive at this equation by first placing a bound on how small the period of module <italic>i+1</italic> can be to achieve a tolerable degree of ambiguity (set by D(ε)) and then saying that the Fisher Information is optimized when this bound is met.</p><p>But Equation (2) above implies that the ratio of scales that we seek to predict has the form:</p><p>r<sub>i</sub> = λ<sub>i</sub>/λ<sub>i+1</sub> = M<sub>i</sub> (C<sub>1</sub>/D(ε)) (3)</p><p>Now Mathis et al. are taking C<sub>1</sub>/D(ε) to be a parameter of O(1) (bottom of their p. 17). But M<sub>i</sub>, the number of grid cells in each module is expected to be in the scores or maybe the hundreds. So, given the general assumptions of Mathis et al., the period ratio r<sub>i</sub> would be expected to be much larger than 1, contrary to experiment. Alternatively, to get an O(1) scale ratio with C<sub>1</sub>/ D(ε) ∼ O(1), you could take M<sub>i</sub> to be O(1). This seems to be the scenario considered in Mathis et al., where they say that M<sub>i</sub> ∼ 3 would be optimal. But we know that each module contains many more cells than that. A final option may be to suppose that C<sub>1</sub>/D(ε) is small in their formalism. But they do not seem to consider this in their paper. <italic>Hence we conclude that the analysis of Mathis et al., as presented in their paper, gives estimates that are in tension with data.</italic></p><p>Ignoring this for the moment, we can proceed further with their analysis. As far as we can tell there is a minor mathematical error in going from their Equation (3.26) to their Equation (3.27) for the population Fisher Information. They chose to scale M<sub>i</sub> as M<sub>i</sub>’= (C<sub>1</sub>/ D(ε))<sup>1/2</sup> M<sub>i</sub>, but this does not lead the scaling in Equation 3.27. The correct choice seems to be to scale M<sub>i</sub> as P<sub>i</sub> = M<sub>i</sub> C<sub>1</sub>/D(ε). This difference leads to different constant factors in front of Equation 3.27 and different scalings of variables in their analysis of regimes of validity. Neither of these changes makes a big difference to the analysis, but it is worth correcting the small error anyway. In any case, the bottom line is that the Fisher Information can be written as:</p><p>J = (D(ε)/λ<sub>0</sub>)<sup>2</sup> (r<sub>0</sub><sup>2</sup> + r<sub>0</sub><sup>2</sup>r<sub>1</sub><sup>2</sup> + r<sub>0</sub><sup>2</sup>r<sub>1</sub><sup>2</sup>r<sub>2</sub><sup>2</sup> +…) (4)</p><p>This is (Equation 3.27) in Mathis et al. rewritten in terms of the scale ratios. To get the coefficients in front right you have to fix the minor scaling error mentioned above. Meanwhile, the constraint on the total number of cells is:</p><p>N (C<sub>1</sub>/D(ε)) = r<sub>0</sub>+r<sub>1</sub>+r<sub>2</sub>+… (5)</p><p>This is simply the constraint on the sum of M<sub>i</sub> written in terms of Equation (3) above.</p><p>It is obvious that (4) is not symmetric between the r<sub>i</sub> and hence the Fisher Information equations of Mathis et al. <italic>do not in general predict a geometric series of periods (i.e. constant r</italic><sub><italic>i</italic></sub><italic>)</italic>. In fact, one can show by optimizing (4) above with a Lagrange multiplier imposing the constraint (5) that:</p><p>r<sub>i</sub>&gt;r<sub>i+1</sub> (6)</p><p>when the Fisher Information is optimized. (For example, in the case of two scales the problem becomes maximizing r<sub>0</sub><sup>2</sup>+ r<sub>0</sub><sup>2</sup>r<sub>1</sub><sup>2</sup> subject to the constraint r<sub>0</sub> + r<sub>1</sub> = constant. If r<sub>1</sub> is large we can ignore the contribution from the first term, and optimizing gives r<sub>0</sub>=r<sub>1</sub>.</p><p>However, the additional r<sub>0</sub><sup>2</sup>term favors making r<sub>0</sub> slightly larger as compared to making r<sub>0</sub> and r<sub>1</sub> equal.) So the scale ratios are <italic>not</italic> all equal at the optimum. Of course, we can try to get a symmetric solution optimizing the Fisher Information J in Equation (4) by supposing that the r<sub>i</sub> are much larger than 1, so that the symmetric product term, (r<sub>0</sub><sup>2</sup>r<sub>1</sub><sup>2</sup> r<sub>2</sub><sup>2</sup> r<sub>3</sub><sup>2</sup>…), dominates Equation (4). Indeed, below their Equation 3.28, this is precisely the limit that Mathis et al. are considering. For r<sub>i</sub> &lt; 3 or so, their analysis is invalid in its prediction of a geometric series of periods. On the other hand, <xref ref-type="fig" rid="fig5">Figure 5B</xref> in their paper very clearly illustrates that for small “contraction factors” (i.e. large scale ratios) the Fisher information does a poor job in approximating the decoding error. So this means that the prediction of a geometric series of periods is based on a tenuous analysis with an uncertain range of validity. The optimal one dimensional grid in our work is perched near the edge of the estimated range where their analysis appears to valid. <italic>Thus Mathis et al. predict a geometric scaling of the grid system only when the scale ratios r</italic><sub><italic>i</italic></sub> <italic>are large, while we know that these ratios are O(1) from experiment. What is more, their equations explicitly predict a hierarchy of scale ratios (Equation 6 above) in the O(1) regime.</italic></p><p>We then considered the possibility that the Fisher information approach of Mathis et al. could be rescued in two dimensions. Translating everything for two dimensional lattices would be a formidable work, so we contented ourselves with the following observations: (1) in two dimensions the Fisher information would scale the same way with the r<sub>1</sub><sup>2</sup> and so would take a similar form to Equation (4) above in terms of these variables, and (2) the constraint expression for the number of cells in terms of the r<sub>i</sub> would still be symmetric between the r<sub>i</sub>. In two dimensions, experiments have shown a geometric series of periods with a period ratio of ∼1.5. This is too small for the last, symmetric term in the Fisher Infomation dominate. <italic>Thus in two dimensions the analysis of Mathis et al. cannot predict a geometric series of grid periods in the regime of O(1) period ratios that applies to the data</italic>.</p><p>Our paper uses very simple, general assumptions to make a number of specific quantitative predictions that Mathis et al. do not. Specifically, we: (1) predict a constant grid scaling ratio (in a regime where their alternative theory predicts a hierarchy of scales), (2) predict the grid scale factor (which they explicitly state they cannot do), (3) explain the 2d triangular grid geometry (which they do not even try to do), and (4) predict the ratio of grid period to grid field width under specific assumptions (which they parametrize in terms of tuning curve widths, contributing to their inability to predict the grid scale factor). We additionally predict the expected number of modules, and estimate the number of cells required in the mEC to implement our proposed grid scheme (see our Discussion).</p><p>The extension to two-dimensional grids is certainly non-trivial. There are many regular two-dimensional lattices, and our paper shows that the triangular lattice is favored. This is in no way implied by a one-dimensional analysis. We were able to study the two-dimensional lattices because we developed an analytical calculation (presented in the Appendix) that greatly simplified the numerical analyses.</p><p>Within the context of specific models of grid to place cell transformations we also show effects on spatial coding of selectively lesioning grid modules. (The latter analyses have significant caveats arising from our lack of knowledge of the precise relation between grid and place cells – this an important component of the comments of Referee 1 and our corresponding edits.)</p><p>All of these results go beyond the idea of a geometric progression of scales that Mathis et al. arrive at in one dimension through an extensive and sometimes inconclusive numerical analysis coupled with a study of the Fisher Information in the grid system, subject to the caveats described above.</p><p><italic>(b) It is true that, as stated in the in the conclusion, in the work of Mathis et al. the optimal ratio depends on “the number of neurons per module and peak firing rate”. But the prediction of the optimal ratio here also varies over a wide range, depending on the assumption on the decoding scheme (and probably the shape of the tuning curves, assumption on the correlation between neurons etc.)</italic>.</p><p>For the reasons stated above, our analysis has a wider range of validity that Mathis et al. (please see our Discussion).</p><p>We do not agree that the optimal ratio here varies over a “wide range”. It is quite remarkable to us that an extremely simplistic winner-take-all decoder and an optimal probabilistic decoder give optimal ratios that are so closely clustered. The results do not depend in detail on the shapes of tuning curves etc. (please see our response to comment 1). Concerning the roles of correlations between grid cells, as Reviewer 1 points out, there is now work by the Herz group (<xref ref-type="bibr" rid="bib38">Mathis et al., 2013</xref>) and by the Roudi group (<xref ref-type="bibr" rid="bib19">Dunn et al., 2015</xref>) that suggests that there are only weak noise correlations between grid cells that are not aligned and of the same period (we now cite this work in the fourth paragraph of the Discussion).</p><p>Whether our prediction is “tight” or not may here be a case of beauty being in the eye of the beholder. The art of doing theory often involves making the right assumptions about the relevant and irrelevant factors. We made simple general assumptions that lead to remarkable (in our view) predictions for the architecture that agree with experiment. A legitimate way to do theoretical neuroscience is to make informed assumptions, build a theory with these assumptions, and then use the match between predictions of the theory and data as guide to whether the assumptions are reasonable. Certainly, methodologically, this seems like a very reasonable way to proceed, and is well within the venerable tradition of theoretical work in the older field of physics.</p><p>Reviewer #3:</p><p><italic>This excellent paper uses a very simple principle for demonstrating that coding of grid cells is better than coding of place cells, and generates some postdictions following this simple principle. The basic idea is that grid cells act as a kind of “Base-b” representation of space, and it is shown that the representation is optimal when the base chosen is base e (2.71828…). From that, various postdictions follow (which conform nicely with known experiments). Specifically, grid cell modules have a constant scale ratio, which should be √</italic>e <italic>in the simplest model, and closer to the real experimental value (1.4) in a probabilistic model of the cells coding. Furthermore, there should be a certain optimal ratio between the grid field width and the spacing between grid points</italic>.</p><p><italic>The paper interacts nicely the papers of the group of Andreas Herz, which deal with similar issues using Fisher information. I have no major concerns, as I think the paper is well written, deals with an important subject, looks sound mathematically, and has a nice treatment of relation to experimental data</italic>.</p><p>Thank you for these remarks. Please see below for changes we have made in response to the specific comments.</p><p><italic>The only issue I would like to be dealt with is to make the Discussion more clear as to the relation between this paper and the papers from the Herz group (including the relevant recent one from 2015). Specifically, they have a treatment of the issue of grid cell coding through Fisher information, and it could be of value to connect the work performed here to their line of thought, at least minimally by adding some discussion to the paper (elaborating on the existing paragraph)</italic>.</p><p>We have now elaborated on the connection to the work from the Herz group. Specifically, we have added discussion of their use of the Fisher information and their different formulation of a resolution constraint. Please see the Discussion. Please also see our response to the editor’s remarks, and our response to Reviewer 2.</p><p><italic>Another small question I am curious about is whether the winner-take-all decoder could be seen as a limit-case of the probabilistic decoder. But if that is the case, I do not completely understand the “leap” from</italic> e <italic>to 2.4</italic>.</p><p>The winner-take-all decoder is not quite a simple limit case of the probabilistic decoder as we have formulated it. One way to think of it is to imagine the WTA decoder as an approximation that replaces the smooth posterior with a flat function that drops to zero outside its support. The slight difference in the optimal ratio arises technically from the truncation of the tails in the Gaussian posterior, and the flattening of the posterior inside the region of support. Compared to the Gaussian, a boxcar likelihood has less precision (because it spreads out uniformly rather than being concentrated on the center), but it also implies less possibility of ambiguity (because it has zero tails). So the WTA decoder chooses a more aggressive (larger) scale ratio that improves precision, without being penalized by increased ambiguity.</p><p>We explain this point in the paragraph of the subsection “Probabilistic decoder” that starts “Why is the predicted scale factor based on the probabilistic decoder somewhat smaller than the prediction based on the winner-take-all analysis? […]”. We have slightly extended this paragraph.</p></body></sub-article></article>