<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="discussion" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">32015</article-id><article-id pub-id-type="doi">10.7554/eLife.32015</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Feature article</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Peer review</subject></subj-group></article-categories><title-group><article-title>To fund or not to fund?</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-18586"><name><surname>Shailes</surname><given-names>Sarah</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0399-9588</contrib-id><email>s.shailes@elifesciences.org</email><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Sarah Shailes</bold> is an Assistant Features Editor at eLife</p></bio></contrib><aff id="aff1"><institution>eLife</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>28</day><month>09</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e32015</elocation-id><history><date date-type="received" iso-8601-date="2017-09-20"><day>20</day><month>09</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2017-09-20"><day>20</day><month>09</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Shailes</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Shailes</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-32015-v1.pdf"/><related-object ext-link-type="url" xlink:href="https://elifesciences.org/articles/e32015v1"><date date-type="v1" iso-8601-date="2017-09-28"><day>28</day><month>09</month><year>2017</year></date></related-object><abstract><p>Funding agencies use many different criteria and peer review strategies to assess grant proposals.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>peer review</kwd><kwd>funding agencies</kwd><kwd>bias</kwd><kwd>science policy</kwd></kwd-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Funding agencies use many different criteria and peer review strategies to assess grant proposals.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p>Earlier this year, the European Research Council (ERC) marked its <ext-link ext-link-type="uri" xlink:href="https://erc.europa.eu/news/ten-years-european-research-council-european-success-story">tenth anniversary</ext-link> with a week of events in various countries. These events celebrated the fact that, among other things, the council has awarded substantial grants to over 7,000 scientists, the majority of whom are under 40 years old, and that ERC grants have contributed to at least 100,000 research papers. However, these 7,000 were the lucky few – last year the council received a total of 7,500 applications and funded just 12% of them.</p><p>The ERC is not alone in being highly selective; the National Institutes of Health in the US, the Biotechnology and Biological Sciences Research Council in the UK, and the Medical Research Council, also in the UK, have all reported success rates of less than 30% for grant proposals (<xref ref-type="bibr" rid="bib1">Matthews, 2016</xref>). In almost all cases the decision to fund or not to fund a proposal will have been taken after two or more of the applicant's peers have assessed the application (or an outline or summary of the application).</p><sec id="s1"><title>Criteria, criteria, criteria</title><p>The sole focus at the ERC is on scientific excellence. &quot;The aim is to recognize the best ideas, and confer status and visibility on the best brains in Europe, while also attracting talent from abroad,&quot; says a spokesperson for the council. Some critics have claimed that this strategy unfairly favors researchers based at institutions in wealthier nations, with two-thirds of ERC grants going to researchers based in just five countries – the UK, Germany, France, the Netherlands and Switzerland – between 2007 and 2013. However, the ERC spokesperson disputes this: &quot;The peer review evaluation process has been carefully designed to identify scientific excellence irrespective of the gender, age, nationality or institution of the principal investigator and other potential biases.&quot; The ERC also takes career breaks and &quot;unconventional career paths&quot; into account when awarding grants.</p><p>Most other funding agencies use two or more criteria to assess research proposals. The National Science Foundation (NSF) – the primary source of federal funding for non-medical basic research in the US – assesses all applications on two major criteria: intellectual merit and broader impacts. &quot;The broader impacts criterion encompasses the potential to benefit society and contribute to the achievement of specific, desired societal outcomes,&quot; says Rob Margetta, a public affairs specialist at the NSF.</p><p>Cancer Research UK – the world's largest cancer research charity – assesses the expertise of the research team submitting the proposal and the originality of the ideas along with several other criteria including the relevance to the charity's current research priorities, the quality of the experimental design, and value for money. &quot;The criteria are outlined on <ext-link ext-link-type="uri" xlink:href="http://www.cancerresearchuk.org/funding-for-researchers/applying-for-funding/how-to-make-a-successful-application">our website</ext-link>,&quot; says Matt Kaiser, head of discovery research at the charity, &quot;so that applicants really understand what they will be judged on.&quot;</p><p>At the Australian Research Council (ARC), different funding schemes employ different criteria to assess applications. For example, in a scheme that supports early career researchers, the investigator themselves and &quot;project quality and innovation&quot; are the two major criteria, accounting for 75% of the assessment score. However, in schemes that promote collaborations with industry and other partners, emphasis is placed on the commitment from partner organizations and project significance and innovation, with the investigators criteria carrying less weight.</p><disp-quote><p>Most funding agencies use two or more criteria to assess research proposals.</p></disp-quote><p>The ARC also tries to avoid various forms of bias when making decisions. &quot;Researchers who are early in their research career or have had an interrupted research career – including employment outside academia, unemployment, child birth, carers' responsibilities and other personal circumstances – will have this taken into account,&quot; says Leanne Harvey, who is the council's executive general manager.</p></sec><sec id="s2"><title>Ask the experts</title><p>Despite using different sets of criteria for different funding schemes, all proposals for ARC funding pass through the same peer review process. Firstly, two or more Detailed Assessors – researchers who have expertise in the same field as the proposal – provide written assessments and ratings based on the relevant criteria. The applicants are then given the opportunity to provide feedback on these assessments before the application is passed on to two or more General Assessors, who are researchers working in the same or related fields. The General Assessors use their own expertise, plus their broader knowledge of research planning and other issues, to examine the written assessments and applicant feedback and give each application an initial assessment score. The initial scores are used to produce a ranked list of proposals for a committee of General Assessors to discuss. This committee makes recommendations that are considered by the CEO and then passed on to the Minister, who determines which proposals should be funded.</p><p>Virtually all applications submitted to the NSF, which received a staggering 49,620 research proposals in the 2015 fiscal year alone, are independently evaluated by at least three external experts who provide written assessments. An NSF program officer examines these assessments and makes recommendations for funding. Before an application can be funded it must be signed off by the relevant division director and then reviewed for business, financial and policy implications by another NSF official.</p><p>Other funding agencies, including Cancer Research UK (CRUK) and the ERC, often use two-step peer review processes to reduce the burden on external reviewers. A panel of external experts who work in the relevant research fields initially screens the proposals based on an outline or summary. Only the proposals that pass this first stage are sent to two or more independent experts, who are asked to provide detailed reviews on the full applications. The original panel then examines these reviews and is responsible for making the final decision about whether to fund the research. For large grant applications to CRUK, the panel may also interview the applicant, providing him or her with the opportunity to discuss and counter any concerns.</p><p>&quot;For most grants, and depending on the size of the award, we seek anywhere between three and six written reviews,&quot; says Kaiser of CRUK. The number of reviewers is an important consideration because two people assessing the same research proposal can come to different conclusions, making it difficult for decision makers to score the proposal and rank it against others. In 2012, a study that examined data on peer review at the Fonds zur Förderung der wissenschaftlichen Forschung in Austria recommended that bioscience research proposals should be assessed by a minimum of three independent reviewers to help compensate for this variation (<xref ref-type="bibr" rid="bib2">Mutz et al., 2012</xref>).</p><p>Once a funding agency has recruited reviewers, it needs to ensure that they follow the agency's policies and apply its selection criteria correctly. At the ERC this starts with the President of the council briefing the chairs of its 25 peer review panels on the key principles of the ERC evaluation process, including its policy on gender balance, measures on widening participation, conflicts of interest and matters of confidentiality.</p><p>As a member of the Association of Medical Research Charities, CRUK is bound by the association's principles of peer review. &quot;This means our review processes are independent, impartial, balanced, accountable and include a diversity of opinion,&quot; says Kaiser. To ensure CRUK is meeting these requirements, the charity's review processes are audited every five years.</p></sec><sec id="s3"><title>A different approach</title><p>The Volkswagen Foundation – a private research agency founded in 1962 in Germany – has been testing out alternative ways to assess applications for some of its funding schemes. For example, in the <italic>Experiment!</italic> scheme, which supports researchers in science and engineering who want to investigate daring new ideas, double-blind peer review ensures that the interdisciplinary jury reviewing the applications assesses the quality of the idea and not the reputation of the person behind it.</p><p>Since <italic>Experiment!</italic> grants are so broad in scope it is possible that funding decisions could be biased towards fields that the jury are more familiar with. To address this issue, the Volkswagen Foundation is currently running a trial that uses a lottery to increase the diversity of the proposals they fund. First, the foundation screens the applications to make sure that they address the program criteria. Anonymized versions of the shortlisted applications are then passed to the jury, which rejects any that are not of sufficient quality. From the pool of good quality proposals that remains, the jury selects the ideas they find most convincing to receive funding. The other good quality proposals have a second chance to be awarded funding through the lottery overseen by the foundation's legal officer. In this way the foundation hopes to &quot;uncover projects which otherwise are easily overlooked,&quot; says program director Ulrike Bischler. &quot;Depending on the results of an independent evaluation, this partial randomization approach might be used in more funding initiatives in the future.&quot;</p><p>Of course the story does not end here. Once a researcher has received his or her grant they will need to do the research that they promised to do. If all goes well, they will submit the results to a journal – where, once again, they will find themselves under scrutiny from two or more of their peers.</p></sec><sec id="s4"><title>Note</title><p>This Feature Article is part of a <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/collections/0a5cf428/peer-review">collection of articles on peer review</ext-link>.</p></sec></body><back><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Matthews</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>UK grant success rates prompt worldwide comparisons</article-title><source>Times Higher Education</source><ext-link ext-link-type="uri" xlink:href="https://www.timeshighereducation.com/news/uk-grant-success-rates-prompt-worldwide-comparisons#survey-answer">https://www.timeshighereducation.com/news/uk-grant-success-rates-prompt-worldwide-comparisons#survey-answer</ext-link><date-in-citation iso-8601-date="2017-09-08">September 08, 2017</date-in-citation></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mutz</surname> <given-names>R</given-names></name><name><surname>Bornmann</surname> <given-names>L</given-names></name><name><surname>Daniel</surname> <given-names>HD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Heterogeneity of inter-rater reliabilities of grant peer reviews and its determinants: a general estimating equations approach</article-title><source>PLoS One</source><volume>7</volume><elocation-id>e48509</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0048509</pub-id><pub-id pub-id-type="pmid">23119041</pub-id></element-citation></ref></ref-list></back></article>