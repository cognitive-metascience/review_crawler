<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">46856</article-id><article-id pub-id-type="doi">10.7554/eLife.46856</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Resolving multisensory and attentional influences across cortical depth in sensory cortices</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-134585"><name><surname>Gau</surname><given-names>Remi</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1535-9767</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-134586"><name><surname>Bazin</surname><given-names>Pierre-Louis</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131273"><name><surname>Trampel</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-91970"><name><surname>Turner</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-168415"><name><surname>Noppeney</surname><given-names>Uta</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Computational Neuroscience and Cognitive Robotics Centre</institution>, <institution>University of Birmingham</institution>, <addr-line><named-content content-type="city">Birmingham</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff2"><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution>, <addr-line><named-content content-type="city">Leipzig</named-content></addr-line>, <country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-28130"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Reviewing editor</role><aff><institution>Radboud University</institution>, <country>Netherlands</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>remi_gau@hotmail.com</email> (RG);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>08</day><month>01</month><year>2020</year></pub-date><volume>9</volume><elocation-id>e46856</elocation-id><history><date date-type="received"><day>14</day><month>03</month><year>2019</year></date><date date-type="accepted"><day>07</day><month>01</month><year>2020</year></date></history><permissions><copyright-statement>Â© 2020, Gau et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Gau et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-46856.pdf"/><abstract><p>In our environment our senses are bombarded with a myriad of signals, only a subset of which is relevant for our goals. Using sub-millimeter-resolution fMRI at 7T we resolved BOLD-response and activation patterns across cortical depth in early sensory cortices to auditory, visual and audiovisual stimuli under auditory or visual attention. In visual cortices, auditory stimulation induced widespread inhibition irrespective of attention, whereas auditory relative to visual attention suppressed mainly central visual field representations. In auditory cortices, visual stimulation suppressed activations, but amplified responses to concurrent auditory stimuli, in a patchy topography. Critically, multisensory interactions in auditory cortices were stronger in deeper laminae, while attentional influences were greatest at the surface. These distinct depth-dependent profiles suggest that multisensory and attentional mechanisms regulate sensory processing via partly distinct circuitries. Our findings are crucial for understanding how the brain regulates information flow across senses to interact with our complex multisensory world.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>European Research Council</institution></institution-wrap></funding-source><award-id>Mult-sens</award-id><principal-award-recipient><name><surname>Noppeney</surname><given-names>Uta</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>Max Planck Society</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Turner</surname><given-names>Robert</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All procedures were approved by the Ethics Committee of the University of Leipzig under the protocol number 273-14: &quot;Magnetresonanz-Untersuchungen am Menschen bei 7 Tesla&quot;. Participants gave written informed consent to participate in this fMRI study.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Data (sufficient to recreate figures) are publicly available on the OSF project of this study: https://osf.io/63dba/.The raw data of the results presented here are available in a BIDS format upon request: the consent form originally signed by the participants did not allow for making raw data publicly available.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Gau R</collab></person-group><year iso-8601-date="2019">2019</year><source>AV - attention - 7T</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/63dba/">https://osf.io/63dba/</ext-link><comment>Open Science Framework, 63dba</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-46856-supp.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>