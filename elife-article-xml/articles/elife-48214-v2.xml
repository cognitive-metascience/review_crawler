<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">48214</article-id><article-id pub-id-type="doi">10.7554/eLife.48214</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The perception and misperception of optical defocus, shading, and shape</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-140573"><name><surname>Mooney</surname><given-names>Scott WJ</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0094-7638</contrib-id><email>scm2011@med.cornell.edu</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-142590"><name><surname>Marlow</surname><given-names>Phillip J</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-58990"><name><surname>Anderson</surname><given-names>Barton L</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">School of Psychology</institution><institution>The University of Sydney</institution><addr-line><named-content content-type="city">Sydney</named-content></addr-line><country>Australia</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Fleming</surname><given-names>Roland W</given-names></name><role>Reviewing Editor</role><aff><institution>University of Giessen</institution><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>12</day><month>07</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e48214</elocation-id><history><date date-type="received" iso-8601-date="2019-05-05"><day>05</day><month>05</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-07-11"><day>11</day><month>07</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Mooney et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Mooney et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-48214-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.48214.001</object-id><p>The human visual system is tasked with recovering the different physical sources of optical structure that generate our retinal images. Separate research has focused on understanding how the visual system estimates (a) environmental sources of image structure and (b) blur induced by the eye’s limited focal range, but little is known about how the visual system distinguishes environmental sources from optical defocus. Here, we present evidence that this is a fundamental perceptual problem and provide insights into how and when the visual system succeeds and fails in solving it. We show that fully focused surface shading can be misperceived as defocused and that optical blur can be misattributed to the material properties and shape of surfaces. We further reveal how these misperceptions depend on the relationship between shading gradients and sharp contours, and conclude that computations of blur are inherently linked to computations of surface shape, material, and illumination.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.48214.002</object-id><title>eLife digest</title><p>We perceive the visual world as made of objects of different shapes, sizes and colors. Some may be smooth, shiny and reflective, whereas others are rough and uneven; some may be in shadow, while others are brightly lit. The brain must identify and distinguish all of these different features to build an accurate, three-dimensional model of the environment.</p><p>Information about any visual feature originates as light bouncing off an object and entering the eye, which then captures the reflected light and focuses it onto the retina. There, cells generate electrical signals for the brain to process. However, different types of visual features can result in the same pattern of activity. The brain must rely on prior knowledge and educated guesses to disentangle the contributions made by different features, but we know little about the processes that make this possible.</p><p>Here, Mooney et al. examine how the visual system can tell whether an object is blurry, or if it presents the smooth light-to-dark shading that can accompany curved shapes. The experiments show that images of shaded curved surfaces can appear blurry even when they are fully in focus. However, adding a specific type of sharp edge, called a bounding contour, eliminates this illusion. This suggests that the brain uses these sharp edges to judge whether an image is in focus. In fact, adding bounding contours can trick the visual system into perceiving a blurry image as sharp.</p><p>Understanding how the human visual system interprets images could lead to advances in computer vision. Artificial vision systems – such as those used in face or license plate recognition – must determine which parts of an image are in focus before attempting to extract visual information. Identifying the cues that enable the human visual system to solve this problem could help to train computers to do the same.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>surface perception</kwd><kwd>focus perception</kwd><kwd>shape perception</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Commonwealth of Australia</institution></institution-wrap></funding-source><award-id>Australian Postgraduate Award</award-id><principal-award-recipient><name><surname>Mooney</surname><given-names>Scott WJ</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The human visual system uses sharp edges to distinguish a smooth, shaded surface from a surface blurred by optical defocus.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>An image of a surface is a product of its three-dimensional (3D) shape, the reflectance and transmittance behavior of its material, the surrounding light sources (the ‘light field’), and the focal parameters of the imaging lens. All of these sources are conflated in the light that reaches the eyes, yet we nevertheless perceive distinct impressions of shape, material, illumination, and focus. One of the fundamental goals of mid-level vision research is to understand how the visual system extracts these different sources of structure.</p><p>Most research into this problem has focused on how the visual system extracts environmental sources of structure – 3D shape, reflectance (color, lightness, gloss), and surface opacity (transparency, translucency, and subsurface scattering). But the focal properties of single-chambered eyes also contribute to the optical structure projected to the retinae. Eyes that utilize refraction to generate a focused image are subject to depth of field defocus, which causes some image regions to be blurred. Optical defects can also cause images to be globally blurred for the increasing number of people that require optical correction. Although there has been a significant body of research into cues that affect the severity of perceived blur (<xref ref-type="bibr" rid="bib4">Ciuffreda et al., 2006</xref>; <xref ref-type="bibr" rid="bib5">Crete et al., 2007</xref>; <xref ref-type="bibr" rid="bib7">Ferzli and Karam, 2006</xref>; <xref ref-type="bibr" rid="bib40">Pentland, 1987</xref>; <xref ref-type="bibr" rid="bib43">Tadmor and Tolhurst, 1994</xref>; <xref ref-type="bibr" rid="bib48">Webster et al., 2002</xref>) and the role of depth of field defocus as a cue to depth (<xref ref-type="bibr" rid="bib11">Held et al., 2010</xref>; <xref ref-type="bibr" rid="bib27">Marshall et al., 1996</xref>; <xref ref-type="bibr" rid="bib28">Mather, 1996</xref>; <xref ref-type="bibr" rid="bib29">Mather, 1997</xref>; <xref ref-type="bibr" rid="bib30">Mather and Smith, 2000</xref>; <xref ref-type="bibr" rid="bib31">Mather and Smith, 2002</xref>; <xref ref-type="bibr" rid="bib36">O'Shea et al., 1997</xref>; <xref ref-type="bibr" rid="bib47">Watt et al., 2005</xref>), it is still unknown how the visual system distinguishes blur from environmental sources of low spatial frequency image structure.</p><p>The computational problem of discriminating optical defocus from environmental sources of low frequency structure does not appear to have been explicitly addressed previously. This may be due to the absence of empirical evidence that the visual system can misattribute image gradients produced by environmental sources to defocus or misattribute gradients produced by defocus to environmental sources. Here, we provide evidence of both. We show that defocus can be experienced in fully focused images and that optical defocus can be misperceived as distortions in the perceived 3D shape of smoothly shaded surfaces.</p><p>Consider the surface depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref>, which was created by illuminating a smooth (i.e., differentiable) Lambertian (‘matte’) surface with a collimated light source. The surface has shallow surface relief to avoid the formation of sharp attached shadows and is viewed along the axis of relief to avoid the formation of self-occluding contours. Although rendered as a fully focused surface, this image elicits a strong perception of blur; while some 3D shape from shading may be perceived, the surface appears ‘contaminated’ by optical defocus.</p><p>The perceptual conflation of low frequency shading gradients and optical focus does not appear to have been previously reported. Most research into the perception of shading has attempted to understand how shading provides information about 3D shape using images where it was assumed or somehow ‘known’ that the intensity gradients were caused by focused patterns of shading. The potential conflation of shading and blur may have been overlooked because of the particular surface geometries and viewing conditions that were used in these studies. Most previous work on shape from shading has studied images that contained sharp contours generated by either smooth self-occlusions or abrupt bounding contours (e.g. <xref ref-type="bibr" rid="bib13">Horn and Brooks, 1989</xref>; <xref ref-type="bibr" rid="bib18">Koenderink et al., 2001</xref>; <xref ref-type="bibr" rid="bib32">Mingolla and Todd, 1986</xref>; <xref ref-type="bibr" rid="bib39">Pentland, 1984</xref>; <xref ref-type="bibr" rid="bib41">Ramachandran, 1988</xref>; <xref ref-type="bibr" rid="bib44">Todd et al., 1996</xref>), while experiments that have used ‘terrain’ surfaces similar to <xref ref-type="fig" rid="fig1">Figure 1</xref> have predominantly been investigations of illumination perception (<xref ref-type="bibr" rid="bib19">Koenderink et al., 2004</xref>; <xref ref-type="bibr" rid="bib20">Koenderink et al., 2007</xref>). This suggests that the visual system may exploit sharp contours in generating percepts of focus when viewing low frequency shading gradients. If so, it should be possible to eliminate the perception of illusory blur in <xref ref-type="fig" rid="fig1">Figure 1</xref> by introducing sharp bounding contours.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.003</object-id><label>Figure 1.</label><caption><title>A deformed matte terrain illuminated by a light source elevated 45° above the line of sight.</title><p>Despite being rendered in full focus, the image appears blurry, and the local features of its 3D shape seem difficult to perceive.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig1-v2.tif"/></fig><p>We informally tested this hypothesis by constructing the images depicted in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The image on the left (2A) was generated by intersecting the surface in <xref ref-type="fig" rid="fig1">Figure 1</xref> with a gray plane parallel to the direction of its relief and occluding all surface regions beyond the depth of that plane (a particular form of ‘planar cut’ dubbed a ‘level cut’). Informal observation suggests that this manipulation enhances the perceived 3D shape of the surface and completely eliminates the perception of blur. Note also that the contour appears to be unambiguously ‘owned’ by (attached to) the shading gradients rather than the homogeneous gray regions. Now consider the image in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, which was created by treating the gray regions in <xref ref-type="fig" rid="fig2">Figure 2A</xref> as parts of a single gray ‘mask’ and rotating it 180 degrees over the shaded surface. This image contains the exact same contours as <xref ref-type="fig" rid="fig2">Figure 2A</xref>, but the gray regions occlude different portions of the shaded surface. There are a number of striking perceptual differences evoked by comparison of <xref ref-type="fig" rid="fig2">Figure 2B</xref> and <xref ref-type="fig" rid="fig2">Figure 2A</xref>. First, whereas the perceived shape from shading is <italic>enhanced</italic> in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, the perception of shape from shading is <italic>impaired</italic> by the contours in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. A second difference is that the border ownership of the contour in <xref ref-type="fig" rid="fig2">Figure 2B</xref> is ambiguous; whereas the contours in <xref ref-type="fig" rid="fig2">Figure 2A</xref> appear unambiguously attached to the shading gradients, the contours in <xref ref-type="fig" rid="fig2">Figure 2B</xref> can appear attached to either side. The way the border ownership is perceived can have a dramatic impact on how the gradients are perceived. When the contours appear attached to the shading gradients, the gradients are perceived as unstructured 2D ‘noise’ (such as variations in pigment) without any clear sense of 3D shape or optical focus. But when the contours appear attached to the gray mask, the shaded surface appears as a partially occluded shaded surface that is just as blurred as the surface in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.004</object-id><label>Figure 2.</label><caption><title>Orientation-intensity covariation induced by level cut contours.</title><p>(<bold>A</bold>) The same shaded surface as shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, but now partially occluded by a gray level cut mask. The mask’s contours were created by intersecting the deformed terrain with a flat plane, and completely eliminate the percepts of blur experienced in <xref ref-type="fig" rid="fig1">Figure 1</xref>. The plot beneath reveals how image intensity at the contour varies as an approximate cosine function of orientation. Every pixel of shading along the contour is represented in this plot as a black dot. The correlation coefficient of the cosine fit and its p-value are shown within the graph. (<bold>B</bold>) The level cut mask that occludes the gradients in (<bold>A</bold>) has been rotated by 180 degrees over the image, which eliminates the relationship between the contours and surface geometry. The plot below the image reveals the destructive effect of this rotation on the systematic covariation between contour orientation and image intensity. Here, the shading gradients are misperceived as either a flat texture or a blurred surface beneath a floating stencil. (<bold>C</bold>) The contours shown in (<bold>A</bold>) now form a mask that occludes every part of the shaded surface in front of the level cut. The orientation-intensity covariation along the contour is equally strong, but the 180° shift in its phase causes the gradients to appear bistable: either bumps lit from below, or concavities lit from above. These gradients appear less focused overall than the unambiguously convex surface in (<bold>A</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48214.005</object-id><label>Figure 2—figure supplement 1.</label><caption><title>The ‘convex’ level cut (top) and rotated (bottom) mask conditions used in Experiment 1.</title><p>The masks in the top row exclude all surface regions at a greater depth than the level cut contours, and the remaining visible gradients appear vividly convex. These masks have been rotated by 180° in the bottom row, which destroys the geometric relationship between the contours and the 3D surface. The percentage of visible gradients increases with the depth of the level cut from left to right. The top-center image is also shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48214.006</object-id><label>Figure 2—figure supplement 2.</label><caption><title>The ‘bistable’ level cut (top) and rotated (bottom) mask conditions used in Experiment 1.</title><p>The level cut contours of the masks in the top row are identical to those in the top row of <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, but the masks now exclude all surface regions at a nearer depth than the level cut contours. The visible gradients are bistable: they can appear as concave dents illuminated from above or convex bumps illuminated from below (or neither). The bottom row depicts rotated versions of these masks that are no longer related to surface geometry. Note that the depth of the level cut now decreases from left to right to produce images with a greater percentage of visible gradients. The top-center image is also shown in <xref ref-type="fig" rid="fig2">Figure 2C</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig2-figsupp2-v2.tif"/></fig></fig-group><p>What is responsible for the striking perceptual differences observed in <xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="fig" rid="fig2">Figure 2B</xref>? Why does one set of contours eliminate the perceived blur, enhance perceived 3D shape, and unambiguously determine the side of the contour that is figure, while the other does not? We suggest here that the perceived focus and enhanced perception of 3D shape arises because the level cut images approximate the geometric and photometric image properties generated by smooth self-occluding contours. More specifically, we will argue that there are two constraints that play a causal role in eliciting the perception of focus in images of shaded surfaces: photogeometric constraints that arise generically along smooth self-occlusions; and the attachment of shading gradients to convex surfaces. We consider each in turn.</p><p>The first constraint arises from the physics of surface reflectance and the projective geometry of smooth self-occluding rims. Both shading intensity and bounding contour shape depend on the same environmental property – the local 3D shape of the surface – and are therefore inherently linked. This link makes it possible to combine two well-known constraints about the shape of the contours generated by smooth self-occlusions and the shading intensity into a novel constraint (<xref ref-type="bibr" rid="bib26">Marlow et al., 2019</xref>). First, it is known that the 3D shape along a smooth self-occluding rim can be derived from its 2D image contour: the local slant of the contour relative to the observer (i.e., how much it deviates from fronto-parallel) is constant (approximately 90 degrees), and the tilt of the surface (the direction in which it slants away from the observer) is specified by the orientation of the rim’s image contour (<xref ref-type="bibr" rid="bib2">Barrow and Tenenbaum, 1978</xref>). Second, it is known that local 3D surface orientation is primarily responsible for shading intensity (<xref ref-type="bibr" rid="bib13">Horn and Brooks, 1989</xref>), subject to some modification by vignetting and/or interreflections (<xref ref-type="bibr" rid="bib22">Langer and Zucker, 1994</xref>). The mutual dependence of contour orientation and shading intensity on the same local 3D surface orientation causes shading to covary with the orientation of the rim’s projected image contour: for a Lambertian surface illuminated by a collimated light source, intensity will decline as a cosine function of contour orientation relative to the brightest point along the contour (i.e., the orientation most closely aligned with the illumination direction). This relationship holds exactly for surfaces illuminated by collimated light sources (see Materials and methods), and we have previously shown that it is statistically robust in natural light fields that contain multiple sources of illumination (<xref ref-type="bibr" rid="bib26">Marlow et al., 2019</xref>). We refer to this relationship as the <italic>orientation-intensity covariation</italic> of the contour and its adjacent shading.</p><p>The covariation of contour orientation and intensity along smooth self-occluding rims generalizes to other types of bounding contours that have been shown to affect perceived shape from shading, such as ‘planar cuts’ (i.e., contours formed by ‘slicing’ a shaded surface with a plane; <xref ref-type="bibr" rid="bib26">Marlow et al., 2019</xref>). The level cut image in <xref ref-type="fig" rid="fig2">Figure 2A</xref> is one example of a planar cut. The orientation-intensity covariation that arises along planar cuts of shaded surfaces is similar to that generated by smooth self-occluding rims (see Materials and methods). The relationship between contour orientation and shading intensity along the contours in <xref ref-type="fig" rid="fig2">Figure 2</xref> is depicted in the plot below each image. Note that <xref ref-type="fig" rid="fig2">Figure 2A</xref> exhibits a clear cosine-like relationship, whereas <xref ref-type="fig" rid="fig2">Figure 2B</xref> exhibits no covariation at all. However, this covariation is generally weaker for planar cuts than for self-occluding rims: more than one 3D surface orientation can generate the same 2D contour orientation, which means that identically oriented planar cut segments can project different shading intensities in the image (as can be seen upon close examination of <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Nonetheless, we previously showed that planar cuts exhibit a robust orientation-intensity covariation (apart from a few degenerate cases; <xref ref-type="bibr" rid="bib26">Marlow et al., 2019</xref>), which suggests that this covariation could provide a reliable cue that the visual system uses to identify the bounding contours of shaded surfaces.</p><p>The second property that links the level cut image in <xref ref-type="fig" rid="fig2">Figure 2A</xref> to images containing smooth self-occlusions is that they are both globally convex (<xref ref-type="bibr" rid="bib16">Koenderink, 1984</xref>). If the covariation of intensity and contour orientation along sharp bounding contours is sufficient to explain the perception of focus in shaded surfaces, then it should not matter whether these contours bound a convex or concave surface. The importance of convexity can be assessed by constructing a level cut surface that removes all of the convex surface regions that appear in front of the cut and displaying only the ‘valleys’ (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). This stimulus exhibits the same orientation-intensity covariation, but is inherently ambiguous: it can be perceived as a convex surface illuminated from below or as a concave surface illuminated from above (e.g. <xref ref-type="bibr" rid="bib41">Ramachandran, 1988</xref>; see <xref ref-type="bibr" rid="bib23">Liu and Todd, 2004</xref>). There are two well-established biases that determine how such ambiguities are resolved: a bias to perceive the illumination as coming from above (<xref ref-type="bibr" rid="bib3">Belhumeur et al., 1999</xref>; <xref ref-type="bibr" rid="bib18">Koenderink et al., 2001</xref>) and a bias to perceive surfaces as convex (<xref ref-type="bibr" rid="bib12">Hill and Bruce, 1994</xref>; <xref ref-type="bibr" rid="bib21">Langer and Bülthoff, 2001</xref>). These two biases are aligned in <xref ref-type="fig" rid="fig2">Figure 2a</xref>, which is presumably why this surface is perceived as a stable convex surface illuminated from above. However, these biases are in conflict in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, causing some perceptual bistability. Informal observations suggest that perceived focus depends on the perceived convexity or concavity of the surface. When the surface appears convex, no clear percept of blur is experienced; but when it appears concave, the surface appears blurred. This implies that the strength of the orientation-intensity covariation along the contours is not the sole determinant of the perception of shading, focus, and contour attachment; the convexity of the surface is also critical.</p><p>Our previous work showed that photogeometric cues along smooth self-occlusions and planar cuts of 1D luminance profiles provide information that predicts when identical luminance gradients are perceived as surface shading of 3D surfaces. The experiments described below were designed to psychophysically assess the relationship between bounding contour orientation, shading intensity, and contour sharpness on the perception of surface shading, optical defocus, and border ownership.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>The goal of Experiment 1 was to test if sharp image contours cause bounded shading gradients to appear more focused when contour orientation covaries with the intensity of the bounded gradients. This covariation is typically exhibited along contours generated by self-occlusions. Self-occlusions feature prominently in prior work on shape from shading, but their contours are difficult to manipulate without altering the geometry of the entire shaded surface (and hence the shading gradients). However, it is possible to approximate the photogeometric behavior along smooth self-occlusions with planar cuts. This was accomplished by slicing the bumpy plane depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref> with a plane oriented perpendicular to the axis of surface relief (a ‘level cut’; see Materials and methods for details). The resulting level cut contours exhibit an orientation-intensity covariation similar to the covariation along self-occluding contours, but unlike self-occlusions, level cuts can be generated anywhere on the surface and place no constraints on local surface curvature. The photogeometric constraints of self-occlusions and level cuts are described in further detail in the Materials and methods.</p><p>To assess the role of the photogeometric covariation along bounding contours in the perception of 3D shape, border ownership, and optical focus, three sets of level cut contours were created by intersecting the surface in <xref ref-type="fig" rid="fig1">Figure 1</xref> with a fronto-parallel plane at different depths along the axis of surface relief. Each set of contours was used to generate four homogenous gray masks. In the first condition, each mask occluded all surface regions that lay at a greater depth than the level cut, leaving only the shaded peaks visible (e.g. <xref ref-type="fig" rid="fig2">Figure 2A</xref>). The visual system’s biases toward interpreting shaded surfaces as convex and top-lit are aligned in this image, and the visible surface regions consequently appear unambiguously convex; we therefore refer to this condition as the ‘convex’ level cut condition. In a second condition, the same contours were used to generate a complementary mask that removed all regions in <italic>front</italic> of the level cut, leaving only the shaded valleys visible (e.g. <xref ref-type="fig" rid="fig2">Figure 2C</xref>). The visual system’s convexity and illumination biases conflict in this image; the shaded regions may appear as concave dents illuminated from above or convex bumps illuminated from below. This conflict can result in some bistability in the perceived illumination direction and convexity/concavity, so we refer to this as the ‘bistable’ level cut condition. The masks in the two ‘rotated’ conditions were created by rotating the level cut masks in the convex and bistable conditions (respectively) by 180° over the underlying gradients, which breaks the orientation-intensity covariation that occurs along the level cuts (e.g. <xref ref-type="fig" rid="fig2">Figure 2B</xref>). The depth of the intersecting plane used to define the contours determined the relative proportion of visible gradients in each masked image; the three depth values were chosen to create masks that preserved 25%, 50%, and 75% of the gradients. Note that the masks in the ‘convex’ conditions with 25% gradients visible are the complements of the masks in the ‘bistable’ conditions with 75% gradients visible, and vice versa. The luminance of each gray mask was set to the average of the original gradient image. The full set of twelve stimuli can be seen in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>.</p><p>If the visual system uses the orientation-intensity covariation between contours and shading to distinguish attached bounding contours from arbitrary edges, then the level cut contour conditions should induce stronger percepts of focus than the rotated contours, which have no relationship with the shading gradients. However, if the visual system is better at inferring contour attachment when the adjacent surface appears convex, then the covariation alone (which provides no curvature cues) may not fully eliminate the perception of blur. If this is true, then the contours in the ‘bistable’ level cut condition should produce weaker percepts of focus than the ‘convex’ level cut condition, as the bistable stimuli may sometimes appear concave.</p><p>Observers judged perceived focus in a paired comparison task and judged perceived surface curvature (convex, concave, or neither) in a separate three-alternative classification task (N = 20). The results confirm our hypotheses and informal observations of the stimuli (<xref ref-type="fig" rid="fig3">Figure 3</xref>) and were analyzed with an ANOVA and appropriate two-sided contrasts. Observers perceived the level cut stimuli (solid lines in <xref ref-type="fig" rid="fig3">Figure 3A</xref>) as appearing more focused than the rotated stimuli (dotted lines in <xref ref-type="fig" rid="fig3">Figure 3A</xref>), <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>19</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>64.68</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf2"><mml:mo>[</mml:mo><mml:mn>25.33</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>43.15</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, Cohen’s <inline-formula><mml:math id="inf3"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1.85</mml:mn></mml:math></inline-formula>, and further perceived the ‘convex’ level cut stimuli as more focused than the ‘bistable’ level cut stimuli, <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>19</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>7.05</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf5"><mml:mo>[</mml:mo><mml:mn>20.45</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>37.73</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, Cohen’s <inline-formula><mml:math id="inf6"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1.62</mml:mn></mml:math></inline-formula>. All observers perceived the ‘convex’ level cut surfaces as convex bumps on all trials (first plot in <xref ref-type="fig" rid="fig3">Figure 3B</xref>), whereas the gradients occluded by rotated masks were most likely to be perceived as neither bumps nor dents (third and fourth plots in <xref ref-type="fig" rid="fig3">Figure 3B</xref>). Perceived focus decreased as a function of increasing gradient visibility in the ‘bistable’ level cut condition, <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>19</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>7.07</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the rotated conditions, <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>19</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>5.05</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, but not the convex level cut condition, <inline-formula><mml:math id="inf9"><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>19</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1.73</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.100</mml:mn></mml:math></inline-formula>, which suggests that the convex level cut masks were the only stimuli that were perceived as (equally) fully focused. In the other three mask conditions, it is likely that observers simply preferred to select images in which less of the perceptually blurred gradients were visible.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.007</object-id><label>Figure 3.</label><caption><title>Perceived focus and shape in Experiment 1.</title><p>(<bold>A</bold>) Perceived focus in Experiment 1. The horizontal axis represents the percentage of gradients visible and the four lines represent the four combinations of mask rotation (level cut vs. rotated) and mask occlusion style (‘convex’ vs. ‘bistable’). The vertical axis represents the percentage of trials in which each stimulus was chosen as appearing most focused. Error bars represent ± 1 S.E.M. (<bold>B</bold>) Perceived curvature type in Experiment 1. In each stacked column plot, the horizontal axis represents the percentage of visible gradients and the vertical axis represents the percentages of observers who perceived that stimulus as convex bumps (black), concave dents (light gray), or neither (dark gray). Each plot depicts a different combination of mask rotation (level cut vs. rotated) and mask occlusion style (‘convex’ vs. ‘bistable’). The observers were the same group as in (<bold>A</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig3-v2.tif"/></fig><p>The data suggest that the effects of the different contours on perceived focus are mediated by the perception of convexity, which is strongest when the contours exhibit an orientation-intensity covariation with the shading and the convex interpretation is consistent with percepts of top-down illumination. Notably, perceived focus and perceived convexity simultaneously decreased in the ‘bistable’ conditions – but not the ‘convex’ conditions – as the ratio of visible gradients increased. There was a significant correlation between perceived focus and the proportion of observers who rated each stimulus as appearing convex in shape, <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.904</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. This correlation remained significant even when the unambiguous ‘convex’ mask conditions were excluded from the analysis, <inline-formula><mml:math id="inf11"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.777</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.014</mml:mn></mml:math></inline-formula>. This finding supports our informal observation from <xref ref-type="fig" rid="fig2">Figure 2</xref>: contours generate stronger cues to contour attachment when they bound surfaces regions that appear convex, and these contours are therefore more likely to propagate focus cues (produced by their sharpness) to the shaded surface. This may also explain why prior studies using globally convex shaded 3D shapes have not observed any illusory gradient blur: the self-occluding contours of these stimuli not only exhibit a strong orientation-intensity covariation (<xref ref-type="bibr" rid="bib26">Marlow et al., 2019</xref>) but also necessarily bound surface regions that have convex curvature in at least one direction. The bounding contours of surface regions that appear concave, however, can appear as occluding edges of the gray mask, similar to the percept that arises from the rotated contours (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) when the gray regions appear to ‘own’ the contours and the surface appears occluded.</p><p>The results of Experiment 1 suggest that sharp contours that exhibit an orientation-intensity covariation with nearby shading (such as level cuts) provide information about image focus and enhances percepts of 3D shape within the shaded surface. In Experiment 2, we tested the importance of this covariation directly by parametrically varying the strength of the orientation-intensity covariation along contours. This was accomplished with images of smoothly shaded ‘ribbons’ on a gray background. These images were created by displaying only the shading gradients immediately adjacent to the mask contours used in Experiment 1. Thus, the only source of information about 3D shape is the relationship between the intensity and orientation along the ribbon. Sixteen unique ribbon paths (two pixels wide) were created by generating more smoothly deformed 3D surfaces and intersecting them with planes to produce level cut contours, but the ribbon was manually shaded in MATLAB according to the orientation of its path. This allowed us to create ribbons that exhibited perfect orientation-intensity covariation that could be parametrically decreased to any arbitrary value. Ribbons were constructed such that intensity decreased in relative intensity from 0.8 to 0.2 as a linear function of its orientation relative to 90° (the brightest point). The covariation was then progressively weakened by gradually adding increasing amounts of random low-frequency noise to the ribbon gradients. The gray background had a relative intensity of exactly 0.5.</p><p><xref ref-type="fig" rid="fig4">Figure 4</xref> depicts three example stimuli. The ribbons in the left panel exhibit a perfect linear orientation-intensity covariation, which induces a vivid percept of surface relief: some of the gray regions are perceived as ‘plateaus’ raised above the adjacent recessed regions. The Pearson correlation coefficient between shading intensity and contour orientation (relative to 90°) measured directly from each image is shown in the lower-right corner. In the center panel, the intensity of a different perfectly covarying ribbon has been mixed in equal proportion with low-frequency noise. A moderate degree of covariation remains, but it is not consistent across the image, and the overall impression of stepped relief is substantially weaker. In the right panel, ribbon intensity has been generated entirely by noise; no covariation is present, and no impression of 3D relief is apparent.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.008</object-id><label>Figure 4.</label><caption><title>Example stimuli used in Experiment 2.</title><p>The stimuli were created by mixing varying amounts of random noise with shaded ribbons designed to exhibit a perfect linear correlation between orientation (relative to 90°) and intensity. The examples shown here increase in noise from left to right. The computed global correlations between relative ribbon orientation and intensity are shown in the bottom-right of each stimulus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig4-v2.tif"/></fig><p>Observers (N = 15) were shown randomly-generated ribbon images with thirteen values of noise proportion ranging from 0% (as in the left panel of <xref ref-type="fig" rid="fig4">Figure 4</xref>) to 100% (as in the right panel of <xref ref-type="fig" rid="fig4">Figure 4</xref>). These values were not evenly spaced but were instead selected to produce an approximately uniform distribution of correlation coefficients between 1 and 0 when the covariation was measured in each image. Each observer viewed all possible pairs of these thirteen noise values and were instructed to select the image that appeared more three-dimensional in each pair.</p><p>The results confirm our informal experience of <xref ref-type="fig" rid="fig4">Figure 4</xref>: 3D shape percepts monotonically decreased in strength as a function of increasing ribbon noise (left plot in <xref ref-type="fig" rid="fig5">Figure 5</xref>). This relationship was verified with a linear regression, with the likelihood of being selected as appearing more three-dimensional decreasing by approximately one percentile for every percentile increase in ribbon noise, <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1.025</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.923</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Further analysis revealed that this effect was mediated by the amount of orientation-intensity covariation present in the images: mean perceived 3D shape strength also decreased monotonically with the value of the Pearson correlation coefficient when the correlations measured from all presented stimuli were sorted into bins of width 0.1 (right plot in <xref ref-type="fig" rid="fig5">Figure 5</xref>). These findings do not directly address the issue of perceived defocus, but do support our hypothesis that the photo-geometric behavior occurring at the very edge of covarying contours (such as the level cut contours in <xref ref-type="fig" rid="fig2">Figure 2A</xref>) is sufficient to generate the vivid impressions of contour attachment observed in the ‘convex’ conditions of Experiment 1. The data also reinforce our previous findings on the importance of this covariation in generating percepts of 3D shaded shape (<xref ref-type="bibr" rid="bib26">Marlow et al., 2019</xref>).</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.009</object-id><label>Figure 5.</label><caption><title>Perceived 3D shape in Experiment 2.</title><p>The horizontal axes represent the percentage of gradient noise in the shaded ribbons (left) and bins of computed Pearson correlation coefficients between ribbon orientation and shading intensity (right). The rightmost bin in the right panel contains all correlation coefficients below 0.4, and all other bins have a width of 0.1. The vertical axis in each plot represents the percentage of trials in which each condition (left) or correlation value (right) was chosen as appearing most vividly 3D out of the total number of trials in which that condition or correlation value appeared. Error bars represent ± 1 S.E.M.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig5-v2.tif"/></fig><p>The results of Experiments 1 and 2 suggest that the <italic>absence</italic> of an orientation-intensity covariation along contours can lead to the misperception of optical defocus and impair the perception of 3D shape. In Experiment 3, we investigated whether image structure that has been optically blurred by defocus can be misperceived as focused if there are sharp contours present nearby that exhibit an orientation-intensity covariation; that is we tested whether the <italic>presence</italic> of such contours can effectively mask the visibility of optically induced blur. We tested this by constructing two variants of the shaded terrain in <xref ref-type="fig" rid="fig1">Figure 1</xref>. One variant was similar to the level cut image, which contained sharp bounding contours a given relief height (middle of the bottom row of <xref ref-type="fig" rid="fig6">Figure 6</xref>), and therefore referred to as the level cut image. The other image was constructed by geometrically smoothing the sharp level cut contours of this image, resulting in the image depicted in the middle of the top row of <xref ref-type="fig" rid="fig6">Figure 6</xref>. As in <xref ref-type="fig" rid="fig1">Figure 1</xref>, the absence of sharp, intensity-correlated contours causes this surface to appear blurred even though it was rendered in full focus. We refer to this surface as the ‘smoothed’ condition. The difference in perceived blur experienced with these two images reinforce the importance of contour sharpness for inducing percepts of focused gradients: the absence of sharp edges in the smoothed condition greatly reduces perceived focus, even though the changes in surface curvature at the edges of the bumps are relatively abrupt.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.010</object-id><label>Figure 6.</label><caption><title>Stimuli used in Experiment 3.</title><p>The top row depicts the ‘smoothed’ surface with no intersecting plane and the bottom row depicts the ‘level cut’ surface with the intersecting plane. From left to right, the columns depict the strong background blur, weak background blur, no blur, weak foreground blur, and strong foreground blur conditions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig6-v2.tif"/></fig><p>To assess sensitivity to optical focus, both shaded surfaces were subject to different degrees of optical defocus, with the focal length set to either the background (so the contours surrounding the bumps in the level cut condition remained sharp) or the tips of the bumps (so the contours in the level cut condition were blurred by defocus). The effects of increasing background blur and increasing foreground blur can be seen to the left and right of the center column in <xref ref-type="fig" rid="fig6">Figure 6</xref>, respectively. The effects of foreground blur on the level cut surface (bottom-right panels) are particularly striking: when the peaks of the surface are affected by defocus blur, the resulting changes in the image gradients do not appear to significantly change the perceived focus of the surface. In the absence of the level cut contours, however, the same defocus manipulation appears to increase apparent blur (top-right panels).</p><p>These informal observations were bolstered by psychophysical experiments that measured perceived focus for all ten stimuli using a paired comparison task (N = 10). The results (depicted in <xref ref-type="fig" rid="fig7">Figure 7</xref>) align with our informal observations of the stimuli in <xref ref-type="fig" rid="fig6">Figure 6</xref> and were analyzed with appropriate contrasts within an ANOVA (the main effects of which are not relevant here). The vertical axis in <xref ref-type="fig" rid="fig7">Figure 7</xref> represents the percentage of trials in which each stimulus was selected as appearing more focused, and the horizontal axis represents the different blur conditions. The blue and red lines depict perceived focus for the smoothed and level cut conditions, respectively. The data reveal that defocus blur can be misperceived as shading: the three stimuli with sharp level cut contours (i.e.the level cut condition with no blur or foreground blur) were perceived as the most focused, even when the peaks of the surface were actually blurred by moderate or severe foreground defocus. Statistical analysis revealed that foreground blur had a significantly larger (more negative) effect on perceived focus than background blur for the level cut surface, <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>9</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>20.74</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf14"><mml:mo>[</mml:mo><mml:mn>45.29</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>56.38</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, Cohen’s <inline-formula><mml:math id="inf15"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>6.91</mml:mn></mml:math></inline-formula>, but not the smoothed surface, <inline-formula><mml:math id="inf16"><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.31</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.763</mml:mn></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf17"><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>11.49</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>8.71</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>. Furthermore, the fully-focused smoothed surface with no contours did not significantly differ in perceived focus to the level cut stimulus with weak background blur, <inline-formula><mml:math id="inf18"><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.71</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.494</mml:mn></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf19"><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>9.28</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>4.84</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>. These results demonstrate that the presence or absence of sharp contours attached to shading gradients can induce both misperceptions of optical focus or defocus (respectively) when they exhibit a systematic intensity-orientation covariation.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.011</object-id><label>Figure 7.</label><caption><title>Perceived focus in Experiment 3.</title><p>The horizontal axis represents the five blur conditions and the vertical axis represents the percentage of trials in which each stimulus was chosen as appearing most focused. The blue and red lines depict the results for the smoothed and level cut surfaces, respectively. Error bars represent ± 1 S.E.M.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig7-v2.tif"/></fig><p>The results of the preceding experiments suggest that the most important mediating factor in the perception of focused shading gradients is the presence of sharp bounding contours that exhibit covariation between their orientation and adjacent shading intensity. In Experiment 3, we found that optical blur was overestimated when these sharp, covarying contours were absent from the image. Actual changes in image focus, however, were effectively undetected by observers when the sharpness of nearby covarying contours was preserved. In Experiment 4, we tested whether this underestimation of optical blur occurs because the smooth gradient structure is misattributed to environmental sources: that is whether the optical effects of defocus are attributed to the 3D shape and reflectance properties of the defocused surface. Surfaces with higher microscopic roughness (e.g. glossy plastic or matte materials) will scatter incoming light in more directions and produce smoother image gradients instead of the sharp, detailed specular reflections produced by low-roughness surfaces (e.g. mirrors). This effect of surface roughness on image structure is further mediated by curvature: high-curvature surfaces vary in surface orientation more rapidly than low-curvature surfaces, and the shading or reflections they produce are therefore more compressed (i.e., have higher spatial frequency) in the image. We have previously demonstrated that the visual system can misperceive low-curvature, low-roughness surfaces as high-curvature, high-roughness surfaces (<xref ref-type="bibr" rid="bib34">Mooney and Anderson, 2014</xref>), as both combinations of surface properties generate similar gradient structure. The local effects of optical blur on image gradients are similar to the effects of increasing surface roughness or decreasing surface curvature: all three of these physical transformations typically reduce the sharpness of image gradients exhibited by the surface. It is therefore likely that the visual system will have difficulty distinguishing these optical and environmental influences on gradient appearance when they do not produce simultaneous changes in contour sharpness or covariation strength.</p><p>We tested this hypothesis by creating three identically shaped surfaces with different reflectance properties: ‘matte’, ‘rough gloss’, and ‘smooth gloss’. All three materials have an identical diffuse shading component, but the two gloss conditions also contain a specular reflectance component. The amount of scattering in this specular component is greater in the ‘rough gloss’ condition, which consequently produces reflections with less detail than the ‘smooth gloss’ condition. The surface’s 3D shape had higher curvature than the shape used in Experiment 3 to increase the likelihood of generating measurable misperceptions of 3D shape. Level cut contours were created by intersecting the surface with a plane, as in previous experiments, but the matte gray planar surface itself was here included as part of the rendered scene rather than a mask added to the image afterward. The combined surface was rendered in a natural light field with cast shadows, inter-reflections, and chromatic information. This was done to test whether misperceptions of optical blur occur in more realistic viewing conditions, which are particularly important for the appearance of glossy materials (<xref ref-type="bibr" rid="bib8">Fleming et al., 2003</xref>; <xref ref-type="bibr" rid="bib38">Pellacini et al., 2000</xref>).</p><p>Each material condition was rendered with five optical defocus conditions, identical to the conditions in Experiment 3: the ‘foreground blur’ conditions blurred the peaks of the surface but preserved the sharp contours, and the ‘background blur’ conditions blurred the sharp contours, leaving the peaks unaffected. The fifteen stimuli are depicted in <xref ref-type="fig" rid="fig8">Figure 8</xref>, which has a similar layout to <xref ref-type="fig" rid="fig6">Figure 6</xref>. Each row depicts a different material and each column depicts a different focus condition. As background blur increases to the left of the center column, perceived focus appears to decrease for all three materials. As foreground blur increases to the right, the changes in gradient appearance are misattributed to transformations in material (the surface appears more matte) and 3D shape (the surface appears less curved).</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.012</object-id><label>Figure 8.</label><caption><title>Stimuli used in Experiment 4.</title><p>From top to bottom, the rows depict the surfaces with matte reflectance, rough gloss, and smooth gloss. From left to right, the columns depict the conditions with strong background (BG) blur, weak background blur, no blur, weak foreground (FG) blur, and strong foreground blur. Perceived focus and gloss were measured for all fifteen stimuli. Perceived 3D shape was measured horizontally across the prominent vertical ridge in the six stimuli outlined in red. The probe points where shape measurements were taken are shown as red dots in the central stimulus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig8-v2.tif"/></fig><p>We measured perceived image focus, perceived surface gloss, and perceived 3D shape in three distinct tasks. Perceived focus and perceived surface gloss were each measured from the same group of observers (N = 10) in two separate paired comparison tasks. Perceived 3D shape was measured from expert observers (N = 5) for six of the stimuli (outlined in red in <xref ref-type="fig" rid="fig8">Figure 8</xref>) using a line of twenty ‘gauge figure’ probes across the prominent ridge on the left side of each image (red dots in central stimulus of <xref ref-type="fig" rid="fig8">Figure 8</xref>). These probe settings were integrated to form cross-sectional profiles of perceived relief (see <xref ref-type="bibr" rid="bib17">Koenderink et al., 1992</xref>; <xref ref-type="bibr" rid="bib18">Koenderink et al., 2001</xref>). The results accord with our findings in Experiment three and support our informal observations of the stimuli, which are described in separate sections below for each of the three measured properties.</p><p>Observer’s reports of perceived focus are depicted in <xref ref-type="fig" rid="fig9">Figure 9</xref> and were analyzed with appropriate ANOVA contrasts, as in Experiment 3. The vertical axis is the percentage of trials in which each image was selected as appearing more focused. The horizontal axis plots the different blur conditions and each colored line represents a different surface reflectance type. The data indicate that background blur (moving from the central ‘no blur’ condition to the left) significantly reduced perceived focus for all three materials, <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>9</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>39.62</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf21"><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>71.86</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mo>-</mml:mo><mml:mn>64.10</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, Cohen’s <inline-formula><mml:math id="inf22"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>13.21</mml:mn></mml:math></inline-formula>. Foreground blur (moving from the ‘no blur’ condition to the right) also reduced perceived focus, <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>9</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>9.75</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf24"><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>29.92</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mo>-</mml:mo><mml:mn>18.65</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, Cohen’s <inline-formula><mml:math id="inf25"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>3.25</mml:mn></mml:math></inline-formula>, but to a significantly lesser extent than background blur, <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>9</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>34.80</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf27"><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>46.53</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mo>-</mml:mo><mml:mn>40.85</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, Cohen’s <inline-formula><mml:math id="inf28"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>11.60</mml:mn></mml:math></inline-formula>. The data also reveal interactions between blur type and material: in the background blur conditions, the surface with smooth gloss was perceived as significantly more focused than the surface with rough gloss, <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>9</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>3.58</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf30"><mml:mo>[</mml:mo><mml:mn>3.16</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>13.98</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, Cohen’s <inline-formula><mml:math id="inf31"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1.19</mml:mn></mml:math></inline-formula>, which was in turn perceived as more focused than the matte surface, <inline-formula><mml:math id="inf32"><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>4.27</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf33"><mml:mo>[</mml:mo><mml:mn>4.03</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>13.11</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, Cohen’s <inline-formula><mml:math id="inf34"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1.42</mml:mn></mml:math></inline-formula>, but there were no significant differences in perceived focus between materials in the foreground blur conditions, <inline-formula><mml:math id="inf35"><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.13</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.901</mml:mn></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf36"><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>6.67</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>5.95</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula> (smooth vs. rough gloss) and <inline-formula><mml:math id="inf37"><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>1.59</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.146</mml:mn></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf38"><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>19.89</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>3.47</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula> (rough gloss vs. matte). That is, perceived focus decreased as surfaces exhibited more scattering in their reflectance function, but only in the background blur conditions. The effect of material in the background blur conditions suggests that the sharp ridge gradients generated by the more specular materials may have provided useful cues to the focus of the surface peaks. Most of these cues would have been destroyed by defocus in the foreground blur conditions, which may explain why the effect of material disappeared.</p><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.013</object-id><label>Figure 9.</label><caption><title>Perceived focus in Experiment 4.</title><p>The horizontal axis represents the five blur conditions and each colored line represents a different reflectance condition. The vertical axis represents the percentage of trials in which each stimulus was selected as appearing most focused. Error bars represent ± 1 S.E.M.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig9-v2.tif"/></fig><p>Perceived gloss is depicted in <xref ref-type="fig" rid="fig10">Figure 10</xref> and was analyzed with analogous contrasts to perceived focus. The layout of the plot is identical to <xref ref-type="fig" rid="fig9">Figure 9</xref>, but the vertical axis now represents the percentage of trials in which each image was selected as appearing glossier. The data indicate that foreground blur (right side) had a large negative effect on perceived gloss relative to the ‘no blur’ condition, <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>9</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>21.57</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf40"><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>42.09</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mo>-</mml:mo><mml:mn>34.10</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, Cohen’s <inline-formula><mml:math id="inf41"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>7.19</mml:mn></mml:math></inline-formula>. This effect was more severe for the materials with less scattering in their reflectance functions, which is likely because they were accurately perceived as being glossier in the no blur condition. The steep slopes for the smooth and rough gloss conditions indicate that optical defocus in the foreground rapidly destroyed the cues to gloss generated by the sharp surface ridge, but these large decrements in perceived gloss were not accompanied by large decrements in perceived focus. Together, these findings imply that observers partially misattributed the reduction in image focus in the foreground blur conditions to a change in surface material. Increasing background blur (left side) had a small significant negative effect on perceived gloss for the smooth gloss condition, <inline-formula><mml:math id="inf42"><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>4.43</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf43"><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>12.95</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mo>-</mml:mo><mml:mn>4.20</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, Cohen’s <inline-formula><mml:math id="inf44"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1.48</mml:mn></mml:math></inline-formula>, but no significant effect for the rough gloss (<inline-formula><mml:math id="inf45"><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.098</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.924</mml:mn></mml:math></inline-formula>, 95% CI [<inline-formula><mml:math id="inf46"><mml:mo>-</mml:mo><mml:mn>8.57</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>7.85</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>]) or matte (<inline-formula><mml:math id="inf47"><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>2.52</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.033</mml:mn></mml:math></inline-formula>, 95% CI <inline-formula><mml:math id="inf48"><mml:mo>[</mml:mo><mml:mn>0.917</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>16.940</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>) conditions after the significance criterion was corrected with the Bonferroni method. This is likely due to the loss of gloss cues generated by the sharp specular reflections near the defocused base of the smooth glossy ridge.</p><fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.014</object-id><label>Figure 10.</label><caption><title>Perceived gloss in Experiment 4.</title><p>The horizontal axis represents the five blur conditions and each colored line represents a different reflectance condition. The vertical axis represents the percentage of trials in which each stimulus was selected as appearing most glossy. Error bars represent ± 1 S.E.M.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig10-v2.tif"/></fig><p>Cross-sectional depth profiles of perceived 3D shape constructed from the gauge figure settings are depicted in <xref ref-type="fig" rid="fig11">Figure 11</xref>. The profiles represent the average across observers after normalizing the mean height of each observer’s reconstructed profiles to the overall mean height. Differences in shape between the mean profiles were analyzed with an ANOVA in which each of the twenty probe positions was considered an independent sample; significant main effects of this ANOVA represent systematic changes in shape between pairs of focus conditions. The profiles for both the matte (top) and smooth gloss (bottom) conditions reveal that foreground blur had a significant effect on the perceived 3D shape of the ridge relative to the fully focused conditions, <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>19</mml:mn><mml:mo>,</mml:mo><mml:mn>38</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>34.48</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>(matte) and <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>19</mml:mn><mml:mo>,</mml:mo><mml:mn>38</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>38.84</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (smooth gloss). Background blur had no significant impact on perceived shape for either the matte surface, <inline-formula><mml:math id="inf51"><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>19,38</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1.42</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1.78</mml:mn></mml:math></inline-formula>, or the smooth gloss surface, <inline-formula><mml:math id="inf52"><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>19,38</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.92</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.569</mml:mn></mml:math></inline-formula>. The transformations in perceived 3D shape in the foreground blur condition involve a systematic reduction in ridge curvature, height, and position, which suggests that these perceptual distortions were not simply caused by a loss of information. As with perceived gloss, these findings indicate that observers partially misattributed the change in gradient structure induced by foreground blur to a reduction in surface curvature.</p><fig id="fig11" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.015</object-id><label>Figure 11.</label><caption><title>Average shape profiles reconstructed from measurements of surface orientation in Experiment 4.</title><p>The top panel depicts profiles for the matte conditions and the bottom panel depicts profiles for the smooth gloss conditions. The horizontal axis in each plot represents probe location and the vertical axis representsthe height of the contour in normalized units of distance relative to the maximum reconstructed height for each observer. The blue, red, and green lines depict the shape profiles for the strong background (BG) blur, no blur, and strong foreground (FG) blur conditions respectively. Error bars represent ± 1 S.E.M. in normalized units.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig11-v2.tif"/></fig><p>Taken together, the results of the three tasks in Experiment 4 reveal that misperceptions of optical defocus are closely coupled with misperceptions of surface properties that also contribute to the smoothness of image gradients. The image features that appear to modulate these misperceptions in our stimuli are the sharp level cut contours that bound the surface ridge. When these contours were defocused by background blur, observers accurately reported a decrease in image focus, but when the ridge was defocused by foreground blur, the largest perceptual changes were instead in material (less gloss) and shape (less curvature). This suggests that the visual system was directly misattributing the smoothness of the image gradients in the foreground blur conditions to the wrong physical sources.</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The demonstrations and experiments presented herein were designed to assess how the visual system disentangles the blurred, low-frequency image gradients generated by optical defocus from low-frequency gradients generated by focused shaded surfaces. The experiments were designed to assess the importance of photometric and geometric constraints in resolving this ambiguity. Our results revealed the existence of two types of misperceptions: illusory percepts of optical defocus when none is present (Experiments 1 and 3), and misperceptions of 3D shape and material when defocus is present but not detected (Experiment 4). We found that the perception of optical defocus arose in all shaded stimuli that lacked sharp bounding contours consistent with geometrically-correlated contours such as self-occlusions or level (planar) cuts of the surface. Taken together, our results indicate that this class of bounding contours plays a critical role in the modulating our experience of optical defocus and 3D shape in otherwise ambiguous images of shaded surfaces.</p><p>The main theoretical idea that shaped our experiments was that there are specific photogeometric constraints exhibited by the bounding contours of shaded surfaces that play a critical role in identifying low spatial frequency intensity gradients as surface shading, determining the contour’s border ownership, and establishing whether the surface is optically focused. We considered two primary constraints. The first was the covariation of contour orientation and shading intensity, which occurs generically for both smooth self-occluding contours and planar cuts (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The results of Experiment 1 confirm our informal observations that the rotated level cut mask (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) not only failed to generate a clear perception of gradient focus, they actually <italic>interfered</italic> with the perception of 3D shape. We attribute this difference to the fact that the contours of level cut masks exhibit a strong orientation-intensity covariation, whereas the contours of the rotated masks do not.</p><p>We directly assessed the importance of this constraint in the perception of shading in Experiment 2, where thin shaded ‘ribbons’ tracing the paths of level cut contours were presented and the intensity-orientation covariation was directly manipulated. Our findings showed that this covariation strongly predicts the perception of 3D shape.</p><p>The second constraint we considered involved the direction of curvature of the shading adjacent to a contour (i.e., whether its perceived as convex or concave), and its role in the perceived attachment and focus of the shading gradients. The results of Experiment 1 support our informal observations that concavities appear less focused than convexities. This demonstrates that the intensity-orientation covariation cannot fully explain the perception of focus in images of shaded surfaces, as identical covariation can elicit percepts of both vivid focus and moderate defocus depending on whether the shaded surface is perceived as convex or concave (respectively). The difference in perceived focus may be caused by differences in perceived contour attachment in these two configurations. The shading gradients of the convex surface appear clearly attached to the contours of the level cut, but the same gradients do not appear clearly attached to the contour when the surface appears concave; the level cut contour can appear as the edge of a ‘cliff’, with the shading appearing at a more distant depth. This suggests that the shading gradients must appear clearly attached to the sharp contour to make full use of the information its sharpness provides about optical focus. This result is also consistent with arguments that the visual system has a bias to interpret abrupt discontinuities in luminance as self-occluding contours rather than sudden changes in surface orientation (<xref ref-type="bibr" rid="bib14">Howard, 1983</xref>; <xref ref-type="bibr" rid="bib23">Liu and Todd, 2004</xref>), which may explain why the contours appear more attached to the shading (and the shading more focused) when the surface appears convex.</p><p>Our findings have implications for the existing literature on both the perception of 3D shape from shading and the perception of optical blur. Prior studies of perceived shape have predominantly used globally convex shaded objects as stimuli (e.g. <xref ref-type="bibr" rid="bib9">Fleming et al., 2004</xref>; <xref ref-type="bibr" rid="bib32">Mingolla and Todd, 1986</xref>; <xref ref-type="bibr" rid="bib35">Nefs et al., 2006</xref>). The self-occluding contours that are invariably exhibited by these stimuli may explain why their shading gradients are always perceived as fully focused and vividly three-dimensional. Our data also imply that attempts to ‘eliminate’ these self-occlusions will only impair perceived 3D shape to the extent that the orientation-intensity covariation exhibited in the image is actually reduced. Artificially cropping a self-occluding contour out of the image, for example, may simply create a new bounding contour that still exhibits enough covariation to induce percepts of focused 3D shading. This may explain why manipulations that rotate shaded terrains (which often exhibit no self-occlusions at all; <xref ref-type="bibr" rid="bib42">Reichel and Todd, 1990</xref>; <xref ref-type="bibr" rid="bib46">Todd and Reichel, 1989</xref>) have been found to negatively affect perceived shape more than cropping the self-occlusions of convex objects (<xref ref-type="bibr" rid="bib9">Fleming et al., 2004</xref>; <xref ref-type="bibr" rid="bib6">Egan and Todd, 2015</xref>). Our results suggest that manipulations involving planar cuts may be a more effective method of investigating the role of contours in surface perception in future work.</p><p>The literature on focus perception has predominantly investigated how the severity of perceived blur varies with the spatial frequency and contrast properties of images (<xref ref-type="bibr" rid="bib29">Mather, 1997</xref>; <xref ref-type="bibr" rid="bib36">O'Shea et al., 1997</xref>; <xref ref-type="bibr" rid="bib43">Tadmor and Tolhurst, 1994</xref>). It has been established that relative changes in apparent blur magnitude can provide information about scene depth (<xref ref-type="bibr" rid="bib40">Pentland, 1987</xref>) and that sharp bounding contours can resolve the depth ordering of ambiguous surfaces (<xref ref-type="bibr" rid="bib27">Marshall et al., 1996</xref>), but to our knowledge, no prior studies have examined how the visual system distinguishes gradients produced by optical blur from gradients produced by environmental sources such as shading. Cases of source misattribution, such as those reported here, are likely difficult to produce with the 2D textures and simple contours employed in past work on focus perception. Our data reveal that identical low-frequency shading gradients can be perceived as vividly focused in some contexts and highly blurred in others, which implies that models of focus perception that rely entirely on local gradient features (or even the presence of sharp contours) are not sufficient. Our findings instead suggest that optical focus may be better characterized as a mid-level perceptual category that interacts with the visual system’s estimation of other mid-level properties such as contour attachment, 3D surface orientation and curvature, surface reflectance, and scene illumination.</p><p>The experiments and demonstrations reported herein have focused on the role of sharp contours that approximate smooth self-occluding rims in providing information about the 3D shape, depth of field, and optical focus of low frequency shading gradients. It seems unlikely, however, that sharp bounding contours are the sole means by which the visual system estimates the optical focus of shaded surfaces. Indeed, we carefully avoided other sources of image contours that could provide information about optical focus, such as the sharp contours generated by either shadows or specular reflections, and only evaluated low-curvature surfaces to avoid generated regions of high spatial frequency shading. Shaded surfaces with regions of high curvature could exhibit enough high spatial frequencies to eliminate percepts of blur, which could explain why some stimuli from prior studies of shading appear focused even in the absence of any contours (e.g. the ‘crater’ in Figure 7 of <xref ref-type="bibr" rid="bib45">Todd et al., 2014</xref>). Specular reflections generated by low-curvature glossy surfaces also have similar spatial frequency properties to high-curvature shading (<xref ref-type="bibr" rid="bib34">Mooney and Anderson, 2014</xref>). The efficacy of specular reflections in eliminating perceived blur can be experienced directly in the left panel of <xref ref-type="fig" rid="fig12">Figure 12</xref>. This surface is identical to that depicted in <xref ref-type="fig" rid="fig6">Figure 6</xref> but was rendered in a natural light field with a specular reflectance component in addition to shading. This image appears as a fully focused, glossy, shaded surface. However, the perception of focus in this image requires that the specular reflections appear linked to the same surface geometry as the shading gradients; if the specular reflections are rotated to arbitrary positions, the shaded surface again appears blurred, and the specular reflections appear as overlaid pigment or a second, independent layer (right panel). Thus, specular reflections must also respect photogeometric constraints that link the specular and diffuse components of reflectance in order to provide information about optical focus, as has been shown previously in the perception of gloss (<xref ref-type="bibr" rid="bib1">Anderson and Kim, 2009</xref>; <xref ref-type="bibr" rid="bib15">Kim et al., 2011</xref>; <xref ref-type="bibr" rid="bib25">Marlow et al., 2012</xref>). The underlying issue in both instances is source attribution: when the relevant image features (image highlights or sharp edges) are attributed to environmental sources (attached specular highlights or attached bounding contours), they simultaneously modulate the perception of environmental properties (gloss or 3D shape) and optical focus. Note that, in general, realistic specular reflections are not optically attached to the surface; when the depth of field is reduced, different regions of specular reflections may only remain sharp when viewed with different focal lengths, and may not be in focus at the same focal length as the attached surface shading. The interactions between specular reflectance, depth of field, and perceived focus are worth consideration in future work.</p><fig id="fig12" position="float"><object-id pub-id-type="doi">10.7554/eLife.48214.016</object-id><label>Figure 12.</label><caption><title>The surface used in Experiment 3 has here been rendered with added specular reflections in a natural light field (left).</title><p>In the right panel, the specular reflections have been rotated by 180 degrees relative to the shading gradients, which breaks their apparent attachment to the surface and reduces the perception of both surface gloss and gradient focus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48214-fig12-v2.tif"/></fig><p>The demonstrations and experiments presented here have focused on a previously unappreciated computational problem: distinguishing low-frequency structure caused by optical blur from the shading gradients of smooth surfaces. We showed that the visual system exploits specific forms of geometric and photometric covariation to generate percepts of optical focus and vivid surface shading, which arise generically along both smooth self-occlusions and planar cuts. Our results indicate that the presence or absence of intensity-orientation correlated contours is a powerful cue to focus: fully focused shaded surfaces can appear blurred in their absence, and actual defocus can be mistaken for transformations in material and shape when correlated contours are nearby. The findings reported herein provide evidence that our perceptions of material, 3D shape, and optical defocus are inherently coupled, which suggests that optical defocus perception does not occur in a completely independent visual pathway to the perception of surface and scene properties. Future work is required to understand the neural processes that exploit these sources of covariation, and the other sources of information that the visual system utilizes to distinguish environmental sources of image structure from optical artifacts induced by the imaging properties of single-chambered eyes.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>The orientation-intensity correlation along self-occluding rims and level cuts</title><p>The equations for Lambertian shading reveal the correlations between contour orientation and shading intensity that are likely to arise along a level cut. <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> shows the equation for Lambertian luminance in observer-centric spherical coordinates of surface orientation and illumination direction (<xref ref-type="bibr" rid="bib24">Mamassian, 1993</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow> <mml:mi/><mml:mfenced close="]" open="{" separators="|"><mml:mrow> <mml:mi/><mml:mn>0</mml:mn> <mml:mi/><mml:mo>,</mml:mo> <mml:mi/> <mml:mi/><mml:mi>r</mml:mi><mml:mi>*</mml:mi><mml:mi>i</mml:mi><mml:mi>*</mml:mi><mml:mfenced open="[" separators="|"><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfenced> <mml:mi/><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf53"><mml:mi>L</mml:mi></mml:math></inline-formula> is observed luminance, <inline-formula><mml:math id="inf54"><mml:mi>r</mml:mi></mml:math></inline-formula> is Lambertian surface albedo, and <inline-formula><mml:math id="inf55"><mml:mi>i</mml:mi></mml:math></inline-formula> is the illumination intensity. The sum in square brackets expresses Lambert’s cosine law of shading in terms of surface tilt <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, surface slant <inline-formula><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, illumination azimuth <inline-formula><mml:math id="inf58"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and illumination elevation <inline-formula><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, all relative to the observer. The subscripts distinguish which pair of spherical coordinates specifies surface orientation (<inline-formula><mml:math id="inf60"><mml:mi>s</mml:mi></mml:math></inline-formula>) and which pair specifies the illumination direction (<inline-formula><mml:math id="inf61"><mml:mi>i</mml:mi></mml:math></inline-formula>). The maximum function maps negative values of <inline-formula><mml:math id="inf62"><mml:mi>L</mml:mi></mml:math></inline-formula> to zero, which represents surface regions that receive no illumination. If albedo <inline-formula><mml:math id="inf63"><mml:mi>r</mml:mi></mml:math></inline-formula>, illumination strength <inline-formula><mml:math id="inf64"><mml:mi>i</mml:mi></mml:math></inline-formula>, and illumination direction <inline-formula><mml:math id="inf65"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> are approximately constant across the image, <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> can be simplified to a function of surface orientation <inline-formula><mml:math id="inf66"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> only:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>⁡</mml:mo><mml:mo>{</mml:mo> <mml:mi/><mml:mn>0</mml:mn> <mml:mi/><mml:mo>,</mml:mo> <mml:mi/> <mml:mi/><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow> <mml:mi/><mml:mo>}</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="inf67"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf68"><mml:mi>B</mml:mi></mml:math></inline-formula> are constants determined by surface albedo, illumination strength, and illumination elevation, and <inline-formula><mml:math id="inf69"><mml:mi>C</mml:mi></mml:math></inline-formula> is a constant determined by illumination azimuth. Note that this function is a cosine of surface tilt with phase <inline-formula><mml:math id="inf70"><mml:mi>C</mml:mi></mml:math></inline-formula> whose amplitude and vertical offset may vary with surface slant across the image. At a self-occluding rim, surface slant approaches 90° (which causes the tilt cosine’s vertical offset <inline-formula><mml:math id="inf71"><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> to approach zero and its amplitude <inline-formula><mml:math id="inf72"><mml:mi>B</mml:mi> <mml:mi/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> to approach <inline-formula><mml:math id="inf73"><mml:mi>B</mml:mi></mml:math></inline-formula>) and surface tilt is equal to the rim contour’s orientation. This further constrains the equation for Lambertian luminance to <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>⁡</mml:mo><mml:mo>{</mml:mo> <mml:mi mathvariant="normal"/><mml:mn>0</mml:mn> <mml:mi/><mml:mo>,</mml:mo> <mml:mi/> <mml:mi/><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow> <mml:mi/><mml:mo>}</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="inf74"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the orientation of the self-occluding rim and <inline-formula><mml:math id="inf75"><mml:mi>B</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf76"><mml:mi>C</mml:mi></mml:math></inline-formula> are constants that determine the amplitude and phase of the cosine function. Note that the rim’s 2D orientation is specified in a full 360° range and not a double-angle 180° range: parallel rim segments bound the surface from opposite sides have opposite orientation (i.e. ± 180°). This equation indicates that at a self-occluding rim, shading luminance decreases as a cosine function of the angular separation between contour orientation and the orientation corresponding to maximal luminance (i.e. where <inline-formula><mml:math id="inf77"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the illumination azimuth). The phase of the cosine is determined by the illumination azimuth and its amplitude is determined by surface slant, Lambertian albedo, illumination elevation, and illumination intensity. For non-Lambertian diffuse reflectance functions with roughness parameters (e.g. <xref ref-type="bibr" rid="bib37">Oren and Nayar, 1993</xref>), this falloff function will not be an exact cosine, but will at least be monotonic and continuous.</p><p>The shading exhibited by surfaces bound by level cut contours is related to contour orientation in a similar way to self-occluding rims. Level cut contour orientation is always equal to surface tilt, but only up to a 180° ambiguity: the adjacent surface could be convex (i.e. the 3D surface normal points out of the shaded region) or concave (i.e. the 3D surface normal points into the shaded region). Slant is unknown along a level cut in principal, but for smooth surfaces, its rate of change along the level cut contour will be constrained and its contribution to shading (the <inline-formula><mml:math id="inf78"><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf79"><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> terms in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) is consequently likely to be dominated by the contribution of surface tilt (the <inline-formula><mml:math id="inf80"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> term in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). The visual system is unlikely to be deterred by any small slant-induced distortions in the shape of the cosine relationship between luminance and contour orientation, which implies that <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> will still approximately hold for level cuts; the overall correlation between image intensity and orientation across large areas of the image is therefore likely to remain high.</p></sec><sec id="s4-2"><title>Experiment 1</title><sec id="s4-2-1"><title>Observers</title><p>The exact number of observers recruited for each experiment was dependent on availability; at least ten observers were recruited for all paired comparison and rating tasks described in Experiments 1 to 4, which has been sufficient to detect the effects of material and shape manipulations in our previous psychophysical studies.</p><p>Twenty first-year psychology students participated in Experiment one for partial course credit. They all had normal or corrected-to-normal vision and were naïve to the aims of the study.</p></sec><sec id="s4-2-2"><title>Apparatus</title><p>Observers were seated approximately 60 cm from a Dell UltraSharp U3014 75.6 cm monitor displaying at a resolution of 2650 × 1600. The display was controlled by a Dell Precision T3600 computer with an Intel Xeon processor running Windows 7 Professional (64-bit). Stimulus presentation and data collection were controlled by OpenGL functions in the Psychophysics Toolbox (version 3.0.10; Brainard, 1997) running in MATLAB (version 2011b; Mathworks, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001622">SCR_001622</ext-link>). Observers were surrounded by a black curtain during the experiment to ensure that the monitor was the primary source of light. Head and eye movements were not restricted. The same apparatus was used for all following experiments.</p></sec><sec id="s4-2-3"><title>Stimuli</title><p>A deformed plane was created using the open-source graphics software Blender (version 2.65; Blender Foundation, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008606">SCR_008606</ext-link>). An initial 20 cm square plane was recursively subdivided four times, then deformed by transforming the height of each vertex using a random ‘distorted noise’ texture generated within Blender, which warps one random noise texture according to the value of another. The contrast of the noise texture was set to 0.25, which ensured that the texture’s intensity was globally continuous (i.e. did not clip). A Catmull-Clark subdivision surface modifier with four iterations was then added to smooth the final surface (<xref ref-type="bibr" rid="bib10">Halstead et al., 1993</xref>). The surface was rendered with Lambertian reflectance (achromatic albedo of <inline-formula><mml:math id="inf81"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:math></inline-formula>) under a collimated light source with 90° azimuth (top-down), 45° elevation from the viewing direction, and a strength of 5. The camera was positioned 20 cm away from the center of the deformed plane and set to orthographic view with 1.8 magnification to just crop out the square boundary of the planar surface. The rendered image was saved as a 1024×1024 16-bit TIFF image and tone-mapped to 8-bit grayscale RGB in Adobe Photoshop (Adobe, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_014199">SCR_014199</ext-link>). The final image is depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref>. The procedure used to generate the homogenous gray masks is described in the main text. The final twelve masked stimuli are depicted in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> (‘convex’ masks) and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> ('bistable' masks).</p></sec><sec id="s4-2-4"><title>Procedure</title><p>Perceived focus was measured using a paired comparison task. Each of the twelve stimuli was compared with each other stimulus across 66 trials. The stimuli were centered in the left and right halves of the display against a black background. In each trial, observers were instructed to select the image in which the visible gradients appeared most focused. The order of trials and arrangement of stimuli within each trial (left vs. right) were random. Perceived surface curvature sign was measured for the same observers using a three-alternative forced choice task. Each stimulus was presented in the center of the display against a black background above three buttons labeled ‘bumps,’ ‘dents,’ and ‘neither.’ Observers were instructed to click the option in each trial that best represented their overall perception of shape from the visible gradients in the image. They were specifically instructed to inspect the entire image before making their decision. The twelve stimuli were presented once in random order.</p></sec></sec><sec id="s4-3"><title>Experiment 2</title><sec id="s4-3-1"><title>Observers</title><p>Fifteen observers participated in Experiment 2. One observer was an author (SM). All other observers were recruited from university colleagues, as the study took place outside semester, were naïve to the aims of the studies, and had normal or corrected-to-normal vision.</p></sec><sec id="s4-3-2"><title>Stimuli</title><p>Sixteen sets of level cut contours were created by first generating sixteen surfaces similar to the surface in <xref ref-type="fig" rid="fig1">Figure 1</xref> (using different random noise textures), then intersecting each surface with a frontoparallel plane. We then used these level cut contours to define the paths of artificial shaded ribbons in MATLAB. The ribbons were centered on the contours and had a uniform width of two pixels. Ribbon orientation was defined in a 360° space by the direction of the 2D normal vector corresponding to the tilt of the terrains used to generate the level cuts (i.e. the normal vectors that point from the gradients into the mask along the level cut mask contours). Ribbon luminance was then defined as a function of ribbon orientation as shown in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>0.6</mml:mn><mml:mi>*</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo> <mml:mi/><mml:mo>⟨</mml:mo> <mml:mi/><mml:mn>0,1</mml:mn> <mml:mi/><mml:mo>⟩</mml:mo><mml:mo>∙</mml:mo><mml:mi>N</mml:mi> <mml:mi/><mml:mo>)</mml:mo></mml:math></disp-formula>where L is shading luminance between zero (black) and one (white) and N is the 2D unit normal vector to the ribbon (which corresponds to 3D surface tilt on the original surface). This equation causes luminance to decrease from 0.8 to 0.2 as a linear function of the angle between ribbon orientation and 90°. The ribbon is brightest where its orientation is equal to 90° and darkest where its orientation is equal to 270°. The background in each image had a luminance value of 0.5, which was the same luminance value exhibited by the exactly vertical segments of the ribbons. These luminance values were scaled to 8-bit achromatic RGB luminance values for display. The resulting shading appears highly similar to the corresponding Lambertian shading on the original rendered surfaces, but varies as a linear function of ribbon orientation instead of obeying Lambert’s cosine law. We employed this linear shading approximation to simplify the large volume of correlation analyses performed during the experiment. The difference between these two monotonically decreasing functions is negligible for the purposes of our analysis, as cosine and linear falloff functions have a Pearson correlation coefficient of 0.979. An example of a perfectly correlated ribbon is depicted in the left panel of <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><p>We systematically weakened the correlation between ribbon orientation and luminance by mixing the linear shading with progressively larger amounts of gradient noise. To accomplish this, we analyzed the Fourier amplitude spectra of the sixteen surfaces used to generate the level cuts for the ribbons and coded a 2D noise generator to generate noise textures that approximately matched the average amplitude spectrum of the shading gradients exhibited by these surfaces. Geometrically identical ribbon paths (two pixels wide) were cut out of these noise textures and mixed with the linearly shaded ribbons in thirteen different proportions. The amount of noise in these mixed ribbons ranged from 0% (which preserves the perfect linear correlation) to 100% (which destroys the linear correlation completely). The intermediate noise values were 15%, 30%, 35%, 40%, 45%, 50%, 55%, 60%, 65%, 70%, and 85%. Note that these values are not spaced at regular intervals, but were instead chosen to generate an approximately uniform distribution of correlation coefficient values. A new random noise texture was generated in real time for each shaded ribbon presented. Example stimuli with 50% and 100% noise are shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><p>We measured the global correlation between contour orientation and shading luminance in each ribbon image presented to observers. This was accomplished in several steps for each image. First, we used the in-built <italic>edge</italic> function in MATLAB to extract the contours from the binary level cut mask. We then calculated discrete approximations of contour orientation in 9 × 9 blocks of pixels using the <italic>regionprops</italic> function, but the resulting orientation values only range from 0° to 180° and do not distinguish between parallel contour segments with opposing normal vectors (i.e. on opposite sides of the mask). We rectified this using a custom program that tests which side of the contour corresponds to the mask and which corresponds to the visible gradients at each pixel along the contour. Contour segments that were detected as bounding the mask from above had their orientation values translated into a range from 180° (a right mask edge) to 360° (a left mask edge). These orientation values match the orientation of 2D normal vectors pointing outward from the gradients into the mask (which would in turn correspond to surface tilt at a convex level cut or self-occluding rim). The code for ribbon generation and correlation computation is available on GitHub (<xref ref-type="bibr" rid="bib33">Mooney, 2019</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/contour-covariation">https://github.com/elifesciences-publications/contour-covariation</ext-link>).</p><p>We calculated sixteen orientation- correlations for each image using sixteen different potential values for the illumination azimuth, which determines which contour orientation corresponds to the brightest shading. The azimuth values increased from 0° to 337.5° in increments of 22.5°. These multiple correlations were measured to account for the possibility that the ribbon noise would induce incidental correlations consistent with illumination directions other than the original top-down direction. Each correlation was calculated by converting the contour orientation values into angular separation values from the azimuth, then computing the Pearson correlation coefficient between angular separation and image intensity at each pixel along the ribbon. The final correlation value <inline-formula><mml:math id="inf82"><mml:mi>ρ</mml:mi></mml:math></inline-formula> for that stimulus was set to the maximum correlation detected across all sixteen potential illumination azimuth values. The correlations were derived in real time throughout the task due to the random noise used to generate each stimulus. Correlations for the example stimuli depicted in <xref ref-type="fig" rid="fig4">Figure 4</xref> are shown in the bottom-right corner of each image. Note that the shaded ribbons with no gradient noise exhibit almost perfect correlations as designed. All correlation coefficients greater than 0.2 were consistent with 90° illumination azimuth, which indicates that there were no strong correlations consistent with arbitrary illumination directions in the experiments.</p></sec><sec id="s4-3-3"><title>Procedure</title><p>Perceived 3D shape strength was measured using a paired comparison task. Observers were shown all possible pairs of the thirteen ribbon noise values in random order. The shaded ribbon paths displayed in each trial were randomly chosen from the sixteen possible sets of contours and were mixed with new randomly generated noise textures in every trial. Observers were instructed to select the image that appeared most vividly three-dimensional overall and were told to fully inspect both images before deciding. The orientation-intensity correlation for each stimulus was derived in real time during the experiment for later analysis.</p></sec></sec><sec id="s4-4"><title>Experiment 3</title><sec id="s4-4-1"><title>Observers</title><p>Ten observers participated in Experiment 3. One observer was the author (SM). All other observers were recruited from university colleagues, were naïve to the experimental aims, and had normal or corrected-to-normal vision.</p></sec><sec id="s4-4-2"><title>Stimuli</title><p>The stimulus geometry was created in the same way as Experiment 1, but the noise texture used to deform the surface had a contrast value of 1. This clipped the noise intensity to black below a level of 0.5, which restrained the height of the deformed surface to a minimum value (i.e. the concave parts of the surface in <xref ref-type="fig" rid="fig1">Figure 1</xref> became flat). The same subdivision modifier was then applied to the surface to smooth over the abrupt discontinuities in surface orientation created by the clipped noise, and the material, illumination, and camera parameters were otherwise identical. A level cut condition was then re-created from the smoothed surface by truncating it with a flat frontoparallel plane positioned 0.1 cm in front of the flat planar regions. The resulting combined surface exhibits sharp level cut contours along the paths where the surfaces intersected, which are similar to the contours in Experiment 1.</p><p>Five optical defocus conditions were applied to both the smoothed and level cut geometry conditions by manipulating virtual camera parameters. In the ‘no blur’ condition, no defocus was used. The four conditions with blur were created by combining two different focal lengths with two lens aperture values. In the two ‘background blur’ conditions, focal length was set to the average distance of the peaks of the five highest bumps (19.3 cm). In the ‘foreground blur’ conditions, focal length was set to the distance of the intersecting plane (19.9 cm). The weak and strong blur conditions for each focal length were defined by setting the F-stop value to 2.8 and 1.4, respectively. The stimuli were rendered to 1024 × 1024 16 bit TIFF images, collated, and simultaneously tone-mapped to 8-bit RGB in Adobe Photoshop. The final ten stimuli are depicted in <xref ref-type="fig" rid="fig6">Figure 6</xref>. The top row depicts the smoothed surface with no intersecting plane and the bottom row depicts the level cut surface with the intersecting plane. From left to right, the columns depict the strong background blur, weak background blur, no blur, weak foreground blur, and strong foreground blur conditions.</p></sec><sec id="s4-4-3"><title>Procedure</title><p>Perceived focus was measured using a paired comparison task identical to the task used in Experiment 1, but due to the lower number of stimuli, the full block of 45 pairs was shown twice in random order to each observer.</p></sec></sec><sec id="s4-5"><title>Experiment 4</title><sec id="s4-5-1"><title>Observers</title><p>Twenty first-year psychology students participated in the focus and gloss tasks of Experiment four for partial course credit. They all had normal or corrected-to-normal vision and were naïve to the aims of the study. Five psychophysically experienced observers participated in the shape task, including an author (SM). This number of observers has been sufficient to detect the effects of surface manipulations on the gauge figure task, which involves a very large number of trials. All observers other than the author were naïve to the aims of the study.</p></sec><sec id="s4-5-2"><title>Stimuli</title><p>The deformed planar surface was created in a similar way to Experiment 3, but the surface was not subdivided after deformation with the high-contrast noise texture. This creates sharp level-cut discontinuities in shape that separate smooth bumps from a planar background, and was done to effectively include the intersecting plane as part of the rendered scene rather than adding it to the image as a mask. Three different reflectance functions were used to create low-roughness specular (‘smooth gloss’), moderate-roughness specular (‘rough gloss’), and shading-only (‘matte’) conditions. All three reflectance functions had a diffuse Lambertian component with 95% strength and achromatic albedo <inline-formula><mml:math id="inf83"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:math></inline-formula>. The smooth gloss and rough gloss conditions had additional specular reflectance components defined by the Beckmann BRDF with 5% strength and roughness coefficients <inline-formula><mml:math id="inf84"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0.025</mml:mn></mml:math></inline-formula> (smooth gloss) and <inline-formula><mml:math id="inf85"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula>(rough gloss).</p><p>We created five different blur conditions by simulating different types of optical defocus in Blender. This was accomplished by changing the properties of the virtual camera, which was positioned at 20 cm from the planar base of the bumpy surface. The ‘no blur’ condition had no optical defocus. In the ‘foreground blur’ and ‘background blur’ conditions, the camera’s focal length was set to the distance between the camera and the planar base of the surface (20 cm) or the closest point on the main visible ridge in the center-left part of the image (18.5 cm), respectively. The weak and strong versions of each blur condition were generated by setting the virtual camera’s aperture to F-stop values of 2.8 and 1.4, respectively. Camera zoom was set to 150 in all conditions to just crop out the square boundary of the deformed plane. The stimuli were illuminated using a natural light field to test whether misperceptions of optical blur occur in more realistic viewing conditions, which are particularly important for the appearance of glossy materials (<xref ref-type="bibr" rid="bib8">Fleming et al., 2003</xref>; <xref ref-type="bibr" rid="bib38">Pellacini et al., 2000</xref>). The light field used was ‘Meadow Trail’, which we have used previously (<xref ref-type="bibr" rid="bib34">Mooney and Anderson, 2014</xref>). The images were rendered to 1024 × 1024 16 bit TIFF images in Blender, then simultaneously tone-mapped to 8-bit RGB in Adobe Photoshop. The final fifteen stimuli are depicted in <xref ref-type="fig" rid="fig8">Figure 8</xref>. The top, middle, and bottom rows show the matte, rough gloss, and smooth gloss materials, respectively. From left to right, the columns depict the strong background blur, weak background blur, no blur, weak foreground blur, and strong foreground blur conditions.</p></sec><sec id="s4-5-3"><title>Procedure</title><p>Perceived focus and gloss were measured using paired comparison tasks in the same way as Experiment 1. Perceived 3D shape was measured for six of the stimuli: the smooth gloss and matte surfaces with no blur, strong background blur, and strong foreground blur (outlined in red in <xref ref-type="fig" rid="fig8">Figure 8</xref>). We employed the gauge figure task to measure shape (see <xref ref-type="bibr" rid="bib17">Koenderink et al., 1992</xref>). Observers adjusted gauge figure probes at twenty points in a horizontal line along the central ridge in the bottom-left quadrant of each image. In each trial, observers adjusted the gauge figure until its disc appeared to lie in the tangent plane to the surface and its rod matched the surface normal. Observers completed four repeats of each of the six images. The images were presented in pseudo-random order to ensure no image was seen twice in a row and the twenty probes for each image were presented in random order. No time limit was given, and observers took approximately forty minutes to complete the experiment.</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Supervision, Investigation, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Informed consent and consent to publish was obtained from each participant in accordance with experimental protocol 2012/2759 approved by the Human Research Ethics Committee (HREC) at the University of Sydney.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.48214.017</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-48214-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files. The code for ribbon generation and correlation computation is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/swjmooney/contour-covariation">https://github.com/swjmooney/contour-covariation</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/contour-covariation">https://github.com/elifesciences-publications/contour-covariation</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>BL</given-names></name><name><surname>Kim</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Image statistics do not explain the perception of gloss and lightness</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/9.11.10</pub-id><pub-id pub-id-type="pmid">20053073</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrow</surname> <given-names>H</given-names></name><name><surname>Tenenbaum</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Recovering intrinsic scene characteristics</article-title><source>Comput Vis Syst</source><volume>2</volume><fpage>3</fpage><lpage>26</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belhumeur</surname> <given-names>PN</given-names></name><name><surname>Kriegman</surname> <given-names>DJ</given-names></name><name><surname>Yuille</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The bas-relief ambiguity</article-title><source>International Journal of Computer Vision</source><volume>35</volume><fpage>33</fpage><lpage>44</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ciuffreda</surname> <given-names>KJ</given-names></name><name><surname>Selenow</surname> <given-names>A</given-names></name><name><surname>Wang</surname> <given-names>B</given-names></name><name><surname>Vasudevan</surname> <given-names>B</given-names></name><name><surname>Zikos</surname> <given-names>G</given-names></name><name><surname>Ali</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>&quot;Bothersome blur&quot;: a functional unit of blur perception</article-title><source>Vision Research</source><volume>46</volume><fpage>895</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2005.10.004</pub-id><pub-id pub-id-type="pmid">16337253</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crete</surname> <given-names>F</given-names></name><name><surname>Dolmiere</surname> <given-names>T</given-names></name><name><surname>Ladret</surname> <given-names>P</given-names></name><name><surname>Nicolas</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The blur effect: perception and estimation with a new no-reference perceptual blur metric</article-title><source>Proceedings of SPIE - the International Society for Optical Engineering</source><pub-id pub-id-type="doi">10.1117/12.702790</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egan</surname> <given-names>EJ</given-names></name><name><surname>Todd</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The effects of smooth occlusions and directions of illumination on the visual perception of 3-D shape from shading</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>24</elocation-id><pub-id pub-id-type="doi">10.1167/15.2.24</pub-id><pub-id pub-id-type="pmid">25761340</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ferzli</surname> <given-names>R</given-names></name><name><surname>Karam</surname> <given-names>LJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Human visual system based no-reference objective image sharpness metric</article-title><conf-name>2006 International Conference on Image Processing</conf-name><fpage>2949</fpage><lpage>2952</lpage><pub-id pub-id-type="doi">10.1109/ICIP.2006.312925</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname> <given-names>RW</given-names></name><name><surname>Dror</surname> <given-names>RO</given-names></name><name><surname>Adelson</surname> <given-names>EH</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Real-world illumination and the perception of surface reflectance properties</article-title><source>Journal of Vision</source><volume>3</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1167/3.5.3</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname> <given-names>RW</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Adelson</surname> <given-names>EH</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Specular reflections and the perception of shape</article-title><source>Journal of Vision</source><volume>4</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/4.9.10</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Halstead</surname> <given-names>M</given-names></name><name><surname>Kass</surname> <given-names>M</given-names></name><name><surname>DeRose</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Efficient, fair interpolation using Catmull-Clark surfaces</article-title><conf-name>Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques</conf-name><fpage>35</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1145/166117.166121</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Held</surname> <given-names>RT</given-names></name><name><surname>Cooper</surname> <given-names>EA</given-names></name><name><surname>O'Brien</surname> <given-names>JF</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Using blur to affect perceived distance and size</article-title><source>ACM Transactions on Graphics</source><volume>29</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1145/1731047.1731057</pub-id><pub-id pub-id-type="pmid">21552429</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname> <given-names>H</given-names></name><name><surname>Bruce</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>A comparison between the hollow-face and 'hollow-potato' illusions</article-title><source>Perception</source><volume>23</volume><fpage>1335</fpage><lpage>1337</lpage><pub-id pub-id-type="doi">10.1068/p231335</pub-id><pub-id pub-id-type="pmid">7761244</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Horn</surname> <given-names>BK</given-names></name><name><surname>Brooks</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="1989">1989</year><source>Shape From Shading</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname> <given-names>IP</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Occluding edges in apparent reversal of convexity and concavity</article-title><source>Perception</source><volume>12</volume><fpage>85</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1068/p120085</pub-id><pub-id pub-id-type="pmid">6646957</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>J</given-names></name><name><surname>Marlow</surname> <given-names>P</given-names></name><name><surname>Anderson</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The perception of gloss depends on highlight congruence with surface shading</article-title><source>Journal of Vision</source><volume>11</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.1167/11.9.4</pub-id><pub-id pub-id-type="pmid">21841140</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>What does the occluding contour tell us about solid shape?</article-title><source>Perception</source><volume>13</volume><fpage>321</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1068/p130321</pub-id><pub-id pub-id-type="pmid">6514517</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname> <given-names>JJ</given-names></name><name><surname>van Doorn</surname> <given-names>AJ</given-names></name><name><surname>Kappers</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Surface perception in pictures</article-title><source>Perception &amp; Psychophysics</source><volume>52</volume><fpage>487</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.3758/BF03206710</pub-id><pub-id pub-id-type="pmid">1437481</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname> <given-names>JJ</given-names></name><name><surname>van Doorn</surname> <given-names>AJ</given-names></name><name><surname>Kappers</surname> <given-names>AM</given-names></name><name><surname>Todd</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Ambiguity and the 'mental eye' in pictorial relief</article-title><source>Perception</source><volume>30</volume><fpage>431</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1068/p3030</pub-id><pub-id pub-id-type="pmid">11383191</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname> <given-names>JJ</given-names></name><name><surname>van Doorn</surname> <given-names>AJ</given-names></name><name><surname>Pont</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Light direction from shad(ow)ed random gaussian surfaces</article-title><source>Perception</source><volume>33</volume><fpage>1405</fpage><lpage>1420</lpage><pub-id pub-id-type="doi">10.1068/p5287</pub-id><pub-id pub-id-type="pmid">15729909</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname> <given-names>JJ</given-names></name><name><surname>Van Doorn</surname> <given-names>AJ</given-names></name><name><surname>Pont</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Perception of illuminance flow in the case of anisotropic rough surfaces</article-title><source>Perception &amp; Psychophysics</source><volume>69</volume><fpage>895</fpage><lpage>903</lpage><pub-id pub-id-type="doi">10.3758/BF03193926</pub-id><pub-id pub-id-type="pmid">18018970</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langer</surname> <given-names>MS</given-names></name><name><surname>Bülthoff</surname> <given-names>HH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A prior for global convexity in local shape-from-shading</article-title><source>Perception</source><volume>30</volume><fpage>403</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1068/p3178</pub-id><pub-id pub-id-type="pmid">11383189</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langer</surname> <given-names>MS</given-names></name><name><surname>Zucker</surname> <given-names>SW</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Shape-from-shading on a cloudy day</article-title><source>Journal of the Optical Society of America A</source><volume>11</volume><fpage>467</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.11.000467</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>B</given-names></name><name><surname>Todd</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Perceptual biases in the interpretation of 3D shape from shading</article-title><source>Vision Research</source><volume>44</volume><fpage>2135</fpage><lpage>2145</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.03.024</pub-id><pub-id pub-id-type="pmid">15183680</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mamassian</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1993">1993</year><chapter-title>Isophotes on a smooth surface related to scene geometry</chapter-title><source>Geometric Methods in Computer Vision II</source><publisher-name>International Society for Optics and Photonics Vol. 2031</publisher-name><fpage>124</fpage><lpage>133</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marlow</surname> <given-names>PJ</given-names></name><name><surname>Kim</surname> <given-names>J</given-names></name><name><surname>Anderson</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The perception and misperception of specular surface reflectance</article-title><source>Current Biology</source><volume>22</volume><fpage>1909</fpage><lpage>1913</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.08.009</pub-id><pub-id pub-id-type="pmid">22959347</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marlow</surname> <given-names>PJ</given-names></name><name><surname>Mooney</surname> <given-names>SWJ</given-names></name><name><surname>Anderson</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Photogeometric cues to perceived surface shading</article-title><source>Current Biology</source><volume>29</volume><fpage>306</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.11.041</pub-id><pub-id pub-id-type="pmid">30612905</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshall</surname> <given-names>JA</given-names></name><name><surname>Burbeck</surname> <given-names>CA</given-names></name><name><surname>Ariely</surname> <given-names>D</given-names></name><name><surname>Rolland</surname> <given-names>JP</given-names></name><name><surname>Martin</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Occlusion edge blur: a cue to relative visual depth</article-title><source>Journal of the Optical Society of America A</source><volume>13</volume><fpage>681</fpage><lpage>688</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.13.000681</pub-id><pub-id pub-id-type="pmid">8867752</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mather</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Image blur as a pictorial depth cue</article-title><source>Proceedings. Biological Sciences</source><volume>263</volume><fpage>169</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1098/rspb.1996.0027</pub-id><pub-id pub-id-type="pmid">8728981</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mather</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The use of image blur as a depth cue</article-title><source>Perception</source><volume>26</volume><fpage>1147</fpage><lpage>1158</lpage><pub-id pub-id-type="doi">10.1068/p261147</pub-id><pub-id pub-id-type="pmid">9509149</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mather</surname> <given-names>G</given-names></name><name><surname>Smith</surname> <given-names>DR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Depth cue integration: stereopsis and image blur</article-title><source>Vision Research</source><volume>40</volume><fpage>3501</fpage><lpage>3506</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(00)00178-4</pub-id><pub-id pub-id-type="pmid">11115677</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mather</surname> <given-names>G</given-names></name><name><surname>Smith</surname> <given-names>DR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Blur discrimination and its relation to blur-mediated depth perception</article-title><source>Perception</source><volume>31</volume><fpage>1211</fpage><lpage>1219</lpage><pub-id pub-id-type="doi">10.1068/p3254</pub-id><pub-id pub-id-type="pmid">12430948</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mingolla</surname> <given-names>E</given-names></name><name><surname>Todd</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Perception of solid shape from shading</article-title><source>Biological Cybernetics</source><volume>53</volume><fpage>137</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1007/BF00342882</pub-id><pub-id pub-id-type="pmid">3947683</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Mooney</surname> <given-names>SWJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>contour-covariation</data-title><source>GitHub</source><version designator="4e49b20">4e49b20</version><ext-link ext-link-type="uri" xlink:href="https://github.com/swjmooney/contour-covariation">https://github.com/swjmooney/contour-covariation</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mooney</surname> <given-names>SW</given-names></name><name><surname>Anderson</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Specular image structure modulates the perception of three-dimensional shape</article-title><source>Current Biology</source><volume>24</volume><fpage>2737</fpage><lpage>2742</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.09.074</pub-id><pub-id pub-id-type="pmid">25455034</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nefs</surname> <given-names>HT</given-names></name><name><surname>Koenderink</surname> <given-names>JJ</given-names></name><name><surname>Kappers</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Shape-from-shading for matte and glossy objects</article-title><source>Acta Psychologica</source><volume>121</volume><fpage>297</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2005.08.001</pub-id><pub-id pub-id-type="pmid">16181604</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Shea</surname> <given-names>RP</given-names></name><name><surname>Govan</surname> <given-names>DG</given-names></name><name><surname>Sekuler</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Blur and contrast as pictorial depth cues</article-title><source>Perception</source><volume>26</volume><fpage>599</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.1068/p260599</pub-id><pub-id pub-id-type="pmid">9488884</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Oren</surname> <given-names>M</given-names></name><name><surname>Nayar</surname> <given-names>SK</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Diffuse reflectance from rough surfaces</article-title><conf-name>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>763</fpage><lpage>764</lpage><pub-id pub-id-type="doi">10.1109/CVPR.1993.341163</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pellacini</surname> <given-names>F</given-names></name><name><surname>Ferwerda</surname> <given-names>JA</given-names></name><name><surname>Greenberg</surname> <given-names>DP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Toward a psychophysically-based light reflection model for image synthesis</article-title><conf-name>Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques</conf-name><fpage>55</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1145/344779.344812</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pentland</surname> <given-names>AP</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Local shading analysis</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>6</volume><fpage>170</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.1984.4767501</pub-id><pub-id pub-id-type="pmid">21869181</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pentland</surname> <given-names>AP</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>A new sense for depth of field</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>9</volume><fpage>523</fpage><lpage>531</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.1987.4767940</pub-id><pub-id pub-id-type="pmid">21869410</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramachandran</surname> <given-names>VS</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Perception of shape from shading</article-title><source>Nature</source><volume>331</volume><fpage>163</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1038/331163a0</pub-id><pub-id pub-id-type="pmid">3340162</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichel</surname> <given-names>FD</given-names></name><name><surname>Todd</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Perceived depth inversion of smoothly curved surfaces due to image orientation</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>16</volume><fpage>653</fpage><lpage>664</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.16.3.653</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadmor</surname> <given-names>Y</given-names></name><name><surname>Tolhurst</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Discrimination of changes in the second-order statistics of natural and synthetic images</article-title><source>Vision Research</source><volume>34</volume><fpage>541</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)90167-8</pub-id><pub-id pub-id-type="pmid">8303837</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todd</surname> <given-names>JT</given-names></name><name><surname>Koenderink</surname> <given-names>JJ</given-names></name><name><surname>van Doorn</surname> <given-names>AJ</given-names></name><name><surname>Kappers</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Effects of changing viewing conditions on the perceived structure of smoothly curved surfaces</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>22</volume><fpage>695</fpage><lpage>706</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.22.3.695</pub-id><pub-id pub-id-type="pmid">8666959</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todd</surname> <given-names>JT</given-names></name><name><surname>Egan</surname> <given-names>EJ</given-names></name><name><surname>Phillips</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Is the perception of 3D shape from shading based on assumed reflectance and illumination?</article-title><source>I-Perception</source><volume>5</volume><fpage>497</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1068/i0645</pub-id><pub-id pub-id-type="pmid">26034561</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todd</surname> <given-names>JT</given-names></name><name><surname>Reichel</surname> <given-names>FD</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Ordinal structure in the visual perception and cognition of smoothly curved surfaces</article-title><source>Psychological Review</source><volume>96</volume><fpage>643</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.96.4.643</pub-id><pub-id pub-id-type="pmid">2798652</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watt</surname> <given-names>SJ</given-names></name><name><surname>Akeley</surname> <given-names>K</given-names></name><name><surname>Ernst</surname> <given-names>MO</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Focus cues affect perceived depth</article-title><source>Journal of Vision</source><volume>5</volume><fpage>834</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1167/5.10.7</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webster</surname> <given-names>MA</given-names></name><name><surname>Georgeson</surname> <given-names>MA</given-names></name><name><surname>Webster</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neural adjustments to image blur</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>839</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1038/nn906</pub-id><pub-id pub-id-type="pmid">12195427</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48214.019</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Fleming</surname><given-names>Roland W</given-names></name><role>Reviewing Editor</role><aff><institution>University of Giessen</institution><country>Germany</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Fleming</surname><given-names>Roland W</given-names></name><role>Reviewer</role><aff><institution>University of Giessen</institution><country>Germany</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Todd</surname><given-names>James</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;The perception and misperception of optical defocus, shading, and shape&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Roland W Fleming as the guest Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The following individual involved in review of your submission has also agreed to reveal their identity: James Todd (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>As you will see below, the reviewers were all quite positive. We therefore are providing all of the comments from the individual reviewers in full so you can decide how you wish to respond to their particular suggestions.</p><p><italic>Reviewer #1:</italic> </p><p>This is a highly original and insightful contribution, which shows for the first time how mid-level perceptual organization processes affect the apparent sharpness (focus) of images. The main, and highly surprising, finding is that identical smooth intensity gradients can appear either blurry or sharp depending on the extent to which sharp contours that encompass them appear to be 'owned by' the image region containing the smooth gradient. When the sharp contours appear to be the boundary of the smooth image region, the gradients appear like shading seen in focus. In contrast, in the absence of such contours, or when they appear to belong to an overlying occluder, the smooth gradients are seen to be out of focus, as if the visual system attributes the smoothness of the gradients to optical blur, rather than shading.</p><p>To my knowledge, no previous studies have considered this causal attribution ambiguity, and the findings have implications for our understanding of 3D shape perception and image interpretation more generally. The study elegantly combines photo-geometric insights derived from considering the generative processes of shading, with phenomenology and behavioural experiments to map out the range of conditions that modify the perception of image sharpness based on border ownership.</p><p>Having said that, there are a couple of aspects of the manuscript that could be improved.</p><p>My first major suggestion is to remove Experiment 2. It is not that the experiment is any way flawed, it's just that the motivation for the experiment and its connection to the other experiments is not sufficiently clear in the manuscript. In short, it's not clear what it adds, as none of the main arguments in the article require the evidence provided by Experiment 2. Moreover, of all the experiments, it is the one with the least surprising result in light of what we already know about shape from shading, including the authors' own work. Removing it would help streamline what is a rather long manuscript.</p><p>My other main comment is that I suspect that the key driver of apparent sharpness is the extent to which sharp features appear to 'belong to' the surface. This is indeed what the authors say (e.g. Discussion, fourth paragraph), but they emphasise the 'photogeometric constraint' of intensity-orientation correlation along the contour as the main driver of the effect. I agree that this affects perceived sharpness because it mediates border ownership. But I think it is probably only one of several ways of getting the sharpness to 'adhere to' the smooth gradients (as indeed the authors note in the seventh paragraph of the Discussion). For example, sharp features that look like surface markings should also make the gradients appear in focus. It would be interesting, for example to make a stereoscopic version of Figure 1 and add a few dots on top. Placing the dots in a plane floating above the surface should leave it looking blurry. But placing the dots stereoscopically on the relief of the surface should make it look sharp because they 'belong to' the surface. If my intuition is correct, it would generalize the results the authors present. If I'm wrong, that would also be interesting as it would demonstrate a truly privileged status for boundary ownership and the intensity-orientation and convexity constraints.</p><p>Overall, however, this is a compelling and important contribution.</p><p><italic>Reviewer #2:</italic> </p><p>This is an outstanding manuscript. The methodology of the work is highly creative, and the paper is clearly written. Because this work makes an important contribution to the field of perception, I strongly recommend that it be accepted for publication.</p><p><italic>Reviewer #3:</italic> </p><p>This is a scholarly piece of work which presents very clear and compelling results which will be of great interest to the, albeit rather small, contingent of people interested in shape from shaping, the perceptions of surface material and blur. The paper presents a novel approach and is certainly worthy of publication. It is very well written. Very unusually for me, I do not have much to say by way of critique – the paper is very good in my view. My only real concern is that, if I read the paper correctly, each experiment relied on small number of rendered surfaces (perhaps even just one) and a relatively small number of stimuli (perhaps just one per conditions) presented a small number of times to a relatively large number of people. It is possible then than some effects rely on or are enhanced by very specific image features that happened to occur in these stimuli. It would have been better if multiple surfaces had been used in each experiment.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48214.020</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>Reviewer #1:</p><disp-quote content-type="editor-comment"><p>[…] There are a couple of aspects of the manuscript that could be improved.</p><p>My first major suggestion is to remove Experiment 2. It is not that the experiment is any way flawed, it's just that the motivation for the experiment and its connection to the other experiments is not sufficiently clear in the manuscript. In short, it's not clear what it adds, as none of the main arguments in the article require the evidence provided by Experiment 2. Moreover, of all the experiments, it is the one with the least surprising result in light of what we already know about shape from shading, including the authors' own work. Removing it would help streamline what is a rather long manuscript.</p></disp-quote><p>We agree that Experiment 2 did not directly address the issue of optical defocus; its purpose was to directly test the idea that the photogeometric covariation of intensity and contour orientation was a sufficient cue that the visual system used to identify shading, which in turn supports our explanation for the role of these contours in Experiment 1. Although we have done other manipulations in a previous paper to assess this as well, this is, in our minds, a more direct test. We have clarified its relevance to questions of perceived focus in our revision:</p><p>“These findings do not directly address the issue of perceived defocus, but do support our hypothesis that the photo-geometric behavior occurring at the very edge of covarying contours (such as the level cut contours in Figure 2A) is sufficient to generate the vivid impressions of contour attachment observed in the ‘convex’ conditions of Experiment 1.”</p><p>It is only a short component of the paper, so we feel that there is no good reason to omit it since it adds information demonstrating the importance of this covariation in identifying shading.</p><disp-quote content-type="editor-comment"><p>My other main comment is that I suspect that the key driver of apparent sharpness is the extent to which sharp features appear to 'belong to' the surface. This is indeed what the authors say (e.g. Discussion, fourth paragraph), but they emphasise the 'photogeometric constraint' of intensity-orientation correlation along the contour as the main driver of the effect. I agree that this affects perceived sharpness because it mediates border ownership. But I think it is probably only one of several ways of getting the sharpness to 'adhere to' the smooth gradients (as indeed the authors note in the seventh paragraph of the Discussion). For example, sharp features that look like surface markings should also make the gradients appear in focus. It would be interesting, for example to make a stereoscopic version of Figure 1 and add a few dots on top. Placing the dots in a plane floating above the surface should leave it looking blurry. But placing the dots stereoscopically on the relief of the surface should make it look sharp because they 'belong to' the surface. If my intuition is correct, it would generalize the results the authors present. If I'm wrong, that would also be interesting as it would demonstrate a truly privileged status for boundary ownership and the intensity-orientation and convexity constraints.</p></disp-quote><p>This was our intuition as well, but it turns out that generating surface textures that appear to ‘belong to’ the smooth shading is not as simple as one might expect. See our 2018 VSS presentation:</p><p>Mooney, S.W.J. and Anderson, B.L. (2018). Illusory transparency and optical blur induced by single shaded surfaces. Journal of Vision 18(10), 889-889.</p><p>Our original thought was to simply add some high frequency texture (fine grained 3D bumps) and render the surface. We thought this should obviously appear in focus, but it doesn’t; the high frequency component does not appear attached to the surface, but instead appears as a fine-grained noise layer through which the surface is viewed, which still appears blurry. This occurs even in stereo (to a lesser extent), as though depth cues from the apparent defocus of the contour-less shaded surface are actively counteracting the stereo depth cues. This is a paper we are currently in the process of writing up. We have not raised it here because it would require going into the details of the results; it will be discussed in the texture follow-up to this work.</p><disp-quote content-type="editor-comment"><p>Overall, however, this is a compelling and important contribution.</p></disp-quote><p>Reviewer #3:</p><disp-quote content-type="editor-comment"><p>[…] Very unusually for me, I do not have much to say by way of critique – the paper is very good in my view. My only real concern is that, if I read the paper correctly, each experiment relied on small number of rendered surfaces (perhaps even just one) and a relatively small number of stimuli (perhaps just one per conditions) presented a small number of times to a relatively large number of people. It is possible then than some effects rely on or are enhanced by very specific image features that happened to occur in these stimuli. It would have been better if multiple surfaces had been used in each experiment.</p></disp-quote><p>This is a fair point. We have generated and examined many variations of the smooth surface geometries included in the paper, and have found that the effects we describe are very general as long as the surface relief and/or illumination elevation are not large enough to induce cast shadows in the image (which introduces additional complex cues to shape and focus). As the exact geometry was otherwise not relevant, we chose to include a greater number of manipulations of other stimulus parameters such as mask type, material, and defocus severity rather than additional geometries.</p></body></sub-article></article>