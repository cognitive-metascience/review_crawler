<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">44838</article-id><article-id pub-id-type="doi">10.7554/eLife.44838</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Primate prefrontal neurons signal economic risk derived from the statistics of recent reward experience</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-33498"><name><surname>Grabenhorst</surname><given-names>Fabian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6455-0648</contrib-id><email>fg292@cam.ac.uk</email><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-48546"><name><surname>Tsutsui</surname><given-names>Ken-Ichiro</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">‡</xref></contrib><contrib contrib-type="author" id="author-129432"><name><surname>Kobayashi</surname><given-names>Shunsuke</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6868-9313</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa2">§</xref></contrib><contrib contrib-type="author" id="author-13988"><name><surname>Schultz</surname><given-names>Wolfram</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8530-4518</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Physiology, Development and Neuroscience</institution><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Lee</surname><given-names>Daeyeol</given-names></name><role>Reviewing Editor</role><aff><institution>Yale School of Medicine</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Ivry</surname><given-names>Richard B</given-names></name><role>Senior Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>‡</label><p>Division of Systems Neuroscience, Graduate School of Life Sciences, Tohoku University, Sendai, Japan</p></fn><fn fn-type="present-address" id="pa2"><label>§</label><p>Department of Neurology, Fukushima Medical University, Fukushima-ken, Japan</p></fn><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>25</day><month>07</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e44838</elocation-id><history><date date-type="received" iso-8601-date="2019-01-07"><day>07</day><month>01</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-07-12"><day>12</day><month>07</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Grabenhorst et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Grabenhorst et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-44838-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.44838.001</object-id><p>Risk derives from the variation of rewards and governs economic decisions, yet how the brain calculates risk from the frequency of experienced events, rather than from explicit risk-descriptive cues, remains unclear. Here, we investigated whether neurons in dorsolateral prefrontal cortex process risk derived from reward experience. Monkeys performed in a probabilistic choice task in which the statistical variance of experienced rewards evolved continually. During these choices, prefrontal neurons signaled the reward-variance associated with specific objects (‘object risk’) or actions (‘action risk’). Crucially, risk was not derived from explicit, risk-descriptive cues but calculated internally from the variance of recently experienced rewards. Support-vector-machine decoding demonstrated accurate neuronal risk discrimination. Within trials, neuronal signals transitioned from experienced reward to risk (risk updating) and from risk to upcoming choice (choice computation). Thus, prefrontal neurons encode the statistical variance of recently experienced rewards, complying with formal decision variables of object risk and action risk.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>prefrontal cortex</kwd><kwd>economic decision</kwd><kwd>risk</kwd><kwd>reward</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Principal Research Fellowship</award-id><principal-award-recipient><name><surname>Schultz</surname><given-names>Wolfram</given-names></name></principal-award-recipient></award-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Programme Grant 095495</award-id><principal-award-recipient><name><surname>Schultz</surname><given-names>Wolfram</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Sir Henry Dale Fellowship 206207/Z/17/Z</award-id><principal-award-recipient><name><surname>Grabenhorst</surname><given-names>Fabian</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>Advanced Grant 293549</award-id><principal-award-recipient><name><surname>Schultz</surname><given-names>Wolfram</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>Caltech Conte Center P50MH094258</award-id><principal-award-recipient><name><surname>Schultz</surname><given-names>Wolfram</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neurons in prefrontal cortex track the variance of experienced rewards to guide economic decisions.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Rewards vary intrinsically. The variation can be characterized by a probability distribution over reward magnitudes. Economists distinguish between risk when probabilities are known and ambiguity when probabilities are only incompletely known. The variability of risky rewards can be quantified by the higher statistical ‘moments’ of probability distributions, such as variance, skewness or kurtosis (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The most frequently considered measure of economic risk is variance (<xref ref-type="bibr" rid="bib8">D'Acremont and Bossaerts, 2008</xref>; <xref ref-type="bibr" rid="bib30">Kreps, 1990</xref>; <xref ref-type="bibr" rid="bib35">Markowitz, 1952</xref>; <xref ref-type="bibr" rid="bib50">Rothschild and Stiglitz, 1970</xref>), although skewness and even kurtosis constitute also feasible risk measures that capture important components of variability (<xref ref-type="bibr" rid="bib4">Burke and Tobler, 2011</xref>; <xref ref-type="bibr" rid="bib8">D'Acremont and Bossaerts, 2008</xref>; <xref ref-type="bibr" rid="bib16">Genest et al., 2016</xref>; <xref ref-type="bibr" rid="bib63">Symmonds et al., 2010</xref>). Thus, among the different definitions of economic risk, variance constitutes the most basic form, and this study will consider only variance as economic risk.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.44838.002</object-id><label>Figure 1.</label><caption><title>Risk, choice task and basic behavior.</title><p>(<bold>A</bold>) Relationship between risk measured as reward variance and reward probability. (<bold>B</bold>) Choice task. The animal made a saccade-choice between two visual stimuli (fractals, ‘objects’) associated with specific base reward probabilities. Object reward probabilities varied predictably trial-by-trial according to a typical schedule for eliciting matching behavior and unpredictably block-wise due to base-probability changes. Probabilities were uncued, requiring animals to derive reward risk internally from the variance of recently experienced rewards. Left-right object positions varied pseudorandomly. (<bold>C</bold>) Matching behavior shown in log ratios of rewards and choices. Relationship between log-transformed choice and reward ratio averaged across sessions and animals (N = 16,346 trials; linear regression; equally populated bins of reward ratios; standard errors of the mean (s.e.m.) were smaller than symbols). (<bold>D</bold>) Cumulative object choices in an example session. The choice ratio in each trial block (given by the slope of the dark blue line) matched the corresponding reward ratio (light blue). (<bold>E</bold>) Adaptation to block-wise reward-probability changes. Matching coefficient (correlation between choice and reward ratio) calculated using seven-trial sliding window around base probability changes (data across sessions, asterisks indicate significant correlation, p&lt;0.05).</p><p><supplementary-material id="fig1sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.003</object-id><label>Figure 1—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig1-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig1-v1.tif"/></fig><p>Existing neurophysiological studies on risk have used explicit, well-established informative cues indicating specific levels of risk in an unequivocal manner (<xref ref-type="bibr" rid="bib14">Fiorillo et al., 2003</xref>; <xref ref-type="bibr" rid="bib31">Lak et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">Ledbetter et al., 2016</xref>; <xref ref-type="bibr" rid="bib37">McCoy and Platt, 2005</xref>; <xref ref-type="bibr" rid="bib39">Monosov, 2017</xref>; <xref ref-type="bibr" rid="bib40">Monosov and Hikosaka, 2013</xref>; <xref ref-type="bibr" rid="bib42">O'Neill and Schultz, 2010</xref>; <xref ref-type="bibr" rid="bib43">O'Neill and Schultz, 2013</xref>; <xref ref-type="bibr" rid="bib48">Raghuraman and Padoa-Schioppa, 2014</xref>; <xref ref-type="bibr" rid="bib57">Stauffer et al., 2014</xref>; <xref ref-type="bibr" rid="bib72">White and Monosov, 2016</xref>). However, in daily life, outside of testing laboratories, animals are confronted with risky rewards without being over-trained on explicit, risk-descriptive cues; they need to estimate themselves the risk from the experienced rewards in order to make economic decisions. Thus, the more natural way to address the inherently risky nature of rewards is to sample the occurrence of reward from experience in a continuous manner, integrate it over time, and compute risk estimates from that information.</p><p>The current study aimed to obtain a more representative view on neuronal risk processing by studying variance risk estimated from experience. To this end, we examined behavioral and neurophysiological data acquired in a probabilistic choice task (<xref ref-type="bibr" rid="bib19">Herrnstein, 1961</xref>; <xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>) in which reward probabilities, and thus risk, changed continuously depending on the animal’s behavior, without being explicitly indicated by specific risk-descriptive cues. Similar to previous risk studies, experienced rewards following choices for specific objects or actions constituted external cues for risk estimation. The terms of risk seeking and risk avoidance indicate that risk processing is subjective. Therefore, we estimated subjective risk from the recent history of the animal’s own choices and rewards, based on logistic regression; we investigated the objective risk derived from experimentally programmed reward probabilities only for benchmark tests, again without explicit risk-descriptive cues. The individually distinct risk attitudes reflect the fact that risk influences economic decisions (<xref ref-type="bibr" rid="bib21">Holt and Laury, 2002</xref>; <xref ref-type="bibr" rid="bib58">Stephens and Krebs, 1986</xref>; <xref ref-type="bibr" rid="bib71">Weber and Milliman, 1997</xref>). Therefore, we followed concepts of decision-making based on competition between object values or action values (<xref ref-type="bibr" rid="bib9">Deco et al., 2013</xref>; <xref ref-type="bibr" rid="bib61">Sutton and Barto, 1998</xref>; <xref ref-type="bibr" rid="bib68">Wang, 2008</xref>) and defined in analogy object risk as the risk attached to individual choice objects and action risk attached to individual actions for obtaining the objects. We studied individual neurons in the dorsolateral prefrontal cortex (DLPFC) of rhesus monkeys where neurons are engaged in reward decisions (<xref ref-type="bibr" rid="bib1">Barraclough et al., 2004</xref>; <xref ref-type="bibr" rid="bib10">Donahue and Lee, 2015</xref>; <xref ref-type="bibr" rid="bib27">Kennerley et al., 2009</xref>; <xref ref-type="bibr" rid="bib54">Seo et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>; <xref ref-type="bibr" rid="bib70">Watanabe, 1996</xref>). The results suggest that DLPFC neurons signal risk derived from internal estimates of ongoing reward experiences.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Choice task and behavior</title><p>Two monkeys performed in a probabilistic choice task (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) in which they repeatedly chose between two visual objects (A and B) to obtain liquid rewards (<xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>). The matching behavior in this task has previously been reported (<xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>). Here, we briefly show the basic pattern of matching behavior across animals before addressing the novel question of how risk influenced behavior and neuronal activity. In the task, both options had the same, constant reward amount but specific, independently set base probabilities during blocks of typically 50–150 trials. Despite the set base probability, each object’s instantaneous reward probability increased in each trial in which the object was not chosen but fell back to its base probability after the object had been chosen (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). Importantly, once reward probability had reached p=1.0, the reward remained available until the animal chose the object. Thus, reward probabilities changed depending on the animal’s choice, and the current, instantaneous level of reward probability was not explicitly cued. Accordingly, to maintain an estimate of reward probability, the animal would need to track internally its own choices and experienced rewards.</p><p>Under these conditions, an efficient strategy consists of repeatedly choosing the object with the higher base probability and choosing the alternative only when its instantaneous reward probability has exceeded the base probability of the currently sampled object (<xref ref-type="bibr" rid="bib7">Corrado et al., 2005</xref>; <xref ref-type="bibr" rid="bib24">Houston and McNamara, 1981</xref>). Aggregate behavior in such tasks usually conforms to the matching law (<xref ref-type="bibr" rid="bib19">Herrnstein, 1961</xref>), which states that the ratio of choices to two alternatives matches the ratio of the number of rewards received from each alternative. Such behavior has been observed in monkeys (<xref ref-type="bibr" rid="bib7">Corrado et al., 2005</xref>; <xref ref-type="bibr" rid="bib32">Lau and Glimcher, 2005</xref>; <xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib60">Sugrue et al., 2004</xref>). Consistent with these previous studies, the animals allocated their choices proportionally to the relative object-reward probabilities (<xref ref-type="fig" rid="fig1">Figure 1C,D</xref>). Through their alternating choices, they detected uncued base-probability changes and adjusted their behavior accordingly (<xref ref-type="fig" rid="fig1">Figure 1E</xref>).</p><p>Thus, the animals’ behavior in the choice task corresponded well to the theoretical assumptions (<xref ref-type="bibr" rid="bib19">Herrnstein, 1961</xref>; <xref ref-type="bibr" rid="bib24">Houston and McNamara, 1981</xref>) and suggested that they estimated well the current reward probabilities of the choice options. On the basis of these choices, we derived specific risk measures that we used as regressors for neuronal responses in DLPFC.</p></sec><sec id="s2-2"><title>Definition of risk measures</title><p>We used two measures of variance risk. The first, objective risk measure linked our study to previous work and provided a foundation for investigating subjective risk. In the choice task, the objective reward probability evolved continually from the animal’s choices (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). This characteristic allowed us to calculate objective risk in each trial as statistical variance derived only from the objective reward probability (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) (reward amount was constant and identical for each option). The second, subjective measure of risk addressed the origin of the neuronal risk information in the absence of informative risk cues. Here, risk was derived directly and subjectively from the evolving statistical variance of recently experienced reward outcomes resulting from specific choices (<xref ref-type="disp-formula" rid="equ5 equ6 equ7 equ8">Equation 5-8)</xref>. We then investigated whether responses in DLPFC neurons associated these two different risk estimates with choice objects (‘object risk’) or with actions required for choosing these objects (‘action risk’), irrespective of the animal’s choice.</p></sec><sec id="s2-3"><title>Neuronal coding of objective risk</title><p>The first, most basic risk measure derived risk as variance in each trial and for each choice object directly from the ‘inverted-U’ function (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) of the true, objective, ‘physical’ reward probability (which depended in each trial on the animal’s previous choices) (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). This risk measure derived from the programmed binary probability (Bernoulli) distribution on each trial that governed actual reward delivery and assumed no temporal decay in subjective perception, attention or memory of reward occurrence and risk. Risk as variance defined in this way increased between p=0.0 and p=0.5 and decreased thereafter, following an inverted-U function (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Thus, probability and risk were not one and the same measure; they correlated positively for the lower half of the probability range (p=0.0 to p=0.5) and inversely for the upper half of the probability range (p=0.5 to p=1.0); higher probability did not necessarily mean higher risk.</p><p>Among 205 task-related DLPFC neurons, 102 neurons had activity related to the objective form of risk, defined as variance derived from the true probability (50% of task-related neurons; p&lt;0.05 for object risk coefficient, multiple linear regression, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>; 185 of 1222 significant task-related responses in different trial periods, 15%; task-relatedness assessed by Wilcoxon test with p&lt;0.005, corrected for multiple comparisons). During the fixation periods, the activity of the neuron shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref> reflected the risk for object B (<xref ref-type="fig" rid="fig2">Figure 2A,B</xref>; p=0.0172, multiple linear regression, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>), whereas the coefficient for object-A risk was non-significant (p<italic>=</italic>0.281). Critically, the signal reflected the variance risk associated with object B and neither reward probability for object A or B (both p&gt;0.36) nor choice, action or left-right cue position (all p&gt;0.25). To further evaluate the object-specificity of the risk response, we classified the response using the angle of regression coefficients (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, see Materials and methods), which confirmed object risk coding rather than risk difference or risk sum coding of the two objects (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Thus, the neuron’s activity signaled the continually evolving true risk for a specific choice object. Across neuronal responses, classification based on the angle of regression coefficients (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) showed 195 significant risk responses in 89 neurons (p&lt;0.05, F-test), of which 75 responses (39%, 47 neurons, <xref ref-type="fig" rid="fig2">Figure 2C</xref>) coded object risk rather than risk difference or risk sum, thus confirming the object-risk coding described above. In the population of DLPFC neurons, object-risk responses occurred in all task periods (<xref ref-type="fig" rid="fig2">Figure 2D</xref>), including the pre-choice periods, in time to inform decision-making.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.44838.004</object-id><label>Figure 2.</label><caption><title>Neuronal coding of objective risk for objects and actions.</title><p>(<bold>A</bold>) Activity from a single DLPFC neuron in fixation period related to the true risk associated with object B, derived from reward probability. Top: peri-event time histogram of impulse rate, aligned to fixation spot onset, sorted into object-risk terciles. Bottom: raster display: ticks indicate impulses, rows indicate trials; gray dots indicate event markers. Yellow shaded zone (500 ms after fixation cue onset) indicates analysis period. (<bold>B</bold>) Beta coefficients (standardized slopes ± s.e.m) from multiple linear regression. Only the object-B risk beta coefficient was significant (p=0.0172, t-test; all other coefficients: p&gt;0.25). (<bold>C</bold>) Categorization of coding risk for object A or B or relative risk (risk difference or risk sum) based on the angle of regression coefficients across neurons. Each symbol represents a neuronal response by its normalized risk slopes for objects A and B; different symbols indicate differently classified neuronal responses as follows. Yellow circles: responses classified as coding object risk; red circle: position of object-B risk response of the neuron shown in A and B; black crosses: responses classified as coding relative risk. (<bold>D</bold>) Percentages of object risk responses for all task epochs (multiple linear regression; 1222 task-related responses from 205 neurons). (<bold>E</bold>) Activity of a single DLPFC neuron related to the true risk for leftward saccades (left action risk) in cue period (pFix: onset of peripheral fixation spot confirming choice).</p><p><supplementary-material id="fig2sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.005</object-id><label>Figure 2—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig2-v1.tif"/></fig><p>We also tested for neuronal coding of objective action risk. Once the left-right position of the risky choice objects was revealed on each trial, the animals could assess the risk associated with leftward or rightward saccade actions. A total of 57 DLPFC neurons coded action risk in cue or post-cue task periods (p&lt;0.05; multiple linear regression, <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, action risk regressors). During the choice cue phase, the activity of the neuron in <xref ref-type="fig" rid="fig2">Figure 2E</xref> reflected the action risk for leftward saccades (p=0.041, multiple linear regression, <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>), whereas the coefficient for rightward saccade risk was non-significant (p=0.78). The signal reflected the variance risk associated with left actions but neither reflected reward probability for left or right actions (both p<italic>&gt;</italic>0.44), nor the actual choice or left-right action (both p<italic>&gt;</italic>0.34) but reflected additionally the left-right cue position (p<italic>=</italic>0.0277).</p><p>Taken together, a significant number of DLPFC neurons showed activity related to the true, objective risk associated with specific objects or actions. Although this basic risk measure accounted for the continually evolving risk levels in the choice task, it did not reflect the assumption that the animals’ risk estimates were likely subjective, owing to imperfect knowledge and memory of the true reward probabilities. Accordingly, we next defined a subjective risk measure, validated its relevance to the animals’ behavior, and tested its coding by DLPFC neurons.</p></sec><sec id="s2-4"><title>Subjective risk: definition and behavior</title><p>Whereas our first, objective risk measure concerned the variance derived from objective (true, programmed) reward probability, our second risk measure assumed imperfect, temporally degrading assessment of recently experienced rewards. To obtain this measure, we established subjective weights for recently experienced rewards using logistic regression on the animal’s reward and choice history (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>), following standard procedures for analyzing subjective decision variables in similar choice tasks (<xref ref-type="bibr" rid="bib7">Corrado et al., 2005</xref>; <xref ref-type="bibr" rid="bib32">Lau and Glimcher, 2005</xref>). These weights (<xref ref-type="fig" rid="fig3">Figure 3A,B</xref>) revealed declining influences of past rewards and past choices on the animal’s current-trial choice, in line with previous matching studies (<xref ref-type="bibr" rid="bib7">Corrado et al., 2005</xref>; <xref ref-type="bibr" rid="bib32">Lau and Glimcher, 2005</xref>). On the basis of this result, the assessment of subjective variance risk in each trial considered only data from the preceding 10 trials.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.44838.006</object-id><label>Figure 3.</label><caption><title>Deriving subjective risk from the variance of reward history.</title><p>(<bold>A</bold>) Subjective weights of influence of recent rewards on object choice, as derived from logistic regression. Filled symbols indicate significance (p&lt;0.005, t-test; pooled across animals). (<bold>B</bold>) Subjective weights of influence of recent choice on object choice, as derived from logistic regression. (<bold>C</bold>) Approach for deriving subjective risk from the variance of recent reward history. Upper panel: vertical black bars represent rewarded choices for object A. Middle/lower panels: trial-by-trial estimates of value (middle) and risk (lower) calculated, respectively, as mean and variance of reward history over last 10 trials (using weights shown in <bold>A</bold>). The dashed magenta line indicates the subjective risk estimate used for neuronal and behavioral analysis. Heavy lines: running average of weighted estimates. Thin solid lines: unweighted, objective value/risk. Value from reward history was highest in the high-probability block, whereas risk was highest in medium-probability block (inverted U-shaped relationship, see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). All units were normalized to allow for visual comparisons. (<bold>D</bold>) Positive influence of subjective risk on choice (risk-seeking attitude) and separate value and risk influences on choice. Object-choice probability increased with risk difference between objects (ΔRisk, sorted by median split; ‘high’ indicates higher risk with object A compared to object B; p&lt;0.002 for all pair-wise choice probability comparisons between adjacent relative-risk levels, χ<sup>2</sup>-tests; N = 16,346 trials). The risk effect added monotonically and consistently to the increase of choice probability with object value difference (ΔValue). (<bold>E</bold>) Logistic regression. Coefficients (β) for relative value (ΔValue, p=4.4 × 10<sup>−39</sup>), relative risk (ΔRisk. p=2.8 × 10<sup>−4</sup>) and left-right bias (Bias, p=0.698) across sessions (t-tests, random-effects analysis). The constant (bias) was not significant, suggesting negligible side bias. The inset shows coefficients for a subset of trials where value difference was minimized (10% of trials); only risk difference was significantly related to choice (p=0.0072) but not value difference (p=0.182), data pooled across animals. (F) Psychometric functions relating relative value and risk to choice probability (across animals and sessions).</p><p><supplementary-material id="fig3sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.009</object-id><label>Figure 3—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig3-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44838.007</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Influences on saccadic reaction times.</title><p>Standardized regression coefficients from multiple regression of saccadic reaction times on different task-related variables. Data pooled across animals; reaction times were z-standardized within each testing session. |Val diff|: unsigned difference in object value between choice objects; Val sum: sum of object values; |Risk diff|: unsigned difference in object risk between choice objects; Risk sum: sum of object risks; Animal: dummy variable denoting animal identity. Asterisks indicate statistical significance (p&lt;0.01, t-test on regression coefficients).</p><p><supplementary-material id="fig3s1sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.008</object-id><label>Figure 3—figure supplement 1—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig3-figsupp1-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig3-figsupp1-v1.tif"/></fig></fig-group><p>For behavioral and neuronal comparisons between subjective risk and value, we first defined ‘object value’ as the recency-weighted reward value for a specific choice object (<xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>). We followed previous studies of matching behavior (<xref ref-type="bibr" rid="bib32">Lau and Glimcher, 2005</xref>) that distinguished two influences on value: the history of recent rewards and the history of recent choices. The first value component related to reward history can be estimated by the mean of subjectively weighted reward history over the past ten trials (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, dashed blue curve, <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>) and provided a useful comparison for our subjective risk measure, which also derived from reward history (described in the next paragraph). To estimate a comprehensive measure of object value for behavioral and neuronal analysis, we incorporated the additional effect of choice history on value, which is distinct from reward history as shown in previous studies of matching behavior (<xref ref-type="bibr" rid="bib32">Lau and Glimcher, 2005</xref>). Thus, we estimated object value based on both subjectively weighted reward history and subjectively weighted choice history (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>); this constituted our main value measure for behavioral and neuronal analyses. (We consider distinctions between reward and choice history and their potential influence on risk in the Discussion).</p><p>We next calculated for each trial the subjective measure of ‘object risk’ as the statistical variance of the distribution of rewards of the preceding ten trials, separately for each object (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, dashed magenta curve; <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>). Specifically, subjective object risk was derived from the sum of the weighted, squared deviations of object-specific rewards from the mean of the object-specific reward distribution over the past ten trials (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>); subjective weighting of the squared deviations with empirically estimated reward weights (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) accounted for declining influences of more remote past trials on behavior. Object risk defined in this manner varied continuously as a function of past rewards and past choices as follows. When block-wise reward probability was low, each reward increased both object risk and value; without further rewards, both risk and value decreased gradually over subsequent trials (<xref ref-type="fig" rid="fig3">Figure 3C</xref> compare blue and magenta curves; note that the blue curve shows the effect of reward history on value). When reward probability was high, object risk increased when a reward was omitted (which drove the instantaneous probability toward the center of the inverted-U function); with further rewards, object risk decreased gradually over subsequent trials (driving the instantaneous probability toward the high end of the inverted U function). Risk was highest in medium-probability blocks with alternating rewarded and unrewarded trials (variations around the peak of the inverted-U function).</p><p>We next assessed the animals’ risk attitude by testing the influence of subjective risk on the animals’ choices. Choice probability increased monotonically with increasing difference in object value (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, ΔValue, derived from the sum of weighted reward and choice histories, <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>). Importantly, risk had an additional influence on object choice: with increasing risk difference between objects (ΔRisk, <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>), choice probability consistently increased for the higher-risk object even with constant value-difference level (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, yellow and orange bars). The more frequent choice of the riskier object at same value level indicated risk-seeking. A specific logistic regression (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, different from the logistic regression estimating the subjective weights for past trials) confirmed the risk-seeking attitude by a significant positive weight (i.e. beta) of risk on the animals’ choices, independent of value (<xref ref-type="fig" rid="fig3">Figure 3E,F</xref>). When using a subset of trials with minimal value difference for this logistic regression, we again found a significant influence of risk on choices (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, inset). Formal comparisons favored this choice model based on subjective object value (derived from both weighted reward history and choice history) and subjective object risk over several alternatives, including models without risk, models with different value definitions, models based on objective (true) reward probabilities and risks, and variants of reinforcement learning models (see <xref ref-type="table" rid="table1">Table 1</xref>). Our subjective risk measure also showed specific relationships to saccadic reaction times, alongside value influences on reaction times (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). These data confirmed the previously observed positive attitudes of macaques towards objective risk (<xref ref-type="bibr" rid="bib16">Genest et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Lak et al., 2014</xref>; <xref ref-type="bibr" rid="bib37">McCoy and Platt, 2005</xref>; <xref ref-type="bibr" rid="bib42">O'Neill and Schultz, 2010</xref>; <xref ref-type="bibr" rid="bib57">Stauffer et al., 2014</xref>) and validated our subjective, experience-based object-risk measure as regressor for neuronal activity.</p><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.44838.010</object-id><label>Table 1.</label><caption><title>Comparison of different models fitted to the animals’ choices.</title><p>Best fitting model indicated in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" valign="top">Model</th><th rowspan="2" valign="top">Description</th><th colspan="2" valign="top">Both animals</th><th colspan="2" valign="top">Animal A</th><th colspan="2" valign="top">Animal B</th></tr><tr><th valign="top">AIC</th><th valign="top">BIC</th><th valign="top">AIC</th><th valign="top">BIC</th><th valign="top">AIC</th><th valign="top">BIC</th></tr></thead><tbody><tr><td valign="top">(1)</td><td valign="top">Value from reward history<sup>1</sup></td><td valign="top">2.2482</td><td valign="top">2.2490</td><td valign="top">1.5077</td><td valign="top">1.5084</td><td valign="top">7.3571</td><td valign="top">7.3636</td></tr><tr><td valign="top">(2)</td><td valign="top">Value from reward history and risk<sup>2</sup></td><td valign="top">2.2477</td><td valign="top">2.2492</td><td valign="top">1.5077</td><td valign="top">1.5092</td><td valign="top">7.3522</td><td valign="top">7.3653</td></tr><tr><td valign="top">(3)</td><td valign="top">Value from choice history<sup>3</sup></td><td valign="top">2.1614</td><td valign="top">2.1622</td><td valign="top">1.4900</td><td valign="top">1.4907</td><td valign="top">6.5043</td><td valign="top">6.5109</td></tr><tr><td valign="top">(4)</td><td valign="top">Value from choice history and risk</td><td valign="top">2.0385</td><td valign="top">2.0400</td><td valign="top">1.4023</td><td valign="top">1.4037</td><td valign="top">7.3528</td><td valign="top">7.3660</td></tr><tr><td valign="top">(5)</td><td valign="top">Value from reward and choice history<sup>4</sup></td><td valign="top">2.0089</td><td valign="top">2.0097</td><td valign="top">1.3914</td><td valign="top">1.3922</td><td valign="top">6.0880</td><td valign="top">6.0945</td></tr><tr><td valign="top"><bold>(6)</bold></td><td valign="top"><bold>Value from reward and choice history and risk</bold></td><td valign="top"><bold>2.0073</bold></td><td valign="top"><bold>2.0088</bold></td><td valign="top"><bold>1.3899</bold></td><td valign="top"><bold>1.3914</bold></td><td valign="top"><bold>6.0747</bold></td><td valign="top"><bold>6.0878</bold></td></tr><tr><td valign="top">(7)</td><td valign="top">Objective reward probabilities<sup>5</sup></td><td valign="top">2.1213</td><td valign="top">2.1220</td><td valign="top">1.4615</td><td valign="top">1.4622</td><td valign="top">6.4972</td><td valign="top">6.5037</td></tr><tr><td valign="top">(8)</td><td valign="top">Objective reward probabilities and objective risk<sup>6</sup></td><td valign="top">2.1210</td><td valign="top">2.1225</td><td valign="top">1.4616</td><td valign="top">1.4631</td><td valign="top">6.4982</td><td valign="top">6.5114</td></tr><tr><td valign="top">(9)</td><td valign="top">Reinforcement learning (RL) model<sup>7</sup></td><td valign="top">2.0763</td><td valign="top">2.0779</td><td valign="top">1.4376</td><td valign="top">1.4391</td><td valign="top">6.2161</td><td valign="top">6.2293</td></tr><tr><td valign="top">(10)</td><td valign="top">RL learning, stack parameter (<xref ref-type="bibr" rid="bib25">Huh et al., 2009</xref>)<sup>8</sup></td><td valign="top">2.0810</td><td valign="top">2.0826</td><td valign="top">1.4374</td><td valign="top">1.4389</td><td valign="top">6.3198</td><td valign="top">6.3330</td></tr><tr><td valign="top">(11)</td><td valign="top">RL, reversal-learning variant<sup>9</sup></td><td valign="top">2.2614</td><td valign="top">2.2630</td><td valign="top">1.5330</td><td valign="top">1.5344</td><td valign="top">7.2808</td><td valign="top">7.2939</td></tr></tbody></table><table-wrap-foot><fn><p>1:Value defined according to <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>; 2: Risk defined according to <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>; 3: Value defined as sum of weighted choice history derived from <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>; 4: Value defined according to <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>; 5: Objective reward probabilities defined according to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>; 6: Objective reward risk defined according to <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>; 7: Standard Rescorla-Wagner RL model updating value of chosen option based on last outcome; 8: Modified RL model incorporating choice-dependency; 9: Modified RL model updating value of chosen and unchosen option based on last outcome.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s2-5"><title>Neuronal coding of subjective risk associated with choice objects</title><p>The subjective variance-risk for specific choice objects, derived from reward history, was coded in 95 of the 205 task-related DLPFC neurons (46%; p&lt;0.05 for object-risk regressors, multiple linear regression, <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>). These 95 neurons showed 153 object-risk-related responses (among 1222 task-related responses, 13%; 28 of the 153 responses coded risk for both objects, and 125 responses only for one object). Importantly, object-risk coding in these neurons was not explained by object value, which was included as covariate in the regression (shared variance between risk and value regressors: R<sup>2</sup> = 0.148 across sessions). A distinct object-choice regressor significantly improved the regression for only 12 responses (p&lt;0.05, partial F-test, <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>), suggesting most object-risk responses (141/153, 92%) were choice-independent (p=1.8 × 10<sup>−25</sup>, z-test). A subset of 66 of 153 risk responses (43%) fulfilled our strictest criteria for coding object risk: they coded risk before choice, only for one object, and irrespective of the actual choice, thus complying with requirements for coding a decision variable analogous to object value.</p><p>The activity of the neuron in <xref ref-type="fig" rid="fig4">Figure 4A–B</xref> illustrates the response pattern of an object-risk neuron. During fixation, the activity of the neuron in <xref ref-type="fig" rid="fig4">Figure 4A</xref> reflected the current risk estimate for object A (p=0.0316, multiple linear regression, <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>), but was not significant for object-B risk (p=0.69), nor for object values (both p&gt;0.22). True to the concept of a decision input, the risk signal occurred well before the monkey made its choice (in time to inform decision-making) and it was not explained by current-trial choice, cue position, or action (all p&gt;0.46). Classification based on the angle of regression coefficients confirmed coding of object risk, rather than relative risk (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Thus, the neuron’s activity signaled the continually evolving subjective risk estimate for a specific choice object and may constitute a suitable input for decision mechanisms under risk.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.44838.011</object-id><label>Figure 4.</label><caption><title>Subjective object-risk coding.</title><p>(<bold>A</bold>) Activity from a single DLPFC neuron coding subjective risk associated with object A before choice (fixation period). Object risk was derived from the variance of recently experienced rewards associated with a specific object. (<bold>B</bold>) Beta coefficients (standardized slopes) from a multiple linear regression of the neuron’s fixation impulse rate showed significant coding only for object-A risk (p=0.0316, t-test; all other coefficients: p&gt;0.22). (<bold>C</bold>) Categorization of coding risk for object A or B, risk difference or risk sum based on the angle of coefficients. Red circle: position of object-A risk response of the neuron shown in A and B. (<bold>D</bold>) Percentages of object-risk responses for all task epochs (multiple regression, 1222 task-related responses from 205 neurons). (<bold>E</bold>) Population activity of object-risk neurons as a function of object value. Activity conformed to the characteristic inverted U-shaped relationship between reward-variance risk and reward probability (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Error bars were smaller than symbols. (<bold>F</bold>) Neuronal risk-updating following reward. Population activity of object-risk neurons at the time of choice (cue period), shown separately for trials in which object risk on the current trial increased (top) or decreased (bottom) following reward on the previous trial. When a reward increased object risk (by increasing reward variance), cue-activity on the following trial (N, magenta) was significantly higher compared to that on the previous trial (N-1, blue; p&lt;0.001, Wilcoxon test), reflecting the updated object risk. Conversely, when a reward decreased object risk (by decreasing reward variance), activity on the following trial (green) decreased correspondingly (p&lt;0.001).</p><p><supplementary-material id="fig4sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.019</object-id><label>Figure 4—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig4-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44838.012</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Anatomical location of recording sites.</title><p>Anterior-posterior position was defined with respect to inter-aural line. Orange crosses indicate locations for all recorded neurons. PS, approximate position of principal sulcus. Lower right anatomical schematic outlines recording locations in upper and lower banks of principal sulcus. Numbers indicate anterior-posterior distance from inter-aural line. .</p><p><supplementary-material id="fig4s1sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.013</object-id><label>Figure 4—figure supplement 1—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig4-figsupp1-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44838.014</object-id><label>Figure 4—figure supplement 2.</label><caption><title>Reward-history control.</title><p>(<bold>A</bold>) Activity of a single DLPFC neuron reflecting the non-linear interaction between rewards in the previous two consecutive trials. The neuron showed stronger activity in the fixation period on the current trial (N) when reward had been received on the previous trial (N-1). By contrast, current-trial activity was stronger when no reward had been received two trials ago (N-2). (B) Percentage of object-risk neurons identified in a supplementary regression (<xref ref-type="disp-formula" rid="equ12">Equation 12</xref>) that included additional covariates for reward, choice and reward × choice history for the preceding two consecutive trials (N-1 and N-2). Inclusion of these control covariates had only a minor effect on the percentage of identified object-risk neurons compared to our main regression (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>). Compared to robust object-risk signals across task periods (magenta), responses explicitly reflecting consecutive reward history (purple) across the two preceding trials, or consecutive reward × choice history (green) or choice history (orange) were rare, likely because these effects were better accounted for by object-value and object-risk regressors.</p><p><supplementary-material id="fig4s2sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.015</object-id><label>Figure 4—figure supplement 2—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig4-figsupp2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44838.016</object-id><label>Figure 4—figure supplement 3.</label><caption><title>Control analyses for neuronal object-risk coding.</title><p>(<bold>A</bold>) Results from supplementary analyses in which neuronal activity was regressed on object-risk measures derived using different exponential weighting functions for past rewards. Numbers next to the color code in the legend state the number of identified object-risk neurons for each color-coded weighting function. The main analysis using empirically derived weighting function from the each animals’ choices identified 95 object-risk neurons. (<bold>B</bold>) Histograms of partial-R<sup>2</sup> values (quantifying explained variance in neuronal activity) for neuronal responses with significant coefficient for subjective risk (upper histogram) and significant coefficient for objective risk (lower histogram). Red lines indicate distribution means. The distributions were significantly different (p=0.0015, Kolmogorov-Smirnov test) with higher distribution mean for subjective risk (p=0.04, Wilcoxon test). (<bold>C</bold>) Results from stepwise multiple regressions. Shown are the fractions of neurons (from 205 recorded DLPFC neurons, pooled across all task periods) with significant effects for the different variables when both objective and subjective value and risk variables were included in the starting set of regressors (upper panel) and when both object risk and action risk variables were included in the starting set (lower panel). These results and those in the main text show that significant numbers of neurons coded object risk across analysis approaches.</p><p><supplementary-material id="fig4s3sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.018</object-id><label>Figure 4—figure supplement 3—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig4-figsupp3-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44838.017</object-id><label>Figure 4—figure supplement 4.</label><caption><title>Numbers of neurons (and percentages of recorded neurons) encoding risk and value for alternative risk definitions.</title><p>The alternative definitions were used for calculating the object risk regressors in our main neuronal regression model (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig4-figsupp4-v1.tif"/></fig></fig-group><p>Classification of neuronal responses based on the angle of regression coefficients (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) showed 159 significant risk responses in 80 neurons (p&lt;0.05, F-test), of which 83 responses (52%, 53 neurons, <xref ref-type="fig" rid="fig4">Figure 4C</xref>) coded object risk rather than risk difference or risk sum. This result confirmed that a substantial number of neurons encoded risk for specific objects; in addition, other neurons encoded risk signals related to both objects as risk difference or risk sum, similar to encoding of value difference and value sum in previous studies (<xref ref-type="bibr" rid="bib5">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Wang et al., 2013</xref>). Object risk signals occurred with high prevalence in early trial epochs, timed to potentially inform decision-making (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). They were recorded in upper and lower principal sulcus banks, confirmed by histology (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>Activity of these object-risk neurons conformed to key patterns that distinguish risk-coding from value-coding. Their population activity followed the typical inverted-U relationship with reward value (cf. <xref ref-type="fig" rid="fig1">Figure 1A</xref>) by increasing as a function of value within the low-value range and decreasing with value within the high-value range (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). Accordingly, reward outcomes should increase or decrease risk-related activity depending on whether the additional reward increased or decreased the variance of recent rewards. This feature of reward-variance risk was implicit in the formulation of our risk regressor (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, magenta curve) and is also illustrated in the population activity in <xref ref-type="fig" rid="fig4">Figure 4F</xref>. Following reward receipt on trial N-1 (blue curves), activity of risk-neurons on the subsequent trial increased only when the reward led to an increase in reward variance (<xref ref-type="fig" rid="fig4">Figure 4F</xref> magenta curve, cf. <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>; ascending slope in <xref ref-type="fig" rid="fig4">Figure 4E</xref>). By contrast, when reward receipt led to decreased reward variance, neuronal activity on the subsequent trial also decreased (<xref ref-type="fig" rid="fig4">Figure 4F</xref> green curve; descending slope in <xref ref-type="fig" rid="fig4">Figure 4E</xref>). Thus, activity of risk neurons followed the evolving statistical variance of rewards, rather than reward probability.</p></sec><sec id="s2-6"><title>Control analyses for subjective object-risk coding</title><p>Further controls confirmed subjective object-risk coding in DLPFC. Sliding-window regression without pre-selecting responses for task-relatedness identified similar numbers of object-risk neurons as our main fixed-window analysis (82/205 neurons, 40%; <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>). This sliding-window regression also confirmed that object-risk signals were not explained by past-trial reward, choice, or reward ×choice history, which were regression covariates.</p><p>Because our behavioral model estimated risk over multiple past trials, we also tested whether history variables from the past two trials could explain object-risk coding (<xref ref-type="disp-formula" rid="equ12">Equation 12</xref>). An extended regression identified some responses that reflected nonlinear interactions between rewards over two consecutive past trials (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2A</xref>); these responses might contribute to risk estimation by detecting changes in reward rate. However, they were rare and did not explain our main finding of object-risk coding (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2B</xref>; 99 risk neurons from <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>).</p><p>Varying the integration-time windows for risk estimation (using different exponentials and integration up to 15 past trials) resulted in some variation of identified numbers of risk neurons but did not affect our main finding of risk-coding in DLPFC neurons (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3A</xref>).</p><p>A direct comparison of objective and subjective risk showed that neuronal activity tended to be better explained by subjective risk. We compared the amount of variance explained by both risk measures when fitting separate regressions. The distributions of partial-R<sup>2</sup> values were significantly different between risk measures (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3B</xref>, p=0.0015, Kolmogorov-Smirnov test), attesting to the neuronal separation of these variables. Specifically, subjective risk explained significantly more variance in neuronal responses compared to objective risk (p=0.0406, Wilcoxon test). When both risk measures were included in a stepwise regression model (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>), and thus competed to explain variance in neuronal activity, we identified more neurons related to subjective risk than to objective risk (107 compared to 83 neurons, <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3C</xref>), of which 101 neurons were exclusively related to subjective risk but not objective risk (shared variance between the two risk measures across sessions: R<sup>2</sup> = 0.111 ± 0.004, mean ± s.e.m.).</p><p>We also considered alternative, more complex definitions of subjective risk that incorporated either weighted reward history or both weighted reward and choice history in the risk calculation. These alternative definitions yielded identical or only slightly higher numbers of identified risk neurons compared to our main risk definition (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>; less than 5% variation in identified neurons). We therefore focused on our main risk definition (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>), which was simpler and more conservative as it incorporated fewer assumptions.</p><p>Finally, we examined effects of potential non-stationarity of neuronal activity (<xref ref-type="bibr" rid="bib11">Elber-Dorozko and Loewenstein, 2018</xref>), by including a first-order autoregressive term in <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>. This resulted in 88 identified risk neurons (compared to 95 neurons in our original analysis). In a further test, we subtracted the activity measured in a control period (at trial start) of the same trial before performing the regression analysis; this procedure should remove effects due to slowly fluctuating neuronal activities. This analysis identified 56 neurons with activity related to risk (note that the control period itself was excluded from this analysis; our original analysis without the control period yielded 81 risk neurons).</p><p>Taken together, object-risk signals reflecting our subjective risk measure occurred in significant numbers of DLPFC neurons and were robust to variation in statistical modeling.</p></sec><sec id="s2-7"><title>Neuronal coding of subjective risk associated with actions</title><p>Neurons in DLPFC process reward values not only for objects (<xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>) but also for actions (<xref ref-type="bibr" rid="bib28">Khamassi et al., 2015</xref>; <xref ref-type="bibr" rid="bib54">Seo et al., 2012</xref>). Accordingly, we derived subjective action risk from the experienced variance of recent rewards related to specific actions. As our task varied reward probability for particular objects independently of reward probability for particular actions, object risk and action risk showed only low intercorrelation (R<sup>2</sup> = 0.153). Among 205 task-related neurons, 90 (44%) coded action risk (148 of 1222 task-related responses, 12%; <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>). A subset of 77 of 148 action-risk signals (52%) fulfilled our strictest criteria for action risk: they coded risk before the saccadic choice, only for one action, and irrespective of the actual choice, thus complying with requirements for coding a decision variable analogous to action value.</p><p>The fixation-period activity of the neuron in <xref ref-type="fig" rid="fig5">Figure 5A</xref> signaled the risk associated with rightward saccades, reflecting the variance of rewards that resulted from recent rightward saccades (<xref ref-type="fig" rid="fig5">Figure 5A</xref>; p=0.0233, multiple linear regression, <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>). The neuronal response was linearly related to risk for rightward but not leftward saccades and failed to correlate with action choice, object choice or action value (all p&gt;0.11; <xref ref-type="fig" rid="fig5">Figure 5B</xref>). Classification based on the angle of regression coefficients confirmed the designation as an action-risk signal (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Thus, the neuron’s activity signaled the continually evolving subjective risk estimate for a specific action.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.44838.020</object-id><label>Figure 5.</label><caption><title>Subjective action-risk coding.</title><p>(<bold>A</bold>) Activity from a single DLPFC neuron coding subjective risk associated with rightward saccades (action R) during fixation. Action risk was derived from the variance of recently experienced rewards associated with a specific action. (<bold>B</bold>) Beta coefficients (standardized slopes) from a multiple linear regression of the neuron’s fixation impulse rate showed significant coding only for action risk associated with rightward saccades (p=0.0233, t-test; all other coefficients: p&gt;0.11). (<bold>C</bold>) Categorization of coding risk for action L or R, risk difference or risk sum based on the angle of coefficients. Red circle: position of action-R risk response of the neuron shown in A and B. (<bold>D</bold>) Percentages of action-risk responses for all task epochs (multiple regression, 1222 task-related responses from 205 neurons). (<bold>E</bold>) Population activity of action-risk neurons as a function of reward value. Activity conformed to the characteristic inverted U-shaped relationship between reward-variance risk and reward probability (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Error bars were smaller than symbols. (<bold>F</bold>) Neuronal risk-updating following reward. Population activity of action-risk neurons at the time of choice (cue period), shown separately for trials in which reward on the previous trial increased (top) or decreased (bottom) action risk. When a reward increased action risk (by increasing reward variance), cue-activity on the following trial (N, magenta) was significantly higher compared to that on the previous trial (N-1, blue; p&lt;0.001, Wilcoxon test), reflecting the updated action risk. Conversely, when a reward decreased action risk (by decreasing reward variance), activity on the following trial (green) decreased correspondingly (p&lt;0.001).</p><p><supplementary-material id="fig5sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.021</object-id><label>Figure 5—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig5-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig5-v1.tif"/></fig><p>Classification of responses based on the angle of regression coefficients (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) showed 149 significant risk responses in 90 neurons (p&lt;0.05, F-test), of which 71 responses (48%, 56 neurons) coded action risk rather than risk difference or risk sum (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). This result confirmed that a substantial number of neurons encoded risk for specific actions, in addition to neurons encoding risk sum or difference. Action-risk signals occurred throughout all trial periods (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Adding an action-choice regressor improved the regression model in only 15 of 148 risk responses (p&lt;0.05, partial F-test). Thus, most responses (133/148, 90%) coded action-risk without additional action-choice coding (p=3.2 × 10<sup>−22</sup>, z-test for dependent samples). Activity of action-risk neurons followed the typical inverted-U relationship with reward value (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). Reward increased the activity of these risk-neurons only when the reward increased current reward variance, but decreased neuronal activity when it led to decreased reward variance (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). Thus, activity of action-risk neurons followed the evolving statistical variance of rewards.</p><p>Object risk and action risk were often coded by distinct neurons. Separate multiple regression analyses (<xref ref-type="disp-formula" rid="equ10 equ14">Equations 10 and 14</xref>) revealed that 43 of the 205 task-related neurons (21%) encoded object risk but not action risk, and 38 of the 205 task-related neurons (19%) encoded action risk but not object risk. A stepwise regression on both object risk and action risk (<xref ref-type="disp-formula" rid="equ15">Equation 15</xref>) resulted in 55 neurons encoding object risk but not action risk (27%) and 38 neurons encoding action risk but not object risk (19%, <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3C</xref>). Controlling for non-stationarity of neuronal responses, we identified 83 action-risk neurons when including a first-order autoregressive term and 56 neurons when subtracting neuronal activity at trial start. Neurons encoding object risk and action risk were intermingled without apparent anatomical clustering in DLPFC (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), similar to previous studies that failed to detect clustering of object-selective and location-selective neurons (<xref ref-type="bibr" rid="bib13">Everling et al., 2006</xref>).</p></sec><sec id="s2-8"><title>Population decoding of object risk and action risk</title><p>To quantify the precision with which downstream neurons could read risk information from DLPFC neurons, we used previously validated population-decoding techniques, including nearest-neighbor and linear support-vector-machine classifiers (<xref ref-type="bibr" rid="bib17">Grabenhorst et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>). We subjected the total of 205 DLPFC neurons to this analysis and did not pre-select risk-neurons for these analyses (‘unselected neurons’). We grouped trials according to terciles of object risk and action risk and performed classification based on low vs. high terciles (see Materials and methods).</p><p>We successfully decoded object risk and action risk from the population of 205 DLPFC neurons, with accuracies of up to 85% correct in pre-choice trial periods (<xref ref-type="fig" rid="fig6">Figure 6A,B</xref>). Decoding-accuracy increased as a function of the number of neurons in the decoding sample (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Both object risk and action risk were coded with good accuracy across task periods, although object risk was coded significantly more accurately than action risk in several periods (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Decoding from randomly sampled small subsets of neurons (N = 20 per sample) showed that risk-decoding accuracy depended on individual neurons’ risk sensitivities (standardized regression slopes; <xref ref-type="fig" rid="fig6">Figure 6C</xref>). Decoding from specifically defined subsets showed that even small numbers of individually significant risk neurons enabled accurate risk-decoding (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). However, individually significant object-risk neurons carried little information about action risk and individually significant action-risk neurons carried little information about object risk (<xref ref-type="fig" rid="fig6">Figure 6D</xref>), attesting to the neuronal separation of object risk and action risk in DLPFC. Decoding of risk from neuronal responses remained significantly above chance in control analyses in which we held constant the value of other task-related variables including object choice, action and cue position (<xref ref-type="fig" rid="fig6">Figure 6E</xref>).</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.44838.022</object-id><label>Figure 6.</label><caption><title>Population decoding of risk from unselected neurons.</title><p>(<bold>A</bold>) Leave-one-out cross-validated decoding accuracy (% correct classification) of a linear support-vector-machine classifier decoding object risk and action risk in pre-cue period. Decoding performance increased with the number of neurons. Analysis was based on normalized single-trial activity of neurons that met decoding criteria without pre-selection for risk-coding. Data for each neuron number show mean (± s.e.m) over 100 iterations of randomly selected neurons. Each neuron entered the decoder twice, to decode risk for object A and B (or action L and R). (<bold>B</bold>) Decoding for object risk and action was significantly above chance (gray line, decoding from shuffled data) in all task epochs (p&lt;0.0001, Wilcoxon test). Object-risk decoding was significantly better than action-risk decoding in specific trial periods (Asterisks: p&lt;0.005, Wilcoxon test). (<bold>C</bold>) Object-risk and action-risk decoding depended on individual neurons’ sensitivities for object risk and action risk. Linear regressions of decoding performance in pre-cue period from 5000 subsets of 20 randomly selected neurons on each subset’s mean single-neuron regression betas for object risk. (<bold>D</bold>) Decoding accuracy for specific subsets of neurons in pre-cue period. All: entire population of neurons. OR sig: significant object-risk neurons (N = 12). AR sig: significant action-risk neurons (N = 9). Rand: randomly selected neurons (mean ± s.e.m over 5000 randomly selected subsets of N = 20 neurons). (E) Decoding accuracy for object-risk decoding from trial subsets in which control variables were held constant. Left to right: decoding object-risk with constant object choice, action and cue position. Decoding was significant for all control variables and all task epochs (p&lt;0.005, Wilcoxon test).</p><p><supplementary-material id="fig6sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.023</object-id><label>Figure 6—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig6-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig6-v1.tif"/></fig><p>Taken together, unselected population activity carried accurate codes for object risk and action risk. These neuronal population codes depended on population size and individual neurons’ risk sensitivities.</p></sec><sec id="s2-9"><title>Dynamic integration of risk with reward history, value and choice in single neurons</title><p>Neurons often signaled object risk irrespective of other factors. However, over the course of a trial, many neurons dynamically integrated risk with behaviorally important variables in specific ways that were predicted on theoretical grounds. We assessed these coding dynamics with a sliding-window regression (<xref ref-type="disp-formula" rid="equ11">Equation 11</xref>), which also served to confirm the robustness of our main fixed-window analysis reported above.</p><p>Our main, subjective risk measure derived from the history of recently received rewards (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>). Following this concept, neurons often combined object-risk signals with information about rewards or choices from previous trials (‘history’ variables). Early on in trials, the neuron in <xref ref-type="fig" rid="fig7">Figure 7A</xref> signaled whether or not reward had been received on the last trial, following the choice of a particular object. This signal was immediately followed by an explicit object-risk signal, reflecting the updated, current-trial risk level given the outcome of the preceding trial. In total, 44 neurons showed such joint coding of reward-choice history variables and explicit object risk (54% of 82 risk-coding neurons from sliding-window regression, 21% of 205 recorded neurons; <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>; <xref ref-type="fig" rid="fig7">Figure 7B</xref>). By dynamically coding information about recent rewards alongside explicit object-risk signals, these DLPFC neurons seemed suited to contribute to internal risk calculation from experience.</p><fig-group><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.44838.024</object-id><label>Figure 7.</label><caption><title>Prefrontal neurons dynamically code risk with other behaviorally important variables.</title><p>(<bold>A</bold>) Neuronal reward history-to-risk transition. A single DLPFC neuron with fixation-period activity that initially reflected whether reward was received from a particular choice on the last trial (‘Last reward × choice’) before reflecting current-trial object risk. Coefficients of partial determination (partial R<sup>2</sup>) obtained from sliding-window multiple regression analysis. The observed single-neuron transition from recent reward history to current object risk is consistent with the behavioral model in <xref ref-type="fig" rid="fig3">Figure 3C</xref> which constructs and updates object-risk estimates from reward experience. (<bold>B</bold>) Numbers of neurons with joint and separate coding of object risk and history variables that were relevant for risk updating (including last-trial reward, last-trial choice, last-trial reward × last trial choice). Numbers derived from sliding window analyses focused on early trial periods relevant to decision-making (trial start until 500 ms post-cue). (<bold>C</bold>) Neuronal value-to-risk integration. A single DLPFC neuron with fixation-period activity encoding both object risk and object value, compatible with the notion of integrating risk and object value into economic utility. (<bold>D</bold>) Number of neurons with joint and separate coding of risk and value. (<bold>E</bold>) Neuronal risk-to-choice transition. A single DLPFC neuron with activity encoding object risk before coding object choice, consistent with decision-making informed by risk. (<bold>F</bold>) Numbers of neurons with joint and separate coding of object risk and object choice. (<bold>G</bold>) Cumulative coding latencies of history variables, object risk, object value, and object choice. Latencies derived from sliding-window regression (first sliding window for which criterion for statistical significance was achieved, see Materials and methods; cumulative proportion of significant neurons normalized to maximum value for each variable). (<bold>H</bold>) Regression coefficients for neurons with joint risk coding and choice coding (left) and joint risk coding and value coding (right). (<bold>I</bold>) Proportion of neurons with joint coding and pure coding of specific task-related variables (left) and proportion of neurons coding different numbers of additional variables (right).</p><p><supplementary-material id="fig7sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.029</object-id><label>Figure 7—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig7-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44838.025</object-id><label>Figure 7—figure supplement 1.</label><caption><title>Coding of risk and value jointly with spatial variables.</title><p>(<bold>A</bold>) Numbers of neurons with joint and separate coding of object risk and left-right cue position. Numbers derived from sliding window analyses (<xref ref-type="disp-formula" rid="equ11">Equation 11</xref>). (<bold>B</bold>) Numbers of neurons with joint and separate coding of object value and left-right cue position. (<bold>C</bold>) Numbers of neurons with joint and separate coding of object risk and left-right action. (<bold>D</bold>) Numbers of neurons with joint and separate coding of object value and left-right action. (<bold>E</bold>) Single neuron transitioning from pre-cue object-risk coding to cue-position coding. (<bold>F</bold>) Single neuron transitioning from pre-cue object-risk coding to action coding. (<bold>G</bold>) Single neuron transitioning from pre-cue object-value coding to action coding.</p><p><supplementary-material id="fig7s1sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.026</object-id><label>Figure 7—figure supplement 1—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig7-figsupp1-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.44838.027</object-id><label>Figure 7—figure supplement 2.</label><caption><title>Utility control.</title><p>(<bold>A</bold>) A single DLPFC neuron encoding the utility associated with object A at the time of choice (cue period). Utility was defined as the weighted linear combination of object value (based on reward history) and object risk, with weighting coefficients derived from individual animals’ choices (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). (<bold>B</bold>) In a multiple regression, utility for object A explained a significant proportion of variance in impulse rate (p=0.025, t-test). (<bold>C</bold>) Categorization of coding utility for objects A or B, utility difference or utility sum based on the angle of coefficients. Red circle: neuron from A and B. (<bold>D</bold>) Percentages of utility-coding neuronal responses across all task epochs (based on multiple regression), calculated with respect to 1222 task-related responses from 205 neurons. (<bold>E</bold>) Population decoding of utility from unselected neurons. Decoding accuracy (% correct classification) for utility based on a linear support-vector-machine classifier (N = 161 neurons). Leave-one-out cross-validated decoding for utility was significantly above chance (gray line, decoding from shuffled data) in all task epochs (p&lt;0.0001; Wilcoxon test). (<bold>F</bold>) Decoding performance increased with the number of neurons, late fixation period. Data for each neuron number show mean (± s.e.m) over 100 iterations of randomly selected neurons. (<bold>G</bold>) Relationship between neuronal utility decoding and individual neuron’s utility sensitivities. Linear regression of decoding performance from 5000 subsets of 20 randomly selected neurons on average utility sensitivity (single-neuron utility regression slope).</p><p><supplementary-material id="fig7s2sdata1"><object-id pub-id-type="doi">10.7554/eLife.44838.028</object-id><label>Figure 7—figure supplement 2—source data 1.</label><caption/><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-44838-fig7-figsupp2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-44838-fig7-figsupp2-v1.tif"/></fig></fig-group><p>According to the mean-variance approach of finance theory (<xref ref-type="bibr" rid="bib8">D'Acremont and Bossaerts, 2008</xref>; <xref ref-type="bibr" rid="bib35">Markowitz, 1952</xref>), the integration of expected value and risk into utility is thought to underlie behavioral preferences. In agreement with this basic concept, some DLPFC neurons dynamically combined object-risk signals with object-value signals (34 neurons, 41% of 82 risk-coding neurons, 17% of 205 neurons; <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>; <xref ref-type="fig" rid="fig7">Figure 7C,D</xref>). The neuron in <xref ref-type="fig" rid="fig7">Figure 7C</xref> showed overlapping object-value and object-risk signals early in trials during the fixation period, in time to inform object choice on the current trial. This result is potentially consistent with the integration of risk and value into utility. Supplementary analyses (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>, described below) provided further evidence that some individual neurons integrated risk and value into utility-like signals (although formal confirmation of utility coding would require additional behavioral testing).</p><p>If neuronal risk signals in DLPFC contributed to decisions, the activity of individual neurons might reflect the forward information flow predicted by computational decisions models (<xref ref-type="bibr" rid="bib9">Deco et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Grabenhorst et al., 2019</xref>; <xref ref-type="bibr" rid="bib68">Wang, 2008</xref>), whereby reward and risk evaluations precede choice. Thus, object risk as an important decision variable and the resulting, subsequent object choice should be jointly represented by neurons during decision-making. Indeed, activity in some DLPFC neurons dynamically combined object risk with the choice the animal was going to make (29 neurons, 35% of 82 risk-coding neurons, 14% of 205 recorded neurons; <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>; <xref ref-type="fig" rid="fig7">Figure 7E,F</xref>). At the time of choice, the neuron in <xref ref-type="fig" rid="fig7">Figure 7E</xref> signaled the risk of a specific object moments before it signaled the object choice for that trial, consistent with theoretically predicted transformations of risk and value signals into choice. Comparing the coding latencies for different variables across the population of DLPFC neurons, signals for reward history, object risk and object value arose significantly earlier than choice signals (p&lt;0.0001, rank-sum tests; <xref ref-type="fig" rid="fig7">Figure 7G</xref>).</p><p>The percentages of neurons coding specific pairs of variables was not significantly different than expected given the probabilities of neurons coding each individual variable (history and risk: χ2 = 1.58, p=0.2094, value and risk: χ2 = 3.54, p=0.0599, choice and risk: χ2 = 0.845, p=0.358). We also tested for relationships in the coding scheme (measured by signed regression coefficients) among neurons with joint risk and choice coding or joint risk and value coding. Across neurons, there was no significant relationship between the regression coefficients (standardized slopes) for the different variables (<xref ref-type="fig" rid="fig7">Figure 7H</xref>). This suggested that while some neurons used corresponding coding schemes for these variables (risk and choice, risk and value) other neurons used opposing coding schemes (see Discussion).</p><p>Overall, the majority of DLPFC neurons coded task-related variables in combination with other variables (<xref ref-type="fig" rid="fig7">Figure 7I</xref>). Thus, ‘pure’ coding of any given variable, including object risk, was rare in DLPFC and many neurons dynamically combined these signals with one or more additional variables (z-tests for dependent samples comparing proportion of joint and pure coding: p&lt;1.6 × 10<sup>−13</sup> for all variables in <xref ref-type="fig" rid="fig7">Figure 7I</xref>). In addition to the risk-related dynamic coding transitions described above, activity in some DLPFC neurons transitioned from coding risk to coding of spatial variables such as cue position or action choice (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>).</p><p>Thus, in addition to pure risk coding, DLPFC neurons frequently combined object risk with other reward and decision parameters on individual trials. These neurons provide a suitable basis for internal risk calculation and for the influence of risk on economic choices.</p></sec><sec id="s2-10"><title>Utility control</title><p>According to approaches in finance theory, risk is integrated with expected value into utility (<xref ref-type="bibr" rid="bib8">D'Acremont and Bossaerts, 2008</xref>; <xref ref-type="bibr" rid="bib35">Markowitz, 1952</xref>). We tested whether neuronal risk responses were accounted for by subjective integration of value (derived from the mean of the recent reward and choice histories) and risk (derived from the variance of the recent reward history). We calculated this mean-variance utility as a weighted sum of object value and risk with the weights for value and risk derived from logistic regression (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>).</p><p>The neuron in <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2A</xref> reflected the utility of object A (p=0.025; <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>, with object-utility regressors substituting object-value regressors); it failed to reflect utility of object B, object risk, cue position or action (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2B</xref>, all p&gt;0.075) but reflected in addition the object choice (p=0.0029). Multiple regression identified such responses reflecting the utility of individual objects (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2C,D</xref>, <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>). Specifically, 109 responses (97 neurons) were classified as coding utility (based on the angle of utility coefficients, <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>). Population decoding accuracy for utility was significant across task periods (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2E</xref>). As for risk, decoding accuracy increased with more neurons and dependent on individual neurons’ sensitivities (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2F,G</xref>).</p><p>Neuronal coding of utility did not account for our main finding of risk-coding. Regression with utility and risk regressors (in addition to object choice, cue position and action regressors) showed 108 neurons with significant risk responses (52.7% of neurons). Among them, 34 neurons were significant for risk but not utility. A stepwise regression resulted in 222 risk responses (of 1222 task-related responses, 18%) and 186 utility responses (15.2%). Thus, risk-related signals were not accounted for by utility.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>These data suggest that neurons in primate DLPFC signal the variance of fluctuating rewards derived from recent experience. Neuronal risk signals correlated with the subjective risk estimated from the animal’s choices and with experimentally programmed, objective risk. The frequently changing risk levels in the choice task required the animal to derive risk from the statistics of experienced rewards, rather than from cues that explicitly signaled risk levels (such as pre-trained risk-associated bar stimuli or fractals), in order to make choices. The variance-tracking neurons encoded risk information explicitly, and distinctly, as object risk and action risk; these risk signals were specific for particular objects or actions, occurred before the animal’s choice, and typically showed little dependence on the object or action being chosen, thus complying with criteria for decision inputs (<xref ref-type="bibr" rid="bib61">Sutton and Barto, 1998</xref>). Some neurons dynamically combined risk with information about past rewards, current object values or future choices, and showed transitions between these codes within individual trials; these characteristics are consistent with theoretical predictions of decision models (<xref ref-type="bibr" rid="bib9">Deco et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Grabenhorst et al., 2019</xref>; <xref ref-type="bibr" rid="bib68">Wang, 2008</xref>) and our model of subjective risk estimation from recent experience (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). These prefrontal risk signals seem to provide important information about dynamically evolving risk estimates as crucial components of economic decisions under uncertainty.</p><p>Risk is an abstract variable that is not readily sensed by decision-makers but requires construction from experience or description (<xref ref-type="bibr" rid="bib20">Hertwig et al., 2004</xref>). Reward probabilities in our choice task varied continually, encouraging the animals to alternately choose different objects and actions. Under such conditions, reward value and risk for objects and actions vary naturally with the animals’ behavior. We followed previous studies that estimated subjective reward values, rather than objective physical values, as explanatory variables for behavior and neurons (<xref ref-type="bibr" rid="bib32">Lau and Glimcher, 2005</xref>; <xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib51">Samejima et al., 2005</xref>; <xref ref-type="bibr" rid="bib60">Sugrue et al., 2004</xref>). To estimate subjective risk, we adapted an established approach for reward-value estimation based on the integration of rewards over a limited window of recent experiences (<xref ref-type="bibr" rid="bib32">Lau and Glimcher, 2005</xref>). We used this approach to calculate subjective risk from the variance of recent reward experiences and showed that the animals’ choices (<xref ref-type="fig" rid="fig3">Figure 3D–F</xref>) and prefrontal neurons (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>) were sensitive to these continually evolving subjective risk estimates, irrespective of value.</p><p>We could detect risk neurons also with an objective risk measure derived from true reward probability. However, our subjective risk measure took into account that the animals likely had imperfect knowledge of true reward probabilities and imperfect memory for past rewards. Accordingly, the observed neuronal relationships to objective risk are unlikely to reflect perfect tracking of the environment by the neurons; rather, the correlation with objective risk is likely explained by the fact that objective and subjective risk were related. This situation is similar to previous studies that compared coding of objective and subjective values (<xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib51">Samejima et al., 2005</xref>).</p><p>Our definition of subjective risk has some limitations. To facilitate comparisons with previous studies, we restricted our definition to the variance of past reward outcomes; we did not extend the risk measure to the variance of past choices, which would not have a clear correspondence in the economic or neuroeconomic literature. Our subjective risk measure provided a reasonable account of behavioral and neuronal data: it had a distinct relationship to choices, was encoded by a substantial number of neurons, and tended to better explain neuronal responses compared to objective risk. We followed the common approach of calculating reward statistics over a fixed temporal window because of its generality and simplicity, and to link our data to previous studies. An extension of this approach could introduce calculation of reward statistics over flexible time windows, possibly depending on the number of times an option was recently chosen (similar to an adaptive learning rate in reinforcement learning models).</p><p>Many neurons encoded risk for specific objects or actions prior to choice and independently of choice. Such pure risk signals could provide inputs for competitive, winner-take-all decision mechanisms that operate on separate inputs for different options (<xref ref-type="bibr" rid="bib9">Deco et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Grabenhorst et al., 2019</xref>; <xref ref-type="bibr" rid="bib68">Wang, 2008</xref>). The presence of DLPFC neurons that transitioned from risk-coding to choice-coding (<xref ref-type="fig" rid="fig7">Figure 7E</xref>) is consistent with this interpretation. In addition to providing pure risk inputs to decision-making, object-risk signals could converge with separately coded value signals to inform utility calculations, which require integration of risk with expected value according to individual risk attitude (<xref ref-type="bibr" rid="bib8">D'Acremont and Bossaerts, 2008</xref>; <xref ref-type="bibr" rid="bib35">Markowitz, 1952</xref>). This possibility is supported by DLPFC neurons that dynamically encoded both risk and value for specific choice objects (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). However, confirmation that DLPFC neurons encode utility will require further experimental testing with formal utility curves (<xref ref-type="bibr" rid="bib16">Genest et al., 2016</xref>; <xref ref-type="bibr" rid="bib57">Stauffer et al., 2014</xref>). Thus, risk neurons in DLFPC seem well suited to contribute to economic decisions, either directly by providing risk-specific decision inputs or indirectly by informing utility calculations.</p><p>Notably, while some DLPFC neurons jointly coded risk with value or choice in a common coding scheme (indicated by regression coefficients of equal sign), this was not the rule across all neurons with joint coding (<xref ref-type="fig" rid="fig7">Figure 7H</xref>). This result and the observed high degree of joint coding, with most DLPFC dynamically coding several task-related variables (<xref ref-type="fig" rid="fig7">Figure 7I</xref>), matches well with previous reports that neurons in DLPFC show heterogeneous coding and mixed selectivity (<xref ref-type="bibr" rid="bib49">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib66">Wallis and Kennerley, 2010</xref>). An implication for the present study might be that risk signals in DLPFC can support multiple cognitive processes in addition to decision-making, as also suggested by the observed relationship between risk and reaction times (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>Objects (or goods) represent the fundamental unit of choice in economics (<xref ref-type="bibr" rid="bib44">Padoa-Schioppa, 2011</xref>; <xref ref-type="bibr" rid="bib52">Schultz, 2015</xref>), whereas reinforcement learning in machine learning conceptualizes choice in terms of actions (<xref ref-type="bibr" rid="bib61">Sutton and Barto, 1998</xref>). The presently observed neuronal separation of object risk and action risk is analogous to neuronal separation of value signals for objects (<xref ref-type="bibr" rid="bib18">Grabenhorst et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Padoa-Schioppa, 2011</xref>; <xref ref-type="bibr" rid="bib55">So and Stuphorn, 2010</xref>; <xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>) and actions (<xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib51">Samejima et al., 2005</xref>; <xref ref-type="bibr" rid="bib54">Seo et al., 2012</xref>). Accordingly, object-risk and action-risk signals could provide inputs to separate mechanisms contributing to the selection of competing objects and actions. Additionally observed neuronal signals related to the sum or difference of object risk could contribute separately to decision-making and motivation processes, similar to previously observed neuronal coding of value sum and value difference (<xref ref-type="bibr" rid="bib5">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Wang et al., 2013</xref>).</p><p>Previous studies reported risk-sensitive neurons in dopaminergic midbrain (<xref ref-type="bibr" rid="bib14">Fiorillo et al., 2003</xref>; <xref ref-type="bibr" rid="bib31">Lak et al., 2014</xref>; <xref ref-type="bibr" rid="bib57">Stauffer et al., 2014</xref>), orbitofrontal cortex (<xref ref-type="bibr" rid="bib42">O'Neill and Schultz, 2010</xref>; <xref ref-type="bibr" rid="bib43">O'Neill and Schultz, 2013</xref>; <xref ref-type="bibr" rid="bib48">Raghuraman and Padoa-Schioppa, 2014</xref>), cingulate cortex (<xref ref-type="bibr" rid="bib37">McCoy and Platt, 2005</xref>; <xref ref-type="bibr" rid="bib39">Monosov, 2017</xref>), basal forebrain (<xref ref-type="bibr" rid="bib34">Ledbetter et al., 2016</xref>; <xref ref-type="bibr" rid="bib40">Monosov and Hikosaka, 2013</xref>), and striatum (<xref ref-type="bibr" rid="bib72">White and Monosov, 2016</xref>). In one study, orbitofrontal neurons encoded ‘offer risk’ for specific juice types (<xref ref-type="bibr" rid="bib48">Raghuraman and Padoa-Schioppa, 2014</xref>), analogous to the presently observed risk signals for specific visual objects. Critically, most previous studies used explicit risk-descriptive cues indicating fixed risk levels and tested neuronal activity when the animals had already learned cue-associated risk levels. One series of studies (<xref ref-type="bibr" rid="bib40">Monosov and Hikosaka, 2013</xref>; <xref ref-type="bibr" rid="bib72">White and Monosov, 2016</xref>) documented how risk responses evolved for novel cues with fixed risk levels, although relations to statistical variance of reward history were not examined. Here, we examined how neuronal risk estimates are derived internally and continually updated based on distinct reward experiences.</p><p>How could variance estimates be computed neurally? In neurophysiological models, reward signals modify synaptic strengths of valuation neurons, with decaying influences for more temporally remote rewards (<xref ref-type="bibr" rid="bib68">Wang, 2008</xref>). Variance-risk could be derived from such neurons by a mechanism that registers deviations of reward-outcomes from synaptically stored mean values. This process may involve risk-sensitive prediction error signals in dopamine neurons (<xref ref-type="bibr" rid="bib31">Lak et al., 2014</xref>; <xref ref-type="bibr" rid="bib57">Stauffer et al., 2014</xref>), orbitofrontal cortex (<xref ref-type="bibr" rid="bib43">O'Neill and Schultz, 2013</xref>) and insula (<xref ref-type="bibr" rid="bib46">Preuschoff et al., 2008</xref>). Our data cannot determine whether DLPFC neurons perform such variance computation or whether the observed risk signals reflected processing elsewhere; resolving this question will require simultaneous recordings from multiple structures. Nevertheless, a role of DLPFC neurons in local risk computation would be consistent with known prefrontal involvement in temporal reward integration (<xref ref-type="bibr" rid="bib1">Barraclough et al., 2004</xref>; <xref ref-type="bibr" rid="bib53">Seo et al., 2007</xref>), including recently described integration dependent on volatility (<xref ref-type="bibr" rid="bib36">Massi et al., 2018</xref>), processing of numerical quantities and mathematical rules (<xref ref-type="bibr" rid="bib3">Bongard and Nieder, 2010</xref>; <xref ref-type="bibr" rid="bib41">Nieder, 2013</xref>), and the currently observed transitions from past reward coding to object risk coding (<xref ref-type="fig" rid="fig7">Figure 7A,B</xref>). Moreover, in one recent study, reward information from past trials enhanced the encoding of current-trial task-relevant information in DLPFC neurons (<xref ref-type="bibr" rid="bib10">Donahue and Lee, 2015</xref>).</p><p>The prefrontal cortex has long been implicated in adaptive behavior (<xref ref-type="bibr" rid="bib38">Miller and Cohen, 2001</xref>), although economic risk processing is often associated with its orbital part, rather than the dorsolateral region studied here (<xref ref-type="bibr" rid="bib42">O'Neill and Schultz, 2010</xref>; <xref ref-type="bibr" rid="bib44">Padoa-Schioppa, 2011</xref>; <xref ref-type="bibr" rid="bib59">Stolyarova and Izquierdo, 2017</xref>). However, DLPFC neurons are well suited to signal object risk and action risk based on recent reward variance: DLPFC neurons process numerical quantities and basic mathematical rules (<xref ref-type="bibr" rid="bib3">Bongard and Nieder, 2010</xref>; <xref ref-type="bibr" rid="bib41">Nieder, 2013</xref>), integrate reward information over time (<xref ref-type="bibr" rid="bib1">Barraclough et al., 2004</xref>; <xref ref-type="bibr" rid="bib10">Donahue and Lee, 2015</xref>; <xref ref-type="bibr" rid="bib36">Massi et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Seo et al., 2007</xref>), and process both visual objects and actions (<xref ref-type="bibr" rid="bib15">Funahashi, 2013</xref>; <xref ref-type="bibr" rid="bib62">Suzuki and Gottlieb, 2013</xref>; <xref ref-type="bibr" rid="bib70">Watanabe, 1996</xref>). Previous studies implicated DLPFC neurons in reward valuation during decision-making (<xref ref-type="bibr" rid="bib6">Cai and Padoa-Schioppa, 2014</xref>; <xref ref-type="bibr" rid="bib23">Hosokawa et al., 2013</xref>; <xref ref-type="bibr" rid="bib27">Kennerley et al., 2009</xref>; <xref ref-type="bibr" rid="bib29">Kim and Shadlen, 1999</xref>; <xref ref-type="bibr" rid="bib67">Wallis and Miller, 2003</xref>). We recently showed that DLPFC neurons encoded object-specific values and their conversion to choices (<xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>). A previous imaging study detected a risk-dependent reward value signal in human lateral prefrontal cortex but no separate, value-independent risk signal (<xref ref-type="bibr" rid="bib64">Tobler et al., 2009</xref>), perhaps due to insufficient spatiotemporal resolution. Importantly, the presently described risk signals were not explained by value, which we controlled for in our regressions. Thus, the presently observed DLPFC risk neurons may contribute to economic decisions beyond separately coded value signals.</p><p>Activity in DLPFC has been implicated in attention (<xref ref-type="bibr" rid="bib12">Everling et al., 2002</xref>; <xref ref-type="bibr" rid="bib56">Squire et al., 2013</xref>; <xref ref-type="bibr" rid="bib62">Suzuki and Gottlieb, 2013</xref>) and the presently observed risk signals may contribute to DLPFC’s object and spatial attentional functions (<xref ref-type="bibr" rid="bib38">Miller and Cohen, 2001</xref>; <xref ref-type="bibr" rid="bib70">Watanabe, 1996</xref>). However, several observations argue against an interpretation of our results solely in terms of attention. First, DLPFC neurons encoded risk with both positive and negative slopes, suggesting no simple relationship to attention, which is usually associated with activity increases (<xref ref-type="bibr" rid="bib2">Beck and Kastner, 2009</xref>; <xref ref-type="bibr" rid="bib22">Hopfinger et al., 2000</xref>; <xref ref-type="bibr" rid="bib56">Squire et al., 2013</xref>). Second, in many neurons, risk signals were dynamically combined with signals related to other task-relevant variables, suggesting specific functions in risk updating and decision-making. Thus, the present neuronal risk signals may contribute to established DLPFC functions in attention (<xref ref-type="bibr" rid="bib12">Everling et al., 2002</xref>; <xref ref-type="bibr" rid="bib56">Squire et al., 2013</xref>; <xref ref-type="bibr" rid="bib62">Suzuki and Gottlieb, 2013</xref>) but also seem to play distinct, more specific roles in decision processes.</p><p>Our findings are consistent with a potential role for DLPFC neurons in signaling economic value and risk, and in converting these signals to choices. This general notion is supported by an earlier study implicating DLPFC in conversions from sensory evidence to oculomotor acts (<xref ref-type="bibr" rid="bib29">Kim and Shadlen, 1999</xref>). However, the choices may not be computed in DLPFC. Indeed, previous studies showed that value signals occur late in DLPFC, at least following those in orbitofrontal cortex (<xref ref-type="bibr" rid="bib27">Kennerley et al., 2009</xref>; <xref ref-type="bibr" rid="bib67">Wallis and Miller, 2003</xref>). A recent study using explicitly cued juice rewards demonstrated conversions from chosen juice signals to action signals in DLPFC but apparently few DLPFC neurons encoded the value inputs to these choices (<xref ref-type="bibr" rid="bib6">Cai and Padoa-Schioppa, 2014</xref>). By contrast, risk in our task was derived from integrated reward history, to which DLPFC neurons are sensitive (<xref ref-type="bibr" rid="bib1">Barraclough et al., 2004</xref>; <xref ref-type="bibr" rid="bib36">Massi et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Seo et al., 2007</xref>). It is possible that DLPFC’s involvement in converting decision parameters (including object risk as shown here) to choice signals depends on task requirements. This interpretation is supported by a recent study which showed that the temporal evolution of decision signals in DLPFC differs between delay-based and effort-based choices, and that orbitofrontal and anterior cingulate cortex differentially influence DLPFC decision signals for these different choice types (<xref ref-type="bibr" rid="bib26">Hunt et al., 2015</xref>).</p><p>In summary, these results show that prefrontal neurons tracked the evolving statistical variance of recent rewards that resulted from specific object choices and actions. Such variance-sensing neurons in prefrontal cortex may provide a physiological basis for integrating discrete event experiences and converting them into representations of abstract quantities such as risk. By coding this quantity as object risk and action risk, these prefrontal neurons provide distinct and specific risk inputs to economic decision processes.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>All animal procedures conformed to US National Institutes of Health Guidelines and were approved by the Home Office of the United Kingdom (Home Office Project Licenses PPL 80/2416, PPL 70/8295, PPL 80/1958, PPL 80/1513). The work has been regulated, ethically reviewed and supervised by the following UK and University of Cambridge (UCam) institutions and individuals: UK Home Office, implementing the Animals (Scientific Procedures) Act 1986, Amendment Regulations 2012, and represented by the local UK Home Office Inspector; UK Animals in Science Committee; UCam Animal Welfare and Ethical Review Body (AWERB); UK National Centre for Replacement, Refinement and Reduction of Animal Experiments (NC3Rs); UCam Biomedical Service (UBS) Certificate Holder; UCam Welfare Officer; UCam Governance and Strategy Committee; UCam Named Veterinary Surgeon (NVS); UCam Named Animal Care and Welfare Officer (NACWO).</p><p>Two adult male macaque monkeys (<italic>Macaca mulatta</italic>) weighing 5.5–6.5 kg served for the experiments. The animals had no history of participation in previous experiments. The number of animals used and the number of neurons recorded for the experiment is typical for studies in this field of research; we did not perform explicit power analysis. A head holder and recording chamber were fixed to the skull under general anaesthesia and aseptic conditions. We used standard electrophysiological techniques for extracellular recordings from single neurons in the sulcus principalis area of the frontal cortex via stereotaxically oriented vertical tracks, as confirmed by histological reconstruction. After completion of data collection, recording sites were marked with small electrolytic lesions (15–20 µA, 20–60 s). The animals received an overdose of pentobarbital sodium (90 mg/kg iv) and were perfused with 4% paraformaldehyde in 0.1 M phosphate buffer through the left ventricle of the heart. Recording positions were reconstructed from 50-µm-thick, stereotaxically oriented coronal brain sections stained with cresyl violet.</p></sec><sec id="s4-2"><title>Behavioral task</title><p>The animals performed an oculomotor free-choice task involving choices between two visual objects to each of which reward was independently and stochastically assigned. Trials started with presentation of a red fixation spot (diameter: 0.6°) in the center of a computer monitor (viewing distance: 41 cm; <xref ref-type="fig" rid="fig1">Figure 1B</xref>). The animal was required to fixate the spot and contact a touch sensitive, immobile resting key. An infrared eye tracking system continuously monitored eye positions (ISCAN, Cambridge, MA). During the fixation period at 1.0–2.0 s after eye fixation and key touch, an alert cue covering the fixation spot appeared for 0.7–1.0 s. At 1.4–2.0 s following offset of the alert cue, two different visual fractal objects (A, B; square, 5° visual angle) appeared simultaneously as ocular choice targets on each side of the fixation spot at 10° lateral to the center of the monitor. Left and right positions of objects A and B alternated pseudorandomly across trials. The animal was required to make a saccadic eye movement to its target of choice within a time window of 0.25–0.75 s. A red peripheral fixation spot replaced the target after 1.0–2.0 s of target fixation. This fixation spot turned to green after 0.5–1.0 s, and the monkey released the touch key immediately after color change. Rewarded trials ended with a fixed quantity of 0.7 ml juice delivered immediately upon key release. A computer-controlled solenoid valve delivered juice reward from a spout in front of the animal's mouth. Unrewarded trials ended at key release and without further stimuli. The fixation requirements restricted eye movements from trial start to cue appearance and, following the animals’ saccade choice, from choice acquisition to reward delivery. This ensured that neuronal activity was minimally influenced by oculomotor activity, especially in our main periods of interest before cue appearance.</p><p>Reward probabilities of object A and B were independently calculated in every trial, depending on the numbers of consecutive unchosen trials (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula>with <italic>P</italic> as instantaneous reward probability, <italic>P<sub>0</sub></italic> as experimentally imposed, base probability setting, and <italic>n</italic> as the number of trials that the object had been consecutively unchosen. This equation describes the probabilistic reward schedule of the Matching Law (<xref ref-type="bibr" rid="bib19">Herrnstein, 1961</xref>) in defining how the likelihood of being rewarded on a target increased with the number of trials after the object was last chosen but stayed at base probability while the object was repeatedly chosen (irrespective of whether that choice was rewarded or not). Reward was thus probabilistically assigned to the object in every trial, and once a reward was assigned, it remained available until the associated object was chosen.</p><p>We varied the base reward probability in blocks of typically 50–150 trials (chosen randomly) without signaling these changes to the animal. We used different base probabilities from the range of p=0.05 to p=0.55, which we chose randomly for each block. The sum of base reward probabilities for objects A and B was held constant so that only relative reward probability varied. The trial-by-trial reward probabilities for objects A and B, calculated according to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> varied within the range of p=0.05 to p=0.99.</p></sec><sec id="s4-3"><title>Calculation of objective risk</title><p>A most basic, assumption-free definition of objective risk derives risk directly from the variance of the true, specifically set (i.e. programmed) reward probabilities that changed on each trial depending on the animal’s matching behavior (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). Accordingly, we used the conventional definition of variance to calculate risk in each trial from the programmed, binary probability distribution (Bernoulli distribution) that governed actual reward delivery in each trial (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo> <mml:mi/><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced> <mml:mi/><mml:mo>×</mml:mo> <mml:mi/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi> <mml:mi/></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>E</mml:mi><mml:mi>V</mml:mi><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>with <italic>p</italic> as the trial-specific probability derived from <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, <italic>m</italic> as reward magnitude (0.7 ml for reward, 0 for no-reward outcome), <italic>k</italic> as outcome (0 ml or set ml of reward) and <italic>EV</italic> as expected value (defined as the sum of probability-weighted reward amounts). In our task, reward magnitude on rewarded trials was held constant at 0.7 ml; the definition generalizes to situations with different magnitudes. With magnitude <italic>m</italic> held constant, variance risk follows an inverted-U function of probability (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The risk for objects A and B, calculated as variance according to <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> varied within the range of var = 0.0003 to var = 0.1225.</p></sec><sec id="s4-4"><title>Objective risk: analysis of neuronal data</title><p>We counted neuronal impulses in each neuron on correct trials relative to different task events with 500 ms time windows that were fixed across neurons: before fixation spot (Pre-fix, starting 500 ms before fixation onset), early fixation (Fix, following fixation onset), late fixation (Fix2, starting 500 ms after fixation spot onset), pre-cue (Pre-cue, starting 500 ms before cue onset), cue (Cue, following cue onset), post-fixation (Post-fix, following fixation offset), before cue offset (Pre-cue off, starting 500 ms before cue offset), after cue offset (Post-cue off, following cue offset), pre-outcome (Pre-outc, starting 500 ms before reinforcer delivery), outcome (Outc, starting at outcome delivery), late outcome (Outc2, starting 500 ms after outcome onset).</p><p>We first identified task-related responses in individual neurons and then used multiple regression analysis to test for different forms of risk-related activity while controlling for the most important behaviorally relevant covariates. We identified task-related responses by comparing activity to a control period (Pre-fix) using the Wilcoxon test (p&lt;0.005, Bonferroni-corrected for multiple comparisons). A neuron was included as task-related if its activity in at least one task period was significantly different to that in the control period. Because the Pre-fixation period served as control period we did not select for task-relatedness in this period and included all neurons with observed impulses in the analysis. We chose the pre-fixation period as control period because it was the earliest period at the start of a trial in which no sensory stimuli were presented. The additional use of a sliding-window regression approach for which no comparison with a control period was performed (see below) confirmed the results of the fixed window analysis that involved testing for task-relationship.</p><p>We used multiple regression analysis to assess relationships between neuronal activity and task-related variables. Statistical significance of regression coefficients was determined using t-test with p&lt;0.05 as criterion. Our analysis followed established approaches previously used to test for value coding in different brain structures (<xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib51">Samejima et al., 2005</xref>). All tests performed were two-sided.</p><p>Each neuronal response was tested with the following multiple regression model to identify responses related to objective, true risk derived from reward probability (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>A</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>A</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>ε</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>with <italic>y</italic> as trial-by-trial neuronal impulse rate, <italic>ObjectChoice</italic> as current-trial object choice (0 for A, one for B), <italic>CuePosition</italic> as current-trial spatial cue position (0 for object A on the left, one for object A on the right), <italic>Action</italic> as current-trial action (0 for left, one for right), <italic>TrueProbA</italic> as the true current-trial reward probability of object A calculated from <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, <italic>TrueProbB</italic> as the true current-trial reward probability of object B calculated from <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, <italic>TrueRiskA</italic> as the true current-trial risk of object A calculated from <italic>TrueProbA</italic> according to <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, <italic>TrueRiskB</italic> as the true current-trial risk of object B calculated from <italic>TrueProbB</italic> according to <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, <italic>β<sub>1</sub></italic> to <italic>β<sub>7</sub></italic> as corresponding regression coefficients, <italic>β<sub>0</sub></italic> as constant, <italic>ε</italic> as residual. A neuronal response was classified as coding object risk if it had a significant coefficient for <italic>TrueRiskA</italic> or <italic>TrueRiskB</italic>.</p><p>We used the same model to test for neuronal coding of objective action risk by substituting the object-risk regressors with action-specific risk regressors (defined by the left-right object arrangement on each trial; thus, if object A appeared on the left side on a given trial, the action L risk regressor would be determined by the object A risk regressor for that trial).</p><p>An alternative method for classification of neuronal risk responses used the angle of regression coefficients (<xref ref-type="bibr" rid="bib65">Tsutsui et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Wang et al., 2013</xref>). This classification method is ‘axis-invariant’ as it is independent on the axis choice for the regression model, that is whether the model includes separate variables for object risk or separate variables for risk sum and difference (<xref ref-type="bibr" rid="bib69">Wang et al., 2013</xref>). However, the regression in this form omits relevant variables coded by DLPFC neurons (choice, cue position, action); accordingly, we use this approach as additional confirmation for our main regression above. We fitted the following regression model (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>):<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mi>y</mml:mi><mml:mo>=</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>A</mml:mi> <mml:mi/><mml:mo>+</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>B</mml:mi><mml:mo>+</mml:mo> <mml:mi/><mml:mi>ε</mml:mi></mml:math></disp-formula></p><p>Using this method, a neuronal response was categorized as risk-related if it showed a significant overall model fit (p&lt;0.05, <italic>F</italic>-test), rather than testing the significance of individual regressors. For responses with significant overall model fit, we plotted the magnitude of the beta coefficients (slopes) of the two object-risk regressors on an x-y plane (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We followed a previous study (<xref ref-type="bibr" rid="bib69">Wang et al., 2013</xref>) and divided the coefficient space into eight equally spaced segments of 45° to categorize neuronal responses based on the polar angle. We classified responses as coding object risk (‘absolute risk’) if their coefficients fell in the segments pointing toward 0° or 180° (object risk A) or toward 90° or 270° (object risk B). We used an analogous procedure for action risk classification. We classified responses as coding risk difference if their coefficients fell in the segments pointing toward 135° or 315° and as coding risk sum if their coefficients fell in the segments pointing toward 45° or 225°.</p></sec><sec id="s4-5"><title>Logistic regression for defining the weight of past reward</title><p>The animal may have paid more attention to immediately past rewards compared to earlier, more remote rewards. To establish a potential subjective reward value weighing, we used a logistic regression to model subjective reward value from the animals’ past rewards and choices, similar to comparable previous behavioral and neurophysiological macaque studies (<xref ref-type="bibr" rid="bib7">Corrado et al., 2005</xref>; <xref ref-type="bibr" rid="bib32">Lau and Glimcher, 2005</xref>; <xref ref-type="bibr" rid="bib60">Sugrue et al., 2004</xref>). As our task involved tracking changing values of objects (one fractal image for each of the two choice options), we formulated the model in terms of object choices rather than action choices. We fitted a logistic regression to the animal’s trial-by-trial choice data to estimate beta coefficients for the recent history of received rewards and recently made choices. Note that in choice tasks such as the one used here, choices and reward outcomes depend not only on reward history but also on choice history (<xref ref-type="bibr" rid="bib32">Lau and Glimcher, 2005</xref>). Thus, rewards and choices were both included into the logistic regression to avoid an omitted variable bias and provide a more accurate estimation of the weighting coefficients. The resulting coefficients quantified the extent to which the animals based their choices on recently received rewards and recently made choices for a given option; thus, the coefficients effectively weighed past trials with respect to their importance for the animal’s behavior. We used the following logistic regression to determine the weighting coefficients for reward history (<inline-formula><mml:math id="inf1"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) and choice history (<inline-formula><mml:math id="inf2"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>):<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>=</mml:mo> <mml:mi/><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo> <mml:mi/></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub> <mml:mi/></mml:mrow></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> [or<inline-formula><mml:math id="inf4"><mml:msub><mml:mrow> <mml:mi/><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>] as the probability of choosing object A (or B) on the <italic>i</italic>th current trial, <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi> <mml:mi/><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math></inline-formula> as reward delivery after choice of object A [or B] on the <italic>i</italic>th trial, <italic>j</italic> is the past trial relative to the <italic>i</italic>th trial, <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi> <mml:mi/><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math></inline-formula> as choice of object A [or B] on the <italic>i</italic>th trial, <italic>N</italic> denoting the number of past trials included in the model (<italic>N</italic> = 10), and <inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> as bias term. Exploratory analysis had shown that the beta coefficients did not differ significantly from 0 for more than (N = 10) past trials. Thus, <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> modeled the dependence of the monkeys’ choices on recent rewards and recent choices for specific objects; by fitting the model we estimated the subjective weights (i.e. betas) that animals placed on recent rewards and choices. Thus, the crucial weighting coefficients reflecting the subjective influence of past rewards and choices were <inline-formula><mml:math id="inf8"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:math></inline-formula> As described below, the <inline-formula><mml:math id="inf10"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> coefficients were also used for calculating the variance that served as the subjective risk measure of our study. The logistic regression was estimated by fitting regressors to a binary indicator function (dummy variable), setting the variable to 0 for the choice of one object and to 1 for the choice of the alternative object, using a binomial distribution with logit link function. The coefficients for reward and choice history from this analysis are plotted in <xref ref-type="fig" rid="fig3">Figure 3A and B</xref> as reward and choice weights.</p></sec><sec id="s4-6"><title>Calculation of weighted, subjective value</title><p>To calculate the subjective value of each reward object from the animal’s experience, we used the weights estimated from the logistic regression (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). We followed previous studies of matching behavior (<xref ref-type="bibr" rid="bib32">Lau and Glimcher, 2005</xref>) that distinguished two influences on value: the history of recent rewards and the history of recent choices. The first object-value component related to reward history, <inline-formula><mml:math id="inf11"><mml:msubsup><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, can be estimated by the mean of subjectively weighted reward history over the past 10 trials (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>):<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:msubsup><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo> <mml:mi/><mml:mfrac><mml:mrow><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced> <mml:mi/></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula>with <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> again as reward delivery after choice of object A on the <italic>i</italic>th trial, <italic>j</italic> as the past trial relative to the <italic>i</italic>th trial, <italic>N</italic> the number of past trials included in the model (<italic>N</italic> = 10), <inline-formula><mml:math id="inf13"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> as regression coefficient for the weight of past rewards (estimated by <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>).</p><p>In tasks used to study matching behavior, such as the present one, it has been shown that choice history has an additional influence on behavior and that this influence can be estimated using logistic regression (<xref ref-type="bibr" rid="bib32">Lau and Glimcher, 2005</xref>). To account for this second object-value component related to choice history, we estimated a subjective measure of object value that incorporated both a dependence on weighted reward history and a dependence on weighted choice history (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:msubsup><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo> <mml:mi/><mml:mfrac><mml:mrow><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced> <mml:mi/></mml:mrow></mml:mrow> <mml:mi/></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula>with <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as reward delivery after choice of object A on the <italic>i</italic>th trial, <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi> <mml:mi/><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math></inline-formula> as choice of object A [or B] on the <italic>i</italic>th trial, <italic>j</italic> as the past trial relative to the <italic>i</italic>th trial, <italic>N</italic> the number of past trials included in the model (<italic>N</italic> = 10), <inline-formula><mml:math id="inf16"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> as regression coefficient for the weight of past rewards and <inline-formula><mml:math id="inf17"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> as regression coefficient for the weight of past choice (estimated by <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). This measure of subjective object value (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>) based on both weighted reward and choice history, <inline-formula><mml:math id="inf18"><mml:msubsup><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, constituted our main value measure for behavioral and neuronal analyses.</p></sec><sec id="s4-7"><title>Calculation of subjective risk</title><p>Our aim was to construct a subjective risk measure that derived risk from the variance of recent reward experiences and that incorporated the typically observed decreasing influence of past trials on the animal’s behavior. We used the following definition as our main measure of subjective object risk (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>with <inline-formula><mml:math id="inf19"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> representing the weighting coefficients for past rewards (derived from <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>), <inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as reward delivery after choice of object A, <italic>j</italic> as index for past trials relative to the current <italic>i</italic>th trial, and <italic>N</italic> as the number of past trials included in the model (<italic>N</italic> = 10); the term <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the mean reward over the last ten trials. Thus, the equation derives subjective object risk from the summed, subjectively weighted, squared deviation of reward amounts in the last ten trials from the mean reward over the last ten trials. By defining risk in this manner, we followed the common economic definition of risk as the mean squared deviation from expected outcome and in addition accounted for each animal’s subjective weighting of past trials. This definition (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>) constituted our main subjective object risk measure for behavioral and neuronal analyses.</p><p>Alternative, more complex definitions of subjective risk in our task could incorporate the weighting of past trials in the calculation of the mean reward (the subtrahend in the numerator of <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>) or incorporate both weighted reward history and weighted choice history in this calculation. We explore these possibilities in a supplementary analysis (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>). In our main risk definition, we also assumed that the animals used a common reward weighting function for value and risk. As described in the Results, for neuronal analysis we also explored alternative past-trial weighting functions for the risk calculation (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). The alternative weights identified similar although slightly lower numbers of risk neurons compared to those obtained with the weights defined by <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>.</p></sec><sec id="s4-8"><title>Testing the influence of subjective object risk on choices</title><p>For out-of-sample validation, we used one half of the behavioral data within each animal to derive weighting coefficients (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) and subsequently used the remaining half for testing the behavioral relevance of object-risk and object-value variables. To do so, we used logistic regression to relate each animal’s choices to the subjective object values and object variance-risks, according to the following equation (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>):<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>with <inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> [or<inline-formula><mml:math id="inf23"><mml:msub><mml:mrow> <mml:mi/><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>] as the probability of choosing left or right on the <italic>i</italic>th trial, <italic>ObjectValueLeft</italic> as current-trial value of the left object (derived from <inline-formula><mml:math id="inf24"><mml:msubsup><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>), <italic>ObjectValueRight</italic> as current-trial value of the right object (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>), <italic>ObjectRiskLeft</italic> as current-trial risk of the left object (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>), <italic>ObjectRiskRight</italic> as current-trial risk of the right object (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>), <italic>β<sub>1</sub></italic> to <italic>β<sub>2</sub></italic> as corresponding regression coefficients, <italic>β<sub>0</sub></italic> as constant, <italic>ε</italic> as residual. The resulting regression coefficients are shown in <xref ref-type="fig" rid="fig3">Figure 3E</xref>. Thus, object choice was modeled as a function of relative object value and relative object risk.</p><p>We compared this model to several alternative behavioral models using Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC; <xref ref-type="table" rid="table1">Table 1</xref>). The alternative models included variations of the above model (described in the legend to <xref ref-type="table" rid="table1">Table 1</xref>), a model based on objective (true) reward probabilities and risks, a standard reinforcement learning model that updated the object-value estimate of the chosen option based on the obtained outcome (<xref ref-type="bibr" rid="bib61">Sutton and Barto, 1998</xref>), a reinforcement learning model that updated object-value estimates of both chosen and unchosen option, and a modified reinforcement learning model that captured time-dependent increases in reward probability in tasks used to study matching behavior (<xref ref-type="bibr" rid="bib25">Huh et al., 2009</xref>).</p></sec><sec id="s4-9"><title>Subjective risk: analysis of neuronal data</title><sec id="s4-9-1"><title>Subjective risk for objects</title><p>Each neuronal response was tested with the following multiple regression model to identify responses related to subjective risk derived from weighted reward history (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>):<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>A</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>A</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>ε</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>with <italic>y</italic> as trial-by-trial neuronal impulse rate, <italic>ObjectChoice</italic> as current-trial object choice (0 for A, one for B), <italic>CuePosition</italic> as current-trial spatial cue position (0 for object A on the left, one for object A on the right), <italic>Action</italic> as current-trial action (0 for left, one for right), <italic>ObjectValueA</italic> as current-trial value of object A (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>), <italic>ObjectValueB</italic> as current-trial value of object B (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>), <italic>ObjectRiskA</italic> as current-trial risk of object A (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>), <italic>ObjectRiskB</italic> as current-trial risk of object B (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>), <italic>β<sub>1</sub></italic> to <italic>β<sub>7</sub></italic> as corresponding regression coefficients, <italic>β<sub>0</sub></italic> as constant, <italic>ε</italic> as residual. This equation differs from <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> as it replaced the regressors for true risk and true probability with regressors for subjective risk and subjective value. A neuronal response was classified coding object risk if it had a significant coefficient for <italic>ObjectRiskA</italic> or <italic>ObjectRiskB</italic>.</p></sec><sec id="s4-9-2"><title>Regression including last-trial history variables</title><p>Object risk was calculated based on reward received in previous trials. Accordingly, we used an additional regression to test whether these variables were directly encoded by DLPFC neurons, and whether object-risk responses were better explained in terms of these history variables. To test for encoding of object risk alongside explicit regressors for last-trial reward, last-trial choice, and last-trial choice × reward, we used the following regression model (<xref ref-type="disp-formula" rid="equ11">Equation 11</xref>):<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>A</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>A</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>ε</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To test whether inclusion of additional regressors for the past two trials affected our main results (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>), we used the following regression model (<xref ref-type="disp-formula" rid="equ12">Equation 12</xref>) that included regressors for rewards, choices, and their interactions on the last two trials:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>A</mml:mi><mml:mtext> </mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>A</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mtext> </mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mn>2</mml:mn><mml:mtext> </mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mn>2</mml:mn><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>ε</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-9-3"><title>Step-wise regression model for testing coding of objective and subjective object risk</title><p>We used this stepwise regression as an additional analysis to test for object-risk coding and action-risk coding when these variables directly competed to explain variance in neuronal responses. Note that the objective, true probabilities used for the analysis were not the base probabilities but the trial-specific probabilities that evolved trial-by-trial from the baseline probabilities according to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. The following variables were included as regressors in the starting set (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>):<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>A</mml:mi><mml:mtext> </mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>A</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>A</mml:mi><mml:mtext> </mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>A</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>ε</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-9-4"><title>Subjective risk for actions</title><p>To test for encoding of action risk, we used the following multiple regression model (<xref ref-type="disp-formula" rid="equ14">Equation 14</xref>):<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>L</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>ε</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>with <italic>y</italic> as trial-by-trial neuronal impulse rate, <italic>ActionValueL</italic> as current-trial value of a leftward saccade, <italic>ActionValueR</italic> as current-trial value of a rightward saccade, <italic>ActionRiskL</italic> as current-trial risk of a leftward saccade, <italic>ActionRiskR</italic> as current-trial risk of a rightward saccade (all other variables as defined above). Note that subjective action values and action risks were not simply spatially referenced object values and object risks but were estimated separately, based on object reward histories and action reward histories. Specifically, regressors for action value and action risk were estimated analogously to those for object value and object risk as described in <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> and <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>, based on filter weights derived from fitting the model in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> for action choice rather than object choice. Thus, for defining action risk, we calculated the variance of rewards that resulted from rightward and leftward saccades within the last ten trials, with coefficients from <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> (calculated for actions) determining how strongly each trial was weighted in the variance calculation. A neuronal response was classified as coding action risk if it had a significant regressor for <italic>ActionRiskL</italic> or <italic>ActionRiskR</italic>.</p></sec><sec id="s4-9-5"><title>Step-wise regression model for testing coding of object risk and action risk</title><p>We used this stepwise regression as an addition analysis to test for object-risk coding and action-risk coding when these variables directly competed to explain variance in neuronal responses. The following variables were included as regressors in the starting set (<xref ref-type="disp-formula" rid="equ15">Equation 15</xref>):<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>A</mml:mi><mml:mtext> </mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>A</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mtext> </mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>L</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>ε</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-9-6"><title>Sliding window regression analysis</title><p>We used additional sliding window multiple regression analyses (using the regression model in <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>) with a 200 ms window that we moved in steps of 25 ms across each trial. To determine whether neuronal activity was significantly related to a given variable we used a bootstrap approach based on shuffled data as follows. For each neuron, we performed the sliding window regression 1000 times on trial-shuffled data and determined a false positive rate by counting the number of consecutive windows in which a regression was significant with p&lt;0.05. We found that less than five per cent of neurons with trial-shuffled data showed more than six consecutive significant analysis windows. In other words, we used the shuffled data to obtain the percentage of neurons with at least one case of six consecutively significant windows. Therefore, we counted a sliding window analysis as significant if a neuron showed a significant (p&lt;0.05) effect for more than six consecutive windows.</p></sec><sec id="s4-9-7"><title>Normalization of population activity</title><p>We subtracted from the measured impulse rate in a given task period the mean impulse rate of the control period and divided by the standard deviation of the control period (z-score normalization). Next, we distinguished neurons that showed a positive relationship to object value and those with a negative relationship, based on the sign of the regression coefficient, and sign-corrected responses with a negative relationship. Normalized activity was used for all population decoding analyses and for <xref ref-type="fig" rid="fig4">Figure 4E,F</xref> and <xref ref-type="fig" rid="fig5">Figure 5E,F</xref>.</p></sec><sec id="s4-9-8"><title>Normalization of regression coefficients</title><p>Standardized regression coefficients were defined as xi(si/sy), xi being the raw slope coefficient for regressor i, and si and sy the standard deviations of independent variable i and the dependent variable, respectively. These coefficients were used for <xref ref-type="fig" rid="fig2">Figure 2B,C</xref>, <xref ref-type="fig" rid="fig3">Figure 3E</xref>, <xref ref-type="fig" rid="fig4">Figure 4B,C</xref>, <xref ref-type="fig" rid="fig5">Figure 5B,C</xref>, <xref ref-type="fig" rid="fig6">Figure 6C</xref>, <xref ref-type="fig" rid="fig7">Figure 7H</xref>, <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2B,C</xref>.</p></sec><sec id="s4-9-9"><title>Population decoding</title><p>We used support vector machine (SVM) and nearest-neighbor (NN) classifiers to quantify the information contained in DLPFC population activity in defined task periods. This method determines how accurately our main variables object risk and action risk were encoded by groups of DLPFC neurons. The SVM classifier was trained on a set of training data to find a linear hyperplane that provides the best separation between two patterns of neuronal population activity defined by a grouping variable (e.g. high vs. low object risk). Decoding was typically not improved by non-linear (e.g. quadratic) kernels. Both SVM and NN classification are biologically plausible in that a downstream neuron could perform similar classification by comparing the input on a given trial with a stored vector of synaptic weights. Both classifiers performed qualitatively similar, although SVM decoding was typically more accurate. We therefore focused our main results on SVM decoding.</p><p>We aggregated z-normalized trial-by-trial impulse rates of independently recorded DLPFC neurons from specific task periods into pseudo-populations. We used all recorded neurons that met inclusion criteria for a minimum trial number, without pre-selecting for risk coding, except where explicitly stated. For each decoding analysis, we created two <italic>n</italic> by <italic>m</italic> matrices with <italic>n</italic> columns defined by the number of neurons and <italic>m</italic> rows by the number of trials. We defined two matrices, one for each group for which decoding was performed (e.g. high vs. low object risk). Thus, each cell in a matrix contained the impulse rate from a single neuron on a single trial measured for a given group. Because neurons were not simultaneously recorded, we randomly matched up trials from different neurons for the same group and then repeated the decoding analysis with different random trial matching (within-group trial matching) 150 times for the SVM and 500 times for the NN. We found these numbers to produce very stable classification results. (We note that this approach likely provides a lower bound for decoding performance as it ignores potential contributions from cross-correlations between neurons; investigation of cross-correlations would require data from simultaneously recorded neurons.) We used a leave-one-out cross-validation procedure whereby a classifier was trained to learn the mapping from impulse rates to groups on all trials except one; the remaining trial was then used for testing the classifier and the procedure repeated until all trials had been tested. An alternative approach of using 80% trials as training data and testing on the remaining 20% produced highly similar results (<xref ref-type="bibr" rid="bib45">Pagan et al., 2013</xref>). We only included neurons in the decoding analyses that had a minimum number of eight trials per group for which decoding was performed. ‘Group’ referred to a trial category for which decoding was performed, such as low risk, high risk, A chosen, B chosen, etc. The minimum defined the lower cut-off in case a recording session contained few trials that belonged to a specific group as in the case of decoding based on risk terciles within each session, separately for object A and object B.</p><p>The SVM decoding was implemented in Matlab (Version R2013b, Mathworks, Natick, MA) using the ‘svmtrain’ and ‘svmclassify’ functions with a linear kernel and the default sequential minimal optimization method for finding the separating hyperplane. We quantified decoding accuracy as the percentage of correctly classified trials, averaged over all decoding analyses for different random within-group trial matchings. To investigate how decoding accuracy depends on population size, we randomly selected a given number of neurons at each step and then determined the percentage correct. For each step (i.e. each possible population size) this procedure was repeated 10 times. We also performed decoding for randomly shuffled data (shuffled group assignment without replacement) with 5000 iterations to test whether decoding on real data differed significantly from chance. Statistical significance (p&lt;0.0001) was determined by comparing vectors of percentage correct decoding accuracy between real data and randomly shuffled data using the rank sum test (<xref ref-type="bibr" rid="bib47">Quian Quiroga et al., 2006</xref>). For all analyses, decoding was performed on neuronal responses taken from the same task period. We trained classifiers to distinguish high from low risk terciles (decoding based on median split produced very similar results).</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Wellcome Trust (Principal Research Fellowship and Programme Grant 095495 to WS; Sir Henry Dale Fellowship 206207/Z/17/Z to FG), the European Research Council (ERC Advanced Grant 293549 to WS), and the National Institutes of Health (NIH) Caltech Conte Center (P50MH094258).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All animal procedures conformed to US National Institutes of Health Guidelines and were approved by the Home Office of the United Kingdom (Home Office Project Licenses PPL 80/2416, PPL 70/8295, PPL 80/1958, PPL 80/1513). The work has been regulated, ethically reviewed and supervised by the following UK and University of Cambridge (UCam) institutions and individuals: UK Home Office, implementing the Animals (Scientific Procedures) Act 1986, Amendment Regulations 2012, and represented by the local UK Home Office Inspector; UK Animals in Science Committee; UCam Animal Welfare and Ethical Review Body (AWERB); UK National Centre for Replacement, Refinement and Reduction of Animal Experiments (NC3Rs); UCam Biomedical Service (UBS) Certificate Holder; UCam Welfare Officer; UCam Governance and Strategy Committee; UCam Named Veterinary Surgeon (NVS); UCam Named Animal Care and Welfare Officer (NACWO).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.44838.030</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-44838-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Source data files have been provided for all figures.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barraclough</surname> <given-names>DJ</given-names></name><name><surname>Conroy</surname> <given-names>ML</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Prefrontal cortex and decision making in a mixed-strategy game</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>404</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1038/nn1209</pub-id><pub-id pub-id-type="pmid">15004564</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname> <given-names>DM</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Top-down and bottom-up mechanisms in biasing competition in the human brain</article-title><source>Vision Research</source><volume>49</volume><fpage>1154</fpage><lpage>1165</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.07.012</pub-id><pub-id pub-id-type="pmid">18694779</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bongard</surname> <given-names>S</given-names></name><name><surname>Nieder</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Basic mathematical rules are encoded by primate prefrontal cortex neurons</article-title><source>PNAS</source><volume>107</volume><fpage>2277</fpage><lpage>2282</lpage><pub-id pub-id-type="doi">10.1073/pnas.0909180107</pub-id><pub-id pub-id-type="pmid">20133872</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burke</surname> <given-names>CJ</given-names></name><name><surname>Tobler</surname> <given-names>PN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reward skewness coding in the insula independent of probability and loss</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>2415</fpage><lpage>2422</lpage><pub-id pub-id-type="doi">10.1152/jn.00471.2011</pub-id><pub-id pub-id-type="pmid">21849610</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname> <given-names>X</given-names></name><name><surname>Kim</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Heterogeneous coding of temporally discounted values in the dorsal and ventral striatum during intertemporal choice</article-title><source>Neuron</source><volume>69</volume><fpage>170</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.11.041</pub-id><pub-id pub-id-type="pmid">21220107</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname> <given-names>X</given-names></name><name><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Contributions of orbitofrontal and lateral prefrontal cortices to economic choice and the good-to-action transformation</article-title><source>Neuron</source><volume>81</volume><fpage>1140</fpage><lpage>1151</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.008</pub-id><pub-id pub-id-type="pmid">24529981</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corrado</surname> <given-names>GS</given-names></name><name><surname>Sugrue</surname> <given-names>LP</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Linear-Nonlinear-Poisson models of primate choice dynamics</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>84</volume><fpage>581</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.1901/jeab.2005.23-05</pub-id><pub-id pub-id-type="pmid">16596981</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D'Acremont</surname> <given-names>M</given-names></name><name><surname>Bossaerts</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neurobiological studies of risk assessment: a comparison of expected utility and mean-variance approaches</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>8</volume><fpage>363</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.3758/CABN.8.4.363</pub-id><pub-id pub-id-type="pmid">19033235</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deco</surname> <given-names>G</given-names></name><name><surname>Rolls</surname> <given-names>ET</given-names></name><name><surname>Albantakis</surname> <given-names>L</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain mechanisms for perceptual and reward-related decision-making</article-title><source>Progress in Neurobiology</source><volume>103</volume><fpage>194</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2012.01.010</pub-id><pub-id pub-id-type="pmid">22326926</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donahue</surname> <given-names>CH</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamic routing of task-relevant signals for decision making in dorsolateral prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>295</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1038/nn.3918</pub-id><pub-id pub-id-type="pmid">25581364</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elber-Dorozko</surname> <given-names>L</given-names></name><name><surname>Loewenstein</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Striatal action-value neurons reconsidered</article-title><source>eLife</source><volume>7</volume><elocation-id>e34248</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34248</pub-id><pub-id pub-id-type="pmid">29848442</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Everling</surname> <given-names>S</given-names></name><name><surname>Tinsley</surname> <given-names>CJ</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Filtering of neural signals by focused attention in the monkey prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>671</fpage><lpage>676</lpage><pub-id pub-id-type="doi">10.1038/nn874</pub-id><pub-id pub-id-type="pmid">12068302</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Everling</surname> <given-names>S</given-names></name><name><surname>Tinsley</surname> <given-names>CJ</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Selective representation of task-relevant objects and locations in the monkey prefrontal cortex</article-title><source>European Journal of Neuroscience</source><volume>23</volume><fpage>2197</fpage><lpage>2214</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2006.04736.x</pub-id><pub-id pub-id-type="pmid">16630066</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiorillo</surname> <given-names>CD</given-names></name><name><surname>Tobler</surname> <given-names>PN</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Discrete coding of reward probability and uncertainty by dopamine neurons</article-title><source>Science</source><volume>299</volume><fpage>1898</fpage><lpage>1902</lpage><pub-id pub-id-type="doi">10.1126/science.1077349</pub-id><pub-id pub-id-type="pmid">12649484</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funahashi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Space representation in the prefrontal cortex</article-title><source>Progress in Neurobiology</source><volume>103</volume><fpage>131</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2012.04.002</pub-id><pub-id pub-id-type="pmid">22521602</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Genest</surname> <given-names>W</given-names></name><name><surname>Stauffer</surname> <given-names>WR</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Utility functions predict variance and skewness risk preferences in monkeys</article-title><source>PNAS</source><volume>113</volume><fpage>8402</fpage><lpage>8407</lpage><pub-id pub-id-type="doi">10.1073/pnas.1602217113</pub-id><pub-id pub-id-type="pmid">27402743</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grabenhorst</surname> <given-names>F</given-names></name><name><surname>Hernádi</surname> <given-names>I</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Prediction of economic choice by primate amygdala neurons</article-title><source>PNAS</source><volume>109</volume><fpage>18950</fpage><lpage>18955</lpage><pub-id pub-id-type="doi">10.1073/pnas.1212706109</pub-id><pub-id pub-id-type="pmid">23112182</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grabenhorst</surname> <given-names>F</given-names></name><name><surname>Báez-Mendoza</surname> <given-names>R</given-names></name><name><surname>Genest</surname> <given-names>W</given-names></name><name><surname>Deco</surname> <given-names>G</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Primate amygdala neurons simulate decision processes of social partners</article-title><source>Cell</source><volume>177</volume><fpage>986</fpage><lpage>998</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.02.042</pub-id><pub-id pub-id-type="pmid">30982599</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrnstein</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Relative and absolute strength of response as a function of frequency of reinforcement</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>4</volume><fpage>267</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1901/jeab.1961.4-267</pub-id><pub-id pub-id-type="pmid">13713775</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertwig</surname> <given-names>R</given-names></name><name><surname>Barron</surname> <given-names>G</given-names></name><name><surname>Weber</surname> <given-names>EU</given-names></name><name><surname>Erev</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Decisions from experience and the effect of rare events in risky choice</article-title><source>Psychological Science</source><volume>15</volume><fpage>534</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2004.00715.x</pub-id><pub-id pub-id-type="pmid">15270998</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holt</surname> <given-names>CA</given-names></name><name><surname>Laury</surname> <given-names>SK</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Risk aversion and incentive effects</article-title><source>American Economic Review</source><volume>92</volume><fpage>1644</fpage><lpage>1655</lpage><pub-id pub-id-type="doi">10.1257/000282802762024700</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfinger</surname> <given-names>JB</given-names></name><name><surname>Buonocore</surname> <given-names>MH</given-names></name><name><surname>Mangun</surname> <given-names>GR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The neural mechanisms of top-down attentional control</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>284</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1038/72999</pub-id><pub-id pub-id-type="pmid">10700262</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosokawa</surname> <given-names>T</given-names></name><name><surname>Kennerley</surname> <given-names>SW</given-names></name><name><surname>Sloan</surname> <given-names>J</given-names></name><name><surname>Wallis</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Single-neuron mechanisms underlying cost-benefit analysis in frontal cortex</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>17385</fpage><lpage>17397</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2221-13.2013</pub-id><pub-id pub-id-type="pmid">24174671</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houston</surname> <given-names>AI</given-names></name><name><surname>McNamara</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>How to maximize reward rate on two variable-interval paradigms</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>35</volume><fpage>367</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1901/jeab.1981.35-367</pub-id><pub-id pub-id-type="pmid">16812223</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huh</surname> <given-names>N</given-names></name><name><surname>Jo</surname> <given-names>S</given-names></name><name><surname>Kim</surname> <given-names>H</given-names></name><name><surname>Sul</surname> <given-names>JH</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Model-based reinforcement learning under concurrent schedules of reinforcement in rodents</article-title><source>Learning &amp; Memory</source><volume>16</volume><fpage>315</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1101/lm.1295509</pub-id><pub-id pub-id-type="pmid">19403794</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname> <given-names>LT</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Hosokawa</surname> <given-names>T</given-names></name><name><surname>Wallis</surname> <given-names>JD</given-names></name><name><surname>Kennerley</surname> <given-names>SW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Capturing the temporal evolution of choice across prefrontal cortex</article-title><source>eLife</source><volume>4</volume><elocation-id>e11945</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.11945</pub-id><pub-id pub-id-type="pmid">26653139</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennerley</surname> <given-names>SW</given-names></name><name><surname>Dahmubed</surname> <given-names>AF</given-names></name><name><surname>Lara</surname> <given-names>AH</given-names></name><name><surname>Wallis</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neurons in the frontal lobe encode the value of multiple decision variables</article-title><source>Journal of Cognitive Neuroscience</source><volume>21</volume><fpage>1162</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21100</pub-id><pub-id pub-id-type="pmid">18752411</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khamassi</surname> <given-names>M</given-names></name><name><surname>Quilodran</surname> <given-names>R</given-names></name><name><surname>Enel</surname> <given-names>P</given-names></name><name><surname>Dominey</surname> <given-names>PF</given-names></name><name><surname>Procyk</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Behavioral regulation and the modulation of information coding in the lateral prefrontal and cingulate cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3197</fpage><lpage>3218</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu114</pub-id><pub-id pub-id-type="pmid">24904073</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>JN</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neural correlates of a decision in the dorsolateral prefrontal cortex of the macaque</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>176</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1038/5739</pub-id><pub-id pub-id-type="pmid">10195203</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kreps</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="1990">1990</year><source>A Course in Microeconomic Theory</source><publisher-loc>Essex</publisher-loc><publisher-name>Pearson Education Limited</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lak</surname> <given-names>A</given-names></name><name><surname>Stauffer</surname> <given-names>WR</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dopamine prediction error responses integrate subjective value from different reward dimensions</article-title><source>PNAS</source><volume>111</volume><fpage>2343</fpage><lpage>2348</lpage><pub-id pub-id-type="doi">10.1073/pnas.1321596111</pub-id><pub-id pub-id-type="pmid">24453218</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname> <given-names>B</given-names></name><name><surname>Glimcher</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Dynamic response-by-response models of matching behavior in rhesus monkeys</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>84</volume><fpage>555</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1901/jeab.2005.110-04</pub-id><pub-id pub-id-type="pmid">16596980</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname> <given-names>B</given-names></name><name><surname>Glimcher</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Value representations in the primate striatum during matching behavior</article-title><source>Neuron</source><volume>58</volume><fpage>451</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.02.021</pub-id><pub-id pub-id-type="pmid">18466754</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ledbetter</surname> <given-names>NM</given-names></name><name><surname>Chen</surname> <given-names>CD</given-names></name><name><surname>Monosov</surname> <given-names>IE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multiple mechanisms for processing reward uncertainty in the primate basal forebrain</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>7852</fpage><lpage>7864</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1123-16.2016</pub-id><pub-id pub-id-type="pmid">27466331</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>Portfolio selection</article-title><source>The Journal of Finance</source><volume>7</volume><fpage>77</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1111/j.1540-6261.1952.tb01525.x</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massi</surname> <given-names>B</given-names></name><name><surname>Donahue</surname> <given-names>CH</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Volatility facilitates value updating in the prefrontal cortex</article-title><source>Neuron</source><volume>99</volume><fpage>598</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.06.033</pub-id><pub-id pub-id-type="pmid">30033151</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCoy</surname> <given-names>AN</given-names></name><name><surname>Platt</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Risk-sensitive neurons in macaque posterior cingulate cortex</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1220</fpage><lpage>1227</lpage><pub-id pub-id-type="doi">10.1038/nn1523</pub-id><pub-id pub-id-type="pmid">16116449</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An integrative theory of prefrontal cortex function</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>167</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id><pub-id pub-id-type="pmid">11283309</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monosov</surname> <given-names>IE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Anterior cingulate is a source of valence-specific information about value and uncertainty</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>134</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00072-y</pub-id><pub-id pub-id-type="pmid">28747623</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monosov</surname> <given-names>IE</given-names></name><name><surname>Hikosaka</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Selective and graded coding of reward uncertainty by neurons in the primate anterodorsal septal region</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>756</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1038/nn.3398</pub-id><pub-id pub-id-type="pmid">23666181</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieder</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Coding of abstract quantity by ‘number neurons’ of the primate brain</article-title><source>Journal of Comparative Physiology A</source><volume>199</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1007/s00359-012-0763-9</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Neill</surname> <given-names>M</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Coding of reward risk by orbitofrontal neurons is mostly distinct from coding of reward value</article-title><source>Neuron</source><volume>68</volume><fpage>789</fpage><lpage>800</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.031</pub-id><pub-id pub-id-type="pmid">21092866</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Neill</surname> <given-names>M</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Risk prediction error coding in orbitofrontal neurons</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>15810</fpage><lpage>15814</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4236-12.2013</pub-id><pub-id pub-id-type="pmid">24089488</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neurobiology of economic choice: a good-based model</article-title><source>Annual Review of Neuroscience</source><volume>34</volume><fpage>333</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-061010-113648</pub-id><pub-id pub-id-type="pmid">21456961</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagan</surname> <given-names>M</given-names></name><name><surname>Urban</surname> <given-names>LS</given-names></name><name><surname>Wohl</surname> <given-names>MP</given-names></name><name><surname>Rust</surname> <given-names>NC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Signals in inferotemporal and perirhinal cortex suggest an untangling of visual target information</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1132</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1038/nn.3433</pub-id><pub-id pub-id-type="pmid">23792943</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preuschoff</surname> <given-names>K</given-names></name><name><surname>Quartz</surname> <given-names>SR</given-names></name><name><surname>Bossaerts</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Human insula activation reflects risk prediction errors as well as risk</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>2745</fpage><lpage>2752</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4286-07.2008</pub-id><pub-id pub-id-type="pmid">18337404</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quian Quiroga</surname> <given-names>R</given-names></name><name><surname>Snyder</surname> <given-names>LH</given-names></name><name><surname>Batista</surname> <given-names>AP</given-names></name><name><surname>Cui</surname> <given-names>H</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Movement intention is better predicted than attention in the posterior parietal cortex</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>3615</fpage><lpage>3620</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3468-05.2006</pub-id><pub-id pub-id-type="pmid">16571770</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raghuraman</surname> <given-names>AP</given-names></name><name><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Integration of multiple determinants in the neuronal computation of economic values</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>11583</fpage><lpage>11603</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1235-14.2014</pub-id><pub-id pub-id-type="pmid">25164656</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname> <given-names>M</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name><name><surname>Warden</surname> <given-names>MR</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Fusi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rothschild</surname> <given-names>M</given-names></name><name><surname>Stiglitz</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Increasing risk: I. A definition</article-title><source>Journal of Economic Theory</source><volume>2</volume><fpage>225</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1016/0022-0531(70)90038-4</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samejima</surname> <given-names>K</given-names></name><name><surname>Ueda</surname> <given-names>Y</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name><name><surname>Kimura</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Representation of action-specific reward values in the striatum</article-title><source>Science</source><volume>310</volume><fpage>1337</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1126/science.1115270</pub-id><pub-id pub-id-type="pmid">16311337</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuronal reward and decision signals: from theories to data</article-title><source>Physiological Reviews</source><volume>95</volume><fpage>853</fpage><lpage>951</lpage><pub-id pub-id-type="doi">10.1152/physrev.00023.2014</pub-id><pub-id pub-id-type="pmid">26109341</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seo</surname> <given-names>H</given-names></name><name><surname>Barraclough</surname> <given-names>DJ</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Dynamic signals related to choices and outcomes in the dorsolateral prefrontal cortex</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>i110</fpage><lpage>i117</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm064</pub-id><pub-id pub-id-type="pmid">17548802</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seo</surname> <given-names>M</given-names></name><name><surname>Lee</surname> <given-names>E</given-names></name><name><surname>Averbeck</surname> <given-names>BB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Action selection and action value in frontal-striatal circuits</article-title><source>Neuron</source><volume>74</volume><fpage>947</fpage><lpage>960</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.037</pub-id><pub-id pub-id-type="pmid">22681697</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>So</surname> <given-names>NY</given-names></name><name><surname>Stuphorn</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Supplementary eye field encodes option and action value for saccades with variable reward</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>2634</fpage><lpage>2653</lpage><pub-id pub-id-type="doi">10.1152/jn.00430.2010</pub-id><pub-id pub-id-type="pmid">20739596</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Squire</surname> <given-names>RF</given-names></name><name><surname>Noudoost</surname> <given-names>B</given-names></name><name><surname>Schafer</surname> <given-names>RJ</given-names></name><name><surname>Moore</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Prefrontal contributions to visual selective attention</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>451</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150439</pub-id><pub-id pub-id-type="pmid">23841841</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stauffer</surname> <given-names>WR</given-names></name><name><surname>Lak</surname> <given-names>A</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dopamine reward prediction error responses reflect marginal utility</article-title><source>Current Biology</source><volume>24</volume><fpage>2491</fpage><lpage>2500</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.08.064</pub-id><pub-id pub-id-type="pmid">25283778</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stephens</surname> <given-names>DW</given-names></name><name><surname>Krebs</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="1986">1986</year><source>Foraging Theory</source><publisher-loc>Chichester</publisher-loc><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stolyarova</surname> <given-names>A</given-names></name><name><surname>Izquierdo</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Complementary contributions of basolateral amygdala and orbitofrontal cortex to value learning under uncertainty</article-title><source>eLife</source><volume>6</volume><elocation-id>e27483</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.27483</pub-id><pub-id pub-id-type="pmid">28682238</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugrue</surname> <given-names>LP</given-names></name><name><surname>Corrado</surname> <given-names>GS</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Matching behavior and the representation of value in the parietal cortex</article-title><source>Science</source><volume>304</volume><fpage>1782</fpage><lpage>1787</lpage><pub-id pub-id-type="doi">10.1126/science.1094765</pub-id><pub-id pub-id-type="pmid">15205529</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Reinforcement Learning</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname> <given-names>M</given-names></name><name><surname>Gottlieb</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Distinct neural mechanisms of distractor suppression in the frontal and parietal lobe</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>98</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1038/nn.3282</pub-id><pub-id pub-id-type="pmid">23242309</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Symmonds</surname> <given-names>M</given-names></name><name><surname>Bossaerts</surname> <given-names>P</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A behavioral and neural evaluation of prospective decision-making under risk</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>14380</fpage><lpage>14389</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1459-10.2010</pub-id><pub-id pub-id-type="pmid">20980595</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tobler</surname> <given-names>PN</given-names></name><name><surname>Christopoulos</surname> <given-names>GI</given-names></name><name><surname>O'Doherty</surname> <given-names>JP</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Risk-dependent reward value signal in human prefrontal cortex</article-title><source>PNAS</source><volume>106</volume><fpage>7185</fpage><lpage>7190</lpage><pub-id pub-id-type="doi">10.1073/pnas.0809599106</pub-id><pub-id pub-id-type="pmid">19369207</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsutsui</surname> <given-names>KI</given-names></name><name><surname>Grabenhorst</surname> <given-names>F</given-names></name><name><surname>Kobayashi</surname> <given-names>S</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A dynamic code for economic object valuation in prefrontal cortex neurons</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>12554</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12554</pub-id><pub-id pub-id-type="pmid">27618960</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname> <given-names>JD</given-names></name><name><surname>Kennerley</surname> <given-names>SW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Heterogeneous reward signals in prefrontal cortex</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>191</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.02.009</pub-id><pub-id pub-id-type="pmid">20303739</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname> <given-names>JD</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neuronal activity in primate dorsolateral and orbital prefrontal cortex during performance of a reward preference task</article-title><source>European Journal of Neuroscience</source><volume>18</volume><fpage>2069</fpage><lpage>2081</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2003.02922.x</pub-id><pub-id pub-id-type="pmid">14622240</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision making in recurrent neuronal circuits</article-title><source>Neuron</source><volume>60</volume><fpage>215</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.034</pub-id><pub-id pub-id-type="pmid">18957215</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>AY</given-names></name><name><surname>Miura</surname> <given-names>K</given-names></name><name><surname>Uchida</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The dorsomedial striatum encodes net expected return, critical for energizing performance vigor</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>639</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1038/nn.3377</pub-id><pub-id pub-id-type="pmid">23584742</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watanabe</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Reward expectancy in primate prefrontal neurons</article-title><source>Nature</source><volume>382</volume><fpage>629</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1038/382629a0</pub-id><pub-id pub-id-type="pmid">8757133</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname> <given-names>EU</given-names></name><name><surname>Milliman</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Perceived risk attitudes: relating risk perception to risky choice</article-title><source>Management Science</source><volume>43</volume><fpage>123</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1287/mnsc.43.2.123</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname> <given-names>JK</given-names></name><name><surname>Monosov</surname> <given-names>IE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neurons in the primate dorsal striatum signal the uncertainty of object-reward associations</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>12735</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12735</pub-id><pub-id pub-id-type="pmid">27623750</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.44838.032</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Lee</surname><given-names>Daeyeol</given-names></name><role>Reviewing Editor</role><aff><institution>Yale School of Medicine</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Louie</surname><given-names>Kenway</given-names> </name><role>Reviewer</role><aff><institution>New York University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Primate prefrontal neurons signal economic risk derived from the statistics of recent reward experience&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Richard Ivry as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Kenway Louie (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>Authors of this work have examined the activity of single neurons in the monkey dorsolateral prefrontal cortex (DLPFC) during probabilistic &quot;matching task&quot; in which the reward probability was adjusted dynamically according to the baseline probability as well as the animal's choice history. The focus of this manuscript is to characterize the signals related to &quot;risk&quot;, namely the uncertainty associated with the reward probability. Whereas previous studies have examined the neural signals related to &quot;objective risk&quot;, the current study have dissected the DLPFC signals related to the experiential/subjective risk signals, and demonstrated how such risk signals co-exist with other signals previously identified in this brain area.</p><p>Essential revisions:</p><p>1) Introduction needs a better organization. Unfortunately, the term risk has been used in a few different ways, and the authors have attempted to clarify how this term is used in the present study, but the first two paragraphs of the Introduction are still somewhat confusing. The manuscript might be easier to digest, if the authors tried to clarify only the terms used in this study, and better avoid mistakenly equating the reward probability and risk (which are different) or using terms such as &quot;variance risk&quot;, which is unnecessarily confusing. In particular, the discussion on loss and risk is not clear, and perhaps unnecessary for the paper since the focus is on variance.</p><p>2) Behavioral effects of risk on choice. Authors should provide more information about how reward risk (or variance) plays into monkey choice behavior in this task. As well documented in these type of matching law tasks, choice behavior is a product of both past rewards and past choices (as also addressed by the reward and choice kernels quantified by the authors). This arises because these kind of environments are dynamic and complex: the baited reward outcomes (probabilistic rewards that remain once armed) are constructed in order to generate matching law behavior, and thus the values of options explicitly depend on past outcomes and choices.</p><p>The question is how does risk (true or subjective) affect behavior, and is it independent of past rewards and choices? The crucial issue is whether risk (or some measure of reward variance) is capturing an aspect of behavior beyond what is captures with past rewards and choices alone. In the analyses in Figure 3, the authors decompose task-relevant information into object value and subjective object risk, and show that both have an effect on monkey choice. However, object value is the weighted sum of reward history (with the weights determined by regression on overall monkey choices), meaning that the effect of past choices is explicitly not captured in the value variable. The key question is whether subjective risk is just capturing the effect of past choices, and I think the authors need to find a way of quantifying the relative influence of risk – above and beyond past choices – on behavior. One way to be to do a formal comparison between the logistic regression based on past rewards and choice and a model using value and risk, or alternatively a full model with rewards, choices, AND risk in the same model. [The authors may have included this kind of analysis (as seems to be the case in the model comparison noted in the Materials and methods, subsection “Testing the influence of subjective object risk on choices”), but the description in the main text only refers to value as a function of past rewards – please correct me if I am mistaken.]</p><p>Understanding of risk, as defined, affects choice behavior is important two reasons. At the behavioral level, it is not yet clear to me how reward risk – objective or subjective – is related to monkey choice behavior in this experiment. One possibility, as detailed above, is that risk is merely capturing the effect of past monkey choices. Alternatively, monkeys may have a preference/aversion for risk itself. The latter point is what I believe the authors are getting at in their analyses (subsection “Subjective risk: definition and behavior”, last paragraph, Figure 3D), but as discussed above this is done by examining the influence of risk on choices for various object value differences, with object values determined solely by reward kernel weighting – this ignores the influence of past choices that may well govern the effect of risk in this task. At the neural level, the authors electrophysiological results show a robust coding for risk in DLPFC neurons, and it is important to distinguish whether these neurons are coding for (objective or subjective) risk itself, or simply some aspect of choice-related strategy that correlates with risk.</p><p>3) Methods to quantify subjective needs to be justified better or improved. The authors have demonstrated that the animals incorporated their choice history to determine their values (Equation 5). Given this, it seems difficult to justify that choice history is not incorporated in the estimation of risk (Equation 6 and 7).</p><p>First, Equation 6 (that estimates value) does so in a classical RL way by assigning higher values to options that have been recently rewarded (more often). However, in this task the true value of an option is considerably lower after having been recently chosen (Equation 1 says the reward probability is lowest after a choice and then grows with each trial the option is not chosen) and animals seem to understand this as shown by their <italic>β<sub>c</sub></italic> weights. Shouldn't the subjective value estimate be based on both past reward and past choice since both influence the animal's choice? The same point applies to Equation 7. If animals understand the task, they should know that the option's risk on the current trial is influenced by its choice history in addition to its reward history.</p><p>Second, Equation 7 attempts to estimate variance. It does not seem to be a true variance measure (i.e. mean squared deviation of quantities from the mean of their distribution) because it is based on the deviation from the sum of those quantities (<italic>OV<sub>A</sub></italic>, from Equation 6) not their mean.</p><p>Beyond that, an estimate of variance should involve the variance of past outcomes, but Equation 7 is also strongly influenced by the variance of the β weights <italic>β<sub>r</sub></italic> and the variance of choice history, and treats reward and non-reward asymmetrically. For instance, if the animal chose option A 10 consecutive times and always received reward, then the reward history has zero variance but <italic>var<sub>A</sub></italic> will still be high because of the variance in the <italic>β<sub>r</sub></italic>. Also, if the animal chose A 10 consecutive times and always received no reward, then <italic>var<sub>A</sub></italic> will be zero, even though it is symmetrical to the previous case and should have the same variance. Finally, if the animal chose B 5 times and then chose A 5 times and always received reward, then <italic>var<sub>A</sub></italic> will be higher than if the animal chose A all 10 times and always received reward, even though the variance of the reward histories from choices of A are the same (5/5 vs. 10/10 rewards), because of the variance in choice history.</p><p>The authors must either 1) use another modelling approach that considers the structure of the &quot;dual assignment with hold&quot; task or 2) substantially revise to make it clear to the reader what the limitations are of the current approach and why they chose to utilize it.</p><p>4) Relationship between neural signals related to objective and subjective risk. One important aspect, however, that is not addressed is the relationship between true risk (i.e. Equation 2) and subjective risk (i.e. Equation 7). How correlated are these measures? At the behavioral level, does subjective risk do a better job at explaining choices compared to true risk?</p><p>At the neural level, the major conclusions of the paper center around the DLPFC coding of subjective object and action risk, but the paper does not clearly show that subjective risk better explains neural responses than true objective risk. For example, based on the stated results, 102 of 205 neurons (subsection “Neuronal coding of objective risk”, second paragraph) significantly responded to true object risk and 96 of 205 neurons (subsection “Neuronal coding of subjective risk associated with choice objects”, first paragraph) coded for subjective object risk. Is there a way for the authors to statistically distinguish whether DLPFC is encoding subjective rather than objective risk? If the different risk measures are uncorrelated (or only mildly correlated), this should be testable; if they are strongly correlated, I am not sure that the neural analyses centered on subjective risk (rather than true risk) are the right approach.</p><p>Showing that the authors' subjective risk estimate better captures choice and/or neural data is important because the specific quantification of subjective risk is not well known or clearly justified. Aside from the issue of the potential relationship between past choice effects and risk (see point 1 above), the weighting of past rewards in the variance calculation feels a bit arbitrary (not the reward kernel itself, which is well known in quantifying choice, but its use in estimating variance). In addition to showing that such a measure better captures behavior/neural responses, it would help if the authors could provide a more formal justification for their form of subjective risk.</p><p>5) Potential problems and weaknesses of decoding analysis. The decoding analysis needs to be strengthened, because currently this analysis does not attempt to distinguish between risk signals and other potentially covarying signals (as was done in the regression analysis). This can be accomplished for example, by balancing the trials with low and high levels of risk, in terms of other potentially confounding variables (c.f. Massi et al., 2018).</p><p>The% of neurons plots common to neuroeconomics studies (and SVM-style decoding that only shows what info is in a pool of neurons and not how good individual neurons are at &quot;encoding&quot;) may not be well suited for dlPFC. dlPFC is a complex spatial- and object- selective, attention, and motivation/reward related area and accordingly multiplexes many signals. It is simply very hard to tell what is going on from the current figures on a cell by cell basis. I want to see if the coding strategies (e.g. value, risk) of the single neurons in dlPFC are consistent across the task epochs in this task to get a better sense of what dlPFC may be doing, and to get a better understanding of the relationship of those value-related variables with spatial and object preferences of single neurons.</p><p>6) Description of models and equations need to be improved.</p><p>6a) The authors introduce two equations (Equations 2 and 7) to define objective and subjective risks, respectively, which are central to this study. However, these two equations can be simplified. For example, it would be much easier to understand this if the Equation 2 is replaced or supplemented by a much more common expression, p(1-p), for a Bernoulli distribution. Including the reward magnitude in Equation 2 doesn't seem necessary, and this causes confusion. In addition, in Equation 7, the coefficient β should be outside the square of the difference between reward and OV. If not, this this needs to be justified/explained better.</p><p>6b) The authors have used Equation 2 in their step-wise regression analysis. However, this seems problematic, because this seems to violate the full-rank assumption, given that ValueA+ValueB = ValueL+ValueR and RiskA+RiskB=RiskL+RiskR? Similarly, is it possible to have both TrueProbA and TrueProbB in Equation 3, given that they sum up to 1, i.e., TrueProbB = (1-TrueProbA)?</p><p>6c) The exact formulation of Equation 8 is a little unclear. The text states that &quot;To account for known choice biases in matching tasks (Lau and Glimcher, 2005), we added to the object-value terms the weighted choice-history using weights derived from Equation 5.&quot; Can the authors state explicitly how ObjectValue was calculated?</p><p>7) Novelty of the task. The authors claim that the animals are not cued and must derive a measure of risk (that would then influence their choices). This important because the authors claim that previous studies used explicit cues during learning while they do not in this study, and claim this is a key advance. While it is true, the cited papers did not utilize trial-by-trial information to look at how subjective value- and risk- are updated on a trial-by-trial basis, their approaches in many other ways seemed pretty similar to this study. Particularly, in this study, the key moment is when the reward probabilities associated with the two options change and the animal must figure out the new probabilities by experiencing two types of external cues: choice options and feedback. Broadly, the idea that this is the first paper to look at risk estimates that are independent of cueing is factually wrong and requires revision.</p><p>Furthermore, even if it was the case, the impact of this is unclear from the current manuscript. Perhaps the authors are most excited about pre-option-presentation object-risk signals in the context of behavioral control? If so, how would this control take place? Is this an arousal / &quot;let's get ready&quot; signal? Or would this signal serve to bias choice to the risky object in a spatial manner (consistent with previous work on dlPFC)? Or is this risk signal to influence SV derivations elsewhere in the brain?</p><p>Specifically, are there reaction time correlates of this early &quot;object risk signal&quot; with action after options are on?</p><p>8) Problem with non-stationarity. The authors should take into account the fact that activity of prefrontal cortex is often non-stationary and is likely to be correlated serially (autocorrelation) across successive trials. This diminishes the effective degree of freedom, and can inflate the estimate of neurons encoding the signals that are related to events in multiple trials (equivalent to low-pass filtering). The authors should refer to a recent paper in <italic>eLife</italic> (&quot;Striatal action-value neurons reconsidered).</p><p>9) Dynamics of DLPFC coding. The authors show that subpopulations of neurons carry multiple signals that could integrate various aspects of reward and choice information (Figure 7). Two additional analyses are important to include. First, is the percentage of neurons showing coding of two variables (last reward x choice and object risk, object value and object risk, etc.) different than that expected given the probabilities of neurons representing either variable?</p><p>Second, for those neurons that carry multiple signals, is the information coded in a consistent manner? For example, do the neurons that represent both value and risk information (Figure C, D) both modulated in the same direction by value and risk? Figure 7 plots the timeline of explained variance, but not the actual direction of modulation. One would expect that, for example in the case of value and risk, that since the behavioral data suggests that choice is driven by increases in value and risk, that neurons integrating that information would represent both in the same manner. A similar argument could be made for risk and choice. A regression weight by regression weight plot, similar to that used for the angle analyses elsewhere in the paper, would be helpful in understanding how this information is integrated across different variable pairs.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Primate prefrontal neurons signal economic risk derived from the statistics of recent reward experience&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Richard Ivry as the Senior Editor, a Reviewing Editor, and two reviewers.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>The authors have largely addressed the concern re: the influence of past choices on subjective value, and Table 1 shows that a model with value (from reward and choice history) and risk performs best. However, the present manuscript is still confusing in that it includes two different measure of subjective object value (Equation 6 and Equation 7).</p><p>First, it is confusing that the two equations both use the same term <italic>OV<sub>A</sub></italic> for the value measure; since they are different definitions, they should have different names. Second, it is entirely not clear which measure is used in which analyses. According to the Materials and methods, Equation 6 (value from rewards alone) is used for the calculation of subjective risk (Equation 8), and Equation 7 (value from rewards and choices) is used for behavioral analyses and neural analyses involving value. So were all neural subjective risk analyses performed only with the measure derived from form Equation 6 <italic>OV<sub>A</sub></italic>? This seems odd, given that the results of the model comparison suggest that value is a function of reward and choice, and the authors use the reward/choice definition of value for neural analyses – shouldn't the subjective risk measure use deviance from <italic>OV<sub>A</sub></italic> determined from reward and choice as well?</p><p>The paper would read more clearly, and be more conceptually unified, if they use a single measure for <italic>OV<sub>A</sub></italic> derived from reward and past choices (Equation 8). Note that this is different than the &quot;risk from choice variance&quot; addressed by the authors in their response letter – it is simply risk as variance of rewards from subjective value (calculated from reward and choice).</p><p>The description in the Materials and methods (subsection “Final calculation of subjective risk”) is incorrect given the revised Equation 8, as it implies that β weights are applied to rewards rather than to the squared deviations of rewards from <italic>OV<sub>A</sub></italic> (as described by the revised equation). For example, &quot;βjrRAβ&quot; term is not in the equation, and &quot;the summed, squared deviation of subjectively weighted reward amounts from the mean weighted value of past rewards)&quot; does not match Equation 8.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.44838.033</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Introduction needs a better organization. Unfortunately, the term risk has been used in a few different ways, and the authors have attempted to clarify how this term is used in the present study, but the first two paragraphs of the Introduction are still somewhat confusing. The manuscript might be easier to digest, if the authors tried to clarify only the terms used in this study, and better avoid mistakenly equating the reward probability and risk (which are different) or using terms such as &quot;variance risk&quot;, which is unnecessarily confusing. In particular, the discussion on loss and risk is not clear, and perhaps unnecessary for the paper since the focus is on variance.</p></disp-quote><p>Thank you for pointing out the need for better organization. We have revised the Introduction accordingly and now focus on the terms used in the present study.</p><p>“Rewards vary intrinsically. The variation can be characterized by a probability distribution over reward magnitudes. […] Thus, among the different definitions of economic risk, variance constitutes the most basic form, and this study will consider only variance as economic risk.”</p><disp-quote content-type="editor-comment"><p>2) Behavioral effects of risk on choice. Authors should provide more information about how reward risk (or variance) plays into monkey choice behavior in this task. As well documented in these type of matching law tasks, choice behavior is a product of both past rewards and past choices (as also addressed by the reward and choice kernels quantified by the authors). This arises because these kind of environments are dynamic and complex: the baited reward outcomes (probabilistic rewards that remain once armed) are constructed in order to generate matching law behavior, and thus the values of options explicitly depend on past outcomes and choices.</p><p>The question is how does risk (true or subjective) affect behavior, and is it independent of past rewards and choices? The crucial issue is whether risk (or some measure of reward variance) is capturing an aspect of behavior beyond what is captures with past rewards and choices alone. In the analyses in Figure 3, the authors decompose task-relevant information into object value and subjective object risk, and show that both have an effect on monkey choice. However, object value is the weighted sum of reward history (with the weights determined by regression on overall monkey choices), meaning that the effect of past choices is explicitly not captured in the value variable. The key question is whether subjective risk is just capturing the effect of past choices, and I think the authors need to find a way of quantifying the relative influence of risk – above and beyond past choices – on behavior. One way to be to do a formal comparison between the logistic regression based on past rewards and choice and a model using value and risk, or alternatively a full model with rewards, choices, AND risk in the same model. [The authors may have included this kind of analysis (as seems to be the case in the model comparison noted in the Materials and methods, subsection “Testing the influence of subjective object risk on choices”), but the description in the main text only refers to value as a function of past rewards – please correct me if I am mistaken.]</p><p>Understanding of risk, as defined, affects choice behavior is important two reasons. At the behavioral level, it is not yet clear to me how reward risk – objective or subjective – is related to monkey choice behavior in this experiment. One possibility, as detailed above, is that risk is merely capturing the effect of past monkey choices. Alternatively, monkeys may have a preference/aversion for risk itself. The latter point is what I believe the authors are getting at in their analyses (subsection “Subjective risk: definition and behavior”, last paragraph, Figure 3D), but as discussed above this is done by examining the influence of risk on choices for various object value differences, with object values determined solely by reward kernel weighting – this ignores the influence of past choices that may well govern the effect of risk in this task. At the neural level, the authors electrophysiological results show a robust coding for risk in DLPFC neurons, and it is important to distinguish whether these neurons are coding for (objective or subjective) risk itself, or simply some aspect of choice-related strategy that correlates with risk.</p></disp-quote><p>We have included a formal comparison of different models of the animals’ behavioral choices. Specifically, we systematically compared models that included different forms of subjective values, with values based on weighted reward history, weighted choice history or both weighted reward and weighted choice history. We compared the effect of adding our main risk measure as a separate regressor to these different forms of value. We also tested a model based on true, objective reward probabilities and risk. Finally, we tested three versions of reinforcement learning models: (i) a standard Rescorla-Wagner model that updated the value of the chosen option following outcomes, (ii) an adaptation of this model that incorporated time-dependent effects related to choice history, which has been proposed as a suitable model of matching behavior (Huh et al., 2009), and (iii) a variant of the Rescorla-Wagner model that updated both the value of the chosen and unchosen option. The results are shown in a new table (Table 1).</p><p>In both animals, the model comparisons favored a model that included subjective value and subjective risk regressors, with subjective value based on both weighted reward and choice history. This result confirms that our main measure of subjective risk was behaviorally meaningful and explained variation in choices that is independent of reward history and choice history.</p><p>To further illustrate this point, we have performed a new logistic regression of choices on value and risk in a subset of trials that minimized the value difference between options. For these trials, value difference did not explain variation in choices (as expected by design of this test) whereas the effect of risk remained significant. Thus, the effect of risk on choices was not explained by value difference. The result of this analysis is shown in Figure 3E, inset and described in the last paragraph of the Results subsection “Subjective risk: definition and behavior”.</p><disp-quote content-type="editor-comment"><p>3) Methods to quantify subjective needs to be justified better or improved. The authors have demonstrated that the animals incorporated their choice history to determine their values (Equation 5). Given this, it seems difficult to justify that choice history is not incorporated in the estimation of risk (Equations 6 and 7).</p><p>First, Equation 6 (that estimates value) does so in a classical RL way by assigning higher values to options that have been recently rewarded (more often). However, in this task the true value of an option is considerably lower after having been recently chosen (Equation 1 says the reward probability is lowest after a choice and then grows with each trial the option is not chosen) and animals seem to understand this as shown by their β<sub>c</sub> weights. Shouldn't the subjective value estimate be based on both past reward and past choice since both influence the animal's choice? The same point applies to Equation 7. If animals understand the task, they should know that the option's risk on the current trial is influenced by its choice history in addition to its reward history.</p><p>Second, Equation 7 attempts to estimate variance. It does not seem to be a true variance measure (i.e. mean squared deviation of quantities from the mean of their distribution) because it is based on the deviation from the sum of those quantities (OV<sub>A</sub>, from Equation 6) not their mean.</p><p>Beyond that, an estimate of variance should involve the variance of past outcomes, but Equation 7 is also strongly influenced by the variance of the β weights β<sub>r</sub> and the variance of choice history, and treats reward and non-reward asymmetrically. For instance, if the animal chose option A 10 consecutive times and always received reward, then the reward history has zero variance but var<sub>A</sub> will still be high because of the variance in the β<sub>r</sub>. Also, if the animal chose A 10 consecutive times and always received no reward, then var<sub>A</sub> will be zero, even though it is symmetrical to the previous case and should have the same variance. Finally, if the animal chose B 5 times and then chose A 5 times and always received reward, then var<sub>A</sub> will be higher than if the animal chose A all 10 times and always received reward, even though the variance of the reward histories from choices of A are the same (5/5 vs. 10/10 rewards), because of the variance in choice history.</p><p>The authors must either 1) use another modelling approach that considers the structure of the &quot;dual assignment with hold&quot; task or 2) substantially revise to make it clear to the reader what the limitations are of the current approach and why they chose to utilize it.</p></disp-quote><p>We thank the reviewers for raising these issues and pointing out the need for clearer and better-justified definitions. In responding to the points raised we have revised our value definition to incorporate choice history and have recalculated all our main analyses accordingly and updated all relevant figures. We now also explicitly discuss the assumptions and limitations of our approach to defining subjective risk. Below we explain these changes in more detail.</p><p>1,1) Should value include choice history? We agree that it is important to use a comprehensive definition of value for behavioral and neuronal analyses. Accordingly, we have now modelled value in direct correspondence to our logistic regression model have now incorporated this component into one scalar value measure. Accordingly, we recalculated all our models with this revised, more comprehensive value definition, which resulted in small changes in the numbers of identified neurons. We have updated all the numbers of identified neurons and all relevant figures to reflect these changes. The new value definition is described in Results, and in Materials and methods, section “Calculation of weighted, subjective value.”</p><p>Results: “As our aim was to study the risk associated with specific objects, we estimated object value by the mean of subjectively weighted reward history over the past ten trials (Figure 3C, dashed blue curve, Equation 6); this object value definition provided the basis for estimating subjective risk as described next. […] (We consider distinctions between reward and choice history and their influence and risk in the Discussion).”</p><p>1,2) Should risk include choice history? One of the main aims of our study was to extend the well-established notion of the risk of choice options by introducing a risk measure derived from an animal’s recent experiences, rather than from pre-trained explicit risk cues. Accordingly, in order to facilitate comparisons with previous behavioral and neurophysiological risk studies, we restricted our definition of risk to the variance of past reward outcomes; we did not extend the risk measure to include the variance of past choices irrespective of rewards as such “risk from choice variance” does not have a correspondence in the economic or neuroeconomic risk literature. Our results suggest that defining risk based on the variance of past rewards provided a reasonable account of behavioral and neuronal data: we show that our main subjective risk measure has a distinct influence on choices, is encoded by a substantial number of neurons, and that this risk measure seems to provide a better explanation of many neuronal responses compared to an alternative, objective risk measure. To better explain our motivation for defining risk based on reward history, we have included the following additional text in Discussion:</p><p>“Our definition of subjective risk has some limitations. To facilitate comparisons with previous studies, we restricted our definition to the variance of past reward outcomes; we did not extend the risk measure to the variance of past choices, which would not have a clear correspondence in the economic or neuroeconomic literature. […] An extension of this approach could introduce calculation of reward statistics over flexible time windows, possibly depending on the number of times an option was recently chosen (similar to an adaptive learning rate in reinforcement learning models).”</p><p>2) Should risk involve subtraction of value calculated from sum or mean? Thank you for pointing this out. For the results presented previously in the manuscript we did follow the general definition of risk and subtracted the mean (rather than the sum) although we used the sum for value definition. (Note that for behavioral and neuronal analyses that test the effect of value, using the sum or mean of the weighted reward history would yield identical results as the number of trials over which the mean is calculated is constant (N = 10 in our case)). For the variance calculation it is of course critical that deviations of single trials are referenced to the mean rather than the sum. We have now corrected the equation in the Materials and methods (Equation 6).</p><p>3) Influence of weights and choice history on variance. We have rewritten the risk equation (now Equation 8) to reflect correctly how we calculated risk: the weight vector was applied to the vector of squared deviations from the mean. With this definition, deviations calculated for more recent trials are given a larger weight in the variance calculation. This approach is similar to the well-established definition of value based on weighted reward history and it is consistent with the notion that the animals base their choice more strongly on reward outcomes of recent trials compared to more remote trials.</p><p>With this definition, variance would not be artificially inflated by the weight vector, and reward and non-reward are not treated asymmetrically. For example, if the animal chose option A 10 consecutive times and always received reward, then reward history has zero variance and multiplication with the weight vector still results in zero variance. The same would be true for the case of 10 consecutively non-rewarded trials.</p><p>The reviewer notes correctly that “if the animal chose B 5 times and then chose A 5 times and always received reward, then <italic>var<sub>A</sub></italic> will be higher than if the animal chose A all 10 times and always received reward”. This is a consequence of the time window (here: 10 trials) used for the variance calculation that also affects the calculation of value in a similar manner. We chose to follow this common approach of using a fixed temporal window over which reward statistics are calculated because of its generality and simplicity, and in order to link our study with these established approaches. An extension of this approach would be to introduce a flexible temporal window that calculates reward statistics over varying time windows, possibly depending on the number of times an option was recently chosen (similar to an adaptive learning rate in Reinforcement learning models).</p><p>To acknowledge the assumptions and limitations of our risk definition we have included the following new text in the Discussion section:</p><p>“We followed the common approach of calculating reward statistics over a fixed temporal window because of its generality and simplicity, and to link our data to previous studies. An extension of this approach could introduce calculation of reward statistics over flexible time windows, possibly depending on the number of times an option was recently chosen (similar to an adaptive learning rate in reinforcement learning models).”</p><disp-quote content-type="editor-comment"><p>4) Relationship between neural signals related to objective and subjective risk. One important aspect, however, that is not addressed is the relationship between true risk (i.e. Equation 2) and subjective risk (i.e. Equation 7). How correlated are these measures? At the behavioral level, does subjective risk do a better job at explaining choices compared to true risk?</p><p>At the neural level, the major conclusions of the paper center around the DLPFC coding of subjective object and action risk, but the paper does not clearly show that subjective risk better explains neural responses than true objective risk. For example, based on the stated results, 102 of 205 neurons (subsection “Neuronal coding of objective risk”, second paragraph) significantly responded to true object risk and 96 of 205 neurons (subsection “Neuronal coding of subjective risk associated with choice objects”, first paragraph) coded for subjective object risk. Is there a way for the authors to statistically distinguish whether DLPFC is encoding subjective rather than objective risk? If the different risk measures are uncorrelated (or only mildly correlated), this should be testable; if they are strongly correlated, I am not sure that the neural analyses centered on subjective risk (rather than true risk) are the right approach.</p><p>Showing that the authors' subjective risk estimate better captures choice and/or neural data is important because the specific quantification of subjective risk is not well known or clearly justified. Aside from the issue of the potential relationship between past choice effects and risk (see point 1 above), the weighting of past rewards in the variance calculation feels a bit arbitrary (not the reward kernel itself, which is well known in quantifying choice, but its use in estimating variance). In addition to showing that such a measure better captures behavior/neural responses, it would help if the authors could provide a more formal justification for their form of subjective risk.</p></disp-quote><p>The objective and subjective risk measures only showed a moderate correlation: mean shared variance was R<sup>2</sup> = 0.111 ± 0.004 (mean ± s.e.m. across sessions). To establish their relevance for behavioral choices, we have included a formal model comparison, summarized in Table 1, which favored a model based on subjective value and subjective risk over a model based on objective value and objective risk. We also examined in direct comparisons whether neuronal responses were better explained by objective or subjective risk. These analyses are described in Results and in Figure 4—figure supplement 3B and C:</p><p>“A direct comparison of objective and subjective risk showed that neuronal activity tended to be better explained by subjective risk. […] When both risk measures were included in a stepwise regression model (Equation 13), and thus competed to explain variance in neuronal activity, we identified more neurons related to subjective risk than to objective risk (107 compared to 83 neurons, Figure 4—figure supplement 3C), of which 101 neurons were exclusively related to subjective risk but not objective risk (shared variance between the two risk measures across sessions: R<sup>2</sup> = 0.111 ± 0.004, mean ± s.e.m.).”</p><disp-quote content-type="editor-comment"><p>5) Potential problems and weaknesses of decoding analysis. The decoding analysis needs to be strengthened, because currently this analysis does not attempt to distinguish between risk signals and other potentially covarying signals (as was done in the regression analysis). This can be accomplished for example, by balancing the trials with low and high levels of risk, in terms of other potentially confounding variables (c.f. Massi et al., 2018).</p><p>The% of neurons plots common to neuroeconomics studies (and SVM-style decoding that only shows what info is in a pool of neurons and not how good individual neurons are at &quot;encoding&quot;) may not be well suited for dlPFC. dlPFC is a complex spatial- and object- selective, attention, and motivation/reward related area and accordingly multiplexes many signals. It is simply very hard to tell what is going on from the current figures on a cell by cell basis. I want to see if the coding strategies (e.g. value, risk) of the single neurons in dlPFC are consistent across the task epochs in this task to get a better sense of what dlPFC may be doing, and to get a better understanding of the relationship of those value-related variables with spatial and object preferences of single neurons.</p></disp-quote><p>We have performed additional decoding analyses in which we balanced risk levels with respect to other task-related variables. The results of these analyses are shown in Figure 6E and confirm significant decoding of risk levels and are described in Results:</p><p>“Decoding of risk from neuronal responses remained significantly above chance in control analyses in which we held constant the value of other task-related variables including object choice, action and cue position (Figure 6E).”</p><p>We also clarify that we used these decoding analyses to examine the extent to which a biologically realistic decoder, such as a downstream neurons could read out risk levels from neuronal population responses, rather than to provide an alternative to the single-neuron regression analysis (subsection “Population decoding of object risk and action risk”). Such a downstream neuron decoding risk from its inputs would of course need to perform the decoding on naturally occurring, “unbalanced” data.</p><p>We thank the reviewer(s) for raising the interesting issue of the complexity of neuronal responses in DLPFC, and their relationships to spatial variables. We have now examined this issue in more detail in our data set and have included these analyses in Figure 7I, Figure 7—figure supplement 1, and described in the Results and in the Discussion.</p><p>Results: “The percentages of neurons coding specific pairs of variables was not significantly different than expected given the probabilities of neurons coding each individual variable (history and risk: χ2 = 1.58, P = 0.2094, value and risk: χ2 = 3.54, P = 0.0599, choice and risk: χ2 = 0.845, P = 0.358). We also tested for relationships in the coding scheme (measured by signed regression coefficients) among neurons with joint risk and choice coding or joint risk and value coding. […] In addition to the risk-related dynamic coding transitions described above, activity in some DLPFC neurons transitioned from coding risk to coding of spatial variables such as cue position or action choice (Figure 7—figure supplement 1).”</p><p>Discussion: “Notably, while some DLPFC neurons jointly coded risk with value or choice in a common coding scheme (indicated by regression coefficients of equal sign), this was not the rule across all neurons with joint coding (Figure 7H). […] An implication for the present study might be that risk signals in DLPFC can support multiple cognitive processes in addition to decision-making, as also suggested by the observed relationship between risk and reaction times (Figure 3—figure supplement 1).”</p><disp-quote content-type="editor-comment"><p>6) Description of models and equations need to be improved.</p><p>6a) The authors introduce two equations (Equations 2 and 7) to define objective and subjective risks, respectively, which are central to this study. However, these two equations can be simplified. For example, it would be much easier to understand this if the Equation 2 is replaced or supplemented by a much more common expression, p(1-p), for a Bernoulli distribution. Including the reward magnitude in Equation 2 doesn't seem necessary, and this causes confusion. In addition, in Equation 7, the coefficient β should be outside the square of the difference between reward and OV. If not, this this needs to be justified/explained better.</p></disp-quote><p>Thank you for pointing out the misplaced β in Equation 7 which we have corrected (now Equation 8). We prefer to keep the reward magnitude term in Equation 2 as the definition of variance in this notation is consistent with previous neuroscientific studies of risk and readily generalizes to situations in which different magnitudes are used. We have included a statement that explains this below the equation: “In our task, reward magnitude on rewarded trials was held constant at 0.7 ml; the definition generalizes to situations with different magnitudes.”</p><disp-quote content-type="editor-comment"><p>6b) The authors have used Equation 2 in their step-wise regression analysis. However, this seems problematic, because this seems to violate the full-rank assumption, given that ValueA+ValueB = ValueL+ValueR and RiskA+RiskB=RiskL+RiskR? Similarly, is it possible to have both TrueProbA and TrueProbB in Equation 3, given that they sum up to 1, i.e., TrueProbB = (1-TrueProbA)?</p></disp-quote><p>Note that subjective action values and action risks were not simply spatially referenced object values and object risks but were estimated separately, based on object reward histories and action reward histories. Accordingly, the stepwise regression approach was not invalidated by the joint inclusion of these regressors in the starting set. Moreover, the true probabilities used for the analysis were not the base probabilities but the trial-specific probabilities that evolved trial-by-trial from the baseline probabilities according to Equation 1. We have clarified these points in the Materials and methods section: “Note that subjective action values and action risks were not simply spatially referenced object values and object risks but were estimated separately, based on object reward histories and action reward histories.”</p><disp-quote content-type="editor-comment"><p>6c) The exact formulation of Equation 8 is a little unclear. The text states that &quot;To account for known choice biases in matching tasks (Lau and Glimcher, 2005), we added to the object-value terms the weighted choice-history using weights derived from Equation 5.&quot; Can the authors state explicitly how ObjectValue was calculated?</p></disp-quote><p>We have now clarified this section by introducing the new Equation 7 and related new text.</p><disp-quote content-type="editor-comment"><p>7) Novelty of the task. The authors claim that the animals are not cued and must derive a measure of risk (that would then influence their choices). This important because the authors claim that previous studies used explicit cues during learning while they do not in this study, and claim this is a key advance. While it is true, the cited papers did not utilize trial-by-trial information to look at how subjective value- and risk- are updated on a trial-by-trial basis, their approaches in many other ways seemed pretty similar to this study. Particularly, in this study, the key moment is when the reward probabilities associated with the two options change and the animal must figure out the new probabilities by experiencing two types of external cues: choice options and feedback. Broadly, the idea that this is the first paper to look at risk estimates that are independent of cueing is factually wrong and requires revision.</p><p>Furthermore, even if it was the case, the impact of this is unclear from the current manuscript. Perhaps the authors are most excited about pre-option-presentation object-risk signals in the context of behavioral control? If so, how would this control take place? Is this an arousal / &quot;let's get ready&quot; signal? Or would this signal serve to bias choice to the risky object in a spatial manner (consistent with previous work on dlPFC)? Or is this risk signal to influence SV derivations elsewhere in the brain?</p><p>Specifically, are there reaction time correlates of this early &quot;object risk signal&quot; with action after options are on?</p></disp-quote><p>Novelty of task: We have toned down the aspect of cue-independence throughout. In the Introduction we added the following sentence” “Similar to previous studies (cited above), experienced rewards following choices for specific objects or actions constituted external cues for risk estimation.” To acknowledge that rewards and choice options of course constituted critical cues for risk estimation, we also removed the emphasis “without requiring explicit, risk-informative cues.” from the last sentence of the Introduction and we removed “in the absence of explicit risk information” from the first sentence of the Discussion. In the first paragraph of the Discussion, when referring to explicit cues, we have added the following: “(such as pre-trained risk-associated bar stimuli or fractals)”. We have also revised the Abstract accordingly.</p><p>Influences on behavior: The Results section ‘Dynamic integration of risk with reward history, value and choice in single neurons’ provides evidence for how DLPFC neurons may integrate risk with choice and value, and we have included new analyses to show that there is also integration with spatially referenced variables in Figure 7—figure supplement 1: “In addition to the risk-related dynamic coding transitions described above, activity in some DLPFC neurons transitioned from coding risk to coding of spatial variables such as cue position or action choice (Figure 7—figure supplement 1).” Moreover, the Discussion covers this topic in several places (sixth and seventh paragraphs, and dedicated Discussion paragraphs covering DLPFC’s contribution to risk and decision-making processes). Taken together, we acknowledge that risk signals in DLPFC may support several functions, in addition to influencing choices, either through local processing or connections to other brain structures.</p><p>Reaction time correlates We have performed a new analysis in which we regress saccadic reaction times on value and risk variables and other factors. These results are shown in Figure 3—figure supplement 1 and mentioned in the Results (subsection “Neuronal coding of subjective risk associated with actions”) and Discussion (sixth paragraph).</p><disp-quote content-type="editor-comment"><p>8) Problem with non-stationarity. The authors should take into account the fact that activity of prefrontal cortex is often non-stationary and is likely to be correlated serially (autocorrelation) across successive trials. This diminishes the effective degree of freedom, and can inflate the estimate of neurons encoding the signals that are related to events in multiple trials (equivalent to low-pass filtering). The authors should refer to a recent paper in eLife (&quot;Striatal action-value neurons reconsidered).</p></disp-quote><p>We have addressed this issue with control analyses as described below. We note that in task such as the present one, probabilities and associated risk change and reset frequently even within trial blocks; accordingly, related neuronal signals tracking value or risk should be quite distinct from any potential non-stationary activity due to noise, drift or unknown sources. We explored whether potential non-stationarity in neuronal activity could have inflated estimates of risk-coding signals, as risk evolved over trials. We performed two control analyses and include these results as follows:</p><p>“Finally, we examined effects of potential non-stationarity of neuronal activity (Elber-Dorozko and Loewenstein, 2018), by including a first order autoregressive term in Equation 10. […] This analysis identified 56 neurons with activity related to risk (note that the control period itself was excluded from this analysis; our original analysis without the control period yields 81 risk neurons).”</p><p>“Controlling for non-stationarity of neuronal responses, we identified 83 action-risk neurons when including a first-order autoregressive term and 56 neurons when subtracting neuronal activity at trial start.”</p><disp-quote content-type="editor-comment"><p>9) Dynamics of DLPFC coding. The authors show that subpopulations of neurons carry multiple signals that could integrate various aspects of reward and choice information (Figure 7). Two additional analyses are important to include. First, is the percentage of neurons showing coding of two variables (last reward x choice and object risk, object value and object risk, etc.) different than that expected given the probabilities of neurons representing either variable?</p><p>Second, for those neurons that carry multiple signals, is the information coded in a consistent manner? For example, do the neurons that represent both value and risk information (Figure C, D) both modulated in the same direction by value and risk? Figure 7 plots the timeline of explained variance, but not the actual direction of modulation. One would expect that, for example in the case of value and risk, that since the behavioral data suggests that choice is driven by increases in value and risk, that neurons integrating that information would represent both in the same manner. A similar argument could be made for risk and choice. A regression weight by regression weight plot, similar to that used for the angle analyses elsewhere in the paper, would be helpful in understanding how this information is integrated across different variable pairs.</p></disp-quote><p>Thank you for suggesting these new analyses which we have now included in Results, Figure 7H, and Discussion:</p><p>Results: “The percentages of neurons coding specific pairs of variables was not significantly different than expected given the probabilities of neurons coding each individual variable (history and risk: χ2 = 1.58, P = 0.2094, value and risk: χ2 = 3.54, P = 0.0599, choice and risk: χ2 = 0.845, P = 0.358). We also tested for relationships in the coding scheme (measured by signed regression coefficients) among neurons with joint risk and choice coding or joint risk and value coding. […] This suggested that while some neurons used corresponding coding schemes for these variables (risk and choice, risk and value) other neurons used opposing coding schemes (see Discussion for further interpretation).”</p><p>Discussion: “Notably, while some DLPFC neurons jointly coded risk with value or choice in a common coding scheme (indicated by regression coefficients of equal sign), this was not the rule across all neurons with joint coding (Figure 7H). This result and the observed high degree of joint coding, with most DLPFC dynamically coding several task-related variables (Figure 7I), matches well with previous reports that neurons in DLPFC show heterogeneous coding and mixed selectivity (Rigotti et al., 2013; Wallis and Kennerley, 2010).”</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>The authors have largely addressed the concern re: the influence of past choices on subjective value, and Table 1 shows that a model with value (from reward and choice history) and risk performs best. However, the present manuscript is still confusing in that it includes two different measure of subjective object value (Equation 6 and Equation 7).</p><p>First, it is confusing that the two equations both use the same term OV<sub>A</sub> for the value measure; since they are different definitions, they should have different names. Second, it is entirely not clear which measure is used in which analyses. According to the Materials and methods, Equation 6 (value from rewards alone) is used for the calculation of subjective risk (Equation 8), and Equation 7 (value from rewards and choices) is used for behavioral analyses and neural analyses involving value. So were all neural subjective risk analyses performed only with the measure derived from form Equation 6 OV<sub>A</sub>? This seems odd, given that the results of the model comparison suggest that value is a function of reward and choice, and the authors use the reward/choice definition of value for neural analyses – shouldn't the subjective risk measure use deviance from OV<sub>A</sub> determined from reward and choice as well?</p><p>The paper would read more clearly, and be more conceptually unified, if they use a single measure for OV<sub>A</sub> derived from reward and past choices (Equation 8). Note that this is different than the &quot;risk from choice variance&quot; addressed by the authors in their response letter – it is simply risk as variance of rewards from subjective value (calculated from reward and choice).</p></disp-quote><p>Thank you for pointing these issues out. We have fully addressed the points by (i) using different names for the two value terms, (ii) clearly stating the purpose and use of each term, (iii) extending our analysis to test the suggested alternative risk definition – the results are shown in a table and indicate that numbers of risk neurons were very similar for the extended risk definition.</p><p>In detail, to rectify the first point, we revised the Results section to clarify for what purposes these definitions were used and, in the Materials and methods, we now use distinct terms for these value definitions. We also clarify that Equation 7 was our main value measure used for behavioral and neuronal analyses whereas Equation 6 was used for comparisons to risk in Figure 3C. These changes to the text are shown below.</p><p>With respect to the second point, our neural subjective risk analyses were performed with the risk measure in Equation 8, based on the sum of the weighted, squared deviations from the mean of the object-specific reward distribution. We prefer this risk definition because it is simple (no additional assumptions about how choice history might be incorporated and weighted), directly interpretable (as deviation of reward from the mean of the reward distribution), and follows previous neuronal studies (which tested risk as variance of a reward distribution). We note that it is partly a <italic>conceptual</italic> question of whether choice history should be incorporated into neuronal measures of value or risk, or considered as a separate behavioral influence (e.g. Lau and Glimcher, 2008, modelled choice history for behavior but based their neuronal value measure on reward history without choice history).</p><p>Nevertheless, we appreciate that other, more elaborate risk definitions are possible and of interest and therefore include the following new analyses. We added a table (Figure 4—figure supplement 4) to compare the numbers of risk neurons obtained with different risk definitions, including the one suggested by the reviewer (incorporating reward and choice history). These alternative definitions yielded identical or only slightly higher numbers of risk neurons compared to our main risk definition (&lt; 5% variation in identified neurons). We therefore focus on our main risk definition (Equation 8), which is simpler and conservative as it makes fewer assumptions.</p><p>Revised Results text: “We followed previous studies of matching behavior (Lau and Glimcher, 2005) that distinguished two influences on value: the history of recent rewards and the history of recent choices. […] Thus, we estimated object value based on both subjectively weighted reward history and subjectively weighted choice history (Equation 7); this constituted our main value measure for behavioral and neuronal analyses.”</p><p>Revised Materials and methods text: <bold>“</bold>We followed previous studies of matching behavior (Lau and Glimcher, 2005) that distinguished two influences on value: the history of recent rewards and the history of recent choices. The first object-value component related to reward history,OVAr, can be estimated by the mean of subjectively weighted reward history over the past ten trials (Equation 6):…”</p><p>“In tasks used to study matching behavior, such as the present one, it has been shown that choice history has an additional influence on behavior and that this influence can be estimated using logistic regression (Lau and Glimcher, 2005). To account for this second object-value component related to choice history, we estimated a subjective measure of object value that incorporated both a dependence on weighted reward history and a dependence on weighted choice history (Equation 7):…”</p><p>New Results text: “We also considered alternative, more complex definitions of subjective risk that incorporated either weighted reward history or both weighted reward and choice history in the risk calculation. […] We therefore focused on our main risk definition (Equation 8), which was simpler and more conservative as it incorporated fewer assumptions.”</p><p>New Materials and methods text: “Alternative, more complex definitions of subjective risk in our task could incorporate the weighting of past trials in the calculation of the mean reward (the subtrahend in the numerator of Equation 8) or incorporate both weighted reward history and weighted choice history in this calculation. We explore these possibilities in a supplementary analysis (Figure 4—figure supplement 4).”</p><disp-quote content-type="editor-comment"><p>The description in the Materials and methods (subsection “Final calculation of subjective risk”) is incorrect given the revised Equation 8, as it implies that β weights are applied to rewards rather than to the squared deviations of rewards from OV<sub>A</sub> (as described by the revised equation). For example, &quot;βjrRA&quot; term is not in the equation, and &quot;the summed, squared deviation of subjectively weighted reward amounts from the mean weighted value of past rewards)&quot; does not match Equation 8.</p></disp-quote><p>Thank you for pointing this out. We have revised the section as follows:</p><p>Revised Materials and methods text: “We used the following definition as our main measure of subjective object risk (Equation 8):</p><p>varA=∑j=1Nβjr(RAi-j-(∑j=1NRAi-j)/N)2N-1</p><p>with βj rrepresenting the weighting coefficients for past rewards (derived from Equation 5), RA as reward delivery after choice of object A, <italic>j</italic> as index for past trials relative to the current <italic>i</italic>th trial, and <italic>N</italic> as the number of past trials included in the model (<italic>N</italic> = 10); the term (∑j=1NRAi-j)/N represents the mean reward over the last ten trials. Thus, the equation derives subjective object risk from the summed, subjectively weighted, squared deviation of reward amounts in the last ten trials from the mean reward over the last ten trials.”</p></body></sub-article></article>