<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">31448</article-id><article-id pub-id-type="doi">10.7554/eLife.31448</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The lawful imprecision of human surface tilt estimation in natural scenes</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-95746"><name><surname>Kim</surname><given-names>Seha</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0356-6168</contrib-id><email>sehakim@upenn.edu</email><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-85605"><name><surname>Burge</surname><given-names>Johannes</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0311-7875</contrib-id><email>jburge@sas.upenn.edu</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Psychology</institution><institution>University of Pennsylvania</institution><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-12394"><name><surname>Gallant</surname><given-names>Jack L</given-names></name><role>Reviewing Editor</role><aff id="aff2"><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>31</day><month>01</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e31448</elocation-id><history><date date-type="received" iso-8601-date="2017-08-25"><day>25</day><month>08</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-01-29"><day>29</day><month>01</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Kim et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Kim et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-31448-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.31448.001</object-id><p>Estimating local surface orientation (slant and tilt) is fundamental to recovering the three-dimensional structure of the environment. It is unknown how well humans perform this task in natural scenes. Here, with a database of natural stereo-images having groundtruth surface orientation at each pixel, we find dramatic differences in human tilt estimation with natural and artificial stimuli. Estimates are precise and unbiased with artificial stimuli and imprecise and strongly biased with natural stimuli. An image-computable Bayes optimal model grounded in natural scene statistics predicts human bias, precision, and trial-by-trial errors without fitting parameters to the human data. The similarities between human and model performance suggest that the complex human performance patterns with natural stimuli are lawful, and that human visual systems have internalized local image and scene statistics to optimally infer the three-dimensional structure of the environment. These results generalize our understanding of vision from the lab to the real world.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.31448.002</object-id><title>eLife digest</title><p>The ability to assess how tilted a surface is, or its ‘surface orientation’, is critical for interacting productively with the environment. For example, it helps organisms to determine whether a particular surface is better suited for walking or climbing. Humans and other animals estimate 3-dimensional (3D) surface orientations from 2-dimensional (2D) images on their retinas. But exactly how they calculate the tilt of a surface from the retinal images is not well understood.</p><p>Scientists have studied how humans estimate surface orientation by showing them smooth (often planar) surfaces with artificial markings. These studies suggested that humans very accurately estimate the direction in which a surface is tilted. But whether humans are as good at estimating surface tilt in the real world, where scenes are more complex than those tested in experiments, is unknown.</p><p>Now, Kim and Burge show that human tilt estimation in natural scenes is often inaccurate and imprecise. To better understand humans’ successes and failures in estimating tilt, Kim and Burge developed an optimal computational model, grounded in natural scene statistics, that estimates tilt from natural images. Kim and Burge found that the model accurately predicted how humans estimate tilt in natural scenes. This suggests that the imprecise human estimates are not the result of a poorly designed visual system. Rather, humans, like the computational model, make the best possible use of the information images provide to perform an estimation task that is very difficult in natural scenes.</p><p>The study takes an important step towards generalizing our understanding of human perception from the lab to the real world.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>tilt</kwd><kwd>slant</kwd><kwd>surface orientation</kwd><kwd>Bayesian estimation</kwd><kwd>natural scene statistics</kwd><kwd>vision</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>EY011747</award-id><principal-award-recipient><name><surname>Burge</surname><given-names>Johannes</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006920</institution-id><institution>University of Pennsylvania</institution></institution-wrap></funding-source><award-id>Startup Funds</award-id><principal-award-recipient><name><surname>Burge</surname><given-names>Johannes</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Human tilt estimation in natural scenes is predicted by an image-computable Bayes optimal model that is grounded in the statistics of natural images and scenes.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Understanding how vision works in natural conditions is a primary goal of vision research. One measure of success is the degree to which performance in a fundamental visual task can be predicted directly from image data. Estimating the 3D structure of the environment from 2D retinal images is just such a task. However, relatively little is known about how the human visual system estimates 3D surface orientation from images of natural scenes.</p><p>3D surface orientation is typically parameterized by slant and tilt. Slant is the amount by which a surface is rotated away from an observer; tilt is the direction in which the surface is rotated (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Compared to slant, tilt has received little attention, even though both are critically important for successful interaction with the 3D environment. For example, even if slant has been accurately estimated, humans must estimate tilt to determine where they can walk. Surface with tilts of 90°, like the ground plane, can sometimes be walked on. Surfaces with tilts of 0° or 180°, like the sides of tree trunks, can never be walked on.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.31448.003</object-id><label>Figure 1.</label><caption><title>Tilt and slant, natural scene database, and tilt prior.</title><p>(<bold>A</bold>) Tilt is the direction of slant. Slant is the amount of rotation out of the reference (e.g., frontoparallel) plane. (<bold>B</bold>) Example stereo-image pair (top) and corresponding stereo-range data (bottom). The gauge figure indicates local surface orientation. To see the scene in stereo 3D, free-fuse the left-eye (LE) and right-eye (RE) images. (<bold>C</bold>) Prior distribution of unsigned tilt in natural scenes, computed from 600 million groundtruth tilt samples in the natural scene database (see Materials and methods). Cardinal surface tilts associated with the ground plane (90°) and tree trunks (0° and 180°) occur far more frequently than oblique tilts in natural scenes. Unsigned tilt, <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>180</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, indicates 3D surface orientation up to a sign ambiguity (i.e., tilt modulo 180°).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig1-v2"/></fig><p>Numerous psychophysical, computational, and neurophysiological studies have probed the human ability to estimate surface slant, surface tilt, and 3D shape. Systematic performance has been observed, and models have been developed that nicely describe performance. Most previous studies have used stimuli having planar (<xref ref-type="bibr" rid="bib42">Stevens, 1983</xref>;<xref ref-type="bibr" rid="bib24">Knill, 1998a</xref><xref ref-type="bibr" rid="bib25">, 1998b</xref>; <xref ref-type="bibr" rid="bib21">Hillis et al., 2004</xref>; <xref ref-type="bibr" rid="bib9">Burge et al., 2010a</xref>; <xref ref-type="bibr" rid="bib39">Rosenholtz and Malik, 1997</xref>; <xref ref-type="bibr" rid="bib38">Rosenberg et al., 2013</xref>; <xref ref-type="bibr" rid="bib34">Murphy et al., 2013</xref>; <xref ref-type="bibr" rid="bib48">Velisavljević and Elder, 2006</xref>; <xref ref-type="bibr" rid="bib41">Saunders and Knill, 2001</xref>; <xref ref-type="bibr" rid="bib51">Welchman et al., 2005</xref>; <xref ref-type="bibr" rid="bib40">Sanada et al., 2012</xref>; <xref ref-type="bibr" rid="bib46">Tsutsui et al., 2001</xref>) or smoothly curved (<xref ref-type="bibr" rid="bib44">Todd et al., 1996</xref>; <xref ref-type="bibr" rid="bib16">Fleming et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Todd, 2004</xref>;<xref ref-type="bibr" rid="bib30">Marlow et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Li and Zaidi, 2000</xref><xref ref-type="bibr" rid="bib28">, 2004</xref>; <xref ref-type="bibr" rid="bib35">Norman et al., 2006</xref>) surface shapes and regular (<xref ref-type="bibr" rid="bib24">Knill, 1998a</xref><xref ref-type="bibr" rid="bib25">, 1998b</xref>; <xref ref-type="bibr" rid="bib21">Hillis et al., 2004</xref>; <xref ref-type="bibr" rid="bib49">Watt et al., 2005</xref>; <xref ref-type="bibr" rid="bib39">Rosenholtz and Malik, 1997</xref>; <xref ref-type="bibr" rid="bib38">Rosenberg et al., 2013</xref>; <xref ref-type="bibr" rid="bib34">Murphy et al., 2013</xref>; <xref ref-type="bibr" rid="bib48">Velisavljević and Elder, 2006</xref>; <xref ref-type="bibr" rid="bib27">Li and Zaidi, 2000</xref><xref ref-type="bibr" rid="bib28">, 2004</xref>; <xref ref-type="bibr" rid="bib51">Welchman et al., 2005</xref>) or random-patterned (<xref ref-type="bibr" rid="bib9">Burge et al., 2010a</xref>; <xref ref-type="bibr" rid="bib16">Fleming et al., 2011</xref>) surface markings. These stimuli are not representative of the variety of surface shapes and markings encountered in natural viewing. Surfaces in natural scenes often have complex surface geometries and are marked by complicated surface textures. Thus, performance with simple artificial scenes may not be representative of performance in natural scenes. Also, models developed with artificial scenes often generalize poorly (or cannot even be applied) to natural scenes. These issues concern not just studies of 3D surface orientation perception but vision and visual neuroscience at large.</p><p>Few studies have examined the human ability to estimate 3D surface orientation using natural photographic images, the stimuli that our visual systems evolved to process. None, to our knowledge, have done so with high-resolution groundtruth surface orientation information. There are good reasons for this gap in the literature. Natural images are complex and difficult to characterize mathematically, and groundtruth data about natural scenes are notoriously difficult to collect. Research with natural stimuli has often been criticized (justifiably) on the grounds that natural stimuli are too complicated or too poorly controlled to allow strong conclusions to be drawn from the results. The challenge, then, is to develop experimental methods and computational models that can be used with natural stimuli without sacrificing rigor and interpretability.</p><p>Here, we report an extensive examination of human 3D tilt estimation from local image information with natural stimuli. We sampled thousands of natural image patches from a recently collected stereo-image database of natural scenes with precisely co-registered distance data (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) (<xref ref-type="bibr" rid="bib11">Burge et al., 2016</xref>). Groundtruth surface orientation was computed directly from the distance data (see Materials and methods). Human observers binocularly viewed the natural patches and estimated the tilt at the center of each patch. The same human observers also viewed artificially-textured planar stimuli matched to the groundtruth tilt, slant, distance, and luminance contrast of the natural stimuli. First, we compared human performance with natural and matched artificial stimuli. Then, we compared human performance to the predictions of an image-computable normative model, a Bayes’ optimal observer, that makes the best possible use of the available image information for the task. This experimental design enables direct, meaningful comparison of human performance across stimulus types, allowing the isolation of important stimulus differences and the interpretation of human response patterns with respect to principled predictions provided by the model.</p><p>A rich set of results emerges. First, tilt estimation in natural scenes is hard; compared to performance with artificial stimuli, performance with natural stimuli is poor. Second, with natural stimuli, human tilt estimates cluster at the cardinal tilts (0°, 90°, 180° and 270°), echoing the prior distribution of tilts in natural scenes (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) (<xref ref-type="bibr" rid="bib11">Burge et al., 2016</xref>; <xref ref-type="bibr" rid="bib52">Yang and Purves, 2003a</xref>;<xref ref-type="bibr" rid="bib53">Yang and Purves, 2003b</xref>; <xref ref-type="bibr" rid="bib1">Adams et al., 2016</xref>). Third, human estimates tend to be more biased and variable when the groundtruth tilts are oblique (e.g., 45°). Fourth, at each groundtruth tilt, the distributions of human and model errors tend to be very similar, even though the error distributions themselves are highly irregular. Fifth, human and model observer trial-by-trial errors are correlated, suggesting that similar (or strongly correlated) stimulus properties drive both human and ideal performance. Together, these results represent an important step towards the goal of being able to predict human percepts of 3D structure directly from photographic images in a fundamental natural task.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Human observers binocularly viewed thousands of randomly sampled patches of natural scenes; they viewed an equal number of stimuli at each of 24 tilt bins between 0° and 360°. The stimuli were presented on a large (2.0 × 1.2 m) stereo front-projection system positioned 3 m from the observer. This relatively long viewing distance minimizes focus cues to flatness. Except for focus cues, the display system recreates the retinal images that would have been formed by the original scene. Each scene was viewed binocularly through a small virtual aperture (1° or 3° of visual angle) positioned 5 arcmin of disparity in front of the sampled point in the scene (<xref ref-type="fig" rid="fig2">Figure 2A</xref>); the viewing situation is akin to looking at the world through a straw (<xref ref-type="bibr" rid="bib33">McDermott, 2004</xref>). Patches were displayed at the random image locations from which they were sampled. Observers reported, using a mouse-controlled probe, the estimated surface tilt at the center of each patch (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). We pooled data across human observers and aperture sizes and converted the tilt estimates to unsigned tilt for analysis (signed tilt modulo 180°) because the estimation of unsigned tilt was similar for all observers and aperture sizes (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). The same observers also estimated surface tilt with an extensive set of artificial planar stimuli that were matched to the tilts, slants, distances, and luminance contrasts of the natural stimuli presented in the experiment. (Each planar artificial stimulus had one of three texture types: 1/f noise, 3.5 cpd plaid, and 5.25 cpd plaid; <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>.) Thus, any observed performance differences between natural and artificial stimuli cannot be attributed to these dimensions.</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.31448.004</object-id><label>Figure 2.</label><caption><title>Experimental stimuli and human tilt responses.</title><p>(<bold>A</bold>) The virtual viewing situation. (<bold>B</bold>) Example natural stimulus (ground plane) and artificial stimulus (3.5cpd plaid). See <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> for all three types. The task was to report the tilt at the center of the small (1° diameter) circle. Aperture sizes were either 3° (shown) or 1° (not shown) of visual angle. Observers set the orientation of the probe (circle and line segments) to indicate estimated tilt. Free-fuse to see in stereo 3D. (<bold>C</bold>) Raw responses for every trial in the experiment. (<bold>D</bold>) Histogram of raw responses (unsigned estimates). The dashed horizontal line shows the uniform distribution of groundtruth tilts presented in the experiment. (Histograms of signed tilt estimates are shown in <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>.) (<bold>E</bold>) Estimate means and (<bold>F</bold>) estimate variances as a function of groundtruth tilt. Human tilt estimates are more biased and variable with natural stimuli (top) than with artificial stimuli (bottom). Data are combined across all three artificial texture types; see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> for performance with each individual texture type. With artificial stimuli, human estimates are unbiased and estimate variance is low. Model observer predictions (minimum mean squared error [MMSE] estimates; black curves) parallel human performance with natural stimuli.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31448.005</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Tilt estimation errors with small vs. large apertures for natural stimuli.</title><p>Signed tilt is defined in <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>360</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with slant in [0°,90°); unsigned tilt is defined in <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>[0°,180°) and slant in [−90°,90°). Estimation errors are analyzed in both the signed tilt and in the unsigned tilt domain. To reduce the possibility that individual stimuli were memorized, sessions with small apertures always preceded sessions with large apertures for a given set of natural stimuli. (<bold>A</bold>) Mean absolute error in the signed tilt domain across trials in each of 24 experimental sessions. The 3° aperture improves signed tilt estimation performance. With a 1° aperture, humans make significantly more sign mistakes (e.g., estimating a tilt of 45° when the groundtruth tilt is 225°). (<bold>B</bold>) Mean absolute error in the unsigned tilt domain across trials in each of 24 experimental sessions. Errors with large and small apertures in the unsigned domain are nearly indistinguishable. (<bold>C</bold>) Unsigned trial-by-trial errors with small vs. large windows for each human observer. The errors are strongly correlated, although there are several examples of 90° shifted estimates in the data of Humans 1 and 3. Overall, with unsigned tilt, performance with small and large windows was nearly equivalent.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig2-figsupp1-v2"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31448.006</object-id><label>Figure 2—figure supplement 2.</label><caption><title>Tilt estimation performance for individual human observers.</title><p>(<bold>A</bold>) Raw tilt responses from Human 1, represented in the signed tilt domain <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>360</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, for natural and artificial stimuli. (<bold>B</bold>) Histogram of raw responses (estimates) in the unsigned domain <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>180</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>C</bold>) Mean estimates and (<bold>D</bold>) estimate variance as a function of groundtruth tilt. (<bold>E–H</bold>) Same as A–D but for Human 2. (<bold>I–L</bold>) Same as A–D but for Human 3.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig2-figsupp2-v2"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31448.007</object-id><label>Figure 2—figure supplement 3.</label><caption><title>Summary statistics for the three different artificial stimulus types: 1/f noise, 3.50 cpd plaid, 5.25 cpd plaid (top, middle, and bottom rows, respectively).</title><p>Estimate variance for 1/f noise texture artificial stimuli is slightly higher than estimate variance with the plaids. The patterns are otherwise similar.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig2-figsupp3-v2"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31448.008</object-id><label>Figure 2—figure supplement 4.</label><caption><title>Histogram of raw responses (estimates) from human observers in the signed tilt domain <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>360</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</title><p>The dashed line represents the uniform distribution of tested stimuli. (<bold>A</bold>) Human responses to natural stimuli. The distribution <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> peaks at cardinal angles, but the peak at 270° is significantly lower than the peak at 90°, similar to the distribution of tilts in natural scenes <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> computed directly from the database. (<bold>B</bold>) Human responses to artificial stimuli are more similar to the distribution of tested tilts. (<bold>C</bold>) Prior probability distribution of groundtruth tilts, computed directly from the natural scene database, in the signed tilt domain.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig2-figsupp4-v2"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31448.009</object-id><label>Figure 2—figure supplement 5.</label><caption><title>Tilt estimation performance with full-field (36° x 21°) viewing of natural stimuli.</title><p>Estimates were obtained for half the natural stimuli in the main experiment (1800 trials) from two of the three human observers. The experimental design was otherwise the same as the original experiment. (<bold>A</bold>) Raw responses. (<bold>B</bold>) Count ratio of estimates for full-field viewing (red) and the data from the main experiment (white). (<bold>C</bold>) Mean estimates as a function of groundtruth tilt. (<bold>D</bold>) Estimate variance as a function of groundtruth tilt. (<bold>E</bold>) Distribution of estimation errors for different groundtruth tilts. (<bold>F</bold>) Distribution of groundtruth tilt for different tilt estimates.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig2-figsupp5-v2"/></fig></fig-group><p>Natural and artificial stimuli elicited strikingly different patterns of performance (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Although many stimuli of both types elicit tilt estimates <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that approximately match the groundtruth tilt (data points on the unity line), a substantial number of natural stimuli elicit estimates that cluster at the cardinal tilts (data points at <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>180</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>270</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). No such clustering occurs with artificial stimuli. The histogram of the human tilt estimates explicitly shows the clustering, or lack thereof (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). With natural stimuli, the distribution of unsigned estimates <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> peaks at 0° and 90° and has a similar shape to the prior distribution of groundtruth tilts in the natural scene database (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; also see <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). If the database is representative of natural scenes, then one might expect the human visual system to use the natural statistics of tilt as a tilt prior in the perceptual processes that convert stimulus measurements into estimates. Standard Bayesian estimation theory predicts that the prior will influence estimates more when measurements are unreliable and will influence estimates less when measurements are reliable (<xref ref-type="bibr" rid="bib23">Knill and Richards, 1996</xref>).</p><p>We summarized 3D tilt estimation performance by computing the mean and variance of the tilt estimates <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as a function of groundtruth tilt (<xref ref-type="fig" rid="fig2">Figure 2E,F</xref>). (The mean and variance were computed using circular statistics because tilt is an angular variable; see Materials and methods.) These summary statistics change systematically with groundtruth tilt, exhibiting patterns reminiscent of the 2D oblique effect (<xref ref-type="bibr" rid="bib2">Appelle, 1972</xref>; <xref ref-type="bibr" rid="bib17">Furmanski and Engel, 2000</xref>; <xref ref-type="bibr" rid="bib20">Girshick et al., 2011</xref>). With natural stimuli, estimates are maximally biased at oblique tilts and unbiased at cardinal tilts; estimate variance is highest at oblique tilts (~60° and ~120°) and lowest at cardinal tilts. With artificial stimuli, estimates are essentially unbiased and are less variable across tilt. The unbiased responses to artificial stimuli imply that the biased responses to natural stimuli accurately reflect biased perceptual estimates, under the assumption that the function that maps perceptual estimates to probe responses is stable across stimulus types (see Materials and methods). (See <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> for performance with each individual artificial texture type.) The summary statistics reveal clear differences between the stimulus types. However, there is more to the data than the summary statistics can reveal. Thus, we analyzed the raw data more closely.</p><p>The probabilistic relationship between groundtruth tilt <inline-formula><mml:math id="inf13"><mml:mi>τ</mml:mi></mml:math></inline-formula> and human tilt estimates <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is shown in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>. Each subplot in <xref ref-type="fig" rid="fig3">Figure 3A</xref> shows the distribution of estimation errors <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for a different groundtruth tilt. With artificial stimuli, estimation errors <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are unimodally distributed and peaked at zero (black symbols). With natural stimuli, estimation errors are more irregularly distributed, and the peak locations change systematically with groundtruth tilt (white points). With cardinal groundtruth tilts (e.g., <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>), the error distributions peak at zero and large errors are rare. With oblique groundtruth tilts (e.g., <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>60</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>120</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>), the error distributions tend to be bi-modal with two prominent peaks at non-zero errors. For example, when groundtruth tilt <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>60</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, the most common errors were −60° and 30°. These errors occurred because observers incorrectly estimated the tilt to be 0° or 90°, respectively, when the correct answer was 60º. Thus, at this groundtruth tilt, the human observers frequently (and incorrectly) estimated cardinal tilts instead of the correct oblique tilt.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.31448.010</object-id><label>Figure 3.</label><caption><title>Distribution of tilt estimation errors for different groundtruth tilts.</title><p>(<bold>A</bold>) Conditional error distributions <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are obtained by binning estimates for each groundtruth tilt (vertical bins in <xref ref-type="fig" rid="fig3">Figure 3B</xref>) and subtracting the groundtruth tilt. With artificial stimuli, the error distributions are centered on 0° (black symbols). With natural stimuli, the error distributions change systematically with groundtruth tilt (white symbols). For cardinal groundtruth tilts (0° and 90°), the most common error is zero. For oblique tilts, the error distributions peak at values other than zero (e.g., arrows in <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>60</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>120</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> subplots). The irregular error distributions are nicely predicted by the MMSE estimator (black curve); shaded regions show 95% confidence intervals on the MMSE estimates from 1000 Monte Carlo simulations of the experiment (see Materials and methods). The MMSE estimator predicts human performance even though zero free parameters were fit to the human responses. (<bold>B</bold>) Raw unsigned tilt estimates with natural stimuli (same data as <xref ref-type="fig" rid="fig2">Figure 2C</xref>, but shown in the unsigned tilt domain). The rectangular box shows estimates in the <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>60</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> tilt bin.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig3-v2"/></fig><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.31448.011</object-id><label>Figure 4.</label><caption><title>Distribution of groundtruth tilts for different tilt estimates.</title><p>(<bold>A</bold>) Conditional distributions of groundtruth tilt <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are obtained by binning groundtruth tilts for each estimated tilt (horizontal bins in <xref ref-type="fig" rid="fig4">Figure 4B</xref>). Unlike the conditional error distributions, these distributions are similar with natural and artificial stimuli. The most probable groundtruth tilt, conditional on the estimate, peaks at the estimated tilt for both stimulus types. Thus, any given estimate is a good indicator of the groundtruth tilt despite the overall poorer performance with natural stimuli. Also, these conditional distributions are well accounted for by the MMSE estimates; shaded regions show 95% confidence intervals on the MMSE estimates from 1000 Monte Carlo simulations of the experiment (see Materials and methods). The MMSE model had zero free parameters to fit to human performance. (<bold>B</bold>) Raw unsigned tilt estimates (same data as <xref ref-type="fig" rid="fig2">Figure 2C</xref>, but shown in the unsigned tilt domain). The box shows groundtruth tilts in the <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>60</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> estimated tilt bin.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31448.012</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Alternative visualization of data in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>.</title><p>Top row: Natural stimuli. Bottom row: Artificial stimuli. (<bold>A</bold>) Conditional probability <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of estimated tilts given groundtruth tilt in the unsigned tilt domain: <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></inline-formula>[0, 180°). Each column of each subplot represents the conditional probability distribution of estimates for a different groundtruth tilt. Hence, each column represents the data in each subplot of <xref ref-type="fig" rid="fig3">Figure 3</xref>. (<bold>B</bold>) Conditional probability <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of groundtruth tilts given estimated tilt. Each row of each subplot represents the conditional probability distribution of groundtruth tilts for a different estimate. Hence, each row represents the data in each subplot of <xref ref-type="fig" rid="fig4">Figure 4</xref>. (<bold>C,D</bold>) Same as (A,B) but in the signed tilt domain: <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></inline-formula>[0°, 360°). For visualization, each conditional probability distribution is normalized so that the maximum probability equals 1.0 (colorbar). The distributions of tilt estimates from natural and artificial stimuli conditional on groundtruth tilts are very different (<bold>A,C</bold>). By contrast, the distributions of groundtruth tilts conditional on estimates from natural and artificial stimuli are very similar (<bold>B,D</bold>). Thus, regardless of the stimulus type, the information provided about groundtruth by human tilt estimates is of similar quality. This property of the estimates should simplify subsequent processing that combines local into more global estimates (see main text).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig4-figsupp1-v2"/></fig></fig-group><p>Tilt estimates from natural stimuli are less accurate at oblique than at cardinal groundtruth tilts. Does this fact imply that oblique tilt estimates (e.g., <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mn>60</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) provide less accurate information about groundtruth tilt than cardinal tilt estimates (e.g., <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>)? No. Each panel in <xref ref-type="fig" rid="fig4">Figure 4A</xref> shows the distribution of groundtruth tilts <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for each estimated tilt. The most probable groundtruth tilt equals the estimated tilt, and the variance of each distribution is approximately constant, regardless of whether the estimated tilt is cardinal or oblique. Furthermore, the estimates from natural and artificial stimuli provide nearly equivalent information about groundtruth (see also <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Thus, even though tilt estimation performance is far poorer at oblique than at cardinal tilts and is far poorer with natural than with artificial stimuli, all tilt estimates regardless of the value are similarly good predictors of groundtruth tilt.</p><p>How can it be that low-accuracy estimates from natural stimuli predict groundtruth nearly as well as high-accuracy estimates from artificial stimuli? Some regions of natural scenes yield high-reliability measurements that make tilt estimation easy; other regions of natural scenes yield low-reliability measurements that make tilt estimation hard. When measurements are reliable, the prior influences estimates less; when measurements are unreliable, the prior influences estimates more. Thus, cardinal tilt estimates can result either from reliable measurements of cardinal tilts or from unreliable measurements of oblique tilts. On the other hand, oblique tilt estimates can only result from reliable measurements of oblique tilts, because the measurements must be reliable enough to overcome the influence of the prior. All these factors combine to make each tilt estimate, regardless of its value, an equally reliable predictor of groundtruth tilt. The uniformly reliable information provided by the estimates about groundtruth (see <xref ref-type="fig" rid="fig4">Figure 4A</xref>) may simplify the computational processes that optimally pool local estimates into global estimates (see Discussion). The generality of this phenomenon across natural tasks remains to be determined. However, we speculate that it may have widespread importance for understanding perception in natural scenes, as well as in other circumstances where measurement reliability varies drastically across spatial location.</p><sec id="s2-1"><title>Normative model</title><p>We asked whether the complicated pattern of human performance with natural stimuli is consistent with optimal information processing. To answer this question, we compared human performance to the performance of a normative model, a Bayes optimal observer that optimizes 3D tilt estimation in natural scenes given a squared error cost function (<xref ref-type="bibr" rid="bib11">Burge et al., 2016</xref>). The model takes three local image cues <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as input — luminance, texture, and disparity gradients — and returns the minimum mean squared error (MMSE) tilt estimate <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as output. (The MMSE estimate is the mean of the posterior probability distribution over groundtruth tilt given the measured image cues.)</p><p>To determine the optimal estimate for each possible triplet of cue values, we use the natural scene database. At each pixel in the database, the image cues are computed directly from the photographic images within a local area, and the groundtruth tilt is computed directly from the distance data (see Materials and methods; [<xref ref-type="bibr" rid="bib11">Burge et al., 2016</xref>]). In other words, the model is ‘image-computable’: the model computes the image cues from image pixels and produces tilt estimates as outputs.</p><p>We approximate the posterior mean <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:munder><mml:mi>τ</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> by computing the sample mean of the groundtruth tilt conditional on each unique image cue triplet (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). The result is a table, or ‘estimate cube,’ where each cell stores the optimal estimate <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for a particular combination of image cues (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.31448.013</object-id><label>Figure 5.</label><caption><title>Normative model for tilt estimation in natural scenes.</title><p>(<bold>A</bold>) The model observer estimate is the minimum mean squared error (MMSE) tilt estimate <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> given three image cue measurements. Optimal estimates are approximated from 600 million data points (90 stereo-images) in the natural scene database: image cue values are computed directly from the photographic images and groundtruth tilts are computed directly from the distance data. (<bold>B</bold>) MMSE estimates for ~260,000 (64<sup>3</sup>) unique image cue triplets are stored in an ‘estimate cube.’ (<bold>C</bold>) Model observer estimates for the 3600 unique natural stimuli used in the experiment. For each stimulus used in the experiment, the image cues are computed, and the MMSE estimate is looked up in the ‘estimate cube.’ Excluding the 3600 experimental stimuli from the 600 million stimuli that determined the estimate cube has no impact on predictions. The optimal estimates within the estimate cube change smoothly with the image cue values; hence, a relatively small number of samples can explore the structure of the full 3D space and provide representative performance measures (see Discussion). (<bold>D</bold>) Proportion variance explained (<inline-formula><mml:math id="inf40"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>) by the normative model for the summary statistics (estimate counts, means, and variances; <xref ref-type="fig" rid="fig2">Figure 2D–F</xref>) and the conditional distributions (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>). All <inline-formula><mml:math id="inf41"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> values are highly significant (<inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig5-v2"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31448.014</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Unsigned tilt estimates: human observers and normative model.</title><p>Each panel shows the unsigned tilt estimate of a human or model observer plotted against the groundtruth tilt of every stimulus presented in the experiment.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig5-figsupp1-v2"/></fig></fig-group><p>In the cue-combination literature, cues are commonly assumed to be statistically independent (<xref ref-type="bibr" rid="bib15">Ernst and Banks, 2002</xref>). In natural scenes, it is not clear whether this assumption holds. Fortunately, the normative model used here is free of assumptions about statistical independence and the form of the joint probability distribution (see Discussion). Thus, our normative model provides a principled benchmark, grounded in natural scene statistics, against which to compare human performance.</p><p>We tested the model observer on the exact same set of natural stimuli used to test human observers (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). The model observer predicts the overall pattern of raw human responses (see also <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). More impressively, the model observer predicts the counts, means, and variances of the human tilt estimates (<xref ref-type="fig" rid="fig2">Figure 2D–F</xref>), the conditional error distributions (<xref ref-type="fig" rid="fig3">Figure 3</xref>), and the conditional groundtruth tilt distributions (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The model explains a large proportion of the variance for all of these performance measures (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). These results indicate that human visual system estimates tilt in accordance with optimal processes that minimize error in natural scenes. We conclude that the biased and imprecise human tilt estimates with natural stimuli are nevertheless lawful.</p><p>Two points are worth emphasizing. First, this model observer had no free parameters that were fit to the human data (<xref ref-type="bibr" rid="bib11">Burge et al., 2016</xref>); instead, the model observer was designed to perform the task optimally given the three image cues. Second, the close agreement between human and model performance suggests that humans use the same cues (or cues that strongly correlate with those) used by the normative model (see Discussion).</p></sec><sec id="s2-2"><title>Trial-by-trial error</title><p>If human and model observers use the same cues in natural stimuli to estimate tilt, variation in the stimuli should cause similar variation in performance. Are human performance and model observer performance similar in individual trials? The same set of natural stimuli was presented to all observers. Thus, it is possible to make direct, trial-by-trial comparisons of the estimation errors that each observer made. If the properties of individual natural stimuli influence estimates similarly across observers, then observer errors across trials should be correlated. Accounting for trial-by-trial errors is one of the most stringent comparisons that can be made between model and human performance.</p><p>Natural stimuli do elicit similar trial-by-trial errors from human and model observers (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The model predicts trial-by-trial human errors far better than chance. We quantify the model-human similarity with the circular correlation coefficients of the trial-by-trial model and human estimates (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). The correlation coefficients are significant. This result implies that the errors are systematically and reliably dependent on the properties of natural stimuli and that these properties affect human and model observers similarly.</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.31448.015</object-id><label>Figure 6.</label><caption><title>Trial-by-trial estimation errors: normative model vs. human observers.</title><p>The diagonal structure in the plots indicates that trial-by-trial errors are correlated. (<bold>A</bold>) Raw trial-by-trial errors with natural stimuli between model and human observers. (<bold>B</bold>) Correlation coefficients (circular) for trial-by-trial errors between model and each human observer. The error bars represent 95% confidence intervals from 1000 bootstrapped samples of the correlation coefficient. The dashed line shows the mean of the correlation coefficients of errors between human observers in natural stimuli (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). (<bold>C</bold>) Bias-corrected errors in natural stimuli. (<bold>D</bold>) Correlation coefficient for bias-corrected errors.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig6-v2"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31448.016</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Trial-by-trial estimation errors between humans.</title><p>The diagonal structure in the plots indicates that trial-by-trial errors are correlated. (<bold>A</bold>) Raw trial-by-trial errors with natural stimuli between humans. (<bold>B</bold>) Correlation coefficients (circular) for trial-by-trial errors between humans. The error bars represent 95% confidence intervals from 1000 bootstrapped samples of the correlation coefficient. (<bold>C</bold>) Bias-corrected errors in natural stimuli. (<bold>D</bold>) Correlation coefficient for bias-corrected errors.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig6-figsupp1-v2"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31448.017</object-id><label>Figure 6—figure supplement 2.</label><caption><title>Six alternative models for predicting human tilt estimation performance.</title><p>Each model sets its estimates equal to (i) the minimum mean squared error (MMSE) estimates based on three cues (i.e., the normative model used in the main text); (ii–iv) the MMSE estimates based on each single cue alone (Luminance; Texture; and Disparity); (v) random tilt samples from the tilt prior (Prior); and (vi) random tilt samples from a uniform distribution of tilts (Random) (<bold>A</bold>) Human trial-by-trial errors plotted against the errors made by each of the models. Upper rows show raw errors; lower rows show bias-corrected errors. (<bold>B</bold>) Circular correlation coefficient for each of the models considered in (A). (<bold>C</bold>) Choice probability for each of the models considered in (A). Here, we define choice probability as the proportion of trials in which the sign of the model error predicts the sign of the human errors. The pattern is similar to that of the circular correlation coefficient. The MMSE model based on three image cues predicts humans tilt estimation errors better than all other models. In addition to the six models shown here, we assessed a number of ad hoc models. Three single-cue models that set the tilt estimate equal to each of the single cue values (i.e., cue-gradient orientation) predict human performance more poorly than the three-cue normative model used in the main text, but better than the prior or the random model. A model that averages the single-cue values (with equal weights) predicts human estimation better than the single-cue MMSE estimators, but worse than the three-cue normative model used in the main text.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig6-figsupp2-v2"/></fig></fig-group><p>However, because both human and model observers produced biased estimates with natural stimuli (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), it is possible that the biases are responsible for the error correlations. To remove the possible influence of bias, we computed the bias-corrected error. On each trial, we subtracted the observer bias at each groundtruth tilt <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mover><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> from the raw error. Human and model bias-corrected errors are also significantly correlated (<xref ref-type="fig" rid="fig6">Figure 6C,D</xref>). The human-human correlation (dashed line in <xref ref-type="fig" rid="fig6">Figure 6B,D</xref>; see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>) sets an upper bound for the model-human correlation. The model-human correlation approaches this bound in some cases. Other measures of trial-by-trial similarity (e.g., choice probability; <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2C</xref>) yield similar conclusions. These results show that natural stimulus variation at a given groundtruth tilt causes similar response variation in human observers and the model observer.</p><p>To ensure that the predictive power of the model observer is not trivial, we developed multiple alternative models. All other models predict human performance more poorly (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). Our results do not rule out the possibility that another model could predict human performance better, but the current MMSE estimator establishes a strong benchmark against which other models must be compared.</p><p>Thus, the normative model, without fitting to the human data, accounts for human tilt estimates at the level of the summary statistics (<xref ref-type="fig" rid="fig2">Figure 2D–F</xref>), the conditional distributions (<xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref>), and the trial-by-trial errors (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Together, this evidence suggests that the human visual system’s perceptual processes and the normative model’s computations are making similar use of similar information. We conclude that the human visual system makes near-optimal use of the available information in natural stimuli for estimating 3D surface tilt.</p></sec><sec id="s2-3"><title>Performance-impacting stimulus factors: Slant, distance, and natural depth variation</title><p>In our experiment, natural and artificial stimuli were matched on many dimensions: tilt, slant, distance, and luminance contrast. These stimulus factors are commonly controlled in perceptual experiments. Consistent with previous reports, slant and distance had a substantial impact on estimation error (<xref ref-type="bibr" rid="bib49">Watt et al., 2005</xref>) with both natural and artificial stimuli (<xref ref-type="fig" rid="fig7">Figure 7</xref>). (Luminance contrast had little impact on performance.)</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.31448.018</object-id><label>Figure 7.</label><caption><title>The effect of slant and distance on tilt estimation error in natural stimuli for human and model observers.</title><p>(<bold>A</bold>) Absolute error decreases linearly with slant. Estimation error decreases approximately 20° as slant changes from 30° to 60°. (<bold>B</bold>) Absolute error increases linearly with distance. Estimation error increases approximately 15° as distance increases from 3 m to 30 m.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig7-v2"/></fig><p>Even after controlling for these stimulus dimensions, tilt estimation with natural stimuli is considerably poorer than tilt estimation with artificial stimuli. Other factors must therefore account for the differences. What are they? In our experiment, each artificial scene consisted of a single planar surface. Natural scenes contain natural depth variation (i.e., complex surface structure); some surfaces are approximately planar, some are curved or bumpy. How are differences in surface planarity related to differences in performance with natural and artificial scenes? To quantify the departure of surface structure from planarity, we defined local <italic>tilt variance</italic> as the circular variance of the groundtruth tilt values in the central 1° area of each stimulus. Then, we examined how estimation error changes with tilt variance.</p><p>First, we found that estimation error increases linearly with tilt variance for both human and model observers (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). Unfortunately, tilt variance co-varies with groundtruth tilt — cardinal tilts tend to have lower tilt variance than oblique tilts, presumably because of the ground plane (<xref ref-type="fig" rid="fig8">Figure 8B</xref>)— which means that the effect of groundtruth tilt could be misattributed to tilt variance. Hence, we repeated the analysis of overall error separately for cardinal tilts alone and for oblique tilts alone. We found that the effect of tilt variance is independent of groundtruth tilt (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). Thus, like slant and distance, tilt variance (i.e., departure from surface planarity) is one of several key stimulus factors that impacts tilt estimation performance.</p><fig-group><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.31448.019</object-id><label>Figure 8.</label><caption><title>The effect of tilt variance on tilt estimation error.</title><p>(<bold>A</bold>) Absolute error increases linearly with tilt variance. Estimation error increases approximately 25° across the range of tilt variance. Artificial stimuli were perfectly planar and had zero local depth variation; hence the individual data point at zero tilt variance. Solid curve shows the model prediction. (<bold>B</bold>) Tilt variance co-varies with groundtruth tilt. Oblique tilts tend to be associated with less planar (i.e., more bumpy) regions of natural scenes. (Tilt variance was computed in 15° wide bins.) (<bold>C</bold>) Same as (A) but conditional on whether groundtruth tilts are cardinal (red, 0° ± 22.5° or 90° ± 22.5°) or oblique (blue, 45° ± 22.5° or 135° ± 22.5°, shaded areas in [B]). Data points are spaced unevenly because they are grouped in quantile bins, such that each data point represents an equal number of stimuli. The solid curves represent the errors of the MMSE estimator for cardinal (red) and oblique (blue) groundtruth tilts. The normative model predicts performance in all cases.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig8-v2"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31448.020</object-id><label>Figure 8—figure supplement 1.</label><caption><title>Tilt estimation performance with near-planar natural stimuli .</title><p>We analyzed performance for the subset of natural stimuli with the lowest tilt variance (tilt variance<inline-formula><mml:math id="inf44"><mml:mo>≤</mml:mo></mml:math></inline-formula>0.2; bottom 25%). For these near-planar stimuli, the mean tilt estimation error is 16.9°, slightly lower than the mean error with artificial stimuli (18.1°; see <xref ref-type="fig" rid="fig7">Figure 7C</xref>). (<bold>A</bold>) Raw responses. (<bold>B</bold>) Histogram of estimates. The dashed curve shows the distribution of groundtruth tilts in this subset of stimuli, which is not uniform. Unlike with planar artificial stimuli (c.f., <xref ref-type="fig" rid="fig2">Figure 2D</xref>), the histogram of human responses to near-planar natural stimuli over-represents the cardinal tilts relative to the frequency of the presented stimuli. The normative model nicely predicts the histogram of human estimates (solid curve). (<bold>C</bold>) Mean estimates as a function of groundtruth tilt. (<bold>D</bold>) Estimate variance as a function of groundtruth tilt. The mean and variance functions are different from the mean and variance functions yielded by planar artificial stimuli. Thus, tilt variance is not solely responsible for the differences between performance with natural and artificial stimuli. (<bold>E</bold>) Tilt estimation errors for different groundtruth tilts. Errors with near-planar natural stimuli (gray symbols) are substantially different than estimation errors with planar artificial stimuli (black symbols). The error distributions shown here are noisier than the plots in <xref ref-type="fig" rid="fig3">Figure 3</xref>, because there are comparatively fewer stimuli with low tilt variance. Still, the model (gray curves) does a reasonable job at predicting the distributions. Indeed, for near-planar natural stimuli, all three humans observers show significant trial-by-trial raw error correlations with the model, and two of three observers show significant trial-by-trial bias-corrected error correlations with the model.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig8-figsupp1-v2"/></fig></fig-group><p>Second, we found that for near-planar natural stimuli, average estimation error with natural and artificial stimuli are closely matched (left-most points in <xref ref-type="fig" rid="fig8">Figure 8A</xref>). Does this result mean that tilt variance accounts for <italic>all</italic> performance differences between natural and artificial stimuli? No. Performance with near-planar natural stimuli is still substantially different from performance with artificial stimuli (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). In addition, individual human and model trial-by-trial estimation errors are still correlated for the near-planar natural stimuli. Furthermore, the patterns of human performance with natural stimuli are robust across a wide range of tilt variance. <xref ref-type="fig" rid="fig9">Figure 9</xref> shows the summary statistics (estimate counts, means, and variances; cf. <xref ref-type="fig" rid="fig2">Figure 2D–F</xref>) for multiple different tilt variances of human observers. Model performance is also similarly robust to tilt variance (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>).</p><fig-group><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.31448.021</object-id><label>Figure 9.</label><caption><title>Robustness of performance measures to tilt variance.</title><p>Human tilt estimation performance with natural stimuli for five tilt variance quintiles (colors). The quintile centers are at 0.12, 0.33, 0.55, 0.76, and 0.97, respectively. (<bold>A</bold>) Estimate count ratio (i.e., the ratio of estimated to presented tilt) at each tilt. With near-planar natural stimuli, cardinal tilts are still estimated much more frequently than with planar artificial stimuli. (<bold>B</bold>) Estimate means. (<bold>C</bold>) The variance of estimates. Except with the highest tilt variance stimuli, the patterns of mean and variance with natural stimuli hold across tilt variances, except for natural stimuli with the highest tilt variance.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig9-v2"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31448.022</object-id><label>Figure 9—figure supplement 1.</label><caption><title>Tilt estimation performance of the normative model with natural stimuli for five tilt variance quintiles.</title><p>The quintile centers are at 0.12, 0.33, 0.55, 0.76, and 0.97, respectively. (<bold>A</bold>) Estimate count ratio (i.e., the ratio of estimated to presented tilts) at each tilt. (<bold>B</bold>) Estimate means. (<bold>C</bold>) Estimate variances. The patterns of estimate counts, estimate means, and estimates variances are robust to tilt variance except at the very highest tilt variances.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-31448-fig9-figsupp1-v2"/></fig></fig-group><p>We conclude that although tilt variance is an important performance-modulating factor, it is not the only factor responsible for performance differences with natural and artificial stimuli. Other factors must be responsible. Understanding these other factors is an important direction for future work.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Estimating 3D surface orientation requires the estimation of both slant and tilt. The current study focuses on tilt estimation. We quantify performance in natural scenes and report that human tilt estimates are often neither accurate nor precise. To connect our work to the classic literature, we matched artificial and natural stimuli on the stimulus dimensions that are controlled most often in typical experiments. The comparison revealed systematic performance differences. The detailed patterns of human performance are predicted, without free parameters to fit the data, by a normative model that is grounded in natural scene statistics and that makes the best possible use of the available image information. Importantly, this model is distinguished from many models of mid-level visual tasks because it is ‘image computable’; that is, it takes image pixels as input and produces tilt estimates as output. Together, the current experiment and modeling effort contributes to a broad goal in vision and visual neuroscience research: to generalize our understanding of human vision from the lab to the real world.</p><sec id="s3-1"><title>Generality of conclusions and future directions</title><sec id="s3-1-1"><title>Influence of full-field viewing</title><p>The main experiment examined tilt estimation performance for small patches of 3D natural scenes (1° and 3° of visual angle). Does tilt estimation performance improve substantially with full-field viewing of the 3D natural scenes? We re-ran the experiment with full-field viewing (36° x 21°; see <xref ref-type="fig" rid="fig1">Figure 1B</xref> for an example full-field scene). We found that human performance is essentially the same (<xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>). Although it may seem surprising that full-field viewing does not substantially improve performance, it makes sense. Scene structure is correlated only over a local area. Except for the ground plane, it is unusual for surfaces to have constant orientation over large visual angles. Thus, scene locations far from the target add little information about local tilt.</p></sec><sec id="s3-1-2"><title>Influence of scale</title><p>Groundtruth surface orientation is computed from a locally planar approximation to the surface structure, but surfaces in natural scenes are generally non-planar. Hence, the area over which groundtruth tilt is computed can affect the values assigned to each surface location. The same is true of the local image cue values. We checked how sensitive our results are to the scale of the local analysis area. We recomputed groundtruth tilt for two scales and recomputed image cue values for four scales (see Materials and methods). All eight combinations of scales yield the same qualitative pattern of results.</p></sec><sec id="s3-1-3"><title>Influence of gaze angle</title><p>The statistics of local surface orientation change with elevation in natural scenes (<xref ref-type="bibr" rid="bib1">Adams et al., 2016</xref>; <xref ref-type="bibr" rid="bib53">Yang and Purves, 2003b</xref>). In our study, scene statistics were computed from range scans and stereo-images (36° x 21° field-of-view) that were captured from human eye height with earth parallel gaze (<xref ref-type="bibr" rid="bib11">Burge et al., 2016</xref>). Different results may characterize other viewing situations, a possibility that could be evaluated in future work. However, the vast majority of eye movements in natural scenes are smaller than 10° (<xref ref-type="bibr" rid="bib26">Land and Hayhoe, 2001</xref>; <xref ref-type="bibr" rid="bib37">Pelz and Rothkopf, 2007</xref>; <xref ref-type="bibr" rid="bib14">Dorr et al., 2010</xref>). Hence, the results presented here are likely to be representative of an important subset of conditions that occur in natural viewing.</p></sec><sec id="s3-1-4"><title>Influence of internal noise</title><p>We examined how well the normative model (i.e., MMSE estimator) predicts human performance with artificial stimuli. The model nicely predicts the unbiased pattern of human estimate means. However, the model predicts estimate variances that are lower than the human estimate variances that we observed (although the predicted and observed patterns are consistent). We do not yet understand the reason for this discrepancy. One possibility is that the normative model used here does not explicitly model how internal noise affects human performance. In natural scenes, natural stimulus variability may swamp internal noise and be the controlling source of uncertainty. But with artificial stimuli, an explicit model of internal noise may be required to account quantitatively for the variance of human performance. Determining the relative importance of natural stimulus variability and internal noise is an important topic for future work.</p></sec><sec id="s3-1-5"><title>Influence of sampling error</title><p>The natural stimuli presented in the experiment were chosen via constrained random sampling (see Materials and methods). Random stimulus sampling increases the likelihood that the reported performance levels are representative of generic natural scenes. One potential concern is that the relatively small number of unique stimuli that can be practically used in an experiment (e.g., n = 3600 in this experiment) precludes a full exploration of the space of optimal estimates (see <xref ref-type="fig" rid="fig5">Figure 5B</xref>). Fortunately, the tilt estimates from the normative model change smoothly with image cue values. Systematic sparse sampling should thus be sufficient to explore the space. To rigorously determine the influence of each cue on performance, future parametric studies should focus on the role of particular image cue combinations and other important stimulus dimensions such as tilt variance.</p></sec><sec id="s3-1-6"><title>Influence of non-optimal cues</title><p>Although the three local image cues used by the normative model are widely studied and commonly manipulated, there is no guarantee that they are the most informative cues in natural scenes. Automatic techniques could be used to find the most informative cues for the task (<xref ref-type="bibr" rid="bib19">Geisler et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Burge and Jaini, 2017</xref>;<xref ref-type="bibr" rid="bib22">Jaini and Burge, 2017</xref>). These techniques have proven useful for other visual estimation tasks with natural stimuli (<xref ref-type="bibr" rid="bib5">Burge and Geisler, 2011</xref><xref ref-type="bibr" rid="bib6">Burge and Geisler, 2012</xref><xref ref-type="bibr" rid="bib7">, 2014</xref><xref ref-type="bibr" rid="bib8">, 2015</xref>). However, in the current task, we speculate that different local cues are unlikely to yield substantially better performance (<xref ref-type="bibr" rid="bib11">Burge et al., 2016</xref>). Also, given the similarities between human and model observer performance, any improved ability to predict human performance is likely to be modest at best. Nevertheless, the only way to be certain is to check.</p></sec><sec id="s3-1-7"><title>3D surface orientation estimation</title><p>The estimation of the 3D structure of the environment is aided by the joint estimation of tilt and slant (Marr’s ‘2.5D sketch’) (<xref ref-type="bibr" rid="bib31">Marr, 1982</xref>). Although we have shown that human and model tilt estimation performance are systematically affected by surface slant (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), the current work only addresses the human ability to estimate unsigned tilt. We have not yet explicitly modeled how humans estimate signed tilt, how humans estimate slant, or how humans jointly estimate slant and tilt. We will attack these problems in the future.</p></sec></sec><sec id="s3-2"><title>Cue-combination with and without independence assumptions</title><p>The standard approach to modeling cue-combination, sometimes known as maximum likelihood estimation, includes a number of assumptions: a squared error cost function, cue independence, unbiased Gaussian-distributed single cue estimates, and a flat or uninformative prior (<xref ref-type="bibr" rid="bib15">Ernst and Banks, 2002</xref>) (but see [<xref ref-type="bibr" rid="bib36">Oruç et al., 2003</xref>]). The approach used here (normative model; see <xref ref-type="fig" rid="fig5">Figure 5</xref>) assumes only a squared error cost function, and is guaranteed to produce the Bayes optimal estimate given the image cues, regardless of the common assumptions . In natural scenes, it is often unclear whether the common assumptions hold. Methods with relatively few assumptions can therefore be powerful tools for establishing principled predictions. We have not yet fully investigated how the image cues are combined in tilt estimation, but we have conducted some preliminarily analyses. For example, a simple average of the single-cue estimates (each based on luminance, texture, or disparity alone) underperforms the three-cue normative model. This result is not surprising given that the individual cues are not independent, that the single cue estimates do not follow Gaussian distribution, and that the tilt prior is not flat. However, the current study is not specifically designed to examine the details of cue combination in tilt estimation. To examine cue-combination in this task rigorously, a parametric stimulus-sampling paradigm should be employed, a topic that will be explored in future work.</p></sec><sec id="s3-3"><title>Local and global tilt estimation</title><p>A grand problem in perception and neuroscience research is to understand how local estimates are grouped into more accurate global estimates. We showed that local tilt estimates are unbiased predictors of groundtruth tilt and have nearly equal reliability (<xref ref-type="fig" rid="fig4">Figure 4</xref>). This result implies that optimal spatial pooling of the local estimates may be relatively simple. Assuming statistical independence (i.e., naïve Bayes), optimal spatial pooling is identical to a simple linear combination of the local estimates: the straight average of <inline-formula><mml:math id="inf45"><mml:mi>N</mml:mi></mml:math></inline-formula> local estimates <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Of course, local groundtruth tilts and estimates are spatially correlated, so the independence assumption will not be strictly correct. However, the spatial correlations could be estimated from the database and incorporated into the computations. Our work thus lays a strong empirically grounded foundation for the investigation of local-global processing in surface orientation estimation.</p></sec><sec id="s3-4"><title>Behavioral experiments with natural images</title><p>In classic studies of surface orientation perception, stimuli are usually limited in at least one of two important respects. If the stimuli are artificial (e.g., computer-graphics generated), groundtruth surface orientation is known but lighting conditions and textures are artificial, and it is uncertain whether results obtained with artificial stimuli will generalize to natural stimuli. If the stimuli are natural (e.g., photographs of real scenes), groundtruth surface orientation is typically unknown which complicates the evaluation of the results. The experiments reported here used natural stereo-images with laser-based measurements of groundtruth surface orientation, and artificial stimuli with tilt, slant, distance, and contrast matched to the natural stimuli. This novel design allows us to relate our results to the classic literature, to determine the generality of results with both natural and artificial stimuli and to isolate performance-controlling differences between the stimuli. In particular, we found that tilt variance is a pervasive performance-altering feature of natural scenes that is not explicitly considered in most investigations. The human visual system must nevertheless contend with tilt variance in natural viewing. We speculate that characterizing its impact is likely to be fundamental for understanding 3D surface orientation estimation in the real-world, just as characterizing the impact of local luminance contrast has been important for understanding how humans detect spatial patterns in noise (<xref ref-type="bibr" rid="bib12">Burgess et al., 1981</xref>).</p></sec><sec id="s3-5"><title>Perception and the internalization of natural scene statistics</title><p>The current study is the latest in a series of reports that have attempted, with ever increasing rigor, to link properties of perception to the statistics of natural images and scenes. Our contribution extends previous work in several respects. First, previous work demonstrated similarity between human and model performance only at the level of summary statistics (<xref ref-type="bibr" rid="bib20">Girshick et al., 2011</xref>; <xref ref-type="bibr" rid="bib4">Burge et al., 2010b</xref>; <xref ref-type="bibr" rid="bib50">Weiss et al., 2002</xref>; <xref ref-type="bibr" rid="bib43">Stocker and Simoncelli, 2006</xref>). We demonstrate that a principled model, operating directly on image data, predicts the summary statistics, the distribution of estimates, and the trial-by-trial errors. Second, previous work showed that human observers behave as if their visual systems have encoded the task-relevant statistics of 2D natural images (<xref ref-type="bibr" rid="bib20">Girshick et al., 2011</xref>). We show that human observers behave as if they have properly encoded the task-relevant joint statistics of 2D natural images and the 3D properties of natural scenes (also see (<xref ref-type="bibr" rid="bib4">Burge et al., 2010b</xref>)). Third, previous work tested and modeled human performance with artificial stimuli only (<xref ref-type="bibr" rid="bib20">Girshick et al., 2011</xref>; <xref ref-type="bibr" rid="bib4">Burge et al., 2010b</xref>; <xref ref-type="bibr" rid="bib50">Weiss et al., 2002</xref>; <xref ref-type="bibr" rid="bib43">Stocker and Simoncelli, 2006</xref>). We test human performance with both natural and artificial stimuli. The dramatic, but lawful, changes in performance with natural stimuli highlight the importance of studies with the stimuli that visual systems evolved to process.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Apparatus</title><p>The stereo images were presented with a ViewPixx Technologies ProPixx projector fitted with a 3D polarization filter. Left and right images were presented sequentially at a refresh rate of 120 Hz (60 Hz per eye) and with the same resolution of the two images (1920 × 1080 pixel). The observer was positioned 3.0 m from a 2.0 × 1.2 m Harkness Clarus 140 XC polarization maintaining projection screen. This viewing distance minimizes the potential influence of screen cues to flatness (e.g., blur). Human observers wore glasses with passive (linear) polarized filters to isolate the image for the left and right eyes. The observer’s head was stabilized with a chin- and forehead-rest. From this viewing position, the projection screen subtended 36° x 21° of visual angle. The disparity-specified distance created by this projection system matched to the distances measured in the original natural scenes. The projection display was linearized over 10 bits of gray level. The maximum luminance was 84 cd/m<sup>2</sup>. The mean luminance was set to 40% of the projection system’s maximum luminance.</p></sec><sec id="s4-2"><title>Participants</title><p>Three human observers participated in the experiment; two were authors, and one was naïve about the purpose of the experiment. Informed consent was obtained from participants before the experiment. The research protocol was approved by the Institutional Review Board of the University of Pennsylvania and is in accordance with the Declaration of Helsinki.</p></sec><sec id="s4-3"><title>Experiment</title><p>Human observers binocularly viewed a small region of a natural scene through a circular aperture (1° or 3° diameter) positioned 5 arcmin of disparity in front of the scene point along the cyclopean line of sight. Observers communicated their tilt estimate with a mouse-controlled probe. Each observer viewed 3600 unique natural stimuli (150 stimuli per tilt bin x 24 tilt bins) presented with each of two apertures in the experiment (7200 total). Natural stimuli were constrained to be binocularly visible (no half-occlusions), to have slants larger than 30°, to have distances between 5 m and 50 m, and to have contrasts between 5% and 40%. Each observer also viewed 1440 unique artificial stimuli (60 stimuli per tilt bin x 24 tilt bins) with two apertures (2880 total). Artificial stimuli (1/f noise and phase- and orientation-randomized plaids) were matched to the natural stimuli on multiple additional dimensions (tilt, slant, distance, and contrast). Natural stimuli were presented in 48 blocks of 150 trials each, and artificial stimuli were presented in 12 blocks of 240 trials each, with interleaved blocks using small and large apertures.</p></sec><sec id="s4-4"><title>Data analysis</title><p>Tilt is a circular (angular) variable. We computed the mean, variance, and error using standard circular statistics. The circular mean is defined as <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:munder><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>τ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the complex mean resultant vector. The circular variance is defined as <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Estimation error <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the circular distance between the tilt estimate and groundtruth.</p></sec><sec id="s4-5"><title>Groundtruth tilt</title><p>Groundtruth tilt <inline-formula><mml:math id="inf51"><mml:mi>τ</mml:mi></mml:math></inline-formula> is computed from the distance data (range map <inline-formula><mml:math id="inf52"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>) co-registered to each natural image in the database. We defined groundtruth tilt <inline-formula><mml:math id="inf53"><mml:mrow><mml:msup><mml:mrow><mml:mi>tan</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi>y</mml:mi></mml:msub><mml:mi>r</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the orientation of the normalized range gradient (<xref ref-type="bibr" rid="bib31">Marr, 1982</xref>). The range gradient was computed by convolving the distance data with a 2D Gaussian kernel having space constant <inline-formula><mml:math id="inf54"><mml:mi>σ</mml:mi></mml:math></inline-formula> and then taking the partial derivatives in the <inline-formula><mml:math id="inf55"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf56"><mml:mi>y</mml:mi></mml:math></inline-formula> image directions (<xref ref-type="bibr" rid="bib11">Burge et al., 2016</xref>). For the results presented in this manuscript, groundtruth tilt was computed using a space constant of <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> arcmin; doubling this space constant does not change the qualitative results. The space constants correspond to kernel sizes of ~0.25°−0.50°.</p></sec><sec id="s4-6"><title>Image cues to tilt</title><p>Image cues to tilt (disparity, luminance, and texture cues) were computed directly from the images. Like groundtruth tilt, image cues were defined as the orientation <inline-formula><mml:math id="inf58"><mml:mrow><mml:msup><mml:mrow><mml:mi>tan</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi>y</mml:mi></mml:msub><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the local disparity and luminance gradients. The local disparity gradient is computed from the disparity image, which is obtained from the left and right eye luminance images via standard local windowed cross-correlation (<xref ref-type="bibr" rid="bib11">Burge et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Tyler and Julesz, 1978</xref>; <xref ref-type="bibr" rid="bib3">Banks et al., 2004</xref>). The window for cross-correlation had the same space constant as the derivative operator that was used to compute the gradient (see below). The texture cue to tilt is defined as the orientation of the major axis of the local amplitude spectrum of the luminance image. This texture cue is non-standard (but see [<xref ref-type="bibr" rid="bib16">Fleming et al., 2011</xref>]). However, this texture cue is more accurate in natural scenes than traditional texture cues (<xref ref-type="bibr" rid="bib11">Burge et al., 2016</xref>; <xref ref-type="bibr" rid="bib13">Clerc and Mallat, 2002</xref>; <xref ref-type="bibr" rid="bib18">Galasso and Lasenby, 2007</xref>; <xref ref-type="bibr" rid="bib29">Malik and Rosenholtz, 1997</xref>; <xref ref-type="bibr" rid="bib32">Massot and Hérault, 2008</xref>). For the main results presented in this manuscript, image cues were computed from the gradients using a space constant of <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> 6 arcmin; using the space constants to <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></inline-formula>3, 6, 9, or 12 arcmin does not change the qualitative results. The space constants correspond to kernel sizes of ~0.25°−1.0°.</p></sec><sec id="s4-7"><title>Local luminance contrast</title><p>Luminance contrast was defined as the root-mean-squared luminance values within a local area weighted by a cosine window. Specifically, luminance contrast is <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf62"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> is the spatial location, <inline-formula><mml:math id="inf63"><mml:mi>W</mml:mi></mml:math></inline-formula> is a cosine window with area <inline-formula><mml:math id="inf64"><mml:mi>A</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the local mean intensity.</p></sec><sec id="s4-8"><title>The output-mapping problem</title><p>On each trial, human observers communicated their perceptual estimate <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> by making a response <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> with a mouse-controlled probe. Unfortunately, the responses are not guaranteed to equal the perceptual estimates. An output-mapping function <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> relates the response to the perceptual estimate, and an estimation function <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> relates the estimate to the groundtruth tilt of each stimulus. When responses are biased, it is hard to conclude whether the biases are due to the output-mapping function or to the estimation function. When responses are unbiased, a stronger case can be made that the human responses equal the perceptual estimates. To obtain unbiased responses <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> from biased estimates <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>≠</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the output mapping function would have to equal exactly the inverse of a biased estimation function: <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>.</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mo>.</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; this possibility seems unlikely and has no explanatory power. Thus, by Occam’s razor, unbiased responses imply unbiased output-mapping and estimation functions: <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Human responses to artificial stimuli were unbiased (<xref ref-type="fig" rid="fig2">Figure 2E</xref>), implying an unbiased output-mapping function. Assuming that the output-mapping function is stable across stimulus types, we conclude that the biased responses to natural stimuli accurately reflect biased perceptual estimates.</p></sec><sec id="s4-9"><title>Monte Carlo simulations</title><p>To determine whether the model predictions are representative of randomly sampled natural stimuli, we simulated 1000 repeats of the experiment. On each repeat, we obtained a different sample of 3600 natural stimuli (150 in each tilt bin) from which we obtained 3600 optimal estimates. The samples are used to compute 95% confidence intervals on the model predictions, which are shown as the shaded regions in <xref ref-type="fig" rid="fig3">Figure 3A</xref> and <xref ref-type="fig" rid="fig4">Figure 4A</xref>.</p></sec></sec></body><back><sec id="s6" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Informed consent was obtained from participants before the experiment. The research protocol was approved by the Institutional Review Board of the University of Pennsylvania (IRB approval protocol number: 824435) and is in accordance with the Declaration of Helsinki.</p></fn></fn-group></sec><sec id="s5" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="sdata1"><object-id pub-id-type="doi">10.7554/eLife.31448.023</object-id><label>Source data 1.</label><caption><title>Source data for full-field natural scene experiment.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-31448-data1-v2.txt"/></supplementary-material><supplementary-material id="sdata2"><object-id pub-id-type="doi">10.7554/eLife.31448.024</object-id><label>Source data 2.</label><caption><title>Source data for human performance.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-31448-data2-v2.txt"/></supplementary-material><supplementary-material id="sdata3"><object-id pub-id-type="doi">10.7554/eLife.31448.025</object-id><label>Source data 3.</label><caption><title>Source data for MMSE model.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-31448-data3-v2.txt"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.31448.026</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-31448-transrepform-v2.pdf"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname> <given-names>WJ</given-names></name><name><surname>Elder</surname> <given-names>JH</given-names></name><name><surname>Graf</surname> <given-names>EW</given-names></name><name><surname>Leyland</surname> <given-names>J</given-names></name><name><surname>Lugtigheid</surname> <given-names>AJ</given-names></name><name><surname>Muryy</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The Southampton-York Natural Scenes (SYNS) dataset: Statistics of surface attitude</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>35805</elocation-id><pub-id pub-id-type="doi">10.1038/srep35805</pub-id><pub-id pub-id-type="pmid">27782103</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Appelle</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Perception and discrimination as a function of stimulus orientation: the &quot;oblique effect&quot; in man and animals</article-title><source>Psychological Bulletin</source><volume>78</volume><fpage>266</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1037/h0033117</pub-id><pub-id pub-id-type="pmid">4562947</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banks</surname> <given-names>MS</given-names></name><name><surname>Gepshtein</surname> <given-names>S</given-names></name><name><surname>Landy</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Why is spatial stereoresolution so low?</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>2077</fpage><lpage>2089</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3852-02.2004</pub-id><pub-id pub-id-type="pmid">14999059</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname> <given-names>J</given-names></name><name><surname>Fowlkes</surname> <given-names>CC</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2010">2010b</year><article-title>Natural-scene statistics predict how the figure-ground cue of convexity affects human depth perception</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>7269</fpage><lpage>7280</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5551-09.2010</pub-id><pub-id pub-id-type="pmid">20505093</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname> <given-names>J</given-names></name><name><surname>Geisler</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Optimal defocus estimation in individual natural images</article-title><source>PNAS</source><volume>108</volume><fpage>16849</fpage><lpage>16854</lpage><pub-id pub-id-type="doi">10.1073/pnas.1108491108</pub-id><pub-id pub-id-type="pmid">21930897</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Burge</surname> <given-names>J</given-names></name><name><surname>Geisler</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Optimal defocus estimates from individual images for autofocusing a digital camera</article-title><conf-name>Proceedings of SPIE</conf-name><pub-id pub-id-type="doi">10.1117/12.912066</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname> <given-names>J</given-names></name><name><surname>Geisler</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal disparity estimation in natural stereo images</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/14.2.1</pub-id><pub-id pub-id-type="pmid">24492596</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname> <given-names>J</given-names></name><name><surname>Geisler</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Optimal speed estimation in natural image movies predicts human performance</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>7900</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms8900</pub-id><pub-id pub-id-type="pmid">26238697</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname> <given-names>J</given-names></name><name><surname>Girshick</surname> <given-names>AR</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2010">2010a</year><article-title>Visual-haptic adaptation is determined by relative reliability</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>7714</fpage><lpage>7721</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6427-09.2010</pub-id><pub-id pub-id-type="pmid">20519546</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname> <given-names>J</given-names></name><name><surname>Jaini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Accuracy maximization analysis for sensory-perceptual tasks: Computational improvements, filter robustness, and coding advantages for scaled additive noise</article-title><source>PLoS Computational Biology</source><volume>13(2)</volume><elocation-id>e1005281</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005281</pub-id><pub-id pub-id-type="pmid">28178266</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname> <given-names>J</given-names></name><name><surname>McCann</surname> <given-names>BC</given-names></name><name><surname>Geisler</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Estimating 3D tilt from local image cues in natural scenes</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.1167/16.13.2</pub-id><pub-id pub-id-type="pmid">27738702</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname> <given-names>AE</given-names></name><name><surname>Wagner</surname> <given-names>RF</given-names></name><name><surname>Jennings</surname> <given-names>RJ</given-names></name><name><surname>Barlow</surname> <given-names>HB</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Efficiency of human visual signal discrimination</article-title><source>Science</source><volume>214</volume><fpage>93</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1126/science.7280685</pub-id><pub-id pub-id-type="pmid">7280685</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clerc</surname> <given-names>M</given-names></name><name><surname>Mallat</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The texture gradient equation for recovering shape from texture</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>24</volume><fpage>536</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1109/34.993560</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dorr</surname> <given-names>M</given-names></name><name><surname>Martinetz</surname> <given-names>T</given-names></name><name><surname>Gegenfurtner</surname> <given-names>KR</given-names></name><name><surname>Barth</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Variability of eye movements when viewing dynamic natural scenes</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>28</elocation-id><pub-id pub-id-type="doi">10.1167/10.10.28</pub-id><pub-id pub-id-type="pmid">20884493</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname> <given-names>MO</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id><pub-id pub-id-type="pmid">11807554</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname> <given-names>RW</given-names></name><name><surname>Holtmann-Rice</surname> <given-names>D</given-names></name><name><surname>Bülthoff</surname> <given-names>HH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Estimation of 3D shape from image orientations</article-title><source>PNAS</source><volume>108</volume><fpage>20438</fpage><lpage>20443</lpage><pub-id pub-id-type="doi">10.1073/pnas.1114619109</pub-id><pub-id pub-id-type="pmid">22147916</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furmanski</surname> <given-names>CS</given-names></name><name><surname>Engel</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>An oblique effect in human primary visual cortex</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>535</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/75702</pub-id><pub-id pub-id-type="pmid">10816307</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Galasso</surname> <given-names>F</given-names></name><name><surname>Lasenby</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Shape from texture: fast estimation of planar surface orientation via fourier analysis</article-title><conf-name>Procedings of the British Machine Vision Conference 2007</conf-name><pub-id pub-id-type="doi">10.5244/C.21.71</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname> <given-names>WS</given-names></name><name><surname>Najemnik</surname> <given-names>J</given-names></name><name><surname>Ing</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Optimal stimulus encoders for natural tasks</article-title><source>Journal of Vision</source><volume>9</volume><fpage>17.1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1167/9.13.17</pub-id><pub-id pub-id-type="pmid">20055550</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girshick</surname> <given-names>AR</given-names></name><name><surname>Landy</surname> <given-names>MS</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1038/nn.2831</pub-id><pub-id pub-id-type="pmid">21642976</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillis</surname> <given-names>JM</given-names></name><name><surname>Watt</surname> <given-names>SJ</given-names></name><name><surname>Landy</surname> <given-names>MS</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Slant from texture and disparity cues: optimal cue combination</article-title><source>Journal of Vision</source><volume>4</volume><fpage>1</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1167/4.12.1</pub-id><pub-id pub-id-type="pmid">15669906</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaini</surname> <given-names>P</given-names></name><name><surname>Burge</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Linking normative models of natural tasks to descriptive models of neural response</article-title><source>Journal of Vision</source><volume>17(12):16</volume><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1167/17.12.16</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Knill</surname> <given-names>DC</given-names></name><name><surname>Richards</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>Perception as Bayesian Inference</source><publisher-loc>New York</publisher-loc><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511984037</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1998">1998a</year><article-title>Surface orientation from texture: ideal observers, generic observers and the information content of texture cues</article-title><source>Vision Research</source><volume>38</volume><fpage>1655</fpage><lpage>1682</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(97)00324-6</pub-id><pub-id pub-id-type="pmid">9747502</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1998">1998b</year><article-title>Ideal observer perturbation analysis reveals human strategies for inferring surface orientation from texture</article-title><source>Vision Research</source><volume>38</volume><fpage>2635</fpage><lpage>2656</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(97)00415-X</pub-id><pub-id pub-id-type="pmid">12116709</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname> <given-names>MF</given-names></name><name><surname>Hayhoe</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>In what ways do eye movements contribute to everyday activities?</article-title><source>Vision Research</source><volume>41</volume><fpage>3559</fpage><lpage>3565</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(01)00102-X</pub-id><pub-id pub-id-type="pmid">11718795</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>A</given-names></name><name><surname>Zaidi</surname> <given-names>Q</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Perception of three-dimensional shape from texture is based on patterns of oriented energy</article-title><source>Vision Research</source><volume>40</volume><fpage>217</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(99)00169-8</pub-id><pub-id pub-id-type="pmid">10793898</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>A</given-names></name><name><surname>Zaidi</surname> <given-names>Q</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Three-dimensional shape from non-homogeneous textures: carved and stretched surfaces</article-title><source>Journal of Vision</source><volume>4</volume><fpage>3</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1167/4.10.3</pub-id><pub-id pub-id-type="pmid">15595891</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malik</surname> <given-names>J</given-names></name><name><surname>Rosenholtz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Computing local surface orientation and shape from texture for curved surfaces</article-title><source>International Journal of Computer Vision</source><volume>23</volume><fpage>149</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1023/A:1007958829620</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marlow</surname> <given-names>PJ</given-names></name><name><surname>Todorović</surname> <given-names>D</given-names></name><name><surname>Anderson</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Coupled computations of three-dimensional shape and material</article-title><source>Current Biology</source><volume>25</volume><fpage>R221</fpage><lpage>R222</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.01.062</pub-id><pub-id pub-id-type="pmid">25784037</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</source><publisher-loc>New York</publisher-loc><publisher-name>W H Freeman &amp; Company</publisher-name></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massot</surname> <given-names>C</given-names></name><name><surname>Hérault</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Model of frequency analysis in the visual cortex and the shape from texture problem</article-title><source>International Journal of Computer Vision</source><volume>76</volume><fpage>165</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1007/s11263-007-0048-x</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Psychophysics with junctions in real images</article-title><source>Perception</source><volume>33</volume><fpage>1101</fpage><lpage>1127</lpage><pub-id pub-id-type="doi">10.1068/p5265</pub-id><pub-id pub-id-type="pmid">15560510</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>AP</given-names></name><name><surname>Ban</surname> <given-names>H</given-names></name><name><surname>Welchman</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Integration of texture and disparity cues to surface slant in dorsal visual cortex</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>190</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1152/jn.01055.2012</pub-id><pub-id pub-id-type="pmid">23576705</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname> <given-names>JF</given-names></name><name><surname>Todd</surname> <given-names>JT</given-names></name><name><surname>Norman</surname> <given-names>HF</given-names></name><name><surname>Clayton</surname> <given-names>AM</given-names></name><name><surname>McBride</surname> <given-names>TR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Visual discrimination of local surface structure: slant, tilt, and curvedness</article-title><source>Vision Research</source><volume>46</volume><fpage>1057</fpage><lpage>1069</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2005.09.034</pub-id><pub-id pub-id-type="pmid">16289208</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oruç</surname> <given-names>I</given-names></name><name><surname>Maloney</surname> <given-names>LT</given-names></name><name><surname>Landy</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Weighted linear cue combination with possibly correlated error</article-title><source>Vision Research</source><volume>43</volume><fpage>2451</fpage><lpage>2468</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(03)00435-8</pub-id><pub-id pub-id-type="pmid">12972395</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pelz</surname> <given-names>JB</given-names></name><name><surname>Rothkopf</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><chapter-title>Oculomotor behavior in natural and man-made environments</chapter-title><person-group person-group-type="editor"><name><surname>van Gompel</surname> <given-names>R. P. G</given-names></name><name><surname>Fischer</surname> <given-names>M</given-names></name><name><surname>Murray</surname> <given-names>W. S</given-names></name><name><surname>Hill</surname> <given-names>R. L</given-names></name></person-group><source>Eye Movements: A Window on Mind and Brain</source><pub-id pub-id-type="doi">10.1016/B978-008044980-7/50033-1</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname> <given-names>A</given-names></name><name><surname>Cowan</surname> <given-names>NJ</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The visual representation of 3D object orientation in parietal cortex</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>19352</fpage><lpage>19361</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3174-13.2013</pub-id><pub-id pub-id-type="pmid">24305830</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenholtz</surname> <given-names>R</given-names></name><name><surname>Malik</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Surface orientation from texture: isotropy or homogeneity (or both)?</article-title><source>Vision Research</source><volume>37</volume><fpage>2283</fpage><lpage>2293</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(96)00121-6</pub-id><pub-id pub-id-type="pmid">9578909</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanada</surname> <given-names>TM</given-names></name><name><surname>Nguyenkim</surname> <given-names>JD</given-names></name><name><surname>Deangelis</surname> <given-names>GC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Representation of 3-D surface orientation by velocity and disparity gradient cues in area MT</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>2109</fpage><lpage>2122</lpage><pub-id pub-id-type="doi">10.1152/jn.00578.2011</pub-id><pub-id pub-id-type="pmid">22219031</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saunders</surname> <given-names>JA</given-names></name><name><surname>Knill</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Perception of 3D surface orientation from skew symmetry</article-title><source>Vision Research</source><volume>41</volume><fpage>3163</fpage><lpage>3183</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(01)00187-0</pub-id><pub-id pub-id-type="pmid">11711141</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevens</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Slant-tilt: the visual encoding of surface orientation</article-title><source>Biological Cybernetics</source><volume>46</volume><fpage>183</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1007/BF00336800</pub-id><pub-id pub-id-type="pmid">6850004</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stocker</surname> <given-names>AA</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Noise characteristics and prior expectations in human visual speed perception</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>578</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1038/nn1669</pub-id><pub-id pub-id-type="pmid">16547513</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todd</surname> <given-names>JT</given-names></name><name><surname>Koenderink</surname> <given-names>JJ</given-names></name><name><surname>van Doorn</surname> <given-names>AJ</given-names></name><name><surname>Kappers</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Effects of changing viewing conditions on the perceived structure of smoothly curved surfaces</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>22</volume><fpage>695</fpage><lpage>706</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.22.3.695</pub-id><pub-id pub-id-type="pmid">8666959</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todd</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The visual perception of 3D shape</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>115</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.01.006</pub-id><pub-id pub-id-type="pmid">15301751</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsutsui</surname> <given-names>K</given-names></name><name><surname>Jiang</surname> <given-names>M</given-names></name><name><surname>Yara</surname> <given-names>K</given-names></name><name><surname>Sakata</surname> <given-names>H</given-names></name><name><surname>Taira</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Integration of perspective and disparity cues in surface-orientation-selective neurons of area CIP</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>2856</fpage><lpage>2867</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.6.2856</pub-id><pub-id pub-id-type="pmid">11731542</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyler</surname> <given-names>CW</given-names></name><name><surname>Julesz</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Binocular cross-correlation in time and space</article-title><source>Vision Research</source><volume>18</volume><fpage>101</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(78)90083-4</pub-id><pub-id pub-id-type="pmid">664265</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Velisavljević</surname> <given-names>L</given-names></name><name><surname>Elder</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Texture properties affecting the accuracy of surface attitude judgements</article-title><source>Vision Research</source><volume>46</volume><fpage>2166</fpage><lpage>2191</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.01.010</pub-id><pub-id pub-id-type="pmid">16504236</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watt</surname> <given-names>SJ</given-names></name><name><surname>Akeley</surname> <given-names>K</given-names></name><name><surname>Ernst</surname> <given-names>MO</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Focus cues affect perceived depth</article-title><source>Journal of Vision</source><volume>5</volume><fpage>7</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1167/5.10.7</pub-id><pub-id pub-id-type="pmid">16441189</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname> <given-names>Y</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name><name><surname>Adelson</surname> <given-names>EH</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Motion illusions as optimal percepts</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>598</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.1038/nn0602-858</pub-id><pub-id pub-id-type="pmid">12021763</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welchman</surname> <given-names>AE</given-names></name><name><surname>Deubelius</surname> <given-names>A</given-names></name><name><surname>Conrad</surname> <given-names>V</given-names></name><name><surname>Bülthoff</surname> <given-names>HH</given-names></name><name><surname>Kourtzi</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>3D shape perception from combined depth cues in human visual cortex</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>820</fpage><lpage>827</lpage><pub-id pub-id-type="doi">10.1038/nn1461</pub-id><pub-id pub-id-type="pmid">15864303</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>Z</given-names></name><name><surname>Purves</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003a</year><article-title>A statistical explanation of visual space</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>632</fpage><lpage>640</lpage><pub-id pub-id-type="doi">10.1038/nn1059</pub-id><pub-id pub-id-type="pmid">12754512</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>Z</given-names></name><name><surname>Purves</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003b</year><article-title>Image/source statistics of surfaces in natural scenes</article-title><source>Network: Computation in Neural Systems</source><volume>14</volume><fpage>371</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_14_3_301</pub-id><pub-id pub-id-type="pmid">12938763</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.31448.029</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Gallant</surname><given-names>Jack L</given-names></name><role>Reviewing Editor</role><aff id="aff3"><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;The lawful imprecision of human surface tilt estimation in natural scenes&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by Reviewing Editor Jack Gallant, and David Van Essen as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Mark Lescroart (Reviewer #1); Michael Landy (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The authors study the ability of human subjects to estimate surface tilt in natural images. They find (among other results) that humans are biased to estimate tilt at cardinal (vertical and horizontal) orientations and show that an image-computable Bayesian model makes estimates of tilt that are similar to human estimates. Both reviewers found the paper to be interesting and timely. Reviewer 2 made only minor comments, but reviewer 1 had some concerns and requested additional behavioral data. The reviewing editor is persuaded by these arguments.</p><p>Essential revisions:</p><p>1) The authors have found that variance (or noise) in tilt in the stimulus leads to less accurate estimation of tilt. However, the natural images used in this study are highly variable, so this result isn't all that surprising. The authors should analyze whether the model is predicting human performance well simply because some trials have more or less tilt variance than others. If this is the case, the result is much less interesting – variance (or noise) in a tilt <italic>should</italic> cause poorer tilt estimates. Similarly, alternative versions of the plots in <xref ref-type="fig" rid="fig3">Figure 3</xref> should be generated with low-tilt-variance scenes, to see if the bias shows up as clearly.</p><p>2) The authors should validate their results using artificial images with more naturalistic textures (e.g., 1/f). In general, the authors should try to introduce variability into the artificial images of the same kind and magnitude as the variability in the natural images.</p><p>3) The authors should perform some control experiments to verify that the results hold for stimuli larger than 3 degrees. If possible, it would be good to verify these effects in complex natural scenes.</p><p><italic>Reviewer #1:</italic></p><p>The authors study the ability of human subjects to estimate surface tilt in natural images. They find (among other results) that humans are biased to estimate tilt at cardinal (vertical and horizontal) orientations and show that an image-computable Bayesian model makes estimates of tilt that are similar to human estimates.</p><p>This is an interesting and timely question, and the study is generally well executed. The figures are nice, the writing is clear, and the dataset the authors use (if it is to be shared) seems a useful contribution. However, I have some concerns about the experimental paradigm. None of these concerns alone are fatal flaws, but when combined with some of the results, they give me doubts about the impact of the rest of the results.</p><p>First, the artificial stimuli are perhaps too simplified (very regularly textured, wholly planar). This is not representative of studies of tilt estimation: several studies have had human subjects estimate surface orientation (or related quantities) of non-planar surfaces (e.g. Todd et al., 1996; Li and Zaidi, 2000; Norman et al., 2006). The highly simple nature of the artificial stimuli here creates several obvious differences between the natural and artificial images. <xref ref-type="fig" rid="fig8">Figure 8A</xref> shows that one such difference – tilt variance, present in the natural images but not in the artificial ones – accounts for all of the difference in mean tilt estimation accuracy between artificial and natural images. Stated another way, the authors have found that variance (or noise) in tilt in the stimulus leads to less accurate estimation of tilt. Note that this is not noise in the image cues or anything else – this is variance in the exact parameter that is being estimated. I may be missing something, but this particular result (which appears to be the biggest effect in the experiment) seems wholly expected to me. So, I am unimpressed by the conclusion statement: &quot;The dramatic, but lawful, fall-off in performance with natural stimuli highlights the importance of performing studies with the stimuli visual systems evolved to process.&quot;</p><p>The large effect of tilt variance calls into question the size of other effects the authors report. <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> shows that, for natural stimuli with low tilt variance, the bias toward estimating vertical (0 and 180 degree) tilts is greatly diminished (the count ratio between estimated and true instances of vertical tilt is very near to 1). (As a side note, <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> is a critical figure and should appear in the main manuscript). It is also not clear how much tilt variance might be affecting the model's predictions of trial-to-trial errors; the authors should analyze whether the model is predicting human performance well simply because some trials have more or less tilt variance than others. If this is the case, the result is much less interesting – variance (or noise) in a tilt <italic>should</italic> cause poorer tilt estimates. Similarly, alternative versions of the plots in <xref ref-type="fig" rid="fig3">Figure 3</xref> should be generated with low-tilt-variance scenes, to see if the bias shows up as clearly.</p><p>The other striking difference between the artificial and natural images is the extreme regularity of the textures in the artificial images (at least of the plaids shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>). The authors also used 1/f noise as a texture in the artificial images – did human performance differ depending on whether the artificial stimuli were plaid or 1/f noise? In general, it seems that adding more types of variation to the artificial stimuli and assessing the effects of that variation would provide a good way to assess what sorts of variation make human performance look more like it does for natural images. I suspect that the authors plan to do this in future work, but I think it would substantially increase the impact of this work to include such data and analyses here.</p><p>Finally – I admit some disappointment with the choice of only showing 3 degree stimuli. To me, this lessens the impact of the work as well; a 3 degree image patch hardly constitutes a &quot;scene&quot;. Thus, the following conclusion statement seems a bit of a reach: &quot;We quantify performance in natural scenes and report that human tilt percepts are often neither accurate nor precise&quot;. Human estimates of tilt given full natural images (including much more context) would likely be better than the estimates reported here. I realize this is a very difficult problem, but <italic>eLife</italic> is also a broad, prestigous journal; studying tilt estimation in natural image <italic>patches</italic> may be a critical step on the way to studying tilt estimation in full scenes, but it also seems less broadly interesting.</p><p>Last, a few notes on the model: First, I am puzzled as to why the authors do not include model performance on their artificial stimuli, too. This seems to be a straightforward and easy test of the generality of the model. Second, it's not clear to me whether the &quot;estimate cube&quot; of optimal mappings between image cues and tilts is computed using some, all, or none of the same images that the subjects saw in the experiment. The authors should clarify this point.</p><p>I should note again that the concerns above are almost entirely about impact. I hesitate to reject an otherwise interesting and well-executed study on grounds that it's just not splashy enough. And there are several interesting and solid results in this paper. The fact that tilt variance is correlated with tilt angle in a large sample of natural scenes seems solidly supported and important. Modulo the questions I raised above, the MMSE model performance appears to provide a good match for human performance in the natural images. The persistent difference between errors estimating cardinal and oblique tilt, as well as the persistent bias to estimate horizontal tilt – both with matched tilt variance – are also interesting. Thus, I am on the fence about this paper, mostly because its impact seems marginal. I could be convinced to accept the paper with revisions or to reject the paper.</p><p> <italic>Reviewer #2:</italic></p><p>This is a lovely paper, showing that a nonparametric Bayesian model of tilt estimation accounts startlingly well for human behavior in a tilt-estimation task. My comments are mainly about improving the clarity, not much more.</p><p>Introduction: Many of my comments are a result of reading it in (my) natural order, i.e., your page order with diversions to the Methods when needed. So, when I got here I wondered whether the patches to be judged were centered on the display or occluded in the position in the original images. That's never stated explicitly but implied by a figure that hasn't come up yet.</p><p>Introduction: You never motivate/justify pooling over tilt sign until much, much later, and so I was surprised you threw information away from the start. I wondered about it again for <xref ref-type="fig" rid="fig3">Figure 3</xref> where, given that you provide disparity, the tilt sign ambiguity from pictorial cues should be alleviated.</p><p><xref ref-type="fig" rid="fig2">Figure 2</xref> et seq.: Why didn't you run the model on the artificial stimuli and show the model fits for those data points (or misfits, as the case may be)?</p><p>Subsection “Normative model”: The citation of <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref> here seems out of place. The analyses for this figure don't appear until the next page. Also, shouldn't all the supplementary figures be cited somewhere in the main text? I think a bunch aren't.</p><p>Subsection “Trial-by-trial Error: Is -&gt; Are.</p><p><xref ref-type="fig" rid="fig8">Figure 8B</xref>:Exactly what bin cutoffs did you use for blue vs. red here?</p><p>Subsection “Effect of Natural Depth Variation”: artificially -&gt; artificial.</p><p>Subsection “Generality of Conclusions and Future Directions”: our -&gt; are.</p><p>Subsection “Cue-combination with and without independence assumptions”: This reference to <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>, since the pooled/averaged model is not in the figure, but merely mentioned in its legend.</p><p>Subsectiion “Experiment”: More details please: What's your definition of contrast, refer to the figure to state what part of the patch they were supposed to judge, were the judged bins over 180 or 360 degrees (I only say this because <xref ref-type="fig" rid="fig1">Figure 1</xref> leads the reader to believe that it's over 180 degrees only). Please show and give details about the two types of artificial stimuli. Do responses to them differ from one another?</p><p>Subsection “Groundtruth tilt”: &quot;atan2&quot; is MATLAB notation, I'd think. You might want to say what you mean there.</p><p>Subsection “Image cues to tilt”: I'd like more detail here as well. The disparity cue must be based on a definition of &quot;local&quot; and a restriction of cross-correlation shifts. The disparity gradient requires a scale. The disparity gradient doesn't have a tilt sign ambiguity, but the texture cue does. I'm not sure &quot;patch size at half height&quot; won't confuse people (you don't mean the viewed patch, but rather the patch after multiplying by the Gaussian window).</p><p><xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>: Actually, I'm rather surprised that the single-cue models (especially those other than disparity) perform as well as they do. It worries me that there are weird regularities in your database. You never say what your definition of luminance is exactly, but why should tilt be dependent on luminance? How are each cue binned? Are the same bins used for the single-cue models, so that those models have vastly smaller measured parameters?</p><p><xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref> legend, line 12: &quot;…but better than the prior or…&quot;.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.31448.030</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The authors have found that variance (or noise) in tilt in the stimulus leads to less accurate estimation of tilt. However, the natural images used in this study are highly variable, so this result isn't all that surprising. The authors should analyze whether the model is predicting human performance well simply because some trials have more or less tilt variance than others.</p></disp-quote><p>We agree that this is an important issue. The estimate means and variances as a function of groundtruth tilt are largely robust to changes in tilt variance (see new <xref ref-type="fig" rid="fig9">Figure 9</xref>). We also now show that the model does predict the distribution of human errors with near-planar natural stimuli (new addition to <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). These results indicate that model’s success at predicting performance is not due simply to the fact that natural stimuli had higher tilt variance on average.</p><disp-quote content-type="editor-comment"><p>If this is the case, the result is much less interesting – variance (or noise) in a tilt should cause poorer tilt estimates.</p></disp-quote><p>We agree that if our model predicted nothing other than an increase in overall estimate variance, the result would not be particularly interesting. However, the model predicts the pattern of estimate means and variances as a function of groundtruth tilt and the distributions of tilt errors. These are all non-trivial predictions.</p><disp-quote content-type="editor-comment"><p>Similarly, alternative versions of the plots in <xref ref-type="fig" rid="fig3">Figure 3</xref> should be generated with low-tilt-variance scenes, to see if the bias shows up as clearly.</p></disp-quote><p>We have done so. We analyzed the subset of near-planar natural stimuli in the experimental dataset and present the results in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1E</xref>. With low-tilt variance (i.e. near-planar) locations in natural scenes (i) the bias persists, (ii) human performance continues to be substantially different with natural and artificial stimuli, and (iii) human performance continues to be predicted by the model.</p><disp-quote content-type="editor-comment"><p>2) The authors should validate their results using artificial images with more naturalistic textures (e.g., 1/f). In general, the authors should try to introduce variability into the artificial images of the same kind and magnitude as the variability in the natural images.</p></disp-quote><p>We now present results (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>) for the 1/f noise textures and two plaid textures separately (see below). The three textures yield similar results although the 1/f textures yield estimates with slightly higher variance. The main point to take from these plots is that the planar artificial stimuli produce very different patterns of results from the natural stimuli (e.g., the lack of bias and the different pattern of variances). Performance with these artificial stimuli is notably different than performance with the near-planar subset of natural stimuli.</p><disp-quote content-type="editor-comment"><p>3) The authors should perform some control experiments to verify that the results hold for stimuli larger than 3 degrees. If possible, it would be good to verify these effects in complex natural scenes.</p></disp-quote><p>As requested, we re-ran our experiment without apertures so that human observers had full-field views of each scene (36ºx21º). We added new <xref ref-type="fig" rid="fig2s5">Figure 2——figure supplement 5</xref> that shows the estimate counts, means, variances, and conditional distributions for full-field viewing. The data with full-field viewing verifies that the results in the original experiment hold for stimuli larger than 3 degrees.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>The authors study the ability of human subjects to estimate surface tilt in natural images. They find (among other results) that humans are biased to estimate tilt at cardinal (vertical and horizontal) orientations and show that an image-computable Bayesian model makes estimates of tilt that are similar to human estimates.</p><p>This is an interesting and timely question, and the study is generally well executed. The figures are nice, the writing is clear, and the dataset the authors use (if it is to be shared) seems a useful contribution. However, I have some concerns about the experimental paradigm. None of these concerns alone are fatal flaws, but when combined with some of the results, they give me doubts about the impact of the rest of the results.</p><p>First, the artificial stimuli are perhaps too simplified (very regularly textured, wholly planar). This is not representative of studies of tilt estimation: several studies have had human subjects estimate surface orientation (or related quantities) of non-planar surfaces (e.g. Todd et al., 1996; Li and Zaidi, 2000; Norman et al., 2006).</p></disp-quote><p>We agree that not all studies have been performed with planar surfaces, but most have. We have clarified the writing to make this point more clear and we now cite Norman et al. (2006), an unfortunate omission in our original submission.</p><disp-quote content-type="editor-comment"><p>The highly simple nature of the artificial stimuli here creates several obvious differences between the natural and artificial images. <xref ref-type="fig" rid="fig8">Figure 8A</xref> shows that one such difference – tilt variance, present in the natural images but not in the artificial ones – accounts for all of the difference in mean tilt estimation accuracy between artificial and natural images. Stated another way, the authors have found that variance (or noise) in tilt in the stimulus leads to less accurate estimation of tilt. Note that this is not noise in the image cues or anything else – this is variance in the exact parameter that is being estimated. I may be missing something, but this particular result (which appears to be the biggest effect in the experiment) seems wholly expected to me. So, I am unimpressed by the conclusion statement: &quot;The dramatic, but lawful, fall-off in performance with natural stimuli highlights the importance of performing studies with the stimuli visual systems evolved to process.&quot; The large effect of tilt variance calls into question the size of other effects the authors report.</p></disp-quote><p>It is true that the absolute error with natural stimuli is smaller when there is no tilt variance; and we agree that this effect is not particularly surprising. But even when natural stimuli have low tilt variance (i.e., are near-planar), performance with natural and artificial stimuli is not the same. Thus, tilt variance alone cannot explain all differences in human performance with natural and artificial stimuli. See new <xref ref-type="fig" rid="fig9">Figure 9</xref> and modified <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>.</p><p>That being said, tilt variance is a pervasive performance impacting stimulus factor in natural scenes that has not been systematically characterized before, and we think it important to do so. We speculate that characterizing the impact of tilt variance is likely to be fundamental understanding 3D surface orientation estimation in the real-world, just as characterizing the impact of local luminance contrast has been important for understanding how humans detect spatial patterns in noisy uncertain backgrounds (Burgess et al., 1981).</p><p>We have reorganized the text to make more clear exactly what we are and what we are not claiming. We hope our efforts to improve clarity make for easier reading.</p><disp-quote content-type="editor-comment"><p><xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> shows that, for natural stimuli with low tilt variance, the bias toward estimating vertical (0 and 180 degree) tilts is greatly diminished (the count ratio between estimated and true instances of vertical tilt is very near to 1). (As a side note, <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> is a critical figure and should appear in the main manuscript). It is also not clear how much tilt variance might be affecting the model's predictions of trial-to-trial errors; the authors should analyze whether the model is predicting human performance well simply because some trials have more or less tilt variance than others. If this is the case, the result is much less interesting – variance (or noise) in a tilt should cause poorer tilt estimates. Similarly, alternative versions of the plots in <xref ref-type="fig" rid="fig3">Figure 3</xref> should be generated with low-tilt-variance scenes, to see if the bias shows up as clearly.</p></disp-quote><p>The count ratio varies between ~0.5 and ~2.0 for low-tilt-variance natural stimuli (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1B</xref>). Across all natural stimuli, the count ratio varied between ~0.5 and ~3.0. Also, the pattern of estimate bias persists with low-tilt-variance stimuli (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1C</xref>). More importantly, we now show the distributions of estimation error for natural stimuli with low tilt variance (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1E</xref>). There remain substantial differences in human performance with tilt-variance-matched natural and artificial stimuli.</p><p>All three human observers show significant trial-by-trial raw error correlations with the model for near-planar natural stimuli. Two of three observers show significant trial-by-trial bias corrected error correlations with the model for near-planar natural stimuli.</p><p>We have substantially re-written the Results section “Performance-impacting Stimulus Factors: Slant, Distance, &amp; Natural Depth Variation” to make all these points more clear. We have included a new <xref ref-type="fig" rid="fig9">Figure 9</xref>, which shows that, although there is an effect of tilt variance, the basic performance patterns are robust to changes in tilt variance. We respectfully decline to move <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1i</xref>nto the main text as we think it breaks up the flow. We hope the changes we have made address the core of the reviewer’s concern.</p><disp-quote content-type="editor-comment"><p>The other striking difference between the artificial and natural images is the extreme regularity of the textures in the artificial images (at least of the plaids shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>). The authors also used 1/f noise as a texture in the artificial images – did human performance differ depending on whether the artificial stimuli were plaid or 1/f noise? In general, it seems that adding more types of variation to the artificial stimuli and assessing the effects of that variation would provide a good way to assess what sorts of variation make human performance look more like it does for natural images. I suspect that the authors plan to do this in future work, but I think it would substantially increase the impact of this work to include such data and analyses here.</p></disp-quote><p>Human performance did not differ appreciably depending on the types of texture in artificial stimuli. Please see the plot above in Essential Revision #2. The variance of tilt estimates is slightly higher with 1/f stimuli, but the qualitative patterns are consistent across all three artificial stimulus textures.</p><p>We agree that studying the effects of the variation is an important question to address and we plan to do in the future. We believe a proper treatment of this question is a major undertaking in its own right and deserves its own manuscript.</p><disp-quote content-type="editor-comment"><p>Finally – I admit some disappointment with the choice of only showing 3 degree stimuli. To me, this lessens the impact of the work as well; a 3 degree image patch hardly constitutes a &quot;scene&quot;. Thus, the following conclusion statement seems a bit of a reach: &quot;We quantify performance in natural scenes and report that human tilt percepts are often neither accurate nor precise&quot;. Human estimates of tilt given full natural images (including much more context) would likely be better than the estimates reported here. I realize this is a very difficult problem, but eLife is also a broad, prestigous journal; studying tilt estimation in natural image patches may be a critical step on the way to studying tilt estimation in full scenes, but it also seems less broadly interesting.</p></disp-quote><p>Please see above Essential Revision #3. We have collected a new dataset with full scene stimuli, as the reviewer requested. We show the data from this new experiment in new figure (<xref ref-type="fig" rid="fig2">Figure 2-supplement 5</xref>) and discuss the result in a new Discussion section titled “Influence of full-field viewing”. Results are essentially unchanged with full-field viewing. While this result may seem surprising at first, it makes sense. Scene structure is correlated only over a relatively local area. Except for the ground plane, it is fairly unusual for surfaces to have a constant orientation over very large visual angles. Thus, scene locations farther than 3º are likely to add little additional information.</p><disp-quote content-type="editor-comment"><p>Last, a few notes on the model: First, I am puzzled as to why the authors do not include model performance on their artificial stimuli, too. This seems to be a straightforward and easy test of the generality of the model.</p></disp-quote><p>With artificial stimuli, the human estimate means are nicely predicted by the model, but the human estimates have higher circular variance than the model predicts, although the pattern is similar (see figure below). We do not yet understand the reason for this discrepancy, but we suspect that an explicit model of internal noise will be required. This is a topic we would like to reserve for future work.</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.31448.028</object-id><label>Author response image 1.</label><caption><title>Model performance on artificial stimuli</title></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-31448-resp-fig1-v2"/></fig><disp-quote content-type="editor-comment"><p>Second, it's not clear to me whether the &quot;estimate cube&quot; of optimal mappings between image cues and tilts is computed using some, all, or none of the same images that the subjects saw in the experiment. The authors should clarify this point.</p></disp-quote><p>The estimate cube includes the same scene locations that were presented in the experiment. However, the estimate cube was constructed from approximately 1 billion samples. Only 3600 of these samples were used as experimental stimuli, a negligible fraction of this total. Excluding the 3600 unique experimental stimuli from the estimate cube has no measurable influence on the model predictions. We now make this point in the <xref ref-type="fig" rid="fig5">Figure 5</xref> caption.</p><disp-quote content-type="editor-comment"><p>I should note again that the concerns above are almost entirely about impact. I hesitate to reject an otherwise interesting and well-executed study on grounds that it's just not splashy enough. And there are several interesting and solid results in this paper. The fact that tilt variance is correlated with tilt angle in a large sample of natural scenes seems solidly supported and important. Modulo the questions I raised above, the MMSE model performance appears to provide a good match for human performance in the natural images. The persistent difference between errors estimating cardinal and oblique tilt, as well as the persistent bias to estimate horizontal tilt – both with matched tilt variance – are also interesting. Thus, I am on the fence about this paper, mostly because its impact seems marginal. I could be convinced to accept the paper with revisions or to reject the paper.</p><p>Reviewer #2:</p><p>This is a lovely paper, showing that a nonparametric Bayesian model of tilt estimation accounts startlingly well for human behavior in a tilt-estimation task. My comments are mainly about improving the clarity, not much more.</p><p>Introduction: Many of my comments are a result of reading it in (my) natural order, i.e., your page order with diversions to the Methods when needed. So, when I got here I wondered whether the patches to be judged were centered on the display or occluded in the position in the original images. That's never stated explicitly but implied by a figure that hasn't come up yet.</p></disp-quote><p>Thanks. We now state that the patches were displayed in their original positions.</p><disp-quote content-type="editor-comment"><p>Introduction: You never motivate/justify pooling over tilt sign until much, much later, and so I was surprised you threw information away from the start. I wondered about it again for <xref ref-type="fig" rid="fig3">Figure 3</xref> where, given that you provide disparity, the tilt sign ambiguity from pictorial cues should be alleviated.</p></disp-quote><p>As you point out, it is true that the disparity cue can alleviate tilt sign ambiguity. We are currently working on a new model that makes use of it. Also, even though the disparity cue is provided in the stimuli, there are some sign confusions in the human response (e.g., see a weak pattern of data points on the lower-right quadrant in the <xref ref-type="fig" rid="fig2">Figure 2C</xref> scatter plot). These sign confusions complicate some of the data analyses. Furthermore, all three humans are remarkably consistent in their unsigned tilt estimation performance. That said these are important issues that we will tackle in our next piece of work.</p><disp-quote content-type="editor-comment"><p><xref ref-type="fig" rid="fig2">Figure 2</xref> et seq.: Why didn't you run the model on the artificial stimuli and show the model fits for those data points (or misfits, as the case may be)?</p></disp-quote><p>Please see response above.</p><disp-quote content-type="editor-comment"><p>Subsection “Normative model”: The citation of <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref> here seems out of place. The analyses for this figure don't appear until the next page.</p></disp-quote><p>Thank you. We reorganized the text so that the material appears in a more natural order. The old Figure S6 is now <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>, and it is referred to only after introducing the analysis of trial-by-trial errors.</p><disp-quote content-type="editor-comment"><p>Also, shouldn't all the supplementary figures be cited somewhere in the main text? I think a bunch aren't.</p></disp-quote><p>All of the supplementary figures were cited in the main text.</p><disp-quote content-type="editor-comment"><p>Subsection “Trial-by-trial Error: Is -&gt; Are.</p></disp-quote><p>Thank you. Fixed.</p><disp-quote content-type="editor-comment"><p><xref ref-type="fig" rid="fig8">Figure 8B</xref>:Exactly what bin cutoffs did you use for blue vs. red here?</p></disp-quote><p>Sorry. Tilt bins were 45º wide. For cardinal tilts, bins were centered on 0º and 90º. For oblique tilts, bins were centered on 45º and 135º. This information has now bin added to the <xref ref-type="fig" rid="fig8">Figure 8</xref> caption.</p><disp-quote content-type="editor-comment"><p>Subsection “Effect of Natural Depth Variation”: artificially -&gt; artificial.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>Subsection “Generality of Conclusions and Future Directions”: our -&gt; are.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>Subsection “Cue-combination with and without independence assumptions”: This reference to <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>, since the pooled/averaged model is not in the figure, but merely mentioned in its legend.</p></disp-quote><p>Fixed. We removed the confusing reference.</p><disp-quote content-type="editor-comment"><p>Subsection “Experiment”: More details please: What's your definition of contrast.</p></disp-quote><p>Luminance contrast was defined as the root-mean-squared luminance values within a local area weighted by a cosine window. Specifically, luminance contrast is <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the spatial location, <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a cosine window with area <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>is the local mean intensity.</p><disp-quote content-type="editor-comment"><p>Refer to the figure to state what part of the patch they were supposed to judge.</p></disp-quote><p>Observers estimated the tilt at the center of the 1º (or 3º) patch marked by the smaller of the two probe circles. This is indicated in the <xref ref-type="fig" rid="fig2">Figure 2</xref> caption.</p><disp-quote content-type="editor-comment"><p>Were the judged bins over 180 or 360 degrees (I only say this because <xref ref-type="fig" rid="fig1">Figure 1</xref> leads the reader to believe that it's over 180 degrees only).</p></disp-quote><p>Observers estimated tilt across all 360º. The groundtruth were sampled from 24 bins, each with a width of 15º. Each bin has 150 samples. We analyzed all the data but focused the majority of our analyses on the unsigned tilts (i.e., 360º modulo 180º).</p><disp-quote content-type="editor-comment"><p>Please show and give details about the two types of artificial stimuli. Do responses to them differ from one another?</p></disp-quote><p>We generated (1) 1/f textured plane, (2) a “sparse” 3.5cpd plaid plane (i.e., a plane textured with the sum of two orthogonal sinusoidal gratings), and (3) a “dense” 5.25cpd plaid plane. These details are included in the main text. We now show examples of all three artificial stimulus types in new <xref ref-type="fig" rid="fig2">Figure 2-supplement 3</xref>. In the same figure, we also show performance for each artificial stimulus type separately. Performance with all three stimulus types is similar. Note that all artificial stimuli were matched to the tilt, slant, distance, and luminance contrast of each patch of natural scene.</p><disp-quote content-type="editor-comment"><p>Subsection “Groundtruth tilt”: &quot;atan2&quot; is MATLAB notation, I'd think. You might want to say what you mean there.</p></disp-quote><p>Thanks. The correction has been made.</p><disp-quote content-type="editor-comment"><p>Subsection “Image cues to tilt”: I'd like more detail here as well. The disparity cue must be based on a definition of &quot;local&quot; and a restriction of cross-correlation shifts. The disparity gradient requires a scale.</p></disp-quote><p>Disparity was estimated using windowed cross correlation. The window for the windowed cross-correlation had the same space constant as the derivative operator used to compute the gradient. This information has been added to the methods section.</p><disp-quote content-type="editor-comment"><p>The disparity gradient doesn't have a tilt sign ambiguity, but the texture cue does.</p></disp-quote><p>Correct. The disparity gradient does not have a tilt sign ambiguity, but in this paper we focused only on recovering the unsigned tilt. We are currently working on generalizing the model so that it can predict signed tilt. In the future work, we will use the signed tilt information provided by the disparity gradient.</p><disp-quote content-type="editor-comment"><p>I'm not sure &quot;patch size at half height&quot; won't confuse people (you don't mean the viewed patch, but rather the patch after multiplying by the Gaussian window).</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p><xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>: Actually, I'm rather surprised that the single-cue models (especially those other than disparity) perform as well as they do. It worries me that there are weird regularities in your database. You never say what your definition of luminance is exactly, but why should tilt be dependent on luminance?</p></disp-quote><p>Why does the orientation of the luminance gradient carry information about tilt? We don’t have a solid grasp of the physics, but it was been previously reported (Potetz &amp; Lee, 2003) that luminance and depth is weakly correlated. But we don’t really know.</p><p>The luminance signals that we used for the computations were proportional to candelas/m<sup>2</sup>. Luminance contrast does not depend on the absolute luminance.</p><disp-quote content-type="editor-comment"><p>How are each cue binned?</p></disp-quote><p>Each cue is binned into 64 unsigned tilts, and the same bins are used for the single-cue models. Thus, three-cue model is binned 64<sup>3</sup> bins.</p><disp-quote content-type="editor-comment"><p>Are the same bins used for the single-cue models, so that those models have vastly smaller measured parameters?</p></disp-quote><p>Yes, the single-cue models had fewer bins (i.e., fewer parameters). Increasing the number of single-cue bins does not improve performance. In other words, cue quantization error is not responsible for the single-cue performance.</p><disp-quote content-type="editor-comment"><p><xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref> legend, line 12: &quot;… but better than the prior or…&quot;.</p></disp-quote><p>Fixed.</p></body></sub-article></article>