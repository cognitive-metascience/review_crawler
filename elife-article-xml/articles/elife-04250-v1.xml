<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">04250</article-id><article-id pub-id-type="doi">10.7554/eLife.04250</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Automatic discovery of cell types and microcircuitry from neural connectomics</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-17193"><name><surname>Jonas</surname><given-names>Eric</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataro1"/></contrib><contrib contrib-type="author" id="author-17688"><name><surname>Kording</surname><given-names>Konrad</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="par-5"/><xref ref-type="other" rid="par-6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Electrical Engineering and Computer Science</institution>, <institution>University of California, Berkeley</institution>, <addr-line><named-content content-type="city">Berkeley</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Physical Medicine and Rehabilitation</institution>, <institution>Northwestern University</institution>, <addr-line><named-content content-type="city">Chicago</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Physical Medicine and Rehabilitation</institution>, <institution>Rehabilitation Institute of Chicago</institution>, <addr-line><named-content content-type="city">Chicago</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Physiology</institution>, <institution>Northwestern University</institution>, <addr-line><named-content content-type="city">Chicago</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-14764"><name><surname>Skinner</surname><given-names>Frances K</given-names></name><role>Reviewing editor</role><aff><institution>University Health Network, and University of Toronto</institution>, <country>Canada</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>jonas@eecs.berkeley.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>30</day><month>04</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>4</volume><elocation-id>e04250</elocation-id><history><date date-type="received"><day>05</day><month>08</month><year>2014</year></date><date date-type="accepted"><day>24</day><month>03</month><year>2015</year></date></history><permissions><copyright-statement>© 2015, Jonas and Kording</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Jonas and Kording</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-04250-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.04250.001</object-id><p>Neural connectomics has begun producing massive amounts of data, necessitating new analysis methods to discover the biological and computational structure. It has long been assumed that discovering neuron types and their relation to microcircuitry is crucial to understanding neural function. Here we developed a non-parametric Bayesian technique that identifies neuron types and microcircuitry patterns in connectomics data. It combines the information traditionally used by biologists in a principled and probabilistically coherent manner, including connectivity, cell body location, and the spatial distribution of synapses. We show that the approach recovers known neuron types in the retina and enables predictions of connectivity, better than simpler algorithms. It also can reveal interesting structure in the nervous system of <italic>Caenorhabditis elegans</italic> and an old man-made microprocessor. Our approach extracts structural meaning from connectomics, enabling new approaches of automatically deriving anatomical insights from these emerging datasets.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.001">http://dx.doi.org/10.7554/eLife.04250.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.04250.002</object-id><title>eLife digest</title><p>The human brain is made up of billions of neurons, which are organised into networks via trillions of connections. The study of the nature of these connections will be central to understanding how the brain works. In recent years, a number of new methods for imaging the brain have made it possible to visualise and map these connections, generating striking images and creating an additional field of neuroscience known as ‘connectomics’.</p><p>However, the sheer volume of data generated by connectomics is now beginning to exceed the capacity of researchers to analyse it. Just as the advent of genome sequencing required the development of statistical techniques to analyse the resulting data, so the emergence of connectomics has created a need for similarly powerful mathematical models in neuroscience.</p><p>Jonas and Kording have developed one such algorithm that can classify the component units of circuits, both biological and man-made, and identify the connections between them. When applied to connectomics data for 950 neurons in the mouse retina, the algorithm generated predictions regarding cell types and patterns of connectivity. The predicted cell types agreed closely with those identified by human neuroanatomists. Results were similarly convincing when the algorithm was applied to the nervous system of the nematode worm and genetic model organism, <italic>Caenorhabditis elegans</italic>, and even when it was asked to classify electronic components and connectivity patterns in a man-made microprocessor.</p><p>Algorithms such as that developed by Jonas and Kording will soon be essential for making sense of the vast quantities of data generated by connectomic studies of the human brain. At present, an analysis of 950 neurons requires several hours, thus refinements that make the process faster will likely be required prior to the analysis of larger human datasets. Such algorithms will open up a range of possibilities for examining the structure of the healthy brain, as well as the changes triggered by developmental abnormalities and disease.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.002">http://dx.doi.org/10.7554/eLife.04250.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>connectomics</kwd><kwd>computation</kwd><kwd>microcircuitry</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>C. elegans</italic></kwd><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006978</institution-id><institution>University of California Berkeley (University of California, Berkeley)</institution></institution-wrap></funding-source><award-id>AWS in Education grant</award-id><principal-award-recipient><name><surname>Jonas</surname><given-names>Eric</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>NSF CISE Expeditions Award CCF-1139158</award-id></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006235</institution-id><institution>Lawrence Berkely National Laboratory</institution></institution-wrap></funding-source><award-id>Award 7076018</award-id></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000185</institution-id><institution>Defense Advanced Research Projects Agency</institution></institution-wrap></funding-source><award-id>XData Award FA8750-12-2-0331</award-id></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS074044</award-id><principal-award-recipient><name><surname>Kording</surname><given-names>Konrad</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS063399</award-id><principal-award-recipient><name><surname>Kording</surname><given-names>Konrad</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.3</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>An algorithm for analysing brain connectivity data identifies cell types and connections in simple (<italic>C. elegans</italic>) and complex (mouse) nervous systems, and can even resolve structure and connectivity in a man-made microprocessor.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Emerging connectomics techniques (<xref ref-type="bibr" rid="bib37">Zador et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Morgan and Lichtman, 2013</xref>) promise to quantify the location and connectivity of each neuron within a tissue volume. These massive datasets will far exceed the capacity of neuroanatomists to manually trace small circuits, thus necessitating computational, quantitative, and automatic methods for understanding neural circuit structure. The impact of this kind of high-throughput transition has been seen before—the rise of sequencing techniques necessitated the development of novel computational methods to understand genomic structure, ushering in bioinformatics as an independent discipline (<xref ref-type="bibr" rid="bib18">Koboldt et al., 2013</xref>).</p><p>The brain consists of multiple kinds of neurons, each of which is hypothesized to have a specific role in overall computation. Neuron types differ in many ways, for example, chemical or morphological, but they also differ in the way they connect to one another (<xref ref-type="bibr" rid="bib31">Seung and Sümbül, 2014</xref>). In fact, the idea of well defined, type-dependent local connectivity patterns (microcircuits) has a long history (<xref ref-type="bibr" rid="bib29">Passingham, 2002</xref>), and is prominent in many areas, from sensory (e.g., retina; <xref ref-type="bibr" rid="bib20">Masland, 2001</xref>) to processing (e.g., neocortex; <xref ref-type="bibr" rid="bib24">Mountcastle, 1997</xref>) to movement (e.g., spinal cord; <xref ref-type="bibr" rid="bib10">Grillner et al., 2005</xref>). These types of repeated computing patterns are a common feature of computing systems, even occurring in man-made computing circuits. It remains an important challenge to develop algorithms to use connectivity-based anatomical data (connectomics) to automatically back out underlying microcircuitry.</p><p>The discovery of structure is a crucial aspect of network science. Early approaches focused on global graph properties, such as the types of scaling present in the network (<xref ref-type="bibr" rid="bib34">Watts and Strogatz, 1998</xref>). While this approach leads to an understanding of the global network, more recent work aims at identifying very small-scale repeat patterns, or motifs, in networks (<xref ref-type="bibr" rid="bib21">Milo et al., 2002</xref>). These motifs are defined not between different node types, but rather represent repeated patterns of topology.</p><p>The discovery of structure in probabilistic graphs is a well-known problem in machine learning. Commonly used algorithms include community-based detection methods (<xref ref-type="bibr" rid="bib8">Girvan and Newman, 2002</xref>) and stochastic block models (<xref ref-type="bibr" rid="bib28">Nowicki and Snijders, 2001</xref>). While these approaches can incorporate the probabilistic nature of neural connections (<xref ref-type="bibr" rid="bib13">Hill et al., 2012</xref>), they do not incorporate the additional richer structure present in connectomics data—the location of cell bodies, the spatial distribution of synapses, and the distances between neurons. It is of particular importance that the probability of connections has a strong spatial component, a factor that is hard to reconcile with many other methods. A model attempting to fully capture the variation in the nervous system should take into account the broad set of available features.</p><p>When it comes to neuroscience and other computing systems, we expect patterns of connectivity much more complex than traditional motifs, exhibiting a strong spatial dependence arising from the complex genetic, chemical, and activity-based neural development processes.</p><p>To address these challenges, here we describe a Bayesian non-parametric model that can discover circuit structure automatically from connectomics data: the cell types, their spatial patterns of interconnection, and the locations of somata and synapses. We show that by incorporating this additional information, our model both accurately predicts the connection as well as agrees with human neuroanatomists as to the identification of cell types. We take as inspiration previous work on identifying cell types automatically from morphology (<xref ref-type="bibr" rid="bib11">Guerra et al., 2011</xref>) and electrophysiology (<xref ref-type="bibr" rid="bib6">Druckmann et al., 2013</xref>).</p><p>We primarily focus on the recently released mouse retina connectome (<xref ref-type="bibr" rid="bib12">Helmstaedter et al., 2013</xref>), but additionally examine the <italic>Caenorhabditis elegans</italic> connectome (<xref ref-type="bibr" rid="bib35">White et al., 1986</xref>). Comparing the cell types discovered by the algorithms with those obtained manually by human anatomists reveals a high degree of agreement. We thus present a scalable probabilistic approach to infer microcircuitry from connectomics data available today and in the future.</p></sec><sec id="s2" sec-type="model"><title>Model</title><p>We build a structured probabilistic model which begins with the generic notion of a cell being a member of a single type—and these types affect soma depth, distribution of synapses, as well as a cell type and distance-dependent connection probability. For example, retinal ganglion cells may synapse on nearby, but not far away, amacrine cells, with bipolar cells clearly tessellating space and synapsing on both. In machine learning parlance, our method is unsupervised—it seeks to discover structure in data and make predictions in the absence of training data. Rather than taking in examples of types annotated by human neuroanatomists, we instead start with the weakest possible assumption in an attempt to algorithmically discover this structure. We contrast this with the supervised approaches taken in <xref ref-type="bibr" rid="bib11">Guerra et al. (2011</xref>), where there is high confidence in the (morphologically defined) types and then a supervised classifier is built, as our goal here is explicit discovery of types.</p><p>From these assumptions (priors) we develop a generative Bayesian model that estimates the underlying cell types and how they connect. We take as input (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) the connectivity matrix of cells (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), a matrix of the distance between cells (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), the per-cell soma depth (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), and the depth profile of the cell's synapses (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). We perform joint probabilistic inference to automatically learn the number of cell types, which cells belong to which type, their type-specific connectivity, and how connections between types vary with distance. We also simultaneously learn the soma depth associated with each type and the typical synaptic density profile (<xref ref-type="fig" rid="fig1">Figure 1F–H</xref>).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.04250.010</object-id><label>Figure 1.</label><caption><title>Deriving circuitry and cell types from connectomics data.</title><p>(<bold>A</bold>) As input we take the connectivity between cells (<bold>B</bold>), the distance between them (<bold>C</bold>), the depth of the cell bodies (<bold>D</bold>), and the depth profile of the synapses (<bold>E</bold>). (<bold>F</bold>) Our algorithm discovers hidden cell types in this connectivity data by assuming all cells of a type share a distance-dependent connectivity profile, similar depth, and a similar synaptic density profile, with cells of other types. This results in a clustering of the cells by those hidden types. (<bold>F</bold>) Shows the cell connectivity matrix with cells of the same type grouped together. (<bold>G</bold>) Shows the learned probability of connection (p(conn)) between our different types at various distances—in this case, the cells are likely to connect when they are close. (<bold>H</bold>) Shows the probability of connection (p(conn)) between two cell types that very rarely connect—there is a background ‘base’ connection rate to account for errors in data, but the probability is very low. (<bold>I</bold>) Shows that we also recover the expected laminarity of types and the depth-specific (<bold>J</bold>) synaptic connectivity. (<bold>K</bold>) We then plot how the connectivity between these types changes as a function of distance between the cell bodies to better understand short-range and long-range connectivity patterns.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.010">http://dx.doi.org/10.7554/eLife.04250.010</ext-link></p></caption><graphic xlink:href="elife-04250-fig1-v1.tif"/></fig></p><p>We start with a model for connectivity, the iSBM (<xref ref-type="bibr" rid="bib17">Kemp et al., 2006</xref>; <xref ref-type="bibr" rid="bib36">Xu et al., 2006</xref>), which has been shown to meaningfully cluster connection graphs while learning the number of hidden groups, or types. We extend this approach by adding distance dependence to model salient aspects of microcircuitry via logistic and exponential distance-link functions. We form a unimodial model of cell body depth and a multimodal synapse density profile model (see ‘Materials and methods’ for mathematical details).</p><p>As an illustrative example, consider a network with only three cell types, labeled A, B, and C. Assume these cells are uniformly distributed in space, and that the probability of connection between any two cells, <italic>c</italic><sub><italic>i</italic></sub> and <italic>c</italic><sub><italic>j</italic></sub>, depends only on their type and their distance, according to a logistic (sigmoidal) function. Let A connect only to nearby B and C cells, but B connect to any C regardless of distance. This is the prior intuition our model is designed to capture.</p><p>For the basic link-distance model, we take as input a connectivity matrix <italic>R</italic> defining the connections between cell <italic>e</italic><sub><italic>i</italic></sub> and <italic>e</italic><sub><italic>j</italic></sub>, as well as a distance function <italic>d</italic>(<italic>e</italic><sub><italic>i</italic></sub>, <italic>e</italic><sub><italic>j</italic></sub>) representing a (physical) distance between adjacent cells. See the supplemental material for extension to multiple connectivity matrices. We assume there exist an unknown number <italic>K</italic> of latent (unobserved) cell types, <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and that each cell <italic>e</italic><sub><italic>i</italic></sub> belongs to a single cell type. We indicate a cell <italic>e</italic><sub><italic>i</italic></sub> is of type <italic>k</italic> using the assignment vector (<italic>c</italic>), so <italic>c</italic><sub><italic>i</italic></sub> = <italic>k</italic>. The observed connectivity between two cells <italic>R</italic>(<italic>e</italic><sub><italic>i</italic></sub>, <italic>e</italic><sub><italic>j</italic></sub>) then depends only on their latent type and their distance through a link function <italic>f</italic>(⋅, <italic>d</italic>(<italic>e</italic><sub><italic>i</italic></sub>, <italic>e</italic><sub><italic>j</italic></sub>)). We assume <italic>f</italic> is parameterized based on the latent type, <italic>c</italic><sub><italic>i</italic></sub> = <italic>m</italic> and <italic>c</italic><sub><italic>j</italic></sub> = <italic>n</italic>, via a parameter <italic>η</italic><sub><italic>mn</italic></sub>, as well as a set of global hyperparameters <italic>θ</italic>, such that the link function is <italic>f</italic>(<italic>d</italic>(<italic>e</italic><sub><italic>i</italic></sub>, <italic>e</italic><sub><italic>j</italic></sub>)|<italic>η</italic><sub><italic>mn</italic></sub>, <italic>θ</italic>).</p><p>We then jointly infer the posterior distribution of the class assignment vector (<italic>c</italic>) = {<italic>c</italic><sub><italic>i</italic></sub>}, the parameter matrix <italic>η</italic><sub><italic>mn</italic></sub>, and the global model hyperparameters <italic>θ</italic>:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>|</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Our subsequent analysis uses both the full posterior distribution over these parameters as well as the most probable, or maximum a posteriori (MAP), estimate.</p><p>For the retina data, we then extend the model with the additional features indicated. Cell soma depth is modeled as a cell-type-dependent Gaussian distribution with latent (unknown) per-type mean and variance. Similarly, each cell has some number <italic>N</italic><sub><italic>i</italic></sub> of synapses, each of which is drawn from a cell-type-specific density profile with up to three modes.</p><p>Inference is performed via MCMC via three composable transition kernels—one for structural, one for per-type parameters, and one for global parameters and hyperparameters. Details of data preprocessing, inference parameters, and runtime can be found in the 'Methods section.</p><sec id="s2-1"><title>Metrics</title><p>To evaluate the quality of the model fit, we need to use information that quantifies aspects of the data for which we have ground truth information. We focus on two aspects of performance. First, if the model works well, then the probability that a pair of neurons is of the same type should be high if the neurons actually are of the same type. Second, the model should assign a high probability of connection between two cells if they have a connection in the underlying data. We term these two factors clustering accuracy and link-prediction accuracy.</p><p>To assess the accuracy of a clustering compared to that determined by neuroanatomists, we employ three metrics—clustering homogeneity, clustering completeness, and the ARI. All metrics equal 1.0 when two clusterings completely agree. Homogeneity reflects the degree to which a found cluster or type contains only a single true type. Completeness measures how much of a true type is contained within a single identified type—a completeness of 1.0 means no true type is split into multiple subtypes. ARI is a metric that reflects both measures (see the supplemental material for more information).</p><p>To assess the accuracy of the model for connections, we use link prediction accuracy. If our model accurately captures the true structure of the data, it should be good at predicting if a link exists. We thus train the model on the data with a subset of the links marked as unobserved and thus compute our predictive accuracy. We perform 10-way cross-validation on a given dataset (<xref ref-type="bibr" rid="bib11">Guerra et al., 2011</xref>), learn the resulting model, and use that model to predict the missing synapses. Each potential link between cells is assigned a probability, and we compute the AUC for the resulting ROC curve. An AUC of 1.0 means that we perfectly predict the presence and absence of the missing synapses. We use link prediction accuracy to quantify how good the model is at discovering the underlying connectivity.</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><p>We will first establish that our algorithm works properly and try to understand its properties using simulated data. Subsequently, we will analyze in detail a dataset on the retina. Lastly, we will briefly discuss the analysis of data from the worm <italic>C. elegans</italic> and from an old man-made microprocessor.</p><sec id="s3-1"><title>Validation with simulated data where ground truth is known</title><p>To validate our model, we performed a series of simulations to test if the model can accurately recover the true underlying network structure and cell type identity. We thus simulate data for which we know the correct structure and compare the estimated structure based on the algorithm (see ‘Materials and methods’) with the one we used for simulation. We find that the model does a good job of recovering the correct number of cell types (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), the cell identities (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), and the spatial extent of each type (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). For comparison, we show the results using the infinite stochastic block model (iSBM) instead (<xref ref-type="fig" rid="fig2">Figure 2A–C</xref>, black line) which assumes that only cell type matters, and thus finds small neighborhoods of connected nodes (instead of global connectivity patterns). This contrast shows that while the regular block model can not correctly deal with distance-dependent connectivity, our model can. Our model converges relatively quickly (see 'Mixing of Markov chains') to an estimate of the most probable values for the cell types, which is enabled by using a combination of simulated annealing and parallelized Markov-chain Monte Carlo (MCMC) (see ‘Materials and methods’ for details). Thus our model at least is promising for application to biological datasets.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.04250.003</object-id><label>Figure 2.</label><caption><title>Correct recovery of true numbers of hidden types in synthetic data when incorporating spatial information.</title><p>(<bold>A</bold>) The infinite stochastic block model (which only uses connectivity information) over-estimates the number of classes as it fails to take distance into account, whereas our modeling of the combination of distance and connectivity finds close to the true number of classes. Conn: connectivity; dist: distance. (<bold>B</bold>) As we increase the true number of types, our method continues to find the correct clustering (as measured by the adjusted Rand index, ARI) whereas the infinite stochastic block model (iSBM) overclusters and thus poorly matches ground truth. (<bold>C</bold>) We examine the spatial extent (size) of the discovered types (clusters) by measuring the two-dimensional standard deviation of the cell locations. The y-axis indicates what fraction of the discovered types had a given spatial extent. Without incorporating distance, we identify a large number of small, spatially-localized types. With distance, we see a correct recovery of the spatial extent of each type.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.003">http://dx.doi.org/10.7554/eLife.04250.003</ext-link></p></caption><graphic xlink:href="elife-04250-fig2-v1.tif"/></fig></p></sec><sec id="s3-2"><title>Model mismatch</title><p>We next analyze how our model performs in cases where the data are generated with assumptions different from ours. To understand the properties of our model, we attempt connectivity inference on four sets of synthetic data. This helps us understand what our model would do if the data do not obey our assumptions.</p><p>We thus generate 10 sets of synthetic data from each of four existing models. The distance-dependent stochastic block model assumes type depends on distance, the traditional stochastic block model has no notion of distance, the mixed membership block model assumes type is combinatorial, and the latent position cluster model assumes that type is clustered-but-continuous.</p><p>If the data are sampled from our model, inference according to our model, unsurprisingly, is good by all measures. It correctly estimates the number of cell types, it is good at predicting connectivity (high area under the curve, AUC), it agrees with human classification (Rand index), it discovers all types, and leads to homogeneous estimates (<xref ref-type="fig" rid="fig3">Figure 3</xref>, first row). If the data come from a block model without distance dependence, we see that it still does well on all meaningful measures (<xref ref-type="fig" rid="fig3">Figure 3</xref>, second row). This is unsurprising, as our model learns the distance dependence, even its absence. For the mixed membership model (<xref ref-type="fig" rid="fig3">Figure 3</xref>, third row), the model grossly overestimates the number of types, by basically allocating a type for each combination of memberships. Otherwise, it still performs relatively well. Lastly, for the latent position clustering model (<xref ref-type="fig" rid="fig3">Figure 3</xref>, fourth row), the model does poorly. If type is continuous instead of discrete, then our model is basically trying to cover a continuous set with a discrete scenario leading to rather poor performance. However, as we do expect cell types to have a discrete biological basis, we might expect our model to do well with real data.<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.04250.004</object-id><label>Figure 3.</label><caption><title>Model inferences when the true generating model differs from our distance-block-model prior.</title><p>Horizontal columns show results with synthetic data generated according to the distance-dependent stochastic block model, the non-distance-dependent stochastic block model, the mixed membership block model, and the latent position cluster model. In all cases histograms represent posterior distribution over the indicated metric. (<bold>A</bold>) The number of types found by the model; the vertical dashed line indicates the ‘true’ type number (not applicable to the mixed membership model). (<bold>B</bold>) The area under the receiver operating characteristic (ROC) curve, indicating link prediction accuracy. (<bold>C</bold>, <bold>D</bold>, <bold>E</bold>) Clustering metrics quantifying degree of type agreement with known ground truth.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.004">http://dx.doi.org/10.7554/eLife.04250.004</ext-link></p></caption><graphic xlink:href="elife-04250-fig3-v1.tif"/></fig></p></sec><sec id="s3-3"><title>Sensitivity to edge effects</title><p>Connectomic efforts so far have reconstructed only small sections of neural tissue. Consequently, many connections to cells outside that tissue volume will be lost. We are concerned that this selective elimination of connectivity along the boundary might give the appearance of distance-dependent connectivity when there is none. We thus performed simulations to check if edge effects could destroy spatial structure and if edge effects could introduce artificial, spurious spatial structure. We measure the degree to which distance-dependent effects can arise from selecting regions that are smaller than the ‘scale’ of connectivity (<xref ref-type="fig" rid="fig4">Figure 4</xref>). We do this by generating two collections of synthetic datasets—one with distance-dependent connectivity and one without. We then in each dataset randomly examine contiguous circular regions with area varying from zero to the entire volume, and empirically calculate the spatial variance in type-dependent connectivity. We find that, if there is no distance dependence, edge effects do not artificially introduce distance dependence. However, if the section we are examining is too small, our model can miss the distance dependence. Thus with respect to distance-dependent connectivity inference, our model errs on the side of caution. But we also find that for spatial extent that is similar to the currently available datasets, the effects of this are quite limited.<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.04250.005</object-id><label>Figure 4.</label><caption><title>Two sets of generated synthetic data, one with spatially dependent connectivity and one without.</title><p>We measure the variance in the connectivity-distance plot for randomly selected regions of each dataset, ranging from single cells to the entire volume. We see that while selecting too small a region can destroy the appearance of distance-dependent connectivity, it does not create it in non-spatial data.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.005">http://dx.doi.org/10.7554/eLife.04250.005</ext-link></p></caption><graphic xlink:href="elife-04250-fig4-v1.tif"/></fig></p></sec><sec id="s3-4"><title>Learning types and circuitry in the retina</title><p>The mouse retina (<xref ref-type="bibr" rid="bib20">Masland, 2001</xref>) is a neural circuit which we expect to have connectivity patterns that are well approximated by our generative model. It is known that there are multiple classes of cells that can be broadly grouped into: ganglion cells that transmit information to the rest of the brain; bipolar cells that connect between different cells; and amacrine cells that feed into the ganglion cells. Recent research (<xref ref-type="bibr" rid="bib12">Helmstaedter et al., 2013</xref>) has produced a large dataset containing both the types of cells from orthogonal approaches, and also the connectivity matrix between all reconstructed cells (<xref ref-type="fig" rid="fig5">Figure 5A</xref>).<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.04250.006</object-id><label>Figure 5.</label><caption><title>Discovering cell classes in the mouse retina connectome.</title><p>Here we show the maximum a posteriori (MAP) estimate for the types in the mouse retina data. (<bold>A</bold>) Input connectivity data for 950 cells for which soma positions were known. (<bold>B</bold>) Clustered connectivity matrix; each arbitrary color corresponds to a single type and will be used to identify that type in the remainder of the plot. (<bold>C</bold>) The spatial distribution of our cell types—each cell type tessellates space. Colors correspond to those in (<bold>B</bold>). (<bold>D</bold>) Connectivity between our clusters as a function of distance—the cluster consisting primarily of retinal ganglion cells (brown nodes on the graph) exhibits the expected near and far connectivity. Conn prob: probability of connection.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.006">http://dx.doi.org/10.7554/eLife.04250.006</ext-link></p></caption><graphic xlink:href="elife-04250-fig5-v1.tif"/></fig></p><p>The algorithm took 8 hr to perform inference, dividing neurons into a set of cell types which reflect known neuroanatomical distinctions (<xref ref-type="fig" rid="fig5">Figure 5</xref> shows the MAP result). For each pair of neurons there is a specific distance-dependent connection probability (<xref ref-type="fig" rid="fig5">Figure 5D</xref>), which is well approximated by the model fit. Moreover, each type of cell is rather isotropically distributed across space (<xref ref-type="fig" rid="fig5">Figure 5C</xref>) as should be expected for true cell types.</p><p>Comparing the results of the algorithm to other information sources allows evaluation of the quality of the type determination. Our types closely reflect the (anatomist-determined) segmentation of cells into retinal ganglion, narrow amacrine, medium/wide amacrine, and bipolar cells (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). We find that the types we find tend to reflect the known laminar distribution in the retina (<xref ref-type="fig" rid="fig6">Figure 6C</xref>) as well as the known synaptic density profiles.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.04250.007</object-id><label>Figure 6.</label><caption><title>Visualizing type inference uncertainty.</title><p>Our fully Bayesian model gives a confidence estimate (posterior probability) that any two given cells are of the same type. In (<bold>A</bold>) we visualize that cell–cell coassignment matrix, showing the probability that cell i is of the same type as cell j on a range from 0.0 to 1.0. The block structure shows subsets of cells which are believed to all belong to the same type. For comparison, (<bold>B</bold>) shows the anatomist-defined type for each cell, grouped broadly into the coarse types identified in the previous panel. (<bold>C</bold>) Link versus cluster accuracy. (<bold>D</bold>) The posterior distribution of receiver operating characteristic (ROC) curves from 10-fold cross-validation when predicting connectivity, as well as (<bold>E</bold>) the area under the curve (AUC) and (<bold>F</bold>) the type agreements with known neuroanatomist types. ARI: adjusted Rand index. Model comparison, showing using human-discovered types with and without distance information, as well as our model incorporating just connectivity, connectivity and distance, or connectivity, distance, and synaptic depth (as well as the alternative latent position cluster model, see text). (<bold>G</bold>) A comparison of the predictive accuracy (AUC) for hand-labeled anatomical data, versus inclusion of additional sources of information, as well as the clustering accuracy. Note that our model sacrifices very little predictive accuracy for additional clustering accuracy. By comparison, conventional methods fail at one or both. ARI: adjusted Rand index. (<bold>H</bold>) The spatial extent (in depth and area) of the types identified by humans and our various algorithmic approaches.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.007">http://dx.doi.org/10.7554/eLife.04250.007</ext-link></p></caption><graphic xlink:href="elife-04250-fig6-v1.tif"/></fig></p><p>The algorithm yields a separation of neurons into a smaller number of types than the fully granular listing of 71 types found by the original authors of the paper (<xref ref-type="bibr" rid="bib12">Helmstaedter et al., 2013</xref>), although it is still highly correlated with those finer type distinctions (see section ‘Mouse retina’). It is our expectation that, with larger datasets, even closer agreement would be found.</p><p>Our fully Bayesian model produces a distribution over probable clusterings. <xref ref-type="fig" rid="fig6">Figure 6</xref> shows this posterior distribution as a cell–cell coassignment matrix, sorted to find maximum block structure. Each large, dark block represents a collection of cells believed with strong probability to be of the same type. When we plot (<xref ref-type="fig" rid="fig6">Figure 6B</xref>) the anatomist-derived cell types along the left, we can see that each block consists of a roughly homogeneous collection of types.</p><p>We evaluate our model along three sets of parameters (<xref ref-type="fig" rid="fig6">Figure 6</xref>): how closely does our clustering agree with neuroanatomists' knowledge? Given two cells, how accurately can our model predict the link between them? And how closely does the spatial extent (within a layer) of our identified types agree with the spatial extent of types identified by neuroanatomists?</p><p>For our model we show the receiver operating characteristic (ROC) curve (<xref ref-type="fig" rid="fig6">Figure 6D</xref>) which shows how the true and false positive rates trade off. We plot the posterior distribution of the area under this curve in <xref ref-type="fig" rid="fig6">Figure 6E</xref>. We then plot the posterior distribution for cluster agreement metrics—completeness, homogeneity, and adjusted Rand index (ARI) (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). We see that our model tends to over-cluster—cells which are of distinct type (at the finest granularity of neuroanatomist-identified type) are grouped as a single type by our model.</p><p>We compare link-prediction accuracy across the methods, including our own (<xref ref-type="fig" rid="fig6">Figure 6G</xref>, AUC, red). We find that given the dataset, many techniques allow for good link-predictive accuracy. All the methods allow decent link prediction with an AUC in the 0.9 range. However, our algorithm clearly outperforms the simple statistical models that only use connectivity.</p><p>As a second measure we compare link-prediction accuracy across the methods (<xref ref-type="fig" rid="fig6">Figure 6G</xref>, ARI, blue). We find that our algorithm far outperforms the controls. We also find that when it is based on more of the same information used by anatomists, then it gets better at agreeing with these anatomists. In particular, using connectivity, distance, synapse distribution, and soma depth leads to the highest ARI. When using the available information, the algorithm produces a good fit to human anatomist judgments.</p><p>Finally we look at the spatial extent of the discovered types both within a layer and between layers (<xref ref-type="fig" rid="fig6">Figure 6H</xref>). We see that, in the absence of distance information, mere connectivity information results in types which only span a small region of space—essentially local cliques. Incorporation of distance information results in types which span the entire extent of the layer. The depth variance of all models continues to be substantially larger than that predicted by human anatomists—future directions of work include attempting to more strongly encode this prior belief of laminarity.</p></sec><sec id="s3-5"><title>Recovering spatial connectivity in multiple graphs simultaneously</title><p>Having shown our model to work on the repeating tessellated, laminar structure of the mammalian retina, we then apply our model to a structurally very different connectome—the whole body of a small roundworm: <italic>C. elegans</italic> is a model system in developmental neuroscience (<xref ref-type="bibr" rid="bib35">White et al., 1986</xref>), with the location and connectivity of each of 302 neurons developmentally determined, leading to early measurement of the connectome. Unlike the retina, only the motor neurons in <italic>C. elegans</italic> exhibit regular distribution in space—along the body axis. Most interneurons are concentrated in various ganglia that project throughout the entire animal, and the sensory neurons are primarily located in a small number of anterior ganglia. <italic>C. elegans</italic> also differs from the retina in that the measured connectome is actually two separate graphs—one of directed chemical synapses and another of undirected electrical synapses. As this is a very different connectome, it allows an interesting generalization test: how well will our model work on such a distinct dataset?</p><p>Using both the chemical and electrical connectivity (see ‘Materials and methods’), we determined the underlying cell types explained by connectivity and distance (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). A superficial inspection of the results shows clustering into groups consisting roughly homogeneously of motor neurons, sensory neurons, and interneurons. Closer examination reveals agreement with the classifications originally outlined by White in 1986 (<xref ref-type="bibr" rid="bib35">White et al., 1986</xref>).<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.04250.008</object-id><label>Figure 7.</label><caption><title>Discovering connectivity and type in <italic>C</italic><italic>. elegans</italic>.</title><p>(<bold>A</bold>) Posterior distribution on cell connectivity as a function of discovered type, similar to <xref ref-type="fig" rid="fig6">Figure 6</xref>. In (<bold>B</bold>) we plot neuroanatomist-derived types along with their labels. Our model shows a high probability of motor neurons, sensory neurons, and various interneuron classes being of the same type. Soma positions along the body axis are plotted in (<bold>C</bold>) where we see that we cluster spatially distributed motor neurons together, whereas head sensory neurons are more likely to be grouped together as well. (<bold>D</bold>) The receiver operating characteristic (ROC) curves for held-out link probability for both the electrical synapses (gap junctions) and chemical synapses in <italic>C. elegans</italic>. (<bold>E</bold>) The posterior distribution of the area under the ROC curve (AUC) for the curves in (<bold>D</bold>). (<bold>F</bold>) Measurements of the agreement of our identified cell types compared to neuroanatomists. The high completeness but low homogeneity (and corresponding low adjusted Rand index, ARI) reflects our model's tendency to group multiple types into a single type.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.008">http://dx.doi.org/10.7554/eLife.04250.008</ext-link></p></caption><graphic xlink:href="elife-04250-fig7-v1.tif"/></fig></p><p>Note our clustering does not perfectly reflect known divisions—several combinations of head and sensory neurons are combined, and a difficult-to-explain group of mostly VB and DB motor neuron types, with VC split between various groups. Our identified cell types thus reflect a ‘coarsening’ of known types, based entirely on connectivity and distance information, even when the organism exhibits substantially less spatial regularity than the retina.</p></sec><sec id="s3-6"><title>Types and connectivity in artificial structures</title><p>To show the applicability of our method to other connectome-style datasets, we obtained the spatial location and interconnectivity of the transistors in a classic microprocessor, the MOS Technology 6502 (used in the Apple II) (<xref ref-type="bibr" rid="bib16">James et al., 2010</xref>). Computer architects use common patterns of transistors when designing circuits, with each transistor having a ‘type’ in the circuit. We identified a region of the processor with complex but known structure containing the primary 8-bit registers X, Y, and S (<xref ref-type="fig" rid="fig8">Figure 8</xref>).<fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.04250.009</object-id><label>Figure 8.</label><caption><title>Discovering connectivity and type in the MOS 6502 microprocessor.</title><p>(<bold>A</bold>) The micrograph of the original microprocessor, with the region containing the registers under study highlighted. (<bold>B</bold>) Our graph consists of the interconnections of MOS field-effect transistors with three terminals, Gate, C1, and C2. The reconstruction technique did not permit resolution of C1 and C2 into source and drain. (<bold>C</bold>) The spatial distribution of the transistors in each cluster show a clear pattern. (<bold>D</bold>) The clusters and connectivity versus distance for connections between Gate and C1, Gate and C2, and C1 and C2 terminals on a transistor. Purple and yellow types have a terminal pulled down to ground and mostly function as inverters. The blue types are clocked, stateful transistors, green control the ALU and orange control the special data bus (SDB).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.009">http://dx.doi.org/10.7554/eLife.04250.009</ext-link></p></caption><graphic xlink:href="elife-04250-fig8-v1.tif"/></fig></p><p>Our algorithm identifies areas of spatial homogeneity that mirror the known structure in the underlying architecture of the circuit, segmenting transistor types recognizable to computer architects. Using the original schematics, we see that one identified type contains the ‘clocked’ transistors, which retain digital state. Two other types contain transistors with pins C1 or C2 connected to ground, mostly serving as inverters. An additional identified type controls the behavior of the three registers of interest (X, Y, and S) with respect to the SB data bus, either allowing them to latch or drive data from the bus. The repeat patterns of spatial connectivity are visible in <xref ref-type="fig" rid="fig8">Figure 8C</xref>, showing the man-made horizontal and vertical layout of the same types of transistors.</p></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>We have presented a machine learning technique that allows cell types and microcircuitry to be discovered from connectomics data. We have shown its applicability to regularly structured laminar neural circuits like the retina, as well as a less structured whole neuronal organism (<italic>C. elegans</italic>) and a classic processor. When compared to existing methods, we show how the incorporation of all of this data yields results that combine both high link-prediction accuracy and high agreement with human anatomists. We have found that combining the available data types allows us to discover cell types and microcircuitry that were known to exist in the systems based on decades of previous research and allows good prediction of connectivity.</p><p>For our probabilistic models, no known solution exists to exactly find the most probable parsing of the neurons into cell types and connectivity patterns. We employ a collection of MCMC techniques (see ‘Materials and methods’), but while different initializations converge to similar ultimate values, we can never realistically obtain the global optimum. There are a broad range of techniques that may offer good approximations to the global optimum and future work could adapt them to find more precise solutions to our problem.</p><p>For our probabilistic model, inference becomes slower as the amount of data increases. Our algorithm required several hours for 1000 neurons. Scaling this class of probabilistic model is an active area of research, and recent results in both variational methods (<xref ref-type="bibr" rid="bib14">Hoffman et al., 2013</xref>) and spectral learning (<xref ref-type="bibr" rid="bib1">Anandkumar et al., 2012</xref>) and future work could adapt them to find faster approximate solutions to our problem.</p><p>Larger datasets will allow algorithms to distinguish more distinct types and we expect closer agreement with existing anatomical knowledge as more data become available. Moreover, in general, for such problems precision increases with the size of the dataset and the cells that we have are not sufficient to statistically distinguish all the cell types known in anatomy (such as the ∼70 in the retina). Still, using only connectivity and distance, it is possible to meaningfully divide neurons into types.</p><p>Our small collection of hand-selected distance-dependent likelihood functions is clearly non-exhaustive, and assumes monotonicity of connectivity probability—for a given class, closer cells are never less likely to connect. This is known to be insufficient for various neural systems. Future models could incorporate a wider variety of likelihood functions, or even learn the global functional form from the data.</p><p>There are a range of previous approaches to the discovery of neural microcircuitry (<xref ref-type="bibr" rid="bib23">Mountcastle, 1957</xref>; <xref ref-type="bibr" rid="bib5">Douglas and Martin, 1991</xref>; <xref ref-type="bibr" rid="bib7">Freund and Buzsáki, 1998</xref>; <xref ref-type="bibr" rid="bib2">Barthó et al., 2004</xref>). These generally involve a great deal of manual labor and ad hoc determination of what constitutes a type of cell—to this day there are disagreements in the literature as to the true types in the mammalian retina. Much as phylogenomics has changed our understanding of animal ontologies, modern large scale data will allow the efficient unbiased discovery of cell types and circuits. The sheer amount of available data demands the introduction of algorithmic approaches.</p><p>The development of automatic identification and quantification of cell type may also provide a new computational phenotype for quantifying the effect of disease, genetic interventions, and developmentally experienced neural activity. Our method can in principle identify neuron types across non-connected graphs, for example, across animals. For example, the types of neurons in one animal can be associated with the types of neurons in another animal, in the same way as this is already possible through molecular markers (<xref ref-type="bibr" rid="bib3">Brown and Hestrin, 2009</xref>). This could be particularly important if cell types appear that are due to properties of the stimuli and experience as opposed to just the molecular properties of cells, such as color and orientation selective types in primary visual cortex (<xref ref-type="bibr" rid="bib19">Lennie and Movshon, 2005</xref>; <xref ref-type="bibr" rid="bib32">Sincich and Horton, 2005</xref>). This would allow comparative quantitative anatomy across animals, and aid the search for the ultimate causes of connectivity.</p><p>Our model combines connectivity, cellular and synaptic properties, and suggests the way towards combining even richer data. Distinct cell types differ in morphology, connectivity, transcriptomics, relation to behavior or stimuli, and many other ways. Algorithms combining these data and type information may allow us to synthesize all the available information from one experiment or even across experiments into a joint model of brain structure and function.</p><p>Our work shows how rich probabilistic models can contribute to computational neuroanatomy. Eventually, algorithms will have to become a central tool for anatomists, as it will progressively become impossible for humans to parse the huge datasets. This transition may follow a similar transition to that of molecular biology (with gene-finding algorithms) and evolutionary biology (with computational phylogenetics). Ultimately, computational approaches may help resolve the significant disagreements across human anatomists.</p></sec><sec id="s5" sec-type="methods"><title>Methods</title><sec id="s5-1"><title>Probabilistic model</title><p>Our model is a extension of the iSBM (<xref ref-type="bibr" rid="bib17">Kemp et al., 2006</xref>; <xref ref-type="bibr" rid="bib36">Xu et al., 2006</xref>) to incorporate spatial relations between entities, inspired by attempts to extend these models with arbitrary discriminative functions (<xref ref-type="bibr" rid="bib25">Murphy, 2012</xref>).</p><p>We take as input a connectivity matrix <italic>R</italic> defining the connections between cell <italic>e</italic><sub><italic>i</italic></sub> and <italic>e</italic><sub><italic>j</italic></sub>, as well as a distance function <italic>d</italic>(<italic>e</italic><sub><italic>i</italic></sub>, <italic>e</italic><sub><italic>j</italic></sub>) representing a (physical) distance between adjacent cells. See the supplemental material for extension to multiple connectivity matrices. We assume there exist an unknown number <italic>K</italic> of latent (unobserved) cell types, <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and that each cell <italic>e</italic><sub><italic>i</italic></sub> belongs to a single cell type. We indicate a cell <italic>e</italic><sub><italic>i</italic></sub> is of type <italic>k</italic> using the assignment vector (<italic>c</italic>), so <italic>c</italic><sub><italic>i</italic></sub> = <italic>k</italic>. The observed connectivity between two cells <italic>R</italic>(<italic>e</italic><sub><italic>i</italic></sub>, <italic>e</italic><sub><italic>j</italic></sub>) then depends only on their latent type and their distance through a link function <italic>f</italic>(⋅, <italic>d</italic>(<italic>e</italic><sub><italic>i</italic></sub>, <italic>e</italic><sub><italic>j</italic></sub>)). We assume <italic>f</italic> is parameterized based on the latent type, <italic>c</italic><sub><italic>i</italic></sub> = <italic>m</italic> and <italic>c</italic><sub><italic>j</italic></sub> = <italic>n</italic>, via a parameter <italic>η</italic><sub><italic>mn</italic></sub>, as well as a set of global hyperparameters <italic>θ</italic>, such that the link function is <italic>f</italic>(<italic>d</italic>(<italic>e</italic><sub><italic>i</italic></sub>, <italic>e</italic><sub><italic>j</italic></sub>)|<italic>η</italic><sub><italic>mn</italic></sub>, <italic>θ</italic>).</p><p>We then jointly infer the MAP estimate of the class assignment vector (<italic>c</italic>) = {<italic>c</italic><sub><italic>i</italic></sub>}, the parameter matrix <italic>η</italic><sub><italic>mn</italic></sub>, and the global model hyperparameters <italic>θ</italic>:<disp-formula id="equ3"><label>(2)</label><mml:math id="m3"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>|</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We describe the spatial ‘logistic-distance Bernoulli’ function here, and others in the supplemental material.</p><p>The ‘logistic-distance Bernoulli’ spatial model assumes that, if cell <italic>e</italic><sub><italic>i</italic></sub> is of type <italic>m</italic> and cell <italic>e</italic><sub><italic>j</italic></sub> is of type <italic>n</italic>, then <italic>η</italic><sub><italic>mn</italic></sub> = (<italic>μ</italic><sub><italic>mn</italic></sub>, <italic>λ</italic><sub><italic>mn</italic></sub>), and the probability that two cells <italic>e</italic><sub><italic>i</italic></sub> and <italic>e</italic><sub><italic>j</italic></sub> are connected is given by<disp-formula id="equ4"><label>(3)</label><mml:math id="m4"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mtext>*</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mtext>*</mml:mtext></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>p</italic><sub><italic>max</italic></sub> and <italic>p</italic><sub><italic>min</italic></sub> are global per-graph parameters.</p><p>We place exponential priors on the latent parameters:<disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>using <italic>λ</italic><sup><italic>hp</italic></sup> and <italic>μ</italic><sup><italic>hp</italic></sup> as global per-graph hyperparameters.</p><p>We use a Dirichlet-process prior on class assignments, which allows the number of classes to be determined automatically. In brief, for <italic>N</italic> total cells, the probability of a cell belonging to a class is proportional to the number of data points already in that class, <italic>N</italic><sub><italic>k</italic></sub>, such that <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> and the probability of the cell belonging to a new class <italic>k</italic>′ is <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mfrac><mml:mi>α</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. <italic>α</italic> is the global concentration parameter—larger values of <italic>α</italic> make the model more likely to propose new classes. We grid the parameter <italic>α</italic> and allow the best value to be learned from the data.</p><p>Where we model cell depth, we assume that each cell type has a typical depth, and thus a Gaussian distribution of <italic>s</italic><sub><italic>i</italic></sub>. We assume <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where the (<italic>s</italic>) superscript indicates these model parameters are associated with the soma-depth portion of our model. We use a conjugate prior for <inline-formula><mml:math id="inf7"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf8"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:msup><mml:mi>χ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. The use of conjugacy simplifies inference while allowing for each cell type to have its own depth mean and distribution.</p><p>We model synapse depth profile that each cell type has a characteristic depth distribution of synaptic contact points, and mixture of Gaussian distributions over cell is <italic>N</italic><sub><italic>i</italic></sub> contact points, <bold>g</bold><sup><bold>i</bold></sup>. We do this by assuming the <inline-formula><mml:math id="inf10"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are drawn from an <italic>M</italic> = 3-component mixture of Gaussians. Thus associated with each cell type <italic>k</italic> is a vector of <italic>M</italic> Gaussian means <inline-formula><mml:math id="inf11"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and a mixture vector <italic>π</italic><sub><italic>k</italic></sub>. This representation can thus model depth distributions of contact points that have up to three modes, an assumption that is well matched in the bulk of anatomical studies of cell-type-dependent connectivity.</p></sec><sec id="s5-2"><title>Inference</title><p>We perform posterior inference via MCMC, annealing on the global likelihood during the traditional burn-in phase. MCMC transition kernels for different parts of the state space can be chained together to construct a kernel whose ergodic distribution is the target ergodic distribution over the entire state space.</p><p>Our first transition kernel (‘structural’) performs Gibbs sampling of the assignment vector <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>|</mml:mo><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The lack of conjugacy in our likelihood model makes an explicit evaluation of the conditional assignment probabilities impossible, motivating us to use an auxiliary variable method (<xref ref-type="bibr" rid="bib26">Neal, 2000</xref>) in which a collection of ephemeral classes is explicitly represented for the duration of the Gibbs scan.</p><p>We then employ a transition kernel to update the per-component parameter values <italic>η</italic><sub><italic>mn</italic></sub>. Conditioned on the assignment vector <bold>c</bold> and the model hyperparameters <italic>θ</italic>, <italic>α</italic> the individual <italic>η</italic><sub><italic>mn</italic></sub> are independent. We slice sample (<xref ref-type="bibr" rid="bib27">Neal, 2003</xref>) each component's parameters, choosing the slice width as a function of the global hyperparameter range.</p><p>The global hyperparameters, both <italic>α</italic> and <italic>θ</italic>, are allowed to take on a discrete set of possible values. As <italic>θ</italic> is often a tuple of possible values, we explore the Cartesian product of all possible values. We then Gibbs sample (our final transition kernel), which is always possible in a small, finite, discrete state space.</p><p>We chain these three kernels together, and then globally anneal on the likelihood from a temperature of <italic>T</italic> = 64 down to <italic>T</italic> = 1 over 900 iterations unless otherwise indicated, and then run the chain for another 100 iterations. We then generate at least 20 samples, each taken from the end of a single Markov chain initialized from different random initial points in the state space. For visualization we pick the chain with the highest log likelihood, but for all numerical comparisons (including link probability and cluster accuracy) we use this full collection of samples from the posterior distribution to estimate the resulting statistics.</p></sec><sec id="s5-3"><title>Link prediction</title><p>To compute link-prediction accuracy, we compute the probability of a link between two cells using each model, trained via 10-fold cross-validation. We use a full collection of posterior samples when computing the link probability, and then compute the area under the ROC curve for each.</p><p>We compare our model with a standard network clustering model, the latent-position clustering model. This model assumes each cell belongs to one of <italic>K</italic> clusters, and each cluster is associated with a <italic>d</italic>-dimensional Gaussian distribution. The probability of a link is then a function of the distance between the data points in this continuous space. We use a variational implementation provided in R (<xref ref-type="bibr" rid="bib30">Salter-Townshend and Murphy, 2013</xref>), parametrically varying the number of latent dimensions and the number of requested groups. While this model provides reasonable link-predictive accuracy, the clusterings dramatically disagree with those from human anatomists.</p></sec><sec id="s5-4"><title>Parameters</title><p>Hierarchical generative models can be sensitive to hyperparameter settings, thus for most hyperparameters we perform inference. In cases where we cannot, we run separate collections of Markov chains at separate settings and show the results across all pooled parameters. For the case of the mouse retina data, we consider maximum link probability <inline-formula><mml:math id="inf13"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0.95</mml:mn><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn><mml:mo>,</mml:mo><mml:mn>0.7</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, variance scales for the synapse density profile of <inline-formula><mml:math id="inf14"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (of normalized depth), and <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>K</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> possible synapse density profile mixture components. For the connectivity-distance-only model we actually perform inference over both <italic>p</italic><sub><italic>max</italic></sub> and <italic>p</italic><sub><italic>min</italic></sub>.</p></sec><sec id="s5-5"><title>Mixing of our Markov chains</title><p>Evaluating whether or not approximate inference methods, such as MCMC, produce samples which are valid approximations of the posterior distribution is an ongoing area of research in the computational statistics community. We use a rough proxy here—synthetic likelihood evaluation. For synthetic datasets of sizes comparable to our real data size, do we recover known ground truth information after running our Markov chains for the appropriate amount of time?</p><p><xref ref-type="fig" rid="fig9">Figures 9</xref> and <xref ref-type="fig" rid="fig10">10</xref> show the cluster accuracy (ARI) to ground truth and the total log score as a function of runtime. We see dramatic changes in log score initially as we vary the temperature, stabilizing as runtime progresses, for each chain. Then we see the characteristic jumps between nearby modes towards the end of the run, in both log score and ARI. Importantly, regardless of whether our model over- or under-estimates the exact posterior variance about the network, we find points in the latent variable space that are both predictive <italic>and</italic> parsimonious, largely agreeing with the human anatomists and predicting existing connections.<fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.04250.011</object-id><label>Figure 9.</label><caption><title>Adjusted Rand index (ARI) for synthetic data as a function of run iteration.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.011">http://dx.doi.org/10.7554/eLife.04250.011</ext-link></p></caption><graphic xlink:href="elife-04250-fig9-v1.tif"/></fig><fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.04250.012</object-id><label>Figure 10.</label><caption><title>Total model score (log score) versus wall clock time. </title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.012">http://dx.doi.org/10.7554/eLife.04250.012</ext-link></p></caption><graphic xlink:href="elife-04250-fig10-v1.tif"/></fig></p></sec><sec id="s5-6"><title>Dataset details</title><sec id="s5-6-1"><title>Mouse retina</title><p>Dense serial electron microscopy of a 114 μm × 80 μm area in the mouse retina by <xref ref-type="bibr" rid="bib12">Helmstaedter et al. (2013</xref>) yielded a listing of places where neurons come into contact. There were over 1000 cells originally, and we selected the 950 for which the location of the soma could be reconstructed from the provided cell plots (soma locations were not provided by the study's authors in machine-readable form). The result was a matrix of the total synapse-like contact area between all pairs of 950 cells. Area was thresholded at 0.1 μm, determined by hand, to yield a 950 × 950 entry matrix that served as input to our algorithm. We measured the distance between cells using the reconstructed soma centers, and used the logistic-distance spatial relation. Hyperprior griddings are shown in the ‘Hyperprior grids and hyperprior inference’ section.</p></sec><sec id="s5-6-2"><title>C. elegans</title><p>We obtained the connectome of <italic>C. elegans</italic> from data published previously (<xref ref-type="bibr" rid="bib33">Varshney et al., 2011</xref>), and isolated the 279 non-pharyngeal neurons, with a total of 6393 chemical synapses and 890 gap junctions originally cleaned up in <xref ref-type="bibr" rid="bib4">Chen et al. (2006</xref>). A cell's position was its distance along the anterior–posterior axis normalized between 0 and 1. We used both networks, the chemical network as a directed graph and the electrical network as an undirected graph. We use the synapse counts with the logistic-distance Poisson likelihood, scaling the counts by 4.0 to compensate for the Poisson's overdispersion.</p></sec><sec id="s5-6-3"><title>Microprocessor</title><p>We extracted the connection graph for the transistors in the MOS 6502 (<xref ref-type="bibr" rid="bib16">James et al., 2010</xref>). Each transistor has three terminals (gate, source, drain), but the methods of the original dataset were unable to consistently resolve which of the C1 and C2 terminals were source and drain, leading to ambiguity in our encoding. We identified a region consisting of three registers X, Y, and S via visual inspection and focused our efforts there. We created a total of six connectivity matrices by examining possible terminal pairings. For example, one graph encodes the connectivity between pins <inline-formula><mml:math id="inf17a"><mml:mrow><mml:mtext>g</mml:mtext></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf17b"><mml:mrow><mml:msub><mml:mtext>c</mml:mtext><mml:mo>1</mml:mo></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> We then have, <inline-formula><mml:math id="inf16"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if transistor <italic>e</italic><sub><italic>j</italic></sub> and <italic>e</italic><sub><italic>j</italic></sub> are connected via pins <italic>g</italic> and <italic>c</italic><sub>1</sub>.</p></sec></sec><sec id="s5-7"><title>Other likelihoods</title><p>We reparameterized the logistic-distance Bernoulli likelihood to better capture the microprocessor data structure. We are explicitly setting the maximum probability <italic>p</italic> of the logistic function on a per-component basis, drawing from a global <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>p</mml:mi><mml:mo>∼</mml:mo><mml:mtext>Beta</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Then <italic>λ</italic> is set for each component as a global hyperparameter, <italic>λ</italic>.</p><p>The ‘logistic-distance Poisson’ spatial model is used to explicitly model the count of synapses, <italic>c</italic>, between two neurons. The probability of <italic>c</italic> synapses between two neurons is distributed <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>c</mml:mi><mml:mo>∼</mml:mo><mml:mtext>Poisson</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic>r</italic> (the ‘rate’) is generated by a scaled logistic function (the logistic function has range [0, 1]). For each component <italic>η</italic><sub><italic>mn</italic></sub> we learn both the threshold <italic>μ</italic><sub><italic>mn</italic></sub> and the rate scaling factor <italic>r</italic><sub><italic>mn</italic></sub>. Thus if cells <italic>m</italic> and <italic>n</italic> are likely to have on average 20 synapses if they are closer than 5 μm, then <italic>μ</italic><sub><italic>mn</italic></sub> = 5 and <italic>r</italic><sub><italic>mn</italic></sub> = 20.</p><p>Thus the probability of <italic>R</italic>(<italic>e</italic><sub><italic>i</italic></sub>, <italic>e</italic><sub><italic>j</italic></sub>) = <italic>c</italic> synapses between two cells <italic>e</italic><sub><italic>i</italic></sub> and <italic>e</italic><sub><italic>j</italic></sub> is given by:<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mtext>*</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>λ</mml:mi></mml:mfrac></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mtext>*</mml:mtext></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><label>(9)</label><mml:math id="m10"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mtext>Poisson</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>λ</italic> and <italic>r</italic><sub><italic>min</italic></sub> are per-graph parameters and we have per-component parameters <inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>Exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>Exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s5-8"><title>Source code and data</title><p>All source code and materials for running experiments can be obtained from the project website, at <ext-link ext-link-type="uri" xlink:href="http://ericmjonas.github.io/connectodiscovery/">http://ericmjonas.github.io/connectodiscovery/</ext-link>.</p><p>All preprocessed data has been made publically available as well.</p></sec><sec id="s5-9"><title>Extension to multiple graphs</title><p>The model can handle multiple graphs <italic>R</italic><sup><italic>q</italic></sup> simultaneously with a shared clustering by extending the likelihood to include the product of the likelihoods of the individual graphs.<disp-formula id="equ11"><label>(10)</label><mml:math id="m11"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo/><mml:msup><mml:mi>η</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mo/></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo/><mml:msup><mml:mi>θ</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mo/></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo/><mml:msup><mml:mi>R</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mo/></mml:mrow><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle><mml:mi>q</mml:mi></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mi>q</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msubsup><mml:mo>|</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo>|</mml:mo><mml:mi>α</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s5-10"><title>Hyperprior grids and hyperprior inference</title><p>For the mouse retina logistic-distance Bernoulli model, we gridded <italic>μ</italic><sup><italic>hp</italic></sup> and <italic>λ</italic><sup><italic>hp</italic></sup> into 40 log<sub>10</sub>-spaced points 1.0 and 80.</p><p>For the <italic>C. elegans</italic> data with the logistic-distance Poisson model, we gridded <italic>μ</italic><sub><italic>hp</italic></sub> and <italic>λ</italic> into 20 log<sub>10</sub>-spaced points between 0.2 and 2.0, and the <italic>ratescale</italic><sup><italic>hp</italic></sup> parameter into 20 log<sub>10</sub>-spaced points between 2.0 and 20.0. We globally set <italic>rate</italic><sub><italic>min</italic></sub> = 0.01.</p><p>For the microprocessor with the logistic-distance with fixed lambda parameter and Bernoulli likelihood, we gridded <italic>mu</italic><sub><italic>hp</italic></sub> into 50 log<sub>10</sub>-spaced points between 10 and 500 and set <italic>λ</italic> = <italic>μ</italic><sub><italic>hp</italic></sub>/10. <inline-formula><mml:math id="inf21"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0.001</mml:mn><mml:mo>,</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>0.02</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and both <italic>p</italic><sub><italic>α</italic></sub> and <inline-formula><mml:math id="inf22"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn><mml:mo>,</mml:mo><mml:mn>2.0</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s5-11"><title>Measuring clustering similarity</title><p>We compare discovered types to known types via cluster comparison metrics: cluster homogeneity, cluster completeness, and the ARI. Homogeneity measures how many true types are in a given found type. If every cell type is split into two types, each subtype is still completely homogeneous. Completeness measures how many members of a given true type are split across found types.</p><p>ARI takes into account both effects (<xref ref-type="bibr" rid="bib15">Hubert and Arabie, 1985</xref>)—two identical clusterings have an ARI of 1.0, while progressively more dissimilar clusters have lower ARIs, becoming slightly negative as the clustering gets anti-correlated.</p><p><xref ref-type="fig" rid="fig11">Figure 11</xref> shows the result of taking 20 different clusters and moving data points between them according to the following operations.<list list-type="simple"><list-item><p>•<bold>distribute</bold>: take a class and distribute its elements uniformly among the remaining types.</p></list-item><list-item><p>•<bold>merge</bold>: take a type and merge it into another existing type.</p></list-item><list-item><p>•<bold>split</bold>: take a type and split it into two distinct types.</p></list-item></list><fig id="fig11" position="float"><object-id pub-id-type="doi">10.7554/eLife.04250.013</object-id><label>Figure 11.</label><caption><title>Type agreement evaluation metrics as a function of splitting types, merging types, and randomly distributing cells between types.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04250.013">http://dx.doi.org/10.7554/eLife.04250.013</ext-link></p></caption><graphic xlink:href="elife-04250-fig11-v1.tif"/></fig></p><p>We can see the impact on ARI, completeness, and homogeneity as we perform these operations on more of the original 20 types. In all cases, ‘distribution’ of one type among the others is detrimental to the metric. Splitting impacts completeness but not homogeneity, and merging impacts homogeneity but not completeness.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Josh Vogelstein for discussions and reading of the manuscript, Finale Doshi-Velez for early discussions on the model, and Erica Peterson, Jonathan Glidden, and Yarden Katz for extensive manuscript review. Funding for compute time was provided by Amazon Web Services ‘AWS in Education’ grants.</p></ack><sec id="s6" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>EJ, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>KK, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group></sec><sec id="s7" sec-type="supplementary-material"><title>Additional files</title><sec id="s7-1" sec-type="datasets"><title>Major dataset</title><p>The following dataset was generated:</p><p><related-object content-type="generated-dataset" id="dataro1" source-id="https://github.com/ericmjonas/circuitdata" source-id-type="uri"><collab collab-type="author">Jonas E</collab>, <year>2015</year><x>, </x><source>Connectomics datasets</source><x>, </x><ext-link ext-link-type="uri" xlink:href="https://github.com/ericmjonas/circuitdata">https://github.com/ericmjonas/circuitdata</ext-link><x>, </x><comment>Canonical, cleaned-up datasets, publicly available at GitHub, originally published in <xref ref-type="bibr" rid="bib33">Varshney et al., 2011</xref> (<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1001066">http://dx.doi.org/10.1371/journal.pcbi.1001066</ext-link>) and <xref ref-type="bibr" rid="bib12">Helmstaedter et al., 2013</xref> (<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12346">http://dx.doi.org/10.1038/nature12346</ext-link>).</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anandkumar</surname><given-names>A</given-names></name><name><surname>Ge</surname><given-names>R</given-names></name><name><surname>Hsu</surname><given-names>D</given-names></name><name><surname>Kakade</surname><given-names>SM</given-names></name><name><surname>Telgarsky</surname><given-names>M</given-names></name></person-group><year>2012</year><article-title>Tensor decompositions for learning latent variable models</article-title><source>Journal of Machine Learning Research</source><volume>15</volume><fpage>1</fpage><lpage>55</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barthó</surname><given-names>P</given-names></name><name><surname>Hirase</surname><given-names>H</given-names></name><name><surname>Monconduit</surname><given-names>L</given-names></name><name><surname>Zugaro</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year>2004</year><article-title>Characterization of neocortical principal cells and interneurons by network interactions and extracellular features</article-title><source>Journal of Neurophysiology</source><volume>92</volume><fpage>600</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1152/jn.01170.2003</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>SP</given-names></name><name><surname>Hestrin</surname><given-names>S</given-names></name></person-group><year>2009</year><article-title>Cell-type identity: a key to unlocking the function of neocortical circuits</article-title><source>Current Opinion in Neurobiology</source><volume>19</volume><fpage>415</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2009.07.011</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>BL</given-names></name><name><surname>Hall</surname><given-names>DH</given-names></name><name><surname>Chklovskii</surname><given-names>DB</given-names></name></person-group><year>2006</year><article-title>Wiring optimization can relate neuronal structure and function</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>103</volume><fpage>4723</fpage><lpage>4728</lpage><pub-id pub-id-type="doi">10.1073/pnas.0506806103</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Douglas</surname><given-names>R</given-names></name><name><surname>Martin</surname><given-names>K</given-names></name></person-group><year>1991</year><article-title>A functional microcircuit for cat visual cortex</article-title><source>The Journal of Physiology</source><volume>440</volume><fpage>735</fpage><lpage>769</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1991.sp018733</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Hill</surname><given-names>S</given-names></name><name><surname>Schürmann</surname><given-names>F</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name></person-group><year>2013</year><article-title>A hierarchical structure of cortical interneuron electrical diversity revealed by automated statistical analysis</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>2994</fpage><lpage>3006</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs290</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freund</surname><given-names>T</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year>1998</year><article-title>Interneurons of the hippocampus</article-title><source>Hippocampus</source><volume>6</volume><fpage>347</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1996)6:4&lt;347::AID-HIPO1&gt;3.0.CO;2-I</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girvan</surname><given-names>M</given-names></name><name><surname>Newman</surname><given-names>ME</given-names></name></person-group><year>2002</year><article-title>Community structure in social and biological networks</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>99</volume><fpage>7821</fpage><lpage>7826</lpage><pub-id pub-id-type="doi">10.1073/pnas.122653799</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grillner</surname><given-names>S</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>De Schutter</surname><given-names>E</given-names></name><name><surname>Silberberg</surname><given-names>G</given-names></name><name><surname>LeBeau</surname><given-names>FE</given-names></name></person-group><year>2005</year><article-title>Microcircuits in action from CPGs to neocortex</article-title><source>Trends in Neurosciences</source><volume>28</volume><fpage>525</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2005.08.003</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guerra</surname><given-names>L</given-names></name><name><surname>McGarry</surname><given-names>LM</given-names></name><name><surname>Robles</surname><given-names>V</given-names></name><name><surname>Bielza</surname><given-names>C</given-names></name><name><surname>Larrañaga</surname><given-names>P</given-names></name><name><surname>Yuste</surname><given-names>R</given-names></name></person-group><year>2011</year><article-title>Comparison between supervised and unsupervised classifications of neuronal cell types: a case study</article-title><source>Developmental Neurobiology</source><volume>71</volume><fpage>71</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1002/dneu.20809</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helmstaedter</surname><given-names>M</given-names></name><name><surname>Briggman</surname><given-names>KL</given-names></name><name><surname>Turaga</surname><given-names>SC</given-names></name><name><surname>Jain</surname><given-names>V</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name><name><surname>Denk</surname><given-names>W</given-names></name></person-group><year>2013</year><article-title>Connectomic reconstruction of the inner plexiform layer in the mouse retina</article-title><source>Nature</source><volume>500</volume><fpage>168</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1038/nature12346</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname><given-names>SL</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Riachi</surname><given-names>I</given-names></name><name><surname>Schürmann</surname><given-names>F</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year>2012</year><article-title>Statistical connectivity provides a sufficient foundation for specific functional connectivity in neocortical neural microcircuits</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>109</volume><fpage>E2885</fpage><lpage>E2894</lpage><pub-id pub-id-type="doi">10.1073/pnas.1202128109</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Blei</surname><given-names>DM</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Paisley</surname><given-names>J</given-names></name></person-group><year>2013</year><article-title>Stochastic variational inference</article-title><source>Journal of Machine Learning Research</source><volume>14</volume><fpage>1303</fpage><lpage>1347</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubert</surname><given-names>L</given-names></name><name><surname>Arabie</surname><given-names>P</given-names></name></person-group><year>1985</year><article-title>Comparing partitions</article-title><source>Journal of Classification</source><volume>2</volume><fpage>193</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1007/BF01908075</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>James</surname><given-names>G</given-names></name><name><surname>Silverman</surname><given-names>B</given-names></name><name><surname>Silverman</surname><given-names>B</given-names></name></person-group><year>2010</year><article-title>Visualizing a classic CPU in action</article-title><source>ACM SIGGRAPH 2010 Talks</source><publisher-loc>New York, USA</publisher-loc><publisher-name>ACM Press</publisher-name><fpage>1</fpage><pub-id pub-id-type="doi">10.1145/1837026.1837061</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kemp</surname><given-names>C</given-names></name><name><surname>Tenenbaum</surname><given-names>J</given-names></name><name><surname>Griffiths</surname><given-names>T</given-names></name></person-group><year>2006</year><article-title>Learning systems of concepts with an infinite relational model</article-title><source>Proceedings of the 21st National Conference on Artificial Intelligence</source><volume>volume 1</volume><publisher-loc>Palo Alto, California</publisher-loc><publisher-name>AAAI Press</publisher-name><fpage>381</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1145/1837026.1837061</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koboldt</surname><given-names>DC</given-names></name><name><surname>Steinberg</surname><given-names>KM</given-names></name><name><surname>Larson</surname><given-names>DE</given-names></name><name><surname>Wilson</surname><given-names>RK</given-names></name><name><surname>Mardis</surname><given-names>ER</given-names></name></person-group><year>2013</year><article-title>The next-generation sequencing revolution and its impact on genomics</article-title><source>Cell</source><volume>155</volume><fpage>27</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2013.09.006</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lennie</surname><given-names>P</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year>2005</year><article-title>Coding of color and form in the geniculostriate visual pathway (invited review)</article-title><source>Journal of the Optical Society of America. A, Optics, Image Science, and Vision</source><volume>22</volume><fpage>2013</fpage><lpage>2033</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.22.002013</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masland</surname><given-names>RH</given-names></name></person-group><year>2001</year><article-title>The fundamental plan of the retina</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>877</fpage><lpage>886</lpage><pub-id pub-id-type="doi">10.1038/nn0901-877</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milo</surname><given-names>R</given-names></name><name><surname>Shen-Orr</surname><given-names>S</given-names></name><name><surname>Itzkovitz</surname><given-names>S</given-names></name><name><surname>Kashtan</surname><given-names>N</given-names></name><name><surname>Chklovskii</surname><given-names>D</given-names></name><name><surname>Alon</surname><given-names>U</given-names></name></person-group><year>2002</year><article-title>Network motifs: simple building blocks of complex networks</article-title><source>Science</source><volume>298</volume><fpage>824</fpage><lpage>827</lpage><pub-id pub-id-type="doi">10.1126/science.298.5594.824</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgan</surname><given-names>JL</given-names></name><name><surname>Lichtman</surname><given-names>JW</given-names></name></person-group><year>2013</year><article-title>Why not connectomics?</article-title><source>Nature Methods</source><volume>10</volume><fpage>494</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2480</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mountcastle</surname><given-names>VB</given-names></name></person-group><year>1957</year><article-title>Modality and topographic properties of single neurons of cat's somatic sensory cortex</article-title><source>Journal of Neurophysiology</source><volume>20</volume><fpage>408</fpage><lpage>434</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mountcastle</surname><given-names>VB</given-names></name></person-group><year>1997</year><article-title>The columnar organization of the neocortex</article-title><source>Brain</source><volume>120</volume><fpage>701</fpage><lpage>722</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>KP</given-names></name></person-group><year>2012</year><source>Machine learning: A probabilistic perspective</source><publisher-loc>Cambridge</publisher-loc><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neal</surname><given-names>RM</given-names></name></person-group><year>2000</year><article-title>Markov chain sampling methods for Dirichlet process mixture models</article-title><source>Journal of Computational and Graphical Statistics</source><volume>9</volume><fpage>249</fpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neal</surname><given-names>R</given-names></name></person-group><year>2003</year><article-title>Slice sampling</article-title><source>Annals of Statistics</source><volume>31</volume><fpage>705</fpage><lpage>741</lpage><pub-id pub-id-type="doi">10.1214/aos/1056562461</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nowicki</surname><given-names>K</given-names></name><name><surname>Snijders</surname><given-names>TA</given-names></name></person-group><year>2001</year><article-title>Estimation and prediction for stochastic blockstructures</article-title><source>Journal of the American Statistical Association</source><volume>96</volume><fpage>1077</fpage><lpage>1087</lpage><pub-id pub-id-type="doi">10.1198/016214501753208735</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Passingham</surname><given-names>RE</given-names></name></person-group><year>2002</year><article-title>The frontal cortex: does size matter?</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>190</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1038/nn0302-190</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salter-Townshend</surname><given-names>M</given-names></name><name><surname>Murphy</surname><given-names>TB</given-names></name></person-group><year>2013</year><article-title>Variational Bayesian inference for the latent position cluster model for network data</article-title><source>Computational Statistics &amp; Data Analysis</source><volume>57</volume><fpage>661</fpage><lpage>671</lpage><pub-id pub-id-type="doi">10.1016/j.csda.2012.08.004</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname><given-names>HS</given-names></name><name><surname>Sümbül</surname><given-names>U</given-names></name></person-group><year>2014</year><article-title>Neuronal cell types and connectivity: Lessons from the retina</article-title><source>Neuron</source><volume>83</volume><fpage>1262</fpage><lpage>1272</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.054</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sincich</surname><given-names>LC</given-names></name><name><surname>Horton</surname><given-names>JC</given-names></name></person-group><year>2005</year><article-title>The circuitry of V1 and V2: integration of color, form, and motion</article-title><source>Annual Review of Neuroscience</source><volume>28</volume><fpage>303</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.28.061604.135731</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varshney</surname><given-names>LR</given-names></name><name><surname>Chen</surname><given-names>BL</given-names></name><name><surname>Paniagua</surname><given-names>E</given-names></name><name><surname>Hall</surname><given-names>DH</given-names></name><name><surname>Chklovskii</surname><given-names>DB</given-names></name></person-group><year>2011</year><article-title>Structural properties of the <italic>Caenorhabditis elegans</italic> neuronal network</article-title><source>PLOS Computational Biology</source><volume>7</volume><fpage>e1001066</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1001066</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watts</surname><given-names>DJ</given-names></name><name><surname>Strogatz</surname><given-names>SH</given-names></name></person-group><year>1998</year><article-title>Collective dynamics of ‘small-world’ networks</article-title><source>Nature</source><volume>393</volume><fpage>440</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1038/30918</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>J</given-names></name><name><surname>Southgate</surname><given-names>E</given-names></name><name><surname>Thomson</surname><given-names>J</given-names></name><name><surname>Brenner</surname><given-names>S</given-names></name></person-group><year>1986</year><article-title>The structure of the nervous system of the nematode <italic>Caenorhabditis elegans</italic></article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>314</volume><fpage>1</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1098/rstb.1986.0056</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Tresp</surname><given-names>V</given-names></name><name><surname>Yu</surname><given-names>K</given-names></name><name><surname>Kriegel</surname><given-names>HP</given-names></name></person-group><year>2006</year><article-title>Infinite hidden relational models</article-title><source>Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence</source><publisher-loc>Arlington, Virginia</publisher-loc><publisher-name>AUAI Press</publisher-name></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zador</surname><given-names>AM</given-names></name><name><surname>Dubnau</surname><given-names>J</given-names></name><name><surname>Oyibo</surname><given-names>HK</given-names></name><name><surname>Zhan</surname><given-names>H</given-names></name><name><surname>Cao</surname><given-names>G</given-names></name><name><surname>Peikon</surname><given-names>ID</given-names></name></person-group><year>2012</year><article-title>Sequencing the connectome</article-title><source>PLOS Biology</source><volume>10</volume><fpage>e1001411</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.1001411</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.04250.014</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Skinner</surname><given-names>Frances K</given-names></name><role>Reviewing editor</role><aff><institution>University Health Network, and University of Toronto</institution>, <country>Canada</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for sending your work entitled “Automatic discovery of cell types and microcircuitry from neural connectomics” for consideration at <italic>eLife</italic>. Your article has been evaluated by a Senior editor, a Reviewing editor and three reviewers, and it was decided that a revised submission should be considered.</p><p>The Reviewing editor and the reviewers discussed their comments before we reached this decision, and the Reviewing editor has assembled the following comments to help you prepare a revised submission.</p><p>Overall, the reviewers felt that the work potentially presents a major advance, addressing the problem of automatic analysis of large, augmented connectomic data sets, a problem that is quickly growing in importance as these data sets become available. As such, the work is considered to be very timely. However, it was also felt that it is unclear how the algorithm works and what its limitations might be. A more careful description and presentation of the method with added discussions is strongly needed. In other words, does the method actually work? As presented, it seems magical and impressive, and there was some concern as to whether it was too good to be true. Data and methods need to be made available as well as having controls and showing ROC curves to allow further evaluation. More rigor in defining terms and labeling figures is also needed. It was also felt that the figures, while aesthetically pleasing, were not helpful for evaluation of the algorithm.</p><p>Specifically, the following aspects were raised:</p><p>1) Explain the algorithm limitations, i.e., where it breaks down (noting that the reviewers appreciate that no algorithm can solve every problem). This might be best done in the context of a toy problem. Any potential user of this algorithm would benefit from understanding its limitations better. Under what conditions does it break down? What are the pressure points? For example, what if the data set has structure, but there really aren't distinct classes (as has been proposed for cortex)? Is it particularly sensitive to some of the choices of priors? Etc. Perhaps other examples would provide better insight into potential failure modes.</p><p>2) Show cross-validation on the link prediction.</p><p>3) As given, the cell type identity analysis is difficult to parse; the circos-style plots don't tell much and the other plots are often inadequately labelled. It is also unclear as to how validation was performed or was to be observed. Simply seeing ROC curves (or reported areas) or something similar is needed. The figures are of low informational value. How would some well-known supervised method do at this task in cross-validation? It would seem, intuitively, to be a natural choice for learning cell identity. See, for example, PMID: 21154911, for a well-defined analysis.</p><p>Show ROC style prediction on the cell types. This can be done by cross-validation i.e., do the clustering, then label all the cells using, e.g., 2/3 of the human-labelled data. Then assign each unknown cell a label based on its co-membership with cells of known labels. This would end up giving a ranked score, since probabilistic. The comparison method would be something like nearest-neighbor assignment. To avoid making this a multi-class prediction problem, one could do it one class at a time and not worry about overlapped assignments. This would set a good standard for methods comparison.</p><p>4) Consideration and discussion about the edge effects in the retinal data.</p><p>The retina data set covers a relatively small volume of the retina (on the order of 100 microns in each direction) and most of the cells whose cell bodies are located within this volume have their neurites “cut off”, i.e. they are not fully contained within the volume. This will lead to edge effects that may impact or bias connection patterns, for example cells whose cell bodies are near the center of the field of view should have more synaptic contacts (more partners in the contact matrix) simply because they are more fully captured in the reconstruction. How have such potential edge effects been accounted for in the present analyses? Could there be improvement in the performance of the clustering algorithm by taking into account such biases?</p><p>5) Chip example was considered less critical as compared to providing more testing and further details and discussion on the retinal and <italic>C.elegans</italic> examples.</p><p>6) The use of connectivity information for characterizing nodes in a neural network has a long history in cognitive and systems neuroscience. It would be nice to point this out. A potential starting point is the review of Passingham et al. (2002) in Nature Reviews Neuroscience.</p><p>7) Additional comments regarding unhelpful/unclear figure aspects for algorithm specifics:</p><p>Are there any known inherent biases in terms of preferences in cluster sizes? It appears, from <xref ref-type="fig" rid="fig4">Figure 4</xref>, that there are a small number of fairly large (and quite uniform) clusters and a larger number of quite small ones (that seem quite noisy with respect to the anatomist's cell classification).</p><p>It is difficult to judge just by visual inspection alone how “good” the agreement between the automated cluster analysis and the anatomist's ordering scheme really is (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, outer ring). Is there a more quantitative way of determining the “fit” of the model with the a priori anatomical cell type assignments?</p><p>In a similar vein, the text claims (in the subsection headed “Recovering spatial connectivity in multiple graphs simultaneously”) that for <italic>C. elegans</italic> the clusters correspond roughly homogenously to moto, sensory and interneurons, but looking at <xref ref-type="fig" rid="fig5">Figure 5</xref>, it is not so clear. Interneurons are found in almost all of the clusters, and very few are truly homogeneous with respect to these three types of neurons. Again, it would be good to think of a way to quantify “homogeneity”.</p><p>In the circular plots (e.g. <xref ref-type="fig" rid="fig1">Figure 1I</xref>), it is not quite clear what the shaded arcs in the center refer to. Is the shading proportional to some sort of density (of what?), and what does the width of these arcs refer to?</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.04250.015</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>However, it was also felt that it is unclear how the algorithm works and what its limitations might be – a more careful description and presentation of the method with added discussions is strongly needed. In other words, does the method actually work? As presented, it seems magical and impressive, and there was some concern as to whether it was too good to be true. Data and methods need to be made available as well as having controls and showing ROC curves to allow further evaluation. More rigor in defining terms and labeling figures is also needed. It was also felt that the figures, while aesthetically pleasing, were not helpful for evaluation of the algorithm</italic>.</p><p>We have added a broad range of additional detailed analysis, added additional figures to directly address the usefulness of the algorithm. We have also added simulations to show under which circumstances the algorithm fails. Indeed, the fact that the reviewers ask for an analysis of failure modes instead of stating (as some high profile journals) that any of the (unavoidable) failure modes would be reasons for rejection, truly impressed us.</p><p><italic>Specifically, the following aspects were raised</italic>:</p><p><italic>1) Explain the algorithm limitations, i.e., where it breaks down (noting that the reviewers appreciate that no algorithm can solve every problem). This might be best done in the context of a toy problem. Any potential user of this algorithm would benefit from understanding its limitations better. Under what conditions does it break down? What are the pressure points? For example, what if the data set has structure, but there really aren't distinct classes (as has been proposed for cortex)? Is it particularly sensitive to some of the choices of priors? Etc. Perhaps other examples would provide better insight into potential failure modes</italic>.</p><p>We have added simulations that show the failure modes of the algorithms. There are two kinds of failures: (1) the algorithm may not succeed at correctly solving the problem it is meant to solve, (2) the problem may not succeed at the problem we scientists want it to solve because it is really solving a different problem. We have added simulations to get at these two issues. In short, as we had already shown with respect to (1), the algorithm works worse and worse as the classes are made more and more similar as this slows down mixing. Such behavior is typical for MCMC algorithms. With respect to (2), we constructed cases where the algorithm infers cell-types where they are none, in the case of continuous distributions and cases where the algorithm does not find the cell-types because other aspects of the cell types (e.g. boundaries of reconstructed areas) make estimation impossible.</p><p><italic>2) Show cross-validation on the link prediction</italic>.</p><p>Done.</p><p><italic>3) As given, the cell type identity analysis is difficult to parse; the circos-style plots don't tell much and the other plots are often inadequately labelled. It is also unclear as to how validation was performed or was to be observed. Simply seeing ROC curves (or reported areas) or something similar is needed. The figures are of low informational value. How would some well-known supervised method do at this task in cross-validation? It would seem, intuitively, to be a natural choice for learning cell identity</italic>. <italic>See, for example, PMID: 21154911, for a well-defined analysis</italic>.</p><p>We have abandoned the Circos-style connectivity plots that, while visually appealing, failed to do an adequate job conveying type and distance-related connectivity. We instead now use a linear plot of connectivity as a function of distance. We feel this more clearly demonstrates the distance-dependent nature of connectivity. We now discuss the possibility and limitations of supervised methods.</p><p><italic>Show ROC style prediction on the cell types. This can be done by cross-validation i.e., do the clustering, then label all the cells using, e.g., 2/3 of the human-labelled data. Then assign each unknown cell a label based on its co-membership with cells of known labels. This would end up giving a ranked score, since probabilistic. The comparison method would be something like nearest-neighbor assignment. To avoid making this a multi-class prediction problem, one could do it one class at a time and not worry about overlapped assignments. This would set a good standard for methods comparison</italic>.</p><p>To address this concern we have expanded the set of metrics we use to quantify clustering considering ARI, completeness, and homogeneity. ROC style predictions would be interesting and doable, if only the datasets were larger. As it stands there are only a very small number of examples of many types and removing more for analysis would weaken our power. We feel that this kind of concern is also well addressed by our new and much more comprehensive analysis of the properties of the algorithm.</p><p><italic>4) Consideration and discussion about the edge effects in the retinal data</italic>.</p><p><italic>The retina data set covers a relatively small volume of the retina (on the order of 100 microns in each direction) and most of the cells whose cell bodies are located within this volume have their neurites “cut off”, i.e. they are not fully contained within the volume. This will lead to edge effects that may impact or bias connection patterns, for example cells whose cell bodies are near the center of the field of view should have more synaptic contacts (more partners in the contact matrix) simply because they are more fully captured in the reconstruction. How have such potential edge effects been accounted for in the present analyses? Could there be improvement in the performance of the clustering algorithm by taking into account such biases</italic>?</p><p>Done.</p><p><italic>5) Chip example was considered less critical as compared to providing more testing and further details and discussion on the retinal and</italic> C.elegans <italic>examples</italic>.</p><p>We somewhat shortened the chip example. We feel it is still useful as it gets the reader to understand that this general class of algorithms could be useful beyond neuroscience.</p><p><italic>6) The use of connectivity information for characterizing nodes in a neural network has a long history in cognitive and systems neuroscience. It would be nice to point this out. A potential starting point is the review of Passingham et al. (2002) in Nature Reviews Neuroscience</italic>.</p><p>We added extensive citations to this part of the literature in a newly created Discussion paragraph.</p><p><italic>7) Additional comments regarding unhelpful/unclear figure aspects for algorithm specifics</italic>:</p><p><italic>Are there any known inherent biases in terms of preferences in cluster sizes? It appears, from</italic> <xref ref-type="fig" rid="fig4"><italic>Figure 4</italic></xref><italic>, that there are a small number of fairly large (and quite uniform) clusters and a larger number of quite small ones (that seem quite noisy with respect to the anatomist's cell classification)</italic>.</p><p>Our model uses a nonparmetric prior which generally believes that a “small” number of clusters can be used to predict the data, but (should the data support it) but is easily dominated by the likelihood of the data. The good agreement in both cluster size and type with synthetic data leads us to believe that this is not an effect of the prior. Additionally, we perform hyperparameter inference over the cluster concentration parameter alpha. This avoids over-biasing the model towards large clusters. Thus we believe the “noise” present in the clusterings truly reflects uncertainty in the data.</p><p><italic>It is difficult to judge just by visual inspection alone how “good” the agreement between the automated cluster analysis and the anatomist's ordering scheme really is (</italic><xref ref-type="fig" rid="fig3"><italic>Figure 3C</italic></xref><italic>, outer ring). Is there a more quantitative way of determining the “fit” of the model with the a priori anatomical cell type assignments</italic>?</p><p>As mentioned above, we added additional fit metrics. We also expanded the discussion of this in the text to make it easier to digest for the readers.</p><p><italic>In a similar vein, the text claims (in the subsection headed “Recovering spatial connectivity in multiple graphs simultaneously”) that for</italic> C. elegans <italic>the clusters correspond roughly homogenously to moto, sensory and interneurons, but looking at</italic> <xref ref-type="fig" rid="fig5"><italic>Figure 5</italic></xref><italic>, it is not so clear. Interneurons are found in almost all of the clusters, and very few are truly homogeneous with respect to these three types of neurons. Again, it would be good to think of a way to quantify “homogeneity”</italic>.</p><p>We added a quantification of homogeneity and toned down the discussion in the text.</p><p><italic>In the circular plots (e.g.</italic> <xref ref-type="fig" rid="fig1"><italic>Figure 1I</italic></xref><italic>), it is not quite clear what the shaded arcs in the center refer to. Is the shading proportional to some sort of density (of what?), and what does the width of these arcs refer to</italic>?</p><p>Per above, we have abandoned the circos-style plots, replacing them with (what we hope are) more intuitive and carefully-annotated linear figures.</p></body></sub-article></article>