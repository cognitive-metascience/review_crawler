<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">48764</article-id><article-id pub-id-type="doi">10.7554/eLife.48764</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>The bottom-up and top-down processing of faces in the human occipitotemporal cortex</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-149440"><name><surname>Fan</surname><given-names>Xiaoxu</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8115-8621</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-149441"><name><surname>Wang</surname><given-names>Fan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-149442"><name><surname>Shao</surname><given-names>Hanyu</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-149443"><name><surname>Zhang</surname><given-names>Peng</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-39918"><name><surname>He</surname><given-names>Sheng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5547-923X</contrib-id><email>sheng@umn.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">State Key Laboratory of Brain and Cognitive Science</institution><institution>Institute of Biophysics, Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution>University of Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Psychology</institution><institution>University of Minnesota</institution><addr-line><named-content content-type="city">Minneapolis</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Meng</surname><given-names>Ming</given-names></name><role>Reviewing Editor</role><aff><institution>South China Normal University</institution><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>14</day><month>01</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e48764</elocation-id><history><date date-type="received" iso-8601-date="2019-05-24"><day>24</day><month>05</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-01-10"><day>10</day><month>01</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Fan et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Fan et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-48764-v2.pdf"/><abstract><p>Although face processing has been studied extensively, the dynamics of how face-selective cortical areas are engaged remains unclear. Here, we uncovered the timing of activation in core face-selective regions using functional Magnetic Resonance Imaging and Magnetoencephalography in humans. Processing of normal faces started in the posterior occipital areas and then proceeded to anterior regions. This bottom-up processing sequence was also observed even when internal facial features were misarranged. However, processing of two-tone Mooney faces lacking explicit prototypical facial features engaged top-down projection from the right posterior fusiform face area to right occipital face area. Further, face-specific responses elicited by contextual cues alone emerged simultaneously in the right ventral face-selective regions, suggesting parallel contextual facilitation. Together, our findings chronicle the precise timing of bottom-up, top-down, as well as context-facilitated processing sequences in the occipital-temporal face network, highlighting the importance of the top-down operations especially when faced with incomplete or ambiguous input.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>face</kwd><kwd>MEG</kwd><kwd>Mooney face</kwd><kwd>top-down</kwd><kwd>occipitotemporal cortex</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Beijing Science and Technology Project</institution></institution-wrap></funding-source><award-id>Z181100001518002</award-id><principal-award-recipient><name><surname>He</surname><given-names>Sheng</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002855</institution-id><institution>Ministry of Science and Technology of the People's Republic of China</institution></institution-wrap></funding-source><award-id>2015CB351701</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Fan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100005200</institution-id><institution>Bureau of International Cooperation, Chinese Academy of Sciences</institution></institution-wrap></funding-source><award-id>153311KYSB20160030</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Peng</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Beijing Science and Technology Project</institution></institution-wrap></funding-source><award-id>Z171100000117003</award-id><principal-award-recipient><name><surname>He</surname><given-names>Sheng</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Both bottom-up and top-down processing are involved in the occipital-temporal face network, with the top-down modulation more extensively engaged when available information is sparse in the face images.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>There is ample evidence to show that the processing of face information involves a distributed neural network of face-sensitive areas in the occipitotemporal cortex and beyond (<xref ref-type="bibr" rid="bib23">Duchaine and Yovel, 2015</xref>; <xref ref-type="bibr" rid="bib45">Haxby et al., 2000</xref>). Three bilateral face-selective areas are considered as the core face-processing system, defined in functional Magnetic Resonance Imaging (fMRI) studies as regions showing significantly higher response to faces than objects, which are Occipital Face Area (OFA) in the inferior occipital gyrus (<xref ref-type="bibr" rid="bib31">Gauthier et al., 2000</xref>; <xref ref-type="bibr" rid="bib44">Haxby et al., 1999</xref>), Fusiform Face Area (FFA) in the fusiform gyrus (<xref ref-type="bibr" rid="bib53">Kanwisher et al., 1997</xref>; <xref ref-type="bibr" rid="bib41">Grill-Spector et al., 2004</xref>) and a face-sensitive area in the posterior superior temporal sulcus (pSTS) (<xref ref-type="bibr" rid="bib47">Hoffman and Haxby, 2000</xref>; <xref ref-type="bibr" rid="bib82">Puce et al., 1998</xref>). Similarly, a number of so-called face patches have been identified in macaque monkeys along the superior temporal sulcus (<xref ref-type="bibr" rid="bib99">Tsao et al., 2003</xref>; <xref ref-type="bibr" rid="bib100">Tsao et al., 2006</xref>; <xref ref-type="bibr" rid="bib101">Tsao et al., 2008</xref>). Although the functional properties of these areas have been studied extensively, we do not yet have a comprehensive understanding of how the face-processing network functions in a dynamic manner. Hierarchical models postulate that face specific processes are initiated in the OFA based on local facial features, then the information is forwarded to higher level regions, such as FFA, for holistic processing (<xref ref-type="bibr" rid="bib45">Haxby et al., 2000</xref>; <xref ref-type="bibr" rid="bib25">Fairhall and Ishai, 2007</xref>; <xref ref-type="bibr" rid="bib58">Liu et al., 2002</xref>). This model is supported by neuroimaging studies showing functional properties of face-selective areas and is consistent with generic local-to-global views of object processing. However, it has been challenged by results from studies in which patients with damaged OFA can still showed FFA activation to faces (<xref ref-type="bibr" rid="bib84">Rossion et al., 2003</xref>; <xref ref-type="bibr" rid="bib97">Steeves et al., 2006</xref>). Further, it was reported that during the perception of faces with minimal local facial features, FFA could still show face-preferential activation without face-selective inputs from OFA (<xref ref-type="bibr" rid="bib85">Rossion et al., 2011</xref>). Thus a non-hierarchical model was proposed postulating that face detection is initiated at the FFA followed by a fine analysis in the OFA (<xref ref-type="bibr" rid="bib85">Rossion et al., 2011</xref>; <xref ref-type="bibr" rid="bib32">Gentile et al., 2017</xref>). These competing models may reflect different modes of operation of the face network under different demands. To reconcile these models, a comprehensive dynamic picture of face processing under different conditions with more detailed temporal information is needed.</p><p>In the current study, we investigated the dynamics of face processing in the ‘core face processing system’ using Magnetoencephalography (MEG) and fMRI. We designed the face-related stimuli specifically to reveal mechanisms for processing 1) normal faces, 2) Mooney faces with very little explicit facial features, 3) distorted faces with internal facial features spatially misarranged, and 4) contextually induced face representations with internal facial features completely missing. During the experiment, subjects were presented with various types of face pictures while MEG signals were recorded. The key effort in this study was in reconstructing the source signals from the MEG sensor data, to obtain a dynamic depiction of cortical responses to faces and other types of stimuli. With the timing of activation revealed in each face-selective area in the ‘core face processing system’, we could uncover when and where face information is processed in the human brain.</p><p>The main findings are briefly summarized here. First, we revealed the basic, mainly bottom-up, processing sequence along ventral temporal cortex by presenting face pictures of famous individuals to subjects. Face processing was initiated in the posterior areas and then proceeded forward to anterior regions. Right OFA (rOFA) and right posterior FFA (rpFFA) were activated very close in time, peaking around 120 ms, while right anterior FFA (raFFA) reached its peak at about 150 ms. The right pSTS (rpSTS) in the dorsal pathway showed a weaker and temporally more variable response, participating in face processing within a time window from 130 to 180 ms. Then, we highlighted the top-down operation in face processing by using two-tone Mooney face images (<xref ref-type="bibr" rid="bib67">Mooney, 1957</xref>) lacking prototypical local facial features. According to the predictive coding theory (<xref ref-type="bibr" rid="bib83">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib70">Murray et al., 2004</xref>; <xref ref-type="bibr" rid="bib69">Mumford, 1992</xref>), face prediction created at FFA based on impoverished information of Mooney faces and prior knowledge is poorly matched with the input representation at OFA due to the lack of explicit local facial features. The activity in OFA, representing ‘residual error’ between top-down prediction and bottom-up input, is then expected to increase subsequently. Consistent with this model, rOFA was activated later than rpFFA, and rpFFA exerted extensive directional influence onto rOFA when processing Mooney faces, suggesting a cortical analysis dominated by rpFFA to rOFA projection. However, when ﻿explicit internal facial features were available but misarranged within a normal face contour, a temporal pattern similar to that of normal faces was observed. Finally, we further investigated the temporal dynamics when face-specific responses were driven by contextual cues alone with the internal face features entirely missing (<xref ref-type="bibr" rid="bib18">Cox et al., 2004</xref>). In this case, rOFA, rpFFA and raFFA were activated somewhat late and almost simultaneously, corresponding to contextual modulation that parallelly facilitated the processing of the core face-processing network.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Face induced MEG signals in the source space</title><p>Subjects were presented with famous faces and familiar objects and instructed to perform a simple classification task (face or object) while their brain activity was recorded using MEG. After a rest period, each subject was scanned with fMRI viewing the same group of face and object images presented in separate blocks. Since each subject underwent both fMRI and MEG measurements, we could compare the face-selective regions defined by fMRI with the reconstructed MEG signals evoked by faces in the source space.</p><p>Subjects’ face-selective regions in the occipitotemporal cortex were localized with fMRI contrasting responses to faces with that to objects. MEG signals at different time points were reconstructed in the source space by computing LCMV beamformer solution on evoked data after preprocessing (<xref ref-type="bibr" rid="bib102">Van Veen et al., 1997</xref>). The estimated activities for the whole cortical surface can be viewed as a 3D spatial distribution of LCMV value (power normalized with noise) at each time point (<xref ref-type="bibr" rid="bib94">Sekihara and Nagarajan, 2008</xref>).</p><p><xref ref-type="fig" rid="fig1">Figure 1</xref> shows the fMRI identified face regions and MEG measured face-evoked signals in a typical subject, displayed in ventral and lateral views of an inflated right hemisphere (Source localization results and fMRI localization results are shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>–<xref ref-type="fig" rid="fig1s4">4</xref> for more individual subjects). Face-selective regions rOFA, rpFFA, raFFA and rpSTS were identified by fMRI localizer (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). MEG responses evoked by faces are shown in 10 ms steps from 120 ms to 160 ms in source space (cortical surface) (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). It could be seen in the MEG signal that the location of a cluster of activation in the right occipital cortex at about 120 ms after stimulus onset is consistent with rOFA. At about 150–160 ms, a cluster of activation was found in posterior part of superior temporal sulcus, overlapping with rpSTS. Two temporally separated clusters of MEG source activation were found in the right fusiform gyrus, one consistent with the location of pFFA (about 130 ms) and another with aFFA (about 150 ms) (see <xref ref-type="video" rid="video1">Video 1</xref>). Similar spatiotemporal patterns of activation could be seen across the 13 subjects tested. These results show that face response areas identified by MEG are highly consistent with that defined by fMRI, thus it is a reasonable approach to extract the MEG time courses based on fMRI-guided region of interest (ROI). In this paper, with the understanding that the sources of MEG signals were constrained by the fMRI defined ROIs, we use the fMRI terms (OFA, FFA and pSTS) to indicate the corresponding cortical area in MEG data.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Face-selective areas identified by fMRI localizer and face-evoked MEG source activation displayed on an inflated right hemisphere of a typical subject.</title><p>(<bold>A</bold>) Face-selective statistical map (faces&gt;objects) showing four face-selective regions (rOFA, rpFFA, raFFA and rpSTS). (<bold>B</bold>) Face-evoked MEG source activation patterns represented as LCMV value maps at different time points (120-160 ms) after the stimulus onset. LCMV values represent signal power normalized by noise.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Face-selective areas identified by fMRI localizer and face-evoked MEG source activation displayed on an inflated right hemisphere of typical subject one.</title><p>(Left) Face-selective statistical map (faces&gt;objects) showing face-selective regions (OFA, pFFA, aFFA and pSTS). (Right) Face-evoked MEG source activation patterns represented as LCMV value maps at different time points (120-160 ms) after the stimulus onset. LCMV values represent signal power normalized by noise. Results of these four subjects were shown here because of their relatively clear fMRI defined face-selective areas.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Face-selective areas identified by fMRI localizer and face-evoked MEG source activation displayed on an inflated right hemisphere of typical subject two.</title><p>(Left) Face-selective statistical map (faces&gt;objects) showing face-selective regions (OFA, pFFA, aFFA and pSTS). (Right) Face-evoked MEG source activation patterns represented as LCMV value maps at different time points (120-160 ms) after the stimulus onset. LCMV values represent signal power normalized by noise. Results of these four subjects were shown here because of their relatively clear fMRI defined face-selective areas.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-fig1-figsupp2-v2.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Face-selective areas identified by fMRI localizer and face-evoked MEG source activation displayed on an inflated right hemisphere of typical subject three.</title><p>(Left) Face-selective statistical map (faces&gt;objects) showing face-selective regions (OFA, pFFA, aFFA and pSTS). (Right) Face-evoked MEG source activation patterns represented as LCMV value maps at different time points (120-160 ms) after the stimulus onset. LCMV values represent signal power normalized by noise. Results of these four subjects were shown here because of their relatively clear fMRI defined face-selective areas.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-fig1-figsupp3-v2.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Face-selective areas identified by fMRI localizer and face-evoked MEG source activation displayed on an inflated right hemisphere of typical subject four.</title><p>(Left) Face-selective statistical map (faces&gt;objects) showing face-selective regions (OFA, pFFA, aFFA and pSTS). (Right) Face-evoked MEG source activation patterns represented as LCMV value maps at different time points (120-160 ms) after the stimulus onset. LCMV values represent signal power normalized by noise. Results of these four subjects were shown here because of their relatively clear fMRI defined face-selective areas.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-fig1-figsupp4-v2.tif"/></fig></fig-group><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-48764-video1.mp4"><label>Video 1.</label><caption><title>MEG activation of a typical subject.</title></caption></media></sec><sec id="s2-2"><title>Bottom-up processing sequence induced by normal faces</title><p>We investigated the typical dynamic sequence for processing faces in the ventral occipitotemporal cortex investigated by presenting subjects with face images of well-known individuals. We analyzed the time courses of face-selective areas identified in the source space. Seven face-selective areas (lOFA, rOFA, lpFFA, rpFFA, raFFA, lpSTS, rpSTS) were identified, guided by fMRI face localizer results from each individual subject, and they were used to extract the face-response time courses of the MEG source data. We averaged the resulting time courses across subjects and the waveforms are shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. Face images induced stronger responses compared to objects in face-selective areas, especially for the right hemisphere. The timing of peak responses for individual ROIs are summarized in <xref ref-type="fig" rid="fig2">Figure 2B and C</xref>, revealing the fundamental temporal characteristics of the neural processing of faces. In the right hemisphere, face-evoked responses emerged earlier in the posterior areas than in the anterior areas, the peak responses occurred at 116 ± 6 ms, 125 ± 5 ms and 150 ± 10 ms for rOFA, rpFFA and raFFA, respectively. Although there is no significant difference between rOFA and rpFFA (t<sub>12</sub> = 1.57, p=0.43, Bonferroni corrected), the peak response timing of raFFA is significantly delayed compared with rpFFA (t<sub>11</sub> = 3.21, p=0.025, Bonferroni corrected), suggesting a bottom-up process. Similarly, OFA reached its peak response earlier than pFFA in the left hemisphere (lOFA:122 ± 5 ms, lpFFA:126 ± 6 ms), although this trend is not statistically significant (t<sub>11</sub> = 0.64, p&gt;0.05, Bonferroni corrected). Responses from the left anterior FFA was not shown because the corresponding activation cluster was not observed clearly in most subjects. In addition, dorsal face-selective region pSTS showed weaker and temporally broader responses, involved in face processing roughly from 130 to 180 ms. The sequential progression from posterior to anterior regions along the ventral occipitotemporal cortex, especially the significantly delayed activation of raFFA, indicates a bottom-up hierarchical functional structure of the ventral face pathway.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Temporal response characteristics of face-selective ROIs.</title><p>(<bold>A</bold>) The time courses of face (solid line) and object (dotted line) induced responses averaged across subjects, for the seven face-selective ROIs. Shaded area means SEM. The green bar indicates significant difference between face and object. ﻿Significance was assessed by ﻿cluster-based permutation test (cluster-defining threshold p&lt;0.05, significance level p&lt;0.05) for each ROI. (<bold>B</bold>) The peak latency averaged across subjects for each ROI (mean ± SEM). The peak latency of raFFA is significantly later than rpFFA (t<sub>11</sub> = 3.21, p=0.025, Bonferroni corrected) (<bold>C</bold>) The mean peak latencies for the face-selective ROIs were shown on inflated cortical surfaces of both hemispheres at corresponding locations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Temporal response characteristics of face-selective ROIs for unfamiliar faces.</title><p>(<bold>A</bold>) The time courses of unfamiliar face (solid line) and object (dotted line) induced responses averaged across subjects, for the seven face-selective ROIs. Shaded area means SEM. (<bold>B</bold>) The peak latency averaged across subjects for each ROI (mean± SEM).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-fig2-figsupp1-v2.tif"/></fig></fig-group><p>In addition to famous faces, we also presented unfamiliar faces to subjects and analyzed the data in the same way. Results showed essentially similar hierarchical dynamic sequences of face processing regardless of face familiarity (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Thus, unfamiliar face images were used in the next experiment reported below.</p></sec><sec id="s2-3"><title>Top-down operation in face processing highlighted by viewing Mooney faces</title><p>While the processing of normal (famous or unfamiliar) faces mainly followed the posterior to anterior (bottom-up) face processing sequence, we further investigated the possibility that under certain stimulus conditions, top-down modulation of face processing could become more prominent. According to the predictive coding theory, when the representation of sensory input in lower areas is poorly matched with the predictions generated from higher level areas, the activity in lower areas representing residual error would be increased (<xref ref-type="bibr" rid="bib83">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib70">Murray et al., 2004</xref>; <xref ref-type="bibr" rid="bib69">Mumford, 1992</xref>). Hence, we adopted the two-tone Mooney face images (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), which could be recognized as faces but lack prototypical local facial features, as the main stimuli in this experiment. Our hypothesis was that when processing Mooney faces which could activate the FFA based on the global configuration, the top-down modulation from FFA to OFA (prediction of facial parts) would be more prominent.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Temporal response characteristics and granger causality analysis for face-selective ROIs during perception of Mooney and normal faces.</title><p>(<bold>A</bold>) Normal and Mooney face images. (<bold>B</bold>) The peak latency averaged across subjects for each face-selective ROI (mean ± SEM). Mooney faces elicited a response with significantly longer latency in rOFA than normal faces (paired t test, t<sub>23</sub> = 4.009, p=0.001). (<bold>C</bold>) Time courses averaged across subjects for bilateral OFA and pFFA. Gray line is OFA and red line is pFFA. Shaded areas denote SEM. ﻿The circles above time courses represent peak latencies of individual subjects. rOFA was engaged significantly later than rpFFA when processing Mooney faces (Paired permutation test p=0.02. Bonferroni corrected). (<bold>D</bold>) Granger causality analysis performed within a series of 50 ms time windows. Arrows represent statistically significant causal effects (p&lt;0.05, FDR corrected, F test. See Materials and methods for details).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-fig3-v2.tif"/></fig><p>In this experiment, subjects (n = 28) were presented with normal unfamiliar faces and Mooney faces, they performed a one-back task, indicating the repetition of the same images. Verbal survey after the MEG experiment indicated that subjects could perceive at least 90% of the Mooney images as faces. In all face-selective areas except rOFA, similar peak latencies were observed during the perception of normal and Mooney faces. ﻿ Strikingly, Mooney face elicited a response with significantly longer latency in rOFA than normal face (paired t test, t<sub>23</sub> = 4.009, p=0.001) (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The temporal relationship of signals in the face-selective areas was quite different during the perception of Mooney faces compared with that of normal ones (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Similar to Experiment 1, OFA were activated slightly earlier than pFFA in response to normal faces (lOFA:124 ± 9 ms, lpFFA: 133 ± 9 ms, Paired permutation test p&gt;0.9; rOFA: 107 ± 4 ms, rpFFA:120 ± 6 ms, Paired permutation test p=0.37. Bonferroni corrected for multiple comparisons). However, when processing Mooney faces, rOFA was engaged significantly later than rpFFA (rOFA:144 ± 8 ms, rpFFA: 117 ± 8 ms. Paired permutation test p=0.02. Bonferroni corrected). The response curve of rOFA was temporally shifted to a later point while the temporal characteristics of rpFFA was not much different from its response to normal faces (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). The temporal relationship between OFA and pFFA in left hemisphere is similar to normal face condition (lOFA:127 ± 10 ms, lpFFA: 133 ± 10 ms. Paired permutation test p&gt;0.9, Bonferroni corrected).</p><p>To further analyze the dynamic causal relationship between OFA and pFFA, we performed Granger causality analysis over sliding time windows of 50 ms duration from 75 to 230 ms after stimulus presentation which covers the periods of essential activation in OFA and pFFA. The significant directed connectivity in each time window is shown in <xref ref-type="fig" rid="fig3">Figure 3D</xref>. There were much more extensive directed influences from pFFA to OFA during the processing of Mooney than normal faces. In particular, rpFFA influenced rOFA in Mooney face condition ﻿continuously from 75 to 170 ms, which was more sparsely observed in normal face condition. Thus response time courses and Granger causality analysis together show that, compared with processing of normal faces, the cortical processing of Mooney faces is more dominated by the top-down rpFFA to rOFA projection.</p></sec><sec id="s2-4"><title>Primarily feedforward processing of face-like stimuli with misarranged internal features</title><p>We also investigated the processing dynamics of face-like stimuli with internal features clearly available but spatially misarranged, to contrast with the processing of normal as well as Mooney faces. The normal external features (hair, chin, face outline) and the locally normal internal features led to the engagement of the face-sensitive areas. Results show that the rOFA, rpFFA and raFFA were activated sequentially (rOFA: 132 ± 7 ms, rpFFA: 133 ± 5 ms, raFFA: 169 ± 12 ms. <xref ref-type="fig" rid="fig4">Figure 4B</xref>). Compared with the responses to normal faces, the activations in the rOFA and rpFFA were somewhat delayed in the case of the distorted faces. However, unlike the Mooney faces, the distorted faces still engaged the OFA earlier than the FFA, presumably because of the explicitly available local facial features. While the dominant signals are consistent with a feedforward processing from OFA to FFA, there was a hint of a predictive error signal, possibly related to the misarranged spatial configurations, that produced a low activity in rOFA at a later stage.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Temporal response characteristics for face-selective ROIs in response to distorted face.</title><p>(<bold>A</bold>) Example stimuli and averaged time courses for each face-selective ROI. The green horizontal bar indicates significant difference between distorted face and object (cluster-defining threshold p&lt;0.01, corrected significance level p&lt;0.05). (<bold>B</bold>) Peak latency averaged across subjects for each ROI. The peak latency of raFFA is significant later than rpFFA (paired t test, p=0.019, t<sub>8</sub> = 2.92).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-fig4-v2.tif"/></fig></sec><sec id="s2-5"><title>Parallel facilitation of face-processing network from contextual cues alone</title><p>In real life, facial features are not always available. Previous studies showed that face-specific responses could be elicited by contextual body cues (<xref ref-type="bibr" rid="bib18">Cox et al., 2004</xref>; <xref ref-type="bibr" rid="bib15">Chen and Whitney, 2019</xref>; <xref ref-type="bibr" rid="bib62">Martinez, 2019</xref>). Here we further investigated the dynamics of contextual ﻿facilitation of face processing when face perception was supported by contextual cues alone without explicit facial features using the same experimental paradigm and data analysis procedures as before.</p><p>Three types of stimuli were presented to subjects: (i) images of highly degraded faces (no internal facial features) with contextual body cues that imply the presence of faces, (ii) similar to images in (i) but with body cues arranged in an incorrect configuration and thus do not imply the presence of faces, (iii) images of objects (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Activation in rOFA, rpFFA and raFFA were significantly higher for the condition in which faces were clearly implied due to the contextual cues compared to the condition when objects were presented (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). However, when ﻿contextual cues were misarranged so that faces were not strongly implied, only the rOFA showed stronger activation than objects at a late stage (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Furthermore, peak latency analysis revealed that during the perception of ‘faces’ generated from contextual cues alone, the rOFA, rpFFA and raFFA were all engaged at about the same and relatively late time (rOFA: 149 ± 12 ms, rpFFA: 149 ± 14 ms, raFFA: 155 ± 11 ms) rather than activated sequentially (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Thus when the presence of a face was facilitated by external cues alone, the evoked responses in the core face-processing network emerged slowly and almost simultaneously.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Temporal response characteristics for face-selective ROIs in response to contextual cues.</title><p>(<bold>A</bold>) Example stimuli. (<bold>B</bold>) Time courses averaged across subjects for each condition. For each ROI, Blue horizontal bars indicate significant difference between degraded faces with relevant body cues and objects, and red horizontal bars indicate significant difference between degraded faces with irrelevant body cues and objects (cluster-defining threshold p &lt; 0.05, corrected significance level p &lt; 0.05). (<bold>C</bold>) The peak latency averaged across subjects for each face-selective ROI (mean± SEM).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-fig5-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Using a combined fMRI and MEG source localization approach, our results systematically revealed an intricately detailed dynamic picture of face information processing. Within the ventral occipitotemporal face processing network, normal faces were processed mainly in a bottom-up manner through the hierarchical pathway where input information was processed sequentially from posterior to anterior ventral temporal cortex. This temporal order was also observed when processing face-like stimuli with misarranged internal facial features. In contrast, during the processing of Mooney faces in the absence of prototypical facial features, top-down modulation was more prominent in which the dominant information flow was from the rpFFA to rOFA. Moreover, face-specific responses from contextual cues alone were evoked late and simultaneously across the rOFA, rpFFA and raFFA, suggesting that contextual facilitation acted parallelly on the core face-processing network. These results advance our understanding of the hierarchical and non-hierarchical models of face perception, especially underscoring the stimulus- and context-dependent nature of the processing sequences.</p><p>During the perception of 2-tone Mooney faces, it is necessary to discount shadows and recover 3D surface structure from 2D images (<xref ref-type="bibr" rid="bib43">Grützner et al., 2010</xref>). Interestingly, only familiar objects, like faces, can be interpreted to be volumetric easily from 2-tone representations (<xref ref-type="bibr" rid="bib68">Moore and Cavanagh, 1998</xref>; <xref ref-type="bibr" rid="bib46">Hegde et al., 2007</xref>). Thus it is supposed that prior knowledge should play an important role in the recovery of 3D shape from Mooney images (<xref ref-type="bibr" rid="bib13">Braje et al., 1998</xref>; <xref ref-type="bibr" rid="bib33">Gerardin et al., 2010</xref>). A top-down model emphasized the guidance of prior experience at higher levels (<xref ref-type="bibr" rid="bib14">Cavanagh, 1991</xref>). This model is supported by evidence from experiments showing that early visual processing is affected by high-level attributes in both human and monkey (<xref ref-type="bibr" rid="bib57">Lee et al., 2002</xref>; <xref ref-type="bibr" rid="bib48">Humphrey et al., 1997</xref>; <xref ref-type="bibr" rid="bib49">Issa et al., 2018</xref>). As briefly mentioned in the results section, the dynamics of MEG signals associated with processing Mooney faces, which highlights the top-down modulation, is consistent with the explanation based on predictive coding model. It proposed that hypotheses or predictions made at higher cortical areas are compared with, through feedback, representations at lower areas to generate residual error, which is then forwarded to higher stages as ‘neural activity’ (<xref ref-type="bibr" rid="bib83">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib70">Murray et al., 2004</xref>; <xref ref-type="bibr" rid="bib28">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib29">Friston, 2010</xref>). Specifically, the face model/prediction is generated at the rpFFA based on the global configuration of Mooney faces using prior knowledge about 3D faces, illumination, and cast shadows. This prediction of expected facial features is then poorly matched with the input representation at the rOFA which lacks the explicit prototypical facial features due to the mixed illumination-invariant and illumination-dependent features, generating an increased signal at rOFA. Thus, the dominant signal at the rOFA (residual) necessarily lags behind the signal at the rpFFA (hypothesis). However, when processing normal faces or face with misarranged facial features, the prominent signal in the early stage of rOFA is mainly due to the strong feedforward input from early visual cortex as rOFA is robustly responsive to the clear facial components. The prediction feedback from rpFFA would be consistent with representation at the rOFA in the case of the normal faces, resulting in little error signal; with the misarranged facial features, there was a hint of a late increase of rOFA signal, possibly indicating that the feedback signal could contain some spatial information as well.</p><p>The timing of face induced neural activation has been studied for a long time with various techniques, such as the combination of MEG and fMRI using representational similarities (<xref ref-type="bibr" rid="bib16">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib17">Cichy et al., 2016</xref>), MEG source localization and intracranial EEG (<xref ref-type="bibr" rid="bib52">Kadipasaoglu et al., 2017</xref>; <xref ref-type="bibr" rid="bib54">Keller et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Ghuman et al., 2014</xref>). An early MEG study suggested two stages (﻿early categorization and ﻿late identification) were involved in face processing (<xref ref-type="bibr" rid="bib58">Liu et al., 2002</xref>). Combined with the fMRI observation that OFA is responsible for identifying facial parts while FFA for holistic configuration (<xref ref-type="bibr" rid="bib89">Rotshtein et al., 2005</xref>; <xref ref-type="bibr" rid="bib59">Liu et al., 2010</xref>; <xref ref-type="bibr" rid="bib78">Pitcher et al., 2011b</xref>; <xref ref-type="bibr" rid="bib3">Arcurio et al., 2012</xref>; <xref ref-type="bibr" rid="bib76">Pitcher et al., 2007</xref>; <xref ref-type="bibr" rid="bib92">Schiltz, 2010</xref>), OFA is expected to respond earlier than FFA. An simultaneous electroencephalogram (EEG)-fMRI study also showed that OFA responded to faces earlier than FFA (OFA: 110 ms; FFA: 170 ms) (<xref ref-type="bibr" rid="bib91">Sadeh et al., 2010</xref>). Using transient stimulation to temporally disrupt local neural processing, Transcranial Magnetic Stimulation (TMS) experiments suggested that OFA processes facial information at about 100/110 ms, while pSTS begins processing face at about 100/140 ms (<xref ref-type="bibr" rid="bib79">Pitcher et al., 2012</xref>; <xref ref-type="bibr" rid="bib80">Pitcher et al., 2014</xref>). However, the sources of N/M170 face selective component remain controversial, it is suggested to come from fusiform gyrus in some studies (<xref ref-type="bibr" rid="bib20">Deffke et al., 2007</xref>; <xref ref-type="bibr" rid="bib55">Kume et al., 2016</xref>; <xref ref-type="bibr" rid="bib75">Perry and Singh, 2014</xref>). While some other studies emphasized the contribution of ﻿inferior occipital gyrus besides fusiform gyrus (<xref ref-type="bibr" rid="bib51">Itier et al., 2006</xref>; <xref ref-type="bibr" rid="bib30">Gao et al., 2013</xref>) or even of pSTS (<xref ref-type="bibr" rid="bib71">Nguyen and Cunnington, 2014</xref>). Our results provide more precise and detailed timing information of the core face network under various stimulus and contextual conditions, ﻿especially the temporal relationship between rpFFA and raFFA. raFFA is engaged significantly later, about 20 ms after the rpFFA, suggesting that the raFFA likely plays a different functional role from rpFFA. This idea is supported by previous anatomical evidence showing that pFFA and aFFA have different cellular architectures (<xref ref-type="bibr" rid="bib105">Weiner et al., 2017</xref>).</p><p>Our results also shed light on the role of internal and external features in face perception. Although when assembling into a whole face, facial features are processed holistically and the representation of internal features are influenced by external features (<xref ref-type="bibr" rid="bib2">Andrews et al., 2010</xref>), eyes in isolation elicit a later but larger N170 (<xref ref-type="bibr" rid="bib10">Bentin et al., 1996</xref>; <xref ref-type="bibr" rid="bib88">Rossion and Jacques, 2011</xref>) and can drive face-selective neurons as well as full-face images (<xref ref-type="bibr" rid="bib50">Issa and DiCarlo, 2012</xref>) in monkeys. In our results, the somewhat slower but still sequential progression of face responses elicited by face-like stimuli with clear but misarranged internal features in face outline further supports that facial features are sufficient to trigger the bottom-up face processing sequence. In addition, certain stimulus manipulations, such as face inversion (<xref ref-type="bibr" rid="bib10">Bentin et al., 1996</xref>), contrast reversal, Mooney transformation or removal of facial features produced comparable (or even increased amplitude) but delayed N170 responses (<xref ref-type="bibr" rid="bib88">Rossion and Jacques, 2011</xref>). Thus it is suggested that as long as the impoverished stimuli is perceived as a face, ﻿inferior temporal cortex areas would be activated (<xref ref-type="bibr" rid="bib64">McKeeff and Tong, 2007</xref>; <xref ref-type="bibr" rid="bib43">Grützner et al., 2010</xref>). Our results provide further more details for this explanation by showing the top-down rpFFA to rOFA projection when the prototypical facial features are lack.</p><p>Besides facial features, contextual information is also important for face ﻿interpretation (<xref ref-type="bibr" rid="bib15">Chen and Whitney, 2019</xref>; <xref ref-type="bibr" rid="bib62">Martinez, 2019</xref>). ﻿Interestingly, FFA can be activated by the perceived presence of faces from contextual body cues alone (<xref ref-type="bibr" rid="bib18">Cox et al., 2004</xref>). Here our MEG data showed that the face-selective areas in ventral core face network were indeed activated by the contextual cues for faces, but they were not activated in any order, instead, they became active together at a late stage. This is similar to the temporal dynamics observed in visual imagery, a top-down process given the absence of visual inputs (<xref ref-type="bibr" rid="bib21">Dijkstra et al., 2018</xref>). Future studies are needed to ﻿elucidate how core face network interacts with other brain regions to trigger the face perception. For example, according to a MEG study using fast periodic visual stimulation approach (<xref ref-type="bibr" rid="bib86">Rossion et al., 2012</xref>; <xref ref-type="bibr" rid="bib87">Rossion et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">de Heering and Rossion, 2015</xref>), top-down attention increase the response in FFA by gamma synchrony between the inferior frontal junction and FFA (<xref ref-type="bibr" rid="bib5">Baldauf and Desimone, 2014</xref>).</p><p>Face perception is shaped by long-term visual experience, for example, familiar faces are processed more efficiently than unfamiliar ones (<xref ref-type="bibr" rid="bib56">Landi and Freiwald, 2017</xref>; <xref ref-type="bibr" rid="bib93">Schwartz and Yovel, 2016</xref>; <xref ref-type="bibr" rid="bib22">Dobs et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Gobbini and Haxby, 2006</xref>). In terms of the dynamics in the ventral occipitotemporal areas, the present results showed little differences between processing famous and unfamiliar faces. This could be due to several reasons. First, many studies suggested that regions in the anterior temporal lobe rather than OFA and FFA ﻿represent face familiarity (<xref ref-type="bibr" rid="bib36">Gobbini and Haxby, 2007</xref>; <xref ref-type="bibr" rid="bib81">Pourtois et al., 2005</xref>; <xref ref-type="bibr" rid="bib98">Sugiura et al., 2011</xref>). However, extended face system is beyond the scope of our current study because some areas in the extended system are too deep to obtain a good MEG source signal. Second, some subjects might not be familiar with all the famous faces we used. Third, familiarity may affect face recognition via high gamma frequency band activity (<xref ref-type="bibr" rid="bib1">Anaki et al., 2007</xref>), which is not included in our data analysis.</p><p>Bilateral pSTS showed weak and multi-peaked responses during both famous and unfamiliar face processing despite the task differences. One possible reason for the multiple peaks of responses is that as a hub for integrating information from multiple sources (e.g., face, body, and voice), STS contains regions that respond to different types of information (<xref ref-type="bibr" rid="bib42">Grossman et al., 2005</xref>; <xref ref-type="bibr" rid="bib11">Bernstein and Yovel, 2015</xref>). A lot of studies have suggested diverse functional role of pSTS in representing changeable aspects of faces, such as expression, lip movement and eye-gaze (<xref ref-type="bibr" rid="bib7">Baseler et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Engell and Haxby, 2007</xref>). Specifically, pSTS is involved in the analysis of facial muscle articulations which are combined to produce facial expressions (<xref ref-type="bibr" rid="bib96">Srinivasan et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">Martinez, 2017</xref>). In addition, pSTS may respond to dynamic motion information conveyed through faces (<xref ref-type="bibr" rid="bib72">O'Toole et al., 2002</xref>).</p><p>Previous studies showed that left and right fusiform gyrus are differentially involved in face/non-face judgements (<xref ref-type="bibr" rid="bib65">Meng et al., 2012</xref>; <xref ref-type="bibr" rid="bib37">Goold and Meng, 2017</xref>), ‘low-level’ face semblance and perceptual learning of face (<xref ref-type="bibr" rid="bib12">Bi et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Feng et al., 2011</xref>; <xref ref-type="bibr" rid="bib63">McGugin et al., 2018</xref>). Interestingly, in our results, the peak latency of the left pFFA was later than that of the right pFFA in all conditions except famous face. Responses evoked from distorted faces with misarranged features had the largest lateral difference (20 ms). One possible reason is that the signal attributed to the left pFFA is in fact a mixture of signals from pFFA and aFFA.</p><p>Although the exact correspondence between human and macaque face-selective areas are still unclear (<xref ref-type="bibr" rid="bib99">Tsao et al., 2003</xref>; <xref ref-type="bibr" rid="bib100">Tsao et al., 2006</xref>; <xref ref-type="bibr" rid="bib101">Tsao et al., 2008</xref>), the dynamic picture of normal face processing revealed in our study is generally similar to that in macaques. Single-unit recording studies showed that activity begins slightly earlier in posterior face patches than anterior ones, reaching peak levels around 126, 133, and 145 ms for ﻿middle lateral (ML)/middle fundus (MF), ﻿anterior lateral (AL), and ﻿anterior medial (AM) (﻿<xref ref-type="bibr" rid="bib27">Freiwald and Tsao, 2010</xref>) ﻿, respectively. Interestingly, there is a discrepancy in response to Mooney faces in high level face patch AM between two monkeys. One of them showed nearly the same peak latency as normal faces but with more sustained activation, while the other did not response to Mooney faces (<xref ref-type="bibr" rid="bib66">Moeller et al., 2017</xref>). This may imply that the processing of Mooney faces is related to individual face detection ability or life experience and face processing is not a simple feedforward process from low level to high level areas. Consistent with that, a more recent study showed a rapid and more sustained response in high level face area (aIT) and an early rising then quickly decreased activity in low level areas in monkeys, a signature of predictive coding model (<xref ref-type="bibr" rid="bib49">Issa et al., 2018</xref>).</p><p>Our study is obviously limited in scope. There are many types of cues and tasks relevant for face perception that could be investigated. In addition to facial features and context, many low level cues contribute to face recognition, such as illumination direction, pigmentation (surface appearance) and contrast polarity (one region brighter than another) (<xref ref-type="bibr" rid="bib90">Russell et al., 2007</xref>; <xref ref-type="bibr" rid="bib95">Sinha et al., 2006</xref>). In particular, neurons tuned for contrast polarity were found in macaque inferotemporal cortex, supporting the notion that low-level image properties are encoded in face regions (<xref ref-type="bibr" rid="bib73">Ohayon et al., 2012</xref>; <xref ref-type="bibr" rid="bib104">Weibert et al., 2018</xref>). We purposely avoided the complication of color cues in this study by using gray-scale images, but we are aware the importance of color in face perception (<xref ref-type="bibr" rid="bib107">Yip and Sinha, 2002</xref>; ﻿<xref ref-type="bibr" rid="bib8">Benitez-Quiroz et al., 2018</xref>). Moreover, the temporal dynamics of face processing could very well be influenced by different tasks. In our results, there is little difference between the temporal patterns in response to unfamiliar faces under face category task (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) and image identity one-back task (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Future studies are needed to more comprehensively investigate the role of behavioral tasks, especially during the relatively late stages of face processing.</p><p>In summary, our study delineated the precise timing of bottom-up, top-down, as well as context-facilitated processing sequences in the occipital-temporal face network. These results provide a way to understand and reconcile previous discrepant findings, revealing the dominant bottom-up processing when explicit facial features were present, and highlighting the importance of the top-down feedback operations when faced with impoverished inputs with unclear or ambiguous facial features.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>All subjects (age range 19–31) provided written informed consent and consent to publish before the experiments, and experimental protocols were approved by the Institutional Review Board of the Institute of Biophysics, Chinese Academy of Sciences (#2017-IRB-004). The image used in <xref ref-type="fig" rid="fig3">Figure 3</xref> is a photograph of one of the authors and The Consent to Publish Form was obtained.</p></sec><sec id="s4-2"><title>Experiment 1 (normal famous and unfamiliar face)</title><p>Fifteen subjects were presented with famous faces (popular film actors, 50% female) and objects (houses, scenery and small manmade objects) and were instructed to perform a category classification task (face or object) while their brain activity was recorded using MEG. Two subjects with excessive head motion (&gt;5 mm) were excluded from further analysis. Each type of image includes 50 exemplars and all faces are own race faces. All images used were equated for contrast and mean luminance using the SHINE toolbox (<xref ref-type="bibr" rid="bib106">Willenbockel et al., 2010</xref>). Each trial was initiated with a fixation with a jittered duration (800–1000 ms), then a grayscale visual image (face or object, 8 × 6 °) was presented at the center of screen for 500 ms, followed by a response period. Subjects were asked to maintain fixation and report whether the image was a face or an object using button press as soon as possible. There were 120 trials for each condition. Nine of the thirteen subjects participated in an additional experiment in which unfamiliar faces were used.</p></sec><sec id="s4-3"><title>Experiment 2 (normal unfamiliar face and Mooney face)</title><p>Experiment two was conducted similar to Experiment 1, except that unfamiliar faces and two-tone Mooney faces were presented to ﻿subjects (n = 28) in separate blocks (15 trials each) during which subjects performed a one-back task. Two subjects with excessive head motion (&gt;5 mm) were excluded from further analysis.</p></sec><sec id="s4-4"><title>Experiment 3 (face-like images with spatially misarranged internal features)</title><p>Experiment three was conducted similar to Experiment 1, except that distorted face and object images were presented to subjects (n = 9). Distorted face images were created by rearranging the eyes, mouth and nose into a nonface configuration (<xref ref-type="bibr" rid="bib58">Liu et al., 2002</xref>).</p></sec><sec id="s4-5"><title>Experiment 4 (contextual cues defined the presence of faces without internal features)</title><p>Experiment four was conducted similar to Experiment 2. Three types of stimuli (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) were created as described in previous study (<xref ref-type="bibr" rid="bib18">Cox et al., 2004</xref>): (i) images of highly degraded faces (no internal facial features) with contextual body cues that imply the presence of faces, (ii) similar to images in (i) but with body cues arranged in an incorrect configuration and thus do not imply the presence of faces, (iii) images of objects. Fifteen subjects participated in this experiment and one of them was excluded from further data analysis due to excessive head motion (&gt;5 mm).</p></sec><sec id="s4-6"><title>MEG data acquisition and analysis</title><p>MEG data were recorded continuously using a 275-channel CTF system. Three coils were attached on the head, one close to nasion, and the other two close to left and right preauricular points respectively. fMRI scanning was performed shortly after MEG data collection, and the locations of coils were marked with vitamin E caplets to align with MEG frames. MEG data analysis was performed using MATLAB (﻿RRID: <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001622">SCR_001622</ext-link>) and Fieldtrip toolbox (﻿<xref ref-type="bibr" rid="bib74">Oostenveld et al., 2011</xref>) (RRID: ﻿<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_004849">SCR_004849</ext-link>) for artifact detection and MNE-python (﻿RRID: <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_005972">SCR_005972</ext-link>) for source analysis (<xref ref-type="bibr" rid="bib38">Gramfort et al., 2013</xref>; <xref ref-type="bibr" rid="bib39">Gramfort et al., 2014</xref>).</p><sec id="s4-6-1"><title>Preprocessing</title><p>After acquisition, we first conducted time correction as there was time delay (measured with a photodiode) between the stimulus onset on the screen and the trigger signal in the recorded MEG data. Then the data were bandpass filtered with a frequency range of 2–80 Hz and epoched from 250 ms before to 550 ms after the stimulus onset. Bad channels and trials contaminated by artifacts including eye blinks, muscle activities and SQUID jumps were removed before further analysis.</p></sec><sec id="s4-6-2"><title>Source localization</title><p>Source localization can be generally divided into two steps, forward solution and inverse solution. Boundary-element model (BEM) which describes the geometry of the head and conductivities of the different tissues, coregistration information between MEG and MRI, and volume source space which defines the position of the source locations (10242 sources per hemisphere and the source spacing is 3.1 mm) were used to calculate forward solution. For inverse solution, we first estimated noise and data covariance matrix from −250 to 0 ms epochs and 100 to 350 ms epochs respectively. Afterwards, the Linearly Constrained Minimum Variance (LCMV) beamformer was calculated using covariance matrix and forward solution (<xref ref-type="bibr" rid="bib102">Van Veen et al., 1997</xref>). The regularization for the whitened data covariance is 0.01. The source orientation which maximizes output source power is selected.</p></sec><sec id="s4-6-3"><title>Time course analysis</title><p>To explore the time course, virtual sensors were computed on the 30 Hz low-pass filtered data using the LCMV beamformer at the grid points within individual face-selective areas. The time course of each face-selective area was extracted from the grid point showing max value of MEG response. Subjects who did not show corresponding face-selective areas in fMRI localizer were excluded from time course extraction (See <xref ref-type="table" rid="table1">Table 1</xref> for details). To identify time-points of significant differences, we performed non-parametric statistical tests with cluster-based multiple comparison correction (<xref ref-type="bibr" rid="bib60">Maris and Oostenveld, 2007</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Number of subjects showing fMRI defined face-selective areas.</title></caption><table frame="hsides" rules="groups"><thead><tr valign="top"><th/><th colspan="2">Experiment 1</th><th>Experiment 2</th><th>Experiment 3</th><th>Experiment 4</th></tr></thead><tbody><tr valign="top"><td/><td>famous face</td><td>Unfamiliar face</td><td/><td/><td/></tr><tr valign="top"><td>IOFA</td><td>13/13</td><td>9/9</td><td>25/26</td><td>9/9</td><td>13/14</td></tr><tr valign="top"><td>IpFFA</td><td>13/13</td><td>9/9</td><td>26/26</td><td>9/9</td><td>14/14</td></tr><tr valign="top"><td>IpSTS</td><td>13/13</td><td>9/9</td><td>18/26</td><td>9/9</td><td>11/14</td></tr><tr valign="top"><td>rOFA</td><td>13/13</td><td>9/9</td><td>26/26</td><td>9/9</td><td>14/14</td></tr><tr valign="top"><td>rpFFA</td><td>13/13</td><td>9/9</td><td>26/26</td><td>9/9</td><td>14/14</td></tr><tr valign="top"><td>raFFA</td><td>12/13</td><td>9/9</td><td>18/26</td><td>9/9</td><td>12/14</td></tr><tr valign="top"><td>rpSTS</td><td>13/13</td><td>9/9</td><td>23/26</td><td>9/9</td><td>14/14</td></tr></tbody></table></table-wrap></sec><sec id="s4-6-4"><title>Peak latency analysis</title><p>For each ROI of each subject, peak latency was defined as the timing of the largest peak within the first 250 ms of averaged response. To avoid the influence of bad source data with weak signal, time course without any time points showing response 5 SDs above the baseline (time average from −250 to 0 ms) was eliminated from peak analysis. The numbers of subjects used in peak latency analysis are summarized in <xref ref-type="table" rid="table2">Table 2</xref>. Two-tailed paired t tests (subjects with missing values were excluded) were used to compare the peak latencies between ROIs. While in Experiment 2, a more rigorous statistical approach, two sample paired permutation test (10000 permutations), was used to compare the peak latencies between pFFA and OFA (See results for details).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Number of subjects used in peak latency analysis.</title></caption><table frame="hsides" rules="groups"><thead><tr valign="top"><th/><th colspan="2">Experiment 1</th><th colspan="2">Experiment 2</th><th>Experiment 3</th><th>Experiment 4</th></tr></thead><tbody><tr valign="top"><td/><td>famous face</td><td>unfamiliar face</td><td>normal face</td><td>Mooney face</td><td>distorted face</td><td>contaxtual cues defined face</td></tr><tr valign="top"><td>IOFA</td><td>13/13</td><td>9/9</td><td>24/26</td><td>24/26</td><td>9/9</td><td>13/14</td></tr><tr valign="top"><td>IpFFA</td><td>12/13</td><td>9/9</td><td>25/26</td><td>25/26</td><td>9/9</td><td>12/14</td></tr><tr valign="top"><td>IpSTS</td><td>11/13</td><td>8/9</td><td>-</td><td>-</td><td>-</td><td/></tr><tr valign="top"><td>rOFA</td><td>13/13</td><td>9/9</td><td>24/26</td><td>26/26</td><td>9/9</td><td>13/14</td></tr><tr valign="top"><td>rpFFA</td><td>13/13</td><td>9/9</td><td>24/26</td><td>25/26</td><td>9/9</td><td>13/14</td></tr><tr valign="top"><td>raFFA</td><td>12/13</td><td>8/9</td><td>18/26</td><td>15/26</td><td>9/9</td><td>10/14</td></tr><tr valign="top"><td>rpSTS</td><td>12/13</td><td>7/9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></table-wrap></sec><sec id="s4-6-5"><title>Granger causality analysis</title><p>To study the regional information flow between ROIs, we employed Granger causality analysis (<xref ref-type="bibr" rid="bib40">Granger, 1969</xref>) which is a statistical technique that based on the prediction of one time series on another. Time courses used in this analysis were extracted from each ROI without low-passed filtering. Causality analysis was performed using Multivariate Granger Causality (MVGC) toolbox (<xref ref-type="bibr" rid="bib6">Barnett and Seth, 2014</xref>). Evoked response was removed from the data by linear regression before further analysis because the time series is assumed to be stationary in Granger causality analysis and this assumption is challenged in evoked brain responses (<xref ref-type="bibr" rid="bib103">Wang et al., 2008</xref>). We conducted separate analysis over a series of overlapping 50 ms time windows (based on a previous study <xref ref-type="bibr" rid="bib4">Ashrafulla et al., 2013</xref>) from 75 to 230 ms, which covers the period of face-induced activation in both OFA and FFA. There is a trade-off between stationary, temporal resolution (shorter is better) and accuracy of model fit (longer is better) when considering the size of time window. Moreover, smaller window is not considered because activity beyond Beta-band is not strong according to the power spectrum. First, the best model order was selected according to Bayesian information criteria (BIC). Then the corresponding vector auto regressive (VAR) model parameters were estimated for the selected model order and the autocovariance sequence for the VAR model was calculated. Then the bidirectional Granger causality values for each pair ROI were obtained by calculating pairwise-conditional time-domain MVGCs based on autocovariance sequence. Finally, to evaluate whether causality values are significantly greater than zero (assume null hypothesis causality value = 0), we performed significance test using F null distribution with FDR correction for multiple comparisons (<xref ref-type="bibr" rid="bib9">Benjamini and Hochberg, 1995</xref>).</p></sec></sec><sec id="s4-7"><title>fMRI data acquisition and analysis</title><p>Scanning was performed on a 3T Siemens Prisma scanner in the Beijing MRI Center for Brain Research. We acquired high-resolution T1-weighed anatomical volumes first, and then performed a run of functional face localizer (<xref ref-type="bibr" rid="bib77">Pitcher et al., 2011a</xref>) with interleaved face and object blocks using a gradient echo-planar sequence (20-channel head coil, TR = 2 s, TE = 30 ms, resolution 2.0 × 2.0 × 2.0 mm, 31 slices, matrix = 96 × 96). fMRI data were analyzed using FreeSurfer ﻿(RRID: <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001847">SCR_001847</ext-link>) and AFNI (RRID: <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_005927">SCR_005927</ext-link>). Face-selective areas were defined as regions that responded more strongly to faces than to objects.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Daniel Kersten for helpful comments on the manuscript and Ling Liu for her help in MEG data analysis. This work was supported by the Beijing Science and Technology Project (Z181100001518002, Z171100000117003), the Ministry of Science and Technology of China grants (2015CB351701) and Bureau of International Cooperation, Chinese Academy of Sciences (153311KYSB20160030).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization,Methodology, Writing</p></fn><fn fn-type="con" id="con2"><p>Methodology, Data acquisition, Data analysis, Funding acquisition</p></fn><fn fn-type="con" id="con3"><p>Methodology, Data acquisition</p></fn><fn fn-type="con" id="con4"><p>Methodology, Data analysis, Funding acquisition</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Investigation, Methodology, Writing, Funding acquisition, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All subjects (age range 19-31) provided written informed consent and consent to publish before the experiments, and experimental protocols were approved by the Institutional Review Board of the Institute of Biophysics, Chinese Academy of Sciences (# 2017-IRB-004). The image used in Figure 3 is a photograph of one of the authors and The Consent to Publish Form was obtained.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>Preprocessing.</title></caption><media mime-subtype="x-script.phyton" mimetype="text" xlink:href="elife-48764-code1-v2.py"/></supplementary-material><supplementary-material id="scode2"><label>Source code 2.</label><caption><title>Source localization.</title></caption><media mime-subtype="x-script.phyton" mimetype="text" xlink:href="elife-48764-code2-v2.py"/></supplementary-material><supplementary-material id="scode3"><label>Source code 3.</label><caption><title>Extract timecourse.</title></caption><media mime-subtype="x-script.phyton" mimetype="text" xlink:href="elife-48764-code3-v2.py"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-48764-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The source data files have been provided for Figures 2, 3, 4, 5 and Figure 2—figure supplement 1. MEG source activation data (processed based on original fMRI and MEG datasets ) have been deposited in Open Science Framework and can be accessed at <ext-link ext-link-type="uri" xlink:href="https://osf.io/vhefz/">https://osf.io/vhefz/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>MEG face experiments</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="archive" xlink:href="https://osf.io/vhefz/">vhefz</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anaki</surname> <given-names>D</given-names></name><name><surname>Zion-Golumbic</surname> <given-names>E</given-names></name><name><surname>Bentin</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Electrophysiological neural mechanisms for detection, configural analysis and recognition of faces</article-title><source>NeuroImage</source><volume>37</volume><fpage>1407</fpage><lpage>1416</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.05.054</pub-id><pub-id pub-id-type="pmid">17689102</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrews</surname> <given-names>TJ</given-names></name><name><surname>Davies-Thompson</surname> <given-names>J</given-names></name><name><surname>Kingstone</surname> <given-names>A</given-names></name><name><surname>Young</surname> <given-names>AW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Internal and external features of the face are represented holistically in face-selective regions of visual cortex</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>3544</fpage><lpage>3552</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4863-09.2010</pub-id><pub-id pub-id-type="pmid">20203214</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcurio</surname> <given-names>LR</given-names></name><name><surname>Gold</surname> <given-names>JM</given-names></name><name><surname>James</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The response of face-selective cortex with single face parts and part combinations</article-title><source>Neuropsychologia</source><volume>50</volume><fpage>2454</fpage><lpage>2459</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.06.016</pub-id><pub-id pub-id-type="pmid">22750118</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashrafulla</surname> <given-names>S</given-names></name><name><surname>Haldar</surname> <given-names>JP</given-names></name><name><surname>Joshi</surname> <given-names>AA</given-names></name><name><surname>Leahy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Canonical Granger causality between regions of interest</article-title><source>NeuroImage</source><volume>83</volume><fpage>189</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.06.056</pub-id><pub-id pub-id-type="pmid">23811410</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldauf</surname> <given-names>D</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural mechanisms of object-based attention</article-title><source>Science</source><volume>344</volume><fpage>424</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1126/science.1247003</pub-id><pub-id pub-id-type="pmid">24763592</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname> <given-names>L</given-names></name><name><surname>Seth</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The MVGC multivariate granger causality toolbox: a new approach to Granger-causal inference</article-title><source>Journal of Neuroscience Methods</source><volume>223</volume><fpage>50</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.10.018</pub-id><pub-id pub-id-type="pmid">24200508</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baseler</surname> <given-names>HA</given-names></name><name><surname>Harris</surname> <given-names>RJ</given-names></name><name><surname>Young</surname> <given-names>AW</given-names></name><name><surname>Andrews</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural responses to expression and gaze in the posterior superior temporal sulcus interact with facial identity</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>737</fpage><lpage>744</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs360</pub-id><pub-id pub-id-type="pmid">23172771</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benitez-Quiroz</surname> <given-names>CF</given-names></name><name><surname>Srinivasan</surname> <given-names>R</given-names></name><name><surname>Martinez</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Facial color is an efficient mechanism to visually transmit emotion</article-title><source>PNAS</source><volume>115</volume><fpage>3581</fpage><lpage>3586</lpage><pub-id pub-id-type="doi">10.1073/pnas.1716084115</pub-id><pub-id pub-id-type="pmid">29555780</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname> <given-names>Y</given-names></name><name><surname>Hochberg</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bentin</surname> <given-names>S</given-names></name><name><surname>Allison</surname> <given-names>T</given-names></name><name><surname>Puce</surname> <given-names>A</given-names></name><name><surname>Perez</surname> <given-names>E</given-names></name><name><surname>McCarthy</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Electrophysiological studies of face perception in humans</article-title><source>Journal of Cognitive Neuroscience</source><volume>8</volume><fpage>551</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.1162/jocn.1996.8.6.551</pub-id><pub-id pub-id-type="pmid">20740065</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname> <given-names>M</given-names></name><name><surname>Yovel</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Two neural pathways of face processing: a critical evaluation of current models</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>55</volume><fpage>536</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.06.010</pub-id><pub-id pub-id-type="pmid">26067903</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname> <given-names>T</given-names></name><name><surname>Chen</surname> <given-names>J</given-names></name><name><surname>Zhou</surname> <given-names>T</given-names></name><name><surname>He</surname> <given-names>Y</given-names></name><name><surname>Fang</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Function and structure of human left fusiform cortex are closely associated with perceptual learning of faces</article-title><source>Current Biology</source><volume>24</volume><fpage>222</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.12.028</pub-id><pub-id pub-id-type="pmid">24412207</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braje</surname> <given-names>W</given-names></name><name><surname>Kersten</surname> <given-names>D</given-names></name><name><surname>Tarr</surname> <given-names>M</given-names></name><name><surname>Troje</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Illumination effects in face recognition</article-title><source>Psychobiology</source><volume>26</volume><fpage>371</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.3758/BF03330623</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cavanagh</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1991">1991</year><chapter-title>What’s up in top-down processing</chapter-title><person-group person-group-type="editor"><name><surname>Gorea</surname> <given-names>A</given-names></name></person-group><source>Representations of Vision</source><publisher-name>Cambridge University Press</publisher-name><fpage>295</fpage><lpage>304</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Whitney</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Tracking the affective state of unseen persons</article-title><source>PNAS</source><volume>116</volume><fpage>7559</fpage><lpage>7564</lpage><pub-id pub-id-type="doi">10.1073/pnas.1812250116</pub-id><pub-id pub-id-type="pmid">30814221</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Similarity-Based fusion of MEG and fMRI reveals Spatio-Temporal dynamics in human cortex during visual object recognition</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3563</fpage><lpage>3579</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw135</pub-id><pub-id pub-id-type="pmid">27235099</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname> <given-names>D</given-names></name><name><surname>Meyers</surname> <given-names>E</given-names></name><name><surname>Sinha</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Contextually evoked object-specific responses in human visual cortex</article-title><source>Science</source><volume>304</volume><fpage>115</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1126/science.1093110</pub-id><pub-id pub-id-type="pmid">15001712</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Heering</surname> <given-names>A</given-names></name><name><surname>Rossion</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rapid categorization of natural face images in the infant right hemisphere</article-title><source>eLife</source><volume>4</volume><elocation-id>e06564</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06564</pub-id><pub-id pub-id-type="pmid">26032564</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deffke</surname> <given-names>I</given-names></name><name><surname>Sander</surname> <given-names>T</given-names></name><name><surname>Heidenreich</surname> <given-names>J</given-names></name><name><surname>Sommer</surname> <given-names>W</given-names></name><name><surname>Curio</surname> <given-names>G</given-names></name><name><surname>Trahms</surname> <given-names>L</given-names></name><name><surname>Lueschow</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>MEG/EEG sources of the 170-ms response to faces are co-localized in the fusiform gyrus</article-title><source>NeuroImage</source><volume>35</volume><fpage>1495</fpage><lpage>1501</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.01.034</pub-id><pub-id pub-id-type="pmid">17363282</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname> <given-names>N</given-names></name><name><surname>Mostert</surname> <given-names>P</given-names></name><name><surname>Lange</surname> <given-names>FP</given-names></name><name><surname>Bosch</surname> <given-names>S</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Differential temporal dynamics during visual imagery and perception</article-title><source>eLife</source><volume>7</volume><elocation-id>e33904</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.33904</pub-id><pub-id pub-id-type="pmid">29807570</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobs</surname> <given-names>K</given-names></name><name><surname>Isik</surname> <given-names>L</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>How face perception unfolds over time</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1258</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-09239-1</pub-id><pub-id pub-id-type="pmid">30890707</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duchaine</surname> <given-names>B</given-names></name><name><surname>Yovel</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A revised neural framework for face processing</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>393</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035518</pub-id><pub-id pub-id-type="pmid">28532371</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engell</surname> <given-names>AD</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Facial expression and gaze-direction in human superior temporal sulcus</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>3234</fpage><lpage>3241</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.06.022</pub-id><pub-id pub-id-type="pmid">17707444</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname> <given-names>SL</given-names></name><name><surname>Ishai</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Effective connectivity within the distributed cortical network for face perception</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2400</fpage><lpage>2406</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl148</pub-id><pub-id pub-id-type="pmid">17190969</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname> <given-names>L</given-names></name><name><surname>Liu</surname> <given-names>J</given-names></name><name><surname>Wang</surname> <given-names>Z</given-names></name><name><surname>Li</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>L</given-names></name><name><surname>Ge</surname> <given-names>L</given-names></name><name><surname>Tian</surname> <given-names>J</given-names></name><name><surname>Lee</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The other face of the other-race effect: an fMRI investigation of the other-race face categorization advantage</article-title><source>Neuropsychologia</source><volume>49</volume><fpage>3739</fpage><lpage>3749</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2011.09.031</pub-id><pub-id pub-id-type="pmid">21971308</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title><source>Science</source><volume>330</volume><fpage>845</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1194908</pub-id><pub-id pub-id-type="pmid">21051642</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The free-energy principle: a unified brain theory?</article-title><source>Nature Reviews Neuroscience</source><volume>11</volume><fpage>127</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1038/nrn2787</pub-id><pub-id pub-id-type="pmid">20068583</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname> <given-names>Z</given-names></name><name><surname>Goldstein</surname> <given-names>A</given-names></name><name><surname>Harpaz</surname> <given-names>Y</given-names></name><name><surname>Hansel</surname> <given-names>M</given-names></name><name><surname>Zion-Golumbic</surname> <given-names>E</given-names></name><name><surname>Bentin</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A magnetoencephalographic study of face processing: m170, gamma-band oscillations and source localization</article-title><source>Human Brain Mapping</source><volume>34</volume><fpage>1783</fpage><lpage>1795</lpage><pub-id pub-id-type="doi">10.1002/hbm.22028</pub-id><pub-id pub-id-type="pmid">22422432</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname> <given-names>I</given-names></name><name><surname>Tarr</surname> <given-names>MJ</given-names></name><name><surname>Moylan</surname> <given-names>J</given-names></name><name><surname>Skudlarski</surname> <given-names>P</given-names></name><name><surname>Gore</surname> <given-names>JC</given-names></name><name><surname>Anderson</surname> <given-names>AW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The fusiform &quot;face area&quot; is part of a network that processes faces at the individual level</article-title><source>Journal of Cognitive Neuroscience</source><volume>12</volume><fpage>495</fpage><lpage>504</lpage><pub-id pub-id-type="doi">10.1162/089892900562165</pub-id><pub-id pub-id-type="pmid">10931774</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gentile</surname> <given-names>F</given-names></name><name><surname>Ales</surname> <given-names>J</given-names></name><name><surname>Rossion</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Being BOLD: the neural dynamics of face perception</article-title><source>Human Brain Mapping</source><volume>38</volume><fpage>120</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1002/hbm.23348</pub-id><pub-id pub-id-type="pmid">27585292</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerardin</surname> <given-names>P</given-names></name><name><surname>Kourtzi</surname> <given-names>Z</given-names></name><name><surname>Mamassian</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Prior knowledge of illumination for 3D perception in the human brain</article-title><source>PNAS</source><volume>107</volume><fpage>16309</fpage><lpage>16314</lpage><pub-id pub-id-type="doi">10.1073/pnas.1006285107</pub-id><pub-id pub-id-type="pmid">20805488</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghuman</surname> <given-names>AS</given-names></name><name><surname>Brunet</surname> <given-names>NM</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Konecky</surname> <given-names>RO</given-names></name><name><surname>Pyles</surname> <given-names>JA</given-names></name><name><surname>Walls</surname> <given-names>SA</given-names></name><name><surname>Destefino</surname> <given-names>V</given-names></name><name><surname>Wang</surname> <given-names>W</given-names></name><name><surname>Richardson</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamic encoding of face information in the human fusiform gyrus</article-title><source>Nature Communications</source><volume>5</volume><elocation-id>5672</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms6672</pub-id><pub-id pub-id-type="pmid">25482825</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural response to the visual familiarity of faces</article-title><source>Brain Research Bulletin</source><volume>71</volume><fpage>76</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.brainresbull.2006.08.003</pub-id><pub-id pub-id-type="pmid">17113931</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural systems for recognition of familiar faces</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>32</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.04.015</pub-id><pub-id pub-id-type="pmid">16797608</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goold</surname> <given-names>JE</given-names></name><name><surname>Meng</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Categorical learning revealed in activity pattern of left fusiform cortex</article-title><source>Human Brain Mapping</source><volume>38</volume><fpage>3648</fpage><lpage>3658</lpage><pub-id pub-id-type="doi">10.1002/hbm.23620</pub-id><pub-id pub-id-type="pmid">28432767</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Luessi</surname> <given-names>M</given-names></name><name><surname>Larson</surname> <given-names>E</given-names></name><name><surname>Engemann</surname> <given-names>DA</given-names></name><name><surname>Strohmeier</surname> <given-names>D</given-names></name><name><surname>Brodbeck</surname> <given-names>C</given-names></name><name><surname>Goj</surname> <given-names>R</given-names></name><name><surname>Jas</surname> <given-names>M</given-names></name><name><surname>Brooks</surname> <given-names>T</given-names></name><name><surname>Parkkonen</surname> <given-names>L</given-names></name><name><surname>Hämäläinen</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>MEG and EEG data analysis with MNE-Python</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>267</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id><pub-id pub-id-type="pmid">24431986</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Luessi</surname> <given-names>M</given-names></name><name><surname>Larson</surname> <given-names>E</given-names></name><name><surname>Engemann</surname> <given-names>DA</given-names></name><name><surname>Strohmeier</surname> <given-names>D</given-names></name><name><surname>Brodbeck</surname> <given-names>C</given-names></name><name><surname>Parkkonen</surname> <given-names>L</given-names></name><name><surname>Hämäläinen</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>MNE software for processing MEG and EEG data</article-title><source>NeuroImage</source><volume>86</volume><fpage>446</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.027</pub-id><pub-id pub-id-type="pmid">24161808</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granger</surname> <given-names>CWJ</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Investigating causal relations by econometric models and Cross-spectral methods</article-title><source>Econometrica</source><volume>37</volume><elocation-id>424</elocation-id><pub-id pub-id-type="doi">10.2307/1912791</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname> <given-names>K</given-names></name><name><surname>Knouf</surname> <given-names>N</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The fusiform face area subserves face perception, not generic within-category identification</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>555</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1038/nn1224</pub-id><pub-id pub-id-type="pmid">15077112</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossman</surname> <given-names>ED</given-names></name><name><surname>Battelli</surname> <given-names>L</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Repetitive TMS over posterior STS disrupts perception of biological motion</article-title><source>Vision Research</source><volume>45</volume><fpage>2847</fpage><lpage>2853</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2005.05.027</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grützner</surname> <given-names>C</given-names></name><name><surname>Uhlhaas</surname> <given-names>PJ</given-names></name><name><surname>Genc</surname> <given-names>E</given-names></name><name><surname>Kohler</surname> <given-names>A</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name><name><surname>Wibral</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neuroelectromagnetic correlates of perceptual closure processes</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>8342</fpage><lpage>8352</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5434-09.2010</pub-id><pub-id pub-id-type="pmid">20554885</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Clark</surname> <given-names>VP</given-names></name><name><surname>Schouten</surname> <given-names>JL</given-names></name><name><surname>Hoffman</surname> <given-names>EA</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The effect of face inversion on activity in human neural systems for face and object perception</article-title><source>Neuron</source><volume>22</volume><fpage>189</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80690-X</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Hoffman</surname> <given-names>EA</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The distributed human neural system for face perception</article-title><source>Trends in Cognitive Sciences</source><volume>4</volume><fpage>223</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01482-0</pub-id><pub-id pub-id-type="pmid">10827445</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hegde</surname> <given-names>J</given-names></name><name><surname>Thompson</surname> <given-names>S</given-names></name><name><surname>Kersten</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Identifying faces in two-tone ('Mooney') images: A psychophysical and fMRI study</article-title><source>Journal of Vision</source><volume>7</volume><elocation-id>624</elocation-id><pub-id pub-id-type="doi">10.1167/7.9.624</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname> <given-names>EA</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Distinct representations of eye gaze and identity in the distributed human neural system for face perception</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>80</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/71152</pub-id><pub-id pub-id-type="pmid">10607399</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphrey</surname> <given-names>GK</given-names></name><name><surname>Goodale</surname> <given-names>MA</given-names></name><name><surname>Bowen</surname> <given-names>CV</given-names></name><name><surname>Gati</surname> <given-names>JS</given-names></name><name><surname>Vilis</surname> <given-names>T</given-names></name><name><surname>Rutt</surname> <given-names>BK</given-names></name><name><surname>Menon</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Differences in perceived shape from shading correlate with activity in early visual Areas</article-title><source>Current Biology</source><volume>7</volume><fpage>144</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/S0960-9822(06)00058-3</pub-id><pub-id pub-id-type="pmid">9016702</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Issa</surname> <given-names>EB</given-names></name><name><surname>Cadieu</surname> <given-names>CF</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural dynamics at successive stages of the ventral visual stream are consistent with hierarchical error signals</article-title><source>eLife</source><volume>7</volume><elocation-id>e42870</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.42870</pub-id><pub-id pub-id-type="pmid">30484773</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Issa</surname> <given-names>EB</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Precedence of the eye region in neural processing of faces</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>16666</fpage><lpage>16682</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2391-12.2012</pub-id><pub-id pub-id-type="pmid">23175821</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itier</surname> <given-names>RJ</given-names></name><name><surname>Herdman</surname> <given-names>AT</given-names></name><name><surname>George</surname> <given-names>N</given-names></name><name><surname>Cheyne</surname> <given-names>D</given-names></name><name><surname>Taylor</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Inversion and contrast-reversal effects on face processing assessed by MEG</article-title><source>Brain Research</source><volume>1115</volume><fpage>108</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2006.07.072</pub-id><pub-id pub-id-type="pmid">16930564</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kadipasaoglu</surname> <given-names>CM</given-names></name><name><surname>Conner</surname> <given-names>CR</given-names></name><name><surname>Baboyan</surname> <given-names>VG</given-names></name><name><surname>Rollo</surname> <given-names>M</given-names></name><name><surname>Pieters</surname> <given-names>TA</given-names></name><name><surname>Tandon</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Network dynamics of human face perception</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0188834</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0188834</pub-id><pub-id pub-id-type="pmid">29190811</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname> <given-names>N</given-names></name><name><surname>McDermott</surname> <given-names>J</given-names></name><name><surname>Chun</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id><pub-id pub-id-type="pmid">9151747</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname> <given-names>CJ</given-names></name><name><surname>Davidesco</surname> <given-names>I</given-names></name><name><surname>Megevand</surname> <given-names>P</given-names></name><name><surname>Lado</surname> <given-names>FA</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Tuning face perception with electrical stimulation of the fusiform gyrus</article-title><source>Human Brain Mapping</source><volume>38</volume><fpage>2830</fpage><lpage>2842</lpage><pub-id pub-id-type="doi">10.1002/hbm.23543</pub-id><pub-id pub-id-type="pmid">28345189</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kume</surname> <given-names>Y</given-names></name><name><surname>Maekawa</surname> <given-names>T</given-names></name><name><surname>Urakawa</surname> <given-names>T</given-names></name><name><surname>Hironaga</surname> <given-names>N</given-names></name><name><surname>Ogata</surname> <given-names>K</given-names></name><name><surname>Shigyo</surname> <given-names>M</given-names></name><name><surname>Tobimatsu</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neuromagnetic evidence that the right fusiform face area is essential for human face awareness: an intermittent binocular rivalry study</article-title><source>Neuroscience Research</source><volume>109</volume><fpage>54</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1016/j.neures.2016.02.004</pub-id><pub-id pub-id-type="pmid">26907522</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landi</surname> <given-names>SM</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Two Areas for familiar face recognition in the primate brain</article-title><source>Science</source><volume>357</volume><fpage>591</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1126/science.aan1139</pub-id><pub-id pub-id-type="pmid">28798130</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>TS</given-names></name><name><surname>Yang</surname> <given-names>CF</given-names></name><name><surname>Romero</surname> <given-names>RD</given-names></name><name><surname>Mumford</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neural activity in early visual cortex reflects behavioral experience and higher-order perceptual saliency</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>589</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1038/nn0602-860</pub-id><pub-id pub-id-type="pmid">12021764</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>J</given-names></name><name><surname>Harris</surname> <given-names>A</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Stages of processing in face perception: an MEG study</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>910</fpage><lpage>916</lpage><pub-id pub-id-type="doi">10.1038/nn909</pub-id><pub-id pub-id-type="pmid">12195430</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>J</given-names></name><name><surname>Harris</surname> <given-names>A</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perception of face parts and face configurations: an FMRI study</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>203</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21203</pub-id><pub-id pub-id-type="pmid">19302006</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual perception of facial expressions of emotion</article-title><source>Current Opinion in Psychology</source><volume>17</volume><fpage>27</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1016/j.copsyc.2017.06.009</pub-id><pub-id pub-id-type="pmid">28950969</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Context may reveal how you feel</article-title><source>PNAS</source><volume>116</volume><fpage>7169</fpage><lpage>7171</lpage><pub-id pub-id-type="doi">10.1073/pnas.1902661116</pub-id><pub-id pub-id-type="pmid">30898883</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGugin</surname> <given-names>RW</given-names></name><name><surname>Ryan</surname> <given-names>KF</given-names></name><name><surname>Tamber-Rosenau</surname> <given-names>BJ</given-names></name><name><surname>Gauthier</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The role of experience in the Face-Selective response in right FFA</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>2071</fpage><lpage>2084</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx113</pub-id><pub-id pub-id-type="pmid">28472436</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKeeff</surname> <given-names>TJ</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The timing of perceptual decisions for ambiguous face stimuli in the human ventral visual cortex</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>669</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhk015</pub-id><pub-id pub-id-type="pmid">16648454</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meng</surname> <given-names>M</given-names></name><name><surname>Cherian</surname> <given-names>T</given-names></name><name><surname>Singal</surname> <given-names>G</given-names></name><name><surname>Sinha</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Lateralization of face processing in the human brain</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>279</volume><fpage>2052</fpage><lpage>2061</lpage><pub-id pub-id-type="doi">10.1098/rspb.2011.1784</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Crapse</surname> <given-names>T</given-names></name><name><surname>Chang</surname> <given-names>L</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The effect of face patch microstimulation on perception of faces and objects</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>743</fpage><lpage>752</lpage><pub-id pub-id-type="doi">10.1038/nn.4527</pub-id><pub-id pub-id-type="pmid">28288127</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mooney</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="1957">1957</year><article-title>Age in the development of closure ability in children</article-title><source>Canadian Journal of Psychology/Revue Canadienne De Psychologie</source><volume>11</volume><fpage>219</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1037/h0083717</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>C</given-names></name><name><surname>Cavanagh</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Recovery of 3D volume from 2-tone images of novel objects</article-title><source>Cognition</source><volume>67</volume><fpage>45</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1016/S0010-0277(98)00014-6</pub-id><pub-id pub-id-type="pmid">9735536</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mumford</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>On the computational architecture of the neocortex. II. the role of cortico-cortical loops</article-title><source>Biological Cybernetics</source><volume>66</volume><fpage>241</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1007/bf00198477</pub-id><pub-id pub-id-type="pmid">1540675</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>SO</given-names></name><name><surname>Schrater</surname> <given-names>P</given-names></name><name><surname>Kersten</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Perceptual grouping and the interactions between visual cortical Areas</article-title><source>Neural Networks</source><volume>17</volume><fpage>695</fpage><lpage>705</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2004.03.010</pub-id><pub-id pub-id-type="pmid">15288893</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname> <given-names>VT</given-names></name><name><surname>Cunnington</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The superior temporal sulcus and the N170 during face processing: single trial analysis of concurrent EEG-fMRI</article-title><source>NeuroImage</source><volume>86</volume><fpage>492</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.047</pub-id><pub-id pub-id-type="pmid">24185024</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Toole</surname> <given-names>AJ</given-names></name><name><surname>Roark</surname> <given-names>DA</given-names></name><name><surname>Abdi</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Recognizing moving faces: a psychological and neural synthesis</article-title><source>Trends in Cognitive Sciences</source><volume>6</volume><fpage>261</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(02)01908-3</pub-id><pub-id pub-id-type="pmid">12039608</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohayon</surname> <given-names>S</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>What makes a cell face selective? the importance of contrast</article-title><source>Neuron</source><volume>74</volume><fpage>567</fpage><lpage>581</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.024</pub-id><pub-id pub-id-type="pmid">22578507</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perry</surname> <given-names>G</given-names></name><name><surname>Singh</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Localizing evoked and induced responses to faces using magnetoencephalography</article-title><source>European Journal of Neuroscience</source><volume>39</volume><fpage>1517</fpage><lpage>1527</lpage><pub-id pub-id-type="doi">10.1111/ejn.12520</pub-id><pub-id pub-id-type="pmid">24617643</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname> <given-names>D</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name><name><surname>Yovel</surname> <given-names>G</given-names></name><name><surname>Duchaine</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>TMS evidence for the involvement of the right occipital face area in early face processing</article-title><source>Current Biology</source><volume>17</volume><fpage>1568</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.07.063</pub-id><pub-id pub-id-type="pmid">17764942</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname> <given-names>D</given-names></name><name><surname>Dilks</surname> <given-names>DD</given-names></name><name><surname>Saxe</surname> <given-names>RR</given-names></name><name><surname>Triantafyllou</surname> <given-names>C</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Differential selectivity for dynamic versus static information in face-selective cortical regions</article-title><source>NeuroImage</source><volume>56</volume><fpage>2356</fpage><lpage>2363</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.03.067</pub-id><pub-id pub-id-type="pmid">21473921</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname> <given-names>D</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name><name><surname>Duchaine</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>The role of the occipital face area in the cortical face perception network</article-title><source>Experimental Brain Research</source><volume>209</volume><fpage>481</fpage><lpage>493</lpage><pub-id pub-id-type="doi">10.1007/s00221-011-2579-1</pub-id><pub-id pub-id-type="pmid">21318346</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname> <given-names>D</given-names></name><name><surname>Goldhaber</surname> <given-names>T</given-names></name><name><surname>Duchaine</surname> <given-names>B</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Two critical and functionally distinct stages of face and body perception</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>15877</fpage><lpage>15885</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2624-12.2012</pub-id><pub-id pub-id-type="pmid">23136426</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname> <given-names>D</given-names></name><name><surname>Duchaine</surname> <given-names>B</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Combined TMS and FMRI reveal dissociable cortical pathways for dynamic and static face perception</article-title><source>Current Biology</source><volume>24</volume><fpage>2066</fpage><lpage>2070</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.07.060</pub-id><pub-id pub-id-type="pmid">25131678</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pourtois</surname> <given-names>G</given-names></name><name><surname>Schwartz</surname> <given-names>S</given-names></name><name><surname>Seghier</surname> <given-names>ML</given-names></name><name><surname>Lazeyras</surname> <given-names>F</given-names></name><name><surname>Vuilleumier</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>View-independent coding of face identity in frontal and temporal cortices is modulated by familiarity: an event-related fMRI study</article-title><source>NeuroImage</source><volume>24</volume><fpage>1214</fpage><lpage>1224</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.10.038</pub-id><pub-id pub-id-type="pmid">15670699</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puce</surname> <given-names>A</given-names></name><name><surname>Allison</surname> <given-names>T</given-names></name><name><surname>Bentin</surname> <given-names>S</given-names></name><name><surname>Gore</surname> <given-names>JC</given-names></name><name><surname>McCarthy</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Temporal cortex activation in humans viewing eye and mouth movements</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>2188</fpage><lpage>2199</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-06-02188.1998</pub-id><pub-id pub-id-type="pmid">9482803</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname> <given-names>R</given-names></name><name><surname>Ballard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname> <given-names>B</given-names></name><name><surname>Caldara</surname> <given-names>R</given-names></name><name><surname>Seghier</surname> <given-names>M</given-names></name><name><surname>Schuller</surname> <given-names>AM</given-names></name><name><surname>Lazeyras</surname> <given-names>F</given-names></name><name><surname>Mayer</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A network of occipito-temporal face-sensitive Areas besides the right middle fusiform gyrus is necessary for normal face processing</article-title><source>Brain</source><volume>126</volume><fpage>2381</fpage><lpage>2395</lpage><pub-id pub-id-type="doi">10.1093/brain/awg241</pub-id><pub-id pub-id-type="pmid">12876150</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname> <given-names>B</given-names></name><name><surname>Dricot</surname> <given-names>L</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Busigny</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Holistic face categorization in higher order visual Areas of the normal and prosopagnosic brain: toward a non-hierarchical view of face perception</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><elocation-id>225</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2010.00225</pub-id><pub-id pub-id-type="pmid">21267432</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname> <given-names>B</given-names></name><name><surname>Prieto</surname> <given-names>EA</given-names></name><name><surname>Boremanse</surname> <given-names>A</given-names></name><name><surname>Kuefner</surname> <given-names>D</given-names></name><name><surname>Van Belle</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A steady-state visual evoked potential approach to individual face perception: effect of Inversion, contrast-reversal and temporal dynamics</article-title><source>NeuroImage</source><volume>63</volume><fpage>1585</fpage><lpage>1600</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.08.033</pub-id><pub-id pub-id-type="pmid">22917988</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname> <given-names>B</given-names></name><name><surname>Torfs</surname> <given-names>K</given-names></name><name><surname>Jacques</surname> <given-names>C</given-names></name><name><surname>Liu-Shuang</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fast periodic presentation of natural images reveals a robust face-selective electrophysiological response in the human brain</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>18</elocation-id><pub-id pub-id-type="doi">10.1167/15.1.18</pub-id><pub-id pub-id-type="pmid">25597037</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rossion</surname> <given-names>B</given-names></name><name><surname>Jacques</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>The N170: understanding the time-course of face perception in the human brain</chapter-title><person-group person-group-type="editor"><name><surname>Luck</surname> <given-names>S. J</given-names></name><name><surname>Kappenman</surname> <given-names>E. S</given-names></name></person-group><source>The Oxford Handbook of Event-Related Potential Components</source><publisher-name>Oxford University Press</publisher-name><fpage>115</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1093/oxfordhb/9780195374148.013.0064</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rotshtein</surname> <given-names>P</given-names></name><name><surname>Henson</surname> <given-names>RN</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name><name><surname>Driver</surname> <given-names>J</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Morphing marilyn into Maggie dissociates physical and identity face representations in the brain</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>107</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1038/nn1370</pub-id><pub-id pub-id-type="pmid">15592463</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname> <given-names>R</given-names></name><name><surname>Biederman</surname> <given-names>I</given-names></name><name><surname>Nederhouser</surname> <given-names>M</given-names></name><name><surname>Sinha</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The utility of surface reflectance for the recognition of upright and inverted faces</article-title><source>Vision Research</source><volume>47</volume><fpage>157</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.11.002</pub-id><pub-id pub-id-type="pmid">17174375</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadeh</surname> <given-names>B</given-names></name><name><surname>Podlipsky</surname> <given-names>I</given-names></name><name><surname>Zhdanov</surname> <given-names>A</given-names></name><name><surname>Yovel</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Event-related potential and functional MRI measures of face-selectivity are highly correlated: a simultaneous ERP-fMRI investigation</article-title><source>Human Brain Mapping</source><volume>31</volume><fpage>1490</fpage><lpage>1501</lpage><pub-id pub-id-type="doi">10.1002/hbm.20952</pub-id><pub-id pub-id-type="pmid">20127870</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiltz</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Holistic perception of individual faces in the right middle fusiform gyrus as evidenced by the composite face illusion</article-title><source>Journal of Vision</source><volume>10</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1167/10.2.25</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname> <given-names>L</given-names></name><name><surname>Yovel</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The roles of perceptual and conceptual information in face recognition</article-title><source>Journal of Experimental Psychology: General</source><volume>145</volume><fpage>1493</fpage><lpage>1511</lpage><pub-id pub-id-type="doi">10.1037/xge0000220</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sekihara</surname> <given-names>K</given-names></name><name><surname>Nagarajan</surname> <given-names>SS</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>Adaptive Spatial Filters for Electromagnetic Brain Imaging</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-540-79370-0</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sinha</surname> <given-names>P</given-names></name><name><surname>Balas</surname> <given-names>B</given-names></name><name><surname>Ostrovsky</surname> <given-names>Y</given-names></name><name><surname>Russell</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Face recognition by humans: nineteen results all computer vision researchers should know about</article-title><source>Proceedings of the IEEE</source><volume>94</volume><fpage>1948</fpage><lpage>1962</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2006.884093</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname> <given-names>R</given-names></name><name><surname>Golomb</surname> <given-names>JD</given-names></name><name><surname>Martinez</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A neural basis of facial action recognition in humans</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>4434</fpage><lpage>4442</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1704-15.2016</pub-id><pub-id pub-id-type="pmid">27098688</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steeves</surname> <given-names>JK</given-names></name><name><surname>Culham</surname> <given-names>JC</given-names></name><name><surname>Duchaine</surname> <given-names>BC</given-names></name><name><surname>Pratesi</surname> <given-names>CC</given-names></name><name><surname>Valyear</surname> <given-names>KF</given-names></name><name><surname>Schindler</surname> <given-names>I</given-names></name><name><surname>Humphrey</surname> <given-names>GK</given-names></name><name><surname>Milner</surname> <given-names>AD</given-names></name><name><surname>Goodale</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The fusiform face area is not sufficient for face recognition: evidence from a patient with dense prosopagnosia and no occipital face area</article-title><source>Neuropsychologia</source><volume>44</volume><fpage>594</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.06.013</pub-id><pub-id pub-id-type="pmid">16125741</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugiura</surname> <given-names>M</given-names></name><name><surname>Mano</surname> <given-names>Y</given-names></name><name><surname>Sasaki</surname> <given-names>A</given-names></name><name><surname>Sadato</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Beyond the memory mechanism: person-selective and nonselective processes in recognition of personally familiar faces</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>699</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21469</pub-id><pub-id pub-id-type="pmid">20350171</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname> <given-names>DY</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Knutsen</surname> <given-names>TA</given-names></name><name><surname>Mandeville</surname> <given-names>JB</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Faces and objects in macaque cerebral cortex</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>989</fpage><lpage>995</lpage><pub-id pub-id-type="doi">10.1038/nn1111</pub-id><pub-id pub-id-type="pmid">12925854</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname> <given-names>DY</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A cortical region consisting entirely of face-selective cells</article-title><source>Science</source><volume>311</volume><fpage>670</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1126/science.1119983</pub-id><pub-id pub-id-type="pmid">16456083</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname> <given-names>DY</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Comparing face patch systems in macaques and humans</article-title><source>PNAS</source><volume>105</volume><fpage>19514</fpage><lpage>19519</lpage><pub-id pub-id-type="doi">10.1073/pnas.0809662105</pub-id><pub-id pub-id-type="pmid">19033466</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname> <given-names>BD</given-names></name><name><surname>van Drongelen</surname> <given-names>W</given-names></name><name><surname>Yuchtman</surname> <given-names>M</given-names></name><name><surname>Suzuki</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>44</volume><fpage>867</fpage><lpage>880</lpage><pub-id pub-id-type="doi">10.1109/10.623056</pub-id><pub-id pub-id-type="pmid">9282479</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Ding</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Estimating Granger causality after stimulus onset: a cautionary note</article-title><source>NeuroImage</source><volume>41</volume><fpage>767</fpage><lpage>776</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.025</pub-id><pub-id pub-id-type="pmid">18455441</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weibert</surname> <given-names>K</given-names></name><name><surname>Flack</surname> <given-names>TR</given-names></name><name><surname>Young</surname> <given-names>AW</given-names></name><name><surname>Andrews</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Patterns of neural response in face regions are predicted by low-level image properties</article-title><source>Cortex</source><volume>103</volume><fpage>199</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2018.03.009</pub-id><pub-id pub-id-type="pmid">29655043</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiner</surname> <given-names>KS</given-names></name><name><surname>Barnett</surname> <given-names>MA</given-names></name><name><surname>Lorenz</surname> <given-names>S</given-names></name><name><surname>Caspers</surname> <given-names>J</given-names></name><name><surname>Stigliani</surname> <given-names>A</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name><name><surname>Zilles</surname> <given-names>K</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Grill-Spector</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The cytoarchitecture of Domain-specific regions in human High-level visual cortex</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>146</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw361</pub-id><pub-id pub-id-type="pmid">27909003</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willenbockel</surname> <given-names>V</given-names></name><name><surname>Sadr</surname> <given-names>J</given-names></name><name><surname>Fiset</surname> <given-names>D</given-names></name><name><surname>Horne</surname> <given-names>GO</given-names></name><name><surname>Gosselin</surname> <given-names>F</given-names></name><name><surname>Tanaka</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Controlling low-level image properties: the SHINE toolbox</article-title><source>Behavior Research Methods</source><volume>42</volume><fpage>671</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.3758/BRM.42.3.671</pub-id><pub-id pub-id-type="pmid">20805589</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yip</surname> <given-names>AW</given-names></name><name><surname>Sinha</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Contribution of color to face recognition</article-title><source>Perception</source><volume>31</volume><fpage>995</fpage><lpage>1003</lpage><pub-id pub-id-type="doi">10.1068/p3376</pub-id><pub-id pub-id-type="pmid">12269592</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48764.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Meng</surname><given-names>Ming</given-names></name><role>Reviewing Editor</role><aff><institution>South China Normal University</institution><country>China</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Meng</surname><given-names>Ming</given-names> </name><role>Reviewer</role><aff><institution>South China Normal University</institution><country>China</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Through four experiments, your article combines fMRI and source-localized Magnetoencephalography (MEG) to investigate the dynamics of face information processing in the human brain. I found most interesting your results of the temporal dynamics of the occipital-temporal face network contingent upon bottom-up processing of normal facial inputs versus top-down processing of impoverished facial inputs, which were supported by converging evidence. While there were criticisms by our reviewers on reliability of MEG source localization, new experiments in the revised version of the article provided solid data that greatly strengthened our confidence with the novel technique approach, complementing a large number of previous neuroimaging and neurophysiological studies. Your findings not only fill the knowledge gap of dynamic interactions between the nodes of core face processing network, but also reconcile previous competing models of bottom-up versus top-down face processing mechanisms. Given the importance of face information processing in cognitive psychology, social and affective neurosciences, as well as artificial intelligence, I believe a broad research community including psychologists, neuroscientists and computer scientists would benefit from reading this article. In addition, I think the novel methodological approach that combines fMRI and MEG with clever stimulus design would inspire future studies to follow these steps to further investigate fine-scale temporal dynamics of other important cognitive brain mechanisms.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for sending your article entitled &quot;The bottom-up and top-down processing of faces in the human occipitotemporal cortex&quot; for peer review at <italic>eLife</italic>. Your article is being evaluated by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation is being overseen by Joshua Gold as the Senior Editor.</p><p>Specifically, we think these major issues need to be fully addressed. In the interest of time, <italic>eLife</italic> normally would only invite a revision if all the major issues could be fully addressed within two months. Should you decide to submit the manuscript elsewhere, I am appending full reviews below that you can use to improve the paper as well:</p><p>Major issues:</p><p>1) The empirical and conceptual advances made in the current study need to be more clearly articulated with respect to previous work. It has been known for a while that the OFA responds at an earlier latency than the FFA (e.g., Liu et al., 2002), and that certain stimulus manipulations, such as face inversion and contrast reversal, lead to delayed responses to faces (Bentin et al., 1996; Rossion et al., 2000; Rossion et al., 2012). Previous fMRI work has shown that difficult to perceive Mooney faces can lead to response delays on the order of several seconds (McKeeff and Tong, 2007). More recent techniques have allowed research groups to provide more refined estimates of the timing of neural responses, such as the fusion of fMRI-MEG analyzed using representational similarity analysis (e.g., Cichy et al., 2014). Periodic visual stimulation has also been used to characterize the timing of neural responses obtained with EEG/MEG by several research groups (e.g., Rossion et al., 2012, 2014; Norcia et al., 2015), and this approach has been successfully applied to characterize top-down effects of feedback during face processing (e.g., Baldauf and Desimone, 2014).</p><p>2) Also, what is lacking significantly is the role of pSTS. We know pSTS is mostly involved in the analysis of facial muscle articulations (also called action units, AUs) and the interpretation of facial expressions and emotion, see Srinivasan et al., 2016, and Martinez, 2017. Also relevant is the role of low-level image features (Weibert et al., 2018), which is also missing from the Discussion; and, the role of color perception (Yip and Sinha, 2002; Benitez-Quiroz et al., 2018).</p><p>3) Another point that needs further discussion is the role of internal versus external face features (Sinha et al., 2006), and context (Sinha, Science 2004; Martinez, 2019). These discussions are essential to frame the results of the present paper within existing models of face perception.</p><p>4) The conclusions of the study rest on the data from a single experiment, and further investigation of the putative effects of top-down feedback and predictive coding are not provided. A follow-up experiment that both replicates and extends the current findings would help strengthen the study.</p><p>5) The reported effects pass statistical significance but not by a large margin. Moreover, there can be concerns that MEG data varies considerably across participants and can lead to heterogeneity of variance, especially across time points. Shuffling of the data with randomized labels would provide a more rigorous approach to statistical analysis.</p><p><italic>Reviewer #1:</italic></p><p>The neural mechanism of face processing has been a central topic of cognitive neuroscience for many years, however, dynamics of such mechanism remains unclear. He and colleagues combined fMRI ROI localization and reconstructing source signals from MEG to address this issue. Specifically, the authors analyzed MEG activity dynamics of the face processing core network that had been localized by fMRI. Most notably, when subjects were seeing famous faces, rOFA and rpFFA activity peaked at around 120 ms while raFFA activity peaked at around 150 ms. By contrast, when subjects were seeing Mooney face images, the rOFA activity peaked significantly later than the rpFFA activity. Given that recognizing faces from Mooney images would rely more heavily on top-down mechanisms, the authors argue for a top-down pathway from the rpFFA to rOFA for face processing.</p><p>The results are clear-cut and the paper is in general well-written. I believe the present study, if in the end published, would be of interests to a broad readership including psychologists and neuroscientists. I only have a few comments that I wish the authors to address:</p><p>1) While recognizing faces from Mooney images would certainly rely heavily on top-down mechanisms, it is hard to rule out the involvement of top-down mechanisms when processing normal face pictures. Intuitively, for example, processing familiar faces would involve more top-down experience driven activity than processing unfamiliar faces. However, the present results seem to suggest no significant differences between processing famous and unfamiliar faces. How come?</p><p>2) The Discussion somewhat overlooks effects potentially driven by different tasks. As far as I understand, subjects performed different tasks for the Mooney face experiment and normal face versus object picture experiments.</p><p>3) Given studies on the functional role of left FFA (e.g., Meng et al., 2012; Bi et al., 2014; Goold and Meng, 2017), I would be greatly interested in Results and Discussions regarding what the present data could reveal about dynamic relations between the left and right face processing core networks.</p><p>4) Some justification would be helpful for using sliding time windows of 50 ms. One possibility is to add power spectrum analysis. In any cases, power spectrum analysis might be helpful for revealing further fine-scale temporal dynamics of brain responses.</p><p><italic>Reviewer #3:</italic></p><p>The authors use MEG to measure cortical responses to normal faces and Mooney face images, and find that in the former case, the putative OFA responds at a somewhat earlier latency than the FFA while in the latter case, the FFA responds at a significantly earlier latency. Granger causality provides additional support for the authors' interpretation that feedback may be occurring from the FFA to the OFA.</p><p>The findings are of some interest but there are some major concerns. First, the discussion of previous work is rather limited and does not cite many related studies that have characterized the timing of face processing in the FFA and OFA. It has been known for a while that the OFA responds at an earlier latency than the FFA (e.g., Liu et al., 2002), and that certain stimulus manipulations, such as face inversion and contrast reversal, lead to delayed responses to faces (Bentin et al., 1996; Rossion et al., 2000; Rossion et al., 2012). Previous fMRI work has shown that difficult to perceive Mooney faces can lead to response delays on the order of several seconds (McKeeff and Tong, 2007). More recent techniques have allowed research groups to provide more refined estimates of the timing of neural responses, such as the fusion of fMRI-MEG analyzed using representational similarity analysis (e.g., Cichy et al., 2014). Periodic visual stimulation has also been used to characterize the timing of neural responses obtained with EEG/MEG by several research groups (e.g., Rossion et al., 2012, 2014; Norcia et al., 2015), and this approach has been successfully applied to characterize top-down effects of feedback during face processing (e.g., Baldauf and Desimone, 2014). The empirical and conceptual advances made in the current study need to be more clearly articulated with respect to previous work, and a clear argument for the specific contributions of this study is needed.</p><p>Another concern is that the conclusions of the study rest on the data from a single experiment, and further investigation of the putative effects of top-down feedback and predictive coding are not provided. Reproducibility is a serious concern in many fields of science, especially psychology and also neuroscience. A follow-up experiment that both replicates and extends the current findings would help strengthen the study. The reported effects pass statistical significance but not by a large margin. Moreover, there can be concerns that MEG data varies considerably across participants and can lead to heterogeneity of variance, especially across time points. Shuffling of the data with randomized labels would provide a more rigorous approach to statistical analysis.</p><p><italic>Reviewer #4:</italic></p><p>Authors present an interesting and timely study of the hierarchical functional computations executed during bottom-up and top-down face processing. The results are mostly consistent with what is known and accepted. This is important to support existing models.</p><p>A point that is lacking significantly is the role of pSTS. We know pSTS is mostly involved in the analysis of facial muscle articulations (also called action units, AUs) and the interpretation of facial expressions and emotion, see Srinivasan et al., 2016, and Martinez, 2017. Also relevant is the role of low-level image features (Weibert et al., 2018), which is also missing from the Discussion; and, the role of color perception (Yip and Sinha, 2002; Benitez-Quiroz et al., 2018).</p><p>Another point that needs further discussion is the role of internal versus external face features (Sinha et al., 2006), and context (Sinha, Science 2004; Martinez, 2019).</p><p>These discussions are essential to frame the results of the present paper within existing models of face perception. With appropriate changes, this could be a strong paper.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48764.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Major issues:</p><p>1) The empirical and conceptual advances made in the current study need to be more clearly articulated with respect to previous work. It has been known for a while that the OFA responds at an earlier latency than the FFA (e.g., Liu et al., 2002), and that certain stimulus manipulations, such as face inversion and contrast reversal, lead to delayed responses to faces (Bentin et al., 1996; Rossion et al., 2000; Rossion et al., 2012). Previous fMRI work has shown that difficult to perceive Mooney faces can lead to response delays on the order of several seconds (McKeeff and Tong, 2007). More recent techniques have allowed research groups to provide more refined estimates of the timing of neural responses, such as the fusion of fMRI-MEG analyzed using representational similarity analysis (e.g., Cichy et al., 2014). Periodic visual stimulation has also been used to characterize the timing of neural responses obtained with EEG/MEG by several research groups (e.g., Rossion et al., 2012, 2014; Norcia et al., 2015), and this approach has been successfully applied to characterize top-down effects of feedback during face processing (e.g., Baldauf and Desimone, 2014).</p></disp-quote><p>We appreciate and agree with this suggestion. The dynamics of face induced neural activation in FFA and OFA has been studied for a long time with various techniques. However, previous results are inconsistent and individually often lack either the spatial (e.g., sensor level EEG/MEG analysis) or temporal precision (e.g., fMRI data). Our results with combined fMRI and MEG measures, provide detailed and novel timing information of the core face network. For example, the relatively large temporal gap between the right anterior and posterior FFA was not reported in previous studies. Furthermore, our results showed that the temporal relationships between OFA and FFA are dependent on the internal facial features as well the context of visual input, which helps to understand how bottom-up and top-down processing together contribute to face perception.</p><p>Many previous studies used the N170/M170 component as the index of face processing in the ventral occipitotemporal cortex, however, the delayed N170/M170 response caused by certain stimulus manipulations (eg: face inversion, Mooney transformation) represents a relatively crude measure of face processing because the difficulty in attributing the sources of the delay. On the other hand, fMRI measures alone showing delayed FFA response to Mooney faces that was initially not recognized as faces simply reflect the time it took subjects to recognize difficult Mooney faces, rather than the real-time dynamics of Mooney face processing. In contrast, our results showed that when the face features were confounded with other shadows, the top-down rpFFA to rOFA projection became more dominated.</p><p>In the revised manuscript, we discussed the different techniques used to investigate the timing of face responses and the top-down modulation in face processing reported in previous studies (Discussion section paragraph three to five).</p><disp-quote content-type="editor-comment"><p>2) Also, what is lacking significantly is the role of pSTS. We know pSTS is mostly involved in the analysis of facial muscle articulations (also called action units, AUs) and the interpretation of facial expressions and emotion, see Srinivasan et al., 2016, and Martinez, 2017. Also relevant is the role of low-level image features (Weibert et al., 2018), which is also missing from the Discussion; and, the role of color perception (Yip and Sinha, 2002; Benitez-Quiroz et al., 2018).</p></disp-quote><p>The temporal responses of bilateral pSTS are broader (multi-peaked) and showed lower signal-to-noise than the ventral face-selective areas (Figure 2 and Figure 2—figure supplement 1). To increase our confidence about the pSTS time course, we analyzed the temporal responses of bilateral pSTS evoked by normal faces based on the additional data (Experiment 2), and the time courses basically remained the same as the previous ones (regardless of the task and face familiarity). We have added more discussion about the role of pSTS and its dynamics, especially in relation to the processing of facial expression, muscle articulations and motion.</p><fig id="respfig1"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-resp-fig1-v2.tif"/></fig><p>We also thank the reviewer for reminding us about the role of low-level features including color, and have added more discussion about their role in face processing.</p><disp-quote content-type="editor-comment"><p>3) Another point that needs further discussion is the role of internal versus external face features (Sinha et al., 2006), and context (Sinha, Science 2004; Martinez, 2019). These discussions are essential to frame the results of the present paper within existing models of face perception.</p></disp-quote><p>We agree that it is important to understand the role of internal versus external face features. Since we were going to obtain more experimental data during the revision, we made the efforts to performed additional MEG experiments to specifically investigate the role of internal versus external face features and context (see #4 below). We have also added more discussion about them.</p><disp-quote content-type="editor-comment"><p>4) The conclusions of the study rest on the data from a single experiment, and further investigation of the putative effects of top-down feedback and predictive coding are not provided. A follow-up experiment that both replicates and extends the current findings would help strengthen the study.</p></disp-quote><p>We thank the editor and reviewer for pushing us to perform a follow-up experiment. We did not just one but three follow-up experiments (one replication and two extensions), which indeed replicated and significantly extended the findings reported in the original version.</p><p>We collected more data for Experiment 2 (normal unfamiliar face vs Mooney face) to confirm the previous results and performed two additional experiments to extend our findings. The replication data and the new experiments are reported in the revised manuscript.</p><p>Replication: we collected data from 15 additional subjects using normal faces and Mooney faces. The results were consistent with previous ones with enhanced statistical power (see Results).</p><p>Extension 1: To further study the role of internal (eyes, nose, mouth) versus external (hair, chin, face outline) face features, we presented distorted face images (explicit internal facial features available but spatially misarranged without changing face contour) to subjects and analyzed data as before. Consistent with our hypothesis, the clear face components (even though misarranged) evoked strong responses in rOFA, without clear evidence of a late signal corresponding to prediction error, indicating that spatial configuration of internal face features was not a prominent part of the prediction error from rFFA to rOFA. In this case, the processing sequence for the distorted faces would be similar to that elicited by normal face.</p><p>Extension 2: In a new experiment, we also investigated the role of context in face processing by presenting three types of stimuli to subjects: (i) images of highly degraded faces with contextual body cues which imply the presence of faces, (ii) images of degraded faces and body cues arranged in an incorrect configuration and thus do not imply the presence of faces, (iii) images of objects. Results showed that rOFA, rpFFA and raFFA are activated almost simultaneously at a late stage, implying a parallel contextual modulation of the core faceprocessing network. This result further emphasize the importance of internal face features in driving the sequential OFA to FFA processing, and help our understanding of the dynamics of contextual modulation in face perception.</p><disp-quote content-type="editor-comment"><p>5) The reported effects pass statistical significance but not by a large margin. Moreover, there can be concerns that MEG data varies considerably across participants and can lead to heterogeneity of variance, especially across time points. Shuffling of the data with randomized labels would provide a more rigorous approach to statistical analysis.</p></disp-quote><p>As described in #4 above, we collected data from additional 15 subjects for the Mooney face experiment (normal unfamiliar faces vs. Mooney faces). Combined with previous data, nonparametric permutation tests were performed to check the significance level of observed time difference between rOFA and rpFFA. The results are consistent with previous ones with enhanced statistical power (see Results).</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>[…] The results are clear-cut and the paper is in general well-written. I believe the present study, if in the end published, would be of interests to a broad readership including psychologists and neuroscientists. I only have a few comments that I wish the authors to address:</p><p>1) While recognizing faces from Mooney images would certainly rely heavily on top-down mechanisms, it is hard to rule out the involvement of top-down mechanisms when processing normal face pictures. Intuitively, for example, processing familiar faces would involve more top-down experience driven activity than processing unfamiliar faces. However, the present results seem to suggest no significant differences between processing famous and unfamiliar faces. How come?</p></disp-quote><p>This is a very valid point. This comment helped us to clarify that the difference between processing Mooney images and normal faces are not absolute. While the top-down mechanisms are more dominant in the case of Mooney faces, it is certainly also involved, but to a less degree, in the processing of normal faces. With regard to the processing of familiar vs. unfamiliar faces, our data show that there was little difference between them. It is likely that familiarity plays a more important role in the more anterior and medial regions of the temporal cortex. We clarified our writings and discussed this issue in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>2) The Discussion somewhat overlooks effects potentially driven by different tasks. As far as I understand, subjects performed different tasks for the Mooney face experiment and normal face versus object picture experiments.</p></disp-quote><p>We thank the reviewer for pointing this out. Yes, category task (face or not) was used in normal (familiar or unfamiliar) faces vs objects experiment, and one-back task was used in normal unfamiliar faces vs Mooney faces experiment. We had the opportunity to check the effects of task using the unfamiliar faces, since the same stimuli were used in the category task and the one-back task. Results show that there was no significant task effect in the timing of activation of the core face areas. We added more description about the different tasks used in the Materials and methods section and also added some discussion in the Discussion section.</p><disp-quote content-type="editor-comment"><p>3) Given studies on the functional role of left FFA (e.g., Meng et al., 2012; Bi et al., 2014; Goold and Meng, 2017), I would be greatly interested in Results and Discussions regarding what the present data could reveal about dynamic relations between the left and right face processing core networks.</p></disp-quote><p>We agree that the dynamic relations between the left and right face networks are interesting. Our results include data from both left and right face networks, though it was not feasible to further separate the left FFA into the anterior and posterior regions. We have added more discussion about the differences between left and right face processing core networks.</p><disp-quote content-type="editor-comment"><p>4) Some justification would be helpful for using sliding time windows of 50 ms. One possibility is to add power spectrum analysis. In any cases, power spectrum analysis might be helpful for revealing further fine-scale temporal dynamics of brain responses.</p></disp-quote><p>The 50 ms time window was selected based on previous study (Ashrafulla et al., 2013), which is a compromise in balancing the temporal precision and reliability of causality analysis. In other words, there is a trade-off between temporal resolution (shorter is better) and accuracy of model fit (longer is better) when considering the size of time window. In addition, we did not consider shorter time window because activity/power drops quickly beyond Β-band based on the power spectrum (see Materials and methods).</p><fig id="respfig2"><label>Author response image 2.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48764-resp-fig2-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>[…] The findings are of some interest but there are some major concerns. First, the discussion of previous work is rather limited and does not cite many related studies that have characterized the timing of face processing in the FFA and OFA. It has been known for a while that the OFA responds at an earlier latency than the FFA (e.g., Liu et al., 2002), and that certain stimulus manipulations, such as face inversion and contrast reversal, lead to delayed responses to faces (Bentin et al., 1996; Rossion et al., 2000; Rossion et al., 2012). Previous fMRI work has shown that difficult to perceive Mooney faces can lead to response delays on the order of several seconds (McKeeff and Tong, 2007). More recent techniques have allowed research groups to provide more refined estimates of the timing of neural responses, such as the fusion of fMRI-MEG analyzed using representational similarity analysis (e.g., Cichy et al., 2014). Periodic visual stimulation has also been used to characterize the timing of neural responses obtained with EEG/MEG by several research groups (e.g., Rossion et al., 2012, 2014; Norcia et al., 2015), and this approach has been successfully applied to characterize top-down effects of feedback during face processing (e.g., Baldauf and Desimone, 2014). The empirical and conceptual advances made in the current study need to be more clearly articulated with respect to previous work, and a clear argument for the specific contributions of this study is needed.</p></disp-quote><p>We appreciate and agree with this suggestion. The dynamics of face induced neural activation in FFA and OFA has been studied for a long time with various techniques. However, previous results are inconsistent and individually often lack either the spatial (e.g., sensor level EEG/MEG analysis) or temporal precision (e.g., fMRI data). Our results with combined fMRI and MEG measures, provide detailed and novel timing information of the core face network. For example, the relatively large temporal gap between the right anterior and posterior FFA was not reported in previous studies. Furthermore, our results showed that the temporal relationships between OFA and FFA are dependent on the internal facial features as well the context of visual input, which helps to understand how bottom-up and top-down processing together contribute to face perception.</p><p>Many previous studies used the N170/M170 component as the index of face processing in the ventral occipitotemporal cortex, however, the delayed N170/M170 response caused by certain stimulus manipulations (eg: face inversion, Mooney transformation) represents a relatively crude measure of face processing because the difficulty in attributing the sources of the delay. On the other hand, fMRI measures alone showing delayed FFA response to Mooney faces that was initially not recognized as faces simply reflect the time it took subjects to recognize difficult Mooney faces, rather than the real-time dynamics of Mooney face processing. In contrast, our results showed that when the face features were confounded with other shadows, the top-down rpFFA to rOFA projection became more dominated.</p><p>In the revised manuscript, we discussed the different techniques used to investigate the timing of face responses and the top-down modulation in face processing reported in previous studies (Discussion section).</p><disp-quote content-type="editor-comment"><p>Another concern is that the conclusions of the study rest on the data from a single experiment, and further investigation of the putative effects of top-down feedback and predictive coding are not provided. Reproducibility is a serious concern in many fields of science, especially psychology and also neuroscience. A follow-up experiment that both replicates and extends the current findings would help strengthen the study. The reported effects pass statistical significance but not by a large margin. Moreover, there can be concerns that MEG data varies considerably across participants and can lead to heterogeneity of variance, especially across time points. Shuffling of the data with randomized labels would provide a more rigorous approach to statistical analysis.</p></disp-quote><p>We thank the editor and reviewer for pushing us to perform a follow-up experiment. We did not just one but three follow-up experiments (one replication and two extensions), which indeed replicated and significantly extended the findings reported in the original version.</p><p>We collected more data for Experiment 2 (normal unfamiliar face vs Mooney face) to confirm the previous results and performed two additional experiments to extend our findings.</p><p>The replication data and the new experiments are reported in the revised manuscript.</p><p>Replication: we collected data from 15 additional subjects using normal faces and Mooney faces. The results were consistent with previous ones with enhanced statistical power (see Results).</p><p>Extension 1: To further study the role of internal (eyes, nose, mouth) versus external (hair, chin, face outline) face features, we presented distorted face images (explicit internal facial features available but spatially misarranged without changing face contour) to subjects and analyzed data as before. Consistent with our hypothesis, the clear face components (even though misarranged) evoked strong resonses in rOFA, without clear evidence of a late signal corresponding to prediction error, indicating that spatial configuration of internal face features was not a prominent part of the prediction error from rFFA to rOFA. In this case, the processing sequence for the distorted faces would be similar to that elicited by normal face.</p><p>Extension 2: In a new experiment, we also investigated the role of context in face processing by presenting three types of stimuli to subjects: (i) images of highly degraded faces with contextual body cues which imply the presence of faces, (ii) images of degraded faces and body cues arranged in an incorrect configuration and thus do not imply the presence of faces, (iii) images of objects. Results showed that rOFA, rpFFA and raFFA are activated almost simultaneously at a late stage, implying a parallel contextual modulation of the core faceprocessing network. This result further emphasize the importance of internal face features in driving the sequential OFA to FFA processing, and help our understanding of the dynamics of contextual modulation in face perception.</p><p>As described in #4 above, we collected data from additional 15 subjects for the Mooney face experiment (normal unfamiliar faces vs. Mooney faces). Combined with previous data, nonparametric permutation tests were performed to check the significance level of observed time difference between rOFA and rpFFA. The results are consistent with previous ones with enhanced statistical power (see Results).</p><disp-quote content-type="editor-comment"><p>Reviewer #4:</p><p>[…] A point that is lacking significantly is the role of pSTS. We know pSTS is mostly involved in the analysis of facial muscle articulations (also called action units, AUs) and the interpretation of facial expressions and emotion, see Srinivasan et al., 2016, and Martinez, 2017. Also relevant is the role of low-level image features (Weibert et al., 2018), which is also missing from the Discussion; and, the role of color perception (Yip and Sinha, 2002; Benitez-Quiroz et al., 2018).</p></disp-quote><p>The temporal responses of bilateral pSTS are broader (multi-peaked) and showed lower signal-to-noise than the ventral face-selective areas (Figure 2 and Figure 2—figure supplement 1). To increase our confidence about the pSTS time course, we analyzed the temporal responses of bilateral pSTS evoked by normal faces based on the additional data (Experiment 2), and the time courses basically remained the same as the previous ones (regardless of the task and face familiarity). We have added more discussion about the role of pSTS and its dynamics, especially in relation to the processing of facial expression, muscle articulations and motion.</p><p>We also thank the reviewer for reminding us about the role of low-level features including color, and have added more discussion about their role in face processing.</p><disp-quote content-type="editor-comment"><p>Another point that needs further discussion is the role of internal versus external face features (Sinha et al., 2006), and context (Sinha, Science 2004; Martinez, 2019).</p><p>These discussions are essential to frame the results of the present paper within existing models of face perception. With appropriate changes, this could be a strong paper.</p></disp-quote><p>We agree that it is important to understand the role of internal versus external face features. Since we were going to obtain more experimental data during the revision, we made the efforts to performed additional MEG experiments to specifically investigate the role of internal versus external face features and context (see response to editor’s #4). We have also added more discussion about them.</p></body></sub-article></article>