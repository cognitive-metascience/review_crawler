<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">30387</article-id><article-id pub-id-type="doi">10.7554/eLife.30387</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Short Report</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Frontal cortex selects representations of the talker’s mouth to aid in speech perception</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-92136"><name><surname>Ozker</surname><given-names>Muge</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7472-4528</contrib-id><email>mozker@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-93527"><name><surname>Yoshor</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-33719"><name><surname>Beauchamp</surname><given-names>Michael S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7599-9934</contrib-id><email>michael.beauchamp@bcm.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Neurosurgery</institution><institution>Baylor College of Medicine</institution><addr-line><named-content content-type="city">Texas</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Michael E. DeBakey Veterans Affairs Medical Center</institution><addr-line><named-content content-type="city">Texas</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-52568"><name><surname>Pasternak</surname><given-names>Tatiana</given-names></name><role>Reviewing Editor</role><aff id="aff3"><institution>University of Rochester</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>27</day><month>02</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e30387</elocation-id><history><date date-type="received" iso-8601-date="2017-07-13"><day>13</day><month>07</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-02-18"><day>18</day><month>02</month><year>2018</year></date></history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-30387-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.30387.001</object-id><p>Human faces contain multiple sources of information. During speech perception, visual information from the talker’s mouth is integrated with auditory information from the talker's voice. By directly recording neural responses from small populations of neurons in patients implanted with subdural electrodes, we found enhanced visual cortex responses to speech when auditory speech was absent (rendering visual speech especially relevant). Receptive field mapping demonstrated that this enhancement was specific to regions of the visual cortex with retinotopic representations of the mouth of the talker. Connectivity between frontal cortex and other brain regions was measured with trial-by-trial power correlations. Strong connectivity was observed between frontal cortex and mouth regions of visual cortex; connectivity was weaker between frontal cortex and non-mouth regions of visual cortex or auditory cortex. These results suggest that top-down selection of visual information from the talker’s mouth by frontal cortex plays an important role in audiovisual speech perception.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>audiovisual speech perception</kwd><kwd>electrocorticography</kwd><kwd>visual cortex</kwd><kwd>frontal cortex</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Veterans Administration Clinical Science Research and Development</institution></institution-wrap></funding-source><award-id>Merit Award Number 1I01CX000325</award-id><principal-award-recipient><name><surname>Yoshor</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS065395</award-id><principal-award-recipient><name><surname>Beauchamp</surname><given-names>Michael S</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U01NS098976</award-id><principal-award-recipient><name><surname>Beauchamp</surname><given-names>Michael S</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>During speech perception, if auditory speech is not informative the frontal cortex will enhance responses in visual regions that represent the mouth of the talker to improve perception.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Speech perception is multisensory: humans combine visual information from the talker’s face with auditory information from the talker’s voice to aid in perception. The contribution of visual information to speech perception is influenced by two factors. First, if the auditory information is noisy or absent, visual speech is more important than if the auditory speech is clear. Current models of speech perception assume that top-down processes serve to incorporate this factor into multisensory speech perception. For instance, visual cortex shows enhanced responses to audiovisual speech containing a noisy or entirely absent auditory component (<xref ref-type="bibr" rid="bib34">Schepers et al., 2015</xref>) raising an obvious question: since visual cortex presumably cannot assess the quality of auditory speech on its own, what is the origin of the top-down modulation that enhances visual speech processing ? Neuroimaging studies have shown that speech reading (perception of visual-only speech) leads to strong responses in frontal regions including the inferior frontal gyrus, premotor cortex, frontal eye fields and dorsolateral prefrontal cortex (<xref ref-type="bibr" rid="bib3">Callan et al., 2014</xref>; <xref ref-type="bibr" rid="bib4">Calvert and Campbell, 2003</xref>; <xref ref-type="bibr" rid="bib18">Hall et al., 2005</xref>; <xref ref-type="bibr" rid="bib24">Lee and Noppeney, 2011</xref>; <xref ref-type="bibr" rid="bib29">Okada and Hickok, 2009</xref>). We predicted that these frontal regions serve as the source of a control signal, which enhances activity in visual cortex when auditory speech is noisy or absent.</p><p>Second, visual information about the content of speech is not distributed equally throughout the visual field. Some regions of the talker’s face are more informative about speech content, with the mouth of the talker carrying the most information (<xref ref-type="bibr" rid="bib35">Vatikiotis-Bateson et al., 1998</xref>; <xref ref-type="bibr" rid="bib36">Yi et al., 2013</xref>). If frontal cortex enhances visual responses during audiovisual speech perception, one should expect this enhancement to occur preferentially in regions of visual cortex which represent the mouth of the talker’s face.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Responses to three types of speech—audiovisual (<italic>AV</italic>), visual-only (<italic>Vis)</italic> and auditory-only (<italic>Aud)</italic> (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) —were measured in 73 electrodes implanted over visual cortex (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Neural activity was assessed using the broadband response amplitude (<xref ref-type="bibr" rid="bib11">Crone et al., 2001</xref>; <xref ref-type="bibr" rid="bib5">Canolty et al., 2007</xref>; <xref ref-type="bibr" rid="bib14">Flinker et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Mesgarani et al., 2014</xref>). The response to <italic>AV</italic> and <italic>Vis</italic> was identical early in the trial, rising quickly after stimulus onset and peaking at about 160% of baseline (<xref ref-type="fig" rid="fig1">Figure 1C</xref>); there was no response to <italic>Aud</italic> speech. The talker’s mouth began moving at 200 ms after stimulus onset, with the onset of auditory speech in the <italic>AV</italic> condition occurring at 283 ms. Shortly thereafter (at 400 ms) the responses to <italic>AV</italic> and <italic>Vis</italic> diverged, with significantly greater responses to <italic>Vis</italic> than <italic>AV</italic>. Examining individual electrodes revealed large variability in the amount of <italic>Vis</italic> enhancement, ranging from −16 to 78% (<xref ref-type="fig" rid="fig1">Figure 1D</xref>).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.30387.002</object-id><label>Figure 1.</label><caption><title>Visual cortex responses to speech.</title><p>(<bold>A</bold>) The speech stimuli consisted of audiovisual recordings of a female talker speaking words (<italic>AV</italic>) edited so that only the visual portion of the recording was presented (<italic>Vis</italic>) or only the auditory portion of the recording was presented (<italic>Aud</italic>). Subjects were instructed to fixate the talker’s mouth (<italic>AV</italic> and <italic>Vis</italic> conditions) or a fixation point presented at the same screen location as the talker’s mouth (<italic>Aud</italic> condition). (<bold>B</bold>) In eight subjects, a total of 73 responsive occipital electrodes were identified. Electrode locations (black circles) are shown on a posterior view (top) and a medial view (bottom) of the left hemisphere of a template brain. (<bold>C</bold>) Broadband responses (70–150 Hz) to <italic>AV</italic> (solid line), <italic>Vis</italic> (dashed line) and <italic>Aud</italic> (dotted line) speech averaged across electrodes. The red arrow indicates the first time point (400 ms) at which the response to <italic>Vis</italic> speech was significantly greater than the response to <italic>AV</italic> speech. On the x-axis, time zero corresponds to the onset of the video (V) followed by the onset of the talker’s mouth movements (M) at 200 ms and onset of auditory speech (<bold>A</bold>) at 283 ms (orange arrows). (<bold>D</bold>) The broadband response enhancement (<italic>Vis – AV</italic>) measured from 200 to 1500 ms. One symbol per electrode (symbols jittered along x-axis for improved visibility).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-30387-fig1-v1"/></fig><p>Enhanced visual cortex responses to <italic>Vis</italic> speech may reflect the increased importance of visual speech for comprehension when auditory speech is not available. However, visual speech information is not evenly distributed across the face: the mouth of the talker contains most of the information about speech content. We predicted that mouth representations in visual cortex should show greater enhancement than non-mouth representations. To test this prediction, we measured the receptive field of the neurons underlying each electrode by presenting small checkerboards at different visual field locations and determining the location with the maximum evoked response (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). As expected, electrodes located near the occipital pole had receptive fields near the center of the visual field, while electrodes located more anteriorly along the calcarine sulcus had receptive fields in the visual periphery (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). In the speech stimulus, the mouth subtended approximately 5 degrees of visual angle. Electrodes with receptive fields of less than five degrees eccentricity were assumed to carry information about the mouth region of the talker’s face and were classified as ‘mouth electrodes’ (<italic>N</italic> = 49). Electrodes with more peripheral receptive fields were classified as ‘non-mouth electrodes’ (<italic>N</italic> = 24). We compared the response in each group of electrodes using a linear mixed-effects (LME) model with the response amplitude as the dependent measure; the electrode type (mouth or non-mouth) and stimulus condition (<italic>AV</italic>, <italic>Vis</italic> or <italic>Aud</italic>) as fixed factors; electrode as a random factor; and the response in mouth electrodes to <italic>AV</italic> speech as the baseline (see <xref ref-type="table" rid="table1">Table 1</xref> for all LME results).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.30387.003</object-id><label>Figure 2.</label><caption><title>Retinotopic organization of speech responses in the visual cortex.</title><p>(<bold>A</bold>) Medial view of a cortical surface model of the left hemisphere brain of a single subject (anonymized subject ID YAI). Posterior electrode e81 (red circle) was located superior to the calcarine sulcus on the occipital pole (red circle) while anterior electrode e65 (blue circle) was located inferior to the calcarine on the medial wall of the hemisphere. The receptive field mapping stimulus consisted of a small checkerboard presented at random screen locations while subjects performed a letter detection task at fixation (not shown). (<bold>B</bold>) The responses evoked by the mapping stimulus in electrodes e81 (left panel) and e65 (right panel). Color scales corresponds to the amplitude of the visual evoked response at each location in the visual field, with the crosshairs showing the center of the visual field and the red and blue circles showing the center of a two-dimensional Gaussian fitted to the response. Electrode e81 had a central receptive field (eccentricity at RF center of 2.5°) while electrode e65 had a peripheral receptive field (eccentricity 10.9°). (<bold>C</bold>) The receptive field location for the two sample electrodes, e81 (red circle) and e65 (blue circle) are shown on the speech stimulus. Subjects were instructed to fixate the talker’s mouth and electrodes were classified as representing the mouth region of the talker’s face (red circles; less than 5° from the center of the mouth, white dashed line) or as non-mouth (blue circles;&gt;5°). Electrode locations of mouth (red) and non-mouth (blue) electrodes shown on posterior and medial brain views. Response enhancement (<italic>Vis-AV</italic>) for each individual electrode; inset bar graph shows mean values (±standard error). (<bold>D</bold>) In a control speech experiment, a white fixation crosshair was presented on the talker’s shoulder, moving the mouth of the talker’s face to the visual periphery. Electrodes were classified as representing the mouth of the talker (red circles; less than 5° from the the center of the mouth, white dashed line) or non-mouth (blue circles;&gt;5°). Mouth electrodes were in the periphery of the visual field (mean eccentricity of 9.5°). Response enhancement (<italic>Vis-AV</italic>) for each individual electrode; inset bar graph shows mean values (±standard error).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-30387-fig2-v1"/></fig><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.30387.004</object-id><label>Table 1.</label><caption><title>LME for Amplitude in Visual Cortex</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Fixed effects:</th><th valign="top">Estimate</th><th valign="top">Std. error</th><th valign="top">DF</th><th valign="top">t-value</th><th valign="top">p-value</th></tr></thead><tbody><tr><td valign="top">Baseline</td><td valign="top">98</td><td valign="top">8.7</td><td valign="top">90.5</td><td valign="top">11.2</td><td valign="top">10<sup>−16</sup></td></tr><tr><td valign="top">A Speech</td><td valign="top">−83</td><td valign="top">5.9</td><td valign="top">134.5</td><td valign="top">−14.2</td><td valign="top">10<sup>−16</sup></td></tr><tr><td valign="top">A Speech x Peripheral RF</td><td valign="top">47</td><td valign="top">9.7</td><td valign="top">133.5</td><td valign="top">4.9</td><td valign="top">10<sup>−6</sup></td></tr><tr><td valign="top">V Speech</td><td valign="top">26</td><td valign="top">5.4</td><td valign="top">132.9</td><td valign="top">4.8</td><td valign="top">10<sup>−6</sup></td></tr><tr><td valign="top">Peripheral RF</td><td valign="top">−65</td><td valign="top">15.2</td><td valign="top">90.5</td><td valign="top">−4.3</td><td valign="top">10<sup>−5</sup></td></tr><tr><td valign="top">V Speech x Peripheral RF</td><td valign="top">−24</td><td valign="top">9.4</td><td valign="top">132.9</td><td valign="top">−2.6</td><td valign="top">0.01</td></tr></tbody></table></table-wrap><p>The model found a significant main effect of stimulus condition, driven by greater responses to <italic>Vis</italic> (95% <italic>vs.</italic> 77%, <italic>Vis vs. AV</italic>; p=10<sup>−6</sup>) and weaker responses to <italic>Aud</italic> (12% <italic>vs.</italic> 77%, <italic>Aud</italic> vs. <italic>AV</italic>; p=10<sup>−16</sup>). Critically, there was also a significant interaction between stimulus condition and electrode type. Mouth electrodes showed a large difference between the responses to <italic>Vis</italic> and <italic>AV</italic> speech while non-mouth electrodes showed almost no difference (26% <italic>vs.</italic> 2%, mouth <italic>vs.</italic> non-mouth <italic>Vis – AV</italic>; p=0.01; <xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><p>An alternative interpretation of this results is raised by the task design, in which subjects were instructed to fixate the mouth of the talker. Therefore, the enhancement could be specific to electrodes with receptive fields near the center of gaze (central <italic>vs.</italic> peripheral) rather than to the mouth of the talker’s face (mouth <italic>vs.</italic> non-mouth).</p><p>To distinguish these two possibilities, we performed a control experiment in which the task instructions were to fixate the shoulder of the talker, rather than the mouth, aided by a fixation crosshair superimposed on the talker’s shoulder (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). With this manipulation, the center of the mouth was located at 10 degrees eccentricity. Electrodes with receptive field centers near the mouth were classified as mouth electrodes (<italic>N</italic> = 19); otherwise, they were classified as non-mouth electrodes (<italic>N</italic> = 13). This dissociated visual field location from mouth location: mouth electrodes had receptive fields located peripherally (mean eccentricity = 9.5 degrees). If the enhancement during <italic>Vis</italic> speech was restricted to the center of gaze, we would predict no enhancement for mouth electrodes in this control experiment due to their peripheral receptive fields. Contrary to this prediction, the LME showed a significant interaction between electrode type and stimulus condition (<xref ref-type="table" rid="table2">Table 2</xref>). Mouth electrodes (all with peripheral receptive fields) showed a large difference between the responses to <italic>Vis</italic> and <italic>AV</italic> speech while non-mouth electrodes showed little difference (50% <italic>vs.</italic> 9%, mouth <italic>vs.</italic> non-mouth <italic>Vis – AV</italic>; p=10<sup>−3</sup>).</p><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.30387.005</object-id><label>Table 2.</label><caption><title>LME for Amplitude in Visual Cortex (Control Experiment)</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Fixed effects:</th><th valign="top">Estimate</th><th valign="top">Std. error</th><th valign="top">DF</th><th valign="top">t-value</th><th valign="top">p-value</th></tr></thead><tbody><tr><td valign="top">Baseline</td><td valign="top">357</td><td valign="top">35</td><td valign="top">32.7</td><td valign="top">10.1</td><td valign="top">10<sup>−11</sup></td></tr><tr><td valign="top">V Speech</td><td valign="top">50</td><td valign="top">7</td><td valign="top">32</td><td valign="top">6.7</td><td valign="top">10<sup>−7</sup></td></tr><tr><td valign="top">Non-mouth RF</td><td valign="top">−287</td><td valign="top">55</td><td valign="top">32.7</td><td valign="top">-5</td><td valign="top">10<sup>−5</sup></td></tr><tr><td valign="top">V Speech x Non-mouth RF</td><td valign="top">−41</td><td valign="top">12</td><td valign="top">32</td><td valign="top">−3.5</td><td valign="top">10<sup>−3</sup></td></tr></tbody></table></table-wrap><p>Both experiments demonstrated a large enhancement for <italic>Vis</italic> compared with <italic>AV</italic> speech in visual cortex electrodes representing the mouth of the talker. However, the visual stimulus in the <italic>Vis</italic> and <italic>AV</italic> conditions was identical. Therefore, a control signal sensitive to the absence of auditory speech must trigger the enhanced visual responses. We investigated responses in frontal cortex as a possible source of top-down control signals (<xref ref-type="bibr" rid="bib9">Corbetta and Shulman, 2002</xref>; <xref ref-type="bibr" rid="bib17">Gunduz et al., 2011</xref>; <xref ref-type="bibr" rid="bib22">Kastner and Ungerleider, 2000</xref>; <xref ref-type="bibr" rid="bib26">Miller and Buschman, 2013</xref>).</p><p><xref ref-type="fig" rid="fig3">Figure 3A</xref> shows the responses during <italic>Vis</italic> speech for a frontal electrode located at the intersection of the precentral gyrus and middle frontal gyrus (Talairach co-ordinates: x = −56, y = −8, z = 33) and a visual electrode located on the occipital pole with a receptive field located in the mouth region of the talker’s face (1.8° eccentricity). Functional connectivity was measured with a Spearman rank correlation of the trial-by-trial response power within each pair. This pair of electrodes showed strong correlation (ρ = 0.57, p=10<sup>−6</sup>): on trials in which the frontal electrode responded strongly, the visual electrode did as well.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.30387.006</object-id><label>Figure 3.</label><caption><title>Functional connectivity with the frontal cortex.</title><p>(<bold>A</bold>) Average broadband responses for each <italic>Vis</italic> speech trial (200–1500 ms, 70–150 Hz) measured simultaneously in a single frontal (green) and a single visual cortex (red) electrode. Trials were ranked based on their response amplitudes (y axis) and shown with respect to their presentation orders (x axis). The Spearman rank correlation between the amplitude time series of the two electrodes was high (ρ = 0.57). (<bold>B</bold>) Left hemisphere of a template brain showing all frontal (green circles), visual cortex mouth (red) and non-mouth (blue) electrodes. Frontal electrodes were located near the precentral sulcus (dashed line). Plot shows Spearman rank correlation between each frontal-visual electrode pair; inset bar graph shows mean values (±standard error).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-30387-fig3-v1"/></fig><p>We predicted that connectivity should be greater for mouth electrodes than non-mouth electrodes. In each subject, we selected the single frontal electrode with the strongest response to <italic>AV</italic> speech (<xref ref-type="fig" rid="fig3">Figure 3B</xref>; average Talairach co-ordinates of the frontal electrode: x = 49, y = −2, z = 42) and measured the connectivity between the frontal electrode and all visual electrodes in that subject (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><p>An LME model (<xref ref-type="table" rid="table3">Table 3</xref>) was fit with the Spearman rank correlation (ρ) between each frontal-visual electrode pair as the dependent measure; the visual electrode type (mouth <italic>vs.</italic> non-mouth) and stimulus condition (<italic>AV</italic>, <italic>Vis</italic> or <italic>Aud</italic>) as fixed factors; electrode as a random factor; and connectivity of mouth electrodes during <italic>AV</italic> speech as the baseline. The largest effect in the model was a main effect of electrode type, driven by strong frontal-visual connectivity in mouth electrodes and weak connectivity in non-mouth electrodes (0.2 <italic>vs.</italic> −0.02; p=10<sup>−5</sup>). There was significantly weaker frontal-visual connectivity during the <italic>Aud</italic> condition in which no visual speech was presented (p=0.01).</p><table-wrap id="table3" position="float"><object-id pub-id-type="doi">10.7554/eLife.30387.007</object-id><label>Table 3.</label><caption><title>LME for Frontal-Visual Cortex Connectivity</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Fixed effects:</th><th valign="top">Estimate</th><th valign="top">Std. error</th><th valign="top">DF</th><th valign="top">t-value</th><th valign="top">p-value</th></tr></thead><tbody><tr><td valign="top">Baseline</td><td valign="top">0.23</td><td valign="top">0.03</td><td valign="top">179.1</td><td valign="top">7.4</td><td valign="top">10<sup>−12</sup></td></tr><tr><td valign="top">Peripheral RF</td><td valign="top">−0.24</td><td valign="top">0.05</td><td valign="top">179.1</td><td valign="top">−4.5</td><td valign="top">10<sup>−5</sup></td></tr><tr><td valign="top">A Speech</td><td valign="top">−0.1</td><td valign="top">0.04</td><td valign="top">141.1</td><td valign="top">−2.5</td><td valign="top">0.01</td></tr><tr><td valign="top">A Speech x Peripheral RF</td><td valign="top">0.16</td><td valign="top">0.07</td><td valign="top">136.5</td><td valign="top">2.4</td><td valign="top">0.02</td></tr><tr><td valign="top">V Speech x Peripheral RF</td><td valign="top">−0.1</td><td valign="top">0.06</td><td valign="top">134</td><td valign="top">−1.6</td><td valign="top">0.1</td></tr><tr><td valign="top">V Speech</td><td valign="top">0.0003</td><td valign="top">0.04</td><td valign="top">134</td><td valign="top">−0.007</td><td valign="top">1</td></tr></tbody></table></table-wrap><p>Our results suggest a model in which frontal cortex modulates mouth regions of visual cortex in a task-specific fashion. Since top-down modulation is an active process that occurs when visual speech is most relevant, this model predicts that frontal cortex should be more active during the perception of <italic>Vis</italic> speech. To test this prediction, we compared the response to the different speech conditions in frontal cortex (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Frontal electrodes responded strongly to all three speech conditions, with a peak amplitude of 109% at 470 ms after stimulus onset. Following the onset of auditory speech (283 ms after stimulus onset), the responses diverged, with a larger response to <italic>Vis</italic> than <italic>AV</italic> or <italic>Aud</italic> speech (53% <italic>vs.</italic> 33%, <italic>Vis vs. AV</italic>; p=0.001; Table 6). Greater responses to <italic>Vis</italic> speech were consistent across frontal electrodes (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.30387.008</object-id><label>Figure 4.</label><caption><title>Frontal cortex responses.</title><p>(<bold>A</bold>) Broadband responses to AV (solid line), Vis (dashed line) and Aud (dotted line) speech averaged across frontal electrodes (locations shown on inset brain). (<bold>B</bold>) Scatter plot showing average responses (from 200 to 1500 ms) to Vis and AV speech for all frontal electrodes (green circles). Black diagonal line indicates the line of equality. (<bold>C</bold>) Broadband responses to AV (solid line) and Vis (dashed line) speech in a single frontal (green trace) and visual cortex (red trace) electrode. Dashed black lines depicts the onset of mouth movements and the onset of auditory speech (note expanded scale on x-axis compared with A). Vertical green line indicates the first time point at which the frontal response to Vis speech was enhanced relative to AV speech. Red line indicates the same for visual cortex. Black arrow highlights latency difference. (<bold>D</bold>) Enhancement latency for all frontal (green circles) and visual (red circles) electrodes. Letter codes along y-axis show different subjects. Vertical green and red lines indicate the average enhancement latency across all frontal and visual electrodes respectively.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-30387-fig4-v1"/></fig><p>Both frontal and visual cortex showed enhanced responses to <italic>Vis</italic> speech. If frontal cortex was the source of top-down modulation that resulted in visual cortex enhancements, one might expect frontal response differences to <italic>precede</italic> visual cortex response differences. To test this idea, we examined the time courses of the response to speech in a sample frontal-visual electrode pair (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). The increased response to <italic>Vis</italic> speech occurred <italic>earlier</italic> in the frontal electrode, beginning at 247 ms after auditory onset versus 457 ms for the visual electrode. To test whether the earlier divergence between <italic>Vis</italic> and <italic>AV</italic> speech in frontal compared with visual cortex was consistent across electrodes, for each electrode we calculated the first time point at which the <italic>Vis</italic> response was significantly larger than the <italic>AV</italic> response (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). 27 out of 29 visual electrodes showed <italic>Vis</italic> response enhancement after their corresponding frontal electrode, with an average latency difference of 230 ms (frontal <italic>vs.</italic> visual: 257 ± 150 ms <italic>vs.</italic> 487 ± 110 ms <italic>vs.</italic> with respect to auditory speech onset, t<sub>34</sub> = 4, p=10<sup>−4</sup>).</p><p>While our primary focus was on frontal modulation of visual cortex, auditory cortex was analyzed for comparison. 44 electrodes located on the superior temporal gyrus responded to <italic>AV</italic> speech (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). As expected, these electrodes showed little response to <italic>Vis</italic> speech but a strong response to <italic>Aud</italic> speech, with a maximal response amplitude of 148% at 67 ms after auditory speech onset. While visual cortex showed a greater response to <italic>Vis</italic> speech than to <italic>AV</italic> speech, there was no significant difference in the response of auditory cortex to <italic>Aud</italic> and <italic>AV</italic> speech, as confirmed by the LME model (p=0.8, <xref ref-type="table" rid="table4">Table 4</xref>). Next, we analyzed the connectivity between frontal cortex and auditory electrodes. The LME revealed a significant effect of stimulus, driven by weaker connectivity in the <italic>Vis</italic> condition in which no auditory speech was presented (p=0.02; <xref ref-type="table" rid="table5">Table 5</xref>). Overall, the connectivity between frontal cortex and auditory cortex was weaker than the connectivity between frontal cortex and visual cortex (0.04 for frontal-auditory <italic>vs.</italic> 0.23 for frontal-visual mouth electrodes; p=0.001, unpaired t-test).</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.30387.009</object-id><label>Figure 5.</label><caption><title>Auditory cortex responses and connectivity.</title><p>(<bold>A</bold>) Broadband responses to <italic>AV</italic> (solid line), <italic>Vis</italic> (dashed line) and <italic>Aud</italic> (dotted line) speech across auditory electrodes located on the STG (purple circles on inset brain). (<bold>B</bold>) The broadband response enhancement (<italic>Aud – AV</italic>) with one symbol per electrode (symbols jittered along x-axis for improved visibility). Electrodes were divided into two groups, those showing a large (&gt;=10%) enhancement (orange circles) and those showing little or no enhancement (yellow circles). (<bold>C</bold>) Anatomical distribution of STG electrodes by <italic>Aud – AV</italic> amplitude. (<bold>D</bold>) Frontal connectivity of STG electrodes by <italic>Aud – AV</italic> amplitude.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-30387-fig5-v1"/></fig><table-wrap id="table4" position="float"><object-id pub-id-type="doi">10.7554/eLife.30387.010</object-id><label>Table 4.</label><caption><title>LME for Amplitude in Auditory Cortex</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Fixed effects:</th><th valign="top">Estimate</th><th valign="top">Std. error</th><th valign="top">DF</th><th valign="top">t-value</th><th valign="top">p-value</th></tr></thead><tbody><tr><td valign="top">Baseline</td><td valign="top">41</td><td valign="top">4.9</td><td valign="top">88.3</td><td valign="top">8.4</td><td valign="top">10<sup>−12</sup></td></tr><tr><td valign="top">V Speech</td><td valign="top">−32</td><td valign="top">5</td><td valign="top">83.2</td><td valign="top">−6.4</td><td valign="top">10<sup>−8</sup></td></tr><tr><td valign="top">A Speech</td><td valign="top">1</td><td valign="top">5.1</td><td valign="top">84.1</td><td valign="top">0.2</td><td valign="top">0.8</td></tr></tbody></table></table-wrap><table-wrap id="table5" position="float"><object-id pub-id-type="doi">10.7554/eLife.30387.011</object-id><label>Table 5.</label><caption><title>LME for Frontal-Auditory Cortex Connectivity</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Fixed effects:</th><th valign="top">Estimate</th><th valign="top">Std. error</th><th valign="top">DF</th><th valign="top">t-value</th><th valign="top">p-value</th></tr></thead><tbody><tr><td valign="top">Baseline</td><td valign="top">0.04</td><td valign="top">0.04</td><td valign="top">83.6</td><td valign="top">1.1</td><td valign="top">0.3</td></tr><tr><td valign="top">V Speech</td><td valign="top">−0.1</td><td valign="top">0.04</td><td valign="top">83.6</td><td valign="top">−2.3</td><td valign="top">0.02</td></tr><tr><td valign="top">A Speech</td><td valign="top">0.02</td><td valign="top">0.04</td><td valign="top">84.4</td><td valign="top">0.4</td><td valign="top">0.7</td></tr></tbody></table></table-wrap><table-wrap id="table6" position="float"><object-id pub-id-type="doi">10.7554/eLife.30387.012</object-id><label>Table 6.</label><caption><title>LME for Amplitude in Frontal Cortex</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Fixed effects:</th><th valign="top">Estimate</th><th valign="top">Std. error</th><th valign="top">DF</th><th valign="top">t-value</th><th valign="top">p-value</th></tr></thead><tbody><tr><td valign="top">Baseline</td><td valign="top">33</td><td valign="top">6.7</td><td valign="top">12.1</td><td valign="top">5</td><td valign="top">10<sup>−4</sup></td></tr><tr><td valign="top">V Speech</td><td valign="top">20</td><td valign="top">5.2</td><td valign="top">14.1</td><td valign="top">3.9</td><td valign="top">10<sup>−3</sup></td></tr><tr><td valign="top">A Speech</td><td valign="top">-7</td><td valign="top">5.8</td><td valign="top">14.4</td><td valign="top">−1.1</td><td valign="top">0.3</td></tr></tbody></table></table-wrap><p>In visual cortex, a subset of electrodes (those representing the mouth) showed a greater response in the <italic>Vis-AV</italic> contrast and greater connectivity with frontal cortex. To determine if the same was true in auditory cortex, we selected the STG electrodes with the strongest response in the <italic>Aud-AV</italic> contrast (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Unlike in visual cortex, in which there was anatomical localization of mouth-representing electrodes on the occipital pole, auditory electrodes with the strongest response in the <italic>Aud-AV</italic> contrast did not show clear anatomical clustering (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Electrodes with the electrodes with the strongest response in the <italic>Aud-AV</italic> contrast had equivalent connectivity with frontal cortex as other STG electrodes (ρ = −0.04 <italic>vs. roh</italic> = 0.06, unpaired t-test = 0.9, p=0.3). This was the opposite of the pattern observed in visual cortex. In order to ensure that signal amplitude was not the main determinant of connectivity, we selected the STG electrodes with the highest signal-to-noise ratio in the AV condition. These electrodes had equivalent connectivity with frontal cortex as other STG electrodes (ρ = −0.002 <italic>vs</italic>. ρ = 0.05, unpaired t-test = 0.4, p=0.7).</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Taken together, these results support a model in which control regions of lateral frontal cortex located near the precentral sulcus selectively modulate visual cortex, enhancing activity with both spatial selectivity—only mouth regions of the face are enhanced —and context selectivity—enhancement is greater when visual speech is more important due to the absence of auditory speech.</p><p>Our results link two distinct strands of research. First, fMRI studies of speech perception frequently observe activity in frontal cortex, especially during perception of a visual-only speech, a task sometimes referred to as speech reading (<xref ref-type="bibr" rid="bib3">Callan et al., 2014</xref>; <xref ref-type="bibr" rid="bib4">Calvert and Campbell, 2003</xref>; <xref ref-type="bibr" rid="bib18">Hall et al., 2005</xref>; <xref ref-type="bibr" rid="bib24">Lee and Noppeney, 2011</xref>; <xref ref-type="bibr" rid="bib29">Okada and Hickok, 2009</xref>). However, the precise role of this frontal activity has been difficult to determine given the relatively slow time resolution of fMRI.</p><p>Second, it is well known that frontal regions in an around the frontal eye fields in the precentral sulcus modulate visual cortex activity during tasks that require voluntary control of spatial or featural attention (<xref ref-type="bibr" rid="bib9">Corbetta and Shulman, 2002</xref>; <xref ref-type="bibr" rid="bib16">Gregoriou et al., 2012</xref>; <xref ref-type="bibr" rid="bib17">Gunduz et al., 2011</xref>; <xref ref-type="bibr" rid="bib22">Kastner and Ungerleider, 2000</xref>; <xref ref-type="bibr" rid="bib26">Miller and Buschman, 2013</xref>; <xref ref-type="bibr" rid="bib31">Popov et al., 2017</xref>). However, it has not been clear how these attentional networks function during other important cognitive tasks, such as speech perception.</p><p>We suggest that frontal-visual attentional control circuits are automatically engaged during speech perception in the service of increasing perceptual accuracy for the processing of this very important class of stimuli. This allows for precise, time-varying control: as the quality of auditory information fluctuates as auditory noise in the environment increases or decreases, frontal cortex can up or down-regulate activity in visual cortex accordingly. It also allows for precise spatial control: as the mouth of the talker contains the most speech information, frontal cortex can selectively enhance visual cortex activity that is relevant for speech perception by enhancing activity in subregions of visual cortex that represent the talker’s mouth. A possible anatomical linkage supporting this processing is the inferior fronto-occipital fasciculus connecting frontal and occipital regions, found in human but not non-human primates (<xref ref-type="bibr" rid="bib6">Catani and Mesulam, 2008</xref>). Most models of speech perception focus on auditory cortex inputs into parietal and frontal cortex (<xref ref-type="bibr" rid="bib19">Hickok and Poeppel, 2004</xref>; <xref ref-type="bibr" rid="bib32">Rauschecker and Scott, 2009</xref>). Our findings suggest that visual cortex should also be considered an important component of the speech perception network, as it is selectively and rapidly modulated during audiovisual speech perception.</p><p>The analysis of auditory cortex responses provides an illuminating contrast with the visual cortex results. While removing auditory speech increased visual cortex response amplitudes and frontal-visual connectivity, removing visual speech did not change auditory cortex response amplitudes or connectivity. A simple explanation for this is that auditory speech is easily intelligible without visual speech, so that no attentional modulation is required. In contrast, perceiving visual-only speech requires speechreading, a difficult task that demands attentional engagement. An interesting test of this idea would be to present auditory speech with added noise. Under these circumstances, attentional engagement would be expected in order to enhance the intelligibility of the noisy auditory speech, with a neural manifestation of increased response amplitudes in auditory cortex and increased connectivity with frontal cortex. The interaction of stimulus and task also provides an explanation for the frontal activations in response to the speech stimuli. The human frontal eye fields show rapid sensory-driven responses, with latencies as short as 24 ms to auditory stimuli and 45 ms to visual stimuli (<xref ref-type="bibr" rid="bib23">Kirchner et al., 2009</xref>). Following initial sensory activation, task demands modulate frontal function, with demanding tasks such as processing visual-only or noisy auditory speech or resulting in enhanced activity (<xref ref-type="bibr" rid="bib7">Cheung et al., 2016</xref>; <xref ref-type="bibr" rid="bib8">Cope et al., 2017</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subject information</title><p>All experimental procedures were approved by the Institutional Review Board of Baylor College of Medicine. Eight human subjects provided written informed consent prior to participating in the research protocol. The subjects (5F, mean age 36, 6L hemisphere) suffered from refractory epilepsy and were implanted with subdural electrodes guided by clinical requirements. Following surgery, subjects were tested while resting comfortably in their hospital bed in the epilepsy monitoring unit.</p></sec><sec id="s4-2"><title>Experiment setup</title><p>Visual stimuli were presented on an LCD monitor (Viewsonic VP150, 1024 × 768 pixels) positioned at 57 cm distance from the subject, resulting in a display size of 30.5° x 22.9°.</p></sec><sec id="s4-3"><title>Receptive field mapping procedures</title><p>Mapping stimulus consisted of a square checkerboard pattern (3° x 3° size) briefly flashed (rate of 2 Hz and a duty cycle of 25%) in different positions on the display monitor to fill a grid over the region of interest in the visual field (63 positions, 7 × 9 grid). 12–30 trials for each position were recorded.</p><p>Subjects fixated at the center of the screen and performed a letter detection task to ensure that they were not fixating on the mapping stimulus. Different letters were randomly presented at the center of the screen (2° in size presented at a rate of 1–4 Hz) and subjects were required to press a mouse button whenever the letter ‘X’ appeared. The mean accuracy was 88 ± 14% with a false alarm rate of 8 ± 14% (mean across subjects ± SD; responses were not recorded for one subject).</p></sec><sec id="s4-4"><title>Speech experiment procedures</title><p>Four video clips of a female talker pronouncing the single syllable words ‘drive’, ‘known’, ‘last’ and ‘meant’ were presented under audiovisual (<italic>AV</italic>), visual (<italic>Vis</italic>) and auditory (<italic>Aud</italic>) conditions. Visual stimuli were presented using the same monitor used for receptive field mapping, with the face of the talker subtending approximately 13 degrees horizontally and 21 degrees vertically. Speech sounds were played through loudspeakers positioned next to the subject’s bed. The average duration of the video clips was ~1500 ms (drive: 1670 ms, known: 1300 ms, last: 1500 ms, meant: 1400 ms). In <italic>AV</italic> and <italic>Vis</italic> trials, mouth movements started at ~200 ms after the video onset on average (drive: 200 ms, known: 233 ms, last: 200 ms, meant: 200 ms). In <italic>AV</italic> trials, auditory vocalizations started at ~283 ms (drive: 267 ms, known: 233 ms, last: 300 ms, meant: 333 ms). Vocalization duration was ~480 ms on average (drive: 500 ms, known: 400 ms, last: 530 ms, meant: 500 ms).</p><p>The three different conditions were randomly intermixed, separated by interstimulus intervals of 2.5 s. 32–64 repetitions for each condition was presented. Subjects were instructed to fixate either the mouth of the talker (during <italic>Vis</italic> and <italic>AV</italic> trials) or a white fixation dot presented at the same location as the mouth of the talker on a gray background (during <italic>Aud</italic> trials and the interstimulus intervals). To ensure attention to the stimuli, subjects were instructed to press a mouse button 20% of trials in which a catch stimulus was presented, consisting of the AV word ‘PRESS’. The mean accuracy was 88 ± 18%, with a false alarm rate of 3 ± 6% (mean across subjects ± SD; for one subject, button presses were not recorded).</p></sec><sec id="s4-5"><title>Control speech experiment procedures</title><p>In a control experiment (one subject, 32 electrodes) identical procedures were used except that the subject fixated crosshairs placed on the shoulder of the talker (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p></sec><sec id="s4-6"><title>Electrode localization and recording</title><p>Before surgery, T1-weighted structural magnetic resonance imaging scans were used to create cortical surface models with FreeSurfer, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001847">SCR_001847</ext-link> (<xref ref-type="bibr" rid="bib12">Dale et al., 1999</xref>; <xref ref-type="bibr" rid="bib13">Fischl et al., 1999</xref>) and visualized using SUMA (<xref ref-type="bibr" rid="bib1">Argall et al., 2006</xref>) within the Analysis of Functional Neuroimages package, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_005927">SCR_005927</ext-link> (<xref ref-type="bibr" rid="bib10">Cox, 1996</xref>). Subjects underwent a whole-head CT after the electrode implantation surgery. The post-surgical CT scan and pre-surgical MR scan were aligned using and all electrode positions were marked manually on the structural MR images. Electrode positions were then projected to the nearest node on the cortical surface model using the AFNI program <italic>SurfaceMetrics</italic>. Resulting electrode positions on the cortical surface model were confirmed by comparing them with the photographs taken during the implantation surgery.</p><p>A 128-channel Cerebus amplifier (Blackrock Microsystems, Salt Lake City, UT) was used to record from subdural electrodes (Ad-Tech Corporation, Racine, WI) that consisted of platinum alloy discs embedded in a flexible silicon sheet. Two types of electrodes were implanted, containing an exposed surface of either 2.3 mm or 0.5 mm; an initial analysis did not suggest any difference in the responses recorded from the two types of electrodes, so they were grouped together for further analysis. An inactive intracranial electrode implanted facing the skull was used as a reference for recording. Signals were amplified, filtered (low-pass: 500 Hz, fourth-order Butterworth filter; high-pass: 0.3 Hz, first-order Butterworth) and digitized at 2 kHz. Data files were converted from Blackrock format to MATLAB 8.5.0 (MathWorks Inc. Natick, MA) and the continuous data stream was epoched into trials. All analyses were conducted separately for each electrode.</p></sec><sec id="s4-7"><title>Receptive field mapping analysis: Evoked potentials</title><p>The voltage signal in each trial (consisting of the presentation of a single checkerboard at a single spatial location) was filtered using a Savitzky-Golay polynomial filter (‘‘sgolayfilt’’ function in Matlab) with polynomial order set to five and frame size set to 11. If the raw voltage exceeded a threshold of 3 standard deviations from the mean voltage, suggesting noise or amplifier saturation, the trial was discarded; &lt;1 trial per electrode discarded on average. The filtered voltage response at each spatial location was averaged, first across trials and then across time-points (from 100 to 300 ms post-stimulus) resulting in a single value for response amplitude; these values were then plotted on a grid corresponding to the visual field (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). A two-dimensional Gaussian function was fit to the responses to approximate the average receptive field of the neurons underlying the electrode. The center of the fitted Gaussian was used as the estimate of the RF center of the neurons underlying the electrode. A high correlation between the fitted Gaussian and the raw evoked potentials indicated an electrode with a high-amplitude, focal receptive field. A threshold of r &gt; 0.7 was used to select only these electrodes for further consideration (<xref ref-type="bibr" rid="bib37">Yoshor et al., 2007</xref>).</p></sec><sec id="s4-8"><title>Speech stimuli analysis: Broadband power</title><p>While for the RF mapping analysis, we used raw voltage as our measure of neural response, speech stimuli evoke a long-lasting response not measurable with evoked potentials. Therefore, our primary measure of neural activity was the broadband (non-synchronous) response in the high-gamma frequency band, ranging from 70 to 150 Hz. This response is thought to reflect action potentials in nearby neurons (<xref ref-type="bibr" rid="bib21">Jacques et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Mukamel et al., 2005</xref>; <xref ref-type="bibr" rid="bib28">Nir et al., 2007</xref>; <xref ref-type="bibr" rid="bib33">Ray and Maunsell, 2011</xref>). To calculate broadband power, the average signal across all electrodes was subtracted from each individual electrode’s signal (common average referencing), line noise at 60, 120, 180 Hz was filtered and the data was transformed to time–frequency space using the multitaper method available in the FieldTrip toolbox (<xref ref-type="bibr" rid="bib30">Oostenveld et al., 2011</xref>) with 3 Slepian tapers; frequency window from 10 to 200 Hz; frequency steps of 2 Hz; time steps of 10 ms; temporal smoothing of 200 ms; frequency smoothing of ±10 Hz.</p><p>The broadband response at each time point following stimulus onset was measured as the percent change from baseline, with the baseline calculated over all trials and all experimental conditions in a time window from −500 to −100 ms before stimulus onset. To reject outliers, if at any point following stimulus onset the response was greater than ten standard deviations from the mean calculated across the rest of the trials, the entire trial was discarded (average of 10 trials were discarded per electrode, range from 1 to 16).</p></sec><sec id="s4-9"><title>Visual cortex electrode selection</title><p>Across eight subjects, we recorded from 154 occipital lobe electrodes. These were winnowed using two criteria. First, a well-demarcated spatial receptive field (see <italic>Receptive Field Mapping Analysis</italic> section, above). Second, a significant (q &lt; 0.01, false-discovery rate corrected) broadband response to <italic>AV</italic> speech; because only the response to <italic>AV</italic> trials was used to select electrodes, we could measure the response to the <italic>Aud</italic> and <italic>Vis</italic> conditions without bias. Out of 154 total occipital lobe electrodes, 73 electrodes met both criteria.</p></sec><sec id="s4-10"><title>Auditory cortex electrode selection</title><p>We recorded from 102 electrodes located on the superior temporal gyrus. 44 out of 102 electrodes showed a significant (q &lt; 0.01, false-discovery rate corrected) broadband response to <italic>AV</italic> speech.</p></sec><sec id="s4-11"><title>Frontal cortex electrode selection</title><p>We recorded from 179 electrodes located in lateral frontal cortex, defined as the lateral convexity of the hemisphere anterior to the central sulcus. 44 out of 179 electrodes showed a significant (q &lt; 0.01, false-discovery rate corrected) broadband response to <italic>AV</italic> speech. In each of the eight subjects, we selected the single frontal electrode that showed the largest broadband response to <italic>AV</italic> speech, measured as the signal-to-noise ratio (μ/σ).</p></sec><sec id="s4-12"><title>Linear mixed effects modeling</title><p>We used the <italic>lme4</italic> package, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_015654">SCR_015654</ext-link> (<xref ref-type="bibr" rid="bib2">Bates et al., 2014</xref>) for the R Project for Statistical Computing, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001905">SCR_001905</ext-link> to perform linear mixed effect (LME) analyses. Complete details of each analysis shown in <italic>Tables.</italic> Similar to an ANOVA (but allowing for missing data), the LME estimated the effect of each factor in units of the dependent variable (equivalent to beta weights in a linear regression) relative to an arbitrary baseline condition (defined in our analysis as the response to <italic>AV</italic> speech) and a standard error.</p></sec><sec id="s4-13"><title>Functional connectivity analysis</title><p>The average high-gamma power in the 200–1500 milliseconds was calculated for each trial, corresponding to this time in which mouth movements occur in the speech stimuli. After calculating the average broadband (70–150 Hz) power for each trial, functional connectivity between the 73 frontal-visual cortex as well as 44 frontal-auditory electrode pairs was measured by calculating the trial-by-trial Spearman rank correlation across trials of the same speech condition (<italic>AV</italic>, <italic>Vis</italic> or <italic>Aud</italic>) (<xref ref-type="bibr" rid="bib15">Foster et al., 2015</xref>; <xref ref-type="bibr" rid="bib20">Hipp et al., 2012</xref>).</p></sec><sec id="s4-14"><title>Time point of enhancement onset analysis</title><p>The average response showed a long-lasting enhancement for visual speech (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). In order to measure the time at which this occurred in individual electrodes, we compared the time course of the response to <italic>Vis</italic> and <italic>AV</italic> condition. The onset of enhancement was defined as the first time point in which there was a long-lasting (&gt;=200 ms) significantly greater response (p&lt;0.05 using a running t-test) to <italic>Vis</italic> compared with <italic>AV</italic> speech. Using these criteria, we were able to measure the enhancement onset time in 7 frontal and 29 visual electrodes (<xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research was funded by Veterans Administration Clinical Science Research and Development Merit Award Number 1I01C × 000325–01A1, NIH R01NS065395 U01NS098976.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con2"><p>Resources, Supervision, Funding acquisition, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All experimental procedures were approved by the Institutional Review Board of Baylor College of Medicine. Eight human subjects provided written informed consent prior to participating in the research protocol. The experimenter who recorded the stimuli used in the experiments gave written authorization for her likeness to be used for illustrating the stimuli in Figures 1 and 2 of the manuscript.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.30387.013</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-30387-transrepform-v1.docx"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Argall</surname> <given-names>BD</given-names></name><name><surname>Saad</surname> <given-names>ZS</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Simplified intersubject averaging on the cortical surface using SUMA</article-title><source>Human Brain Mapping</source><volume>27</volume><fpage>14</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1002/hbm.20158</pub-id><pub-id pub-id-type="pmid">16035046</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bates</surname> <given-names>D</given-names></name><name><surname>Mächler</surname> <given-names>M</given-names></name><name><surname>Bolker</surname> <given-names>B</given-names></name><name><surname>Walker</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Fitting linear mixed-effects models using lme4</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1406.5823">https://arxiv.org/abs/1406.5823</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callan</surname> <given-names>DE</given-names></name><name><surname>Jones</surname> <given-names>JA</given-names></name><name><surname>Callan</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multisensory and modality specific processing of visual speech in different regions of the premotor cortex</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>389</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00389</pub-id><pub-id pub-id-type="pmid">24860526</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvert</surname> <given-names>GA</given-names></name><name><surname>Campbell</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Reading speech from still and moving faces: the neural substrates of visible speech</article-title><source>Journal of Cognitive Neuroscience</source><volume>15</volume><fpage>57</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1162/089892903321107828</pub-id><pub-id pub-id-type="pmid">12590843</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canolty</surname> <given-names>RT</given-names></name><name><surname>Soltani</surname> <given-names>M</given-names></name><name><surname>Dalal</surname> <given-names>SS</given-names></name><name><surname>Edwards</surname> <given-names>E</given-names></name><name><surname>Dronkers</surname> <given-names>NF</given-names></name><name><surname>Nagarajan</surname> <given-names>SS</given-names></name><name><surname>Kirsch</surname> <given-names>HE</given-names></name><name><surname>Barbaro</surname> <given-names>NM</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatiotemporal dynamics of word processing in the human brain</article-title><source>Frontiers in Neuroscience</source><volume>1</volume><fpage>185</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.3389/neuro.01.1.1.014.2007</pub-id><pub-id pub-id-type="pmid">18982128</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catani</surname> <given-names>M</given-names></name><name><surname>Mesulam</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The arcuate fasciculus and the disconnection theme in language and aphasia: History and current state</article-title><source>Cortex</source><volume>44</volume><fpage>953</fpage><lpage>961</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2008.04.002</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname> <given-names>C</given-names></name><name><surname>Hamiton</surname> <given-names>LS</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The auditory representation of speech sounds in human motor cortex</article-title><source>eLife</source><volume>5</volume><elocation-id>e12577</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12577</pub-id><pub-id pub-id-type="pmid">26943778</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cope</surname> <given-names>TE</given-names></name><name><surname>Sohoglu</surname> <given-names>E</given-names></name><name><surname>Sedley</surname> <given-names>W</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Jones</surname> <given-names>PS</given-names></name><name><surname>Wiggins</surname> <given-names>J</given-names></name><name><surname>Dawson</surname> <given-names>C</given-names></name><name><surname>Grube</surname> <given-names>M</given-names></name><name><surname>Carlyon</surname> <given-names>RP</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name><name><surname>Rowe</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evidence for causal top-down frontal contributions to predictive processes in speech perception</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>2154</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01958-7</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title><source>Nature Reviews Neuroscience</source><volume>3</volume><fpage>201</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1038/nrn755</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crone</surname> <given-names>NE</given-names></name><name><surname>Boatman</surname> <given-names>D</given-names></name><name><surname>Gordon</surname> <given-names>B</given-names></name><name><surname>Hao</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Induced electrocorticographic gamma activity during auditory perception</article-title><source>Clinical Neurophysiology</source><volume>112</volume><fpage>565</fpage><lpage>582</lpage><pub-id pub-id-type="doi">10.1016/S1388-2457(00)00545-9</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis. I. Segmentation and surface reconstruction</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id><pub-id pub-id-type="pmid">9931268</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis. II: Inflation, flattening, and a surface-based coordinate system</article-title><source>NeuroImage</source><volume>9</volume><fpage>195</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0396</pub-id><pub-id pub-id-type="pmid">9931269</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flinker</surname> <given-names>A</given-names></name><name><surname>Korzeniewska</surname> <given-names>A</given-names></name><name><surname>Shestyuk</surname> <given-names>AY</given-names></name><name><surname>Franaszczuk</surname> <given-names>PJ</given-names></name><name><surname>Dronkers</surname> <given-names>NF</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name><name><surname>Crone</surname> <given-names>NE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Redefining the role of Broca’s area in speech</article-title><source>PNAS</source><volume>112</volume><fpage>2871</fpage><lpage>2875</lpage><pub-id pub-id-type="doi">10.1073/pnas.1414491112</pub-id><pub-id pub-id-type="pmid">25730850</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname> <given-names>BL</given-names></name><name><surname>Rangarajan</surname> <given-names>V</given-names></name><name><surname>Shirer</surname> <given-names>WR</given-names></name><name><surname>Parvizi</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Intrinsic and task-dependent coupling of neuronal population activity in human parietal cortex</article-title><source>Neuron</source><volume>86</volume><fpage>578</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.018</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gregoriou</surname> <given-names>GG</given-names></name><name><surname>Gotts</surname> <given-names>SJ</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cell-type-specific synchronization of neural activity in FEF with V4 during Attention</article-title><source>Neuron</source><volume>73</volume><fpage>581</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.019</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gunduz</surname> <given-names>A</given-names></name><name><surname>Brunner</surname> <given-names>P</given-names></name><name><surname>Daitch</surname> <given-names>A</given-names></name><name><surname>Leuthardt</surname> <given-names>EC</given-names></name><name><surname>Ritaccio</surname> <given-names>AL</given-names></name><name><surname>Pesaran</surname> <given-names>B</given-names></name><name><surname>Schalk</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural correlates of visual?spatial attention in electrocorticographic signals in humans</article-title><source>Frontiers in Human Neuroscience</source><volume>5</volume><elocation-id>89</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2011.00089</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname> <given-names>DA</given-names></name><name><surname>Fussell</surname> <given-names>C</given-names></name><name><surname>Summerfield</surname> <given-names>AQ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Reading fluent speech from talking faces: typical brain networks and individual differences</article-title><source>Journal of Cognitive Neuroscience</source><volume>17</volume><fpage>939</fpage><lpage>953</lpage><pub-id pub-id-type="doi">10.1162/0898929054021175</pub-id><pub-id pub-id-type="pmid">15969911</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname> <given-names>G</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dorsal and ventral streams: a framework for understanding aspects of the functional anatomy of language</article-title><source>Cognition</source><volume>92</volume><fpage>67</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2003.10.011</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hipp</surname> <given-names>JF</given-names></name><name><surname>Hawellek</surname> <given-names>DJ</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Siegel</surname> <given-names>M</given-names></name><name><surname>Engel</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Large-scale cortical correlation structure of spontaneous oscillatory activity</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>884</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1038/nn.3101</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacques</surname> <given-names>C</given-names></name><name><surname>Witthoft</surname> <given-names>N</given-names></name><name><surname>Weiner</surname> <given-names>KS</given-names></name><name><surname>Foster</surname> <given-names>BL</given-names></name><name><surname>Rangarajan</surname> <given-names>V</given-names></name><name><surname>Hermes</surname> <given-names>D</given-names></name><name><surname>Miller</surname> <given-names>KJ</given-names></name><name><surname>Parvizi</surname> <given-names>J</given-names></name><name><surname>Grill-Spector</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Corresponding ECoG and fMRI category-selective signals in human ventral temporal cortex</article-title><source>Neuropsychologia</source><volume>83</volume><fpage>14</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.07.024</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname> <given-names>S</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Mechanisms of visual attention in the human cortex</article-title><source>Annual Review of Neuroscience</source><volume>23</volume><fpage>315</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.23.1.315</pub-id><pub-id pub-id-type="pmid">10845067</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirchner</surname> <given-names>H</given-names></name><name><surname>Barbeau</surname> <given-names>EJ</given-names></name><name><surname>Thorpe</surname> <given-names>SJ</given-names></name><name><surname>Regis</surname> <given-names>J</given-names></name><name><surname>Liegeois-Chauvel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Ultra-rapid sensory responses in the human frontal eye field region</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>7599</fpage><lpage>7606</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1233-09.2009</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>H</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Physical and perceptual factors shape the neural mechanisms that integrate audiovisual signals in speech comprehension</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>11338</fpage><lpage>11350</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6510-10.2011</pub-id><pub-id pub-id-type="pmid">21813693</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Cheung</surname> <given-names>C</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Phonetic feature encoding in human superior temporal gyrus</article-title><source>Science</source><volume>343</volume><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id><pub-id pub-id-type="pmid">24482117</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Buschman</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical circuits for the control of attention</article-title><source>Current Opinion in Neurobiology</source><volume>23</volume><fpage>216</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.11.011</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukamel</surname> <given-names>R</given-names></name><name><surname>Gelbard</surname> <given-names>H</given-names></name><name><surname>Arieli</surname> <given-names>A</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Fried</surname> <given-names>I</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Coupling between neuronal firing, field potentials, and fmri in human auditory cortex</article-title><source>Science</source><volume>309</volume><fpage>951</fpage><lpage>954</lpage><pub-id pub-id-type="doi">10.1126/science.1110913</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nir</surname> <given-names>Y</given-names></name><name><surname>Fisch</surname> <given-names>L</given-names></name><name><surname>Mukamel</surname> <given-names>R</given-names></name><name><surname>Gelbard-Sagiv</surname> <given-names>H</given-names></name><name><surname>Arieli</surname> <given-names>A</given-names></name><name><surname>Fried</surname> <given-names>I</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Coupling between neuronal firing rate, gamma lfp, and bold fmri is related to interneuronal correlations</article-title><source>Current Biology</source><volume>17</volume><fpage>1275</fpage><lpage>1285</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.06.066</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okada</surname> <given-names>K</given-names></name><name><surname>Hickok</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Two cortical mechanisms support the integration of visual and auditory speech: a hypothesis and preliminary data</article-title><source>Neuroscience Letters</source><volume>452</volume><fpage>219</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2009.01.060</pub-id><pub-id pub-id-type="pmid">19348727</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popov</surname> <given-names>T</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>FEF-controlled alpha delay activity precedes stimulus-induced gamma-band activity in visual cortex</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>4117</fpage><lpage>4127</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3015-16.2017</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname> <given-names>JP</given-names></name><name><surname>Scott</surname> <given-names>SK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>718</fpage><lpage>724</lpage><pub-id pub-id-type="doi">10.1038/nn.2331</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ray</surname> <given-names>S</given-names></name><name><surname>Maunsell</surname> <given-names>JHR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Different origins of gamma rhythm and high-gamma activity in macaque visual cortex</article-title><source>PLoS Biology</source><volume>9</volume><elocation-id>e1000610</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000610</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schepers</surname> <given-names>IM</given-names></name><name><surname>Yoshor</surname> <given-names>D</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Electrocorticography reveals enhanced visual cortex responses to visual Speech</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>4103</fpage><lpage>4110</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu127</pub-id><pub-id pub-id-type="pmid">24904069</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vatikiotis-Bateson</surname> <given-names>E</given-names></name><name><surname>Eigsti</surname> <given-names>IM</given-names></name><name><surname>Yano</surname> <given-names>S</given-names></name><name><surname>Munhall</surname> <given-names>KG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Eye movement of perceivers during audiovisual speech perception</article-title><source>Perception &amp; Psychophysics</source><volume>60</volume><fpage>926</fpage><lpage>940</lpage><pub-id pub-id-type="doi">10.3758/BF03211929</pub-id><pub-id pub-id-type="pmid">9718953</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname> <given-names>A</given-names></name><name><surname>Wong</surname> <given-names>W</given-names></name><name><surname>Eizenman</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Gaze patterns and audiovisual speech enhancement</article-title><source>Journal of Speech Language and Hearing Research</source><volume>56</volume><fpage>471</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1044/1092-4388(2012/10-0288)</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshor</surname> <given-names>D</given-names></name><name><surname>Bosking</surname> <given-names>WH</given-names></name><name><surname>Ghose</surname> <given-names>GM</given-names></name><name><surname>Maunsell</surname> <given-names>JHR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Receptive fields in human visual cortex mapped with surface electrodes</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2293</fpage><lpage>2302</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl138</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.30387.015</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Pasternak</surname><given-names>Tatiana</given-names></name><role>Reviewing Editor</role><aff id="aff4"><institution>University of Rochester</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>[Editors’ note: this article was originally rejected after discussions between the reviewers, but the authors were invited to resubmit after an appeal against the decision.]</p><p>Thank you for submitting your work entitled &quot;Retinotopic Modulation of Visual Responses to Speech by Frontal Cortex Measured with Electrocorticography&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Edward F Chang (Reviewer #2).</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your work will not be considered further for publication in <italic>eLife</italic>.</p><p>Both reviewers listed a number of strengths of the paper, including the importance of the problem, experimental design and data presentation. However, they were not convinced that the observed frontal modulation of visual responses during speech is actually of retinotopic nature. In addition, there was a concern about the long latencies of responses as well as about the absence of direct evidence that the correlation between activity in frontal and visual regions is a reflection of top-down modulatory influences. The reviews list several other constructive comments and suggestions, which we hope you will find helpful.</p><p><italic>Reviewer #1:</italic> </p><p>This is a well-written paper reporting an interesting ECoG investigation of frontal modulation of visual responses during speech perception. The core finding is that visual responses in areas of occipital cortex independently determined to represent the fovea exhibit stronger responses when participants see visual-only-speech stimuli compared to audio-visual speech stimuli. The authors also find that the trial-to-trial amplitude of responses between frontal cortex and visual areas is related for visual areas representing the fovea (functional connectivity), and that the responses in frontal cortex precede (by ~175ms) those in visual cortex.</p><p>I have one major concern that would merit collection of some additional data; at present the authors interpretation is not distinguishable from a less interesting alternative. My second concern (2 below) could be addressed through reconsideration of the interpretation of the findings but, would also benefit from some additional data collected from control participants.</p><p>1) My principal concern is that the authors have not provided direct evidence that the modulation of visual responses is in fact retinotopic-their argument is that because participant were instructed to fixate the mouth region of the stimuli, and because the effects are observed only in visual areas corresponding to the fovea, the effect is retinotopic. But those data are ambiguous between the authors interpretation and the more bland interpretation that frontal cortex only (ever) modulates visual responses during speech processing in regions of visual cortex corresponding to the fovea. In other words, on the basis of the current set of studies, there is no way to know whether it would be possible for frontal cortex to modulate peripheral representations-knowing that is key for the authors' strong claim that frontal-occipital modulations are in fact retinotopic specific.</p><p>Decisive evidence could be brought to bear on this if an additional 1 or 2 patients could be studied using (a) the same exact paradigm to show that they replicate the core finding, and then (b) have participants fixate somewhere besides the mouth. Now it should be the case that the frontal modulation of visual areas is for visual areas that are independently determined to represent the periphery (and specifically, the part of the periphery in which the mouth falls). If the authors obtain those data they would have decisively disambiguated their interpretation from the more prosaic alternative and, would have strong evidence to claim (as they do) that frontal-occipital modulations are indeed retinotopic.</p><p>Related to this: note that in <xref ref-type="fig" rid="fig1">Figure 1F</xref> and <xref ref-type="fig" rid="fig2">Figure 2E</xref>, the correlations are largely driven by electrodes that correspond to the visual 'periphery' (i.e., &gt; 5 degrees). If you look at the correlations for only responses in the periphery, there is clear modulation even between 7 degrees and 15 degrees (i.e., in visual regions that definitely are not processing the mouth). Why would there be such modulation according to the authors account? In contrast, on the more prosaic alternative one would expect there to be such a general bias even for nonfoveal visual electrodes.</p><p>2) I was surprised at the relatively late latencies in frontal cortex and visual cortex, with the mean frontal latency starting around 400ms and the visual latency around 660ms post stimulus. Given that it takes ~200ms to articulate a syllable, this means that the frontal responses are 2 syllables behind the speech signal, and the visual modulations are 3 syllables behind the speech signal (!). If visual cortex is enhancing its response 3 syllables late, then what possible relevance could that enhancement have on processing? Perhaps a more nuanced discussion is warranted-for instance, could it be that top-down modulation is not useful for improving comprehension of the currently ambiguous syllable but that it will kick in several syllables down the line? If that is the case, it would seem feasible to have psychophysical evidence (from typical subjects) to independently support that. EG listening to normal AV of someone saying 'bi-da-ku-ma-ti-pu…etc', then the audio becomes noisy at syllable n, but the benefit of increased attention to the mouth for disambiguating the speech signal does not kick in until syllable n + 2 in the speech stream.</p><p><italic>Reviewer #2:</italic> </p><p>Ozker and colleagues present an experiment in which the contributions of visual and frontal neural populations to speech perception are evaluated. By comparing responses on intracranially implanted electrodes between visual, audio-visual, and auditory speech conditions, they show that visual electrodes show stronger responses to V compared to AV speech. This effect is modulated by whether electrodes have receptive fields that include the location of the mouth in the visual stimulus. Likewise, frontal electrodes show the same pattern, where there are greater responses to V compared to AV speech. In the crucial analyses, they report a correlation between V response amplitudes in frontal and visual electrodes, and a lag between these regions, which they suggest reflects top-down modulation of visual responses by frontal electrodes.</p><p>The authors tackle a very interesting and important question for understanding mechanisms of speech perception. I believe the results have important implications both for the audio-visual speech literature, and also for more general theories of top-down modulation effects in speech. The stimuli and experiment are well-designed, and the results are presented in a clear and concise manner. I have the following comments/suggestions:</p><p>1) Although I agree with the authors' general premise that frontal regions may provide top-down modulatory signals to other (sensory/perceptual) regions, I am not convinced that they have shown such effects here. First, the claims are based on correlations between response amplitudes across regions during Vis trials. Unfortunately, there is no trial-by-trial behavior to measure comprehension or discrimination effects, which makes it difficult to make strong claims about what variability in response amplitude reflects. They might be able to compensate for this lack of behavior by examining stimulus decoding or classification accuracy. By understanding how well a particular trial can be decoded using only visual information from visual and/or frontal electrodes, it would allow a more direct examination of what frontal electrodes contribute. As it stands, I believe the claims are limited by the classic problem of not knowing whether the correlated activity of two regions is causal, caused by a third region, or correlated due to an unmeasured but otherwise correlated phenomenon.</p><p>2) I'm a little confused about the precise claims regarding top-down modulation by frontal neural populations. Is there a reason why the effect should be limited to visual stimuli and visual cortex? Shouldn't there also be a (smaller) effect for auditory stimuli with frontal-auditory electrode pairs? Perhaps there was no coverage of auditory areas in the patients in this cohort, however the authors have published auditory data, so I wonder whether they see analogous effects. This is important because their claims are built on the notion that frontal cortex helps when the stimulus is noisy, which can be true in both modalities.</p><p>3) Electrode selection: From the Materials and methods and Results sections, it appears that only one frontal electrode per subject was used. This electrode was then paired with every visual electrode in that subject?</p><p>a) The anatomical criterion is not well-motivated. I don't really even understand what the criterion was (&quot;a location near the precentral sulcus, the location of the human frontal eye fields and other attentional control areas&quot; is very vague). How many electrodes per subject were actually implanted in these regions?</p><p>b) The functional criterion potentially makes more sense, since it will allow for electrodes with higher SNR to be evaluated. However, in general I don't see why only one electrode per subject was used.</p><p>c) Finally, I think that showing negative results for electrodes in other non-frontal regions could help bolster the claims about frontal areas being the specific modulatory signal. As it stands, the vague definition of &quot;frontal&quot; in the paper makes it hard to evaluate what region(s) are even being claimed. Another approach would be to ask whether there is a topography to these very heterogeneous regions, where some frontal regions show stronger modulatory effects than others.</p><p>4) In <xref ref-type="fig" rid="fig1">Figure 1F</xref>, although there is a clear negative correlation, it is also obvious that this correlation is only true for peripheral visual electrodes. If each population is considered separately, there will likely be no effect in the central electrodes. This indicates that within the foveal region, there is no sensitivity to eccentricity, which means that across visual cortex, this is a non-linear effect.a. Related, the correlation for peripheral electrodes is actually quite difficult for me to interpret. First, there are a group of electrodes (around 8-10 deg) that show no difference between V and AV. At even greater eccentricities, the effect actually reverses, where V &gt; AV. First, this suggests that the average responses in <xref ref-type="fig" rid="fig1">Figure 1E</xref> obscure some important variability in the nature of the peripheral response. Second, it suggests that the negative correlation with eccentricity in peripheral visual cortex is not an accurate description. Instead, the data suggest that as you go to greater eccentricities, the enhancement effect actually reverses, and that the assumption that the enhancement effect is V &gt; AV is not always true.</p><p>5) A similar problem exists in <xref ref-type="fig" rid="fig2">Figure 2E</xref>. To the extent that there is a negative correlation in central visual electrodes, it is of a different nature than in peripheral electrodes. Whereas the central electrodes may show a correlation where lower eccentricity is associated with a stronger frontal-visual correlation, peripheral electrodes show a reversal with eccentricity, where less eccentric electrodes show a positive correlation, and more eccentric electrodes show a negative correlation. Again, this changes the interpretation of how eccentricity affects these effects. Indeed, the most eccentric peripheral electrodes have about the same predictive power with frontal activity as the least eccentric peripheral electrodes.</p><p>[Editors’ note: what now follows is the decision letter after the authors submitted for further consideration.]</p><p>Thank you for submitting your article &quot;Retinotopic Modulation of Visual Responses to Speech by Frontal Cortex Measured with Electrocorticography&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Andrew King as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Bradford Mahon (Reviewer #1); Edward F Chang (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The revised manuscript addressed quite well the reservations raised in the previous review. However, there are two remaining issues that both reviewers agreed should be addressed. These are listed below.</p><p>Essential revisions:</p><p>1) The analysis comparing the frontal-auditory and frontal-visual correlations.</p><p>Reviewer 2 states &quot;I strongly suspect that there are electrodes within the cloud shown in <xref ref-type="fig" rid="fig3">Figure 3E</xref> that are tuned to the speech sounds on each trial. Are they overall stronger correlations compared to non-relevant auditory electrodes? &quot; Please address this question in your revision.</p><p>2) Question about frontal electrodes: Reviewer 2 raises the question about the frontal electrodes, suggesting that they may actually be showing auditory responses. Please address this point in the revision.</p><p><italic>Reviewer #1:</italic> </p><p>This revised manuscript addresses directly and decisively the main concern I had from the last round, which was that the core finding was ambiguous between general enhancement of the fovea or specifically the region of visual cortex that processes the mouth. The authors have included a control experiment in which the patient is instructed to fixate on the shoulder of the person in the video--they now show that there is enhancement of visual electrodes processing the mouth even though the mouth is now in the periphery. I consider that lock down evidence for their core claim and am grateful to the authors to taking up that concern and addressing it empirically.</p><p>I am also satisfied with the authors' responses to my other concerns, which were relatively minor.</p><p>The paper will have a major impact on the field-</p><p><italic>Reviewer #2:</italic> </p><p>The authors have addressed my major concerns from the original version of the manuscript. The redone analyses and new data are convincing and help clarify some of the lingering questions about the specificity of the top-down effects. I believe this paper will be an important contribution to the literature.</p><p>I have a couple of remaining issues that pertain to the interpretation:</p><p>1) I greatly appreciate the authors adding the analysis shown in <xref ref-type="fig" rid="fig3">Figure 3C-E</xref> on effects in auditory electrodes. I think this adds an important dimension to the story. However, I think the analysis comparing the frontal-auditory and frontal-visual correlations is misleading. They claim in the Results section that connectivity was stronger for visual than auditory, however they are actually comparing the TUNED visual electrodes and all auditory electrodes. I strongly suspect that there are electrodes within the cloud shown in <xref ref-type="fig" rid="fig3">Figure 3E</xref> that are tuned to the speech sounds on each trial. Are they overall stronger correlations compared to non-relevant auditory electrodes? I think this is important because it addresses the fundamental claim of whether the frontal top-down modulation is modality-specific. If my hypothesis is correct, then combined with the new analysis in response to reviewer 1, I think the story is actually that top-down modulation occurs most strongly for perceptually relevant features (mouth movements and acoustic-phonetic features).</p><p>2) I also appreciate the authors clarifying the electrode selection procedures for frontal regions. I am satisfied with SNR as the primary criterion, however I still don't understand the motivation for choosing only 1 electrode per subject. Specifically, I am unsure what these frontal electrodes are actually doing, and I have a suspicion that they are actually showing auditory responses. I think this might be the case given that the average response-locked time-courses (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) are very similar to auditory responses that have been observed in this dorsal frontal region before (e.g., strong responses above baseline with rapid onsets; see Cheung et al., 2016). In other words, are the electrodes in that region with the highest SNR simply &quot;auditory&quot; electrodes? Would similar effects be observed with the same connectivity analysis between STG and visual electrodes? In my opinion, this is a different interpretation of the effect than if the frontal electrodes were representing some more abstract information about the stimulus or something about attentional modulation, which is the current claim.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.30387.016</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the author responses to the first round of peer review follow.]</p><disp-quote content-type="editor-comment"><p>Both reviewers listed a number of strengths of the paper, including the importance of the problem, experimental design and data presentation. However, they were not convinced that the observed frontal modulation of visual responses during speech is actually of retinotopic nature. In addition, there was a concern about the long latencies of responses as well as about the absence of direct evidence that the correlation between activity in frontal and visual regions is a reflection of top-down modulatory influences. The reviews list several other constructive comments and suggestions, which we hope you will find helpful.</p></disp-quote><p>We thank the reviewers for their many helpful criticisms and suggestions. We have completely changed our approach. The major highlights are:</p><p>1) We have collected additional ECOG data using a very interesting control experiment suggested by reviewer 1. This experiment allows for the dissociation of the effects of visual field location (central <italic>vs.</italic> peripheral) and the effects of facial location (mouth <italic>vs</italic>. non-mouth) by presenting the talking face stimulus in the visual periphery (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p><p>2) As suggested by reviewer 2, we have now measured connectivity between frontal cortex and auditory cortex, using responses measured from 44 auditory cortex electrodes (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>3) Both reviewers were confused by our treatment of the eccentricity of each visual electrode as a continuous variable. As they pointed out, it is not clear that there should be a linear relationship between eccentricity in response. These analyses have been removed.</p><p>4) Our original focus on visual field eccentricity was confusing. In fact, our main prediction is that visual speech information is concentrated in the mouth region of the talker’s face and the representation of this part of the face should be enhanced. Therefore, we have reanalyzed all of our data using a categorical classification of electrodes as coding either the mouth of the talker’s face or the remainder of the face.</p><p>5) Reviewer 2 was confused by our selection criteria for frontal electrodes. We now use a single, simple criterion suggested by the reviewer (signal-to-noise ratio).</p><p>6) We have extended our quantitative latency analysis to include all frontal-visual electrode pairs, divided by subjects.</p><p>Reviewer #1:</p><disp-quote content-type="editor-comment"><p>This is a well-written paper reporting an interesting ECoG investigation of frontal modulation of visual responses during speech perception. The core finding is that visual responses in areas of occipital cortex independently determined to represent the fovea exhibit stronger responses when participants see visual-only-speech stimuli compared to audio-visual speech stimuli. The authors also find that the trial-to-trial amplitude of responses between frontal cortex and visual areas is related for visual areas representing the fovea (functional connectivity), and that the responses in frontal cortex precede (by ~175ms) those in visual cortex.</p><p>I have one major concern that would merit collection of some additional data; at present the authors interpretation is not distinguishable from a less interesting alternative. My second concern (2 below) could be addressed through reconsideration of the interpretation of the findings but, would also benefit from some additional data collected from control participants.</p><p>1) My principal concern is that the authors have not provided direct evidence that the modulation of visual responses is in fact retinotopic-their argument is that because participant were instructed to fixate the mouth region of the stimuli, and because the effects are observed only in visual areas corresponding to the fovea, the effect is retinotopic. But those data are ambiguous between the authors interpretation and the more bland interpretation that frontal cortex only (ever) modulates visual responses during speech processing in regions of visual cortex corresponding to the fovea. In other words, on the basis of the current set of studies, there is no way to know whether it would be possible for frontal cortex to modulate peripheral representations-knowing that is key for the authors' strong claim that frontal-occipital modulations are in fact retinotopic specific.</p><p>Decisive evidence could be brought to bear on this if an additional 1 or 2 patients could be studied using (a) the same exact paradigm to show that they replicate the core finding, and then (b) have participants fixate somewhere besides the mouth. Now it should be the case that the frontal modulation of visual areas is for visual areas that are independently determined to represent the periphery (and specifically, the part of the periphery in which the mouth falls). If the authors obtain those data they would have decisively disambiguated their interpretation from the more prosaic alternative and, would have strong evidence to claim (as they do) that frontal-occipital modulations are indeed retinotopic.</p></disp-quote><p>We agree with the reviewer that our initial study confounded the location of enhancement: since the talker’s mouth was always foveal, the observed enhancement could be specific to the location of the talker’s mouth, or specific to the center of gaze. We thank the reviewer for suggesting a very interesting control experiment. We programmed the experiment and have collected data using this design (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). The results show that enhancement is specific to the mouth of the talker, not the center of gaze.</p><disp-quote content-type="editor-comment"><p>Related to this: note that in <xref ref-type="fig" rid="fig1">Figure 1F</xref> and <xref ref-type="fig" rid="fig2">Figure 2E</xref>, the correlations are largely driven by electrodes that correspond to the visual 'periphery' (i.e., &gt; 5 degrees). If you look at the correlations for only responses in the periphery, there is clear modulation even between 7 degrees and 15 degrees (i.e., in visual regions that definitely are not processing the mouth). Why would there be such modulation according to the authors account? In contrast, on the more prosaic alternative one would expect there to be such a general bias even for nonfoveal visual electrodes.</p></disp-quote><p>We agree and this analysis has been removed from the paper; see points 3 and 4, above.</p><disp-quote content-type="editor-comment"><p>2) I was surprised at the relatively late latencies in frontal cortex and visual cortex, with the mean frontal latency starting around 400ms and the visual latency around 660ms post stimulus. Given that it takes ~200ms to articulate a syllable, this means that the frontal responses are 2 syllables behind the speech signal, and the visual modulations are 3 syllables behind the speech signal (!). If visual cortex is enhancing its response 3 syllables late, then what possible relevance could that enhancement have on processing? Perhaps a more nuanced discussion is warranted-for instance, could it be that top-down modulation is not useful for improving comprehension of the currently ambiguous syllable but that it will kick in several syllables down the line? If that is the case, it would seem feasible to have psychophysical evidence (from typical subjects) to independently support that. EG listening to normal AV of someone saying 'bi-da-ku-ma-ti-pu…etc', then the audio becomes noisy at syllable n, but the benefit of increased attention to the mouth for disambiguating the speech signal does not kick in until syllable n + 2 in the speech stream.</p></disp-quote><p>We apologize that the latency values reported in the original manuscript were confusing. The times reported in the original manuscript were calculated from the beginning of the video clip rather than the beginning of speech. We now report latencies with respect to this. For instance, “an average latency difference of 230 ms (frontal <italic>vs.</italic> visual: 257 ± 150 ms <italic>vs.</italic> 487 ± 110 ms <italic>vs.</italic> with respect to auditory speech onset, t<sub>34</sub> = 4, p = 10<sup>-4</sup>).”</p><p>While attentional enhancement may take some time to “kick in”, it can provide a sustained boost in processing efficiency. While in our experiments, a single word was presented, leading to a brief attentional response in frontal cortex, in a longer speech stream, attention to the mouth would be long-lasting, producing a sustained response in frontal cortex and sustained enhancement of visual speech information in visual cortex.</p><p>Reviewer #2:</p><disp-quote content-type="editor-comment"><p>Ozker and colleagues present an experiment in which the contributions of visual and frontal neural populations to speech perception are evaluated. By comparing responses on intracranially implanted electrodes between visual, audio-visual, and auditory speech conditions, they show that visual electrodes show stronger responses to V compared to AV speech. This effect is modulated by whether electrodes have receptive fields that include the location of the mouth in the visual stimulus. Likewise, frontal electrodes show the same pattern, where there are greater responses to V compared to AV speech. In the crucial analyses, they report a correlation between V response amplitudes in frontal and visual electrodes, and a lag between these regions, which they suggest reflects top-down modulation of visual responses by frontal electrodes.</p><p>The authors tackle a very interesting and important question for understanding mechanisms of speech perception. I believe the results have important implications both for the audio-visual speech literature, and also for more general theories of top-down modulation effects in speech. The stimuli and experiment are well-designed, and the results are presented in a clear and concise manner. I have the following comments/suggestions:</p><p>1) Although I agree with the authors' general premise that frontal regions may provide top-down modulatory signals to other (sensory/perceptual) regions, I am not convinced that they have shown such effects here. First, the claims are based on correlations between response amplitudes across regions during Vis trials. Unfortunately, there is no trial-by-trial behavior to measure comprehension or discrimination effects, which makes it difficult to make strong claims about what variability in response amplitude reflects. They might be able to compensate for this lack of behavior by examining stimulus decoding or classification accuracy. By understanding how well a particular trial can be decoded using only visual information from visual and/or frontal electrodes, it would allow a more direct examination of what frontal electrodes contribute. As it stands, I believe the claims are limited by the classic problem of not knowing whether the correlated activity of two regions is causal, caused by a third region, or correlated due to an unmeasured but otherwise correlated phenomenon.</p></disp-quote><p>We thank the reviewer for this suggestion. We attempted a decoding analysis using a number of methods with our data. Unfortunately, with only 8 to 16 trials for each of the presented words, there was not enough data to perform a reliable classification analysis.</p><disp-quote content-type="editor-comment"><p>2) I'm a little confused about the precise claims regarding top-down modulation by frontal neural populations. Is there a reason why the effect should be limited to visual stimuli and visual cortex? Shouldn't there also be a (smaller) effect for auditory stimuli with frontal-auditory electrode pairs? Perhaps there was no coverage of auditory areas in the patients in this cohort, however the authors have published auditory data, so I wonder whether they see analogous effects. This is important because their claims are built on the notion that frontal cortex helps when the stimulus is noisy, which can be true in both modalities.</p></disp-quote><p>We thank the reviewer for this suggestion, we agree that an examination of frontal-auditory connectivity is interesting and important. We have performed this analysis and now report it in <xref ref-type="fig" rid="fig3">Figure 3C-E</xref>.</p><disp-quote content-type="editor-comment"><p>3) Electrode selection: From the Materials and methods and Results sections, it appears that only one frontal electrode per subject was used. This electrode was then paired with every visual electrode in that subject?</p><p>a) The anatomical criterion is not well-motivated. I don't really even understand what the criterion was (&quot;a location near the precentral sulcus, the location of the human frontal eye fields and other attentional control areas&quot; is very vague). How many electrodes per subject were actually implanted in these regions?</p><p>b) The functional criterion potentially makes more sense, since it will allow for electrodes with higher SNR to be evaluated. However, in general I don't see why only one electrode per subject was used.</p><p>c) Finally, I think that showing negative results for electrodes in other non-frontal regions could help bolster the claims about frontal areas being the specific modulatory signal. As it stands, the vague definition of &quot;frontal&quot; in the paper makes it hard to evaluate what region(s) are even being claimed. Another approach would be to ask whether there is a topography to these very heterogeneous regions, where some frontal regions show stronger modulatory effects than others.</p></disp-quote><p>We thank the reviewer for these helpful suggestions.</p><p>a) We eliminated the anatomical criterion.</p><p>b) As suggested by the reviewer, we now use the SNR as our sole criterion, picking the frontal electrode with the highest SNR in each subject. We also used the two electrodes with the highest SNR in each subject and found similar results. In general, there is a great deal of heterogeneity in frontal electrodes and examining only electrodes with the most robust response helps ensure reliability.</p><p>Subsection “Frontal Cortex Electrode Selection”: We recorded from 179 electrodes located in lateral frontal cortex, defined as the lateral convexity of the hemisphere anterior to the central sulcus. 44 out of 179 electrodes showed a significant (q &lt; 0.01, false-discovery rate corrected) broadband response to <italic>AV</italic> speech. In each of the 8 subjects, we selected the single frontal electrode that showed the largest broadband response to <italic>AV</italic> speech, measured as the signal-to-noise ratio (μ/σ).</p><p>c) We now show that connectivity between frontal cortex and mouth electrodes is significant greater than connectivity between frontal cortex and non-mouth electrodes or connectivity between frontal cortex and auditory cortex.</p><disp-quote content-type="editor-comment"><p>4) In <xref ref-type="fig" rid="fig1">Figure 1F</xref>, although there is a clear negative correlation, it is also obvious that this correlation is only true for peripheral visual electrodes. If each population is considered separately, there will likely be no effect in the central electrodes. This indicates that within the foveal region, there is no sensitivity to eccentricity, which means that across visual cortex, this is a non-linear effect.</p><p>a. Related, the correlation for peripheral electrodes is actually quite difficult for me to interpret. First, there are a group of electrodes (around 8-10 deg) that show no difference between V and AV. At even greater eccentricities, the effect actually reverses, where V &gt; AV. First, this suggests that the average responses in <xref ref-type="fig" rid="fig1">Figure 1E</xref> obscure some important variability in the nature of the peripheral response. Second, it suggests that the negative correlation with eccentricity in peripheral visual cortex is not an accurate description. Instead, the data suggest that as you go to greater eccentricities, the enhancement effect actually reverses, and that the assumption that the enhancement effect is V &gt; AV is not always true.</p><p>5) A similar problem exists in <xref ref-type="fig" rid="fig2">Figure 2E</xref>. To the extent that there is a negative correlation in central visual electrodes, it is of a different nature than in peripheral electrodes. Whereas the central electrodes may show a correlation where lower eccentricity is associated with a stronger frontal-visual correlation, peripheral electrodes show a reversal with eccentricity, where less eccentric electrodes show a positive correlation, and more eccentric electrodes show a negative correlation. Again, this changes the interpretation of how eccentricity affects these effects. Indeed, the most eccentric peripheral electrodes have about the same predictive power with frontal activity as the least eccentric peripheral electrodes.</p></disp-quote><p>We agree completely and this analysis has been removed from the paper; see points 3 and 4 in the first section of the response.</p><p>[Editors' note: the author responses to the re-review follow.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The analysis comparing the frontal-auditory and frontal-visual correlations.</p><p>Reviewer 2 states &quot;I strongly suspect that there are electrodes within the cloud shown in <xref ref-type="fig" rid="fig3">Figure 3E</xref> that are tuned to the speech sounds on each trial. Are they overall stronger correlations compared to non-relevant auditory electrodes? &quot; Please address this question in your revision.</p></disp-quote><p>While the primary focus of our manuscript is on frontal modulation of visual cortex, we agree that auditory cortex provides an important comparison. We have performed new analyses and created a new figure (<xref ref-type="fig" rid="fig5">Figure 5</xref>) to provide more information. We now report in the Results section:</p><p>“While our primary focus was on frontal modulation of visual cortex, auditory cortex was analyzed for comparison. 44 electrodes located on the superior temporal gyrus responded to <italic>AV</italic> speech (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). As expected, these electrodes showed little response to <italic>Vis</italic> speech but a strong response to <italic>Aud</italic> speech, with a maximal response amplitude of 148percent at 67 ms after auditory speech onset. While visual cortex showed a greater response to <italic>Vis</italic> speech than to <italic>AV</italic> speech, there was no significant difference in the response of auditory cortex to <italic>Aud</italic> and <italic>AV</italic> speech, as confirmed by the LME model (p = 0.8, Supplementary <xref ref-type="table" rid="table4">table 4</xref>). Next, we analyzed the connectivity between frontal cortex and auditory electrodes. The LME revealed a significant effect of stimulus, driven by weaker connectivity in the <italic>Vis</italic> condition in which no auditory speech was presented (p = 0.02; Supplementary <xref ref-type="table" rid="table5">table 5</xref>). Overall, the connectivity between frontal cortex and auditory cortex was weaker than the connectivity between frontal cortex and visual cortex (0.04 for frontal-auditory <italic>vs.</italic> 0.23 for frontal-visual mouth electrodes; p = 0.001, unpaired t-test).</p><p>In visual cortex, a subset of electrodes (those representing the mouth) showed a greater response in the <italic>Vis-AV</italic> contrast and greater connectivity with frontal cortex. To determine if the same was true in auditory cortex, we selected the STG electrodes with the strongest response in the <italic>Aud-AV</italic> contrast (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Unlike in visual cortex, in which there was anatomical localization of mouth-representing electrodes on the occipital pole, auditory electrodes with the strongest response in the <italic>Aud-AV</italic> contrast did not show clear anatomical clustering (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Electrodes with the electrodes with the strongest response in the <italic>Aud-AV</italic> contrast had equivalent connectivity with frontal cortex as other STG electrodes (ρ = -0.04 <italic>vs. roh</italic> = 0.06, unpaired t-test = 0.9, p = 0.3). This was the opposite of the pattern observed in visual cortex. In order to ensure that signal amplitude was not the main determinant of connectivity, we selected the STG electrodes with the highest signal-to-noise ratio in the AV condition. These electrodes had equivalent connectivity with frontal cortex as other STG electrodes (ρ = -0.002 <italic>vs.</italic> ρ =0.05, unpaired t-test = 0.4, p = 0.7).”</p><disp-quote content-type="editor-comment"><p>2) Question about frontal electrodes: Reviewer 2 raises the question about the frontal electrodes, suggesting that they may actually be showing auditory responses. Please address this point in the revision.</p></disp-quote><p>We now write in the Discussion section:</p><p>“The analysis of auditory cortex responses provides an illuminating contrast with the visual cortex results. While removing auditory speech increased visual cortex response amplitudes and frontal-visual connectivity, removing visual speech did not change auditory cortex response amplitudes or connectivity. A simple explanation for this is that auditory speech is easily intelligible without visual speech, so that no attentional modulation is required. In contrast, perceiving visual-only speech requires speechreading, a difficult task that demands attentional engagement. An interesting test of this idea would be to present auditory speech with added noise. Under these circumstances, attentional engagement would be expected in order to enhance the intelligibility of the noisy auditory speech, with a neural manifestation of increased response amplitudes in auditory cortex and increased connectivity with frontal cortex. The interaction of stimulus and task also provides an explanation for the frontal activations in response to the speech stimuli. The human frontal eye fields show rapid sensory-driven responses, with latencies as short as 24 ms to auditory stimuli and 45 ms to visual stimuli (Kirchner et al., 2009). Following initial sensory activation, task demands modulate frontal function, with demanding tasks such as processing visual-only or noisy auditory speech or resulting in enhanced activity (Cheung et al., 2016, Cope et al., 2017).”</p></body></sub-article></article>