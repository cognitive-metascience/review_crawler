<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">54846</article-id><article-id pub-id-type="doi">10.7554/eLife.54846</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A compositional neural code in high-level visual cortex can explain jumbled word reading</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-170855"><name><surname>Agrawal</surname><given-names>Aakash</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8320-4516</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-170856"><name><surname>Hari</surname><given-names>KVS</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1264-1895</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-136374"><name><surname>Arun</surname><given-names>SP</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9602-5066</contrib-id><email>sparun@iisc.ac.in</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Centre for BioSystems Science &amp; Engineering, Indian Institute of Science</institution><addr-line><named-content content-type="city">Bangalore</named-content></addr-line><country>India</country></aff><aff id="aff2"><label>2</label><institution>Department of Electrical Communication Engineering, Indian Institute of Science</institution><addr-line><named-content content-type="city">Bangalore</named-content></addr-line><country>India</country></aff><aff id="aff3"><label>3</label><institution>Centre for Neuroscience, Indian Institute of Science</institution><addr-line><named-content content-type="city">Bangalore</named-content></addr-line><country>India</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute of Mental Health, National Institutes of Health</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Senior Editor</role><aff><institution>Radboud University</institution><country>Netherlands</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>05</day><month>05</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e54846</elocation-id><history><date date-type="received" iso-8601-date="2020-01-02"><day>02</day><month>01</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-05-04"><day>04</day><month>05</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Agrawal et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Agrawal et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-54846-v3.pdf"/><abstract><p>We read jubmled wrods effortlessly, but the neural correlates of this remarkable ability remain poorly understood. We hypothesized that viewing a jumbled word activates a visual representation that is compared to known words. To test this hypothesis, we devised a purely visual model in which neurons tuned to letter shape respond to longer strings in a compositional manner by linearly summing letter responses. We found that dissimilarities between letter strings in this model can explain human performance on visual search, and responses to jumbled words in word reading tasks. Brain imaging revealed that viewing a string activates this letter-based code in the lateral occipital (LO) region and that subsequent comparisons to stored words are consistent with activations of the visual word form area (VWFA). Thus, a compositional neural code potentially contributes to efficient reading.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>“Aoccdrnig to a rseearch at Cmabridge Uinervtisy, it deos not mttaer in what oredr the ltteers in a wrod are, the olny iprmoatnt tihng is taht the frist and lsat ltteer be in the rghit pclae.”</p><p>The above text is an example of the so-called “Cambridge University effect”, a meme that often circulates on the Internet. While the statement has several caveats, people do seem able to read jumbled words – at least short ones – with remarkable ease. But how is this possible? Answering this question has proven difficult because reading involves many different processes. These include analyzing the visual appearance of a word as well as recalling its pronunciation and meaning.</p><p>To find out why people are so good at reading jumbled words, Agrawal et al. tested healthy volunteers on a word recognition task. The volunteers viewed strings of letters and had to decide whether each was a word or a non-word. The more closely a jumbled non-word resembled a real word, the longer the volunteers took to categorize it. PENICL took longer than EPNCIL, for example. Words in which some of the original letters had been replaced were easier to categorize than words in which letters had only been swapped. And, as shown by the 'Cambridge University effect', swapping the first and last letters had a greater effect than swapping the middle ones.</p><p>Agrawal et al. proposed that this is because when people view a string of letters, visual areas of the brain become active in a pattern representing those letters. The brain then compares this pattern to stored representations of known words. To test this idea, Agrawal et al. developed a computer model consisting of a group of artificial neurons. Each neuron responded more to some letters than others, and the response of the model to a word could be obtained by adding together its responses to all of the letters. This model, based only on processing information using sight, predicted how long volunteers took to process jumbled words. This finding in turn suggests that sound, pronunciation or meaning of the word do not contribute as much to jumbled word reading as previously believed.</p><p>Finally, the volunteers performed the same task inside a brain scanner. This revealed the brain regions responsible for processing letter strings and for comparing them to stored words. By identifying the brain circuitry that supports reading – of both intact and jumbled words – these findings could ultimately prove useful in diagnosing and treating reading disorders.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>language</kwd><kwd>reading</kwd><kwd>word recognition</kwd><kwd>orthographic processing</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009053</institution-id><institution>Wellcome Trust/DBT India Alliance</institution></institution-wrap></funding-source><award-id>IA/S/17/1/503081</award-id><principal-award-recipient><name><surname>Arun</surname><given-names>SP</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009053</institution-id><institution>Wellcome Trust/DBT India Alliance</institution></institution-wrap></funding-source><award-id>500027/Z/09/Z</award-id><principal-award-recipient><name><surname>Arun</surname><given-names>SP</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007780</institution-id><institution>Indian Institute of Science</institution></institution-wrap></funding-source><award-id>DBT-IISc partnership programme</award-id><principal-award-recipient><name><surname>Arun</surname><given-names>SP</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Viewing a jumbled word activates an efficient visual representation in high-level visual cortex that is matched to stored words in the word form area.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Reading is a recent cultural invention, yet we are remarkably efficient at reading words and even jmulbed wrods (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). What makes a jumbled word easy or hard to read? This question has captured the popular imagination through demonstrations such as the Cambridge University effect (<xref ref-type="bibr" rid="bib57">Rawlinson, 1976</xref>; <xref ref-type="bibr" rid="bib26">Grainger and Whitney, 2004</xref>), depicted in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. Reading a word or a jumbled word can be influenced by a variety of factors such as visual, phonological and linguistic processing (<xref ref-type="bibr" rid="bib41">Norris, 2013</xref>; <xref ref-type="bibr" rid="bib25">Grainger, 2018</xref>). At the visual level, word reading is easy when similar shapes are substituted (<xref ref-type="bibr" rid="bib48">Perea et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Perea and Panadero, 2014</xref>), when the first and last letters are preserved (<xref ref-type="bibr" rid="bib59">Rayner et al., 2006</xref>), when there are fewer transpositions (<xref ref-type="bibr" rid="bib23">Gomez et al., 2008</xref>), when word shape is preserved (<xref ref-type="bibr" rid="bib41">Norris, 2013</xref>; <xref ref-type="bibr" rid="bib25">Grainger, 2018</xref>). At the linguistic level, it is easier to read frequent words, words with frequent bigrams or trigrams as well as shuffled words that preserve intermediate units such as consonant clusters or morphemes (<xref ref-type="bibr" rid="bib41">Norris, 2013</xref>; <xref ref-type="bibr" rid="bib25">Grainger, 2018</xref>). Despite these insights, it is not clear how these factors combine, what their distinct contributions are, and more generally, how word representations relate to letter representations.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Reading jumbled words.</title><p>(<bold>A</bold>) We are extremely good at reading jumbled words, as illustrated by the popular Cambridge University effect. (<bold>B</bold>) Visual search array showing two oddball targets (OFRGET and FOGRET) among many instances of FORGET. OFRGET is easy to find but not FOGRET. (<bold>C</bold>) Schematic representation of these strings in visual search space, arranged such that similar items (corresponding to harder searches) are nearby. Thus, FOGRET is visually more similar to FORGET compared to OFRGET (i.e. d<sub>1</sub> &gt; d<sub>2</sub>). This makes FOGRET easy to recognize as FORGET compared to OFRGET.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-fig1-v3.tif"/></fig><p>Here, we hypothesized that, viewing a string of letters activates a visual representation that is compared with the representation of stored words. To probe visual processing, we devised a visual search task in which subjects had to find an oddball target string among distractor strings. This task does not require any explicit reading and is driven by shape representations in visual cortex (<xref ref-type="bibr" rid="bib63">Sripati and Olson, 2010a</xref>; <xref ref-type="bibr" rid="bib74">Zhivago and Arun, 2014</xref>). An example visual search array containing two oddball targets is shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. It can be seen that finding OFRGET is easy among FORGET, whereas finding FOGRET is hard (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), showing that FOGRET is more visually similar to FORGET. This makes FOGRET easy to recognize as FORGET, whereas OFRGET is harder. Thus, the visual similarity of the jumbled words FOGRET and OFRGET to the original word FORGET (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) potentially explains why transposing the middle letters renders a word easier to read than transposing its edge letters. This example suggests that orthographic processing can potentially be explained by purely visual processing (as indexed by visual search) without invoking any linguistic factors. However, one must be careful since subjects may have been reading during visual search, thereby activating non-visual lexical or linguistic factors.</p><p>To overcome this confound, we asked whether visual search involving letter strings can be explained using a neurally plausible model containing only visual factors. We drew upon two well-established principles of object representations in high-level visual cortex. First, perceptually similar images elicit similar activity in single neurons (<xref ref-type="bibr" rid="bib43">Op de Beeck et al., 2001</xref>; <xref ref-type="bibr" rid="bib63">Sripati and Olson, 2010a</xref>; <xref ref-type="bibr" rid="bib74">Zhivago and Arun, 2014</xref>). Accordingly, we used visual search for single letters to create artificial neurons tuned for letters. Second, the neural response to multiple objects is an average of the individual object responses (<xref ref-type="bibr" rid="bib76">Zoccolan et al., 2005</xref>; <xref ref-type="bibr" rid="bib21">Ghose and Maunsell, 2008</xref>; <xref ref-type="bibr" rid="bib74">Zhivago and Arun, 2014</xref>). Accordingly, we created neural responses to letter strings as a linear sum of single letter responses. We define such responses as <italic>compositional</italic> because the response to wholes is explained by the parts. This stands in contrast to proposals for open bigram detectors (<xref ref-type="bibr" rid="bib26">Grainger and Whitney, 2004</xref>) and for local combination detectors (<xref ref-type="bibr" rid="bib14">Dehaene et al., 2005</xref>; <xref ref-type="bibr" rid="bib15">Dehaene et al., 2010</xref>) according to which reading is enabled by neurons tuned for higher order combinations of letters. Our model only assumes neurons tuned for letter shape and retinal position, as observed in high-level visual cortex (<xref ref-type="bibr" rid="bib32">Lehky and Tanaka, 2016</xref>). It does not capture any information about bigram or higher order detectors, or about other lexical or linguistic factors. We used this model to explain human performance on visual search as well as word recognition tasks. Finally, using brain imaging, we identified the neural substrates for both the letter code as well as subsequent lexical decisions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We performed six key experiments and several supporting experiments (reported in the Appendix). In Experiment 1, subjects performed visual search involving single letters, and we used this to construct artificial neurons tuned for letter shape. In Experiments 2–4, we show that search for longer strings can be predicted using these artificial neurons with a simple compositional rule. In Experiment 5, we show that this model also explains human performance on a commonly studied word recognition task. Finally, in Experiment 6, we measured brain activations during word recognition to elucidate the underlying neural representations.</p><sec id="s2-1"><title>Experiment 1: Single letter searches</title><p>In Experiment 1, subjects had to perform an oddball visual search task involving uppercase letters (n = 26), lowercase letters (n = 26) and digits (n = 10). An example search with two oddball targets is shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, illustrating how finding W is harder compared to finding T in an array of Ns. In the actual experiment, search arrays consisted of only one oddball target among 15 distractors, and subjects had to indicate the side of the screen (let/right) containing the target (see Materials and methods).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Single letter discrimination (Experiment 1).</title><p>(<bold>A</bold>) Visual search array showing two oddball targets (W and T) among many Ns. It can be seen that finding W is harder compared to finding T. The actual experiment comprised search arrays with only one oddball target among 15 distractors. (<bold>B</bold>) Visual search space for uppercase letters obtained by multidimensional scaling of observed dissimilarities. Nearby letters represent hard searches. Distances in this 2D plot are highly correlated with the observed distances (r = 0.82, p&lt;0.00005). Letter activations along the x-axis are taken as responses of Neuron 1 (<italic>blue</italic>), and along the y-axis are taken as Neuron 2 (<italic>red</italic>), etc. The tick marks indicate the response of each letter along that neuron. (<bold>C</bold>) Responses of Neuron 1 and Neuron 2 shown separately for each letter. Neuron 1 responds best to O, whereas Neuron 2 responds best to L. (<bold>D</bold>) Correlation between observed distances and MDS embedding as a function of number of MDS dimensions. The <italic>black</italic> line represents the split-half correlation with error bars representing s.d calculated across 100 random splits.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-fig2-v3.tif"/></fig><p>Subjects were highly consistent in their responses (split-half correlation between average search times of odd- and even-numbered subjects: r = 0.87, p&lt;0.00005). We calculated the reciprocal of search times for each letter pair which is a measure of distance between them (<xref ref-type="bibr" rid="bib2">Arun, 2012</xref>). These letter dissimilarities were significantly correlated with previously reported subjective dissimilarity ratings (Appendix 1).</p><p>Since shape dissimilarity in visual search matches closely with neural dissimilarity in visual cortex (<xref ref-type="bibr" rid="bib63">Sripati and Olson, 2010a</xref>; <xref ref-type="bibr" rid="bib74">Zhivago and Arun, 2014</xref>), we asked whether these letter distances can be used to reconstruct the underlying neural responses to single letters. To do so, we performed a multidimensional scaling (MDS) analysis, which finds the n-dimensional coordinates of all letters such that their distances match the observed visual search distances. In the resulting plot for two dimensions for uppercase letters (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), nearby letters correspond to small distances that is long search times. The coordinates of letters along a particular dimension can then be taken as the putative response of a single neuron. For example, the first dimension represents the activity of a neuron that responds strongest to the letter O and weakest to X (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Likewise the second dimension corresponds to a neuron that responds strongest to L and weakest to E (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We note that the same set of distances can be obtained from a different set of neural responses: a simple coordinate axis rotation would result in another set of neural responses with an equivalent match to the observed distances. Thus, the estimated activity from MDS represents one possible solution to how neurons should respond to individual letters so as to collectively produce behavior.</p><p>As expected, increasing the number of MDS dimensions led to increased match to the observed letter dissimilarities (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Taking 10 MDS dimensions, which explain nearly 95% of the variance, we obtained the single letter responses of 10 such artificial neurons. We used these single letter responses to predict their response to longer letter strings in all the experiments. Varying this choice yielded qualitatively similar results. Analogous results for all letters and numbers are shown in Appendix 1.</p></sec><sec id="s2-2"><title>Experiment 2: Bigram searches</title><p>Next, we proceeded to ask whether searches for longer strings can be explained using single letter responses. In Experiment 2, we asked subjects to perform oddball searches involving bigrams. We chose seven uppercase letters (A, D, H, I, M, N, T) and combined them in all possible ways to obtain 49 bigram stimuli. Subjects performed all possible pairs of <sup>49</sup>C<sub>2</sub> searches with one bigram as target and another as distractor (see Materials and methods). An example search is depicted in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. It can be seen that, finding TA among AT is harder than finding UT among AT. Thus, letter transpositions are more similar compared to letter substitutions, consistent with the classic results on reading (<xref ref-type="bibr" rid="bib41">Norris, 2013</xref>; <xref ref-type="bibr" rid="bib25">Grainger, 2018</xref>). To characterize the effect of bigram frequency, we included both frequent bigrams (e.g. IN, TH) and infrequent bigrams (e.g. MH, HH). As before, subjects were highly consistent in their performance (split-half correlation between odd and even-numbered subjects across all bigrams: r = 0.82, p&lt;0.00005).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Discrimination of strings is explained using single letters (Expts 2–4).</title><p>(<bold>A</bold>) Example search array with two oddball targets (UT and TA) among the bigram AT. It can be seen that UT is easier to find than TA, showing that letter substitution causes a bigger visual change compared to transposition. (<bold>B</bold>) Schematic diagram of how the bigram response is obtained from letter responses. Consider two neurons selective to single letters A, T and U. These letters can be represented in a 2D space in which the response to each neuron lies along one axis. For each neuron, we take the response to a bigram to be a weighted sum of the single letter responses. Thus, the bigram response lies along the line joining the two stimuli. Note that the bigrams AT and TA can be distinguished only if there is unequal summation. In the schematic, the first position is taken to have higher magnitude, as a result of which the response to AT is closer to A than to T. (<bold>C</bold>) Observed dissimilarities between bigram pairs plotted against predictions of the letter model for word-word pairs (<italic>red diamonds</italic>), frequent bigram pairs (<italic>blue circles</italic>) and all other bigram pairs (<italic>gray dots</italic>), for Experiment 2. Model correlation is shown at the top left, along with the data consistency for comparison. Asterisks indicate the statistical significance of the correlations (**** is p&lt;0.00005). (<bold>D</bold>) Average observed search reaction time for upright (dark) and inverted (pale) bigram searches for repeated letter pairs (AA-BB pairs) and transposed letter pairs (AB-BA pairs) in Experiment 3. Asterisks indicate statistical significance of the main effect of orientation in an ANOVA (see text for details; **** is p&lt;0.00005). (<bold>E</bold>) Mean modulation index of the summation weights, calculated as |w1-w2|/|w1+w2|, where w1 and w2 are the bigram summation weights, averaged across the 10 neurons in the letter model for upright (dark) and inverted (pale) bigrams. The asterisk indicates statistical significance calculated on a sign-rank test comparing the modulation index across 10 neurons (* is p&lt;0.05). (<bold>F</bold>) Observed dissimilarities between six-letter strings in visual search (Experiment 4) plotted against predicted dissimilarities from the single letter model for word-word pairs (<italic>red dots</italic>) and all other pairs (<italic>gray dots</italic>). Model correlation is shown at the top left with data consistency for comparison. Asterisks indicate statistical significance of the correlations (**** is p&lt;0.00005). (<bold>G</bold>) Cross-validated model correlation for the letter model (<italic>dark</italic>) and the Orthographic Levenshtein distance (OLD) model (<italic>light</italic>). For each model, the cross-validated correlation is the correlation between model predictions trained on one half of the data and the observed response times from the other half. The upper bound on model fits is the split-half correlation (r<sub>sh</sub>) shown in black with shaded error bars representing standard deviation across 1000 random splits. The asterisk indicates statistical significance of the comparison obtained by estimating the fraction of bootstrap samples in which the observed difference was violated (** is p&lt;0.005). (<bold>H</bold>) Cross-validated letter model correlation for word-word pairs and nonword-nonword pairs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-fig3-v3.tif"/></fig><p>Next, we asked whether bigram search performance can be explained using neurons tuned to single letters estimated from Experiment 1. The essential principle for constructing bigram responses is depicted in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. In monkey visual cortex, the response of single neurons to two simultaneously presented objects is an average of the single object responses (<xref ref-type="bibr" rid="bib76">Zoccolan et al., 2005</xref>; <xref ref-type="bibr" rid="bib74">Zhivago and Arun, 2014</xref>; <xref ref-type="bibr" rid="bib52">Pramod and Arun, 2018</xref>). This averaging can easily be biased through changes in divisive normalization (<xref ref-type="bibr" rid="bib21">Ghose and Maunsell, 2008</xref>). Therefore, we took the response of each neuron to a bigram to be a weighted sum of its responses to the constituent letters (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Specifically, the response of a neuron to the bigram AB is given by r<sub>AB</sub> = w<sub>1</sub>r<sub>A</sub> + w<sub>2</sub>r<sub>B</sub>, where r<sub>AB</sub> is the response to AB, r<sub>A</sub> and r<sub>B</sub> are its responses to the constituent letters A and B, and w<sub>1</sub>, w<sub>2</sub> are the summation weights reflecting the importance of letters A and B in the summation. Note that the model also does not incorporate any information specific to a particular bigram and is purely based on combining single letters. Note also that if w<sub>1</sub> = w<sub>2</sub>, the bigram response to AB and BA will be identical. Thus, discriminating letter transpositions necessarily requires asymmetric summation in at least one of the neurons.</p><p>To summarize, the letter model for bigrams has two unknown spatial weighting parameters for each of the 10 neurons, resulting in 2 × 10 = 20 free parameters. To calculate dissimilarities between a pair of bigrams, we calculated the Euclidean distance between the 10-dimensional response vectors corresponding to the two bigrams. The data collected in the experiment comprised dissimilarities (1/RT) from 1176 (<sup>49</sup>C<sub>2</sub>) searches involving all possible pairs of 49 bigrams. To estimate the model parameters, we optimized them to match the observed bigram dissimilarities using standard nonlinear fitting algorithms (see Materials and methods).</p><p>This letter model yielded excellent fits to the observed data (r = 0.85, p&lt;0.00005; <xref ref-type="fig" rid="fig3">Figure 3C</xref>). To assess whether the model explains all the systematic variance in the data, we calculated an upper bound estimated from the inter-subject consistency (see Materials and methods). This consistency measure (r<sub>data</sub> = 0.90) was close to the model fit, suggesting that the model captured nearly all the systematic variance in the data. As predicted in the schematic figure (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), the estimated spatial summation weights were unequal (absolute difference between w<sub>1</sub> and w<sub>2</sub>, mean ± sd: 0.07 ± 0.04). To assess whether this difference is statistically significant, we randomly shuffled the observed dissimilarities and estimated these weights. The absolute difference between shuffled weights was significantly smaller than for the original weights (average absolute difference: 0.03 ± 0.02; p&lt;0.005, sign-rank test across 10 neurons).</p><p>According to an influential account of word reading, specialized detectors are formed for frequently occurring combinations of letters (<xref ref-type="bibr" rid="bib14">Dehaene et al., 2005</xref>). If this were the case, searches involving frequent bigrams (e.g. TH, ND) or two letter words (e.g. AN, AM) should produce larger model errors compared to infrequent bigrams, since our model does not incorporate any bigram-selective units. Alternatively, if bigram discrimination was driven entirely by single letters, we should find no difference in errors. In keeping with this latter prediction, we observed no visually obvious difference in model fits for frequent bigram pairs or word-word pairs compared to other bigram pairs (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). To quantify this observation, we compared the model error (absolute difference between observed and predicted dissimilarity) for the 20 bigram pairs with the largest mean bigram frequency with the model error of the 20 pairs with the lowest mean bigram frequency. This too revealed no systematic difference (mean ± sd of residual error: 0.10 ± 0.08 for the 20 most frequent bigrams and words; 0.11 ± 0.09 for 20 least frequent bigrams; p=0.80, rank-sum test). Thus, model errors are not systematically different for frequent compared to infrequent bigram pairs. We conclude that bigram search can be explained entirely using single neurons tuned to single letters.</p></sec><sec id="s2-3"><title>Experiment 3: Upright versus inverted bigrams</title><p>In the letter model described above, the response to bigrams is a weighted sum of the single letter responses. As detailed earlier, a critical prediction of this model is that the response to transposed bigrams such as AB and BA will be different only if the summation weights are unequal. By contrast, repeated letter bigrams such as AA and BB will remain discriminable regardless of the nature of summation, since their response will be proportional to the respective single letter responses. Since reading expertise can modulate sensitivity to letter transpositions, we reasoned that familiarity might modulate the summation to make it more asymmetric. We therefore predicted that this would make transposed letter searches (with AB as target and BA as distractor, or vice-versa) easier to discriminate in a familiar upright orientation compared to the (unfamiliar) inverted orientation. By contrast, searches involving repeated letter bigrams (with AA as target and BB as distractor), which also have a change in two letters, will remain equally easy in both upright and inverted orientations.</p><p>We tested this prediction in Experiment 3 by asking subjects to perform searches involving upright and inverted bigrams (see Materials and methods). The essential findings are summarized in <xref ref-type="fig" rid="fig3">Figure 3D</xref>. As predicted, subjects discriminated repeated letter bigrams (AA-BB searches) equally well at both upright and inverted orientations, but were substantially faster at discriminating transposed letter pairs (AB-BA searches) in the upright orientation (<xref ref-type="fig" rid="fig3">Figure 3D</xref>; for detailed analyses see Appendix 2). We obtained similar results on comparing upright and inverted trigrams as well (Appendix 2). Correspondingly, we observed a larger difference in the model summation weights for upright compared to inverted bigrams (<xref ref-type="fig" rid="fig3">Figure 3E</xref>).</p><p>We conclude that familiarity leads to asymmetric spatial summation. We note, however, that this familiarity could be due to purely visual familiarity of the letters or due to linguistic factors, which we cannot distinguish in our study.</p></sec><sec id="s2-4"><title>Experiment 4: Generalization to longer strings</title><p>The above analyses show that the letter-based model explains dissimilarities in visual search between bigrams, which rarely contain valid words. We therefore wondered whether these results would extend to longer strings which form words. In Experiment 4, subjects performed visual search involving six-letter strings that were either valid compound words (e.g. FORGET, TEAPOT) or pseudowords (FORPOT, TEAGET). The single letter model yielded excellent fits to the data (<xref ref-type="fig" rid="fig3">Figure 3F</xref>). These fits were superior to a widely used measure of string similarity, the Orthographic Levenshtein Distance (OLD) model (<xref ref-type="fig" rid="fig3">Figure 3G</xref>). Importantly, the letter model fits were equivalent for both word-word pairs and nonword-nonword pairs (<xref ref-type="fig" rid="fig3">Figure 3H</xref>). These and other analyses are described in Appendix 3.</p><p>We performed several experiments to investigate this for other string lengths. Again, the letter model yielded excellent fits across all string lengths (Appendix 4). We also tested lowercase and mixed-case strings because word shape is thought to play a role when letters vary in size or have upward and downward deflections (<xref ref-type="bibr" rid="bib45">Pelli and Tillman, 2007</xref>). Even here, the letter model, without any explicit representation of overall word shape, was able to accurately predict most of the search performance. These results are detailed in Appendix 4.</p></sec><sec id="s2-5"><title>Estimating letter dissimilarities from string dissimilarities</title><p>The letter model described is neurally plausible and compositional, but is based on dissimilarities between letters presented in isolation. It could be that the representation of a letter within a bigram, although compositional, differs from its representation when seen in isolation. To explore these possibilities we developed an alternate model in which bigram dissimilarities can be predicted using a sum of (unknown) part dissimilarities at different locations. The resulting model, which we denote as the part sum model, yielded comparable fits to the data. It is completely equivalent to the letter model under certain conditions. Unlike the letter model which is nonlinear and could suffer from multiple local minima, the part sum model is linear and its parameters can be estimated uniquely using standard linear regression. Its complexity can be drastically reduced using simplifying assumptions without affecting model fits. These results are detailed in Appendix 5.</p></sec><sec id="s2-6"><title>Experiment 5: Lexical decision task</title><p>The above experiments show that discrimination of strings in visual search can be explained by neurons tuned for single letter shape with letter responses that combine linearly. Could the same shape representation drive reading behavior? We evaluated this possibility through two separate word recognition experiments.</p><p>In Experiment 5, we used a widely used paradigm for word recognition, a lexical decision task (<xref ref-type="bibr" rid="bib41">Norris, 2013</xref>; <xref ref-type="bibr" rid="bib25">Grainger, 2018</xref>), in which subjects have to indicate whether a string of letters is a word or not using a keypress. To develop a quantitative model of lexical decision times, we drew from models of lexical decision in which responses are thought to be based on accumulation of evidence toward or against word status (<xref ref-type="bibr" rid="bib55">Ratcliff et al., 2004</xref>; <xref ref-type="bibr" rid="bib56">Ratcliff and McKoon, 2008</xref>).</p><p>Consider what happens when we view the string ‘PENICL’, as opposed to the string ‘EPNCIL’ (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Since PENICL is visually more similar to the stored word ‘PENCIL’, it is more likely to be confused with a real word and will take longer to be adjudged a nonword. By contrast, the string ‘EPNCIL’ will take much less time to respond, since it is far away from any stored word (<xref ref-type="bibr" rid="bib17">Dufau et al., 2012</xref>; <xref ref-type="bibr" rid="bib72">Yap et al., 2015</xref>). Thus, we predict that the response time for a nonword will be inversely proportional to its distance to the nearest word (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). We also predict that this comparison will be affected by the strength of the stored word representation, such that matches to frequent words are easier. In other words, we predict that response times for nonwords will be inversely proportional to word frequency. Finally, by the same account, when we view the string ‘PENCIL’, the match to the stored word PENCIL takes no time (the distance being negligible) and the response is therefore dominated by word frequency. We tested these two predictions on the observed lexical decision times.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Lexical decision task behavior (Experiment 5).</title><p>(<bold>A</bold>) Schematic of visual word space, with one stored word (PENCIL) and two nonwords (PENICL and EPNCIL). We hypothesize that subjects would take longer to categorize a nonword when it is similar to a word, that is RT for PENICL would be larger than for EPNCIL. Thus, 1/RT would be proportional to this dissimilarity. Likewise we predicted that subjects would be faster to respond to frequent words which have a stronger stored representation. (<bold>B</bold>) Response times for words in the lexical decision task, sorted in descending order. The solid line represents the mean categorization time for words and the shaded bars represent s.e.m. Some example words are indicated using dotted lines. The split-half correlation between subjects (<italic>r<sub>sh</sub></italic>) is indicated on the top. (<bold>C</bold>) Cross-validated model correlation between observed and predicted word response times across all words for various models: log word frequency (<italic>blue</italic>), number of orthographic neighbors (<italic>orange</italic>), log mean bigram frequency (<italic>purple</italic>), log mean letter frequency (<italic>cyan</italic>) and a combined model containing all these factors (<italic>red</italic>). Shaded error bars indicate mean ± sd of the correlation across 1000 random splits of the observed data. The asterisk indicates statistical significance of the comparison obtained by estimating the fraction of bootstrap samples in which the observed difference was violated (* is p&lt;0.05, ** is p&lt;0.005). (<bold>D</bold>) Response times for nonwords in the lexical decision task, sorted in descending order. Conventions as in (<bold>A</bold>). (<bold>E</bold>) Observed reciprocal response times for nonwords in the lexical decision task plotted against letter model predictions fit to the full data (450 nonwords). Some example nonwords are depicted. (<bold>F</bold>) Percent change in response time (nonword-RT – word-RT)/word-RT for middle and edge letter transpositions and for middle and edge substitutions for observed data (<italic>left</italic>) and for letter model predictions (<italic>right</italic>). MS: middle substitution. In both cases, asterisks represent statistical significance comparing the means of the corresponding groups using a rank-sum test (* is p&lt;0.05, ** is p&lt;0.005, etc.). (<bold>G</bold>) Observed reciprocal response times plotted against the Orthographic Levenshtein Distance (OLD), a popular model for edit distance between strings. (<bold>H</bold>) Cross-validated model correlation between observed and predicted nonword RTs for the letter model, OLD model, lexical model and the combined neural+lexical model. Conventions are as in (<bold>B</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-fig4-v3.tif"/></fig><p>In this experiment, the words comprised four, five or six-letter words and the nonwords consisted of random strings and jumbled versions of the words (see Materials and methods). Subjects were highly accurate in responding to both words and nonwords (mean ± sd: 96 ± 2% for words, 95 ± 3% for nonwords). Importantly, their response times across words and nonwords were consistent between subjects as evidenced by a significant split-half correlation (correlation between odd- and even-numbered subjects: r = 0.59 for words, r = 0.73 for nonwords, p&lt;0.00005).</p><p>We started by characterizing response times for words. To depict the systematic variation in word response times, we plotted them in descending order (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Subjects took longer to respond to infrequent words like MALICE compared to frequent words like MUSIC. As predicted, response times for words showed a negative correlation with log word frequency (r = −0.5, p&lt;0.00005 across 450 words). We also estimated other lexical factors such as the logarithm of the letter frequency (averaged across letters of the string), logarithm of the bigram frequency (averaged across all bigrams in the string), and the number of orthographic neighbors (i.e. number of nearby words in the lexicon), which are standard measures in linguistic corpora (see Materials and methods).</p><p>To avoid overfitting, we trained a model based on each factor on one half of the subjects and tested it on the other half. This cross-validated performance is shown for all lexical factors in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. It can be seen that the word frequency is the best predictor of word response times (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). To assess whether all lexical factors together predict word response times any better, we fit a combined model in which the word response times are modeled as a linear sum of the four factors. The combined model performance was slightly better than the performance of the word frequency model alone (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). To assess the statistical significance of these results, we performed a bootstrap analysis. On each trial, we trained all models on the response times obtained from considering only one randomly chosen half of subjects. We calculated the correlation between each model’s predictions on the other half of the data, and repeated this procedure 1000 times. Across these samples, the word frequency model performance rarely fell below all other individual models (p&lt;0.005), but was slightly worse than the combined model (p&lt;0.05). We conclude that word response times are determined primarily by word frequency and to a lesser degree by letter frequency. We note that the dependence of word response times on word frequency is non-compositional, since it cannot be explained by letter frequency.</p><p>Next we characterized the nonword response times. The nonword responses are plotted in descending order in <xref ref-type="fig" rid="fig4">Figure 4D</xref>. Subjects took longer to respond to jumbled words like PENICL (original word: PENCIL) with fewer transpositions compared to VTAOCE (original word: OCTAVE) with more transpositions. To test whether nonword to word dissimilarity can predict nonword response times, we took the letter model with 10 neurons (with single letter tuning from visual seach) and its spatial summation weights to match the reciprocal of the nonword responses for each word length. We optimized the spatial summation weights based on our observation that summation weights varied across visual search experiments, and that this could reflect differing attentional resources across letter positions as required for each experiment. This model yielded excellent fits to the data (r = 0.70, p&lt;0.00005; <xref ref-type="fig" rid="fig4">Figure 4E</xref>) that were comparable to the data consistency (<italic>r<sub>data</sub></italic> = 0.84).</p><p>Importantly, this model was able to explain many classic phenomena in orthographic processing. Specifically, subjects took longer to respond to nonwords obtained by transposing a letter of a word, compared to nonwords obtained through letter substitution – these trends were present in the model predictions as well (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). Likewise, subjects took longer when the middle letters were transposed compared to when the edge letters were transposed – as did the model predictions (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). These effects replicate the classic orthographic processing effects reported across many studies (<xref ref-type="bibr" rid="bib24">Grainger et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Norris, 2013</xref>; <xref ref-type="bibr" rid="bib75">Ziegler et al., 2013</xref>; <xref ref-type="bibr" rid="bib25">Grainger, 2018</xref>).</p><p>Next we asked whether a widely used measure of orthographic distance could explain the same data. We selected the Orthographic Levenshtein Distance (OLD), in which the net distance between two strings is calculated as the minimum number of letter additions, transpositions and deletions required to transform one string into another. The OLD model yielded relatively poorer predictions of the data (r = 0.36, p&lt;0.00005; <xref ref-type="fig" rid="fig4">Figure 4G</xref>).</p><p>We compared the letter model with two alternate models: the OLD model and a model based on lexical factors. The OLD model is as described above. In the lexical model, the nonword response time is modeled as a linear sum of log word frequency, log mean bigram frequency of words, log mean bigram frequency of nonwords, # orthographic neighbors, log letter frequency. Since all three models have different numbers of free parameters, we compared their performance using cross-validation: we trained each model on one-half of the subjects and evaluated it on the other half of the subjects. The resulting cross-validated model fits are shown in <xref ref-type="fig" rid="fig4">Figure 4H</xref>. The letter model outperformed both the OLD model and the lexical model (model correlations: r = 0.56 ± 0.02, 0.33 ± 0.01 and 0.35 ± 0.01 for the neural, OLD and lexical models; fraction of bootstrap samples with neural &lt;other models: p&lt;0.005; <xref ref-type="fig" rid="fig4">Figure 4H</xref>). To be absolutely certain that the superior fit of the letter model was not simply due to having more free parameters, we compared the lexical model with a reduced version of the letter model with only five free parameters (SID model; Appendix 5). Even this reduced model yielded fits were better than the lexical model (SID model correlation: r = 0.48 ± . 02). To assess whether the model trained on visual search data would also be able to predict nonword response times, we took the model trained on the visual search data in Experiment 4, and calculated the word-nonword distances using this model. This too yielded a significant positive correlation (r = 0.39, p&lt;0.00005) that was better than the OLD and lexical models. Finally, a combined model – in which the neural and lexical model predictions were linearly combined – proved to explain more variance than either model (<xref ref-type="fig" rid="fig4">Figure 4H</xref>).</p><p>In sum, we conclude that word response times are explained primarily by word frequency and nonword response times are explained primarily by the distance between the nonword and the nearest word calculated using the compositional neural code.</p><p>As a further test of the ability of this compositional code to explain word reading, we performed an additional experiment in which subjects had to recognize the identity of a jumbled word. Here too, response times were explained best by the letter model compared to lexical and OLD models (Appendix 6).</p></sec><sec id="s2-7"><title>Experiments 6–7: Neural correlates of lexical decisions</title><p>The above results show that visual discrimination of strings can be explained using a letter-based compositional neural code, and that dissimilarities calculated using this code can explain human performance on nonwords during lexical decision tasks. Here, we sought to uncover the brain regions that represent this code and guide eventual lexical decisions. In Experiment 6, we recorded BOLD responses using fMRI while subjects performed a lexical decision task.</p><p>Since lexical decision times for nonwords can be predicted using perceptual dissimilarity, we performed a separate experiment to directly estimate perceptual dissimilarities using visual search (Experiment 7; see Materials and methods). Additionally, to compare semantic representations in different ROIs, we estimated the semantic dissimilarity by calculating the cosine distance between GloVe (<xref ref-type="bibr" rid="bib47">Pennington et al., 2014</xref>) feature vectors between word pair (see Materials and methods). Importantly, the perceptual and semantic dissimilarities were uncorrelated (r = 0.03, p=0.55), thereby allowing us to identify regions with distinct or overlapping perceptual/semantic representations. The perceptual and semantic representations are visualized in Appendix 7.</p><p>We identified several possible regions of interest (ROIs) using a combination of functional localizers and anatomical considerations (see Materials and methods). These included the early and mid-level visual areas (V1-V3 and V4), the object-selective lateral occipital region (LO), and two language areas: the visual word form area (VWFA) which selectively responds to words and a broad region in the temporal gyrus reading network (TG). Except for VWFA, all other ROIs were bilateral. The inflated brain map of a representative subject with these ROIs is shown in <xref ref-type="fig" rid="fig5">Figure 5A</xref>.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Lexical task fMRI (Experiment 6).</title><p>(<bold>A</bold>) ROIs for an example subject, showing V1–V3 (cyan), V4 (blue), LO (yellow), VWFA (red) and TG (maroon). (<bold>B</bold>) Example difference between perceptual and semantic spaces. In perceptual space, the representation of TRAIL is closer to its visual similar counterpart TRIAL, whereas in semantic space, its representation is closer to its synonym PATH. (<bold>C</bold>) Correlation between neural dissimilarity in each ROI with perceptual dissimilarity between strings measured using visual search (Experiment 7). Error bars indicate standard deviation of the correlation between the group perceptual dissimilarity and ROI dissimilarities calculated repeatedly by resampling of dissimilarity values with replacement across 1000 iterations. Asterisks along the length of each bar indicate statistical significance of the correlation between group behavior and group ROI dissimilarity (** is p&lt;0.005 across 1000 bootstrap samples). Horizontal lines indicate the fraction of bootstrap samples in which the observed difference was violated (* is p&lt;0.05, ** is p&lt;0.005, etc.). All significant comparisons are indicated. (<bold>D</bold>) Correlation between neural dissimilarity in each ROI with semantic dissimilarity for words. Other details are same as in (<bold>C</bold>). (<bold>E</bold>) Correlation between mean VWFA activity (averaged across subjects and voxels) with mean lexical decision time for both words (purple circles) and nonwords (green squares). Each point corresponds to one string and example word and nonword is highlighted. Asterisks indicate statistical significance (**** is p&lt;0.00005). (<bold>F</bold>) Correlation between lexical decision time and mean activity within each ROI separately for words and nonwords. Error bars indicate standard deviation across 1000 bootstrap splits. Asterisks indicate statistical significance (** is p&lt;0.005).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-fig5-v3.tif"/></fig><p>In the event-related runs, subjects had to make a response on each trial to indicate whether a string displayed on the screen was a word or not. A total of 64 five-letter strings (32 words and 32 nonwords formed using 10 single letters) were shown. Subjects also viewed the 10 single letters, to which they had to make no response. Subjects were highly accurate (mean ±std of accuracy: 94 ± 4%) and showed consistent response time variations (split-half correlation between odd and even subjects: r<sub>sh</sub> = 0.54 and 0.79 for words and nonwords, p&lt;0.00005). As before, the lexical decision time for words was negatively correlated with word frequency (r = −0.42, p&lt;0.05). Likewise, the lexical decision times for nonwords were strongly correlated with the word-nonword dissimilarity measured in visual search in Experiment 7 (r = −0.68, p&lt;0.00005). These results reconfirm the findings of the previous experiment performed outside the scanner.</p><p>We then compared the overall brain activation levels for words, nonwords and letters in each ROI. While V4 showed greater activation for words compared to nonwords, VWFA and TG regions showed greater activation to nonwords compared to words, presumably reflecting greater engagement to discriminate nonwords that are highly similar to words (Appendix 7). Although the visual regions did not show differential overall activations, there could still be differential activation at the population level for words and nonwords. This revealed above-chance decoding in all ROIs, and better separation between words and substituted compared to transposed nonwords, matching the trend observed in behavior (Appendix 7).</p></sec><sec id="s2-8"><title>Neural basis of perceptual space</title><p>Next, we sought to compare the neural representations in each ROI with perceptual and semantic representations. The perceptual and semantic representations can be quite distinct, as depicted in <xref ref-type="fig" rid="fig5">Figure 5B</xref>: in perceptual space, TRAIL and TRIAL can be quite similar since one is obtained from the other by transposing letters, but the word PATH is distinct. By contrast, in semantic space, TRAIL and PATH have similar meanings and usage whereas TRIAL is distinct. Indeed, perceptual and semantic dissimilarities across words were uncorrelated for the words used in this experiment (r = 0.03, p=0.55).</p><p>To investigate these issues, we calculated the neural dissimilarity for each ROI between a given pair of stimuli as the cross-validated Mahalanobis distance between the voxel-wise activations evoked by the two stimuli. We selected this distance metric because it prioritizes the more reliable voxels. The cross-validation procedure calculates Euclidean distances by multiplying activations across runs to avoid bias due to noise. We then averaged this dissimilarity across subjects to get an average neural dissimilarity for that ROI. We then compared this neural dissimilarity in each ROI with perceptual dissimilarities estimated from visual search. This match to perceptual dissimilarity is shown in <xref ref-type="fig" rid="fig5">Figure 5C</xref>. Among the ROIs tested, only the LO dissimilarities showed a significant correlation (correlation between 1024 pairwise dissimilarities involving <sup>32</sup>C<sub>2</sub> words, <sup>32</sup>C<sub>2</sub> nonwords, and 32 word-nonword pairs: r = 0.16, p&lt;0.00005; <xref ref-type="fig" rid="fig5">Figure 5C</xref>). A searchlight analysis confirmed that the match to perceptual dissimilarities was strongest in a region centred around the bilateral LO region (Appendix 7). Thus, neural dissimilarity in the LO region match best with the perceptual dissimilarities observed in visual search. We therefore conclude that LO is the likely neural substrate for the compositional letter code.</p><p>To further investigate the link between the compositional letter code and the LO representation, we performed several additional analyses. First, we asked whether the neural activation of each voxel in LO could be explained using a linear sum of the single letter activations. Indeed, model fits were comparable for words and nonwords (Appendix 7). This parallels our finding that dissimilarity in visual search was predicted equally well for word-word and nonword-nonword pairs (<xref ref-type="fig" rid="fig3">Figure 3H</xref>). Second, we confirmed that both the neural tuning for single letters, and the summation weights estimated from the behavioral data in the letter model were qualitatively similar to their counterparts estimated from voxel activations in LO (Appendix 7).</p><p>In sum, we conclude that the LO region is the likely neural substrate for the compositional letter code predicted from behavior.</p></sec><sec id="s2-9"><title>Neural basis of semantic space</title><p>Next we compared neural representations in each ROI to semantic space. The match to semantic space was significant only in the LO and TG regions (correlation between 496 pairwise dissimilarities between words: r = 0.18 ± 0.05 for LO, 0.22 ± 0.04 for TG; <xref ref-type="fig" rid="fig5">Figure 5D</xref>). A searchlight analysis confirmed that semantic dissimilarities were best correlated with the TG region with additional peaks in prefrontal and motor regions (Appendix 7).</p><p>The above analysis shows that neural activations in LO are correlated with both perceptual and semantic dissimilarities, but these correlations cannot be directly compared since they are based on different pairs of stimuli. To investigate whether the neural representation in LO matches better with perceptual or semantic space, we compared the match for word-word pairs alone. This revealed no significant difference between the two correlations (r = 0.16 ± . 04 for LO with visual search, r = 0.16 ± 0.05 for LO with semantic dissimilarites; p=0.49 across 1000 bootstrap samples). To confirm that there is no shared variance between the perceptual and semantic space correlation, we calculated the partial correlation between neural dissimilarities in LO for word-word pairs and the perceptual dissimilarities after factoring out the dependence on semantic dissimilarities (or vice-versa). As expected, both partial correlations were significant (partial correlations: r = 0.13, p&lt;0.005 with perceptual space; r = 0.17, p&lt;0.0005 with semantic space). We conclude that both LO and TG regions represent semantic space.</p></sec><sec id="s2-10"><title>Neural basis of lexical decisions</title><p>If the LO region represents each string (word or nonword) using a compositional code, then according to the preceding experiments, lexical decisions for words and nonwords must involve some comparison with stored word representations. Recall that lexical decision times for words are correlated with word frequency, and lexical decision times for nonwords are correlated with word-nonword dissimilarity. We therefore asked whether these lexical decision times are correlated with the average activity (across voxels and subjects) in a given ROI. The resulting correlations are shown in <xref ref-type="fig" rid="fig5">Figure 5F</xref>. Across the ROIs, only the VWFA showed a consistently positive correlation with lexical decision times for both words and nonwords (r = 0.52, p&lt;0.005 for words; r = 0.47, p&lt;0.05 for nonwords, <xref ref-type="fig" rid="fig5">Figure 5E</xref>). A searchlight analysis confirmed that there was indeed a peak in the correlation with lexical decision times centred on the VWFA, with additional peaks in the parietal and frontal regions (Appendix 7). Interestingly, VWFA activations were larger for nonwords compared to words (mean ± std of VWFA activations across subjects: 1.46 ± 0.22 for words, 2.03 ± 0.28 for nonwords; p&lt;0.005, signed-rank test across 17 subject activations). However, activations were similar for transposed nonwords compared to substituted words (mean ±std VWFA activations across subjects: 1.42 ± 0.33 for transposed nonwords, 1.38 ± 0.33 for substituted nonwords; p=0.62, signed-rank test). We conclude that lexical decisions are driven by the VWFA.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we investigated whether jumbled word reading can be explained using a purely visual representation. We have two major findings. First, we show that a compositional neural code explains visual search for string and responses to nonwords during reading tasks including many orthographic processing phenomena. Second, when subjects performed a lexical decision task, neural dissimilarities in the LO region matched best with perceptual dissimilarities, and lexical decision times were correlated with the activation of the visual word form area (VWFA). This suggests that viewing a string of letters activates a compositional neural code in LO that is subsequently matched with stored word representations in the VWFA. Below we discuss these findings in relation to the existing literature.</p><sec id="s3-1"><title>Relation to models of reading</title><p>Our compositional letter code stands in stark contrast to existing models of reading. Existing models of reading assume explicit encoding of letter position and do not account for letter shape (<xref ref-type="bibr" rid="bib23">Gomez et al., 2008</xref>; <xref ref-type="bibr" rid="bib13">Davis, 2010</xref>; <xref ref-type="bibr" rid="bib42">Norris and Kinoshita, 2012</xref>; <xref ref-type="bibr" rid="bib41">Norris, 2013</xref>). By contrast, our model encodes letter shape explicitly and position implicitly through asymmetric spatial summation. The implicit coding of letter position avoids the complication of counting transpositions (<xref ref-type="bibr" rid="bib73">Yarkoni et al., 2008</xref>; <xref ref-type="bibr" rid="bib72">Yap et al., 2015</xref>). Our model can thus easily be extended to any language by simply estimating letter dissimilarities using visual search and then estimating the unknown summation weights from visual search for longer strings.</p><p>Unlike existing models of reading, our compositional letter code is neurally plausible and grounded in well-known principles of object representations. The first principle is that images that elicit similar activity across neurons in high-level visual cortex will appear perceptually similar (<xref ref-type="bibr" rid="bib43">Op de Beeck et al., 2001</xref>; <xref ref-type="bibr" rid="bib63">Sripati and Olson, 2010a</xref>; <xref ref-type="bibr" rid="bib74">Zhivago and Arun, 2014</xref>). This is non-trivial because it is not necessarily true in lower visual areas or in image pixels (<xref ref-type="bibr" rid="bib54">Ratan Murty and Arun, 2015</xref>). We have turned this principle around to construct artificial neurons whose shape tuning matches visual search. The second principle is that the neural response to multiple objects is typically the average of the individual object responses (<xref ref-type="bibr" rid="bib76">Zoccolan et al., 2005</xref>; <xref ref-type="bibr" rid="bib64">Sripati and Olson, 2010b</xref>) that can be biased toward a weighted sum (<xref ref-type="bibr" rid="bib21">Ghose and Maunsell, 2008</xref>; <xref ref-type="bibr" rid="bib6">Bao and Tsao, 2018</xref>). Finally, we note that our letter code assumes no explicit calculations of letter position in a word, since the neurons in our model only need to be tuned for retinal position. We speculate that these neurons may be tuned not only to retinal position but also to the relative size and position of letters, as observed in high-level visual cortex (<xref ref-type="bibr" rid="bib63">Sripati and Olson, 2010a</xref>; <xref ref-type="bibr" rid="bib70">Vighneshvel and Arun, 2015</xref>).</p></sec><sec id="s3-2"><title>Relation to theories of word recognition</title><p>We have found that lexical decisions for nonwords are driven by the dissimilarity between the viewed string and the nearest word. This idea is consistent with the well-known Interactive Activation model (<xref ref-type="bibr" rid="bib36">McClelland and Rumelhart, 1981</xref>; <xref ref-type="bibr" rid="bib60">Rumelhart and McClelland, 1982</xref>), where viewing a string activates the nearest word representation. However, the Interactive Activation model does not explain lexical decisions or scrambled word reading, and also does not integrate letter shape and position into a unified code. Our findings are consistent with previous work showing that nonword responses are influenced by the number of orthographic neighbors (<xref ref-type="bibr" rid="bib72">Yap et al., 2015</xref>). Likewise, we found word frequency to be a major factor influencing lexical decisions, in keeping with previous work (<xref ref-type="bibr" rid="bib55">Ratcliff et al., 2004</xref>; <xref ref-type="bibr" rid="bib17">Dufau et al., 2012</xref>; <xref ref-type="bibr" rid="bib72">Yap et al., 2015</xref>). We note also that personal familiarity with words, as opposed to the word frequency estimated from text corpora, might also influence lexical decisions (<xref ref-type="bibr" rid="bib12">Colombo et al., 2006</xref>; <xref ref-type="bibr" rid="bib31">Kuperman and Van Dyke, 2013</xref>). We have gone further to demonstrate a unified letter-based code that integrates letter shape and position, and localized the underlying neural substrates of the letter code to the LO region, and the comparison process to the VWFA. We propose that the compositional shape code provides a quick match to unscramble a word, failing which subjects may initiate more detailed symbolic manipulation.</p><p>The success of our letter code challenges the widely held belief that efficient visual processing of letter strings requires higher-order detectors for letter combinations (<xref ref-type="bibr" rid="bib26">Grainger and Whitney, 2004</xref>; <xref ref-type="bibr" rid="bib14">Dehaene et al., 2005</xref>; <xref ref-type="bibr" rid="bib16">Dehaene et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Grainger, 2018</xref>). The presence of these specialized detectors should have caused larger model errors for valid words and frequent n-grams, but we observed no such trend (<xref ref-type="fig" rid="fig3">Figure 3</xref>). However, it is possible that there are combination detectors in subsequent stages where multiple letters have to activate single syllables. So what happens to visual letter representations upon expertise with reading? Our comparison of upright and inverted bigrams suggests that reading should increase letter discrimination and increase the asymmetry of spatial summation (<xref ref-type="fig" rid="fig3">Figure 3D,E</xref>). This is consistent with our recent finding that reading makes words more predictable from letters (<xref ref-type="bibr" rid="bib1">Agrawal et al., 2019</xref>). It is also consistent with differences in letter position effects for symbols and letters (<xref ref-type="bibr" rid="bib11">Chanceaux and Grainger, 2012</xref>; <xref ref-type="bibr" rid="bib61">Scaltritti et al., 2018</xref>). We propose that both processes may be driven by visual exposure: repeated viewing of letters makes them more discriminable (<xref ref-type="bibr" rid="bib37">Mruczek and Sheinberg, 2005</xref>), while viewing letter combinations induces asymmetric spatial weighting or increased separability. Whether these effects require active discrimination such as letter-sound association training or can be induced even by passive viewing will require comparing letter string discrimination under these paradigms.</p></sec><sec id="s3-3"><title>Neural basis of word recognition</title><p>Our results elucidate the neural representations that guide lexical decision in several ways. First, we found that perceptual dissimilarities between strings, regardless of word/nonword status, matched best with neural representations in the LO region (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). This is consistent with similar findings using letters (<xref ref-type="bibr" rid="bib1">Agrawal et al., 2019</xref>) and natural objects (<xref ref-type="bibr" rid="bib29">Khaligh-Razavi and Kriegeskorte, 2014</xref>).</p><p>Second, we have found that semantic dissimilarities between words matched both with temporal gyrus regions as well as with LO (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). The former finding is consistent with temporal gyrus regions participating in the reading network (<xref ref-type="bibr" rid="bib20">Friederici and Gierhan, 2013</xref>), while the latter is concordant with other semantic properties such as animacy encoded in LO (<xref ref-type="bibr" rid="bib8">Bracci and Op de Beeck, 2016</xref>; <xref ref-type="bibr" rid="bib53">Proklova et al., 2016</xref>; <xref ref-type="bibr" rid="bib68">Thorat et al., 2019</xref>). Whether these semantic properties are encoded directly by LO or are a consequence of feedback from language/semantic areas can be distinguished using methods with higher temporal resolution such as MEG or intracranial recordings.</p><p>Third, our results confirm and extend our understanding of the VWFA. We found a striking correlation between lexical decision times for words as well as nonwords in the VWFA (<xref ref-type="fig" rid="fig5">Figure 5E</xref>), suggesting that it is involved in comparing the viewed string with stored words. The finding that VWFA activity is positively correlated with word response times (which reflect word frequency as shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref>) is consistent with previous studies showing that VWFA activity shows weak activity for frequent words (<xref ref-type="bibr" rid="bib30">Kronbichler et al., 2004</xref>; <xref ref-type="bibr" rid="bib71">Vinckier et al., 2007</xref>). The finding that VWFA activity is correlated with nonword response times (which reflect perceptual distance to the corresponding word, as shown in <xref ref-type="fig" rid="fig5">Figure 5E</xref>), is consistent with observations that VWFA is modulated by orthographic similarity to words (<xref ref-type="bibr" rid="bib71">Vinckier et al., 2007</xref>; <xref ref-type="bibr" rid="bib3">Baeck et al., 2015</xref>). Finally, our finding that VWFA activations were stronger for nonwords compared to words (<xref ref-type="fig" rid="fig5">Figure 5E</xref>), has also been observed recently (<xref ref-type="bibr" rid="bib7">Bouhali et al., 2019</xref>). While this might seem paradoxical considering its status as a word form area, the higher activity for nonwords is likely due to many of them being perceptually similar to words, making the lexical decision difficult. That VWFA is activated strongly for hard lexical decisions is also concordant with its higher activation for inverted compared to upright words while making lexical decisions (<xref ref-type="bibr" rid="bib10">Carlos et al., 2019</xref>).</p><p>Fourth, our results point a way to resolve contradictory findings regarding VWFA in the literature. Some studies have reported equal activity in VWFA for words and nonwords (<xref ref-type="bibr" rid="bib4">Baker et al., 2007</xref>), and others have reported higher activity for word-like stimuli (<xref ref-type="bibr" rid="bib71">Vinckier et al., 2007</xref>; <xref ref-type="bibr" rid="bib22">Glezer et al., 2009</xref>) – but these observations have been made while subjects performed tasks orthogonal to reading. There have been surprisingly few studies of VWFA activations during word processing tasks (<xref ref-type="bibr" rid="bib3">Baeck et al., 2015</xref>; <xref ref-type="bibr" rid="bib66">Sussman et al., 2018</xref>; <xref ref-type="bibr" rid="bib7">Bouhali et al., 2019</xref>; <xref ref-type="bibr" rid="bib10">Carlos et al., 2019</xref>). By comparing brain activations directly with behavioral responses during a lexical decision task, we found an interesting functional dissociation whereby orthographic (perceptual) similarity between strings was encoded not by VWFA but by LO (<xref ref-type="fig" rid="fig5">Figure 5C</xref>) and lexical decisions were encoded by VWFA and not LO (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). This finding implies that most orthographic processing phenomena are driven by compositional neural representations in LO, rather than by the VWFA. These findings are consistent with recent intracranial EEG recordings that report a progression from early to late, or letter-level to word-level representations along the ventral occipitotemporal cortex regions (<xref ref-type="bibr" rid="bib67">Thesen et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Hirshorn et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">Lochy et al., 2018</xref>). We suggest that fine-grained comparisons between brain activations and behavior will elucidate the roles of the many cortical areas involved in reading.</p></sec><sec id="s3-4"><title>Does the compositional letter code explain orthographic processing?</title><p>Our letter code explains many orthographic processing phenomena reported in the literature. Its integrated representation of both letter shape and position explains both letter transposition and substitution effects and their relative importance (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). Its asymmetric spatial weighting favoring the first letter (Appendix 3), explains the first-letter advantage observed previously (<xref ref-type="bibr" rid="bib61">Scaltritti et al., 2018</xref>). It also explains why increasing letter spacing can benefit reading in poor readers, presumably because it increases asymmetry in spatial summation (<xref ref-type="bibr" rid="bib77">Zorzi et al., 2012</xref>).</p><p>To elucidate how various jumbled versions of a word are represented according to this neural code, we calculated responses of the letter model trained on data from Experiment 4, and visualized the distances using multidimensional scaling (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). It can be seen transposing the edge letters (OFRGET) results in a bigger change than transposing the middle letters (FOGRET), thus explaining many transposed letter effects (<xref ref-type="bibr" rid="bib41">Norris, 2013</xref>). Likewise, it can be seen that substituting a dissimilar letter (FORXET) leads to a large change compared to substituting a similar letter (FORCET). Replacing G with C in FORGET leads to a smaller change than replacing with X, thus explaining how priming is stronger when similar letters are substituted (<xref ref-type="bibr" rid="bib35">Marcet and Perea, 2017</xref>). Finally, the letter subset FRGT is closer to FORGET than the same letters reversed (TGRF), thereby explaining subset priming (<xref ref-type="bibr" rid="bib26">Grainger and Whitney, 2004</xref>; <xref ref-type="bibr" rid="bib14">Dehaene et al., 2005</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Predicting reading difficulty using the letter model.</title><p>(<bold>A</bold>) Visual word space predicted by the letter model for a word (FORGET) and its jumbled versions. Letter model predictions were based on training the model on compound words (Experiment 4). The plot was obtained by performing multidimensional scaling on the pairwise dissimilarities between strings predicted by the letter model. It can be seen that classic features of orthographic processing are captured by the letter model, including priming effects such as FRGT (<italic>green</italic>) being more similar to FORGET than TGRF (<italic>red</italic>). (<bold>B</bold>) The letter model can be used to sort jumbled words by their reading difficulty, allowing us to create any desired reading difficulty profile along a sentence. <italic>Top row</italic>: Sentence with increasing reading difficulty. <italic>Middle row</italic>: sentence with fluctuating reading difficulty. <italic>Bottom row</italic>: sentence with decreasing reading difficulty. (<bold>C</bold>) The letter model yields a composite measure of reading difficulty that combines letter substitution and transposition effects. Sentences with digit substitutions (<italic>second row</italic>) can thus be placed along a continuum of reading difficulty relative to other sentences (<italic>first, third and fourth rows</italic>) with increasing degree of scrambling.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-fig6-v3.tif"/></fig><p>Finally, as a powerful demonstration of this code, we used it to arbitrarily manipulate reading difficulty along a sentence (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), or across multiple transpositions and even number substitutions (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). We propose that this compositional neural code can serve as a powerful baseline for the purely visual shape-based representation triggered by viewing words, thereby enabling the study of higher order linguistic influences on reading processes.</p></sec><sec id="s3-5"><title>Relation between word recognition and reading sentences</title><p>Our results constitute an important first step in understanding how we read single words, but reading sentences is much more complex, with potentially many words sampled with each eye movement (<xref ref-type="bibr" rid="bib58">Rayner, 1998</xref>). Our ability to sample multiple letters or words at a single glance is limited by two factors. The first is our visual acuity, which reduces with eccentricity. The second is crowding, by which letters become unrecognizable when flanked by other letters – this effect increases with eccentricity (<xref ref-type="bibr" rid="bib46">Pelli and Tillman, 2008</xref>).</p><p>The visual search experiments in our study involved searching for an oddball target (consisting of multiple letters) among multiple distractors. This would most certainly have involved detecting and making saccades to peripheral targets. By contrast, the word recognition tasks in our study involved subjects looking at words presented at the fovea. Our finding that visual search dissimilarity explains word recognition then implies that shape representations are qualitatively similar in the fovea and periphery. Furthermore, the structure of the letter model suggests a possible mechanistic explanation for crowding. Neural responses might show greater sensitivity to spatial location at the fovea compared to the periphery, leading to more discriminable representations of multiple letters. Alternatively, neural responses to multiple letters might be more predictable from single letters at the fovea but not in the periphery. Both possibilities would predict reduced recognition with closely spaced flankers. Distinguishing these possibilities will require testing neural responses in higher visual areas to single letters and multi-letter strings of both familiar and unfamiliar scripts. Ultimately understanding reading fully will require not only asking how letters combine to form words but also how words combine to form larger units of meaning (<xref ref-type="bibr" rid="bib44">Pallier et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Nelson et al., 2017</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All subjects had normal or corrected-to-normal vision and gave informed consent to an experimental protocol approved by the Institutional Human Ethics Committee of the Indian Institute of Science (IHEC # 6–15092017). All subjects were fluent English-speaking students at the institute, where English is the medium of instruction. All subjects were multi-lingual and knew at least one other Indian language apart from English.</p><sec id="s4-1"><title>Experiment 1 – Single letter searches</title><sec id="s4-1-1"><title>Procedure</title><p>A total of 16 subjects (eight males, 24.4 ± 2.5 years) participated in this experiment. Subjects were seated comfortably in front of a computer monitor placed ~60 cm away under the control of custom programs written in Psychtoolbox (<xref ref-type="bibr" rid="bib9">Brainard, 1997</xref>) and MATLAB. In all experiments, we selected sample sizes based on our previous studies which yielded highly consistent data (<xref ref-type="bibr" rid="bib1">Agrawal et al., 2019</xref>).</p></sec><sec id="s4-1-2"><title>Stimuli</title><p>Single letter images were created using the Arial font. There were 62 stimuli in all comprising 26 uppercase letters (A-Z), 26 lowercase letters (a-z), and 10 digits (0–9). Uppercase stimuli were scaled to have a height of 1°.</p></sec><sec id="s4-1-3"><title>Task</title><p>Subjects were asked to perform an oddball search task without any constraints on eye movements. Each trial began with a fixation cross shown for 0.5 s followed by a 4 × 4 search array (measuring 40° by 25°). The search array always contained only one oddball target with 15 identical distractors. Subject were instructed to locate the oddball target as quickly and as accurately as possible, and respond with a key press (‘Z’ for left, ‘M’ for right). A red line divided the screen in two halves. The search display was turned off after the response or after 10 s, whichever was sooner. All stimuli were presented in white against a black background. Incorrect or missed trials were repeated after a random number of other trials. Subjects completed a total of 3782 correct trials (<sup>62</sup>C<sub>2</sub> letter pairs x two repetitions with either letter as target once). For each search pair, the oddball target appeared equally often on the left and right sides so as to avoid creating any response bias. Only correct responses were considered for further analysis. The main experiment was preceded by 20 practice trials involving unrelated stimuli.</p></sec><sec id="s4-1-4"><title>Data analysis</title><p>Subjects were highly accurate on this task (mean ±std: 98 ± 1%). Outliers in the reaction times were removed using built-in routines in MATLAB (<italic>isoutlier</italic> function, MATLAB R2018a). This function removes any value greater than three scaled absolute deviations away from the median, and was applied to each search pair separately. This step removed 6.8% of the response time data, but we obtained qualitatively similar results without this step.</p></sec></sec><sec id="s4-2"><title>Estimation of single letter tuning using multidimensional scaling</title><p>To estimate neural responses to single letters from the visual search data, we used a multidimensional scaling (MDS) analysis. We first calculated the average search time for each letter pair by averaging across subjects and trials. We then converted this search time (RT) into a distance measure by taking its reciprocal (1/RT). This is a meaningful measure because it represents the underlying rate of evidence accumulation in visual search (<xref ref-type="bibr" rid="bib65">Sunder and Arun, 2016</xref>), behaves like a mathematical distance metric (<xref ref-type="bibr" rid="bib2">Arun, 2012</xref>) and combines linearly with a variety of factors (<xref ref-type="bibr" rid="bib50">Pramod and Arun, 2014</xref>; <xref ref-type="bibr" rid="bib51">Pramod and Arun, 2016</xref>; <xref ref-type="bibr" rid="bib65">Sunder and Arun, 2016</xref>). Next, we took all pairwise distances between letters and performed MDS to embed letters into n dimensions, where we varied n from 1 to 15. This yielded n-dimensional coordinates corresponding to each letter, whose distances matched best with the observed distances. We then took the activation of each letter along a given dimension as the response of a single neuron. Throughout we performed MDS embedding into 10 dimensions, resulting in single letter responses of 10 neurons. We obtained qualitatively similar results on varying this number of dimensions.</p></sec><sec id="s4-3"><title>Estimation of data reliability</title><p>To obtain upper bounds on model performance, we reasoned that any model can predict the data as well as the consistency of the data itself. Thus, a model trained on one half of the subjects can only predict the other half as well as the split-half correlation <italic>r<sub>sh</sub></italic>. This process was repeated 100 times to obtain the mean and standard deviation of the split-half correlation. However, when a model is trained on all the data, the upper bound will be larger than the split-half correlation. We obtained this upper bound, which represents the reliability of the entire data (<italic>r<sub>data</sub></italic>) by applying a Spearman-Brown correction on the split-half correlation, as given by <italic>r<sub>data</sub> = 2r<sub>sh</sub>/(r<sub>sh</sub>+1)</italic>.</p></sec><sec id="s4-4"><title>Experiment 2 – Bigram searches</title><p>A total of eight subjects (five male, aged 25.6 ± 2.9 years) took part in this experiment. We chose seven uppercase letters (A, D, H, I, M, N, T) and combined them in all possible ways to obtain 49 bigram stimuli. These letters were chosen to maximize the number of two-letter words for example HI, IT, IN, AN, AM, AT, AD, AH, and HA. Letters measured 3° along the longer dimension. Subjects completed 2352 correct trials (<sup>49</sup>C<sub>2</sub> search pairs x two repetitions). All other details were identical to Experiment 1. Letter/Bigram frequencies were obtained from an online database (<ext-link ext-link-type="uri" xlink:href="http://norvig.com/mayzner.html">http://norvig.com/mayzner.html</ext-link>).</p><sec id="s4-4-1"><title>Data analysis</title><p>Subjects were highly accurate on this task (mean ±std: 97.6 ± 1.8%). Outliers in the reaction times were removed using built-in routines in MATLAB (<italic>isoutlier</italic> function, MATLAB R2018a). This step removed 8% of the response time data, but we obtained qualitatively similar results without this step.</p></sec></sec><sec id="s4-5"><title>Estimating letter model parameters from observed dissimilarities</title><p>The total dissimilarity between two bigrams in the letter model is calculated by calculating the average dissimilarity across all neurons. For each neuron, the dissimilarity between bigrams AB and CD is given by:<disp-formula id="equ1"><mml:math id="m1"><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="|" open="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>|</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub> <mml:mi/><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi> <mml:mi/><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the responses of the neuron to individual letters A, B, C and D respectively (derived from single letter dissimilarities), and <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the spatial summation weights for the first and second letters of the bigram. Note that <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the only free parameters for each neuron.</p><p>To estimate the spatial weights of each neuron, we adjusted them so as to minimize the squared error between the observed and predicted dissimilarity. This adjustment was done using standard gradient descent methods starting from randomly initialized weights (<italic>nlinfit</italic> function, MATLAB R2018a). We followed a similar approach for experiments involving longer strings.</p></sec><sec id="s4-6"><title>Experiment 3 – Upright and inverted bigrams</title><sec id="s4-6-1"><title>Methods</title><p>A total of eight subjects (six males, aged 24 ± 1.5 years) participated in this experiment. Six uppercase letters: A, L, N, R, S, and T were combined in all pairs to form a total of 36 stimuli. These uppercase letters were chosen because their images change when inverted (as opposed to letters like H that are unaffected by inversion), and were chosen to maximize the occurrence of frequent bigrams. The same stimuli were inverted to create another set of 36 stimuli. Detailed analyses for this experiment are presented in Appendix 2.</p></sec></sec><sec id="s4-7"><title>Experiment 4 – compound words</title><p>A total of eight subjects (four female, aged 25 ± 2.5 years) participated. Twelve three-letter words were chosen: ANY, FOR, TAR, KEY, SUN, TEA, ONE, MAT, GET, PAD, DAY, POT. Each word was jumbled to obtain 12 three-letter nonwords containing the same letters. The 12 words were combined to form 36 compound words (shown in Appendix 3), such that they appeared equally on the left and right half of the compound words. Detailed analyses for this experiment are included in Appendix 3.</p><sec id="s4-7-1"><title>Calculation of Orthographic Levenshtein Distance (OLD)</title><p>For each pair of strings, we calculated the OLD metric using built-in MATLAB function ‘editdistance’. This function estimates the number of insertions, deletions, or substitutions are required to convert one string to other. We set the substitution cost to 2, but obtained qualitatively similar results on varying this cost.</p></sec></sec><sec id="s4-8"><title>Experiment 5 – Lexical decision task</title><sec id="s4-8-1"><title>Procedure</title><p>A total of 16 subjects (nine male, aged 24.8 ± 2.1 years) participated in this task as well as the jumbled word task.</p></sec><sec id="s4-8-2"><title>Stimuli</title><p>The stimuli comprised 450 words + 450 nonwords. Words were chosen to avoid multiple possible anagrams (i.e. we avoid words like RATS that could be anagrammed as STAR, ARTS) and to maximize the range of word frequency. The nonwords were either random strings or modified versions of the 450 words (<xref ref-type="table" rid="table1">Table 1</xref>). Strings were presented in uppercase and subtended 1° in visual angle.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Non-word stimuli in lexical decision task (Experiment 5).</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top">Variations of word ABCDE</th><th valign="top">four letter words</th><th valign="top">five letter words</th><th valign="top">six letter words</th><th valign="top">Total</th></tr></thead><tbody><tr><td valign="top">1)</td><td valign="top">Edge transpositions: BACDE or ABCED</td><td valign="top">15</td><td valign="top">15</td><td valign="top">20</td><td valign="top">50</td></tr><tr><td valign="top">2)</td><td valign="top">Middle transposition: ACBDE or ABDCE</td><td valign="top">15</td><td valign="top">15</td><td valign="top">20</td><td valign="top">50</td></tr><tr><td valign="top">3)</td><td valign="top">Two-step edge transposition: CBADE or ABEDC</td><td valign="top">0</td><td valign="top">20</td><td valign="top">30</td><td valign="top">50</td></tr><tr><td valign="top">4)</td><td valign="top">Two-step middle transposition: ADCBE</td><td valign="top">0</td><td valign="top">20</td><td valign="top">30</td><td valign="top">50</td></tr><tr><td valign="top">5)</td><td valign="top">Random transposition: CDABE, ACDBE, etc.</td><td valign="top">25</td><td valign="top">35</td><td valign="top">40</td><td valign="top">100</td></tr><tr><td valign="top">6)</td><td valign="top">Edge substitution: MZCDE or ABCMZ</td><td valign="top">15</td><td valign="top">15</td><td valign="top">20</td><td valign="top">50</td></tr><tr><td valign="top">7)</td><td valign="top">Middle substitution: ABMZE</td><td valign="top">15</td><td valign="top">15</td><td valign="top">20</td><td valign="top">50</td></tr><tr><td valign="top">8)</td><td valign="top">Random substitution and permutation: <break/>MACZE, AMDEZ, etc.</td><td valign="top">15</td><td valign="top">15</td><td valign="top">20</td><td valign="top">50</td></tr><tr><td valign="top"/><td valign="top">Total</td><td valign="top">100</td><td valign="top">150</td><td valign="top">200</td><td valign="top">450</td></tr></tbody></table></table-wrap></sec><sec id="s4-8-3"><title>Task</title><p>Each trial began a fixation cross shown for 0.75 s followed by a letter string for 0.2 s after which the screen went blank. The trial ended either with the subject’s response or after at most 3 s. Subjects were instructed to press ‘Z’ for words and ‘M’ for nonwords as quickly and accurately as possible. All stimuli were presented at the centre of the screen and were white letters against a black background. Before starting the main task, subjects were given 20 practice trials using other words and nonwords not included in the main experiment.</p></sec><sec id="s4-8-4"><title>Data analysis</title><p>Some nonwords were removed from further analysis due to low accuracy (n = 8, average accuracy &lt;20%). Subjects made accurate responses for both words and nonwords (mean ±std of accuracy: 96 ± 2% for words, 95 ± 3% for nonwords). Outliers in the reaction times were removed using built-in routines in MATLAB (<italic>isoutlier</italic> function, MATLAB R2018a). This step removed 6.4% of the data, but we obtained qualitatively similar results without this step.</p></sec></sec><sec id="s4-9"><title>Experiment 6 (Lexical Decision Task – fMRI)</title><p>A total of 17 subjects (10 males, 25 ± 4.2 years) participated in this experiment. All subjects were screened for safety and comfort beforehand to avoid adverse outcomes in the scanner.</p><sec id="s4-9-1"><title>Stimuli</title><p>The functional localizer block included English words, objects, scrambled words, and scrambled objects. In each run, 14 images were randomly selected from a pool of images. The English words list comprised of 90 five-letter words. Each word was divided into grids of dimension 9 × 3. Scrambled words were generated by randomly shuffling the grids. The object pool comprised 80 naturalistic objects. To generate scrambled objects, the phase of the Fourier transformed images was scrambled and then reconstructed back using inverse Fourier transform. The object images were about 4.5° along the longer dimension and the height of the word stimuli subtended 2° of visual angle.</p><p>The event block consisted of 10 single letters and 64 five-letter strings (32 words and 32 nonwords formed using these single letters). The stimulus set comprised of 64 five-letter words and nonwords. The words were chosen from a wide range of frequency of occurrence and the nonwords were created by manipulating the chosen words that is They were: 1) 8-middle transposed version of words, 2) 8-edge transposed version of words, 3) 8-middle substituted version of words, and 4) 8-edge substituted version of words. The stimuli subtended 2° in height, which was the same as in the localizer block. All stimuli were presented as white against a black background.</p></sec><sec id="s4-9-2"><title>Procedure</title><p>In the localizer block, a total of 16 images were presented for 0.8 s with an inter stimulus interval of 0.2 s. There were 14 unique stimuli and 2 of them repeated at random time point, in which subjects performed one-back task. Each block ended with a blank screen with fixation cross present for 4 s. Thus, each block lasted 20 s. Each block was repeated thrice in each run.</p><p>In the event-related design block, an image was presented at the centre of the screen for 300 ms followed by 3.7 s of blank screen with a fixation cross. In a run, all 74 stimuli were presented once along with 16 trials of fixation cross to jitter inter stimulus interval. Hence there were a total of 92 trials including 4 s fixation trials at the start and end of each run. Each run lasted 376 s. Subjects performed lexical decision task only on strings and were instructed to not press any key for single letters. Overall, subjects completed 2 runs of localizer block, 8 runs of event block and a structural scan block.</p></sec><sec id="s4-9-3"><title>Data acquisition</title><p>Subjects viewed images in a mirror-based projection system. Functional MRI data was acquired using a 32-channel head coil on a 3T Siemens Skyra scanner at HealthCare Global Hospital, Bengaluru. Functional scans were performed using a T2*-weighted gradient-echo-planar imaging sequence with the following parameters: TR = 2 s, TE = 28 ms, flip angle = 79<sup>o</sup>, voxel size = 3×3 × 3 mm<sup>3</sup>, field of view = 192×192 mm<sup>2</sup>, and 33 axial-oblique slices covering the whole brain. Anatomical scans were performed using T1-weighted images with the following parameters: TR = 2.30 s, TE = 1.99 ms, flip angle = 9°, voxel size = 1×1 × 1 mm<sup>3</sup>, field of view = 256×256 × 176 mm<sup>3</sup>.</p></sec><sec id="s4-9-4"><title>Data preprocessing</title><p>All raw fMRI data were processed using the SPM 12 toolbox (<ext-link ext-link-type="uri" xlink:href="https://www.fil.ion.ucl.ac.uk/spm/software/spm12/">https://www.fil.ion.ucl.ac.uk/spm/software/spm12/</ext-link>, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_007037">SCR_007037</ext-link>). Raw images were realigned, slice-time corrected, co-registered with the anatomical image, segmented, and finally normalized to the MNI305 anatomical template. The results were qualitatively similar without normalization. Smoothing operation was performed only on functional localizer blocks using a Gaussian kernel with FWHM of 5 mm. All SPM parameters were set to default and the voxel size after normalization was set to 3 × 3×3 mm<sup>3</sup>. Prior to normalization, the data was preprocessed using GLMdenoise v1.4 (<xref ref-type="bibr" rid="bib28">Kay et al., 2013</xref>). This step improved the signal-to-noise ratio in the data by regressing out the noise pattern common across all the voxels in the brain. The noise pattern is estimated from voxels unrelated to the task. The activity corresponding to each condition was estimated by modeling the denoised data using a generalized linear model (GLM) in SPM after removing the low frequency drift using a high-pass filter with a cutoff at 128 s. The event block data was modeled using 89 regressors (74 stimuli + one fixation + six motion regressors + eight runs). The localizer block data was modeled using 13 regressors (four stimuli + one fixation + six motion regressors + two runs).</p></sec><sec id="s4-9-5"><title>ROI definitions</title><p>All the regions of interest (ROI) were defined using functional localizer while taking the anatomical location into consideration. Early visual area was defined as the region that responds more to the scrambled object than fixation cross. This functional region was further parsed into V1-V3 and V4 using an anatomical mask from SPM anatomy toolbox (<xref ref-type="bibr" rid="bib19">Eickhoff et al., 2005</xref>). Lateral Occipital (LO) region was defined as a group of voxels that responded more to objects than scrambled objects. The voxels in the LO region was restricted to Inferior Temporal Gyrus, Inferior Occipital Gyrus, and Middle Occipital Gyrus. These anatomical regions were obtained from Tissue Probability Map (TPM) labels in SPM 12. Visual Word Form Area (VWFA) was defined as a region that responded more for words than scrambled words within fusiform Gyrus. The activity for known words was also higher in Superior and Middle Temporal regions. These groups of voxels were grouped under Temporal Gyrus (TG) label. For each contrast, voxel-level threshold of p&lt;0.001 (uncorrected) or cluster level threshold p&lt;0.05 (FWE correction) was used to obtain a contiguous region. For one subject, very few VWFA voxels cross the pre-specified threshold. Hence, the threshold was lowered to p=0.1 (uncorrected). The VWFA voxels were restricted to top-40 voxels (based on T-value in the function localizer contrast). All these regions were visualized on the inflated brain using the BSPMVIEW toolbox (<ext-link ext-link-type="uri" xlink:href="http://www.bobspunt.com/bspmview/">http://www.bobspunt.com/bspmview/</ext-link>).</p></sec><sec id="s4-9-6"><title>Calculation of neural dissimilarity (fMRI)</title><p>For each ROI and subject, the pair-wise dissimilarity between any two image pairs was computed using the cross-validated Mahalanobis distance (<italic>rsa.distanceLDC</italic> function, RSA toolbox) (<xref ref-type="bibr" rid="bib40">Nili et al., 2014</xref>). Briefly, it calculates the leave-one-run-out Mahalanobis distance, and the final dissimilarity matrix is estimated by averaging across all the runs. Outliers in dissimilarity values across subjects were removed using built-in routines in MATLAB (<italic>isoutlier</italic> function, MATLAB R2018a). This function was applied to each dissimilarity pair separately, and removed 12.3% of the dissimilarity data. The results were qualitatively similar without this step. The median dissimilarity across all the subjects was considered for further analysis. We obtained qualitatively similar results for other distance measures.</p></sec><sec id="s4-9-7"><title>Calculation of semantic dissimilarity</title><p>The semantic distance between every pair of words was computed as the cosine distance between the GloVe feature vectors (<xref ref-type="bibr" rid="bib47">Pennington et al., 2014</xref>) activated by the two words (MATLAB function <italic>word2vec</italic>). These features are based on the co-occurrence statistics of words in a large text corpus, and therefore reflect semantic dissimilarity rather than purely visual dissimilarity.</p></sec></sec><sec id="s4-10"><title>Experiment 7 (Five-letter string searches)</title><p>A total of 11 subjects (six males, 26 ± 2.7 years) participated in this experiment, of which seven also participated in Experiment 6. Stimuli were identical to Experiment 6, except that they were scaled down to a height of 1° to allow placement in a visual search array. Subjects performed a total of 2048 correct trials (<sup>32</sup>C<sub>2</sub> search pairs x two conditions (words and nonwords) + 32 word-nonword pairs x two repetitions). All trials were interleaved, and incorrect/missed trials appeared randomly later in the task but were not analyzed. All other details were identical to Experiment 1.</p><sec id="s4-10-1"><title>Data analysis</title><p>Subjects were highly accurate on this task (mean ±std: 98.6 ± 1%). Outliers in the reaction times were removed using built-in routines in MATLAB (<italic>isoutlier</italic> function, MATLAB R2018a). This step removed 7% of the response time data, but we obtained qualitatively similar results without this step.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>Funding. This research study was funded by Intermediate and Senior Fellowships (Grant Numbers 500027/Z/09/Z and IA/S/17/1/503081 respectively) from the Wellcome Trust/DBT India Alliance and the DBT-IISc partnership program (to SPA).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All subjects gave informed consent to an experimental protocol approved by the Institutional Human Ethics Committee of the Indian Institute of Science (IHEC # 6-15092017).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-54846-transrepform-v3.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data and code necessary to reproduce the results are available in an Open Science Framework repository at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/384ZW">https://doi.org/10.17605/OSF.IO/384ZW</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>VisionLabIISc</collab></person-group><year iso-8601-date="2020">2020</year><data-title>jumbledwordsfMRI</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="doi">10.17605/OSF.IO/384ZW</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname> <given-names>A</given-names></name><name><surname>Hari</surname> <given-names>KVS</given-names></name><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reading increases the compositionality of visual word representations</article-title><source>Psychological Science</source><volume>30</volume><fpage>1707</fpage><lpage>1723</lpage><pub-id pub-id-type="doi">10.1177/0956797619881134</pub-id><pub-id pub-id-type="pmid">31697615</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Turning visual search time on its head</article-title><source>Vision Research</source><volume>74</volume><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.04.005</pub-id><pub-id pub-id-type="pmid">22561524</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baeck</surname> <given-names>A</given-names></name><name><surname>Kravitz</surname> <given-names>D</given-names></name><name><surname>Baker</surname> <given-names>C</given-names></name><name><surname>Op de Beeck</surname> <given-names>HP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Influence of lexical status and orthographic similarity on the multi-voxel response of the visual word form area</article-title><source>NeuroImage</source><volume>111</volume><fpage>321</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.01.060</pub-id><pub-id pub-id-type="pmid">25665965</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname> <given-names>CI</given-names></name><name><surname>Liu</surname> <given-names>J</given-names></name><name><surname>Wald</surname> <given-names>LL</given-names></name><name><surname>Kwong</surname> <given-names>KK</given-names></name><name><surname>Benner</surname> <given-names>T</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual word processing and experiential origins of functional selectivity in human extrastriate cortex</article-title><source>PNAS</source><volume>104</volume><fpage>9087</fpage><lpage>9092</lpage><pub-id pub-id-type="doi">10.1073/pnas.0703300104</pub-id><pub-id pub-id-type="pmid">17502592</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balota</surname> <given-names>DA</given-names></name><name><surname>Yap</surname> <given-names>MJ</given-names></name><name><surname>Cortese</surname> <given-names>MJ</given-names></name><name><surname>Hutchison</surname> <given-names>KA</given-names></name><name><surname>Kessler</surname> <given-names>B</given-names></name><name><surname>Loftis</surname> <given-names>B</given-names></name><name><surname>Neely</surname> <given-names>JH</given-names></name><name><surname>Nelson</surname> <given-names>DL</given-names></name><name><surname>Simpson</surname> <given-names>GB</given-names></name><name><surname>Treiman</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The english lexicon project</article-title><source>Behavior Research Methods</source><volume>39</volume><fpage>445</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.3758/BF03193014</pub-id><pub-id pub-id-type="pmid">17958156</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname> <given-names>P</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Representation of multiple objects in macaque category-selective Areas</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>1774</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-04126-7</pub-id><pub-id pub-id-type="pmid">29720645</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouhali</surname> <given-names>F</given-names></name><name><surname>Bézagu</surname> <given-names>Z</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A mesial-to-lateral dissociation for orthographic processing in the visual cortex</article-title><source>PNAS</source><volume>116</volume><fpage>21936</fpage><lpage>21946</lpage><pub-id pub-id-type="doi">10.1073/pnas.1904184116</pub-id><pub-id pub-id-type="pmid">31591198</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Op de Beeck</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociations and associations between shape and category representations in the two visual pathways</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>432</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2314-15.2016</pub-id><pub-id pub-id-type="pmid">26758835</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlos</surname> <given-names>BJ</given-names></name><name><surname>Hirshorn</surname> <given-names>EA</given-names></name><name><surname>Durisko</surname> <given-names>C</given-names></name><name><surname>Fiez</surname> <given-names>JA</given-names></name><name><surname>Coutanche</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Word inversion sensitivity as a marker of visual word form area lateralization: an application of a novel multivariate measure of laterality</article-title><source>NeuroImage</source><volume>191</volume><fpage>493</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.02.044</pub-id><pub-id pub-id-type="pmid">30807821</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chanceaux</surname> <given-names>M</given-names></name><name><surname>Grainger</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Serial position effects in the identification of letters, digits, symbols, and shapes in peripheral vision</article-title><source>Acta Psychologica</source><volume>141</volume><fpage>149</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2012.08.001</pub-id><pub-id pub-id-type="pmid">22964055</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colombo</surname> <given-names>L</given-names></name><name><surname>Pasini</surname> <given-names>M</given-names></name><name><surname>Balota</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dissociating the influence of familiarity and meaningfulness from word frequency in naming and lexical decision performance</article-title><source>Memory &amp; Cognition</source><volume>34</volume><fpage>1312</fpage><lpage>1324</lpage><pub-id pub-id-type="doi">10.3758/BF03193274</pub-id><pub-id pub-id-type="pmid">17225511</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The spatial coding model of visual word identification</article-title><source>Psychological Review</source><volume>117</volume><fpage>713</fpage><lpage>758</lpage><pub-id pub-id-type="doi">10.1037/a0019738</pub-id><pub-id pub-id-type="pmid">20658851</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name><name><surname>Vinckier</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The neural code for written words: a proposal</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>335</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.05.004</pub-id><pub-id pub-id-type="pmid">15951224</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Pegado</surname> <given-names>F</given-names></name><name><surname>Braga</surname> <given-names>LW</given-names></name><name><surname>Ventura</surname> <given-names>P</given-names></name><name><surname>Nunes Filho</surname> <given-names>G</given-names></name><name><surname>Jobert</surname> <given-names>A</given-names></name><name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></name><name><surname>Kolinsky</surname> <given-names>R</given-names></name><name><surname>Morais</surname> <given-names>J</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>How learning to read changes the cortical networks for vision and language</article-title><source>Science</source><volume>330</volume><fpage>1359</fpage><lpage>1364</lpage><pub-id pub-id-type="doi">10.1126/science.1194140</pub-id><pub-id pub-id-type="pmid">21071632</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Morais</surname> <given-names>J</given-names></name><name><surname>Kolinsky</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Illiterate to literate: behavioural and cerebral changes induced by reading acquisition</article-title><source>Nature Reviews Neuroscience</source><volume>16</volume><fpage>234</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1038/nrn3924</pub-id><pub-id pub-id-type="pmid">25783611</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dufau</surname> <given-names>S</given-names></name><name><surname>Grainger</surname> <given-names>J</given-names></name><name><surname>Ziegler</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How to say “no” to a nonword: A leaky competing accumulator model of lexical decision</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>38</volume><fpage>1117</fpage><lpage>1128</lpage><pub-id pub-id-type="doi">10.1037/a0026948</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname> <given-names>J</given-names></name><name><surname>Humphreys</surname> <given-names>GW</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Visual search and stimulus similarity</article-title><source>Psychological Review</source><volume>96</volume><fpage>433</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.96.3.433</pub-id><pub-id pub-id-type="pmid">2756067</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickhoff</surname> <given-names>SB</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name><name><surname>Mohlberg</surname> <given-names>H</given-names></name><name><surname>Grefkes</surname> <given-names>C</given-names></name><name><surname>Fink</surname> <given-names>GR</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name><name><surname>Zilles</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title><source>NeuroImage</source><volume>25</volume><fpage>1325</fpage><lpage>1335</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.12.034</pub-id><pub-id pub-id-type="pmid">15850749</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname> <given-names>AD</given-names></name><name><surname>Gierhan</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The language network</article-title><source>Current Opinion in Neurobiology</source><volume>23</volume><fpage>250</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.10.002</pub-id><pub-id pub-id-type="pmid">23146876</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghose</surname> <given-names>GM</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatial summation can explain the attentional modulation of neuronal responses to multiple stimuli in area V4</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>5115</fpage><lpage>5126</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0138-08.2008</pub-id><pub-id pub-id-type="pmid">18463265</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glezer</surname> <given-names>LS</given-names></name><name><surname>Jiang</surname> <given-names>X</given-names></name><name><surname>Riesenhuber</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Evidence for highly selective neuronal tuning to whole words in the &quot;visual word form area&quot;</article-title><source>Neuron</source><volume>62</volume><fpage>199</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.03.017</pub-id><pub-id pub-id-type="pmid">19409265</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez</surname> <given-names>P</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Perea</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The overlap model: a model of letter position coding</article-title><source>Psychological Review</source><volume>115</volume><fpage>577</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1037/a0012667</pub-id><pub-id pub-id-type="pmid">18729592</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grainger</surname> <given-names>J</given-names></name><name><surname>Dufau</surname> <given-names>S</given-names></name><name><surname>Montant</surname> <given-names>M</given-names></name><name><surname>Ziegler</surname> <given-names>JC</given-names></name><name><surname>Fagot</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Orthographic processing in baboons (Papio papio)</article-title><source>Science</source><volume>336</volume><fpage>245</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1126/science.1218152</pub-id><pub-id pub-id-type="pmid">22499949</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grainger</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Orthographic processing: a ‘mid-level’ vision of reading: The 44th Sir Frederic Bartlett Lecture</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>71</volume><fpage>335</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1080/17470218.2017.1314515</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grainger</surname> <given-names>J</given-names></name><name><surname>Whitney</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Does the huamn mnid raed wrods as a wlohe?</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>58</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2003.11.006</pub-id><pub-id pub-id-type="pmid">15588808</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirshorn</surname> <given-names>EA</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Ward</surname> <given-names>MJ</given-names></name><name><surname>Richardson</surname> <given-names>RM</given-names></name><name><surname>Fiez</surname> <given-names>JA</given-names></name><name><surname>Ghuman</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decoding and disrupting left midfusiform gyrus activity during word reading</article-title><source>PNAS</source><volume>113</volume><fpage>8162</fpage><lpage>8167</lpage><pub-id pub-id-type="doi">10.1073/pnas.1604126113</pub-id><pub-id pub-id-type="pmid">27325763</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname> <given-names>KN</given-names></name><name><surname>Rokem</surname> <given-names>A</given-names></name><name><surname>Winawer</surname> <given-names>J</given-names></name><name><surname>Dougherty</surname> <given-names>RF</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>GLMdenoise: a fast, automated technique for denoising task-based fMRI data</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.3389/fnins.2013.00247</pub-id><pub-id pub-id-type="pmid">24381539</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kronbichler</surname> <given-names>M</given-names></name><name><surname>Hutzler</surname> <given-names>F</given-names></name><name><surname>Wimmer</surname> <given-names>H</given-names></name><name><surname>Mair</surname> <given-names>A</given-names></name><name><surname>Staffen</surname> <given-names>W</given-names></name><name><surname>Ladurner</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The visual word form area and the frequency with which words are encountered: evidence from a parametric fMRI study</article-title><source>NeuroImage</source><volume>21</volume><fpage>946</fpage><lpage>953</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.10.021</pub-id><pub-id pub-id-type="pmid">15006661</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuperman</surname> <given-names>V</given-names></name><name><surname>Van Dyke</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Reassessing word frequency as a determinant of word recognition for skilled and unskilled readers</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>39</volume><fpage>802</fpage><lpage>823</lpage><pub-id pub-id-type="doi">10.1037/a0030859</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehky</surname> <given-names>SR</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural representation for object recognition in inferotemporal cortex</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>23</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.12.001</pub-id><pub-id pub-id-type="pmid">26771242</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levenshtein</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Binary codes capable of correcting deletions, insertions, and reversals</article-title><source>Soviet Physics Doklady</source><volume>10</volume><fpage>707</fpage><lpage>710</lpage></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lochy</surname> <given-names>A</given-names></name><name><surname>Jacques</surname> <given-names>C</given-names></name><name><surname>Maillard</surname> <given-names>L</given-names></name><name><surname>Colnat-Coulbois</surname> <given-names>S</given-names></name><name><surname>Rossion</surname> <given-names>B</given-names></name><name><surname>Jonas</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Selective visual representation of letters and words in the left ventral occipito-temporal cortex with intracerebral recordings</article-title><source>PNAS</source><volume>115</volume><fpage>E7595</fpage><lpage>E7604</lpage><pub-id pub-id-type="doi">10.1073/pnas.1718987115</pub-id><pub-id pub-id-type="pmid">30038000</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marcet</surname> <given-names>A</given-names></name><name><surname>Perea</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Is nevtral NEUTRAL? visual similarity effects in the early phases of written-word recognition</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>24</volume><fpage>1180</fpage><lpage>1185</lpage><pub-id pub-id-type="doi">10.3758/s13423-016-1180-9</pub-id><pub-id pub-id-type="pmid">27873186</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname> <given-names>JL</given-names></name><name><surname>Rumelhart</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>An interactive activation model of context effects in letter perception: I. an account of basic findings</article-title><source>Psychological Review</source><volume>88</volume><fpage>375</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.88.5.375</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mruczek</surname> <given-names>RE</given-names></name><name><surname>Sheinberg</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Distractor familiarity leads to more efficient visual search for complex stimuli</article-title><source>Perception &amp; Psychophysics</source><volume>67</volume><fpage>1016</fpage><lpage>1031</lpage><pub-id pub-id-type="doi">10.3758/BF03193628</pub-id><pub-id pub-id-type="pmid">16396010</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mueller</surname> <given-names>ST</given-names></name><name><surname>Weidemann</surname> <given-names>CT</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Alphabetic letter identification: effects of Perceivability, similarity, and Bias</article-title><source>Acta Psychologica</source><volume>139</volume><fpage>19</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2011.09.014</pub-id><pub-id pub-id-type="pmid">22036587</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname> <given-names>MJ</given-names></name><name><surname>El Karoui</surname> <given-names>I</given-names></name><name><surname>Giber</surname> <given-names>K</given-names></name><name><surname>Yang</surname> <given-names>X</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Koopman</surname> <given-names>H</given-names></name><name><surname>Cash</surname> <given-names>SS</given-names></name><name><surname>Naccache</surname> <given-names>L</given-names></name><name><surname>Hale</surname> <given-names>JT</given-names></name><name><surname>Pallier</surname> <given-names>C</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neurophysiological dynamics of phrase-structure building during sentence processing</article-title><source>PNAS</source><volume>114</volume><fpage>E3669</fpage><lpage>E3678</lpage><pub-id pub-id-type="doi">10.1073/pnas.1701590114</pub-id><pub-id pub-id-type="pmid">28416691</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Wingfield</surname> <given-names>C</given-names></name><name><surname>Walther</surname> <given-names>A</given-names></name><name><surname>Su</surname> <given-names>L</given-names></name><name><surname>Marslen-Wilson</surname> <given-names>W</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A toolbox for representational similarity analysis</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id><pub-id pub-id-type="pmid">24743308</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Models of visual word recognition</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>517</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.08.003</pub-id><pub-id pub-id-type="pmid">24012145</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname> <given-names>D</given-names></name><name><surname>Kinoshita</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reading through a noisy channel: why there's nothing special about the perception of orthography</article-title><source>Psychological Review</source><volume>119</volume><fpage>517</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1037/a0028450</pub-id><pub-id pub-id-type="pmid">22663560</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname> <given-names>H</given-names></name><name><surname>Wagemans</surname> <given-names>J</given-names></name><name><surname>Vogels</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Inferotemporal neurons represent low-dimensional configurations of parameterized shapes</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>1244</fpage><lpage>1252</lpage><pub-id pub-id-type="doi">10.1038/nn767</pub-id><pub-id pub-id-type="pmid">11713468</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pallier</surname> <given-names>C</given-names></name><name><surname>Devauchelle</surname> <given-names>AD</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cortical representation of the constituent structure of sentences</article-title><source>PNAS</source><volume>108</volume><fpage>2522</fpage><lpage>2527</lpage><pub-id pub-id-type="doi">10.1073/pnas.1018711108</pub-id><pub-id pub-id-type="pmid">21224415</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname> <given-names>DG</given-names></name><name><surname>Tillman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Parts, wholes, and context in reading: a triple dissociation</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e680</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000680</pub-id><pub-id pub-id-type="pmid">17668058</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname> <given-names>DG</given-names></name><name><surname>Tillman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The uncrowded window of object recognition</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1129</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1038/nn.2187</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pennington</surname> <given-names>J</given-names></name><name><surname>Socher</surname> <given-names>R</given-names></name><name><surname>Manning</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>GloVe: global vectors for word representation</article-title><conf-name>Empirical Methods in Natural Language Processing (EMNLP)</conf-name><fpage>1532</fpage><lpage>1543</lpage><pub-id pub-id-type="doi">10.3115/v1/D14-1162</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perea</surname> <given-names>M</given-names></name><name><surname>Duñabeitia</surname> <given-names>JA</given-names></name><name><surname>Carreiras</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>R34D1NG W0RD5 W1TH NUMB3R5</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>34</volume><fpage>237</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.34.1.237</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perea</surname> <given-names>M</given-names></name><name><surname>Panadero</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Does viotin activate violin more than viocin? on the use of visual cues during visual-word recognition</article-title><source>Experimental Psychology</source><volume>61</volume><fpage>23</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1027/1618-3169/a000223</pub-id><pub-id pub-id-type="pmid">23948388</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pramod</surname> <given-names>RT</given-names></name><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Features in visual search combine linearly</article-title><source>Journal of Vision</source><volume>14</volume><fpage>6</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1167/14.4.6</pub-id><pub-id pub-id-type="pmid">24715328</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pramod</surname> <given-names>RT</given-names></name><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Object attributes combine additively in visual search</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.1167/16.5.8</pub-id><pub-id pub-id-type="pmid">26967014</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pramod</surname> <given-names>RT</given-names></name><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Symmetric objects become special in perception because of generic computations in neurons</article-title><source>Psychological Science</source><volume>29</volume><fpage>95</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1177/0956797617729808</pub-id><pub-id pub-id-type="pmid">29219748</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname> <given-names>D</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Disentangling representations of object shape and object category in human visual cortex: the Animate-Inanimate distinction</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00924</pub-id><pub-id pub-id-type="pmid">26765944</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratan Murty</surname> <given-names>NA</given-names></name><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamics of 3D view invariance in monkey inferotemporal cortex</article-title><source>Journal of Neurophysiology</source><volume>113</volume><fpage>2180</fpage><lpage>2194</lpage><pub-id pub-id-type="doi">10.1152/jn.00810.2014</pub-id><pub-id pub-id-type="pmid">25609108</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Gomez</surname> <given-names>P</given-names></name><name><surname>McKoon</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A diffusion model account of the lexical decision task</article-title><source>Psychological Review</source><volume>111</volume><fpage>159</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.111.1.159</pub-id><pub-id pub-id-type="pmid">14756592</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>McKoon</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The diffusion decision model: theory and data for two-choice decision tasks</article-title><source>Neural Computation</source><volume>20</volume><fpage>873</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.12-06-420</pub-id><pub-id pub-id-type="pmid">18085991</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rawlinson</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>The significance of letter position in word recognition</article-title><source>IEEE Aerospace and Electronic Systems Magazine</source><volume>22</volume><fpage>26</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1109/MAES.2007.327521</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Eye movements in reading and information processing: 20 years of research</article-title><source>Psychological Bulletin</source><volume>124</volume><fpage>372</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.124.3.372</pub-id><pub-id pub-id-type="pmid">9849112</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname> <given-names>K</given-names></name><name><surname>White</surname> <given-names>SJ</given-names></name><name><surname>Johnson</surname> <given-names>RL</given-names></name><name><surname>Liversedge</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Raeding wrods with jubmled lettres: there is a cost</article-title><source>Psychological Science</source><volume>17</volume><fpage>192</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2006.01684.x</pub-id><pub-id pub-id-type="pmid">16507057</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname> <given-names>DE</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>An interactive activation model of context effects in letter perception: part 2. the contextual enhancement effect and some tests and extensions of the model</article-title><source>Psychological Review</source><volume>89</volume><fpage>60</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.89.1.60</pub-id><pub-id pub-id-type="pmid">7058229</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scaltritti</surname> <given-names>M</given-names></name><name><surname>Dufau</surname> <given-names>S</given-names></name><name><surname>Grainger</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Stimulus orientation and the first-letter advantage</article-title><source>Acta Psychologica</source><volume>183</volume><fpage>37</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2017.12.009</pub-id><pub-id pub-id-type="pmid">29306099</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simpson</surname> <given-names>IC</given-names></name><name><surname>Mousikou</surname> <given-names>P</given-names></name><name><surname>Montoya</surname> <given-names>JM</given-names></name><name><surname>Defior</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A letter visual-similarity matrix for Latin-based alphabets</article-title><source>Behavior Research Methods</source><volume>45</volume><fpage>431</fpage><lpage>439</lpage><pub-id pub-id-type="doi">10.3758/s13428-012-0271-4</pub-id><pub-id pub-id-type="pmid">23055176</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sripati</surname> <given-names>AP</given-names></name><name><surname>Olson</surname> <given-names>CR</given-names></name></person-group><year iso-8601-date="2010">2010a</year><article-title>Global image dissimilarity in macaque inferotemporal cortex predicts human visual search efficiency</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>1258</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1908-09.2010</pub-id><pub-id pub-id-type="pmid">20107054</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sripati</surname> <given-names>AP</given-names></name><name><surname>Olson</surname> <given-names>CR</given-names></name></person-group><year iso-8601-date="2010">2010b</year><article-title>Responses to Compound Objects in Monkey Inferotemporal Cortex: The Whole Is Equal to the Sum of the Discrete Parts</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>7948</fpage><lpage>7960</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0016-10.2010</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sunder</surname> <given-names>S</given-names></name><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Look before you seek: Preview adds a fixed benefit to all searches</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1167/16.15.3</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussman</surname> <given-names>BL</given-names></name><name><surname>Reddigari</surname> <given-names>S</given-names></name><name><surname>Newman</surname> <given-names>SD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The impact of inverted text on visual word processing: An fMRI study</article-title><source>Brain and Cognition</source><volume>123</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.bandc.2018.02.004</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thesen</surname> <given-names>T</given-names></name><name><surname>McDonald</surname> <given-names>CR</given-names></name><name><surname>Carlson</surname> <given-names>C</given-names></name><name><surname>Doyle</surname> <given-names>W</given-names></name><name><surname>Cash</surname> <given-names>S</given-names></name><name><surname>Sherfey</surname> <given-names>J</given-names></name><name><surname>Felsovalyi</surname> <given-names>O</given-names></name><name><surname>Girard</surname> <given-names>H</given-names></name><name><surname>Barr</surname> <given-names>W</given-names></name><name><surname>Devinsky</surname> <given-names>O</given-names></name><name><surname>Kuzniecky</surname> <given-names>R</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sequential then interactive processing of letters and words in the left fusiform gyrus</article-title><source>Nature Communications</source><volume>3</volume><elocation-id>1284</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms2220</pub-id><pub-id pub-id-type="pmid">23250414</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorat</surname> <given-names>S</given-names></name><name><surname>Proklova</surname> <given-names>D</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The nature of the animacy organization in human ventral temporal cortex</article-title><source>eLife</source><volume>8</volume><elocation-id>e47142</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47142</pub-id><pub-id pub-id-type="pmid">31496518</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vighneshvel</surname> <given-names>T</given-names></name><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Does linear separability really matter? complex visual search is explained by simple search</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/13.11.10</pub-id><pub-id pub-id-type="pmid">24029822</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vighneshvel</surname> <given-names>T</given-names></name><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Coding of relative size in monkey inferotemporal cortex</article-title><source>Journal of Neurophysiology</source><volume>113</volume><fpage>2173</fpage><lpage>2179</lpage><pub-id pub-id-type="doi">10.1152/jn.00907.2014</pub-id><pub-id pub-id-type="pmid">25589595</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinckier</surname> <given-names>F</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Jobert</surname> <given-names>A</given-names></name><name><surname>Dubus</surname> <given-names>JP</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hierarchical coding of letter strings in the ventral stream: dissecting the inner organization of the visual word-form system</article-title><source>Neuron</source><volume>55</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.05.031</pub-id><pub-id pub-id-type="pmid">17610823</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yap</surname> <given-names>MJ</given-names></name><name><surname>Sibley</surname> <given-names>DE</given-names></name><name><surname>Balota</surname> <given-names>DA</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Rueckl</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Responding to nonwords in the lexical decision task: insights from the english lexicon project</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>41</volume><fpage>597</fpage><lpage>613</lpage><pub-id pub-id-type="doi">10.1037/xlm0000064</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname> <given-names>T</given-names></name><name><surname>Balota</surname> <given-names>D</given-names></name><name><surname>Yap</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Moving beyond coltheart's N: a new measure of orthographic similarity</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>15</volume><fpage>971</fpage><lpage>979</lpage><pub-id pub-id-type="doi">10.3758/PBR.15.5.971</pub-id><pub-id pub-id-type="pmid">18926991</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhivago</surname> <given-names>KA</given-names></name><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Texture discriminability in monkey inferotemporal cortex predicts human texture perception</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>2745</fpage><lpage>2755</lpage><pub-id pub-id-type="doi">10.1152/jn.00532.2014</pub-id><pub-id pub-id-type="pmid">25210165</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziegler</surname> <given-names>JC</given-names></name><name><surname>Hannagan</surname> <given-names>T</given-names></name><name><surname>Dufau</surname> <given-names>S</given-names></name><name><surname>Montant</surname> <given-names>M</given-names></name><name><surname>Fagot</surname> <given-names>J</given-names></name><name><surname>Grainger</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Transposed-letter effects reveal orthographic processing in baboons</article-title><source>Psychological Science</source><volume>24</volume><fpage>1609</fpage><lpage>1611</lpage><pub-id pub-id-type="doi">10.1177/0956797612474322</pub-id><pub-id pub-id-type="pmid">23757307</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname> <given-names>D</given-names></name><name><surname>Cox</surname> <given-names>DD</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multiple object response normalization in monkey inferotemporal cortex</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>8150</fpage><lpage>8164</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2058-05.2005</pub-id><pub-id pub-id-type="pmid">16148223</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zorzi</surname> <given-names>M</given-names></name><name><surname>Barbiero</surname> <given-names>C</given-names></name><name><surname>Facoetti</surname> <given-names>A</given-names></name><name><surname>Lonciari</surname> <given-names>I</given-names></name><name><surname>Carrozzi</surname> <given-names>M</given-names></name><name><surname>Montico</surname> <given-names>M</given-names></name><name><surname>Bravar</surname> <given-names>L</given-names></name><name><surname>George</surname> <given-names>F</given-names></name><name><surname>Pech-Georgel</surname> <given-names>C</given-names></name><name><surname>Ziegler</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Extra-large letter spacing improves reading in dyslexia</article-title><source>PNAS</source><volume>109</volume><fpage>11455</fpage><lpage>11459</lpage><pub-id pub-id-type="doi">10.1073/pnas.1205566109</pub-id><pub-id pub-id-type="pmid">22665803</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Additional analysis for Experiment 1</title><p>The results in the main text were presented for uppercase English letters (<xref ref-type="fig" rid="fig2">Figure 2</xref>), but in Experiment 1 we also collected visual search data for all English letters and digits (n = 62 characters in all, comprising 26 uppercase + 26 lowercase + 10). We did so in order to predict the visual dissimilarity between letter strings containing both mixed case letters as well as numbers.</p><p>To visualize the dissimilarity relations between the 62 characters used, we performed multidimensional scaling. In the resulting plot (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>), nearby characters represent hard searches. A number of interesting patterns can be seen: letters like C, G, Q, O are nearby which is expected given their shared curvatures. Letter pairs such as (M,W) and number pairs such as (6,9) are similar due to mirror confusion (<xref ref-type="bibr" rid="bib69">Vighneshvel and Arun, 2013</xref>).</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Visual search space for letters and digits.</title><p>(<bold>A</bold>) Visual search space for letters (uppercase and lowercase) and digits obtained by multidimensional scaling of observed dissimilarities. Nearby letters represent hard searches. Distances in this 2D plot are highly correlated with the observed distances (r = 0.79, p&lt;0.00005). (<bold>B</bold>) Correlation between observed distances and MDS embedding as a function of number of MDS dimensions. The horizontal line represents the split-half correlation with error bars representing s.d calculated across 100 random splits.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app1-fig1-v3.tif"/></fig><p>Next, we investigated the degree to which the observed pairwise dissimilarities are captured by the multidimensional embedding as a function of the number of dimensions. In the resulting plot (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>), it can be seen that nearly 89% of the variance is captured by 10 dimensions as before, which reaches roughly the reliability of the dissimilarity data itself. For the analyses involving mixed case searches or fewer searches, we took a total of six neurons for the letter model, which explain 87.7% of the variance in the pairwise dissimilarities.</p></sec><sec id="s9" sec-type="appendix"><title>Can letter dissimilarity be predicted using low-level visual features?</title><p>To investigate whether single letter dissimilarity can be predicted using low-level visual features, we attempted to predict letter dissimilarities using two models. In the first model, which we call the pixel model, we calculated the dissimilarity between letters to be the absolute difference in pixel intensities between the images of the two letters. This pixel-based model showed a significant correlation (r = 0.50, p&lt;0.00005) but was far from the reliability of the data itself (<italic>r<sub>sh</sub></italic> = 0.90; <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>). In the second model, we calculated the dissimilarity between two letters as the vector distance between the responses evoked by a population of simulated V1 neurons (<xref ref-type="bibr" rid="bib54">Ratan Murty and Arun, 2015</xref>). This V1 model also showed a significant correlation (r = 0.44, p&lt;0.00005) but again far from the reliability of the data itself). We conclude that single letter dissimilarity can only be partially predicted by low-level visual features.</p></sec><sec id="s10" sec-type="appendix"><title>Is visual search dissimilarity related to subjective dissimilarity?</title><p>In this study, we have used visual search as a natural and objective measure for visual dissimilarity. However, previous studies have measured letter dissimilarity either through confusions in letter recognition, or through subjective dissimilarity ratings (<xref ref-type="bibr" rid="bib38">Mueller and Weidemann, 2012</xref>; <xref ref-type="bibr" rid="bib62">Simpson et al., 2013</xref>). We have previously shown that subjective dissimilarity for abstract silhouettes is strongly correlated with visual search dissimilarity (<xref ref-type="bibr" rid="bib51">Pramod and Arun, 2016</xref>). This may not hold for letters since subjects can activate letter representations that are modified through extensive familiarity. To investigate how visual search dissimilarity compares with subjective similarity ratings for letters, we compared search dissimilarities for uppercase letters against two sets of previously reported similarity data. First, we compared visual search dissimilarities with subjective dissimilarity ratings (<xref ref-type="bibr" rid="bib62">Simpson et al., 2013</xref>). This revealed a significant positive correlation (r = 0.69, p&lt;0.0005). Second, we compared visual search dissimilarities with letter confusion data (<italic>3</italic>). To convert letter confusion response times, which are a measure of similarity, into dissimilarities, we took their reciprocals, and then compared them with visual search dissimilarities. This revealed a significant positive, albeit weaker correlation (r = 0.34, p&lt;0.0005).</p></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><sec id="s11" sec-type="appendix"><title>Upright and inverted bigrams and trigrams</title><p>It has been observed that readers are more sensitive to letter transpositions for letters of their familiar script. Since discrimination of letter transpositions in the letter model is a direct consequence of asymmetric spatial summation (main text, <xref ref-type="fig" rid="fig3">Figure 3</xref>), we predicted that readers should show more asymmetric spatial summation for familiar letters compared to unfamiliar letters. As a strong test of this prediction, we compared visual search performance on upright letters (which are highly familiar) with inverted letters (which are unfamiliar) across two experiments, one on bigrams and the other on trigrams.</p><p>The comparison of upright and inverted letter strings is also interesting for a second reason. If reading or familiarity with upright letters led to the formation of specialized detectors for longer strings, then we predict that the letter model (which assumes responses to be driven by single letters only) should yield worse fits for upright compared to inverted letters.</p><p>We tested the above two predictions in the following two experiments.</p></sec><sec id="s12" sec-type="appendix"><title>Experiment 3: Upright vs inverted bigrams</title><sec id="s12-1"><title>Methods</title><p>A total of eight subjects (six males, aged 24 ± 1.5 years) participated in this experiment. Six uppercase letters: A, L, N, R, S, and T were combined in all pairs to form a total of 36 stimuli. These uppercase letters were chosen because their images change when inverted (as opposed to letters like H that are unaffected by inversion), and were chosen to maximize the occurrence of frequent bigrams. The same stimuli were inverted to create another set of 36 stimuli. Stimuli subtended ~4° along the longer dimension. Subjects performed all possible searches among the upright letters (<sup>36</sup>C<sub>2</sub> = 630 searches) with two repetitions and likewise for inverted letters. All trials were interleaved. All other details were exactly as in Experiment 2.</p></sec></sec><sec id="s13" sec-type="appendix"><title>Results</title><p>We observed interesting differences in search difficulty depending on the nature of the bigrams. This pattern is illustrated in <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1A-B</xref>. When the target and distractors consisted of repeated letters (e.g. TT among AA in <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1A</xref>), search is equally easy when the array is upright or inverted. In contrast if the target and distractors are transposed versions of each other (e.g. TA among AT in <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1B</xref>), search is easier in the upright array compared to when it is inverted.</p><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Letter model fits for upright and inverted bigrams.</title><p>(<bold>A</bold>) Example oddball search array for a repeated letter target (TT) among identical repeated-letter distractors (AA). It can be seen that inverting this search array does not affect search difficulty. (<bold>B</bold>) Example oddball search array for transposed letters (TA among AT). It can be seen by inverting this search array makes the search substantially more difficult. (<bold>C</bold>) Average search times in the oddball search task for repeated-letter searches (AA-BB) and transposed letter (AB-BA) searches. Error bars represent s.e.m calculated across subjects. Asterisks represent statistical significance (**** is p&lt;0.00005), as obtained using an ANOVA on the response times with subject, bigram and orientation as factors (see text). (<bold>D</bold>) Dissimilarity of inverted bigram pairs plotted against the dissimilarity of upright bigram pairs. Correlation is shown at the top left. Asterisks indicate statistical significance of the correlations (**** is p&lt;0.00005). (<bold>E</bold>) Cross-validated model correlation of the letter model for upright bigrams and inverted bigrams. <italic>Shaded gray bars</italic> represent the upper bound achievable in each case given the consistency of the data, calculated using the split-half correlation <italic>r<sub>sh</sub></italic>. (<bold>F</bold>) Predicted RT from the letter model for repeated letter pairs and transposed letter pairs. Asterisks denote statistical significance as obtained using a sign-rank test on the predicted RTs between upright and inverted conditions. (<bold>G</bold>) Spatial modulation index for each neuron in the letter model for upright and inverted bigrams. (<bold>H</bold>) Average spatial modulation index for upright and inverted bigrams. Asterisks represent statistical significance (* is p&lt;0.05) obtained using a sign-rank test on the spatial modulation index across the 10 neurons.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app2-fig1-v3.tif"/></fig><p>To confirm that this effect is present across all such pairs, we compared observed response times for these two types of searches between upright and inverted conditions (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1C</xref>). Response times for the AA-BB searches were comparable for upright and inverted conditions (mean ± sd of RT: 0.66 ± 0.09 s for upright, 0.67 ± 0.1 s for inverted). To assess the statistical significance of this difference, we performed an ANOVA with subject (eight levels), bigram (15 pairs) and orientation (upright vs inverted) as factors. We observed no significant difference in the response times between upright and inverted conditions for AA-BB searches (p=0.65 for main effect of orientation; p&lt;0.00005 for subject and bigram factors, p&gt;0.05 for all interactions).</p><p>Next, we compared transposed letter (AB-BA) searches. Here, subjects were clearly faster on the upright searches compared to inverted searches (mean ± sd of RT: 1.58 ± 0.25 s for upright, 3.12 ± 0.76 s for inverted). This difference was statistically significant (p&lt;0.00005 for main effect of orientation; p&lt;0.0005 for subject and p&lt;0.05 for bigram factors, p&lt;0.05 for interactions between pairs and orientation. Other interaction effects were not significant).</p><p>To compare bigram dissimilarity between upright and inverted bigrams, we plotted one against the other. This revealed a highly significant correlation (r = 0.80, p&lt;0.00005; <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1D</xref>). Here too it can be seen that the transposed letter searches are clearly faster when they are upright whereas the repeated letter searches show no such difference.</p><p>Thus, inversion slows down transposed letter searches but not repeated letter searches.</p></sec><sec id="s14" sec-type="appendix"><title>Explaining upright and inverted bigram dissimilarity using the letter model</title><p>We fit the letter model to both upright and inverted bigram searches using a total of 10 neurons with single letter responses derived from Experiment 1. The letter model yielded excellent fits on both upright and inverted bigrams. In both cases, the model fits approached the data consistency (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1E</xref>), implying that the model explained nearly all the explainable variance in the data.</p><p>To compare model fits for upright vs inverted bigrams, we performed a bootstrap analysis. Each time, we selected subjects with replacement and fit the letter model to the average dissimilarity computed for this random pool of subjects. Each time, we calculated a normalized correlation measure that takes into account the difference in data reliability between upright and inverted trigram searches. This normalized correlation is simply the model correlation divided by the data consistency. To assess statistical significance, we calculated the fraction of times the normalized correlation in the upright samples was larger than the inverted samples. This analysis revealed significant difference in model performance between upright and inverted searches, but in the opposite direction (average model correlation: r = 0.92 for upright, 0.9 for inverted; fraction of upright &lt;inverted normalized model correlation: p=0). Thus, upright searches are more predictable than inverted searches using the letter model.</p><p>Next, we asked whether the letter model can explain the intriguing observation that inversion affects transposed letter searches but not repeated letter searches. This is easy to explain in the letter model: The response to repeated letter bigrams such as AA is unaltered (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), and therefore the dissimilarity between AA and TT is unaffected by the asymmetry in spatial summation. By contrast, the dissimilarity between transposed letter pairs like AT and TA is directly driven by the asymmetry in spatial summation. We also note that the search TT among AA is much easier than the search for TA among AT. This is also explained by the letter model by the fact that the response to repeated letters is the same as the response to individual letters, leaving their discrimination unaltered. By contrast transposed letters are much more similar since their neural responses are much closer (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><p>To be sure that letter model predictions show the same pattern, we plotted the average response time predicted by the letter model for repeated letter (AA-BB) and transposed letter (AB-BA) searches. To assess the statistical significance, we performed a sign-rank test on the predicted RT. The letter model predictions were exactly as expected (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1F</xref>).</p><p>Next, we analyzed the model parameters in the letter model to ascertain whether the spatial summation in the neurons was indeed different for upright and inverted bigrams. To quantify the degree of asymmetry, we calculated for each neuron a spatial modulation index of the form MI = abs(w1-w2)/(w1+w2) where w1 and w2 are the estimated weights for each letter in the bigram. To avoid unnaturally large modulation indices, w1 and w2 values smaller than 0.01 were set to 0.01. The spatial modulation index for all 10 neurons for upright and inverted bigrams is shown in <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1G</xref>. It can be seen that the modulation index is larger in most cases for the upright bigrams. This difference was statistically significant, as assessed using a sign-rank test on the spatial modulation indices (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1H</xref>).</p></sec><sec id="s15" sec-type="appendix"><title>Experiment S1: Upright and inverted trigrams</title><p>Here, we asked whether the above results would extend to trigrams. We tested two predictions. First, we predicted greater spatial modulation for upright compared to inverted trigrams, on the premise that better discrimination of trigram transpositions should be driven by asymmetric spatial summation. Second, if repeated viewing of a trigram or word led to the formation of specialized trigram detectors, then the letter model (which is based only on knowledge of single letters) should produce larger errors compared to other trigrams. We tested this prediction by comparing model fits for searches involving frequent trigrams and words compared to other searches.</p><sec id="s15-1"><title>Methods</title><p>A total of nine subjects (six females, aged 24.5 ± 2.3 years) participated in the experiment. Six uppercase letters: A, G, N, R, T and Y were combined in all possible three-letter combination to form a total of 216 stimuli. These letters were chosen to include as many three-letter words as possible. In all, 15 three-letter words could be created using these letters (ANT, ANY, ART, GAG, GAY, NAG, NAY, RAG, RAN, RAT, RAY, TAG, TAN, TAR, and TRY).</p><p>Since the total number of possible search pairs is large (<sup>216</sup>C<sub>2</sub> = 23,220 pairs), we chose 500 search pairs such that the regression matrix of the part-sum model had full rank that is all the model parameters can be estimated reliably using linear regression. These 500 searches consisted of 368 random search pairs, 105 (<sup>15</sup>C<sub>2</sub>) word-word pairs, 15 (<sup>3!</sup>C<sub>2</sub>) transposed pairs of nonword comprised of letters G,N, and R. Further, another set of 15 (<sup>3!</sup>C<sub>2</sub>) transposed pairs were created using the word TAR. The search pairs formed using the words TAR, ART and RAT were presented only once (although they were counted as both word-word pairs and transposed pairs in the main analysis).</p><p>Subjects performed the same searches using upright and inverted trigrams. Stimuli subtended ~5° along the longer dimension. All subjects completed 2000 correct trials (500 searches x 2 orientations x two repetitions). All other details were identical to Experiment 1.</p></sec></sec><sec id="s16" sec-type="appendix"><title>Results</title><p>An example oddball array in the trigram experiment is shown in <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2A</xref>. Note that it is no longer meaningful to compare repeated letter trigrams (AAA-BBB) with transposed trigrams (ABC-BCA) because the repeated letter pairs contain two unique letters, whereas the transposed trigrams contain three unique letters. Subjects were highly consistent in both upright and inverted searches (split-half correlation between even and odd- subjects: r = 0.76 and 0.80, p&lt;0.00005). Upright and inverted dissimilarities were highly correlated (r = 0.80, p&lt;0.00005; <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2B</xref>), although upright searches had higher dissimilarity compared to inverted searches.</p><fig id="app2fig2" position="float"><label>Appendix 2—figure 2.</label><caption><title>Letter model fits for upright and inverted trigrams.</title><p>(<bold>A</bold>) Example trigram search array containing letter transpositions, with oddball target (NAR) among distractors (ARN). It can be seen that this search is substantially harder when inverted compared to upright. (<bold>B</bold>) Dissimilarity for inverted trigram searches (1/RT) plotted against dissimilarity for upright trigram searches for word-word pairs (red circles, n = 105), transposed letter pairs (blue diamonds, n = 30), and other pairs (gray circles, n = 365). (<bold>C</bold>) Observed dissimilarity for upright trigrams plotted against the predicted dissimilarity from the letter model with symbol conventions as in (<bold>B</bold>). (<bold>D</bold>) Cross-validated letter model correlation for upright and inverted trigrams. (<bold>E</bold>) Average spatial modulation index (across 10 neurons) for the first and second letters in the trigram. (<bold>F</bold>) Same as (<bold>E</bold>) but for the first and third letters. (<bold>G</bold>) Same as (<bold>E</bold>) but for the second and third letters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app2-fig2-v3.tif"/></fig><p>Next, we asked whether the letter model can predict dissimilarities between upright trigrams. As before, letter model predictions were highly correlated with the observed data (r = 0.79, p&lt;0.00005; <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2C</xref>) and this model fit approached the data consistency itself (<italic>r<sub>data</sub></italic> = 0.88). Model fits errors were acctually lower for transposed pairs compared to word-word pairs and other pairs (mean ± sd error: 0.1 ± 0.08 for word pairs; 0.07 ± 0.06 for transposed pairs; 0.11 ± 0.08 for other pairs; p=0.02, rank-sum test). The letter model was also able to predict dissimilarities between various trigram transpositions (r = 0.69, p&lt;0.00005; <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2C</xref>). Thus, trigram dissimilarities can be predicted by the letter model regardless of word status or trigram frequency.</p><p>We then compared model fits for upright and inverted trigrams. In both cases, the letter model predictions (r = 0.78 and 0.73 for upright and inverted) were close to the consistency of the data (<italic>r<sub>data</sub></italic> = 0.85 and 0.78; <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2D</xref>). To compare these model fits for upright vs inverted statistically, we performed a bootstrap analysis as before (Experiment 3). This analysis revealed no significant difference in model performance between upright and inverted searches (fraction of upright &lt;inverted normalized model correlation: p=0.07).</p><p>Finally we asked whether the spatial summation weights of the letter model were systematically different between upright and inverted trigrams. Since there are three spatial modulation weights for each neuron, we calculated the spatial modulation index for all possible pairs of weights (<xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2E,F,G</xref>). The spatial modulation ratio was larger for upright compared to inverted trigrams in two of the three pairs, and this difference attained statistical significance for the first and third letters in the trigram (<xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2F</xref>). We conclude that the spatial modulation is stronger for upright compared to inverted trigrams.</p></sec></boxed-text></app><app id="appendix-3"><title>Appendix 3</title><boxed-text><sec id="s17" sec-type="appendix"><title>Additional analysis for Experiment 4 (compound words)</title><p>In Experiment 4, we created compound words by combining two valid words such as FORGET from FOR and GET (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2A</xref>). This resulted in some valid words (e.g. FORGET, TEAPOT) and many invalid words (e.g. FORPOT and TEAGET). The full stimulus set is shown in <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>.</p><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>Stimulus set used for Experiment 4 (Compound Words).</title><p>The left and the right three letters words were combined to form a six-letter string. The strings that formed compound words are highlighted in red.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app3-fig1-v3.tif"/></fig><fig id="app3fig2" position="float"><label>Appendix 3—figure 2.</label><caption><title>Visual search for compound words (Experiment 4).</title><p>(<bold>A</bold>) Three-letter words (<italic>top</italic>) used to create compound words (<italic>bottom</italic>). (<bold>B</bold>) Illustration of letter and trigram models. In the letter model, the response to a compound word is a weighted sum of responses to the six single letters. In the trigram model, the response to a compound word is a weighted sum of its two trigrams. (<bold>C</bold>) Observed dissimilarity for compound words plotted against predicted dissimilarity from the letter model for word pairs (<italic>red</italic>) and other pairs (<italic>gray</italic>). (<bold>D</bold>) Cross-validated model correlations for the letter model, trigram model and the Orthographic Levenshtein distance (OLD) model. The upper bound on model fits is the split-half correlation (<italic>r<sub>sh</sub></italic>), shown in black with shaded error bars representing standard deviation across 30 random splits. Horizontal lines above shaded error bar depicts significant difference across different models. (<bold>E</bold>) Cross-validated model fits of the letter model for word-words pairs and nonword-nonword pairs. (<bold>F</bold>) Observed dissimilarities for three-letter words (<italic>black</italic>) and nonwords (<italic>red</italic>) plotted against letter model predictions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app3-fig2-v3.tif"/></fig><p>If valid words are driven by specialized detectors, responses to valid words should be less predictable by the single letter model. We formulated two specific predictions. First, we hypothesize that the dissimilarity between valid words (e.g. FORMAT vs TEAPOT) would yield larger model errors compared to invalid word pairs (e.g. DAYFOR vs ANYMAT). Second, we predicted that the dissimilarity between two invalid compound words (e.g. DAYFOR vs ANYMAT) should be explained better by their constituent trigrams (DAY, FOR, ANY, MAT) rather than by their constituent letters (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2B</xref>).</p></sec><sec id="s18" sec-type="appendix"><title>Methods</title><p>A total of eight subjects (four female, aged 25 ± 2.5 years) participated in the experiment. Twelve three-letter words were chosen: ANY, FOR, TAR, KEY, SUN, TEA, ONE, MAT, GET, PAD, DAY, POT. Each word was scrambled to obtain 12 three-letter nonwords containing the same letters. The 12 words were combined to form 36 compound words (<xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>), such that they appeared equally on the left and right half of the compound words. It can be seen that there are seven valid words, whereas the other compound words are pseudowords that carry no meaning. The compound words measured 6° along the longer dimension. Subjects completed 1260 correct trials (<sup>36</sup>C<sub>2</sub> search pairs x two repetitions). Additionally, subjects also performed visual search on three-letter words (n = 132, <sup>12</sup>C<sub>2</sub> × 2 repetitions) and their jumbled versions (n = 132). Trials timed out after 15 s. All other details were identical to Experiment 1.</p><p>Subjects were highly accurate on this task (mean ±std: 98 ± 1%). Outliers in the reaction times were removed using built-in routines in MATLAB (isoutlier function, MATLAB R2018a). This step removed 6.4% of the response time data.</p></sec><sec id="s19" sec-type="appendix"><title>Results</title><p>We recruited eight subjects to perform oddball search involving pairs of trigrams as well as six-letter strings. In all there were 12 three-letter words which resulted in <sup>12</sup>C<sub>2</sub> = 66 searches and 36 compound six-letter strings which resulted in <sup>36</sup>C<sub>2</sub> = 630 searches. We also included 12 three-letter nonwords created by transposing each three-letter words, resulting in an additional <sup>12</sup>C<sub>2</sub> = 66 searches. As before, subjects were highly consistent in their responses (split-half correlation between odd and even subjects: r = 0.54, p&lt;0.00005 for three-letter words; r = 0.46, p&lt;0.00005 for three-letter nonwords; r = 0.65, p&lt;0.00005 for six-letter words).</p><p>We started by using the single letter model as before to predict compound word responses. We took single neuron responses as before from Experiment 1, and took the response of each neuron to a compound word to be a weighted sum of its responses to the individual letters. Using these compound word responses, we calculated the dissimilarity between pairs of compound words, and used nonlinear fitting to obtain the best model parameters. The single letter model yielded excellent fits to the data (r = 0.68, p&lt;0.00005; <xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2C</xref>). This performance was comparable to the data consistency estimated as before (<italic>r<sub>data</sub></italic> = 0.72).</p><p>Next, we asked whether discrimination between compound words can be explained better as a combination of two valid three-letter words, or as a combination of all the constituent six letters. To address this question we constructed a new compositional model based on trigrams, and asked if its performance was better than the single letter model (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2D</xref>). The trigram-based letter model used trigram dissimilarity to construct neurons with trigram tuning, and spatial summation over the two trigrams to predict the 6-gram responses. To compare the performance of both models even though they have different numbers of free parameters, we used cross-validation: we fit both models on half the subjects and tested their performance on the other half. The letter model outperformed the trigram model (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2D</xref>). Because both models were trained on half the subjects and tested on the other half, the upper bound on their performance is simply the split-half correlation between the two halves of the data (denoted by <italic>r<sub>sh</sub></italic>). Indeed the letter model performance was close to this upper bound (<italic>r<sub>sh</sub></italic> = 0.56; <xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2D</xref>), suggesting that it explained nearly all the explainable variance in the data. Finally, the letter model outperformed a widely used model for orthographic distance – the Orthographic Levenshtein Distance (OLD) (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2D</xref>). Thus, compound word discrimination can be understood from single letters.</p><p>Finally, the letter model fits for word-word pairs and nonword-nonword pairs were not significantly different (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2E</xref>). This further validates the absence of local combination detectors (<xref ref-type="bibr" rid="bib14">Dehaene et al., 2005</xref>) in perception.</p></sec><sec id="s20" sec-type="appendix"><title>Three-letter word and nonword dissimilarities</title><p>To investigate whether the letter model can predict dissimilarities between three-letter words and non-words, we fit a separate letter model with six neurons as before to the word and non-word dissimilarities. If frequent viewing of words led to the formation of specialized word detectors, the letter model would show worse model fits compared to nonwords. However, we observed no such pattern: the letter model fits were equivalent for words (r = 0.69, p&lt;0.00005; <xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2F</xref>) and nonwords (r = 0.57, p&lt;0.00005; <xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2F</xref>) – and these fits approached the respective data consistencies (<italic>r<sub>data</sub></italic> = 0.67 for words, 0.68 for nonwords). We conclude that three-letter string dissimilarities can be predicted by the letter model regardless of word status.</p></sec><sec id="s21" sec-type="appendix"><title>Spatial summation weights</title><p>To investigate the spatial summation weights for each neuron, we plotted the estimated spatial summation weights separately (<xref ref-type="fig" rid="app3fig3">Appendix 3—figure 3</xref>). It can be seen that spatial summation is heterogeneous across neurons, but the spatial summation of the first neuron follows the characteristic W-shaped curve for letter position observed in studies of reading.</p><fig id="app3fig3" position="float"><label>Appendix 3—figure 3.</label><caption><title>Spatial summation weights for each neuron.</title><p>Estimated spatial summation weights (mean ±std across many random starting points of the nonlinear model fit algorithm) for each neuron in the letter model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app3-fig3-v3.tif"/></fig></sec></boxed-text></app><app id="appendix-4"><title>Appendix 4</title><boxed-text><sec id="s22" sec-type="appendix"><title>Experiments with longer strings</title><p>In the main text, we showed that bigram dissimilarity in visual search can be explained using a simple letter model with single letter responses that match perception, and a compositional spatial summation rule that predicts responses to bigrams. Here we asked whether this approach would generalize to longer strings of letters.</p><p>To this end, we performed four additional experiments on longer strings. In Experiment S2, we created trigrams with a fixed middle letter and all possible combinations of flanking letters, to create multiple three-letter words. In Experiment S3, subjects performed searches involving three-, four-, five- and six-letter searches with uppercase, lowercase and mixed case strings. In Experiments S4 and S5, we attempted to optimize the search pairs used to estimate model parameters.</p></sec><sec id="s23" sec-type="appendix"><title>Methods</title><sec id="s23-1"><title>Experiment S2: Trigrams with fixed middle letter</title><p>A total of eight subjects (five males, aged 23.9 ± 1.8 years) participated in this experiment. Seven uppercase letters: A, E, I, P, S, T and Y were combined (around the stem R that is xRx) in all pairs to form a total of 49 stimuli. These letters were chosen to maximize the occurrence of three-letter words and psuedowords in the stimulus set. The longer dimension of the stimuli was ~5°. Each subject completed searches corresponding to all possible pairs of stimuli (<sup>49</sup>C<sub>2</sub> = 1176) with two trials for each search. All other details were identical to Experiment 2.</p></sec><sec id="s23-2"><title>Experiment S3: Random string searches</title><p>A total of 12 subjects (nine female, aged 24.8 ± 1.64 years) participated in this experiment. All 26 uppercase and lowercase letters were used to create 1800 stimuli, which were organized into 900 stimulus pairs with varying string length. These 900 pairs comprised 300 6-gram uppercase pairs, 100 6-gram lowercase pairs, 100 6-gram mixed-case pairs, 100 5-gram uppercase pairs, 50 4-gram uppercase pairs, 50 3-gram uppercase pairs and 200 pairs with uppercase strings of differing lengths (50 pairs each of 6- vs 5-grams, 6- vs 4-grams, 5- vs 4-grams, 5- vs 3-grams = 200 pairs total). For each string length, letters were randomly combined to form strings with a constraint that all 26 letters should appear at least once at each location. Each stimulus pair was shown in two searches (with either item as target, and either on the left or right side). The trial timed out at 15 s for all searches.</p></sec><sec id="s23-3"><title>Experiment S4 – Optimized four-letter searches</title><p>In all, eight subjects (five females, aged 23.5 ± 2.3 years) participated in this experiment. To maximize the importance of each spatial location in a four-letter uppercase string, stimuli were created such that there were at least 75 search pairs with the same letter at either of the corresponding locations. Further, to reliably estimate the model parameters, the randomly chosen letters were arranged to minimize the condition number of the linear regression matrix X of the ISI model described below. In all there were 300 search pairs. The trial timed out after 15 s. All other details were similar to Experiment 2.</p></sec><sec id="s23-4"><title>Experiment S5 – Optimized six-letter searches</title><p>A total of nine subjects (five males, aged 24.1 ± 2.2 years) participated in this experiment. We chose 300 search pairs with six-letter strings, according to the same criteria as in Experiment S4. All other details were the same as in Experiment S4.</p></sec></sec><sec id="s24" sec-type="appendix"><title>Results</title><p>Cross-validated model fits across all experiments are shown in <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>. It can be seen that the letter model fit is close to the split-half consistency of the data. Thus, visual discrimination of longer strings can be explained using a compositional neural code. Below we discuss some experiment-specific findings of interest.</p><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>Letter model performance for varying length strings.</title><p>For each experiment, we obtained a cross-validated measure of model performance using six neurons as follows: each time we divided the subjects randomly into two halves, and trained the letter model on one half of the subjects and tested it on the other half. This was repeated for 30 random splits. The correlation between the model predictions and the average dissimilarity from the held-out half of the data was taken to be the model fit. The correlation between the observed dissimilarity between the two random splits of subjects is then the upper bound on model performance (mean ±std shown as <italic>gray shaded bars</italic>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app4-fig1-v3.tif"/></fig></sec><sec id="s25" sec-type="appendix"><title>Lowercase and mixed-case strings</title><p>Word shape is thought to play a role in reading lowercase letters, because of the upward deflection (e.g. l, d) and downward deflections (e.g. p, g) of letters which might confer a specific overall shape to a word. To conclusively establish this would require factoring out the contribution of individual letters to word discrimination, as with the letter model. We were therefore particularly interested in whether the letter model would predict the dissimilarity between lowercase and mixed-case strings where word shape might potentially play a role. As can be seen in <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>, cross-validated model predictions for lowercase letters were highly correlated with the observed data (r = 0.59, p&lt;0.00005). This correlation approached the upper bound given by the split-half reliability itself (<italic>r<sub>sh</sub></italic> = 0.64). Likewise, model predictions for mixed-case letters were also highly correlated with the observed data (r = 0.59, p&lt;0.00005; <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>). However, in this case model fits were well below the split-half consistency (<italic>r<sub>sh</sub></italic> = 0.72), suggesting that there is still some systematic unexplained variance in mixed-case strings. This gap in model fit could be simply due to the relatively few mixed-case searches used in this experiment (n = 100), or because of unaccounted factors like word shape. Nonetheless, the letter model explains a substantial fraction of variation in both lowercase and mixed case strings, suggesting that it can be used as a powerful baseline to elucidate the contribution of word shape to reading.</p></sec><sec id="s26" sec-type="appendix"><title>Unequal length strings</title><p>The letter model can be used to calculate responses to any string length, provided the spatial summation weights are known. Given the relatively few searches for unequal lengths in our data, we fit the letter model to unequal length strings using six neurons. Doing so still raised a fundamental issue: which subset of the six spatial summation weights for each neuron should be used to calculate the response to a four-letter string? This requires aligning the four-letter string to the six-letter string in some manner.</p><p>To address this issue, we evaluated the letter model fit on four possible alignments between longer and shorter strings, and asked whether model predictions were better for any one alignment compared to others. We aligned the smaller length string to either the left, right, centre or edge of the longer string. Model performance for these different variations is shown in <xref ref-type="table" rid="app4table1">Appendix 4—table 1</xref>. It can be seen that the model fits are comparable across different choices. However, edge alignment is slightly but not significantly better than other choices. We therefore used edge alignment for all subsequent model predictions.</p><table-wrap id="app4table1" position="float"><label>Appendix 4—table 1.</label><caption><title>Model fits for various choices of string alignment.</title></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" valign="top">Alignment</th><th colspan="4" valign="top">Letter model correlation</th></tr><tr><th valign="top"> six vs five letter strings</th><th valign="top">six vs four-letter strings</th><th valign="top">five vs three-letter strings</th><th valign="top">four vs three-letter strings</th></tr></thead><tbody><tr><td valign="top">Left: ABCDEF vs EFGHxx</td><td valign="top">0.54</td><td valign="top">0.66</td><td valign="top">0.58</td><td valign="top">0.57</td></tr><tr><td valign="top">Right: ABCDEF vs xxEFGH</td><td valign="top">0.51</td><td valign="top">0.66</td><td valign="top">0.57</td><td valign="top">0.58</td></tr><tr><td valign="top">Centre: ABCDEF vs xEFGHx</td><td valign="top">-</td><td valign="top"><bold>0.68</bold></td><td valign="top">0.58</td><td valign="top">-</td></tr><tr><td valign="top">Edge: ABCDEF vs EFxxGH</td><td valign="top"><bold>0.55</bold></td><td valign="top">0.63</td><td valign="top"><bold>0.60</bold></td><td valign="top"><bold>0.59</bold></td></tr></tbody></table></table-wrap><p>In each case, we fit the letter model with unknown weights corresponding to the longer length. The alignment is indicated by the position of ‘x”s in the string. For instance, ‘Left’ alignment means that a 6-letter string ABCDEF is matched to a four-letter string EFGH by assuming that the response to EFGH is created using the first four weights of spatial summation. Likewise, right alignment means that EFGH is aligned to the right, and therefore its response is created using the last four weights in the six-letter letter model. The best alignment is highlighted for each column in <bold>bold</bold>. None of the correlation coefficient differences were statistically significant (p&gt;0.05, Fisher’s z-test).</p></sec></boxed-text></app><app id="appendix-5"><title>Appendix 5</title><boxed-text><sec id="s27" sec-type="appendix"><title>Estimating letter dissimilarity from bigrams</title><sec id="s27-1"><title>Part-sum model</title><p>The letter model described in the text has many desirable features but requires as input the responses to single letters, which were obtained from searches involving single isolated letters. However, it could be that bigram representations can be understood in terms of component letter responses that are different from the responses of letters seen in isolation. It could also be that letter responses are different at each location.</p><p>To address these issues, we developed an alternate model in which bigram dissimilarities can be written in terms of unknown single letter dissimilarities. These single letter dissimilarities can be estimated in the model. In this model, which we call the part-sum model, the dissimilarity between two bigrams AB and CD is written as the sum of all pairs of part dissimilarities in the two bigrams (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1A</xref>). Specifically:<disp-formula id="equ2"><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">B</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:msub><mml:mi mathvariant="normal">L</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:msub><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">X</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">X</mml:mi><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">W</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">W</mml:mi><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula>where CL<sub>AC</sub> is the dissimilarity between letters at Corresponding Left (CL) locations (A and C), CR<sub>BD</sub> is the dissimilarity between letters at the Corresponding Right (CR) locations (B and D), X<sub>AD</sub> and X<sub>BC</sub> are the dissimilarities between letters across locations in the two bigrams (A and D, B and C), and W<sub>AB</sub> and W<sub>CD</sub> are the dissimilarities of letters within each bigram.</p><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>Predicting bigram dissimilarity using part-sum model.</title><p>(<bold>A</bold>) Schematic of the part sum model. According to this model, the dissimilarity (1/RT) between bigrams ‘AB’ and ‘CD’ is written as a linear sum of dissimilarities of its corresponding part terms (AC and BD, shown in red), across part terms (AD and BC, shown in yellow), and within part terms (AB and CD, shown in blue). (<bold>B</bold>) Correlation between the observed and predicted dissimilarities (1/seconds). Each point represents one search pair (n = <sup>49</sup>C<sub>2</sub>=1176). Word-word pairs are highlighted using red diamonds, and frequent bigram pairs are highlighted using blue circles. Dotted lines represent unity slope line. (<bold>C</bold>) Correlation between the estimated weights at corresponding location left with estimated weights at 1) corresponding location right (red), 2) across location (yellow), and 3) within location (blue). Each point represents one letter pair (n = <sup>7</sup>C<sub>2</sub>=21). Dotted lines represent positive and negative unity slope line. (<bold>D</bold>) Perceptual space of the single letter dissimilarities, that are the model coefficients of part terms at left corresponding location (<bold>E</bold>) Schematic of the Independent Spatial Interaction model. In this model, we use the observed letter-pair dissimilarities and only estimate the weights of these letter-pair dissimilarities across different locations. (<bold>F</bold>) Comparing part-sum and ISI model fits. Bar plots represents mean correlation coefficient between the observed and predicted dissimilarities. Error bars represent one standard deviation across 30 splits. Black horizontal line represents mean split-half correlation (r<sub>sh</sub>) and the shaded error bar represents one standard deviation around the mean. (****, p&lt;0.00005, **, p&lt;0.005).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app5-fig1-v3.tif"/></fig><p>The part-sum model works because a given letter dissimilarity CL<sub>AC</sub> will occur in the dissimilarity of many bigram pairs (e.g. in the pair AB-CD and in AE-CF) thereby allowing us to estimate its unique contribution. Since there are seven parts, there are <sup>7</sup>C<sub>2</sub> = 21 possible part-pairs of each type (i.e. for CL, CR, X and W terms), resulting in 21 × 4 = 84 unknown part dissimilarities. Since a given bigram experiment contains all possible <sup>49</sup>C<sub>2</sub> = 1176 bigram searches, there are many more observations than unknowns. The combined set of bigram dissimilarities can be written in the form of a matrix equation <bold>y</bold> = <bold>Xb</bold> where <bold>y</bold> is a 1176 × 1 vector of observed bigram dissimilarities, <bold>X</bold> is a 1176 × 85 matrix containing the number of times (0, 1 or 2) a given letter-pair of each type (CL, CR, X and W) contributes to the overall dissimilarity, and b is a 85 × 1 vector of unknown letter dissimilarities of each type (21 each of CL, CR, X and W and one constant term). The unknown letter dissimilarities of each type was estimated using standard linear regression (<italic>regress</italic> function, MATLAB).</p><p>The part sum model has several advantages over the letter model: (1) It is linear which means that its parameters can be uniquely estimated; (2) it is compositional in that the net dissimilarity between two bigrams is explained using the constituent parts without invoking more complex interactions; (3) it can account for potentially different part relations at each location in the two bigrams. We have previously shown that the part-sum model can explain the dissimilarities between a variety of objects (<xref ref-type="bibr" rid="bib51">Pramod and Arun, 2016</xref>).</p><p>The part sum model yielded excellent fits to the data (r = 0.88, p&lt;0.00005; <xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1B</xref>) that were close to the reliability of the data (<italic>r<sub>data</sub></italic> = 0.90). As before, we observed no systematic deviations between model fits for frequent bigrams compared to infrequent bigrams (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1B</xref>; average absolute residual error for the top 20 bigram pairs with highest mean bigram frequency: 0.09 ± 0.1 s<sup>−1</sup>; for the bottom-20 bigram pairs: 0.11 ± 0.08 s<sup>−1</sup>; p=0.42, rank-sum test). To assess whether the part dissimilarities of each type (CL, CR, X and W) were related to each other, we plotted each of CR, X and W terms against the CL terms (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1C</xref>). The CR and X terms were highly positively correlated (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1C</xref>), whereas the W terms were negative in sign and negatively correlated (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1C</xref>). The negative values of the W terms means that bigrams with dissimilar letters become less dissimilar, an effect akin to distractor heterogeneity in visual search (<xref ref-type="bibr" rid="bib18">Duncan and Humphreys, 1989</xref>; <xref ref-type="bibr" rid="bib69">Vighneshvel and Arun, 2013</xref>). We conclude that the CL, CR, X and W terms in the part-sum model are driven by a common part representation.</p><p>To visualize this underlying letter representation, we performed multidimensional scaling on the estimated part dissimilarities of the CL terms. In the resulting plot, nearby letters represent similar letters (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1D</xref>). It can be seen that I and T, M and N are similar as in the single-letter representation (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>). These single letter dissimilarities estimated from bigrams using the part-sum model were highly correlated with the single-letter dissimilarities directly observed from visual search with isolated letters (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1D</xref>).</p><p>We conclude that bigram dissimilarities can be predicted from a common underlying letter representation that is identical to that of single isolated letters.</p></sec></sec><sec id="s28" sec-type="appendix"><title>Equivalence between part-sum and letter model</title><p>Given that the part-sum model and letter model both give equivalent fits to the data, we investigated how they are related. Consider a single neuron whose response to a bigram AB is given by: <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where r<sub>A</sub> and r<sub>B</sub> are its responses to A &amp; B, and α is the spatial weight of A relative to B. Similarly its response to the bigram CD can be written as r<sub>CD</sub>=αr<sub>C</sub> + r<sub>D</sub>. Then the dissimilarity between AB and CD can be written as<disp-formula id="equ3"><mml:math id="m3"><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula><disp-formula id="equ4"><mml:math id="m4"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">α</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula><disp-formula id="equ5"><mml:math id="m5"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">α</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ6"><mml:math id="m6"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">α</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ7"><mml:math id="m7"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ8"><mml:math id="m8"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced> <mml:mi/><mml:mo>-</mml:mo> <mml:mi/><mml:mi mathvariant="normal">α</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>Thus, the squared dissimilarity between AB and CD can be written as a weighted sum of squared dissimilarities between parts at corresponding locations (A-C and B-D), parts at opposite locations (A-D and B-C) and between parts within each bigram (A-B and C-D), which is essentially the same as the part-sum model. The same argument extends to multiple neurons because the total bigram dissimilarity will be the sum of bigram dissimilarities across all neurons.</p><p>There are however two important differences. First, the part sum model is written in terms of a weighted sum of part dissimilarities, whereas the above equation refers to a weighted sum of squared dissimilarities. However, the squared sum of distances and a weighted sum of distances are highly correlated, so the essential relation will still hold. Second, the letter model predicts that the across-bigram terms (X<sub>AD</sub>, X<sub>BC</sub>) should be similar in magnitude but opposite in sign to the within-bigram terms (W<sub>AB</sub>, W<sub>CD</sub>). These weights are similar in magnitude but not exactly equal, as can be seen in Fig S8C. The part-sum model thus allows for greater flexibility in part interactions compared to the letter model.</p></sec><sec id="s29" sec-type="appendix"><title>Reducing part-sum model complexity (ISI model)</title><p>The observation that a common set of letter dissimilarities drive the part-sum model suggests that the part-sum model can be simplified. We therefore devised a reduced version of the part-sum model – called the Independent Spatial Interaction (ISI) model – in which the CL, CR, X and W terms are scaled versions of the single letter dissimilarities (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1E</xref>). Specifically, the dissimilarity between bigrams AB and CD is:<disp-formula id="equ9"><mml:math id="m9"><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced> <mml:mi/><mml:mo>+</mml:mo> <mml:mi/><mml:mi>c</mml:mi></mml:math></disp-formula>where <italic>d<sub>AC</sub></italic> is the observed dissimilarity between the left letters A &amp; C from visual search and <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is an unknown scaling term, <italic>d<sub>BD</sub></italic> is the observed dissimilarity between the right letters B &amp; D, and α<sub>20</sub> is an unknown scaling term. Likewise, α<sub>11</sub>is an unknown scaling term for the net dissimilarity <inline-formula><mml:math id="inf6"><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> between letters across locations, β<sub>11</sub> is the unknown scaling term for the net dissimilarity <inline-formula><mml:math id="inf7"><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> between letters within the two bigrams and <italic>c</italic> is a constant. Thus, the ISI model has only 5 free parameters: <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub> <mml:mi/><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi> <mml:mi/><mml:mi>c</mml:mi></mml:math></inline-formula>. These parameters can be estimated by solving the matrix equation <bold>y</bold> = <bold>Xb</bold> where <bold>y</bold> is a 1176x1 vector of observed bigram dissimilarities, <bold>X</bold> is a 1176 x 5 matrix containing the net single dissimilarity of each type (CL, CR, X &amp; W) that contributes to the total dissimilarity, and <bold>b</bold> is a 5 x 1 vector of unknown weights corresponding to the contribution of each type of dissimilarity (plus a constant).</p><p>The performance of the ISI model is summarized in <xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1F</xref>. It can be seen that, despite having only five free parameters compared to 85 parameters of the part-sum model, the ISI model yields comparable fits to the data (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1F</xref>).</p></sec><sec id="s30" sec-type="appendix"><title>ISI model performance across all experiments</title><p>Next we asked whether the ISI model can be generalized to explain dissimilarities between longer strings. Consider two n-letter strings <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The net dissimilarity between the two strings can be written as:<disp-formula id="equ10"><mml:math id="m10"><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo> <mml:mi/><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:math></disp-formula></p><p>In this manner, we fit the ISI model to all experiments. The resulting cross-validated model fits are shown together with the letter model in <xref ref-type="fig" rid="app5fig2">Appendix 5—figure 2</xref>. It can be seen that the ISI model performance is comparable to that of the letter model across all experiments.</p><fig id="app5fig2" position="float"><label>Appendix 5—figure 2.</label><caption><title>ISI and letter model performance across all experiments.</title><p>For each experiment, we obtained a cross-validated measure of both neural and ISI model performance as follows: each time we divided the subjects randomly into two halves, and trained the letter model on one half of the subjects and tested it on the other half. This was repeated for 30 random splits. The correlation between the model predictions and the average dissimilarity from the held-out half of the data was taken to be the model fit. The correlation between the observed dissimilarity between the two random splits of subjects is then the upper bound on model performance (mean ±std shown as <italic>gray shaded bars</italic>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app5-fig2-v3.tif"/></fig></sec><sec id="s31" sec-type="appendix"><title>Reducing the complexity of the ISI model</title><p>According to the ISI model, the net dissimilarity between two n-grams can be written as a weighted sum of dissimilarities between letter pairs that are varying distances apart. We wondered if the ISI model can be simplified further if there is a systematic pattern whereby these weight corresponding to a given letter pair varies systematically with letter position and distance between the letters.</p><p>To assess this possibility, we plotted model coefficients of the ISI model estimated from Experiment S3 along two dimensions. First, we asked if the contribution of letter pairs at corresponding locations in the two n-grams varies with letter position. For varying string lengths (three-, four-, five- and six-letter strings), we observed a characteristic U-shaped function whereby the edge letters contribute more to the net dissimilarity compared to the middle letters (<xref ref-type="fig" rid="app5fig3">Appendix 5—figure 3A</xref>). Second, we asked if model weights decrease systematically with inter-letter distance. This was indeed the case regardless of the starting letter in the pair (<xref ref-type="fig" rid="app5fig3">Appendix 5—figure 3B</xref>). Finally, we note that across and within part terms are roughly equal in magnitude but opposite in sign (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1C</xref>).</p><fig id="app5fig3" position="float"><label>Appendix 5—figure 3.</label><caption><title>Reducing the ISI model.</title><p>(<bold>A</bold>) ISI model coefficients <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> as a function of starting letter position <italic>i</italic>, for Experiment S3, for varying string lengths. (<bold>B</bold>) ISI model coefficients <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as a function of inter-letter distance <italic>k</italic> for Experiment S3, for varying string lengths. (<bold>C</bold>) ISI model coefficients (both α<sub>ik</sub> and <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi mathvariant="normal">β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) plotted against the predicted ISI model coefficients from the SID model. Both models are fitted to data from Experiment 4 (compound words). (<bold>D</bold>) Observed dissimilarity in Experiment four plotted against predicted dissimilarity from the SID model. (<bold>E</bold>) Cross-validated model correlation for ISI and SID models.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app5-fig3-v3.tif"/></fig><p>The above pattern of weights in the ISI model suggest that we can make two simplifying assumptions. First, the weight of the starting letter is a U-shaped function when the inter-letter distance is zero <inline-formula><mml:math id="inf14"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>. Second, weights decrease exponentially thereafter with increasing inter-letter distance. Specifically:<disp-formula id="equ11"><mml:math id="m11"><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi> <mml:mi/><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi> <mml:mi/><mml:mi>i</mml:mi> <mml:mi/><mml:mo>=</mml:mo> <mml:mi/><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>n</mml:mi></mml:math></disp-formula><disp-formula id="equ12"><mml:math id="m12"><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">τ</mml:mi></mml:mrow></mml:msup> <mml:mi/><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi> <mml:mi/><mml:mi>k</mml:mi> <mml:mi/><mml:mo>≥</mml:mo> <mml:mi/><mml:mn>1</mml:mn></mml:math></disp-formula><disp-formula id="equ13"><mml:math id="m13"><mml:msub><mml:mrow><mml:mi mathvariant="normal">β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mi/><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi> <mml:mi/><mml:mi>k</mml:mi> <mml:mi/><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:math></disp-formula>where a, b, c and τ are the free parameters in this model. This simplified model, which we call the Spatial Interaction Decay (SID) model has only five parameters and can be used to predict the dissimilarities between strings of arbitrary length. The model parameters are obtained using nonlinear gradient descent methods (<italic>nlinfit</italic> function, MATLAB).</p><p>To illustrate the performance of the SID model in comparison to the ISI model, we fit the model to 6-letter compound words (Experiment 4). To compare the two models, we plotted the ISI model terms directly estimated from the search data against the ISI model terms predicted from the SID model. This yielded a strong positive correlation (<xref ref-type="fig" rid="app5fig3">Appendix 5—figure 3C</xref>). The SID model also yielded excellent fits to the data (<xref ref-type="fig" rid="app5fig3">Appendix 5—figure 3D</xref>), and both models yielded comparable fits (<xref ref-type="fig" rid="app5fig3">Appendix 5—figure 3E</xref>).</p><p>To evaluate this pattern across all experiments, we fit both SID and ISI models to all experiments. Here too we obtained qualitatively similar fits for the two models (<xref ref-type="fig" rid="app5fig4">Appendix 5—figure 4</xref>). To confirm whether the SID model trained on one experiment can capture the variations in another, we trained the SID model on data from Experiment S5 and evaluated it on all other experiments. This too yielded largely similar but smaller predictions (<xref ref-type="fig" rid="app5fig4">Appendix 5—figure 4</xref>). This decrease in model fit suggests that model parameters are somewhat dependent on the search pairs chosen.</p><fig id="app5fig4" position="float"><label>Appendix 5—figure 4.</label><caption><title>ISI and SID model fits across all experiments.</title><p>Cross-validated model fits for the ISI and SID models across all experiments. In each case the SID and ISI models were fit on a randomly chosen half of the subjects and tested on the other half. The SID (ES5) bars refer to the SID model trained on Experiment S5 and tested on data from a randomly chosen half of subjects in each experiment.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app5-fig4-v3.tif"/></fig><p>We conclude that dissimilarities between arbitrary letter strings can be predicted using highly simplified models that operate on single letter dissimilarities and simple compositional rules.</p></sec><sec id="s32" sec-type="appendix"><title>Comparing upright and inverted bigrams using part-sum model</title><p>The results in Appendix 2 were based on fitting the letter model to upright and inverted bigrams but assuming a fixed set of single letter responses derived from uppercase letters. The fact that the letter model yielded excellent fits to both upright and inverted bigrams validates this assumption. Nonetheless, we wondered whether differences between upright and inverted bigram searches can be explained solely by different letter representations or by differences in letter interactions.</p><p>To investigate this possibility, we fit the part-sum model to upright and inverted bigram searches (<xref ref-type="fig" rid="app5fig5">Appendix 5—figure 5A</xref>). The part-sum model also yielded equivalent fits to both upright and inverted searches (<xref ref-type="fig" rid="app5fig5">Appendix 5—figure 5B</xref>). If model predictions were similar, we reasoned that the difference between upright and inverted searches must be explained by differences in model parameters. To this end, we compared the estimated letter dissimilarities of each type (CL, CR, X and W) in the upright and inverted searches (<xref ref-type="fig" rid="app5fig5">Appendix 5—figure 5C</xref>). Model terms were comparable in magnitude for the CL terms, but were systematically weaker for both CR, X and W terms for inverted compared to upright searches (<xref ref-type="fig" rid="app5fig5">Appendix 5—figure 5C</xref>). However in all cases, the recovered letter dissimilarities were correlated between upright and inverted conditions (correlation between upright and inverted model terms: r = 0.93, 0.91, 0.97 and 0.87 for CL, CR, X and W terms; all correlations p&lt;0.00005).</p><fig id="app5fig5" position="float"><label>Appendix 5—figure 5.</label><caption><title>Part-sum model fits for upright and inverted bigrams.</title><p>(<bold>A</bold>) Schematic of the part-sum model, in which the net dissimilarity between two bigrams is given as a linear sum of letter dissimilarities at corresponding locations (CL and CR), across-bigrams (X) and within-bigrams (W). (<bold>B</bold>) Cross-validated model correlation of the part sum model for upright and inverted bigrams. (<bold>C</bold>) Average model coefficients (mean ±sem) of each type for upright and inverted bigrams. Asterisks denote statistical significance (**** is p&lt;0.00005) obtained on a sign-rank test comparing 15 letter dissimilarities between upright and inverted conditions).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app5-fig5-v3.tif"/></fig></sec><sec id="s33" sec-type="appendix"><title>Comparing upright and inverted trigrams using part-sum model</title><p>The part sum model applied to trigrams is depicted in <xref ref-type="fig" rid="app5fig6">Appendix 5—figure 6A</xref>. In this model, the net dissimilarity between two trigrams can be written as a sum of single letter dissimilarities at every possible pair of locations. These locations are grouped as corresponding letters at left (C1), middle (C2) and right (C3) locations, letters across trigrams that are one letter apart starting from the left letter (XN1) or the middle letter (XN2), letters across trigrams that are two letters apart (XF), letters within each trigram that are one letter apart starting from the left letter (WN1) or middle letter (WN2), and letters within each trigram that are two letters apart (WF). Thus the full part-sum model has 9 groups of letter dissimilarities (C1, C2, C3, XN1, XN2, XF, WN1, WN2, WF) each having <sup>6</sup>C<sub>2</sub> = 15 unknown single letter dissimilarities. Together with a constant term, this part-sum model has 9 × 15 + 1 = 136 free parameters. Since we have 500 searches each for upright and inverted trigrams, the part-sum model can be fit to this data to estimate these free parameters using standard linear regression.</p><fig id="app5fig6" position="float"><label>Appendix 5—figure 6.</label><caption><title>Part-sum model fits for upright and inverted trigrams.</title><p>(<bold>A</bold>) Schematic of part-sum model for trigrams. (<bold>B</bold>) Cross-validated model correlation of part-sum model for upright and inverted trigrams. (<bold>C</bold>) Average model coefficient (averaged across <sup>6</sup>C<sub>2</sub> = 15 terms) of each type for upright and inverted trigrams. Asterisks indicate statistical significance (* is p&lt;0.05, ** is p&lt;0.005, etc) calculated using a sign-rank test comparing the upright and inverted model terms. (<bold>D</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app5-fig6-v3.tif"/></fig><p>Cross-validated model fits for the part-sum model are shown in <xref ref-type="fig" rid="app5fig6">Appendix 5—figure 6B</xref>. It can be seen that the part-sum model explains nearly all the explainable variance in the data for both upright and inverted trigrams (<xref ref-type="fig" rid="app5fig6">Appendix 5—figure 6B</xref>). This in turn means that differences between upright and inverted trigrams can be explained using differences in model parameters. This was indeed the case: on plotting the strength of model terms of each type it was clear that seven of the nine types of model terms (C1, C2, C3, XN2, XF, WN2, WF) were systematically larger for upright trigrams compared to inverted trigrams (<xref ref-type="fig" rid="app5fig6">Appendix 5—figure 6C</xref>). Finally, we confirmed that model terms for upright and inverted trigrams were highly correlated (correlation between upright and inverted model terms, averaged across nine model term types: r = 0.65 ± 0.1, p&lt;0.05 in all cases).</p><p>We conclude that upright and inverted trigram searches can be explained using the part-sum model driven by a common single letter representation.</p></sec></boxed-text></app><app id="appendix-6"><title>Appendix 6</title><boxed-text><sec id="s34" sec-type="appendix"><title>Jumbled word reading (Experiment S6)</title><p>Here, in Experiment S6, we tested subjects on a jumbled word reading task, where they had to view a jumbled word and recognize the original word.</p></sec><sec id="s35" sec-type="appendix"><title>Methods</title><sec id="s35-1"><title>Procedure</title><p>A total of 16 subjects (nine male, aged 24.8 ± 2.1 years) participated in the task. Other details were similar to Experiment 5.</p></sec><sec id="s35-2"><title>Stimuli</title><p>We chose 300 words such that no two words were anagrams of each other. These comprised 75 four-letter words, 150 five-letter words and 75 six-letter words. Jumbled words were created by shuffling two, three, or four letters of each word. There were an equal proportion of two-, three-, and four-letter transpositions. All stimuli were presented in uppercase against a black background.</p></sec><sec id="s35-3"><title>Task</title><p>Each trial began with a fixation cross shown for 0.5 s followed by a jumbled word that appeared for 5 s (for the first six subjects) and 7 s (for the rest), or until the subject made a response by pressing the space bar on the keyboard. Subjects were asked to press a key as soon as they could recognize the unjumbled word. To ensure that subjects correctly recognized the unjumbled word, they were asked to type the unjumbled word within 10 s of pressing the space bar. The response time was taken as the time at which the subject pressed the space bar. To avoid any memory effects, the same set of jumbled words were shown to all subjects exactly once. We analysed response times only on trials in which the subject subsequently entered the correct word.</p></sec><sec id="s35-4"><title>Data analysis</title><p>Subjects were reasonably accurate on this task (average accuracy: 59.5 ± 8% across 300 words). Response times for wrongly typed words were discarded. Words correctly solved by more than six subjects (n = 238) were included for further analysis. Since trials were self-paced, we did not remove any outliers in the reaction times. Lexical properties were obtained from the English Lexicon Project (<xref ref-type="bibr" rid="bib5">Balota et al., 2007</xref>).</p></sec></sec><sec id="s36" sec-type="appendix"><title>Results</title><p>Of a total of 300 jumbled words tested, we selected for further analysis 238 words that were correctly unjumbled by more than two-thirds of the subjects. Subjects responded quickly and accurately to these words (mean ±std of accuracy: 71 ± 9%; response time: 2.13 ± 0.33 s across 238 words). Subjects took longer to respond to some jumbled words (e.g. REHID) compared to others (e.g. DBTOU), as seen in the sorted response times (<xref ref-type="fig" rid="app6fig1">Appendix 6—figure 1A</xref>). These patterns were consistent across subjects, as evidenced by a significant split-half correlation (r = 0.55, p&lt;0.00005 between odd- and even-numbered subjects).</p><fig id="app6fig1" position="float"><label>Appendix 6—figure 1.</label><caption><title>Jumbled word task (Experiment S7).</title><p>(<bold>A</bold>) Response times in the jumbled word task sorted in descending order. Shaded error bars represent s.e.m. Some example words are indicated using dotted lines. The split-half correlation between subjects (<italic>r<sub>sh</sub></italic>) is indicated on the top left. (<bold>B</bold>) Schematic of visual word space, with one stored word (DRINK) and two jumbled versions (DRNIK and NIRDK). We predicted that the time taken by subjects to unscramble a jumbled word would be proportional to its dissimilarity to the stored word. Thus, subjects would take longer to unscramble NIRDK compared to DRINK. (<bold>C</bold>) Observed response times in the jumbled word task plotted against predictions from the letter model based on single letters with spatial summation. Each point represents one word. Asterisks indicate statistical significance (**** is p&lt;0.00005). (<bold>D</bold>) Observed response times in the jumbled word task plotted against Orthographic Levenshtein (OL) distance. Each point represents one word. Asterisks indicate statistical significance (**** is p&lt;0.00005). (<bold>E</bold>) Cross-validated model correlations for the letter model, OLD model, lexical model and the neural+lexical model. Model correlations were obtained by training each model on one half of subjects, and evaluating the correlation on the other half (error bars represent standard deviation across 1000 random splits). The upper bound on model fits is the split-half correlation (<italic>r<sub>sh</sub></italic>), shown in black with shaded error bars representing standard deviation across the same random splits. All correlations were individually statistically significant (p&lt;0.00005). Horizontal lines above shaded error bar depicts significant difference across different models that is the fraction of splits in which the observed difference was violated. All significant comparisons are indicated.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app6-fig1-v3.tif"/></fig><p>Can these patterns in unscrambling time be explained using the letter model? To do so, we reasoned that jumbled words with large dissimilarity to the original word will take longer to elicit a response (<xref ref-type="fig" rid="app6fig1">Appendix 6—figure 1B</xref>). Accordingly, we took the average response times to each jumbled word and asked whether it can be predicted using the single letter model described previously. For each word length, we optimized the weights of the single letter model to find the best fit to this data, and then combined the predictions across all word lengths to obtain a composite measure of performance. The single letter model yielded excellent fits to the data (r = 0.76, p&lt;0.00005; <xref ref-type="fig" rid="app6fig1">Appendix 6—figure 1C</xref>). This model fit was comparable to the data consistency (<italic>r<sub>data</sub></italic> = 0.70). An alternate distance model - Orthographic Levenshtein (OL) distance (<xref ref-type="bibr" rid="bib33">Levenshtein, 1966</xref>) – calculates the number of edits required to transform one string to other. This model neither accounts for letter similarity nor the position of edit. Hence, it fails to account for all the variance in the data (r = 0.44, p&lt;0.00005; <xref ref-type="fig" rid="app6fig1">Appendix 6—figure 1D</xref>).</p><p>The above finding shows that human performance on unscrambling words is driven primarily by the visual dissimilarity between the jumbled and original word. However, it does not rule out the presence of lexical factors. To assess this possibility, we formulated a model to predict the unscrambling time as a linear sum of many lexical factors. We used five lexical properties: log word frequency, log mean letter frequency, log mean bigram frequency of the jumbled word, log mean bigram frequency of the unjumbled that is original word, and the number of orthographic neighbors (see Materials and methods). To avoid overfitting by either model, we trained both models on one-half of the subjects and tested it on the other half. This lexical model yielded relatively poor fits (r = 0.30, p&lt;0.00005, <xref ref-type="fig" rid="app6fig1">Appendix 6—figure 1E</xref>) compared to visual dissimilarity from both single letter model and OL distance model. The difference in model fits was statistically significant (p&lt;0.05, Fisher’s z-test). Among the lexical factors, word frequency and letter frequency contributed the most compared to the others (partial correlation of each lexical factor after accounting for all others: r = −0.23, p&lt;0.0005 for log word frequency, r = 0.18, p&lt;0.05 for log mean letter frequency; r = 0.05, p=0.49 for log mean bigram frequency of jumbled word; r = −0.02, p=0.77 for log mean bigram frequency in original word; r = 0.04, p=0.58 for number of orthographic neighbors).</p><p>To assess the extent of shared variance in the two models, we calculated the partial correlation between the observed data and the lexical model predictions after factoring out the contribution from visual dissimilarity. This revealed a small partial correlation (r = 0.31, p&lt;0.00005). Conversely, the partial correlation for the single letter model after factoring out the lexical model was much higher (r = 0.75, p&lt;0.00005). Thus, visual dissimilarity from the single letter model dominates jumbled word reading.</p><p>Finally, we asked whether both visual dissimilarity and lexical factors contribute to the jumbled word task. We created a combined model in which the jumbled word response times were a linear combination of the predictions of both models. This combined model yielded better predictions than either model by itself (r = 0.78, p&lt;0.00005, <xref ref-type="fig" rid="app6fig1">Appendix 6—figure 1E</xref>). To assess the statistical significance of these results, we performed a bootstrap analysis. On each trial, we trained three models on the dissimilarity obtained from considering only one randomly chosen half of subjects: the visual dissimilarity model, the lexical model and the combined model. We calculated the correlation between all three model predictions on the other half of the data, and repeated this procedure 1000 times. The OL distance model does not have any free parameters, hence the distances were directly correlated with the other half of the data. Across these samples, the lexical model fits never exceeded the visual dissimilarity model, suggesting that the visual dissimilarity model was significantly better (p&lt;0.05). Likewise, the combined model was only marginally better than the visual letter model (fraction of combined &lt;visual: p=0.07) but was significantly better than the lexical model (fraction of combined &lt;lexical: p=0).</p><p>We conclude that performance on the jumbled word task relies primarily on visual dissimilarity. We propose that this initial visual representation of a word allows the subject to make a quick guess at the correct word without explicit symbolic manipulation.</p></sec></boxed-text></app><app id="appendix-7"><title>Appendix 7</title><boxed-text><sec id="s37" sec-type="appendix"><title>Additional analyses for Experiments 6 and 7</title><sec id="s37-1"><title>Stimulus set</title><p>32 words were chosen of varying frequency of occurrence and the nonwords were created by either transposition or substitution of middle or edge letters. 10 single letters: E, S, A, R, O, L, I, T, N, and D were used to form words. The full set of strings used Experiments 6 and 7 is shown below.</p><table-wrap id="app7table1" position="float"><label>Appendix 7—table 1.</label><caption><title>List of 32 words and 32 nonwords used in Experiment 6 &amp; 7.</title><p>All words and nonwords were created from 10 single letters whose activations were also measured in the experiment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" valign="top">Middle Letter Transposition</th><th colspan="2" valign="top">Edge Letter Transposition</th><th colspan="2" valign="top">Middle Letter Substitution</th><th colspan="2" valign="top">Edge Letter Substitution</th></tr><tr><th valign="top"> Words</th><th valign="top">Nonwords</th><th valign="top">Words</th><th valign="top">Nonwords</th><th valign="top">Words</th><th valign="top">Nonwords</th><th valign="top">Words</th><th valign="top">Nonwords</th></tr></thead><tbody><tr valign="top"><td><italic>AORTA</italic></td><td>AROTA</td><td><italic>STOLE</italic> </td><td>TSOLE</td><td><italic>NOISE</italic> </td><td>NANSE</td><td><italic>ONION</italic> </td><td>ESION</td></tr><tr valign="top"><td><italic>DRAIN</italic></td><td>DARIN</td><td><italic>OASIS</italic></td><td>AOSIS</td><td><italic>ERROR</italic></td><td>EDLOR</td><td><italic>RADIO</italic></td><td>EEDIO</td></tr><tr valign="top"><td><italic>TREND</italic></td><td>TERND</td><td><italic>SOLID</italic></td><td>OSLID</td><td><italic>DRILL</italic></td><td>DTELL</td><td><italic>ASSET</italic></td><td>EESET</td></tr><tr valign="top"><td><italic>ATLAS</italic></td><td>ALTAS</td><td><italic>TRAIN</italic></td><td>RTAIN</td><td><italic>ARISE</italic></td><td>AOESE</td><td><italic>TEASE</italic></td><td>RDASE</td></tr><tr valign="top"><td><italic>DRONE</italic></td><td>DRNOE</td><td><italic>ORDER</italic></td><td>ORDRE</td><td><italic>LITRE</italic></td><td>LINOE</td><td><italic>ENTER</italic></td><td>ENTRO</td></tr><tr valign="top"><td><italic>LEARN</italic></td><td>LERAN</td><td><italic>INDIA</italic></td><td>INDAI</td><td><italic>SLIDE</italic></td><td>SLONE</td><td><italic>IDEAL</italic></td><td>IDEDI</td></tr><tr valign="top"><td><italic>SANTA</italic></td><td>SATNA</td><td><italic>RINSE</italic></td><td>RINES</td><td><italic>NASAL</italic></td><td>NATDL</td><td><italic>ADORE</italic></td><td>ADODI</td></tr><tr valign="top"><td><italic>INSET</italic></td><td>INEST</td><td><italic>SNAIL</italic></td><td>SNALI</td><td><italic>ALIEN</italic></td><td>ALOTN</td><td><italic>LASER</italic></td><td>LASRO</td></tr></tbody></table></table-wrap></sec></sec><sec id="s38" sec-type="appendix"><title>ROI definitions</title><table-wrap id="app7table2" position="float"><label>Appendix 7—table 2.</label><caption><title>Variability in ROI definitions across subjects.</title><p>For each ROI we report the mean and standard deviation across subjects of the number of voxels, and the XYZ location of the voxel with peak T-value in the normalized brain.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">ROI</th><th valign="top">Definition</th><th valign="top">#voxels <break/>(mean ± sd)</th><th valign="top">ROI peak location</th></tr></thead><tbody><tr><td valign="top"> V1-V3</td><td valign="top">Voxels activated forscrambled &gt; fixation overlaid with anatomical mask of V1-V3</td><td valign="top">398 ± 131</td><td valign="top">X: 8 ± 17 <break/>Y: -96 ± 5 <break/>Z: 6 ± 9</td></tr><tr><td valign="top"> V4</td><td valign="top">Voxels activated forscrambled &gt; fixation overlaid with anatomical mask of V4</td><td valign="top">185 ± 63</td><td valign="top">X: 5 ± 26 <break/>Y: -88 ± 3 <break/>Z: 27 ± 11</td></tr><tr><td valign="top"> LO</td><td valign="top">Voxels activated for object &gt; scrambled and not in other ROIs</td><td valign="top">371 ± 115</td><td valign="top">X: -17 ± 43 <break/>Y: -66 ± 15 <break/>Z: -19 ± 5</td></tr><tr><td valign="top"> VWFA</td><td valign="top">Voxels with known words &gt; scrambled word in a contiguous region in fusiform gyrus</td><td valign="top">52 ± 15</td><td valign="top">X: -44 ± 4 <break/>Y: -50 ± 5 <break/>Z: -17 ± 5</td></tr><tr><td valign="top"> TG</td><td valign="top">Voxels with nativewords &gt; scrambled word in a contiguous region in temporal gyrus</td><td valign="top">289 ± 182</td><td valign="top">X: -44 ± 39 <break/>Y: -43 ± 18 <break/>Z: 3 ± 9</td></tr></tbody></table></table-wrap></sec><sec id="s39" sec-type="appendix"><title>Visualization of perceptual and semantic space</title><p>To visualize words and nonwords in perceptual space, we performed a multidimensional scaling (MDS) analysis of the visual search data (Experiment 7). Briefly, MDS finds the best-fitting 2D coordinates that best match with the observed distances. In the resulting plot, nearby stimuli correspond to hard searches. The perceptual space for words and nonwords is shown in <xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1 A-B</xref>. It can be seen that stimuli with common first letters are grouped together. MDS coordinates for nonwords was rotated without altering their overall configuration so as to best match the MDS coordinates for words.</p><fig id="app7fig1" position="float"><label>Appendix 7—figure 1.</label><caption><title>Multi-dimensional representation of words and nonwords.</title><p>(<bold>A</bold>) Perceptual space for words. we used multidimensional scaling to find the 2D coordinates of all words that best match the observed distances. In the resulting plot, nearby words indicate hard searches. The correlation coefficient between dissimilarities in 2D plane and the observed data is shown. Asterisks indicate significant correlation (**** is p&lt;0.00005). (<bold>B</bold>) Same as (<bold>A</bold>) but for nonwords. (<bold>C</bold>) Same as (<bold>A</bold>) but for semantic space of words.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app7-fig1-v3.tif"/></fig><p>The semantic dissimilarities were estimated using the GloVe features (<xref ref-type="bibr" rid="bib47">Pennington et al., 2014</xref>), and visualized using MDS analysis (<xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1C</xref>). In the resulting plot, semantically related words/frequently cooccurring words are closer to each other.</p></sec><sec id="s40" sec-type="appendix"><title>Neural activity corresponding to words, nonwords, and letters</title><p>For each category of stimuli that is words, nonwords, and letters, we averaged the activity values across voxels and subjects within each ROI. The mean activity values are shown in <xref ref-type="fig" rid="app7fig2">Appendix 7—figure 2A-E</xref>. Since the activation levels can also be influenced by the reaction time (RT) in the lexical decision task, we regressed out the contributions of observed RT for each subject. Specifically, we estimated the contribution of RT by solving the linear equation <bold>y</bold> = <bold>Xb</bold>. Here <bold>y</bold> is 64 × 1 vector of mean beta values, <bold>X</bold> is 64 × 2 matrix containing the RTs in the first column and ones in the second column, and <bold>b</bold> is a 2 × 1 vector of unknown model coefficients. We estimated the vector <bold>b</bold> (estimated using the MATLAB function <italic>regress</italic>). Next, we subtracted the contribution of RT from the mean beta values and replotted the average activity values for words and nonwords (<xref ref-type="fig" rid="app7fig2">Appendix 7—figure 2F</xref>).</p><fig id="app7fig2" position="float"><label>Appendix 7—figure 2.</label><caption><title>Neural activity.</title><p>(<bold>A</bold>) Average activation levels for words, nonwords, and letters. Error bar indicate ±1 s.e.m. across subjects. Asterisks indicate statistical significance (* is p&lt;0.05, ** is p&lt;0.005, etc. in a sign-rank test comparing subject-wise average activations). (<bold>B</bold>)-(<bold>E</bold>). Same as in A but for V4, Lateral Occipital areas, Visual Word Form Area, and Temporal Gyri respectively. (<bold>F</bold>) Mean activation level after regressing out the reaction time in the lexical decision task. Error bars indicate ±1 s.e.m. across subjects. (<bold>G</bold>) Cross-validated classification accuracy for transposed word-nonword pairs (<italic>dark</italic>) and substituted word-nonword pairs (<italic>light</italic>). Error bars indicate s.e.m. across subjects. Asterisks indicate statistical significance (* is p&lt;0.05, ** is p&lt;0.005, etc. in a sign-rank test comparing subject-wise accuracy w.r.t. chance level). (<bold>H</bold>) Schematic of the voxel model. The response of each voxel across strings is modeled as a linear combination of the constituent letter responses. Bottom: Hypothetical model fits based on the presence (right) or absence (left) of local combination detectors. Predicted responses for words will deviate from the observed responses under the influence of LCD. (<bold>I</bold>) Average model correlation (normalized using split-half correlation) for each ROI for words (dark) and nonwords (light). Error bar indicates s.e.m. across subject.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app7-fig2-v3.tif"/></fig></sec><sec id="s41" sec-type="appendix"><title>Word vs nonword classification</title><p>For each ROI and subject, we built linear classifier to discriminate between words and nonwords (using the built-in MATLAB routine <italic>fitcdiscr</italic>). We built separate classifiers to distinguish the activity pattern of transposed and substituted nonwords from their corresponding word activity patterns. The resulting decoding accuracy is shown in <xref ref-type="fig" rid="app7fig2">Appendix 7—figure 2G</xref>. It can be seen that decoding accuracy for substituted nonwords is significantly better than for transposed nonwords (<xref ref-type="fig" rid="app7fig2">Appendix 7—figure 2G</xref>). Correspondingly, in behavior, subjects were faster at responding to substituted nonwords compared to transposed nonwords (response times, mean ± sd: 1.03 ± 0.08 s for 16 substituted nonwords, 1.20 ± 0.15 s for 16 transposed nonwords, p&lt;0.005, rank-sum test comparing average response times).</p></sec><sec id="s42" sec-type="appendix"><title>Can string responses be predicted from single letters?</title><p>We modeled the response of each voxel across the 64 strings (32 words, 32 nonwords) as a linear combination of the single letter activations (<xref ref-type="fig" rid="app7fig2">Appendix 7—figure 2H</xref>). We evaluated model fits by comparing model correlations separately for words and nonwords. If string responses were driven by specialized detectors for letter combinations (such as those present in words), then we reasoned that model correlations would be worse for words compared to nonwords. By contrast, if there are no specialized detectors of this kind, model fits would be equivalent for words and nonwords.</p><p>We calculated cross-validated model fits by training the model on half the trials and testing it on the other half of the trials. Since voxels could vary widely in their reliability of responses to the stimuli, we normalized the model fit of each voxel by its split-half reliability. The average noise-corrected model fit (averaged across voxels and subjects) is shown in <xref ref-type="fig" rid="app7fig2">Appendix 7—figure 2I</xref>. This revealed no systematic difference in model performance for words and nonwords in any of the ROIs (<xref ref-type="fig" rid="app7fig2">Appendix 7—figure 2I</xref>). We obtained qualitatively similar results using a searchlight, where there were no clear regions in which model fits differed for words and nonwords (<xref ref-type="fig" rid="app7fig4">Appendix 7—figure 4D</xref>).</p><fig id="app7fig3" position="float"><label>Appendix 7—figure 3.</label><caption><title>Representation dissimilarity matrix.</title><p>(<bold>A</bold>) Average pair-wise dissimilarity values for all possible pairs of words (1-32) and nonwords (33-64). Colour bar represents dissimilarity values that were estimated using a cross-validated, normalized variation of Mahalanobis distance. (<bold>B</bold>) - (<bold>E</bold>) Same as in A but for V4, Lateral Occipital areas, Visual Word Form Area, and Temporal Gyri respectively. (<bold>F</bold>) Pair-wise dissimilarity values in the semantic space across all possible pairs of words. Color bar represents dissimilarity values that is 1 – r (correlation between feature vectors for a given word pair) (<bold>G</bold>) Average pair-wise perceptual dissimilarities (1/search reaction time) for all possible pairs of words, nonwords, and corresponding word-nonword pairs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app7-fig3-v3.tif"/></fig><fig id="app7fig4" position="float"><label>Appendix 7—figure 4.</label><caption><title>Searchlight analysis.</title><p>(<bold>A</bold>) Searchlight map of correlation between neural activity and lexical decision time for each voxel. (<bold>B</bold>) Searchlight map of correlation between neural dissimilarity and search dissimilarities in behavior. (<bold>C</bold>) Searchlight map of correlation between neural dissimilarity and semantic dissimilarities. (<bold>D</bold>) Searchlight map depicting the difference in model fit for words versus nonwords for each voxel, averaged across subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app7-fig4-v3.tif"/></fig><p>To further validate the letter model, we compared the single letter tuning along each MDS dimension with the observed single letter tuning in each ROI (<xref ref-type="fig" rid="app7fig5">Appendix 7—figure 5A</xref>). For each ROI, we grouped voxels with similar response profile and matched it to the MDS dimension (<xref ref-type="fig" rid="app7fig5">Appendix 7—figure 5A</xref>). We obtained similar single letter tuning and weight profiles for voxels across different ROIs. However, this analysis is inconclusive because there is no systematic way to compare a small set of neurons inferred from behavior with the much larger, possibly overcomplete set of voxel activations observed in brain imaging. Likewise, we grouped voxels with similar summation weights to compare the weight profiles in behavior and brain imaging. However this analysis is also inconclusive because different MDS-derived neurons might contribute differently towards behavior, so the summation weights cannot be directly averaged to make overall comparisons between ROI activations and behavior. Despite these caveats, there is a general match between tuning profiles and summation weights observed in behavior with those observed in different brain regions.</p><fig id="app7fig5" position="float"><label>Appendix 7—figure 5.</label><caption><title>Comparison of letter tuning and summation weights.</title><p>(<bold>A</bold>) (<italic>Left</italic>) Response of 6 MDS neurons for all the 10 letters. (<italic>Right</italic>) Single letters response across all the voxels (concatenated across subjects) within a given ROI. Each voxels is sorted into one of six groups depending on which MDS neuron it matches best. The height of each ROI plot is logarithmically scaled to match the number of voxels across all subjects. Black dashed lines are used to separate the clusters corresponding to each MDS neuron. (<bold>B</bold>) Same as (<bold>A</bold>) but showing the summation weights corresponding to each MDS neuron or ROI voxel.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54846-app7-fig5-v3.tif"/></fig></sec><sec id="s43" sec-type="appendix"><title>Neural dissimilarity values across words and nonwords</title><p>Within a given ROI, we calculated the pair-wise dissimilarity values using a noise-normalized variation of cross-validated Mahalanobis distance (<xref ref-type="bibr" rid="bib40">Nili et al., 2014</xref>). The median dissimilarity matrix across all subjects is shown in <xref ref-type="fig" rid="app7fig3">Appendix 7—figure 3A-E</xref>. The semantic distance between every pair of words was computed as the cosine distance between the GloVe (<xref ref-type="bibr" rid="bib47">Pennington et al., 2014</xref>) feature vectors activated by the two words (<xref ref-type="fig" rid="app7fig3">Appendix 7—figure 3F</xref>). The perceptual distance (1/search reaction time) averaged across all subjects for word-word pairs, nonword-nonword pairs, and word-nonword pair is shown in <xref ref-type="fig" rid="app7fig3">Appendix 7—figure 3G</xref></p></sec><sec id="s44" sec-type="appendix"><title>Searchlight analyses</title><p>To identify other brain regions that might show the effects observed in the individual ROIs, we performed a whole-brain searchlight analysis. Specifically, for each voxel in a given subjects’ brain, we considered a local neighborhood of 27 voxels (3 × 3×3 voxels) and performed the following analyses of interest. We obtained similar results for larger searchlight volumes. The resulting maps were smoothed using a Gaussian filter with FWHM of 3 mm.</p></sec><sec id="s45" sec-type="appendix"><title>Searchlight for regions that match lexical decision time</title><p>For each voxel, its activity across strings is correlated with mean lexical decision time. The resulting whole brain correlation map is averaged across subjects. Overall, activity in VWFA, Superior Parietal Lobe (SPL), Pre-Frontal and motor cortex was correlated with lexical decision time. This correlation map was visualized on the brain surface (<xref ref-type="fig" rid="app7fig4">Appendix 7—figure 4A</xref>).</p></sec><sec id="s46" sec-type="appendix"><title>Searchlight for regions that match perceptual space</title><p>For the neighborhood of each voxel, we calculated the pairwise neural dissimilarity for all word-word, nonword-nonword, and word-nonword pairs for a given subject, and averaged this across subjects. We then calculated the correlation between this local neural dissimilarity and the corresponding string dissimilarities estimated using experiment 7. This correlation map was visualized on the brain surface (<xref ref-type="fig" rid="app7fig4">Appendix 7—figure 4B</xref>).</p></sec><sec id="s47" sec-type="appendix"><title>Searchlight for regions that match semantic space</title><p>For the neighborhood of each voxel, we calculated the pairwise neural dissimilarity for all word-word pairs for a given subject and averaged this across subjects. We then calculated the correlation between this local neural dissimilarity and the corresponding semantic dissimilarities. This correlation map was visualized on the brain surface (<xref ref-type="fig" rid="app7fig4">Appendix 7—figure 4C</xref>).</p></sec><sec id="s48" sec-type="appendix"><title>Searchlight for comparing linear model fits between words and nonwords</title><p>For each subject and voxel, we modeled the response to strings as a linear combination of its single letter responses. The model fits (correlation between observed and predicted string responses) was evaluated separately for words and nonwords. The difference in the mean model fits between words and nonword is visualized on the brain surface (<xref ref-type="fig" rid="app7fig4">Appendix 7—figure 4D</xref>).</p></sec><sec id="s49" sec-type="appendix"><title>Match between letter model and fMRI data</title><p>The letter model described throughout the study is derived from dissimilarities measured in behavior in two steps. First, the dissimilarities between single letters were used to construct single neurons tuned to letter shape, whose activity predicts these dissimilarities. Second, the summation weights of each neuron were adjusted so that they match the dissimilarities between longer strings.</p><p>Given that we recorded responses to single letters as well as strings in fMRI, we wondered whether these can be matched in some manner to the letter tuning and summation weights derived from behavior in the letter model. Any direct comparison is fraught with the difficulty that many single letter tuning functions could produce the same behavior. For instance, simply rotating the MDS-derived tuning functions could yield another set of neurons that match the observed letter dissimilarities. This is further compounded by the fact that the MDS-derived neurons contribute unequally to behavior, and by the fact that this mapping could change completely with increasing numbers of neurons. Thus, it is unreasonable to expect voxel tuning for single letters or the summation weights to match exactly with the behaviorally derived tuning.</p><p>Nonetheless, we attempted to find a broad link between the single letter tuning and summation weights observed in behavior with those observed in each ROI. The results are summarized in <xref ref-type="fig" rid="app7fig5">Appendix 7—figure 5A</xref>. Since there are only 10 single letters, 6 MDS neurons were sufficient to explain &gt;95% of the variance of the pair-wise single letter dissimilarities observed in Experiment 1. For each MDS neuron, we identified the voxels whose activity for single letters had the least residual error compared to other MDS neurons. In this manner, we sorted the voxels into six groups corresponding to each MDS neuron. The resulting plots are shown in <xref ref-type="fig" rid="app7fig5">Appendix 7—figure 5A</xref>. It can be see that all ROIs show single letter tuning profiles similar to the behaviorally derived single letter tuning profiles. The corresponding summation weights for these voxels are shown in <xref ref-type="fig" rid="app7fig5">Appendix 7—figure 5B</xref>. Once again, it can be seen that many ROIs show similar summation weights as those observed in behavior.</p></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54846.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute of Mental Health, National Institutes of Health</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewer</role><aff><institution>National Institute of Mental Health, National Institutes of Health</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The manuscript presents a detailed and in-depth study of word processing, providing evidence that a compositional letter code potentially makes a major contribution to word reading and can account for reading of jumbled words. One of the major strengths of this work is the combination of careful behavioral experiments, a simple neural model based on properties of neurons in monkey IT cortex, and fMRI data in human participants.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A compositional letter code in high-level visual cortex explains how we read jumbled words&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Chris I Baker as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Floris de Lange as the Senior Editor.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The reviewers find the manuscript describes a very interesting and detailed study of the potential contribution of a compositional letter code to word reading using novel approaches across behavioral and neuroimaging data. A simple model of responses to individual letter shapes can explain performance in visual search tasks and capture some aspects of word and non-word processing. Further, neuroimaging data suggests that the compositional letter code corresponds to processing in a region in lateral occipital cortex, with the visual word form area (VWFA) corresponding to processing of properties related to word knowledge. One of the major strengths of this work is the combination of careful behavioral experiments, a simple neural model based on properties of neurons in monkey inferotemporal cortex, and fMRI data in human participants.</p><p>While the manuscript is well-written and comprehensive, the reviewers felt there are a number of revisions that would strengthen the work. These revisions would require significant reframing of the manuscript to more accurately reflect the data and the existing literature.</p><p>Essential revisions:</p><p>1) The conclusions tend to be overstated. The manuscript provides an interesting demonstration of what a visually based model can accomplish, but the authors go too far in their interpretation. The compositional letter code explains aspects of word and non-word processing, but not everything related to word reading. For example, in the lexical decision task, response times for words are explained by word frequency and there will be contributions from semantics, especially when reading strings of words/sentences. The authors should be clearer in describing what the data show and not over-reach into an account of word reading that discounts other factors without justification. This will also require a change in the title of the paper since most of the tasks the authors employ are not about word reading.</p><p>2) Related to point (1), the authors make a series of assertions that are not supported by their data and also present an oversimplified and somewhat biased review of the literature. The authors should clarify and justify these assertions. In particular:</p><p>a) &quot;we hypothesized that word reading is enabled by a purely visual representation&quot;. There is a large and robust literature demonstrating how various linguistic properties have substantial effects on word recognition and the authors should acknowledge and cite this literature.</p><p>b) Visual search is very different than reading. It should be made clear that this task is not a test of the more general computations involved in reading.</p><p>c) &quot;This difference in visual similarity explains why transposing the middle letters renders a word easier to read than transposing its edge letters&quot;. The authors should more clearly explain or qualify this assertion.</p><p>d) The two sentences in the Introduction seem to contradict each other. It is first suggested that word reading could be explained by a purely visual code, but then it is stated that reading could be a confound. It seems that what the authors want to say is that many different tasks involving words <italic>can</italic> (but are not necessarily always) be achieved by visual computations. This is a different claim than what is written throughout the paper.</p><p>e) The work as presented is not a test of the hypothesis that cortical regions involved in word recognition have tuning for letter combinations. Evidence for this hypothesis is based on a very different paradigm and the effect has been replicated by at least 3 different labs. The authors should clearly explain how the phenomenon they report aligns with those data.</p><p>f) Conceptually it is unclear how the bigram model lines up with the main assertion of the manuscript that performance can be understood in terms of the summation of performance for single letters. As soon as additional weights are added for bigrams isn't this akin to saying that there is a different representation of bigrams than of individual letters?</p><p>g) &quot;According to an influential account of word reading…. &quot;. This influential account of word reading is about the word recognition process not the detection of bigrams in a search array. It makes no comment or prediction about how the proposed neurons would be involved in the tasks developed by the authors. This is not to say that the model of bigram detectors in word-selective cortex is correct, just that the present work can be seen as orthogonal to this model and the authors need to clarify and justify how their data relates.</p><p>h) Subsection “Experiment 5: Lexical decision task” – why is the model retrained? If the model based on letters in general is used in this task shouldn't the same weights be used?</p><p>i) The summary sentence &quot;In sum, we conclude that word response times are explained by word frequency and nonword response times are explained by the distance between the nonword and the nearest word calculated using the compositional neural code.&quot; accurately captures what the data show. It highlights how the authors' model makes interesting predictions in a variety of contexts. But it also contradicts the main assertions laid out in the Introduction as they note that many factors that are not purely visual explain a major portion of the variance.</p><p>j) Some of the findings on the fMRI task have been reported a number of times in the literature (e.g. the difference in VWFA response to words and pseudowords). The manuscript would be strengthened by relating it to these previous studies.</p><p>3) The authors find that LO is the region that is most sensitive to their visual similarity metric.</p><p>a) Given prior work showing the critical role of VWFA in reading, this result would seem to suggest that visual similarity is not at the core of reading per se and this issue should be discussed.</p><p>b) The paragraph beginning with &quot;To further investigate the link between the compositional…&quot; needs clarifying. Further, prior work has focused on the VWFA for the claim of bigram detectors and the authors needs to explain how the finding of a compositional code in LO relates to that work.</p><p>c) The authors also show that LO has a representation of semantic space. This suggests a contribution beyond visual properties. What does this mean for the role of these neurons? What are the implications of this overlap for theories of orthographic and semantic processing? This needs to be discussed.</p><p>4) Some results do not seem to support the ideas of the paper, but are framed as doing so. Specifically:</p><p>a) &quot;To quantify this observation, we asked whether the model error for each bigram pair, calculated as the absolute difference between observed and predicted dissimilarity, covaried with the average bigram frequency of the two bigrams (for both frequent bigrams and words). […] We conclude that bigram search can be explained entirely using single neurons tuned to single letters.&quot;</p><p>The significant negative correlation does not seem to fit with the later statement that &quot;model errors are not systematically different for frequent compared to infrequent bigram pairs&quot;. The authors should clarify this point in this paragraph, but also explain how this result fits with main message of the paper. There are a number of effects that are against the idea the main hypothesis &quot;that word reading is enabled by a purely visual representation&quot; and that the model &quot;explained human performance in both visual search as well as word reading tasks.&quot; (Abstract). For instance, the effect of familiarity on asymmetric spatial summation (subsection “Experiment 3: Upright versus inverted bigrams”), and the frequency effect (Experiment 5).</p><p>b) If the claim is &quot;there are no specialized detectors for letter combinations&quot;, it's not clear how Experiment 5 (where the same letters in different orders give different RTs) fits with this. This seems to strongly support a role for particular letter combinations.</p><p>5) In the analysis of mean VWFA activity, response times do not seem to be controlled. This is important because RTs might influence the magnitude of the β coefficients in a systematic way that does not directly correspond to the relevant processing. These should be included in the model.</p><p>6) In a number of places in the manuscript, the authors draw conclusions about how neurons are tuned, but without directly recording from neurons, it is not possible to draw this conclusion. For example, &quot;Our main finding is that viewing a string activates a compositional letter code, consisting of neurons tuned to single letters whose response to longer strings is a linear sum of single letter responses&quot;.</p><p>7) It was mentioned that outliers in dissimilarity values across subjects were removed using built-in routines in MATLAB (isoutlier function, MATLAB R2018a). This is unusual and not typically seen in dissimilarity analyses. Are the findings the same without this procedure? How many outliers are removed? How is this threshold calculated/selected?</p><p>8) The manuscript mentions that cross-validation was performed but details on this are absent. In any case, pairwise dissimilarity calculations should be performed across runs (i.e., not correlating between items in the same run). This is particularly true because the trials are close together and influenced by the adjacent BOLD response.</p><p>9) It would be worth discussing how these findings relate to results (and associated theories) regarding whether individuals and specifically the VWFA process orthographic stimuli in a holistic versus part-based fashion (e.g., Carlos et al., 2019). More generally, behavioral responses to inverted words are relevant to this work, and worthy of more discussion (for how such findings are, or are not, consistent with this manuscript).</p><p>10) The authors report an exhaustive series of experiments and there is extensive supplementary material – while the authors should be commended for providing so much material, it does make the manuscript challenging to read at times, requiring the reader to jump around different parts of the text to fully understand the methods and results. The authors should include information such as the number of subjects in each experiment and key aspects of the stimuli (total number, how selected etc.), and protocol (e.g. timing of stimulus presentation, stimulus size etc.) in the main text. Further, there appears to be some inconsistency between the naming of sections. In the main text the authors refer the reader to, for example, &quot;Section S7&quot;, but the supplementary material itself is labelled as &quot;Section A7&quot;, and the tables and figures are numbered sequentially throughout the supplementary material such that the relevant figure in &quot;Section A7&quot; might be &quot;Figure S16&quot; – all very confusing. The authors should reconsider how they are structuring the supplementary material to make it more intuitively organized.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54846.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The conclusions tend to be overstated. The manuscript provides an interesting demonstration of what a visually based model can accomplish, but the authors go too far in their interpretation. The compositional letter code explains aspects of word and non-word processing, but not everything related to word reading. For example, in the lexical decision task, response times for words are explained by word frequency and there will be contributions from semantics, especially when reading strings of words/sentences. The authors should be clearer in describing what the data show and not over-reach into an account of word reading that discounts other factors without justification. This will also require a change in the title of the paper since most of the tasks the authors employ are not about word reading.</p></disp-quote><p>Thank you for raising this concern. We do agree with you that effects related to word frequency are not compositional i.e. cannot be explained using single letters. We have now carefully reviewed the entire manuscript to avoid overstating our results. We have also changed the title to “A compositional neural code in high-level visual cortex enables jumbled word reading”, which we think is an accurate summary of our findings.</p><disp-quote content-type="editor-comment"><p>2) Related to point (1), the authors make a series of assertions that are not supported by their data and also present an oversimplified and somewhat biased review of the literature. The authors should clarify and justify these assertions. In particular:a) &quot;we hypothesized that word reading is enabled by a purely visual representation&quot;. There is a large and robust literature demonstrating how various linguistic properties have substantial effects on word recognition and the authors should acknowledge and cite this literature.</p></disp-quote><p>We agree that linguistic properties have substantial effects on word recognition, and we have now modified the Introduction to make this point explicit. We have also reworded our hypothesis to make it clearer.</p><disp-quote content-type="editor-comment"><p>b) Visual search is very different than reading. It should be made clear that this task is not a test of the more general computations involved in reading.</p></disp-quote><p>We completely agree, and we have acknowledged this in the Introduction.</p><disp-quote content-type="editor-comment"><p>c) &quot;This difference in visual similarity explains why transposing the middle letters renders a word easier to read than transposing its edge letters&quot;. The authors should more clearly explain or qualify this assertion.</p></disp-quote><p>We meant that FOGRET is easy to read as FORGET because it is visually similar to FORGET. By contrast OFRGET is harder to recognize since it is visually dissimilar to FORGET. We have now made this clear in the Introduction.</p><disp-quote content-type="editor-comment"><p>d) The two sentences in the Introduction seem to contradict each other. It is first suggested that word reading could be explained by a purely visual code, but then it is stated that reading could be a confound. It seems that what the authors want to say is that many different tasks involving words can (but are not necessarily always) be achieved by visual computations. This is a different claim than what is written throughout the paper.</p></disp-quote><p>We meant that visual search for strings might have invoked some reading process, in which case explaining reading phenomena using visual search would be circular. To overcome this confound we have to demonstrate that visual search for strings can be explained using a purely visual model that does not include any lexical or linguistic factors, which is what we have done in our study. We have now reworked this paragraph to make it clearer (Introduction).</p><disp-quote content-type="editor-comment"><p>e) The work as presented is not a test of the hypothesis that cortical regions involved in word recognition have tuning for letter combinations. Evidence for this hypothesis is based on a very different paradigm and the effect has been replicated by at least 3 different labs. The authors should clearly explain how the phenomenon they report aligns with those data.</p></disp-quote><p>We now discuss these studies in relation to ours in the Discussion.</p><disp-quote content-type="editor-comment"><p>f) Conceptually it is unclear how the bigram model lines up with the main assertion of the manuscript that performance can be understood in terms of the summation of performance for single letters. As soon as additional weights are added for bigrams isn't this akin to saying that there is a different representation of bigrams than of individual letters?</p></disp-quote><p>Our letter-based model in figure only assumes neurons tuned for single letters and summation weights that depend on letter position, and therefore contains no information specific to any particular bigram. We have clarified this in the Results.</p><disp-quote content-type="editor-comment"><p>g) &quot;According to an influential account of word reading…. &quot;. This influential account of word reading is about the word recognition process not the detection of bigrams in a search array. It makes no comment or prediction about how the proposed neurons would be involved in the tasks developed by the authors. This is not to say that the model of bigram detectors in word-selective cortex is correct, just that the present work can be seen as orthogonal to this model and the authors need to clarify and justify how their data relates.</p></disp-quote><p>Actually there have been several proposals in the literature about open bigrams (Grainger and Whitney, 2004 ) and local combination detectors (Dehaene et al., 2005) according to which reading is enabled by detectors of higher order combinations of letters. Our model does stand in contrast to such proposals since it does not assume any tuning for letter combinations, yet it explains many aspects of orthographic processing. We have now reworked the text to make this clear (Introduction).</p><disp-quote content-type="editor-comment"><p>h) Subsection “Experiment 5: Lexical decision task” – why is the model retrained? If the model based on letters in general is used in this task shouldn't the same weights be used?</p></disp-quote><p>We retrained the model based on our observation that the spatial summation weights varied across the visual search experiments, presumably reflecting differing attentional resources across letter position. We have now updated the text to make this clear.</p><p>To assess whether the model trained on visual search data would also be able to predict nonword response times, we took the model trained on the visual search data in Experiment 4, and calculated the word-nonword distances using this model. This too yielded a significant positive correlation (r = 0.39, p &lt; 0.00005) that was better than the OLD and lexical models. We have now included this in the Results.</p><disp-quote content-type="editor-comment"><p>i) The summary sentence &quot;In sum, we conclude that word response times are explained by word frequency and nonword response times are explained by the distance between the nonword and the nearest word calculated using the compositional neural code.&quot; accurately captures what the data show. It highlights how the authors' model makes interesting predictions in a variety of contexts. But it also contradicts the main assertions laid out in the Introduction as they note that many factors that are not purely visual explain a major portion of the variance.</p></disp-quote><p>Thank you for this observation. We have revised the text throughout to more closely reflect these points.</p><disp-quote content-type="editor-comment"><p>j) Some of the findings on the fMRI task have been reported a number of times in the literature (e.g. the difference in VWFA response to words and pseudowords). The manuscript would be strengthened by relating it to these previous studies.</p></disp-quote><p>We have now reworked the Discussion to include these points.</p><disp-quote content-type="editor-comment"><p>3) The authors find that LO is the region that is most sensitive to their visual similarity metric.</p><p>a) Given prior work showing the critical role of VWFA in reading, this result would seem to suggest that visual similarity is not at the core of reading per se and this issue should be discussed.</p></disp-quote><p>Thank you for raising this point. We have now included these points in the Discussion.</p><disp-quote content-type="editor-comment"><p>b) The paragraph beginning with &quot;To further investigate the link between the compositional…&quot; needs clarifying. Further, prior work has focused on the VWFA for the claim of bigram detectors and the authors needs to explain how the finding of a compositional code in LO relates to that work.</p></disp-quote><p>We now have reworked this paragraph to make it clearer (subsection “Neural basis of perceptual space”). We now discuss our findings in relation to previous studies on VWFA (subsection “Neural basis of word recognition”).</p><disp-quote content-type="editor-comment"><p>c) The authors also show that LO has a representation of semantic space. This suggests a contribution beyond visual properties. What does this mean for the role of these neurons? What are the implications of this overlap for theories of orthographic and semantic processing? This needs to be discussed.</p></disp-quote><p>We now acknowledge these points in the Discussion.</p><disp-quote content-type="editor-comment"><p>4) Some results do not seem to support the ideas of the paper, but are framed as doing so. Specifically:</p><p>a) &quot;To quantify this observation, we asked whether the model error for each bigram pair, calculated as the absolute difference between observed and predicted dissimilarity, covaried with the average bigram frequency of the two bigrams (for both frequent bigrams and words). […] We conclude that bigram search can be explained entirely using single neurons tuned to single letters.&quot;</p><p>The significant negative correlation does not seem to fit with the later statement that &quot;model errors are not systematically different for frequent compared to infrequent bigram pairs&quot;. The authors should clarify this point in this paragraph, but also explain how this result fits with main message of the paper.</p></disp-quote><p>Thank you for drawing our attention to this point. This was a weak negative correlation and we were concerned that it may not be real. Upon carefully analysing the data and model fits, we noticed that the weak correlation was not robustly significant across slightly different random starting points in the model fits. However the comparison between the top 20 vs. bottom 20 frequent bigrams was more robust and therefore we have included only this analysis and removed the correlation part.</p><disp-quote content-type="editor-comment"><p>There are a number of effects that are against the idea the main hypothesis &quot;that word reading is enabled by a purely visual representation&quot; and that the model &quot;explained human performance in both visual search as well as word reading tasks.&quot; (Abstract). For instance, the effect of familiarity on asymmetric spatial summation (subsection “Experiment 3: Upright versus inverted bigrams”), and the frequency effect (Experiment 5).</p></disp-quote><p>We agree with you in general. We have now carefully reviewed the text throughout to more accurately reflect our findings. However, the effect of familiarity could be purely visual in nature and not necessarily due to linguistic factors, but our study does not distinguish between these possibilities. We have acknowledged this point in the Results.</p><disp-quote content-type="editor-comment"><p>b) If the claim is &quot;there are no specialized detectors for letter combinations&quot;, it's not clear how Experiment 5 (where the same letters in different orders give different RTs) fits with this. This seems to strongly support a role for particular letter combinations.</p></disp-quote><p>No, we disagree. It would imply a role for particular letter combinations only if letters were equally important at every letter position in the word. Our model, which is based on tuning for single letters and with weights for each letter position, is able to predict the variation in nonword RT across different scrambled versions of a word (Figure 4). The fact that the spatial summation weights in the model are different at each letter position challenges this implicit assumption that all letters contribute equally.</p><disp-quote content-type="editor-comment"><p>5) In the analysis of mean VWFA activity, response times do not seem to be controlled. This is important because RTs might influence the magnitude of the β coefficients in a systematic way that does not directly correspond to the relevant processing. These should be included in the model.</p></disp-quote><p>We have now included an analysis of mean activations of each ROI after regressing out the word and nonword RTs from the activations (Appendix 7—figure 2). However, any generic effect of longer response times on mean activations would have produced artefactual correlations in all regions, but we observed a correlation only in the VWFA (Figure 5F).</p><disp-quote content-type="editor-comment"><p>6) In a number of places in the manuscript, the authors draw conclusions about how neurons are tuned, but without directly recording from neurons, it is not possible to draw this conclusion. For example, &quot;Our main finding is that viewing a string activates a compositional letter code, consisting of neurons tuned to single letters whose response to longer strings is a linear sum of single letter responses&quot;.</p></disp-quote><p>That’s true. We have now reworked the text now to make this clearer.</p><disp-quote content-type="editor-comment"><p>7) It was mentioned that outliers in dissimilarity values across subjects were removed using built-in routines in MATLAB (isoutlier function, MATLAB R2018a). This is unusual and not typically seen in dissimilarity analyses. Are the findings the same without this procedure? How many outliers are removed? How is this threshold calculated/selected?</p></disp-quote><p>Yes, we obtained qualitatively similar without this step. Excluding outliers increased the split-half consistency of the data. The ‘isoutlier’ function by default removes any value greater than three scaled absolute deviations away from the median and was applied to each dissimilarity pair separately. This step removed 12.3% of the dissimilarity data. The default threshold was used here. We have now updated the Materials and methods to include these points.</p><disp-quote content-type="editor-comment"><p>8) The manuscript mentions that cross-validation was performed but details on this are absent. In any case, pairwise dissimilarity calculations should be performed across runs (i.e., not correlating between items in the same run). This is particularly true because the trials are close together and influenced by the adjacent BOLD response.</p></disp-quote><p>We have now included these details in the manuscript.</p><disp-quote content-type="editor-comment"><p>9) It would be worth discussing how these findings relate to results (and associated theories) regarding whether individuals and specifically the VWFA process orthographic stimuli in a holistic versus part-based fashion (e.g., Carlos et al., 2019). More generally, behavioral responses to inverted words are relevant to this work, and worthy of more discussion (for how such findings are, or are not, consistent with this manuscript).</p></disp-quote><p>While behavioural and neural responses to inverted words during lexical decisions have indeed been studied, they relate to recognizing familiar objects in novel orientations. Our finding relates to how part summation happens in upright versus inverted orientations, and therefore seemed somewhat unrelated. However, the finding that VWFA responds more strongly to inverted compared to upright words during lexical decisions is consistent with our finding that difficult nonwords activate VWFA strongly. We have now acknowledged this in the Discussion.</p><disp-quote content-type="editor-comment"><p>10) The authors report an exhaustive series of experiments and there is extensive supplementary material – while the authors should be commended for providing so much material, it does make the manuscript challenging to read at times, requiring the reader to jump around different parts of the text to fully understand the methods and results. The authors should include information such as the number of subjects in each experiment and key aspects of the stimuli (total number, how selected etc.), and protocol (e.g. timing of stimulus presentation, stimulus size etc.) in the main text. Further, there appears to be some inconsistency between the naming of sections. In the main text the authors refer the reader to, for example, &quot;Section S7&quot;, but the supplementary material itself is labelled as &quot;Section A7&quot;, and the tables and figures are numbered sequentially throughout the supplementary material such that the relevant figure in &quot;Section A7&quot; might be &quot;Figure S16&quot; – all very confusing. The authors should reconsider how they are structuring the supplementary material to make it more intuitively organized.</p></disp-quote><p>Thank you for these comments. We have included more details of each experiment in the main text. We have also carefully reviewed the text and supplementary material to make them more consistent. Finally, figures in the appendix are now numbered according to each section (e.g. Section A7 contains Appendix 7—figures 1, 2 etc).</p></body></sub-article></article>