<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">10989</article-id><article-id pub-id-type="doi">10.7554/eLife.10989</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Demixed principal component analysis of neural population data</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-19777"><name><surname>Kobak</surname><given-names>Dmitry</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5639-7209</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-40998"><name><surname>Brendel</surname><given-names>Wieland</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-41000"><name><surname>Constantinidis</surname><given-names>Christos</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-41001"><name><surname>Feierstein</surname><given-names>Claudia E</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8002-922X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-1230"><name><surname>Kepecs</surname><given-names>Adam</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-19268"><name><surname>Mainen</surname><given-names>Zachary F</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-41003"><name><surname>Qi</surname><given-names>Xue-Lian</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-40999"><name><surname>Romo</surname><given-names>Ranulfo</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-10010"><name><surname>Uchida</surname><given-names>Naoshige</given-names></name><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-41002"><name><surname>Machens</surname><given-names>Christian K</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Champalimaud Neuroscience Program</institution>, <institution>Champalimaud Centre for the Unknown</institution>, <addr-line><named-content content-type="city">Lisbon</named-content></addr-line>, <country>Portugal</country></aff><aff id="aff2"><label>2</label><institution>École Normale Supérieure</institution>, <addr-line><named-content content-type="city">Paris</named-content></addr-line>, <country>France</country></aff><aff id="aff3"><label>3</label><institution>Centre for Integrative Neuroscience, University of Tübingen</institution>, <addr-line><named-content content-type="city">Tübingen</named-content></addr-line>, <country>Germany</country></aff><aff id="aff4"><label>4</label><institution>Wake Forest University School of Medicine</institution>, <addr-line><named-content content-type="city">Winston-Salem</named-content></addr-line>, <country>United States</country></aff><aff id="aff5"><label>5</label><institution>Cold Spring Harbor Laboratory</institution>, <addr-line><named-content content-type="city">Cold Spring Harbor</named-content></addr-line>, <country>United States</country></aff><aff id="aff6"><label>6</label><institution content-type="dept">Instituto de Fisiología Celular-Neurociencias</institution>, <institution>Universidad Nacional Autónoma de México</institution>, <addr-line><named-content content-type="city">Mexico City</named-content></addr-line>, <country>Mexico</country></aff><aff id="aff7"><label>7</label><institution>El Colegio Nacional</institution>, <addr-line><named-content content-type="city">Mexico City</named-content></addr-line>, <country>Mexico</country></aff><aff id="aff8"><label>8</label><institution>Harvard University</institution>, <addr-line><named-content content-type="city">Cambridge</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>van Rossum</surname><given-names>Mark CW</given-names></name><role>Reviewing editor</role><aff id="aff9"><institution>University of Edinburgh</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>christian.machens@neuro.fchampalimaud.org</email></corresp><fn fn-type="con" id="equal-contrib"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="pub" publication-format="electronic"><day>12</day><month>04</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e10989</elocation-id><history><date date-type="received"><day>19</day><month>08</month><year>2015</year></date><date date-type="accepted"><day>07</day><month>04</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Kobak et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Kobak et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-10989-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.10989.001</object-id><p>Neurons in higher cortical areas, such as the prefrontal cortex, are often tuned to a variety of sensory and motor variables, and are therefore said to display mixed selectivity. This complexity of single neuron responses can obscure what information these areas represent and how it is represented. Here we demonstrate the advantages of a new dimensionality reduction technique, demixed principal component analysis (dPCA), that decomposes population activity into a few components. In addition to systematically capturing the majority of the variance of the data, dPCA also exposes the dependence of the neural representation on task parameters such as stimuli, decisions, or rewards. To illustrate our method we reanalyze population data from four datasets comprising different species, different cortical areas and different experimental tasks. In each case, dPCA provides a concise way of visualizing the data that summarizes the task-dependent features of the population response in a single figure.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.001">http://dx.doi.org/10.7554/eLife.10989.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.10989.002</object-id><title>eLife digest</title><p>Many neuroscience experiments today involve using electrodes to record from the brain of an animal, such as a mouse or a monkey, while the animal performs a task. The goal of such experiments is to understand how a particular brain region works. However, modern experimental techniques allow the activity of hundreds of neurons to be recorded simultaneously. Analysing such large amounts of data then becomes a challenge in itself.</p><p>This is particularly true for brain regions such as the prefrontal cortex that are involved in the cognitive processes that allow an animal to acquire knowledge. Individual neurons in the prefrontal cortex encode many different types of information relevant to a given task. Imagine, for example, that an animal has to select one of two objects to obtain a reward. The same group of prefrontal cortex neurons will encode the object presented to the animal, the animal’s decision and its confidence in that decision. This simultaneous representation of different elements of a task is called a ‘mixed’ representation, and is difficult to analyse.</p><p>Kobak, Brendel et al. have now developed a data analysis tool that can ‘demix’ neural activity. The tool breaks down the activity of a population of neurons into its individual components. Each of these relates to only a single aspect of the task and is thus easier to interpret. Information about stimuli, for example, is distinguished from information about the animal’s confidence levels.</p><p>Kobak, Brendel et al. used the demixing tool to reanalyse existing datasets recorded from several different animals, tasks and brain regions. In each case, the tool provided a complete, concise and transparent summary of the data. The next steps will be to apply the analysis tool to new datasets to see how well it performs in practice. At a technical level, the tool could also be extended in a number of different directions to enable it to deal with more complicated experimental designs in future.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.002">http://dx.doi.org/10.7554/eLife.10989.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>prefrontal cortex</kwd><kwd>principal component analysis</kwd><kwd>dimensionality reduction</kwd><kwd>population activity</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Rat</kwd><kwd>Other</kwd><kwd><italic>Rhesus Macaque</italic></kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100005032</institution-id><institution>Fundação Bial</institution></institution-wrap></funding-source><award-id>Fellowship, 389/14</award-id><principal-award-recipient><name><surname>Kobak</surname><given-names>Dmitry</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A new data analysis tool provides a concise way of visualizing neural data that summarizes all the relevant features of the population response in a single figure.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In many state of the art experiments, a subject, such as a rat or a monkey, performs a behavioral task while the activity of tens to hundreds of neurons in the animal’s brain is monitored using electrophysiological or imaging techniques. The common goal of these studies is to relate the external task parameters, such as stimuli, rewards, or the animal’s actions, to the internal neural activity, and to then draw conclusions about brain function. This approach has typically relied on the analysis of single neuron recordings. However, as soon as hundreds of neurons are taken into account, the complexity of the recorded data poses a fundamental challenge in itself. This problem has been particularly severe in higher-order areas such as the prefrontal cortex, where neural responses display a baffling heterogeneity, even if animals are carrying out rather simple tasks (<xref ref-type="bibr" rid="bib3">Brody et al., 2003</xref>; <xref ref-type="bibr" rid="bib24">Machens et al., 2010</xref>; <xref ref-type="bibr" rid="bib16">Hernández et al., 2010</xref>; <xref ref-type="bibr" rid="bib25">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Rigotti et al., 2013</xref>).</p><p>Traditionally, this heterogeneity has often been neglected. In neurophysiological studies, it is common practice to pre-select cells based on particular criteria, such as responsiveness to the same stimulus, and to then average the firing rates of the pre-selected cells. This practice eliminates much of the richness of single-cell activities, similar to imaging techniques with low spatial resolution, such as MEG, EEG, or fMRI. While population averages can identify some of the information that higher-order areas process, they ignore much of the fine structure of the single cell responses (<xref ref-type="bibr" rid="bib47">Wohrer et al., 2013</xref>). Indeed, most neurons in higher cortical areas will typically encode several task parameters simultaneously, and therefore display what has been termed <italic>mixed selectivity</italic> (<xref ref-type="bibr" rid="bib36">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Pagan and Rust, 2014</xref>; <xref ref-type="bibr" rid="bib29">Park et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Raposo et al., 2014</xref>).</p><p>Instead of looking at single neurons and selecting from or averaging over a population of neurons, neural population recordings can be analyzed using dimensionality reduction methods (for a review, see <xref ref-type="bibr" rid="bib9">Cunningham and Yu, 2014</xref>). In recent years, several such methods have been developed that are specifically targeted to electrophysiological data, working on the level of single spikes (<xref ref-type="bibr" rid="bib30">Pfau et al., 2013</xref>), accommodating different time scales of latent variables (<xref ref-type="bibr" rid="bib48">Yu et al., 2009</xref>), or accounting for the dynamics of the population response (<xref ref-type="bibr" rid="bib4">Buesing et al., 2012a</xref>; <xref ref-type="bibr" rid="bib5">2012b</xref>; <xref ref-type="bibr" rid="bib7">Churchland et al., 2012</xref>). However, these approaches reduce the dimensionality of the data without taking task parameters, i.e., sensory and motor variables controlled or monitored by the experimenter, into account. Consequently, mixed selectivity remains in the data even after the dimensionality reduction step.</p><p>The problem can be addressed by dimensionality reduction methods that are informed by the task parameters (<xref ref-type="bibr" rid="bib24">Machens et al., 2010</xref>; <xref ref-type="bibr" rid="bib22">Machens, 2010</xref>; <xref ref-type="bibr" rid="bib2">Brendel et al., 2011</xref>; <xref ref-type="bibr" rid="bib25">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib33">Raposo et al., 2014</xref>). We have previously introduced a dimensionality reduction technique, <italic>demixed principal component analysis (dPCA)</italic> (<xref ref-type="bibr" rid="bib2">Brendel et al., 2011</xref>), that emphasizes two goals. It aims to find a decomposition of the data into latent components that (a) are easily interpretable with respect to the experimentally controlled and monitored task parameters; and (b) preserve the original data as much as possible, ensuring that no valuable information is thrown away. Here we present a radically modified version of this method, and illustrate that it works well on a wide variety of experimental data. The new version of the method has the same objectives as the older version (<xref ref-type="bibr" rid="bib2">Brendel et al., 2011</xref>), but is more principled, more flexible, and has an analytical solution, meaning that it does not suffer from any numerical optimization problems. Furthermore, the new mathematical formulation highlights similarities to and differences from related well-known methods such as principal component analysis (PCA) and linear discriminant analysis (LDA).</p><p>The dPCA code is available at <ext-link ext-link-type="uri" xlink:href="http://github.com/machenslab/dPCA">http://github.com/machenslab/dPCA</ext-link> for Matlab and Python.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Existing approaches</title><p>We illustrate the classical approaches to analyzing neural activity data from higher-order areas in <xref ref-type="fig" rid="fig1">Figure 1</xref>. To be specific, we consider recordings from the prefrontal cortex (PFC) of monkeys performing a somatosensory working memory task (<xref ref-type="bibr" rid="bib40">Romo et al., 1999</xref>; <xref ref-type="bibr" rid="bib3">Brody et al., 2003</xref>). In this task, monkeys were required to discriminate two vibratory stimuli presented to the fingertip. The stimuli F1 and F2 were separated by a 3 s delay, and the monkeys had to report which stimulus had a higher frequency by pressing one of the two available buttons (<xref ref-type="fig" rid="fig1">Figure 1a</xref>).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.003</object-id><label>Figure 1.</label><caption><title>Existing approaches to population analysis, illustrated with recordings from monkey PFC during a somatosensory working memory task (<xref ref-type="bibr" rid="bib40">Romo et al., 1999</xref>).</title><p>(<bold>a</bold>) Cartoon of the paradigm, adapted from <xref ref-type="bibr" rid="bib41">Romo and Salinas (2003)</xref>. Legend shows 12 experimental conditions. (<bold>b</bold>) Average per-condition firing rates (PSTHs) for four exemplary neurons out of <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>832</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Colors refer to stimulus frequencies F1 and line styles (dashed/solid) refer to decision, see legend in (<bold>a</bold>). (<bold>c</bold>) Fraction of cells, significantly (<inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, two-way ANOVA) tuned to stimulus and decision at each time point. (<bold>d</bold>) Left: Distribution of stimulus tuning effect sizes across neural population at F1 period (black arrow in <bold>c</bold>). Significantly tuned neurons are shown in dark gray. Right: Same for decision at F2 period (gray arrow in <bold>c</bold>). (<bold>e</bold>) The average of zero-centered PSTHs over all significantly tuned neurons (for neurons with negative effect size, the sign of PSTHs was flipped). Arrows mark time-points that were used to select the significant cells. (<bold>f</bold>) Fraction of cells, significantly (<inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, linear regression) tuned to stimulus and decision at each time point. (<bold>g</bold>) Distribution of regression coefficients of neural firing rates to stimulus (during F1 period) and decision (during F2 period). (<bold>h</bold>) Stimulus and decision components produced by the method of <xref ref-type="bibr" rid="bib25">Mante et al. (2013)</xref>. Briefly, neural PSTHs are weighted by the regression coefficients. (<bold>i</bold>) Fraction of variance captured by the first 20 principal components. (<bold>j</bold>) Distributions of weights used to produce the first six principal components (weights are elements of the eigenvectors of the <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> covariance matrix). (<bold>k</bold>) First six principal components (projections of the full data onto the eigenvector directions).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.003">http://dx.doi.org/10.7554/eLife.10989.003</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig1-v2"/></fig></p><p>When we focus on the neural representation of the stimulus F1 and the decision, we have to take 12 experimental conditions into account (six possible values of F1 and two possible decisions). For each of these conditions, we can average each neuron’s spike trains over trials and then smooth the resulting time series in order to estimate the neuron’s time-dependent firing rate (also known as peri-stimulus time histogram or PSTH). We find that the PSTHs of many neurons are tuned to the stimulus F1, the decision, or both (<xref ref-type="fig" rid="fig1">Figure 1b</xref>; so-called <italic>mixed selectivity</italic>), and different neurons generally show different tuning. Our goal is to characterize and summarize the tuning of all <inline-formula><mml:math id="inf5"><mml:mi>N</mml:mi></mml:math></inline-formula> recorded neurons.</p><p>The most standard and widespread approach is to resort to a statistical test (e.g. a two-way analysis of variance or ANOVA), in order to check whether the firing rate of a neuron depends significantly on the frequency F1 or on the monkey’s decision. Such a test can be run for each neuron and each time point, in which case the population tuning over time is often summarized as the fraction of cells significantly tuned to stimulus or decision at each time point (<inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig1">Figure 1c</xref>). In addition to providing such a 'summary statistics', this approach is also used to directly visualize the population activity. For that purpose, one selects the subset of neurons significantly tuned to stimulus or decision (e.g. by focusing on a particular time point, <xref ref-type="fig" rid="fig1">Figure 1d</xref>) and then averages their PSTHs. The resulting 'population average' is shown in <xref ref-type="fig" rid="fig1">Figure 1e</xref>, where we also took the sign of the effect size into account. The population average is generally thought to demonstrate the 'most typical' firing pattern among the cells encoding the corresponding parameter. Importantly, this method yields one single population average or 'component' for each parameter. Each such component can be understood as a linear combination (or a linear <italic>readout</italic>) of the individual PSTHs, with all <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> significant neurons for a parameter having the same weights <inline-formula><mml:math id="inf8"><mml:mrow> <mml:mo>± </mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and all others having weight zero.</p><p>In a related approach, the firing rates of each neuron at each time point are linearly regressed on stimulus and decision (<xref ref-type="fig" rid="fig1">Figure 1f</xref>) (<xref ref-type="bibr" rid="bib3">Brody et al., 2003</xref>). <xref ref-type="bibr" rid="bib25">Mante et al. (2013)</xref> suggested to use the regression coefficients of all <inline-formula><mml:math id="inf9"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons (<xref ref-type="fig" rid="fig1">Figure 1g</xref>) as weights to form linear combinations of PSTHs representing stimulus and decision tuning (<xref ref-type="fig" rid="fig1">Figure 1h</xref>). This approach, which the authors call 'targeted dimensionality reduction' (TDR), also yields one component per task parameter: in our example, we obtain one component for the stimulus and one for the decision (<xref ref-type="fig" rid="fig1">Figure 1h</xref>; see Materials and methods for details).</p><p>Both of these approaches are <italic>supervised</italic>, meaning that they are informed by the task parameters. At the same time, they do not seek to faithfully represent the whole dataset and are prone to losing some information about the neural activities. Indeed, the two components from <xref ref-type="fig" rid="fig1">Figure 1e</xref> explain only 23% of the total variance of the population firing rates and the two components from <xref ref-type="fig" rid="fig1">Figure 1h</xref> explain only 22% (see Materials and methods). Consequently, a naive observer would not be able to infer from the components what the original neural activities looked like.</p><p>While such supervised approaches can be extended in various ways to produce more components and capture more variance, a more direct way to avoid this loss of information is to resort to <italic>unsupervised</italic> methods such as principal component analysis (PCA). This method extracts a set of principal components (PCs) that are linear combinations of the original PSTHs, just as the population averages above. However, the weights to form these linear combinations are chosen so as to maximize the amount of explained variance (first six components explain 69% of variance, see <xref ref-type="fig" rid="fig1">Figure 1i–k</xref>). The principal components can be thought of as 'building blocks' of neural activity: PSTHs of actual neurons are given by linear combinations of PCs, with the first PCs being more informative than the later ones. However, since PCA is an unsupervised method, information about stimuli and decisions is not taken into account, and the resulting components can retain mixed selectivity and therefore fail to highlight neural tuning to the task parameters.</p><p>The most striking observation when comparing supervised and unsupervised approaches is how different the results look. Indeed, PCA paints a much more complex picture of the population activity, dominated by strong temporal dynamics, with several stimulus- and decision-related components. At the same time, none of the methods can fully demix the stimulus and decision information: even the supervised methods show decision-related activity in the stimulus components and stimulus-related activity in the decision components (<xref ref-type="fig" rid="fig1">Figure 1e,h</xref>).</p></sec><sec id="s2-2"><title>Demixed principal component analysis (dPCA)</title><p>To address these problems, we developed a modified version of PCA that not only compresses the data, but also demixes the dependencies of the population activity on the task parameters. We will first explain that these two goals generally constitute a trade-off, then suggest a solution to this trade-off for a single task parameter, and then generalize to multiple task parameters.</p><p>The trade-off between demixing and compression is illustrated in <xref ref-type="fig" rid="fig2">Figure 2</xref>, where we compare linear discriminant analysis (LDA, <xref ref-type="fig" rid="fig2">Figure 2a,b</xref>), PCA (<xref ref-type="fig" rid="fig2">Figure 2c,d</xref>), and dPCA (<xref ref-type="fig" rid="fig2">Figure 2e–h</xref>). We will first focus on a single task parameter and seek to reduce the activity of <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> neurons responding to three different stimuli. For each stimulus, the joint activity of the two neurons traces out a trajectory in the space of firing rates as time progresses (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). The aim of 'demixing' in this simplified case is to find a linear mapping (<italic>decoder</italic>) of the neural activity that separates the different stimuli (<xref ref-type="fig" rid="fig2">Figure 2a</xref>) and ignores the time-dependency. We can use LDA in order to determine a projection of the data that optimally separates the three stimuli. However, LDA will generally not preserve the 'geometry' of the original neural activity: firing patterns for stimuli 1 and 2 are close to each other and far away from stimulus 3, whereas in the LDA projection all three stimuli are equally spaced (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). More generally, decoding is always prone to distorting the data and therefore tends to impede a proper reconstruction of the original data from the reduced description.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.004</object-id><label>Figure 2.</label><caption><title>Linear dimensionality reduction.</title><p>(<bold>a</bold>) Linear discriminant analysis maps the firing rates of individual neurons onto a latent component that allows us to decode a task parameter of interest. Shades of grey inside each neuron show the proportion of variance due to the various task parameters (e.g. stimulus, decision, and time), illustrating mixed selectivity. In contrast, the LDA component is maximally demixed. (<bold>b</bold>) At any moment in time, the population firing rate of <inline-formula><mml:math id="inf11"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons is represented by a point in the <inline-formula><mml:math id="inf12"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional space; here <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. Each trial is represented by a trajectory in this space. Colors indicate different stimuli and dot sizes represent time. The LDA component for stimulus is given by the projection onto the LDA axis (black line); projections of all points are shown along this line. All three stimuli are clearly separated, but their geometrical relation to each other is lost. (<bold>c</bold>) Principal component analysis linearly maps the firing rates into a few principal components such that a second linear transformation can reconstruct the original firing rates. (<bold>d</bold>) The same set of points as in (<bold>b</bold>) is projected onto the first PCA axis. However, the stimuli are no longer separated. Rather, the points along the PCA axis have complex dependencies on stimulus and time (mixed selectivity). The PCA axis minimizes the distances between the original points and their projections. (<bold>e</bold>) Demixed principal component analysis also compresses and decompresses the firing rates through two linear transformations. However, here the transformations are found by both minimizing the reconstruction error and enforcing a demixing constraint on the latent variables. (<bold>f</bold>) The same set of points as in (<bold>b</bold>) projected onto the first dPCA decoder axis. The three stimuli are clearly separated (as in LDA), but some information about the relative distances between classes is preserved as well (as in PCA). (<bold>g</bold>) The same data as in (<bold>b</bold>) linearly decomposed into the time effect, the stimulus effect, and the noise. (<bold>h</bold>) The dPCA projection from (<bold>f</bold>) has to be mapped onto a different axis, given by the dPCA encoder, in order to reconstruct the stimulus class means (large colored circles). The decoder and encoder axes together minimize the reconstruction error between the original data and the stimulus class means.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.004">http://dx.doi.org/10.7554/eLife.10989.004</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig2-v2"/></fig></p><p>The aim of compression is to find a linear mapping (decoder) that reduces the dimensionality and preserves the original data as much as possible (<xref ref-type="fig" rid="fig2">Figure 2c,d</xref>). Using PCA, we determine a projection of the data that minimizes the reconstruction error between the projections and the original points. In contrast to LDA, PCA seeks to preserve the geometry of the neural activity, and thereby yields the most faithful reduction of the data (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). However, the PCA projection does not properly separate the stimuli and mixes the time-dependency with the stimulus-dependency.</p><p>The wildly different projection axes for LDA (<xref ref-type="fig" rid="fig2">Figure 2b</xref>) and PCA (<xref ref-type="fig" rid="fig2">Figure 2d</xref>) seem to suggest that the goals of demixing and compression are essentially incompatible in this example. However, we can achieve both goals by assuming that the reconstruction of the original data works along a separate <italic>encoder</italic> axis (<xref ref-type="fig" rid="fig2">Figure 2f,h</xref>). Given this additional flexibility, we first choose a decoder axis that reconciles the decoding and compression objectives. Once projected onto this axis, all three stimuli are separated from each other, as in LDA, yet their geometrical arrangement is approximately preserved, as in PCA (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). In turn, when reconstructed along the encoder axis, the projected data still approximates the original data (<xref ref-type="fig" rid="fig2">Figure 2h</xref>).</p><p>To define these ideas more formally, we assume that we simultaneously recorded the spike trains of <inline-formula><mml:math id="inf14"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons. Let <inline-formula><mml:math id="inf15"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> be our data matrix with <inline-formula><mml:math id="inf16"><mml:mi>N</mml:mi></mml:math></inline-formula> rows, in which the <inline-formula><mml:math id="inf17"><mml:mi>i</mml:mi></mml:math></inline-formula>-th row contains the instantaneous firing rate (i.e. binned or smoothed spike train) of the <inline-formula><mml:math id="inf18"><mml:mi>i</mml:mi></mml:math></inline-formula>-th neuron for all task conditions and all trials (assumed to be centered, i.e., with row means subtracted). Classical PCA compresses the data with a decoder matrix <inline-formula><mml:math id="inf19"><mml:mi mathvariant="bold">𝐃</mml:mi></mml:math></inline-formula>. The resulting principal components can then be linearly de-compressed through an encoder matrix <inline-formula><mml:math id="inf20"><mml:msup><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:math></inline-formula>, approximately reconstructing the original data (<xref ref-type="bibr" rid="bib15">Hastie et al., 2009</xref>). The optimal decoder matrix is found by minimizing the squared error between the original data, <inline-formula><mml:math id="inf21"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>, and the reconstructed data, <inline-formula><mml:math id="inf22"><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐃𝐗</mml:mi></mml:mrow></mml:math></inline-formula>, given by<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In the toy example of <xref ref-type="fig" rid="fig2">Figure 2</xref>, the data matrix <inline-formula><mml:math id="inf23"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> is of size <inline-formula><mml:math id="inf24"><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula>, and the decoder matrix <inline-formula><mml:math id="inf25"><mml:mi mathvariant="bold">𝐃</mml:mi></mml:math></inline-formula> is of size <inline-formula><mml:math id="inf26"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. Crucially, the information about task parameters does not enter the loss function and hence PCA neither decodes nor demixes these parameters.</p><p>In our method, which we call <italic>demixed PCA (dPCA)</italic>, we make two changes to this classical formulation. First, we require that the compression and decompression steps reconstruct not the neural activity directly, but the neural activity averaged over trials and over some of the task parameters. In the toy example, the reconstruction target is the matrix of stimulus averages, <inline-formula><mml:math id="inf27"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>, which has the same size as <inline-formula><mml:math id="inf28"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>, but in which every data point is replaced by the average neural activity for the corresponding stimulus, as shown in <xref ref-type="fig" rid="fig2">Figure 2h</xref>. Second, we gain additional flexibility in this quest by compressing the data with a linear mapping <inline-formula><mml:math id="inf29"><mml:mi mathvariant="bold">𝐃</mml:mi></mml:math></inline-formula>, yet decompressing it with another linear mapping <inline-formula><mml:math id="inf30"><mml:mi mathvariant="bold">𝐅</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). The respective matrices are chosen by minimizing the loss function<disp-formula id="equ2"><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Accordingly, for each stimulus, the neural activities are projected close to the average stimulus, which allows us both to decode the stimulus value and to preserve the relative distances of the neural activities.</p><p>In order to see how this approach preserves all aspects of the original data, and not just some averages, we note that the data in our toy example included both stimulus and time. The matrix <inline-formula><mml:math id="inf31"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> can be understood as part of a linear decomposition of the full data <inline-formula><mml:math id="inf32"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> into parameter-specific averages: a time-varying part, <inline-formula><mml:math id="inf33"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, that is obtained by averaging <inline-formula><mml:math id="inf34"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> over stimuli, and a stimulus-varying part, <inline-formula><mml:math id="inf35"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>, that is obtained by averaging <inline-formula><mml:math id="inf36"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> over time. Any remaining parts of the activity are captured in a noise term (<xref ref-type="fig" rid="fig2">Figure 2g</xref>). In turn, we can find separate decoder and encoder axes for each of these averages. Once more than <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> neurons are considered, these decoder and encoder axes constitute a dimensionality reduction step that reduces the data into a few components, each of which properly decodes one of the task parameters. In turn, the original neural activity can be reconstructed through linear combinations of these components, just as in PCA.</p><p>The key ideas of this toy example can be extended to any number of task parameters. In this manuscript, all datasets will have three parameters: time, stimulus, and decision, and we will decompose the neural activities into five parts: condition-independent, stimulus-dependent, decision-dependent, dependent on the stimulus-decision interaction, and noise (see <xref ref-type="fig" rid="fig8">Figure 8</xref> in the Materials and methods):<disp-formula id="equ3"><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Individual terms are again given by a series of averages. This decomposition is fully analogous to the variance (covariance) decomposition done in ANOVA (MANOVA). The only important difference is that the standard (M)ANOVA decomposition for three parameters A, B, and C, would normally have <inline-formula><mml:math id="inf38"><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mn>3</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> terms corresponding to the main effects of A, B, C, pairwise interactions AB, BC, and AC, three-way interaction ABC, and the noise. Here we join some of these terms together, as we are not interested in demixing those (see Materials and methods).</p><p>Once this decomposition is performed, dPCA finds separate decoder and encoder matrices for each term <inline-formula><mml:math id="inf39"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> by minimizing the loss function<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>dPCA</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>ϕ</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM3"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Each term within the sum can be minimized separately by using <italic>reduced-rank regression</italic>, the solution of which can be obtained analytically in terms of singular value decompositions (see Materials and methods). Each row <inline-formula><mml:math id="inf40"><mml:mi mathvariant="bold">𝐝</mml:mi></mml:math></inline-formula> of each <inline-formula><mml:math id="inf41"><mml:msub><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> yields one <italic>demixed principal component</italic> <inline-formula><mml:math id="inf42"><mml:mi mathvariant="bold">𝐝𝐗</mml:mi></mml:math></inline-formula> and, similar to PCA, we order the components by the amount of explained variance. Note that the decoder/encoder axes corresponding to two different task parameters <inline-formula><mml:math id="inf43"><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf44"><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> are found independently from each other and may end up being non-orthogonal (in contrast to PCA where principal axes are all orthogonal). In a nutshell, the loss function ensures that each set of decoder/encoder axes reconstructs the individual, parameter-specific terms, <inline-formula><mml:math id="inf45"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>, thereby yielding proper demixing, and the data decomposition ensures that the combination of all decoder/encoder pairs allows to reconstruct the original data, <inline-formula><mml:math id="inf46"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>.</p><p>There are a few other technical subtleties (see Materials and methods for details). (1) We formulated dPCA for simultaneously recorded neural activities. However, all datasets analyzed in this manuscript have been recorded sequentially across many sessions, and so to apply dPCA we have to use 'pseudo-trials'. (2) Similar to any other decoding method, dPCA is prone to overfitting and so we introduce a regularization term and perform cross-validation to choose the regularization parameter. (3) The data and variance decompositions from above are exact only if the dataset is <italic>balanced</italic>, i.e., if the same number of trials were recorded in each condition. If this is not the case, one can use a re-balancing procedure. (4) A previous version of dPCA (<xref ref-type="bibr" rid="bib2">Brendel et al., 2011</xref>) used the same variance decomposition but a different and less flexible loss function. The differences are layed out in the Materials and methods section.</p></sec><sec id="s2-3"><title>Somatosensory working memory task in monkey PFC</title><p>We first applied dPCA to the dataset presented above (<xref ref-type="bibr" rid="bib40">Romo et al., 1999</xref>; <xref ref-type="bibr" rid="bib3">Brody et al., 2003</xref>), encompassing 832 neurons from two animals. As is typical for PFC, each neuron has a distinct response pattern and many neurons show mixed selectivity (some examples are shown in <xref ref-type="fig" rid="fig1">Figure 1b</xref>). Several previous studies have sought to make sense of these heterogeneous response patterns by separately analyzing different task periods, such as the stimulation and delay periods (<xref ref-type="bibr" rid="bib40">Romo et al., 1999</xref>; <xref ref-type="bibr" rid="bib3">Brody et al., 2003</xref>; <xref ref-type="bibr" rid="bib24">Machens et al., 2010</xref>; <xref ref-type="bibr" rid="bib1">Barak et al., 2010</xref>), the decision period (<xref ref-type="bibr" rid="bib20">Jun et al., 2010</xref>), or both (<xref ref-type="bibr" rid="bib16">Hernández et al., 2010</xref>). With dPCA, however, we can summarize the main features of the neural activity across the whole trial in a single figure (<xref ref-type="fig" rid="fig3">Figure 3</xref>).<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.005</object-id><label>Figure 3.</label><caption><title>Demixed PCA applied to recordings from monkey PFC during a somatosensory working memory task (<xref ref-type="bibr" rid="bib40">Romo et al., 1999</xref>).</title><p>(<bold>a</bold>) Cartoon of the paradigm, adapted from <xref ref-type="bibr" rid="bib41">Romo and Salinas (2003)</xref>. (<bold>b</bold>) Demixed principal components. Top row: first three condition-independent components; second row: first three stimulus components; third row: first three decision components; last row: first stimulus/decision interaction component. In each subplot, the full data are projected onto the respective dPCA decoder axis, so that there are 12 lines corresponding to 12 conditions (see legend). Thick black lines show time intervals during which the respective task parameters can be reliably extracted from single-trial activity (using pseudotrials with all recorded neurons), see Materials and methods. Note that the vertical scale differs across rows. Ordinal number of each component is shown in a circle; explained variances are shown as percentages. (<bold>c</bold>) Cumulative variance explained by PCA (black) and dPCA (red). Demixed PCA explains almost the same amount of variance as standard PCA. Dashed line shows an estimate of the fraction of 'signal variance' in the data, the remaining variance is due to noise in the PSTH estimates (see Materials and methods). (<bold>d</bold>) Variance of the individual demixed principal components. Each bar shows the proportion of total variance, and is composed out of four stacked bars of different color: gray for condition-independent variance, blue for stimulus variance, red for decision variance, and purple for variance due to stimulus-decision interactions. Each bar appears to be single-colored, which signifies nearly perfect demixing. Pie chart shows how the total signal variance is split among parameters. (<bold>e</bold>) Upper-right triangle shows dot products between all pairs of the first 15 demixed principal axes. Stars mark the pairs that are significantly and robustly non-orthogonal (see Materials and methods). Bottom-left triangle shows correlations between all pairs of the first 15 demixed principal components. Most of the correlations are close to zero.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.005">http://dx.doi.org/10.7554/eLife.10989.005</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig3-v2"/></fig></p><p>Just as in PCA, we can think of the demixed principal components (<xref ref-type="fig" rid="fig3">Figure 3b</xref>) as the 'building blocks' of the observed neural activity, in that the activity of each single neuron is a linear combination (weighted average) of these components. These building blocks come in four distinct categories: some are condition-independent (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, top row); some depend only on stimulus F1 (second row); some depend only on decision (third row); and some depend on stimulus and decision together (bottom row). The components can be easily seen to demix the parameter dependencies, which is exactly what dPCA aimed for. Indeed, the components shown in <xref ref-type="fig" rid="fig3">Figure 3b</xref> are projections of the PSTHs of all neurons onto the most prominent decoding axes; each projection (each subplot) shows 12 lines corresponding to 12 conditions. As intended, condition-independent components have all 12 lines closely overlapping, stimulus components have two lines for each stimulus closely overlapping, etc.</p><p>The overall variance explained by the dPCA components (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, red line) is very close to the overall variance explained by the PCA components (black line). Accordingly, we barely lost any variance by imposing the demixing constraint, and the population activity is accurately represented by the obtained dPCA components.</p><p>The dPCA analysis captures the major findings previously obtained with these data: the persistence of the F1 tuning during the delay period (component #5; <xref ref-type="bibr" rid="bib40">Romo et al., 1999</xref>; <xref ref-type="bibr" rid="bib23">Machens et al., 2005</xref>), the temporal dynamics of short-term memory (components ##5, 10, 13; <xref ref-type="bibr" rid="bib3">Brody et al., 2003</xref>; <xref ref-type="bibr" rid="bib24">Machens et al., 2010</xref>; <xref ref-type="bibr" rid="bib1">Barak et al., 2010</xref>), the 'ramping' or 'climbing' activities in the delay period (components ##1–3; <xref ref-type="bibr" rid="bib3">Brody et al., 2003</xref>; <xref ref-type="bibr" rid="bib24">Machens et al., 2010</xref>); and pronounced decision-related activities (component #6, <xref ref-type="bibr" rid="bib20">Jun et al., 2010</xref>). We note that the decision components resemble derivatives of each other; these higher-order derivatives likely arise due to slight variations in the timing of responses across neurons (see Appendix B for more details).</p><p>The first stimulus component (#5) looks similar to the stimulus components that we obtained with standard regression-based methods (<xref ref-type="fig" rid="fig1">Figure 1e,h</xref>) but now we have further components as well. Together they show how stimulus representation evolves in time. In particular, plotting the first two stimulus components against each other (see <xref ref-type="other" rid="media1">Video 1</xref>) illustrates how stimulus representation rotates in the neural space during the delay period so that the encoding subspaces during F1 and F2 periods are not the same (but far from orthogonal either).<media content-type="glencoe play-in-place height-250 width-310" id="media1" mime-subtype="mp4" mimetype="video" xlink:href="elife-10989-media1.mp4"><object-id pub-id-type="doi">10.7554/eLife.10989.006</object-id><label>Video 1.</label><caption><title>Stimulus representation in the somatosensory working memory task</title><p>Two leading stimulus dPCs in the somatosensory working memory task (components #5 and #10 as horizontal and vertical axis correspondingly). Each frame of this movie corresponds to one time point <inline-formula><mml:math id="inf47"><mml:mi>t</mml:mi></mml:math></inline-formula>. Each dot is the average between two decision conditions with the same F1 stimulus. Fading 'tails' show last sections of the trajectories. See <xref ref-type="fig" rid="fig3">Figure 3</xref> for the color code.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.006">http://dx.doi.org/10.7554/eLife.10989.006</ext-link></p></caption></media></p><p>As explained above, the demixed principal axes are not constrained to be orthogonal. The angles between the encoding axes are shown in <xref ref-type="fig" rid="fig3">Figure 3e</xref>, upper-right triangle; we discuss them later, together with other datasets. Pairwise correlations between components are all close to zero (<xref ref-type="fig" rid="fig3">Figure 3e</xref>, lower-left triangle), as should be expected since the components are considered to represent independent signals.</p><p>To assess whether the condition tuning of individual dPCA components was statistically significant, we used each component as a linear decoder to classify conditions. Specifically, stimulus components were used to classify stimuli, decision components to classify decisions, and interaction components to classify all 12 conditions. We used cross-validation to measure time-dependent classification accuracy and a shuffling procedure to assess whether it was significantly above chance (see Materials and methods). Time periods of significant tuning are marked in <xref ref-type="fig" rid="fig3">Figure 3b</xref> with horizontal black lines.</p></sec><sec id="s2-4"><title>Visuospatial working memory task in monkey PFC</title><p>We next applied dPCA to recordings from the PFC of monkeys performing a visuospatial working memory task (<xref ref-type="bibr" rid="bib31">Qi et al., 2011</xref>, <xref ref-type="bibr" rid="bib32">2012</xref>; <xref ref-type="bibr" rid="bib27">Meyer et al., 2011</xref>). In this task, monkeys first fixated a small white square at the centre of a screen, after which a square S1 appeared for 0.5 s in one of eight locations around the centre (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). After a 1.5 s delay, a second square S2 appeared for 0.5 s in either the same ('match') or the opposite ('non-match') location. Following another 1.5 s delay, a green and a blue choice target appeared in locations orthogonal to the earlier presented stimuli. Monkeys had to saccade to the green target to report a match condition, and to the blue one to report a non-match.<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.007</object-id><label>Figure 4.</label><caption><title>Demixed PCA applied to recordings from monkey PFC during a visuospatial working memory task (<xref ref-type="bibr" rid="bib31">Qi et al., 2011</xref>).</title><p>Same format as <xref ref-type="fig" rid="fig3">Figure 3</xref>. (<bold>a</bold>) Cartoon of the paradigm, adapted from <xref ref-type="bibr" rid="bib41">Romo and Salinas (2003)</xref>. (<bold>b</bold>) Demixed principal components. In each subplot there are ten lines corresponding to ten conditions (see legend). Color corresponds to the position of the last shown stimulus (first stimulus for <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> s, second stimulus for <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> s). In non-match conditions (dashed lines) the colour changes at <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> s. Solid lines correspond to match conditions and do not change colors. (<bold>c</bold>) Cumulative variance explained by PCA and dPCA components. Dashed line marks fraction of signal variance. (<bold>d</bold>) Explained variance of the individual demixed principal components. Pie chart shows how the total signal variance is split between parameters. (<bold>e</bold>) Upper-right triangle shows dot products between all pairs of the first 15 demixed principal axes, bottom-left triangle shows correlations between all pairs of the first 15 demixed principal components.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.007">http://dx.doi.org/10.7554/eLife.10989.007</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig4-v2"/></fig></p><p>We analyzed the activity of 956 neurons recorded in the lateral PFC of two monkeys performing this task. Proceeding exactly as before, we obtained the average time-dependent firing rate of each neuron for each condition. Following the original studies, we eliminated the trivial rotational symmetry of the task by collapsing the eight possible stimulus locations into five locations that are defined with respect to the preferred location of each neuron (0°, 45°, 90°, 135°, or 180° away from the preferred location, see Materials and methods). As a consequence, we obtained ten conditions: five possible stimulus locations, each paired with two possible decisions of the monkey.</p><p>The dPCA results are shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. As before, stimulus and decision are well separated at the population level despite being intermingled at the single-neuron level; at the same time dPCA captures almost the same amount of variance as PCA. One notable difference from before is the presence of strong interaction components in <xref ref-type="fig" rid="fig4">Figure 4b</xref>. However, these interaction components are in fact stimulus components in disguise. In match trials, S2 and S1 appear at the same location, and in non-match trials at opposite locations. Information about S2 is therefore given by a non-linear function of stimulus S1 and the trial type (i.e. decision), which is here captured by the interaction components.</p><p>Here again, our analysis summarizes previous findings obtained with this dataset. For instance, the first and the second decision components show tuning to the match/non-match decision during the S2 period and in the subsequent delay period. Using these components as fixed linear decoders, we achieve single-trial classification accuracy of match vs. non-match of 75% for <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (cross-validated, see Materials and methods, <xref ref-type="fig" rid="fig12">Figure 12</xref>), which is approximately equal to the state-of-the-art classification performance reported previously (<xref ref-type="bibr" rid="bib26">Meyers et al., 2012</xref>).</p><p>Constantinidis et al. have also recorded population activity in PFC before starting the training (both S1 and S2 stimuli were presented exactly as above, but there were no cues displayed and no decision required). When analyzing this pre-training population activity with dPCA, the first stimulus and the first interaction components come out close to the ones shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, but there are no decision and no 'memory' components present (data not shown), in line with previous findings (<xref ref-type="bibr" rid="bib26">Meyers et al., 2012</xref>). These task-specific components appear in the population activity only after extensive training.</p></sec><sec id="s2-5"><title>Olfactory discrimination task in rat OFC</title><p>Next, we applied dPCA to recordings from the OFC of rats performing an odor discrimination task (<xref ref-type="bibr" rid="bib13">Feierstein et al., 2006</xref>). This behavioral task differs in two crucial aspects from the previously considered tasks: it requires no active storage of a stimulus, and it is self-paced. To start a trial, rats entered an odor port, which triggered delivery of an odor with a random delay of 0.2–0.5 s. Each odor was uniquely associated with one of the two available water ports, located to the left and to the right from the odor port (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). Rats could sample the odor for as long as they wanted (up to 1 s), and then had to move to one of the water ports. If they chose the correct water port, reward was delivered following an anticipation period of random length (0.2–0.5 s).<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.008</object-id><label>Figure 5.</label><caption><title>Demixed PCA applied to recordings from rat OFC during an olfactory discrimination task (<xref ref-type="bibr" rid="bib13">Feierstein et al., 2006</xref>).</title><p>Same format as <xref ref-type="fig" rid="fig3">Figure 3</xref>. (<bold>a</bold>) Cartoon of the paradigm, adapted from <xref ref-type="bibr" rid="bib46">Wang et al. (2013)</xref>. (<bold>b</bold>) Each subplot shows one demixed principal component. In each subplot there are four lines corresponding to four conditions (see legend). Two out of these four conditions were rewarded and are shown by thick lines. (<bold>c</bold>) Cumulative variance explained by PCA and dPCA components. (<bold>d</bold>) Explained variance of the individual demixed principal components. Pie chart shows how the total signal variance is split between parameters. (<bold>e</bold>) Upper-right triangle shows dot products between all pairs of the first 15 demixed principal axes, bottom-left triangle shows correlations between all pairs of the first 15 demixed principal components.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.008">http://dx.doi.org/10.7554/eLife.10989.008</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig5-v2"/></fig></p><p>We analyzed the activity of 437 neurons recorded in five rats in four conditions: two stimuli (left and right) each paired with two decisions (left and right). Two of these conditions correspond to correct (rewarded) trials, and two correspond to error (unrewarded) trials. Since the task was self-paced, each trial had a different length; in order to align events across trials, we restretched (time-warped) the firing rates in each trial (see Materials and methods). Alignment methods without time warping led to similar results (data not shown).</p><p>Just as neurons from monkey PFC, neurons in rat OFC exhibit diverse firing patterns and mixed selectivity (<xref ref-type="bibr" rid="bib13">Feierstein et al., 2006</xref>). Nonetheless, dPCA was able to demix the population activity (<xref ref-type="fig" rid="fig5">Figure 5</xref>). In this dataset, interaction components separate rewarded and unrewarded conditions (thick and thin lines in <xref ref-type="fig" rid="fig5">Figure 5b</xref>, bottom row), i.e., correspond to neurons tuned either to reward, or to the absence of reward.</p><p>The overall pattern of neural tuning across task epochs agrees with the findings of the original study (<xref ref-type="bibr" rid="bib13">Feierstein et al., 2006</xref>). Interaction components are by far the most prominent among all the condition-dependent components, corresponding to the observation that many neurons are tuned to the presence/absence of reward. Decision components come next, with the caveat that decision information may also reflect the rat’s movement direction and/or position, as was pointed out previously (<xref ref-type="bibr" rid="bib13">Feierstein et al., 2006</xref>). Stimulus components are less prominent, but nevertheless show clear stimulus tuning, demonstrating that even in error trials there is reliable information about stimulus identity in the population activity.</p><p>Curiously, the first interaction component (#4) already shows significant tuning to reward in the anticipation period. In other words, neurons tuned to presence/absence of reward start firing before the reward delivery (or, on error trials, before the reward could have been delivered). We return to this observation in the next section.</p></sec><sec id="s2-6"><title>Olfactory categorization task in rat OFC</title><p><xref ref-type="bibr" rid="bib21">Kepecs et al. (2008)</xref> extended the experiment of <xref ref-type="bibr" rid="bib13">Feierstein et al. (2006)</xref> by using odor mixtures instead of pure odors, thereby varying the difficulty of each trial (<xref ref-type="bibr" rid="bib45">Uchida and Mainen, 2003</xref>). In each trial, rats experienced mixtures of two fixed odors with different proportions (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). Left choices were rewarded if the proportion of the 'left' odor was above 50%, and right choices otherwise. Furthermore, the waiting time until reward delivery (anticipation period) was increased to 0.3–2 s.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.009</object-id><label>Figure 6.</label><caption><title>Demixed PCA applied to recordings from rat OFC during an olfactory categorization task (<xref ref-type="bibr" rid="bib21">Kepecs et al., 2008</xref>).</title><p>Same format as <xref ref-type="fig" rid="fig3">Figure 3</xref> (<bold>a</bold>) Cartoon of the paradigm, adapted from <xref ref-type="bibr" rid="bib46">Wang et al. (2013)</xref>. (<bold>b</bold>) Each subplot shows one demixed principal component. In each subplot there are ten lines corresponding to ten conditions (see legend). Six out of these ten conditions were rewarded and are shown with thick lines; note that the pure left (red) and the pure right (blue) odors did not have error trials. Inset shows mean rate of the second interaction component during the anticipation period. (<bold>c</bold>) Cumulative variance explained by PCA and dPCA components. (<bold>d</bold>) Explained variance of the individual demixed principal components. Pie chart shows how the total signal variance is split between parameters. (<bold>e</bold>) Upper-right triangle shows dot products between all pairs of the first 15 demixed principal axes, bottom-left triangle shows correlations between all pairs of the first 15 demixed principal components.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.009">http://dx.doi.org/10.7554/eLife.10989.009</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig6-v2"/></fig></p><p>We analyzed the activity of 214 OFC neurons from three rats recorded in 8 conditions, corresponding to four odor mixtures, each paired with two decisions (left and right). During the presentation of pure odors (100% right and 100% left) rats made essentially no mistakes, and so we excluded these data from the dPCA computations (which require that all parameter combinations are present, see Discussion). Nevertheless, we displayed these additional two conditions in <xref ref-type="fig" rid="fig6">Figure 6</xref>.</p><p>The dPCA components shown in <xref ref-type="fig" rid="fig6">Figure 6b</xref> are similar to those presented in <xref ref-type="fig" rid="fig5">Figure 5b</xref>. Here again, some of the interaction components (especially the second one, #5) show strong tuning already during the anticipation period, i.e. before the actual reward delivery. The inset in <xref ref-type="fig" rid="fig6">Figure 6b</xref> shows the mean value of the component #5 during the anticipation period, separating correct (green) and incorrect (red) trials for each stimulus. The characteristic U-shape for the error trials and the inverted U-shape for the correct trials agrees well with the predicted value of the rat’s uncertainty in each condition (<xref ref-type="bibr" rid="bib21">Kepecs et al., 2008</xref>). Accordingly, this component can be interpreted as corresponding to the rat’s uncertainty or confidence about its own choice, confirming the results of <xref ref-type="bibr" rid="bib21">Kepecs et al. (2008)</xref>. In summary, both the main features of this dataset, as well as some of the subtleties, are picked up and reproduced by dPCA.</p></sec><sec id="s2-7"><title>Universal features of the PFC population activity</title><p>One of the key advantages of applying dPCA to these four datasets is that we can now compare them far more easily than was previously possible. This comparison allows us to highlight several general features of the population activity in prefrontal areas.</p><p>First, most of the variance of the neural activity is always captured by the condition-independent components that together amount to 65–90% of the signal variance (see pie charts in <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6d</xref>; see Materials and methods for definition of 'signal variance'). These components capture the temporal modulations of the neural activity throughout the trial, irrespective of the task condition. Their striking dominance in the data may come as a surprise, as such condition-independent components are usually not analyzed or shown (cf. <xref ref-type="fig" rid="fig1">Figure 1e,h</xref>), even though condition-independent firing has been described even in sensory areas (<xref ref-type="bibr" rid="bib43">Sornborger et al., 2005</xref>). These components are likely explained in part by an overall firing rate increase during certain task periods (e.g. during stimulus presentation). More speculatively, they could also be influenced by residual sensory or motor variables that vary rhythmically with the task, but are not controlled or monitored (<xref ref-type="bibr" rid="bib35">Renart and Machens, 2014</xref>). The attentional or motivational state of animals, for instance, often correlates with breathing (<xref ref-type="bibr" rid="bib17">Huijbers et al., 2014</xref>), pupil dilation (<xref ref-type="bibr" rid="bib11">Eldar et al., 2013</xref>), body movements (<xref ref-type="bibr" rid="bib14">Gouvêa et al., 2014</xref>), etc.</p><p>Second, even though dPCA, unlike PCA, does not enforce orthogonality between encoding axes corresponding to different task parameters, most of them turned out to be close to orthogonal to each other (<xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6e</xref>, upper triangle), as has been observed before (<xref ref-type="bibr" rid="bib2">Brendel et al., 2011</xref>; <xref ref-type="bibr" rid="bib37">Rishel et al., 2013</xref>; <xref ref-type="bibr" rid="bib33">Raposo et al., 2014</xref>). Nevertheless, many pairs were significantly non-orthogonal, meaning that neurons expressing one of the components tended to also express the other one. Throughout the four datasets, we identified 277 pairs of axes (among the first 15 axes) corresponding to different parameters. Of these, 38, i.e. 14%, were significantly non-orthogonal with <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula> (8 out of 53 if we do not take time axes into account).</p><p>Third, all dPCA components in each of the datasets are distributed across the whole neural population (as opposed to being exhibited only by a subset of cells). For each component and each neuron, the corresponding encoder weight shows how much this particular component is exhibited by this particular neuron. For each component, the distribution of weights is strongly unimodal, centred at zero (<xref ref-type="fig" rid="fig7">Figure 7a</xref>), and rather symmetric (although it is skewed to one side for some components). In other words, there are no distinct sub-populations of neurons predominantly expressing a particular component; rather, each individual neuron can be visualized as a random linear combination of these components. We confirmed this observation by applying a recently developed clustering algorithm (<xref ref-type="bibr" rid="bib38">Rodriguez and Laio, 2014</xref>) to the population of neurons in the 15-dimensional space of dPC weights. In all cases, the algorithm found only one cluster (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). An alternative clustering analysis with Gaussian mixture models yielded similar results (data not shown). This absence of any detectable clusters of neurons has been noted before (<xref ref-type="bibr" rid="bib24">Machens et al., 2010</xref>) and was recently observed in other datasets as well (<xref ref-type="bibr" rid="bib33">Raposo et al., 2014</xref>).<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.010</object-id><label>Figure 7.</label><caption><title>Encoder weights for the leading dPCA components across the neural population.</title><p>(<bold>a</bold>) Distributions of encoder weights for the 15 leading dPCA components across the neural population, in each of the four datasets. Each subplot shows 15 probability density curves, one curve per component (bin width 0.005). The distribution corresponding to the first component is highlighted in red. (<bold>b</bold>) Clustering of neurons by density peaks (<xref ref-type="bibr" rid="bib38">Rodriguez and Laio, 2014</xref>). For each dataset we took the first 15 dPCA components, and then ran the clustering algorithm in the 15-dimensional space of encoding weights. The clustering algorithm works in two steps: first, it computes a local density for each point (i.e., for each neuron), using a Gaussian kernel with <inline-formula><mml:math id="inf53"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>. Second, for each point it finds the minimal distance to a point with higher local density (if there is no such point, then the distance to the furthest point is taken). Each subplot shows local density on the horizontal axis plotted against distance to the next point with higher density on the vertical axis; each dot corresponds to one of the <inline-formula><mml:math id="inf54"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons. Cluster centres are characterized by high local density and large distance to the point of even higher density; they should appear as outliers in the upper-right corner of the plot (see <xref ref-type="bibr" rid="bib38">Rodriguez and Laio, 2014</xref>, for details). In each case, there is only one such outlier (bigger dot), indicating a single cluster.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.010">http://dx.doi.org/10.7554/eLife.10989.010</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig7-v2"/></fig></p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Mixed selectivity of neurons in higher cortical areas has been increasingly recognized as a problem for the analysis of neurophysiological recordings, with many different approaches suggested to deal with it (<xref ref-type="bibr" rid="bib3">Brody et al., 2003</xref>; <xref ref-type="bibr" rid="bib24">Machens et al., 2010</xref>; <xref ref-type="bibr" rid="bib22">Machens, 2010</xref>; <xref ref-type="bibr" rid="bib2">Brendel et al., 2011</xref>; <xref ref-type="bibr" rid="bib36">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Pagan and Rust, 2014</xref>; <xref ref-type="bibr" rid="bib29">Park et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Raposo et al., 2014</xref>; <xref ref-type="bibr" rid="bib9">Cunningham and Yu, 2014</xref>). The main strength and the main novelty of the method suggested here (dPCA) is that it offers a unified and principled way of analyzing such data.</p><p>Demixed PCA combines the strengths of existing supervised and unsupervised approaches to neural population data analysis (<xref ref-type="table" rid="tbl1">Table 1</xref>, see also the first section of the Results). Supervised methods can characterize population tuning to various parameters of interest but often do not faithfully represent the whole dataset. Unsupervised methods can capture the overall variance but are not informed by task parameters. Our method yields components that capture almost as much variance as PCA does, but are demixed.</p><p>We view both properties as equally important. On one hand, demixing can greatly simplify visualization and interpretation of neural population data. Indeed, in all cases presented here, all the major aspects of the population activity that had previously been reported are directly visible on the dPCA summary figure. On the other hand, faithful representation of the population activity (i.e. 'capturing variance') avoids that a particular interpretation distorts characteristic features of the data. The latter feature is particularly important for the development of theoretical models, which otherwise may inherit an interpretation bias without being aware of it.</p><p>Apart from being a useful tool for analyzing any particular dataset, dPCA highlights common features of neural activity when applied to several datasets, allowing to adopt a comparative approach to study population activity.<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.011</object-id><label>Table 1.</label><caption><p>Demixed PCA in comparison with existing methods. <bold>Columns:</bold> 'Signif.' refers to the method of counting significantly tuned cells, as shown in <xref ref-type="fig" rid="fig1">Figure 1c–e</xref>. TDR refers to the 'targeted dimensionality reduction' of <xref ref-type="bibr" rid="bib25">Mante et al. (2013)</xref> shown in <xref ref-type="fig" rid="fig1">Figure 1f–h</xref>. LDA stands for linear discriminant analysis, but this column applies to any classification method (e.g. support vector machine, ordinal logistic regression, etc.). All classification methods can be used to summarize population tuning via a time-dependent classification accuracy (e.g. <xref ref-type="bibr" rid="bib26">Meyers et al., 2012</xref>). PCA stands for principal component analysis, as shown in <xref ref-type="fig" rid="fig1">Figure 1i–k</xref>. FA stands for factor analysis, GPFA for Gaussian process factor analysis (<xref ref-type="bibr" rid="bib48">Yu et al., 2009</xref>), LDS for hidden linear dynamical system (<xref ref-type="bibr" rid="bib4">Buesing et al., 2012a</xref>; <xref ref-type="bibr" rid="bib5">2012b</xref>), jPCA is the method introduced in <xref ref-type="bibr" rid="bib7">Churchland et al. (2012)</xref> . Some of the existing methods can be extended to become more general, but here we refer to how these methods are actually used in the original research. <bold>Rows:</bold> The first two rows are the two defining goals of dPCA. Following rows highlight notable features of other methods.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.011">http://dx.doi.org/10.7554/eLife.10989.011</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Signif.</th><th>TDR</th><th>LDA</th><th>PCA</th><th>FA</th><th>GPFA</th><th>jPCA</th><th>LDS</th><th><bold>dPCA</bold></th></tr></thead><tbody><tr><td>Takes task parameters into account &amp; provides summary statistics of population tuning</td><td>✓</td><td>✓</td><td>✓</td><td/><td/><td/><td/><td/><td>✓</td></tr><tr><td>Allows to reconstruct neural firing (captures variance)</td><td/><td/><td/><td>✓</td><td>✓</td><td>✓</td><td/><td>✓</td><td>✓</td></tr><tr><td>Based on dynamical model</td><td/><td/><td/><td/><td/><td/><td>✓</td><td>✓</td><td/></tr><tr><td>Based on probabilistic model</td><td/><td/><td/><td/><td>✓</td><td>✓</td><td/><td>✓</td><td/></tr><tr><td>Takes spike trains as input</td><td/><td/><td/><td/><td/><td>✓</td><td/><td>✓</td><td/></tr></tbody></table></table-wrap></p><sec id="s3-1"><title>Relationship to other methods, including our earlier work</title><p>The method presented here is conceptually based on our previous work (<xref ref-type="bibr" rid="bib22">Machens, 2010</xref>; <xref ref-type="bibr" rid="bib24">Machens et al., 2010</xref>; <xref ref-type="bibr" rid="bib2">Brendel et al., 2011</xref>), but is technically very different. The original approach from <xref ref-type="bibr" rid="bib24">Machens et al. (2010)</xref> only works for two parameters of interest, such as time and stimulus. <xref ref-type="bibr" rid="bib22">Machens (2010)</xref> suggested a partial generalization to multiple parameters and <xref ref-type="bibr" rid="bib2">Brendel et al. (2011)</xref> introduced the full covariance decomposition and developed a probabilistic model. However, all of them imposed orthogonality on the decoder/encoder axes (and as a result did not distinguish them), a constraint that cannot be easily relaxed. While we have previously argued that orthogonality is a desirable feature of the decomposition, we now believe that it is better not to impose it upfront. First, by looking across many datasets, we have learnt that encoding subspaces can sometimes be highly non-orthogonal (<xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6e</xref>) and hence not demixable under orthogonality constraints. Second, by not imposing orthogonality, we can easier identify components that are truly orthogonal. Third, removing the orthogonality constraint allowed us to obtain a simple analytical solution in terms of singular value decompositions (see Materials and methods) and hence to avoid local minima, convergence issues, and any additional optimization-related hyperparameters.</p><p>To demonstrate these advantages, we ran the algorithm of <xref ref-type="bibr" rid="bib2">Brendel et al. (2011)</xref>, dPCA-2011, on all our datasets. The resulting components were similar to the components presented here, with the amount of variance captured by the first 15 components being very close; but the achieved demixing was worse. For each component we defined a <italic>demixing index</italic> (see Materials and methods) that is equal to 1 if the component is perfectly demixed. For all datasets, these indices were significantly higher with our current dPCA-2015 method than with dPCA-2011. Moreover, dPCA-2011 failed to find some weak components at all. For comparison, see <xref ref-type="fig" rid="fig14">Figure 14</xref> in the Materials and methods.</p><p>Another method, called 'targeted dimensionality reduction' (TDR) has recently been suggested for neural data analysis and is similar in spirit to dPCA in that it looks for demixing linear projections (<xref ref-type="bibr" rid="bib25">Mante et al., 2013</xref>). As mentioned above, the original application of this method yields only one component per task parameter and ignores the condition-independent components. While TDR can be extended in various ways to yield more components, no principled way of doing it has been suggested so far. Comparison of dPCA with TDR on our datasets shows that dPCA demixes the task-parameter dependencies better than TDR (see <xref ref-type="fig" rid="fig14">Figure 14</xref> in the Materials and methods).</p><p>For an in-depth discussion of the relationship between dPCA and LDA/MANOVA, we refer the reader to the Methods. Briefly, LDA is a one-way technique, meaning that only one parameter (class id) is associated with each data point. Therefore, LDA cannot directly be applied to the demixing problem. While LDA could be generalized to deal with several parameters in a systematic way, such a generalization has not been used for dimensionality reduction of neural data and does not have an established name in the statistical literature (we call it <italic>factorial LDA</italic>). We believe that for the purposes of dimensionality reduction, dPCA is a superior approach since it combines a reasonably high class separation with low reconstruction error, whereas LDA only optimizes class separation without taking the (potential) reconstruction error into account (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). MANOVA, on the other hand, is a statistical test closely related to LDA that deals with multiple parameters. However, it deals with isolating the contribution of each parameter from residual noise rather than from the other parameters, and is therefore not suited for demixing.</p></sec><sec id="s3-2"><title>Limitations and future work</title><p>While we believe that dPCA is an easy-to-use method of visualizing complex data sets with multiple task parameters, several limitations should be kept in mind. First, dPCA as presented here works only with discrete parameters, and all possible parameter combinations must be present in the data. This limitation is the downside of the large flexibility of the method: apart from the demixing constraint, we do not impose any other constraints on the latent variables and their estimation remains essentially non-parametric. In order to be able to treat continuous parameters or missing data (missing parameter combinations), we would need to further constrain the estimation of these latent variables, using e.g. a parametric model. One simple possibility is to directly use a parametric model for the activity of the single neurons, such as the linear model used in <xref ref-type="bibr" rid="bib25">Mante et al. (2013)</xref>, in order to fill in any missing data points, and then run dPCA subsequently.</p><p>Second, the number of neurons needs to be sufficiently high in order to obtain reliable estimates of the demixed components. In our datasets, we found that at least <inline-formula><mml:math id="inf55"><mml:mo>∼</mml:mo></mml:math></inline-formula>100 neurons were needed to achieve satisfactory demixing. The number is likely to be higher if more than three task parameters are to be demixed, as the number of interaction terms grows exponentially with the number of parameters. This trade-off between model complexity and demixing feasibility should be kept in mind when deciding how many parameters to put into the dPCA procedure. In cases when there are many task parameters of interest, dPCA is likely to be less useful than the more standard parametric single-unit approaches (such as linear regression). As a trivial example, imagine that only <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> neuron has been recorded; it might have strong and significant tuning to various parameters of interest, but there is no way to demix (or decode) these parameters from the recorded 'population.'</p><p>Third, even with a large number of neurons, a dataset may be non-demixable, in which case dPCA would fail. For instance, if the high-variance directions of the stimulus and the decision parts of the neural activities fully overlap, then there is no linear decoder that can demix the two parameters.</p><p>Finally, dPCA components corresponding to the same parameter (e.g. successive stimulus components) are here chosen to be orthogonal, similarly to PCA. This can make successive components difficult to interpret (e.g. the second and the third stimulus components in <xref ref-type="fig" rid="fig3">Figure 3</xref>). To make them more interpretable, the orthogonality constraint could be replaced with some other constraints, such as e.g. requiring each component to have activity 'localized' in time. This problem may be addressed in future work.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>We will first explain the dPCA algorithm in the most well-behaved case of simultaneously recorded and fully balanced data. A dataset with categorical predictors is called <italic>balanced</italic> when there is the same number of data points for each combination of predictors; in our case this means that there is the same number of trials for each combination of task parameters. This scenario is unlikely in most practical applications where the experimenter often does not have full control over some of the task parameters (such as e.g. animal’s decisions). Our suggestion for unbalanced datasets is to use what amounts to a 're-balancing' procedure as explained below. Finally, we will deal with the case of sequentially recorded neurons (all datasets analyzed in this manuscript fall into this category).</p><sec id="s4-1"><title>Mathematical notation</title><p>In each of the datasets analyzed in this manuscript, trials can be labeled with two parameters: 'stimulus' and 'decision'. Note that a 'reward' label is not needed, because its value can be deduced from the other two due to the deterministic reward protocols in all tasks. In this situation, for each stimulus <inline-formula><mml:math id="inf57"><mml:mi>s</mml:mi></mml:math></inline-formula> (out of <inline-formula><mml:math id="inf58"><mml:mi>S</mml:mi></mml:math></inline-formula>) and decision <inline-formula><mml:math id="inf59"><mml:mi>d</mml:mi></mml:math></inline-formula> (out of <inline-formula><mml:math id="inf60"><mml:mi>Q</mml:mi></mml:math></inline-formula>), we have a collection of <inline-formula><mml:math id="inf61"><mml:mi>K</mml:mi></mml:math></inline-formula> trials with <inline-formula><mml:math id="inf62"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons recorded in each trial. For each trial <inline-formula><mml:math id="inf63"><mml:mi>k</mml:mi></mml:math></inline-formula> (out of <inline-formula><mml:math id="inf64"><mml:mi>K</mml:mi></mml:math></inline-formula>) and neuron <inline-formula><mml:math id="inf65"><mml:mi>n</mml:mi></mml:math></inline-formula> (out of <inline-formula><mml:math id="inf66"><mml:mi>N</mml:mi></mml:math></inline-formula>) we have a recorded spike train. We denote the filtered (or binned) spike train by <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM4">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and assume that it is sampled at <inline-formula><mml:math id="inf68"><mml:mi>T</mml:mi></mml:math></inline-formula> time points <inline-formula><mml:math id="inf69"><mml:mi>t</mml:mi></mml:math></inline-formula>. To explicitly denote all task parameters, we will write either <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM5">t</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM6">s</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM7">d</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM8">k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for the filtered spike train of one neuron and <inline-formula><mml:math id="inf72"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for the vector of filtered spike trains of all <inline-formula><mml:math id="inf73"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons. The latter notation is more compact and also highlights the tensorial character of the data.</p><p>These data can be thought of as <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:math></inline-formula> time-dependent neural trajectories (<inline-formula><mml:math id="inf75"><mml:mi>K</mml:mi></mml:math></inline-formula> trials for each of the <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:math></inline-formula> conditions) in the <inline-formula><mml:math id="inf77"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional space <inline-formula><mml:math id="inf78"><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). The number of distinct data points in this <inline-formula><mml:math id="inf79"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional space is <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>. We collect the full data with all single trials in a matrix <inline-formula><mml:math id="inf81"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> of size <inline-formula><mml:math id="inf82"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, i.e. <inline-formula><mml:math id="inf83"><mml:mi>N</mml:mi></mml:math></inline-formula> rows and <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> columns. Averaging all <inline-formula><mml:math id="inf85"><mml:mi>K</mml:mi></mml:math></inline-formula> trials for each neuron, stimulus, and decision, yields mean firing rates (PSTHs) that can be collected in a smaller matrix <inline-formula><mml:math id="inf86"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> of size <inline-formula><mml:math id="inf87"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-2"><title>Marginalization procedure</title><p>Consider one single neuron first. We can decompose its filtered spike trains, <inline-formula><mml:math id="inf88"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, into a set of averages (which we call <italic>marginalizations</italic>) over various combinations of parameters. We will denote the average over a set of parameters <inline-formula><mml:math id="inf89"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi id="XM9">a</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM10">b</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM11" mathvariant="normal">…</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> by angular brackets <inline-formula><mml:math id="inf90"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mo id="XM12">⋅</mml:mo><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Let us define the following marginalized averages:<disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>d</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⋅</mml:mo><mml:mi>d</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>d</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>d</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⋅</mml:mo><mml:mi>d</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mphantom><mml:mo>=</mml:mo></mml:mphantom></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>d</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf91"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> is simply the overall mean firing rate of our neuron, <inline-formula><mml:math id="inf92"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is the average time-varying firing rate once the overall mean has been subtracted, etc. The right-hand side shows the same averaging procedure in the more explicit form using ANOVA-style notation, in which averages of <inline-formula><mml:math id="inf93"><mml:mi>x</mml:mi></mml:math></inline-formula> over everything apart from the explicitly mentioned parameters, e.g., the stimulus <inline-formula><mml:math id="inf94"><mml:mi>s</mml:mi></mml:math></inline-formula>, are denoted by terms of the form <inline-formula><mml:math id="inf95"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mrow id="XM103"><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>⁣</mml:mo><mml:mrow id="XM104"><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>. One can directly see that the original neural activities are given by the sum of all marginalizations:<disp-formula id="equ6"><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This decomposition is identical to the one used in factorial ANOVA (<xref ref-type="bibr" rid="bib42">Rutherford, 2001</xref>; <xref ref-type="bibr" rid="bib6">Christensen, 2011</xref>) where task parameters are called <italic>factors</italic>. The ANOVA literature uses a slightly different notation with task parameters (<inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) replaced by indices (<inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and with Greek letters designating individual terms:<disp-formula id="equ7"><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ζ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We will use our notation, though, to keep the connection with the task parameters more explicit.</p><p>For the purposes of demixing neural signals in the context of our datasets, we combine some of these terms together. Indeed, demixing a time-independent pure stimulus term <inline-formula><mml:math id="inf98"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> from a stimulus-time interaction term <inline-formula><mml:math id="inf99"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> makes little sense because we expect <italic>all</italic> neural components to change with time. Hence, we group the terms as follows (without changing the notation):<disp-formula id="equ8"><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:munder></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:munder></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:munder></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here the first term on the right-hand side is the mean firing rate, the last term is the trial-to-trial noise, and we call the other terms condition-independent term, stimulus term, decision term, and stimulus-decision interaction term. This decomposition is illustrated in <xref ref-type="fig" rid="fig8">Figure 8</xref> for several exemplary neurons (we only show the decomposition of the PSTH part, leaving out the noise term).<fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.012</object-id><label>Figure 8.</label><caption><title>Marginalization procedure.</title><p>PSTHs of three exemplary neurons from the somatosensory working memory task decomposed into marginalizations.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.012">http://dx.doi.org/10.7554/eLife.10989.012</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig8-v2"/></fig></p><p>We apply this marginalization procedure to every neuron, splitting the whole data matrix <inline-formula><mml:math id="inf100"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> into parts. Assuming from now on that the data matrix is centered (i.e. <inline-formula><mml:math id="inf101"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for all neurons), we can write the decomposition in the matrix form<disp-formula id="equ9"><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are labels and not indices, and all terms are understood to be matrices of the same <inline-formula><mml:math id="inf106"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> size, so e.g. <inline-formula><mml:math id="inf107"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is not an <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> sized matrix, but the full size <inline-formula><mml:math id="inf109"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> matrix with <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> unique values replicated <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:math></inline-formula> times. Crucially, the marginalization procedure ensures that all terms are uncorrelated and that the <inline-formula><mml:math id="inf112"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> covariance matrix <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐗𝐗</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM113"><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is linearly decomposed into the sum of covariance matrices from each marginalization (see Appendix A for the proof):<disp-formula id="equ10"><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here all covariance matrices are defined with the same denominator, i.e. <inline-formula><mml:math id="inf114"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM114"><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-3"><title>Core dPCA: loss function and algorithm</title><p>Given a decomposition <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>ϕ</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>noise</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the loss function of dPCA is given by<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>ϕ</mml:mi></mml:munder><mml:msub><mml:mi>L</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM115"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where each <inline-formula><mml:math id="inf116"><mml:msub><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> is an encoder matrix with <inline-formula><mml:math id="inf117"><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> columns and each <inline-formula><mml:math id="inf118"><mml:msub><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> is a decoder matrix with <inline-formula><mml:math id="inf119"><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> rows. Here and below, matrix norm signifies Frobenius norm, i.e. <inline-formula><mml:math id="inf120"><mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mi id="XM116" mathvariant="bold">𝐗</mml:mi><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. In the remaining discussion, it will often be sufficient to focus on the individual loss functions <inline-formula><mml:math id="inf121"><mml:msub><mml:mi>L</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>, in which case we will drop the indices <inline-formula><mml:math id="inf122"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> on the decoder and encoder matrices for notational convenience, and simply write <inline-formula><mml:math id="inf123"><mml:mi mathvariant="bold">𝐅</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf124"><mml:mi mathvariant="bold">𝐃</mml:mi></mml:math></inline-formula>.</p><p>Without any additional constraints, the decoder and encoder are only defined up to their product <inline-formula><mml:math id="inf125"><mml:mi mathvariant="bold">𝐅𝐃</mml:mi></mml:math></inline-formula> of rank <inline-formula><mml:math id="inf126"><mml:mi>q</mml:mi></mml:math></inline-formula>. To make the decomposition unique, we will assume that <inline-formula><mml:math id="inf127"><mml:mi mathvariant="bold">𝐅</mml:mi></mml:math></inline-formula> has orthonormal columns and that components are ordered such that their variance (row variance of <inline-formula><mml:math id="inf128"><mml:mi mathvariant="bold">𝐃𝐗</mml:mi></mml:math></inline-formula>) is decreasing. The reason for this choice will become clear below.</p><p>This loss function penalizes the difference between the marginalized data <inline-formula><mml:math id="inf129"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> and the reconstructed full data <inline-formula><mml:math id="inf130"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>, i.e., the full data projected with the decoders <inline-formula><mml:math id="inf131"><mml:mi mathvariant="bold">𝐃</mml:mi></mml:math></inline-formula> onto a low-dimensional latent space and then reconstructed with the encoders <inline-formula><mml:math id="inf132"><mml:mi mathvariant="bold">𝐅</mml:mi></mml:math></inline-formula> (see <xref ref-type="other" rid="media2">Video 2</xref>). The loss function thereby favours variance in marginalization <inline-formula><mml:math id="inf133"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> and punishes variance coming from all other marginalizations and from trial-to-trial noise. Given that the marginalized averages are uncorrelated with each other, we can make this observation clear by writing,<disp-formula id="equ13"><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here the first term corresponds to the non-explained variance in marginalization <inline-formula><mml:math id="inf134"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> and the second term corresponds to the variance coming from all other marginalizations and from trial-to-trial noise. The dPCA objective is to minimize both.<media content-type="glencoe play-in-place height-250 width-310" id="media2" mime-subtype="mp4" mimetype="video" xlink:href="elife-10989-media2.mp4"><object-id pub-id-type="doi">10.7554/eLife.10989.013</object-id><label>Video 2.</label><caption><title>Illustration of the dPCA algorithm.</title><p>Illustration of the dPCA algorithm using the somatosensory working memory task.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.013">http://dx.doi.org/10.7554/eLife.10989.013</ext-link></p></caption></media></p><p>We note that the loss function <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>L</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> is of the general form <inline-formula><mml:math id="inf136"><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM121"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐀𝐗</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, with <inline-formula><mml:math id="inf137"><mml:mrow><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐅𝐃</mml:mi></mml:mrow></mml:math></inline-formula>. For an arbitrary <inline-formula><mml:math id="inf138"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf139"><mml:mi mathvariant="bold">𝐀</mml:mi></mml:math></inline-formula>, minimization of the loss function amounts to a classical regression problem with the well-known ordinary least squares (OLS) solution, <inline-formula><mml:math id="inf140"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mmultiscripts><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐗𝐗</mml:mi><mml:mmultiscripts><mml:mo stretchy="false">)</mml:mo><mml:mo>⊤</mml:mo></mml:mmultiscripts></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>⊤</mml:mo></mml:mmultiscripts></mml:mrow></mml:math></inline-formula>. In our case, <inline-formula><mml:math id="inf141"><mml:mrow><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐅𝐃</mml:mi></mml:mrow></mml:math></inline-formula> is an <inline-formula><mml:math id="inf142"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix of rank <inline-formula><mml:math id="inf143"><mml:mi>q</mml:mi></mml:math></inline-formula>, which we will make explicit by writing <inline-formula><mml:math id="inf144"><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:math></inline-formula>. The dPCA loss function therefore amounts to a linear regression problem with an additional rank constraint on the matrix of regression coefficients. This problem is known as <italic>reduced-rank regression (RRR)</italic> (<xref ref-type="bibr" rid="bib18">Izenman, 1975</xref>; <xref ref-type="bibr" rid="bib34">Reinsel and Velu, 1998</xref>; <xref ref-type="bibr" rid="bib19">Izenman, 2008</xref>) and can be solved via the singular value decomposition.</p><p>To see this, we write <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The first term, <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, consists of the regression residuals that cannot be accounted for by any linear transformation of <inline-formula><mml:math id="inf147"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>. It is straightforward to verify that these regression residuals, <inline-formula><mml:math id="inf148"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, are orthogonal to <inline-formula><mml:math id="inf149"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib15">Hastie et al., 2009</xref>, Section 3.2) and hence also orthogonal to <inline-formula><mml:math id="inf150"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM124"><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula>. This orthogonality allows us to split the loss function into two terms,<disp-formula id="equ14"><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where the first term captures the (unavoidable) error of the least squares fit while the second term describes the additional loss suffered through the rank constraint. Since the first term does not depend on <inline-formula><mml:math id="inf151"><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:math></inline-formula>, the problem reduces to minimizing the second term.</p><p>To minimize the second term, we note that the best rank-<inline-formula><mml:math id="inf152"><mml:mi>q</mml:mi></mml:math></inline-formula> approximation to <inline-formula><mml:math id="inf153"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula> is given by its first <inline-formula><mml:math id="inf154"><mml:mi>q</mml:mi></mml:math></inline-formula> principal components (Eckart-Young-Mirsky theorem). Accordingly, if we write <inline-formula><mml:math id="inf155"><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:math></inline-formula> for the matrix of the <inline-formula><mml:math id="inf156"><mml:mi>q</mml:mi></mml:math></inline-formula> leading principal directions (left singular vectors) <inline-formula><mml:math id="inf157"><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of <inline-formula><mml:math id="inf158"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula>, then the best approximation is given by <inline-formula><mml:math id="inf159"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula> and hence <inline-formula><mml:math id="inf160"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To summarize, the reduced-rank regression problem posed above can be solved in a three-step procedure:</p><list list-type="order"><list-item><p>Compute the OLS solution <inline-formula><mml:math id="inf161"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup id="XM128"><mml:mi mathvariant="bold">𝐗𝐗</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Perform PCA of <inline-formula><mml:math id="inf162"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula> and take the <inline-formula><mml:math id="inf163"><mml:mi>q</mml:mi></mml:math></inline-formula> leading principal components to obtain the best low-rank approximation: <inline-formula><mml:math id="inf164"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf165"><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula> matrix of the <inline-formula><mml:math id="inf167"><mml:mi>q</mml:mi></mml:math></inline-formula> leading principal directions (left singular vectors) of <inline-formula><mml:math id="inf168"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Factorize the matrix <inline-formula><mml:math id="inf169"><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:math></inline-formula> into decoder and encoder matrices, <inline-formula><mml:math id="inf170"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐅𝐃</mml:mi></mml:mrow></mml:math></inline-formula>, by choosing <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf172"><mml:mrow><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list><p>Conveniently, the extracted decoder/encoder pairs do not depend on how many pairs are extracted: the <inline-formula><mml:math id="inf173"><mml:mi>i</mml:mi></mml:math></inline-formula>-th pair is given by <inline-formula><mml:math id="inf174"><mml:mrow><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf175"><mml:mrow><mml:mi mathvariant="bold">𝐝</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mi>i</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>OLS</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, independent of <inline-formula><mml:math id="inf176"><mml:mi>q</mml:mi></mml:math></inline-formula>. Indeed, this feature motivated the above choice that <inline-formula><mml:math id="inf177"><mml:mi mathvariant="bold">𝐅</mml:mi></mml:math></inline-formula> should have orthonormal columns.</p></sec><sec id="s4-4"><title>Regularization</title><p>A standard way to avoid overfitting in regression problems is to add a quadratic penalty to the cost function, which is often called ridge regression (RR). This approach can be used in reduced-rank regression as well. Specifically, we can add a ridge penalty term to the loss function <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>L</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ15"><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The RR solution modifies the OLS solution from above to<disp-formula id="equ16"><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In turn, the reduced-rank solution can be obtained as described above: <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>RR</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf181"><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:math></inline-formula> are the first <inline-formula><mml:math id="inf182"><mml:mi>q</mml:mi></mml:math></inline-formula> principal directions of <inline-formula><mml:math id="inf183"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>RR</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>We found it convenient to define <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM133"><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∥</mml:mo><mml:mi id="XM132" mathvariant="bold">𝐗</mml:mi><mml:mo>∥</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, since this makes the values of <inline-formula><mml:math id="inf185"><mml:mi>λ</mml:mi></mml:math></inline-formula> comparable across datasets. As explained below, we used cross-validation to select the optimal value of <inline-formula><mml:math id="inf186"><mml:mi>λ</mml:mi></mml:math></inline-formula> in each dataset.</p></sec><sec id="s4-5"><title>Unbalanced data</title><p>The data and variance decomposition carried out by the marginalization procedure can break down when the dataset is unbalanced, i.e., when the number of data points (trials) differs between conditions. We illustrate this problem with a two-dimensional toy example in <xref ref-type="fig" rid="fig9">Figure 9</xref>. We assume two task parameters (factors), each of which can take only two possible values. The overall mean as well as the interaction term are taken to be zero, so that <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Since the number of trials, <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, depends on the condition, the trial index runs through the values <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. As shown in <xref ref-type="fig" rid="fig9">Figure 9a</xref>, all three terms on the right-hand side exhibit zero correlation between <inline-formula><mml:math id="inf190"><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf191"><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. A balanced dataset with the same number of data points in each of the four possible conditions (<xref ref-type="fig" rid="fig9">Figure 9b</xref>) also has zero correlation. However, an unbalanced dataset, as shown in <xref ref-type="fig" rid="fig9">Figure 9c</xref>, exhibits strong positive correlation (<inline-formula><mml:math id="inf192"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>). Accordingly, the covariance matrix of the full data can no longer be split into marginalized covariances. To avoid this and other related problems, we can perform a 're-balancing' procedure by reformulating dPCA in terms of PSTHs and noise covariance.<fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.014</object-id><label>Figure 9.</label><caption><title>Balanced and unbalanced data.</title><p>(<bold>a</bold>) In this toy example there are two task parameters (factors), with two possible values each. Parameter A (left) is represented by the size of the dot, parameter B (middle) is represented by the color of the dot, noise is Gaussian with zero mean and zero correlation (right). Interaction term is equal to zero. (<bold>b</bold>) Balanced case with <inline-formula><mml:math id="inf193"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> data points in each of the four parameter combinations. Overall correlation is zero. (<bold>c</bold>) Unbalanced case with <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> for two parameter combinations and <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> for the other two. Overall correlation is 0.8.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.014">http://dx.doi.org/10.7554/eLife.10989.014</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig9-v2"/></fig></p><p>In the balanced case, the dPCA loss function <inline-formula><mml:math id="inf196"><mml:msub><mml:mi>L</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> can be rewritten as the sum of two terms with one term depending on the PSTHs and another term depending on the trial-to-trial variations,<disp-formula id="equ17"><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where we used the fact that <inline-formula><mml:math id="inf197"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf198"><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>noise</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are orthogonal to <inline-formula><mml:math id="inf199"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>noise</mml:mi></mml:msub></mml:math></inline-formula> (see Appendix A). We now define <inline-formula><mml:math id="inf200"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>PSTH</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>noise</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> which is simply a matrix of the same size as <inline-formula><mml:math id="inf201"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> with the activity of each trial replaced by the corresponding PSTH. In addition, we observe that the squared norm of any centered data matrix <inline-formula><mml:math id="inf202"><mml:mi mathvariant="bold">𝐘</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf203"><mml:mi>n</mml:mi></mml:math></inline-formula> data points can be written in terms of its covariance matrix <inline-formula><mml:math id="inf204"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐘𝐘</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, namely <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">∥</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mo>∥</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo fence="false" stretchy="false">∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and so<disp-formula id="equ18"><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi>S</mml:mi><mml:mi>Q</mml:mi><mml:mi>T</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The first term consists of <inline-formula><mml:math id="inf206"><mml:mi>K</mml:mi></mml:math></inline-formula> replicated copies: <inline-formula><mml:math id="inf207"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>PSTH</mml:mi></mml:msub></mml:math></inline-formula> contains <inline-formula><mml:math id="inf208"><mml:mi>K</mml:mi></mml:math></inline-formula> replicated copies of <inline-formula><mml:math id="inf209"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> (which we defined above as the matrix of PSTHs) and <inline-formula><mml:math id="inf210"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> contains <inline-formula><mml:math id="inf211"><mml:mi>K</mml:mi></mml:math></inline-formula> replicated copies of <inline-formula><mml:math id="inf212"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> (which we take to be a marginalization of <inline-formula><mml:math id="inf213"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula>, with <inline-formula><mml:math id="inf214"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>ϕ</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>). We can eliminate the replications and drop the factor <inline-formula><mml:math id="inf215"><mml:mi>K</mml:mi></mml:math></inline-formula> to obtain<disp-formula id="equ19"><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>Q</mml:mi><mml:mi>T</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In the unbalanced case, we can directly use this last formulation where all occurrences of <inline-formula><mml:math id="inf216"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> have been replaced by <inline-formula><mml:math id="inf217"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula>. This is especially useful for neural data, where some combinations of task parameters may occur more often than others. The 're-balanced' dPCA loss function treats all parameter combinations as equally important, independent of their occurrence frequency. It stands to reason to 're-balance' the noise covariance matrix as well by defining it as follows:<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>noise</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>noise</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM150">s</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM151">d</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM152">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo maxsize="120%" minsize="120%">⟨</mml:mo><mml:mrow id="XM156"><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>noise</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM153">s</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM154">d</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM155">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf218"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>noise</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM157">s</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM158">d</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM159">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the covariance matrix for the <inline-formula><mml:math id="inf219"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM160">s</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM161">d</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM162">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> parameter combination. This formulation, again, treats noise covariance matrices from different parameter combinations as equally important, independent of how many data points there are for each parameter combination.</p><p>Putting everything together and including the regularization term as well, we arrive at the following form of the dPCA loss function:<disp-formula id="equ21"><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>Q</mml:mi><mml:mi>T</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This loss function can be minimized as described in the previous section. Specifically, the full rank solution with <inline-formula><mml:math id="inf220"><mml:mrow><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐅𝐃</mml:mi></mml:mrow></mml:math></inline-formula> becomes<disp-formula id="equ22"><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>Q</mml:mi><mml:mi>T</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The reduced-rank solution can then be obtained by setting <inline-formula><mml:math id="inf221"><mml:mrow><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf222"><mml:mrow><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐀</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf223"><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:math></inline-formula> are the first <inline-formula><mml:math id="inf224"><mml:mi>q</mml:mi></mml:math></inline-formula> principal directions of <inline-formula><mml:math id="inf225"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mi>RR</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-6"><title>Missing data</title><p>Even when using the re-balanced formulation of the loss function, we still need data from all possible parameter combinations. In neurophysiological experiments, however, one may run into situations where not all combinations of stimuli could be presented to an animal before it decided to abort the task, or where an animal never carried out a particular decision, etc. This problem is particularly severe if individual task parameters can take many values. What should one do in these cases? The key problem here is that dPCA as formulated above makes no assumptions about how the firing rates of individual neurons depend on the task parameters. (Nor is there an explicit assumption about how the demixed components depend on the task parameters.) If some task conditions have not been recorded, then the only way out is to add more assumptions, or, more formally, to replace the non-parametric estimates of individual neural firing rates (or demixed components) by parametric estimates. We could for instance fit a simple linear model to the firing rate of each neuron at each time step (<xref ref-type="bibr" rid="bib25">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib3">Brody et al., 2003</xref>),<disp-formula id="equ23"><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>and then use this model to 'fill in' the missing data. More sophisticated ways of dealing with missing data could be envisaged as well and may provide a venue for future research.</p></sec><sec id="s4-7"><title>Sequentially recorded data</title><p>For sequentially recorded datasets, the matrix <inline-formula><mml:math id="inf226"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> cannot be meaningfully constructed. However, we can still work with the PSTH matrix <inline-formula><mml:math id="inf227"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> that can be decomposed into marginalizations: <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Consequently, we can use the same formulation of the loss function as in the simultaneously recorded unbalanced case (see above). The only difference is that the noise covariance matrix is not available (noise correlations cannot be estimated when neurons are recorded in different sessions). In this manuscript we took as <inline-formula><mml:math id="inf229"><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>noise</mml:mi></mml:msub></mml:math></inline-formula> the diagonal matrix with individual noise variances of each neuron on the diagonal. We used the re-balanced version <inline-formula><mml:math id="inf230"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>noise</mml:mi></mml:msub></mml:math></inline-formula> (average noise covariance matrix across all conditions), but found that the difference between re-balanced and non-rebalanced noise covariance matrices was always minor and did not noticeably influence the dPCA solutions.</p></sec><sec id="s4-8"><title>Variance calculations</title><p>As all datasets analyzed in this manuscript were sequentially recorded, we always reported fractions of the PSTH variance (as opposed to the total PSTH+noise variance) explained by our components, i.e. fractions of variance explained in <inline-formula><mml:math id="inf231"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula>. We defined the fraction of explained variance in a standard way:<disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mover accent="true" id="XM172"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM173"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐅𝐃</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mover accent="true" id="XM174"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This formula can be used to compute the fraction of variance explained by each dPCA component (by plugging in its encoder <inline-formula><mml:math id="inf232"><mml:mi mathvariant="bold">𝐟</mml:mi></mml:math></inline-formula> and decoder <inline-formula><mml:math id="inf233"><mml:mi mathvariant="bold">𝐝</mml:mi></mml:math></inline-formula>); these are the numbers reported on <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6b,d</xref> and used to order the components. The same formula can be used to compute the cumulative fraction of variance explained by the first <inline-formula><mml:math id="inf234"><mml:mi>q</mml:mi></mml:math></inline-formula> components (by stacking their encoders and decoders as columns and rows of <inline-formula><mml:math id="inf235"><mml:mi mathvariant="bold">𝐅</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf236"><mml:mi mathvariant="bold">𝐃</mml:mi></mml:math></inline-formula> respectively); these are the numbers reported on <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6c</xref>. Note that the cumulative explained variance is close to the sum of individually explained variances but not exactly equal to it since the dPCA components are not completely uncorrelated. The same formula holds for standard PCA using <inline-formula><mml:math id="inf237"><mml:mrow><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>pca</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, i.e., the matrix of stacked together principal directions (<xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6c</xref>).</p><p>Using the decomposition <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, we can split the fraction of explained variance into additive contributions from different marginalizations:<disp-formula id="equ25"><mml:math id="m25"><mml:mrow><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>ϕ</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:msub id="XM175"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM176"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐅𝐃</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mover accent="true" id="XM177"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We used this decomposition to produce the bar plots in <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6d</xref>, showing how the explained variance of each single dPCA component is split between marginalizations.</p><p>Following the approach of <xref ref-type="bibr" rid="bib24">Machens et al. (2010)</xref>, we note that our PSTH estimates <inline-formula><mml:math id="inf239"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> must differ from the 'true' underlying PSTHs due to the finite amount of recorded trials. Hence, some fraction of the total variance of <inline-formula><mml:math id="inf240"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> is coming from this residual noise. We can estimate this fraction as follows. Our estimate of the noise variance of the <inline-formula><mml:math id="inf241"><mml:mi>n</mml:mi></mml:math></inline-formula>-th neuron is given by <inline-formula><mml:math id="inf242"><mml:msub><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the <inline-formula><mml:math id="inf243"><mml:mi>n</mml:mi></mml:math></inline-formula>-th diagonal element of <inline-formula><mml:math id="inf244"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>noise</mml:mi></mml:msub></mml:math></inline-formula>. There are on average <inline-formula><mml:math id="inf245"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> trials being averaged to compute the PSTHs for this neuron. So a reasonable estimate of the residual noise variance of the <inline-formula><mml:math id="inf246"><mml:mi>n</mml:mi></mml:math></inline-formula>-th neuron is <inline-formula><mml:math id="inf247"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Accordingly, we define the total residual noise sum of squares as<disp-formula id="equ26"><mml:math id="m26"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In turn, the fraction of total <italic>signal variance</italic> is computed as <inline-formula><mml:math id="inf248"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mover accent="true" id="XM178"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> which is the dashed line shown in <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6c</xref>. Note that each component likewise has contributions from both signal and noise variance, and hence the fraction of total signal variance does not constitute an upper bound on the number of components.</p><p>The residual noise variance is not split uniformly across marginalizations: the fraction falling into marginalization <inline-formula><mml:math id="inf249"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is proportional to the respective number of degrees of freedom, <inline-formula><mml:math id="inf250"><mml:msub><mml:mi>K</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>. This can be explicitly computed; for a centered dataset with <inline-formula><mml:math id="inf251"><mml:mi>S</mml:mi></mml:math></inline-formula> stimuli, <inline-formula><mml:math id="inf252"><mml:mi>Q</mml:mi></mml:math></inline-formula> decisions, and <inline-formula><mml:math id="inf253"><mml:mi>T</mml:mi></mml:math></inline-formula> time points the total number of degrees of freedom (per neuron) is <inline-formula><mml:math id="inf254"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and is split into <inline-formula><mml:math id="inf255"><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for time, <inline-formula><mml:math id="inf256"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> for stimulus, <inline-formula><mml:math id="inf257"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> for decision, and <inline-formula><mml:math id="inf258"><mml:mrow><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> for the stimulus-decision interaction (compare with the formulas in the Marginalization Procedure section). Accordingly, we computed the residual noise sum of squares falling into marginalization <inline-formula><mml:math id="inf259"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> as<disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Θ</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>K</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The pie charts in <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6d</xref> show the amount of variance in each marginalization, with estimated contributions of the residual noise variance subtracted: <inline-formula><mml:math id="inf260"><mml:mrow><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow id="XM181"><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:msub id="XM180"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="normal">Θ</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow><mml:mo mathsize="120%" stretchy="false">/</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow id="XM183"><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mover accent="true" id="XM182"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To display the percentage values on the pie charts, percentages were rounded using the 'largest remainder method', so that the sum of the rounded values remained 100%.</p></sec><sec id="s4-9"><title>Demixing indices</title><p>We defined the <italic>demixing index</italic> of each component as <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo mathvariant="bold">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo mathvariant="bold">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. This index can range from 1/4 to 1 (since there are four marginalizations) and the closer it is to 1, the better demixed the component is. As an example, for the somatosensory working memory dataset, the average demixing index over the first 15 PCA components is 0.76<inline-formula><mml:math id="inf262"> <mml:mo>± </mml:mo></mml:math></inline-formula>0.16 (mean<inline-formula><mml:math id="inf263"> <mml:mo>± </mml:mo></mml:math></inline-formula>SD), and over the first 15 dPCA components is 0.98<inline-formula><mml:math id="inf264"> <mml:mo>± </mml:mo></mml:math></inline-formula>0.02, which means that dPCA achieves much better demixing (<inline-formula><mml:math id="inf265"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0002</mml:mn></mml:mrow></mml:math></inline-formula>, Mann-Whitney-Wilcoxon ranksum test). For the first 15 components of dPCA-2011 (<xref ref-type="bibr" rid="bib2">Brendel et al., 2011</xref>) it was 0.95<inline-formula><mml:math id="inf266"> <mml:mo>± </mml:mo></mml:math></inline-formula>0.03, significantly less than for the current dPCA (<inline-formula><mml:math id="inf267"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0008</mml:mn></mml:mrow></mml:math></inline-formula>). This difference may seem small, but is clearly visible in the projections by the naked eye. For comparison, the average demixing index of individual neurons in this dataset is 0.55<inline-formula><mml:math id="inf268"> <mml:mo>± </mml:mo></mml:math></inline-formula>0.18. In other datasets these numbers are similar, and the same differences were significant in all cases.</p></sec><sec id="s4-10"><title>Angles between dPCs</title><p>In <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6e</xref>, stars mark the pairs of components whose encoding axes <inline-formula><mml:math id="inf269"><mml:msub><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf270"><mml:msub><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> are significantly and robustly non-orthogonal. These were identified as follows: In Euclidean space of <inline-formula><mml:math id="inf271"><mml:mi>N</mml:mi></mml:math></inline-formula> dimensions, two random unit vectors (from a uniform distribution on the unit sphere) have dot product (cosine of the angle between them) distributed with mean zero and standard deviation <inline-formula><mml:math id="inf272"><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula>. For large <inline-formula><mml:math id="inf273"><mml:mi>N</mml:mi></mml:math></inline-formula> the distribution is approximately Gaussian. To avoid the problems inherent to multiple comparisons, we chose a conservative significance level of <inline-formula><mml:math id="inf274"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, which means that two axes are significantly non-orthogonal if <inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>3.3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Coordinates of <inline-formula><mml:math id="inf276"><mml:msub><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> quantify how much this component contributes to the activity of each neuron. Hence, if cells exhibiting one component also tend to exhibit another, the dot product between the axes <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is positive (note that <inline-formula><mml:math id="inf278"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is approximately equal to the correlation between the coordinates of <inline-formula><mml:math id="inf279"><mml:msub><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf280"><mml:msub><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>). Sometimes, however, the dot product has large absolute value only due to several outlying cells. To ease interpretation, we marked with stars only those pairs of axes for which the Kendall (robust) correlation was significant at <inline-formula><mml:math id="inf281"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula> level (in addition to the above criterion on <inline-formula><mml:math id="inf282"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>).</p></sec><sec id="s4-11"><title>Experimental data</title><p>Brief descriptions of experimental paradigms are provided in the Results section and readers are referred to the original publications for all further details. Here we describe the selection of animals, sessions, and trials for the present manuscript. In all experiments neural recordings were obtained in multiple sessions, so most of the neurons were not recorded simultaneously. All four datasets used in this manuscript have been made available at <ext-link ext-link-type="uri" xlink:href="http://crcns.org">http://crcns.org</ext-link> (<xref ref-type="bibr" rid="bib39">Romo et al., 2016</xref>; <xref ref-type="bibr" rid="bib8">Constantinidis et al., 2016</xref>; <xref ref-type="bibr" rid="bib12">Feierstein et al., 2016</xref>; <xref ref-type="bibr" rid="bib44">Uchida et al., 2016</xref>).</p><list list-type="order"><list-item><p>Somatosensory working memory task in monkeys (<xref ref-type="bibr" rid="bib40">Romo et al., 1999</xref>; <xref ref-type="bibr" rid="bib3">Brody et al., 2003</xref>). We used data from two monkeys (code names RR14 and RR15) that were trained with the same set of vibratory frequencies, and we selected only the sessions where all six frequencies {10, 14, 18, 26, 30, 34} Hz were used for the first stimulation (other sessions were excluded). Monkeys made few mistakes (overall error rate was 6%), and here we analyzed only correct trials. Monkey RR15 had an additional 3 s delay after the end of the second stimulation before it was cued to provide the response. Using the data from monkey RR13 (that experienced a different frequency set) led to very similar dPCA components (data not shown).</p></list-item><list-item><p>Visuospatial working memory task in monkeys (<xref ref-type="bibr" rid="bib31">Qi et al., 2011</xref>; <xref ref-type="bibr" rid="bib27">Meyer et al., 2011</xref>; <xref ref-type="bibr" rid="bib32">Qi et al., 2012</xref>). We used the data from two monkeys (code names AD and EL) that were trained with the same spatial task. Monkeys made few mistakes (overall error rate was 8%), and here we analysed only correct trials. The first visual stimulus was presented at 9 possible spatial locations arranged in a 3<inline-formula><mml:math id="inf283"><mml:mo>×</mml:mo></mml:math></inline-formula>3 grid (<xref ref-type="fig" rid="fig4">Figure 4a</xref>); here we excluded all the trials where the first stimulus was presented in the centre position.</p></list-item><list-item><p>Olfactory discrimination task in rats (<xref ref-type="bibr" rid="bib13">Feierstein et al., 2006</xref>). We used the data from all five rats (code names N1, P9, P5, T5, and W1). Some rats were trained with two distinct odors, some with four, some with six, and one rat experienced mixtures of two fixed odors in varying proportions. In all cases each odor was uniquely associated with one of the two available water ports (left/right). Following the original analysis (<xref ref-type="bibr" rid="bib13">Feierstein et al., 2006</xref>), we grouped all odors associated with the left/right reward together as a 'left/right odor'. For most rats, caproic acid and 1-hexanol (shown in <xref ref-type="fig" rid="fig5">Figures 5</xref>–<xref ref-type="fig" rid="fig6">6a</xref>) were used as the left/right odor. We excluded from the analysis all trials that were aborted by rats before reward delivery (or before waiting 0.2 s at the reward port for the error trials).</p></list-item><list-item><p>Olfactory categorization task in rats (<xref ref-type="bibr" rid="bib21">Kepecs et al., 2008</xref>). We used the data from all three rats (code names N1, N48, and N49). Note that recordings from one of the rats (N1) were included in both this and previous datasets; when we excluded it from either of the datasets, the results stayed qualitatively the same (data not shown). We excluded from the analysis all trials that were aborted by rats before reward delivery (or before waiting 0.3 s at the reward port for the error trials).</p></list-item></list></sec><sec id="s4-12"><title>Selection of neurons</title><p>For our analysis, we only selected neurons which had been recorded in each possible condition (combination of parameters), which avoids the missing data problems explained above. Additionally, we required that in each condition there were at least <inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> trials, to reduce the standard error of the mean when averaging over trials, and also for cross-validation purposes. The cutoff was set to <inline-formula><mml:math id="inf285"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>min</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> for both working memory datasets, and to <inline-formula><mml:math id="inf286"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>min</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> for both olfactory datasets (due to less neurons available).</p><p>We have further excluded very few neurons with mean firing rates over 50 Hz, as such neurons can bias the variance-based analysis. Firing rates above 50 Hz were atypical in all datasets (number of excluded neurons for each dataset: 5 / 2 / 1 / 0). This exclusion had a minor positive effect on the components. We did not apply any variance-stabilizing transformations, but if the square-root transformation was applied, the results stayed qualitatively the same (data not shown).</p><p>No other pre-selection of neurons was used. This procedure left 832 neurons (230 / 602 for individual animals, order as above) in the somatosensory working memory dataset, 956 neurons (182 / 774) in the visuospatial working memory dataset, 437 neurons in the olfactory discrimination dataset (166 / 30 / 9 / 106 / 126), and 214 neurons in the olfactory categorization dataset (67 / 38 / 109).</p></sec><sec id="s4-13"><title>Preprocessing of the neural data</title><p>The spike trains were filtered with a Gaussian kernel (<inline-formula><mml:math id="inf287"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> ms) and sampled at 100 Hz to produce single-trial instantaneous firing rates.</p><p>In the visuospatial working memory dataset we identified the preferred location of each neuron as the location that evoked maximum mean firing rate in the 500 ms time period while the first stimulus was shown. The neural tuning was shown before to have a symmetric bell shape (<xref ref-type="bibr" rid="bib31">Qi et al., 2011</xref>; <xref ref-type="bibr" rid="bib27">Meyer et al., 2011</xref>), with each neuron having its own preferred location. We then re-sorted the trials (separately for each neuron) such that only five distinct stimuli were left: preferred location, 45°, 90°, 135°, and 180° away from the preferred location.</p><p>In both olfactory datasets trials were self-paced. Accordingly, trials last different amounts of time, and firing rates cannot simply be averaged over trials. We used the following time warping (re-stretching) procedure to equalize the length of all trials and to align several events of interest (<xref ref-type="fig" rid="fig10">Figure 10</xref>) separately in each dataset. We defined five alignment events: odor poke in, odor poke out, water poke in, reward delivery, and water poke out. First, we aligned all trials on odor poke in (<inline-formula><mml:math id="inf288"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) and computed median times of the four other events <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>…</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (for the time of reward delivery, we took the median over all correct trials). Second, we set <inline-formula><mml:math id="inf290"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> to be the minimal waiting time between water port entry and reward delivery across the whole experiment (<inline-formula><mml:math id="inf291"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> s for the olfactory discrimination task and <inline-formula><mml:math id="inf292"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> s for the olfactory categorization task). Finally, for each trial with instantaneous firing rate <inline-formula><mml:math id="inf293"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM191">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> we set <inline-formula><mml:math id="inf294"><mml:mrow><mml:mrow><mml:msub id="XM192"><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo rspace="4.2pt">,</mml:mo><mml:mi id="XM193">i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, to be the times of alignment events on this particular trial (for error trials we took <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), and stretched <inline-formula><mml:math id="inf296"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM194">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> along the time axis in a piecewise-linear manner to align each <inline-formula><mml:math id="inf297"><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> with the corresponding <inline-formula><mml:math id="inf298"><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>.<fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.015</object-id><label>Figure 10.</label><caption><title>Re-stretching (time warping) procedure.</title><p>We defined several alignment events (such as odour poke in, odour poke out, etc.) and for each trial found the times <inline-formula><mml:math id="inf299"><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of these events. After aligning all trials on <inline-formula><mml:math id="inf300"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (left) we computed median times <inline-formula><mml:math id="inf301"><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> for all other events. Then for each trial we re-stretched the firing rate on each interval <inline-formula><mml:math id="inf302"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub id="XM199"><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM200"><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> to align it with <inline-formula><mml:math id="inf303"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub id="XM201"><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM202"><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> (right). After such re-stretching, all events are aligned and the trials corresponding to one condition can be averaged.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.015">http://dx.doi.org/10.7554/eLife.10989.015</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig10-v2"/></fig></p><p>We made sure that time warping did not introduce any artifacts by considering an alternative procedure, where short (<inline-formula><mml:math id="inf304"> <mml:mo>± </mml:mo></mml:math></inline-formula>450 ms) time intervals around each <inline-formula><mml:math id="inf305"><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> were cut out of each trial and concatenated together; this procedure is similar to the pooling of neural data performed in the original studies (<xref ref-type="bibr" rid="bib13">Feierstein et al., 2006</xref>; <xref ref-type="bibr" rid="bib21">Kepecs et al., 2008</xref>). The dPCA analysis revealed qualitatively similar components (data not shown).</p></sec><sec id="s4-14"><title>Cross-validation to select regularization parameter</title><p>As noted above, we renormalized the regularization parameter <inline-formula><mml:math id="inf306"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM204"><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∥</mml:mo><mml:mi id="XM203" mathvariant="bold">𝐗</mml:mi><mml:mo>∥</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, and then used cross-validation to find the optimal value of <inline-formula><mml:math id="inf307"><mml:mi>λ</mml:mi></mml:math></inline-formula> for each dataset. To separate the data into training and testing sets, we held out one random trial for each neuron in each condition as a set of <inline-formula><mml:math id="inf308"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:math></inline-formula> test 'pseudo-trials' <inline-formula><mml:math id="inf309"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>test</mml:mi></mml:msub></mml:math></inline-formula> (as the neurons were not recorded simultaneously, we do not have recordings of all <inline-formula><mml:math id="inf310"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons in any actual trial). Remaining trials were averaged to form a training set of PSTHs <inline-formula><mml:math id="inf311"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>train</mml:mi></mml:msub></mml:math></inline-formula> and an estimate of the noise covariance matrix <inline-formula><mml:math id="inf312"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mtext>train</mml:mtext></mml:msub></mml:math></inline-formula>. Note that <inline-formula><mml:math id="inf313"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>test</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf314"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>train</mml:mi></mml:msub></mml:math></inline-formula> have the same dimensions.</p><p>We then performed dPCA on <inline-formula><mml:math id="inf315"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>train</mml:mi></mml:msub></mml:math></inline-formula> for various values of <inline-formula><mml:math id="inf316"><mml:mi>λ</mml:mi></mml:math></inline-formula> between <inline-formula><mml:math id="inf317"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf318"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> (on a logarithmic grid). For each <inline-formula><mml:math id="inf319"><mml:mi>λ</mml:mi></mml:math></inline-formula>, we selected ten components in each marginalization (i.e. 40 components in total) to obtain <inline-formula><mml:math id="inf320"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM205">λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf321"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM206">λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and computed the normalized reconstruction error <inline-formula><mml:math id="inf322"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>CV</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM207">λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on the test set (see below). We repeated this procedure ten times for different train-test splittings and averaged the resulting functions <inline-formula><mml:math id="inf323"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>CV</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM208">λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In all cases the average function <inline-formula><mml:math id="inf324"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>CV</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM209">λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> had a clear minimum (<xref ref-type="fig" rid="fig11">Figure 11</xref>) that we selected as the optimal <inline-formula><mml:math id="inf325"><mml:mi>λ</mml:mi></mml:math></inline-formula>. The values of <inline-formula><mml:math id="inf326"><mml:mi>λ</mml:mi></mml:math></inline-formula> selected for each dataset were <inline-formula><mml:math id="inf327"><mml:mrow><mml:mn>2.6</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> / <inline-formula><mml:math id="inf328"><mml:mrow><mml:mn>5.8</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> / <inline-formula><mml:math id="inf329"><mml:mrow><mml:mn>5.8</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> / <inline-formula><mml:math id="inf330"><mml:mrow><mml:mn>5.8</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We also performed the same procedure in each marginalization separately, but in all datasets the optimal values of <inline-formula><mml:math id="inf331"><mml:mi>λ</mml:mi></mml:math></inline-formula> were similar across marginalizations (<xref ref-type="fig" rid="fig11">Figure 11</xref>). We therefore chose to use the same value of <inline-formula><mml:math id="inf332"><mml:mi>λ</mml:mi></mml:math></inline-formula> for all marginalizations.<fig id="fig11" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.016</object-id><label>Figure 11.</label><caption><title>Cross-validation errors depending on the regularization parameter <inline-formula><mml:math id="inf333"><mml:mi>λ</mml:mi></mml:math></inline-formula>.</title><p>Each subplot corresponds to one dataset and shows mean (solid lines) and min/max (boundaries of shaded regions) of the relative cross-validation errors for ten repetitions. Different colors refer to different marginalizations (see legend), the minima are marked by dots. Black color shows all marginalizations together, i.e. <inline-formula><mml:math id="inf334"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>CV</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM215">λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.016">http://dx.doi.org/10.7554/eLife.10989.016</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig11-v2"/></fig></p><p>Interestingly, for all our datasets <inline-formula><mml:math id="inf335"><mml:mrow><mml:msub id="XM211"><mml:mi>min</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow id="XM212"><mml:msub><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>CV</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM210">λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was only slightly smaller than <inline-formula><mml:math id="inf336"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>CV</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn id="XM213">0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, so the regularization term had almost no influence. Presumably, this result stems from our diagonal (and thus non-singular) noise covariance matrices, and therefore does not necessarily hold for simultaneously recorded data.</p><p>To compute <inline-formula><mml:math id="inf337"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>CV</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM216">λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we used <inline-formula><mml:math id="inf338"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>test</mml:mi></mml:msub></mml:math></inline-formula> to predict <inline-formula><mml:math id="inf339"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>train</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ28"><mml:math id="m28"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>CV</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM223">λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>ϕ</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM221"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mi id="XM217">train</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM218">ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM219">λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM220">λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>test</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:msub id="XM222"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>train</mml:mi></mml:msub><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This is the residual training-set variance not explained by the test data. Note that it would not make sense to exchange <inline-formula><mml:math id="inf340"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>test</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf341"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>train</mml:mi></mml:msub></mml:math></inline-formula> in this formula: the decoder and encoder are fitted to the training data, and should only be applied to the test data for the purposes of cross-validation. An alternative approach, in which we predicted the test data rather than the training data, yielded similar results (data not shown).</p></sec><sec id="s4-15"><title>Cross-validation to measure classification accuracy</title><p>We used decoding axis <inline-formula><mml:math id="inf342"><mml:mi mathvariant="bold">𝐝</mml:mi></mml:math></inline-formula> of each dPC in stimulus, decision, and interaction marginalizations as a linear classifier to decode stimulus, decision, or condition respectively. Black lines on <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6b</xref> show time periods of significant classification. A more detailed description follows below.</p><p>We used 100 iterations of stratified Monte Carlo leave-group-out cross-validation, where on each iteration we held out one trial for each neuron in each condition as a set of <inline-formula><mml:math id="inf343"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:math></inline-formula> test 'pseudo-trials' <inline-formula><mml:math id="inf344"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>test</mml:mi></mml:msub></mml:math></inline-formula> and averaged over remaining trials to form a training set <inline-formula><mml:math id="inf345"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>train</mml:mi></mml:msub></mml:math></inline-formula> (see above). After running dPCA on <inline-formula><mml:math id="inf346"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>train</mml:mi></mml:msub></mml:math></inline-formula>, we used decoding axes of the first three stimulus/decision/interaction dPCs as a linear classifier to decode stimulus/decision/condition respectively. Consider e.g. the first stimulus dPC: first, for each stimulus, we computed the mean value of this dPC separately for every time-point. Then we projected each test trial on the corresponding decoding axis and classified it at each time-point according to the closest class mean. The proportion of test trials (out of <inline-formula><mml:math id="inf347"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:math></inline-formula>) classified correctly resulted in a time-dependent classification accuracy, which we averaged over 100 cross-validation iterations. Note that this is a stratified procedure: even though in reality some conditions have many fewer trials than others, here we classify exactly the same number of 'pseudo-trials' per condition. At the same time, as the coordinates of individual data points in each pseudo-trial are pooled from different sessions, the influence of noise correlations on the classification accuracies is neglected, similar to <xref ref-type="bibr" rid="bib26">Meyers et al. (2012)</xref>.</p><p>We then used 100 shuffles to compute the distribution of classification accuracies expected by chance. On each iteration and for each neuron, we shuffled all available trials between conditions, respecting the number of trials per condition (i.e. all <inline-formula><mml:math id="inf348"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> trials were shuffled and then randomly assigned to the conditions such that all values <inline-formula><mml:math id="inf349"><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> stayed the same). Then exactly the same classification procedure as above (with 100 cross-validation iterations) was applied to the shuffled dataset to find mean classification accuracy for the first stimulus, decision, and interaction components. All 100 shuffling iterations resulted in a set of 100 time-dependent accuracies expected by chance.</p><p>The time periods when actual classification accuracy exceeded all 100 shuffled decoding accuracies in at least ten consecutive time bins are marked by black lines on <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig6">6</xref>. Components without any periods of significant classification are not shown. See <xref ref-type="fig" rid="fig12">Figure 12</xref> for classification accuracies in each dataset. The Monte Carlo computations took <inline-formula><mml:math id="inf350"><mml:mo>∼</mml:mo></mml:math></inline-formula>8 hr for each of the larger datasets on a 6 core 3.2 Ghz Intel i7-3930K processor.<fig id="fig12" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.017</object-id><label>Figure 12.</label><caption><title>Cross-validated time-dependent classification accuracies of linear classifiers (black lines) given by the first three stimulus/decision/interaction dPCs (columns) in each dataset (rows).</title><p>Shaded gray regions show distribution of classification accuracies expected by chance as estimated by 100 iterations of shuffling procedure.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.017">http://dx.doi.org/10.7554/eLife.10989.017</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig12-v2"/></fig></p></sec><sec id="s4-16"><title>Implementation of classical approaches (<xref ref-type="fig" rid="fig1">Figure 1</xref>)</title><p>The two-way ANOVA shown in <xref ref-type="fig" rid="fig1">Figure 1c–e</xref> was performed as follows. The two factors were stimulus (with six levels) and decision (with two levels), the interaction term was included, and a separate ANOVA was run for the firing rate of each neuron at each time point. Significance level was set at <inline-formula><mml:math id="inf351"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>. Effect size was defined as partial omega squared with a sign given by the sign of the correlation coefficient between firing rate and the corresponding parameter. It can take values between <inline-formula><mml:math id="inf352"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and 1, with 0 meaning no effect. For one-way ANOVA with a single two-level factor (which is a t-test), it would reduce to the signed <inline-formula><mml:math id="inf353"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> between firing rate and factor level.</p><p>For <xref ref-type="fig" rid="fig1">Figure 1f–g</xref>, we ran linear regressions for the firing rate of each neuron at <inline-formula><mml:math id="inf354"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula> s and <inline-formula><mml:math id="inf355"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>3.75</mml:mn></mml:mrow></mml:math></inline-formula> s, taking F1 stimulus value in Hz as one predictor, decision as another one, and including an interaction effect. Predictors were standardized (so regression coefficients in <xref ref-type="fig" rid="fig1">Figure 1g</xref> are standardized coefficients). The components shown in <xref ref-type="fig" rid="fig1">Figure 1f</xref> were constructed following the 'targeted dimensionality reduction' method presented in <xref ref-type="bibr" rid="bib25">Mante et al. (2013)</xref> (see below for more details).</p><p>To compute the proportion of explained PSTH variance, we arranged the two components (obtained by either method) into a matrix <inline-formula><mml:math id="inf356"><mml:mi mathvariant="bold">𝐙</mml:mi></mml:math></inline-formula> of <inline-formula><mml:math id="inf357"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> size. Both the PSTH data matrix <inline-formula><mml:math id="inf358"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf359"><mml:mi mathvariant="bold">𝐙</mml:mi></mml:math></inline-formula> were centered by subtracting row means. Linear regression was used to find reconstruction weights <inline-formula><mml:math id="inf360"><mml:mrow><mml:mi mathvariant="bold">𝐁</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐙</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup id="XM224"><mml:mi mathvariant="bold">𝐙𝐙</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> minimizing reconstruction error <inline-formula><mml:math id="inf361"><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM225"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐁𝐙</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>. Then the proportion of explained variance was computed as <inline-formula><mml:math id="inf362"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM226"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐁𝐙</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mover accent="true" id="XM227"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The PCA on <xref ref-type="fig" rid="fig1">Figure 1i–k</xref> was done on the centered PSTH data matrix <inline-formula><mml:math id="inf363"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula>. Let its singular value decomposition be <inline-formula><mml:math id="inf364"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐔𝐒𝐕</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. Then each subplot on <xref ref-type="fig" rid="fig1">Figure 1j</xref> is a histogram of elements of one column of <inline-formula><mml:math id="inf365"><mml:mi mathvariant="bold">𝐔</mml:mi></mml:math></inline-formula> and each subplot on <xref ref-type="fig" rid="fig1">Figure 1j</xref> is one column of <inline-formula><mml:math id="inf366"><mml:mi mathvariant="bold">𝐕</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-17"><title>Comparison of dPCA to PCA in each marginalization</title><p>To understand the differences of dPCA with respect to other (demixing) methods, we will make several explicit comparisons. The first method we will consider is performing a series of standard PCAs in each marginalization separately. This procedure can be understood in two ways: after performing PCA on <inline-formula><mml:math id="inf367"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> and obtaining the matrix <inline-formula><mml:math id="inf368"><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> for the <inline-formula><mml:math id="inf369"><mml:mi>k</mml:mi></mml:math></inline-formula> leading principal directions, we can use this matrix to project either the marginalized data or the full data.</p><p>In the first case we obtain the principal components of the corresponding marginalization, <inline-formula><mml:math id="inf370"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. However, while these components provide a particular decomposition or visualization of the data, they do not constitute <italic>readouts</italic> of the neural activity, since they are based on projecting the marginalized data. One particular advantage of the dPCA formulation is that it operates on the raw data, so that the decoders (and encoders) can actually be used on single trials. In turn, the visualization of the data found through dPCA also provides insights into the utility of the respective population code for the brain.</p><p>In the second case we obtain <inline-formula><mml:math id="inf371"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula> components from the full data, so that these components could be obtained by projecting single-trial activities. However, now there is no guarantee that these components will be demixed. For a simple counter-example, consider <xref ref-type="fig" rid="fig13">Figure 13</xref>: the stimulus marginalization <inline-formula><mml:math id="inf372"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> consists of three points (one for each stimulus) located roughly on a horizontal axis, and so the first principal axis of <inline-formula><mml:math id="inf373"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> is roughly horizontal. It is easy to see that the projection of the full data onto this axis will be not only stimulus-, but also time-dependent.<fig id="fig13" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.018</object-id><label>Figure 13.</label><caption><title>Toy example illustrating the pseudo-inverse intuition.</title><p>(<bold>a</bold>) Firing rate trajectories of two neurons for three different stimuli. (<bold>b</bold>) Same data with dPCA decoding and encoding axes. The encoding axes are approximately equivalent to the axes of the principal components in this case.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.018">http://dx.doi.org/10.7554/eLife.10989.018</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig13-v2"/></fig></p><p>Nonetheless, we can obtain a reasonable approximation to the dPCA solution using PCA in each marginalization. Namely, <inline-formula><mml:math id="inf374"><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> can be taken to constitute the encoders <inline-formula><mml:math id="inf375"><mml:msub><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>. In turn, the decoders <inline-formula><mml:math id="inf376"><mml:msub><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> are obtained by a pseudo-inverse <inline-formula><mml:math id="inf377"><mml:mrow><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf378"><mml:mi mathvariant="bold">𝐔</mml:mi></mml:math></inline-formula> is a matrix with <inline-formula><mml:math id="inf379"><mml:mrow><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> columns obtained by joining together all <inline-formula><mml:math id="inf380"><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>. We found that this procedure provides a close approximation of the actual decoder and encoder matrices, provided one chooses a reasonable value of <inline-formula><mml:math id="inf381"><mml:mi>k</mml:mi></mml:math></inline-formula>: choosing <inline-formula><mml:math id="inf382"><mml:mi>k</mml:mi></mml:math></inline-formula> too small results in poor demixing, and choosing <inline-formula><mml:math id="inf383"><mml:mi>k</mml:mi></mml:math></inline-formula> too large results in overfitting. In our datasets, <inline-formula><mml:math id="inf384"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> provides a good trade-off.</p><p>This approximate solution highlights the conditions under which dPCA will work well, i.e., result in well-demixed components that capture most of the variance of the data: the main principal axes of different marginalizations <inline-formula><mml:math id="inf385"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> need to be non-collinear. In other words, principal subspaces of different marginalizations should not overlap.</p></sec><sec id="s4-18"><title>Comparison of dPCA with targeted dimensionality reduction</title><p>Next, we compare dPCA with 'targeted dimensionality reduction' (TDR), the method proposed by <xref ref-type="bibr" rid="bib25">Mante et al. (2013)</xref>. Briefly, the algorithm underlying TDR works as follows:</p><list list-type="order"><list-item><p>Perform PCA of the trial-average neural data <inline-formula><mml:math id="inf386"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> and define a 'denoising' matrix <inline-formula><mml:math id="inf387"><mml:mi mathvariant="bold">𝐊</mml:mi></mml:math></inline-formula> as a linear projector on the space spanned by the leading principal axes. Here we used 20 principal axes: <inline-formula><mml:math id="inf388"><mml:mrow><mml:mi mathvariant="bold">𝐊</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mn>20</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mn>20</mml:mn><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>For each neuron <inline-formula><mml:math id="inf389"><mml:mi>i</mml:mi></mml:math></inline-formula>, regress its firing rate at each time point <inline-formula><mml:math id="inf390"><mml:mi>t</mml:mi></mml:math></inline-formula> on stimulus, decision, and interaction between them:<disp-formula id="equ29"><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf391"><mml:mi>d</mml:mi></mml:math></inline-formula> is any suitable parametrization of decision, e.g. <inline-formula><mml:math id="inf392"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mrow> <mml:mo>± </mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf393"><mml:mi>s</mml:mi></mml:math></inline-formula> is stimulus value (e.g. actual stimulation frequency in the somatosensory working memory task in monkeys).</p></list-item><list-item><p>Take <inline-formula><mml:math id="inf394"><mml:mi>N</mml:mi></mml:math></inline-formula> values of <inline-formula><mml:math id="inf395"><mml:mrow><mml:msubsup><mml:mi>β</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM233">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as defining an <inline-formula><mml:math id="inf396"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional vector <inline-formula><mml:math id="inf397"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM234">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and analogously define <inline-formula><mml:math id="inf398"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM235">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf399"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM236">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf400"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mi>o</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM237">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. <xref ref-type="bibr" rid="bib25">Mante et al. (2013)</xref> did not use the condition-independent term <inline-formula><mml:math id="inf401"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mi>o</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM238">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in their original study, but we treat it here on equal footing.</p></list-item><list-item><p>Project each <inline-formula><mml:math id="inf402"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mi>ϕ</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM239">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> onto the space spanned by the leading PCA axes: <inline-formula><mml:math id="inf403"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐊</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mi>ϕ</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM240">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>For each of the three parameters select one vector <inline-formula><mml:math id="inf404"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mo>*</mml:mo><mml:mi>ϕ</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐊</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mi>ϕ</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup id="XM241"><mml:mi>t</mml:mi><mml:mo>*</mml:mo><mml:mi>ϕ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf405"><mml:msubsup><mml:mi>t</mml:mi><mml:mo>*</mml:mo><mml:mi>ϕ</mml:mi></mml:msubsup></mml:math></inline-formula> is the time point at which the norm of <inline-formula><mml:math id="inf406"><mml:mrow><mml:mi mathvariant="bold">𝐊</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mi>ϕ</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM242">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is maximal.</p></list-item><list-item><p>Finally, stack the obtained vectors to form a matrix <inline-formula><mml:math id="inf407"><mml:mrow><mml:mi mathvariant="bold">𝐁</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow id="XM243"><mml:mpadded width="+2.8pt"><mml:msubsup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mo>*</mml:mo><mml:mi>s</mml:mi></mml:msubsup></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="+2.8pt"><mml:msubsup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mo>*</mml:mo><mml:mi>d</mml:mi></mml:msubsup></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="+2.8pt"><mml:msubsup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:mpadded><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mo>*</mml:mo><mml:mi>o</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and perform QR decomposition <inline-formula><mml:math id="inf408"><mml:mrow><mml:mi mathvariant="bold">𝐁</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐐𝐑</mml:mi></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf409"><mml:mi mathvariant="bold">𝐐</mml:mi></mml:math></inline-formula> orthonormal and <inline-formula><mml:math id="inf410"><mml:mi mathvariant="bold">𝐑</mml:mi></mml:math></inline-formula> upper triangular to find a set of three orthogonal demixing axes (as columns of <inline-formula><mml:math id="inf411"><mml:mi mathvariant="bold">𝐐</mml:mi></mml:math></inline-formula>). If the individual vectors in <inline-formula><mml:math id="inf412"><mml:mi mathvariant="bold">𝐁</mml:mi></mml:math></inline-formula> are far from orthogonal, then the resulting axes will strongly depend on the order of stacking them into the matrix <inline-formula><mml:math id="inf413"><mml:mi mathvariant="bold">𝐁</mml:mi></mml:math></inline-formula>. We found that we obtain best results if we select this order <italic>ad hoc</italic> for each dataset; the orders we used were stimulus <inline-formula><mml:math id="inf414"><mml:mo>→</mml:mo></mml:math></inline-formula> decision <inline-formula><mml:math id="inf415"><mml:mo>→</mml:mo></mml:math></inline-formula> interaction for the somatosensory working memory task, stimulus <inline-formula><mml:math id="inf416"><mml:mo>→</mml:mo></mml:math></inline-formula> interaction <inline-formula><mml:math id="inf417"><mml:mo>→</mml:mo></mml:math></inline-formula> decision for the visuospatial working memory task, and interaction <inline-formula><mml:math id="inf418"><mml:mo>→</mml:mo></mml:math></inline-formula> decision <inline-formula><mml:math id="inf419"><mml:mo>→</mml:mo></mml:math></inline-formula> stimulus for both olfactory tasks. The vector <inline-formula><mml:math id="inf420"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">𝜷</mml:mi><mml:mi>o</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM244">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was always used last.</p></list-item></list><p>We applied TDR to all our datasets and observed that dPCA consistently outperforms it in terms of capturing variance and demixing task parameters. First, unlike dPCA, TDR yields only one component per task parameter. Second, even this component tends to retain more mixed selectivity than the corresponding dPCA component. Some representative components are shown in <xref ref-type="fig" rid="fig14">Figure 14</xref>.<fig id="fig14" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.019</object-id><label>Figure 14.</label><caption><title>Some demixed components as given by three different demixing methods (rows) in various datasets and marginalizations (columns).</title><p>Empty subplots mean that the corresponding method did not find any components. All projections were <inline-formula><mml:math id="inf421"><mml:mi>z</mml:mi></mml:math></inline-formula>-scored to make them of the same scale. Barplots on the right show fractions of variance in each marginalization for each component (stimulus in blue, decision in red, interaction in purple, condition-independent in gray): <inline-formula><mml:math id="inf422"><mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM247"><mml:mi mathvariant="bold">𝐝</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM248"><mml:mi mathvariant="bold">𝐝</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. Barplots consisting of a single colour correspond to perfect demixing.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.019">http://dx.doi.org/10.7554/eLife.10989.019</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig14-v2"/></fig></p></sec><sec id="s4-19"><title>Comparison of dPCA with LDA</title><p>Linear Discriminant Analysis (LDA) is usually understood as a one-way technique: there is only one parameter (class id) associated with each data point, whereas in this manuscript we dealt with three parameters simultaneously. Therefore, LDA in its standard form cannot directly be applied to the demixing problem. We can, however, use the same data and covariance decomposition<disp-formula id="equ30"><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="50pt"/><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>that dPCA is using and construct a separate LDA for each marginalization <inline-formula><mml:math id="inf423"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>. To the best of our knowledge, this framework does not have an established name, so we call it <italic>factorial LDA</italic>.</p><p>Let us first consider the case of finding demixed components for marginalization <inline-formula><mml:math id="inf424"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>. We will denote the remaining part of the data matrix as <inline-formula><mml:math id="inf425"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the remaining part of the covariance matrix as <inline-formula><mml:math id="inf426"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. In turn, the goal of LDA will be to find linear projections that have high variance in <inline-formula><mml:math id="inf427"><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> and low variance in <inline-formula><mml:math id="inf428"><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. In LDA, these matrices are usually called <italic>between-class</italic> and <italic>within-class</italic> covariance matrices (<xref ref-type="bibr" rid="bib15">Hastie et al., 2009</xref>). The standard treatment of LDA is to maximize the multivariate signal-to-noise ratio<disp-formula id="equ31"><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf429"><mml:mi mathvariant="bold">𝐃</mml:mi></mml:math></inline-formula> is the matrix with discriminant axes in rows. The well-known solution is that <inline-formula><mml:math id="inf430"><mml:msub><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mi>LDA</mml:mi></mml:msub></mml:math></inline-formula> is given by the leading eigenvectors of <inline-formula><mml:math id="inf431"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (stacked together as rows), or, equivalently, as eigenvectors of <inline-formula><mml:math id="inf432"><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>More useful for our purposes is the reformulation of LDA as a reduced-rank regression problem (<xref ref-type="bibr" rid="bib19">Izenman, 2008</xref>; <xref ref-type="bibr" rid="bib10">De la Torre, 2012</xref>). When classes are balanced, it can be formulated as<disp-formula id="equ32"><mml:math id="m32"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>LDA</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM254"><mml:msub><mml:mi mathvariant="bold">𝐆</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐅𝐃𝐗</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf433"><mml:msub><mml:mi mathvariant="bold">𝐆</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> is a <italic>class indicator matrix</italic>. This matrix has as many rows as there are possible values of parameter <inline-formula><mml:math id="inf434"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> and specifies which data point is labeled with which parameter value: <inline-formula><mml:math id="inf435"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if the <inline-formula><mml:math id="inf436"><mml:mi>j</mml:mi></mml:math></inline-formula>-th data point belongs to class <inline-formula><mml:math id="inf437"><mml:mi>i</mml:mi></mml:math></inline-formula> (has <inline-formula><mml:math id="inf438"><mml:mi>i</mml:mi></mml:math></inline-formula>-th value of the parameter <inline-formula><mml:math id="inf439"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>) and <inline-formula><mml:math id="inf440"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> otherwise. In the toy example shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, there are three classes with five points each, and so <inline-formula><mml:math id="inf441"><mml:msub><mml:mi mathvariant="bold">𝐆</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> will be a <inline-formula><mml:math id="inf442"><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula> matrix of zeros and ones. In this reformulation of LDA, the main interest is in the decoder matrix <inline-formula><mml:math id="inf443"><mml:mi mathvariant="bold">𝐃</mml:mi></mml:math></inline-formula>, whereas the encoder matrix <inline-formula><mml:math id="inf444"><mml:mi mathvariant="bold">𝐅</mml:mi></mml:math></inline-formula>, which serves to map the low-dimensional representation onto the class indicator matrix, plays only an auxiliary role.</p><p>In contrast, the dPCA loss function is<disp-formula id="equ33"><mml:math id="m33"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>dPCA</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow id="XM255"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐅𝐃𝐗</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf445"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> is the matrix of the same size as <inline-formula><mml:math id="inf446"><mml:msub><mml:mi mathvariant="bold">𝐆</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> with <inline-formula><mml:math id="inf447"><mml:mi>j</mml:mi></mml:math></inline-formula>-th column being the class centroid of the class to which the <inline-formula><mml:math id="inf448"><mml:mi>j</mml:mi></mml:math></inline-formula>-th point belongs. This comparison highlights the difference between the two methods: LDA looks for decoders that allow to reconstruct class identity (as encoded by <inline-formula><mml:math id="inf449"><mml:msub><mml:mi mathvariant="bold">𝐆</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>) whereas dPCA looks for decoders that allow to reconstruct class means (as encoded by <inline-formula><mml:math id="inf450"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>). <xref ref-type="fig" rid="fig2">Figure 2b,f,h</xref> provides a toy example of a situation when these two goals yield very different solutions: the LDA projection separates the three classes better than the dPCA projection, but the dPCA projection preserves the information about the distance between classes.</p><p>Using the explicit solution for reduced-rank regression, one can show that <inline-formula><mml:math id="inf451"><mml:msub><mml:mi>L</mml:mi><mml:mi>LDA</mml:mi></mml:msub></mml:math></inline-formula> does indeed have eigenvectors of <inline-formula><mml:math id="inf452"><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as a solution <inline-formula><mml:math id="inf453"><mml:msub><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mi>LDA</mml:mi></mml:msub></mml:math></inline-formula> for decoder (see Section 8.5.3 in <xref ref-type="bibr" rid="bib19">Izenman, 2008</xref>). Following the similar logic for <inline-formula><mml:math id="inf454"><mml:msub><mml:mi>L</mml:mi><mml:mi>dPCA</mml:mi></mml:msub></mml:math></inline-formula>, one can derive the corresponding expression for the dPCA decoder: <inline-formula><mml:math id="inf455"><mml:msub><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mi>dPCA</mml:mi></mml:msub></mml:math></inline-formula> is given by the eigenvectors of <inline-formula><mml:math id="inf456"><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> (personal communication with Maneesh Sahani).</p><p>A statistical test known as MANOVA can be seen as another possible factorial generalization of LDA. Given the same data and covariance decomposition, MANOVA tests if the effect of <inline-formula><mml:math id="inf457"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is statistically significant by analyzing eigenvalues of <inline-formula><mml:math id="inf458"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>noise</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The eigenvectors of this matrix can in principle serve as decoders, but these projections are optimized to separate the contribution of <inline-formula><mml:math id="inf459"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> from noise, not from the contributions of noise and other parameters. Hence, MANOVA is not the appropriate method for demixing purposes.</p><p>While the toy example of <xref ref-type="fig" rid="fig2">Figure 2</xref> illustrates that dPCA and LDA will in principle have very different solutions, we note that in all datasets considered here factorial LDA and dPCA yielded very similar components. This may reflect several pecularities of the data: for instance, the population activity for different values of the same parameter was spaced rather evenly, and all decisions were binary. Nevertheless, we emphasize that dPCA is better suited for (demixed) dimensionality reduction due to its focus on reconstructing the original data, as explained and discussed in the Results (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></sec><sec id="s4-20"><title>Comparison of dPCA with previous versions</title><p>Demixed PCA as presented here is conceptually based on our previous work. <xref ref-type="bibr" rid="bib24">Machens et al. (2010)</xref> suggested a demixing method called <italic>difference of covariances (DOC)</italic> that can only handle two parameters, e.g. stimulus <inline-formula><mml:math id="inf460"><mml:mi>s</mml:mi></mml:math></inline-formula> and time <inline-formula><mml:math id="inf461"><mml:mi>t</mml:mi></mml:math></inline-formula>. Given PSTHs <inline-formula><mml:math id="inf462"><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM256">s</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM257">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, DOC first constructs stimulus-dependent and time-dependent marginalizations <inline-formula><mml:math id="inf463"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM258">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow id="XM261"><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM259">s</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM260">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf464"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM262">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow id="XM265"><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM263">s</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM264">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and then computes the difference between the stimulus and time covariance matrices <inline-formula><mml:math id="inf465"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow id="XM268"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM266">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM267">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf466"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow id="XM271"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM269">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM270">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ34"><mml:math id="m34"><mml:mrow><mml:mrow><mml:mtext>𝐒</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Eigenvectors of S with maximum (positive) eigenvalues correspond to directions with maximum stimulus variance and minimum decision variance. Vice versa, eigenvectors with minimum (negative) eigenvalues correspond to directions with maximum decision variance and minimum stimulus variance. In the toy example presented in <xref ref-type="fig" rid="fig2">Figure 2</xref> DOC finds the axis that is very close to the first PCA axis of class centroids (which is also very close to the dPCA encoder axis shown on the figure), providing worse demixing than both LDA and dPCA.</p><p>A possible extension of DOC to more than two parameters is described in <xref ref-type="bibr" rid="bib22">Machens (2010)</xref>. Here the PSTHs are assumed to depend on <inline-formula><mml:math id="inf467"><mml:mi>M</mml:mi></mml:math></inline-formula> parameters, and the method constructs <inline-formula><mml:math id="inf468"><mml:mi>M</mml:mi></mml:math></inline-formula> marginalizations by averaging over all parameters except one. The respective covariance matrices <inline-formula><mml:math id="inf469"><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> are then formed as above. The extension of DOC seeks to find the matrix of orthogonal directions <inline-formula><mml:math id="inf470"><mml:mi mathvariant="bold">𝐔</mml:mi></mml:math></inline-formula> such that<disp-formula id="equ35"><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>is maximized subject to <inline-formula><mml:math id="inf471"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐔</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐈</mml:mi></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf472"><mml:mrow><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow id="XM274"><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For <inline-formula><mml:math id="inf473"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> this can be shown to be equivalent to the original DOC. Note that <xref ref-type="bibr" rid="bib22">Machens (2010)</xref> did not address the interaction terms.</p><p>The connection between the current dPCA and the DOC approach can be made more explicit if we consider the full covariance decomposition <inline-formula><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and introduce into the dPCA loss function an additional constraint that both encoder and decoder should be given by the same matrix with orthonormal columns: <inline-formula><mml:math id="inf475"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐃</mml:mi><mml:mi>ϕ</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Then<disp-formula id="equ36"><mml:math id="m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo>∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∼</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where the first equality follows from properties of the decomposition, the second equality from the properties of the orthonormal matrices <inline-formula><mml:math id="inf476"><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>, and the third equality uses the definition of the covariance. This derivation shows that the difference of covariances <inline-formula><mml:math id="inf477"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> emerges from the dPCA loss function if the decoder and encoder are given by the same set of orthogonal axes. However, such axes <inline-formula><mml:math id="inf478"><mml:msub><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> from different marginalizations <inline-formula><mml:math id="inf479"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> will in general not be orthogonal to each other, whereas both DOC and its generalization insisted on orthogonal axes.</p><p>Both the original DOC and its extension ignored interaction terms. <xref ref-type="bibr" rid="bib2">Brendel et al. (2011)</xref> introduced interaction terms and the full covariance splitting <inline-formula><mml:math id="inf480"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as described in this manuscript, and developed a probabilistic dPCA model based on probabilistic PCA (PPCA); to remove ambiguity we call this method dPCA-2011. Similar to PPCA, dPCA-2011 assumes that the data are described by a linear model with Gaussian residuals, i. e.<disp-formula id="equ37"><mml:math id="m37"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐳</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐖𝐳</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>but the prior over the components z is chosen such that the components are sparsely distributed over marginalizations. In other words, the prior is chosen such that those components are favored that have variance in only one marginalization. Under the constraint that decoding directions W are orthogonal, the model can be fit using the expectation-maximization algorithm. However, the probabilistic formulation of <xref ref-type="bibr" rid="bib2">Brendel et al. (2011)</xref> still suffers from the orthogonality constraint. As explained in the Discussion, the orthogonality constraint is too rigid and can prevent successful demixing if parameter subspaces are sufficiently non-orthogonal. Indeed, we applied dPCA-2011 to all our datasets and observed that dPCA-2015 showed better demixing (<xref ref-type="fig" rid="fig14">Figure 14</xref>). Moreover, dPCA-2011 failed to find any decision components in the visuospatial working memory task.</p><p>In addition, the formulation of dPCA in this manuscript is radically simplified compared to <xref ref-type="bibr" rid="bib2">Brendel et al. (2011)</xref>, features an analytic solution and is easier to compare with other linear dimensionality reduction techniques.</p></sec><sec id="s4-21"><title>Appendix A. Mathematical properties of the marginalization procedure</title><p>Above we presented marginalization procedure for three parameters. In order to generalize it for an arbitrary number of parameters, we introduce a more general notation. We denote as <inline-formula><mml:math id="inf481"><mml:mi mathvariant="normal">Ψ</mml:mi></mml:math></inline-formula> the set of parameters (in the previous section <inline-formula><mml:math id="inf482"><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi id="XM309">t</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM310">s</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM311">d</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; note that the trial index is not included into <inline-formula><mml:math id="inf483"><mml:mi mathvariant="normal">Ψ</mml:mi></mml:math></inline-formula>) and write <inline-formula><mml:math id="inf484"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>ψ</mml:mi></mml:msub></mml:math></inline-formula> to denote a decomposition term that depends on a subset of parameters <inline-formula><mml:math id="inf485"><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>⊆</mml:mo><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow></mml:math></inline-formula>. In particular, <inline-formula><mml:math id="inf486"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∅</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In full analogy to the 3-parameter case, each term can be iteratively computed via<disp-formula id="equ38"><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">⟨</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⊂</mml:mo><mml:mi>ψ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>∖</mml:mo><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>∖</mml:mo><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⊂</mml:mo><mml:mi>ψ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf487"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mo id="XM314">⋅</mml:mo><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>∖</mml:mo><mml:mi>ψ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes averaging over all parameters that are not elements of <inline-formula><mml:math id="inf488"><mml:mi>ψ</mml:mi></mml:math></inline-formula> and averaging over the trial index. This equation can be rewritten in a non-iterative way by expanding the sum; this yields the expression with alternating signs that is similar to our ANOVA-style equations above:<disp-formula id="equ39"><mml:math id="m39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-1"><mml:mrow><mml:mtext>(</mml:mtext><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow><mml:mtext>)</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⊆</mml:mo><mml:mi>ψ</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>∖</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∪</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>One can verify that this formula correctly describes the 3-parameter case presented above; the general case can be proven by induction. The noise term is defined via<disp-formula id="equ40"><mml:math id="m40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi mathvariant="normal">∅</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This decomposition has several useful properties. First, the average of any marginalization <inline-formula><mml:math id="inf489"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>ψ</mml:mi></mml:msub></mml:math></inline-formula> over any parameter <inline-formula><mml:math id="inf490"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mi>ψ</mml:mi></mml:mrow></mml:math></inline-formula> is zero. This can be seen from the equation <inline-formula><mml:math id="inf491"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋆</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> because after averaging over <inline-formula><mml:math id="inf492"><mml:mi>γ</mml:mi></mml:math></inline-formula> all terms will split into pairs with opposite signs (indeed, for each <inline-formula><mml:math id="inf493"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∋</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula> there is another <inline-formula><mml:math id="inf494"><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∖</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). Second, all marginalizations are pairwise uncorrelated, i.e. their covariance is zero: <inline-formula><mml:math id="inf495"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow id="XM320"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>ψ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>χ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. This can be seen from equation <inline-formula><mml:math id="inf496"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋆</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> because <inline-formula><mml:math id="inf497"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>ψ</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf498"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>χ</mml:mi></mml:msub></mml:math></inline-formula> both consist of an even number of terms with alternating signs, so their product will also consist of an even number of terms with alternating signs, and after averaging over <inline-formula><mml:math id="inf499"><mml:mi mathvariant="normal">Ψ</mml:mi></mml:math></inline-formula> all terms will become equal to <inline-formula><mml:math id="inf500"><mml:mrow> <mml:mo>± </mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and cancel each other. Third, from the definition of the noise term it follows that any marginalization <inline-formula><mml:math id="inf501"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>ψ</mml:mi></mml:msub></mml:math></inline-formula> is uncorrelated with the noise term: <inline-formula><mml:math id="inf502"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow id="XM321"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>ψ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>noise</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>The fact that all marginalizations and the noise are pairwise uncorrelated allows to segregate the variance of <inline-formula><mml:math id="inf503"><mml:mi>x</mml:mi></mml:math></inline-formula> (here we assume that <inline-formula><mml:math id="inf504"><mml:mi>x</mml:mi></mml:math></inline-formula> is centered, i.e. <inline-formula><mml:math id="inf505"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>):<disp-formula id="equ41"><mml:math id="m41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">⟨</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:munder><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Turning now to the multivariate case, if we replace <inline-formula><mml:math id="inf506"><mml:mi>x</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf507"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, everything remains true but variances should be replaced by covariance matrices:<disp-formula id="equ42"><mml:math id="m42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that in ANOVA literature one usually talks about decomposing <italic>sums of squares</italic> <inline-formula><mml:math id="inf508"><mml:mrow><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and in MANOVA literature about decomposing <italic>scatter matrices</italic> <inline-formula><mml:math id="inf509"><mml:mrow><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐱𝐱</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, because (co)variances of different terms are computed from these sums using different denominators (depending on the corresponding number of degrees of freedom) and do not add up. We do not make this distinction and prefer to talk about decomposing the (co)variance, i.e. all (co)variances here are defined with the same denominator equal to the total number of sample points.</p></sec><sec id="s4-22"><title>Appendix B. Fourier-like components from temporal variations</title><p>Consider the decision components in the somatosensory working memory task, <xref ref-type="fig" rid="fig3">Figure 3</xref>. Here the second and the third components are closely resembling the first and second temporal derivatives of the leading decision component. To illustrate why these components are likely to be artifacts of the underlying sampling process, consider a highly simplified example in which a population of <inline-formula><mml:math id="inf510"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons is encoding a one-dimensional bell-shaped signal <inline-formula><mml:math id="inf511"><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM335">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the population vector <inline-formula><mml:math id="inf512"><mml:mi mathvariant="bold">𝐚</mml:mi></mml:math></inline-formula>, i.e the population response is given by <inline-formula><mml:math id="inf513"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM336">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM337">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. In this case, the population response lies in the one-dimensional subspace spanned by <inline-formula><mml:math id="inf514"><mml:mi mathvariant="bold">𝐚</mml:mi></mml:math></inline-formula> and the covariance matrix has rank one:<disp-formula id="equ43"><mml:math id="m43"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow id="XM340"><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM338">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM339">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐚𝐚</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow id="XM342"><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM341">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Now consider the case in which the neurons are not recorded simultaneously but are pooled from different sessions. In behavioural experiments it is unavoidable that the onset of (self-timed) neural responses will vary by tenths or hundreds of milliseconds. Hence, the individual response <inline-formula><mml:math id="inf515"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM343">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of neuron <inline-formula><mml:math id="inf516"><mml:mi>i</mml:mi></mml:math></inline-formula> will experience a small time-shift <inline-formula><mml:math id="inf517"><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> so that <inline-formula><mml:math id="inf518"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM344">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow id="XM345"><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, see <xref ref-type="fig" rid="fig15">Figure 15</xref> for an example with Gaussian tuning curves. If <inline-formula><mml:math id="inf519"><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is small we can do a Taylor expansion around <inline-formula><mml:math id="inf520"><mml:mi>t</mml:mi></mml:math></inline-formula>,<disp-formula id="equ44"><mml:math id="m44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><fig id="fig15" position="float"><object-id pub-id-type="doi">10.7554/eLife.10989.020</object-id><label>Figure 15.</label><caption><title>Fourier-like artifacts in PCA.</title><p>(Left) In this toy example, single neuron responses are generated from the same underlying Gaussian but are randomly shifted in time. (Right) First three PCA components of the population data. While the leading component resembles the true signal, higher order components look like higher Fourier harmonics. They are artifacts of the jitter in time.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.10989.020">http://dx.doi.org/10.7554/eLife.10989.020</ext-link></p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-10989-fig15-v2"/></fig></p><p>where we neglect higher-order corrections for simplicity, but the extension is straight-forward. Let <inline-formula><mml:math id="inf521"><mml:mi mathvariant="bold-italic">𝝉</mml:mi></mml:math></inline-formula> be the vector of time-shifts of all neurons and let <inline-formula><mml:math id="inf522"><mml:mrow><mml:mi mathvariant="bold">𝐛</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mo>∘</mml:mo><mml:mi mathvariant="bold-italic">𝝉</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> be the element-wise vector product of <inline-formula><mml:math id="inf523"><mml:mi mathvariant="bold">𝐚</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf524"><mml:mi mathvariant="bold-italic">𝝉</mml:mi></mml:math></inline-formula>, i.e. <inline-formula><mml:math id="inf525"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow id="XM350"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mo>∘</mml:mo><mml:mi mathvariant="bold-italic">𝝉</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Then the population response can be written as<disp-formula id="equ45"><mml:math id="m45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Hence, the covariance matrix becomes approximately<disp-formula id="equ46"><mml:math id="m46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where we assumed for simplicity that <inline-formula><mml:math id="inf526"><mml:mrow><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mo>⟂</mml:mo><mml:mi mathvariant="bold">𝐛</mml:mi></mml:mrow></mml:math></inline-formula>. In other words, time-shifts between observations will result in additional PCA components that roughly resemble the temporal derivatives of the source component.</p></sec><sec id="s4-23"><title>Data and code</title><p>The dPCA code is available at <ext-link ext-link-type="uri" xlink:href="http://github.com/machenslab/dPCA">http://github.com/machenslab/dPCA</ext-link> for Matlab and Python. All four datasets used in this manuscript have been made available at <ext-link ext-link-type="uri" xlink:href="http://crcns.org">http://crcns.org</ext-link> (<xref ref-type="bibr" rid="bib39">Romo et al., 2016</xref>; <xref ref-type="bibr" rid="bib8">Constantinidis et al., 2016</xref>; <xref ref-type="bibr" rid="bib12">Feierstein et al., 2016</xref>; <xref ref-type="bibr" rid="bib44">Uchida et al., 2016</xref>). Our preprocessing and the main analysis scripts (Matlab) are available at <ext-link ext-link-type="uri" xlink:href="http://github.com/machenslab/elife2016dpca">http://github.com/machenslab/elife2016dpca</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Bial Foundation, grant #389/14. We thank Matthew Kauffman, Valerio Mante, and Emmanuel Procyk for helpful input along the way, and Maneesh Sahani for a valuable discussion on the connection between dPCA and LDA. We furthermore thank Nuno Calaim for help with creating animations.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf2"><p>NU: Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="conflict" id="conf1"><p>The other authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>DK, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>WB, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>CC, Acquisition of data</p></fn><fn fn-type="con" id="con4"><p>CEF, Acquisition of data</p></fn><fn fn-type="con" id="con5"><p>AK, Acquisition of data</p></fn><fn fn-type="con" id="con6"><p>ZFM, Acquisition of data</p></fn><fn fn-type="con" id="con7"><p>X-LQ, Acquisition of data</p></fn><fn fn-type="con" id="con8"><p>RR, Acquisition of data</p></fn><fn fn-type="con" id="con9"><p>NU, Acquisition of data</p></fn><fn fn-type="con" id="con10"><p>CKM, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neuronal population coding of parametric working memory</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>9424</fpage><lpage>9430</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1875-10.2010</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Brendel</surname><given-names>W</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Demixed principal component analysis</article-title><conf-name>Advances in Neural Information Processing Systems 24</conf-name><fpage>2654</fpage><lpage>2662</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Hernández</surname><given-names>A</given-names></name><name><surname>Zainos</surname><given-names>A</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Timing and neural encoding of somatosensory parametric working memory in macaque prefrontal cortex</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>1196</fpage><lpage>1207</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg100</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buesing</surname><given-names>L</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Learning stable, regularised latent models of neural population dynamics</article-title><source>Network</source><volume>23</volume><fpage>24</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.3109/0954898X.2012.677095</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Buesing</surname><given-names>L</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Spectral learning of linear dynamics from generalised-linear observations with application to neural population data</article-title><conf-name>Advances in Neural Information Processing Systems 25</conf-name><fpage>1682</fpage><lpage>1690</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Christensen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>Plane answers to complex questions: the theory of linear models</chapter-title><source>Springer Science &amp; Business Media</source><pub-id pub-id-type="doi">10.1111/j.1751-5823.2011.00159_25.x</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Foster</surname><given-names>JD</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><volume>487</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature11129</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Constantinidis</surname></name><name><surname>Qi</surname><given-names>XL</given-names></name><name><surname>Meyer</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>Single-neuron spike train recordings from macaque prefrontal cortex during a visual working memory task before and after training</data-title><source>CRCNS.org</source><pub-id pub-id-type="doi">10.6080/K0ZW1HVD</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dimensionality reduction for large-scale neural recordings</article-title><source>Nature Neuroscience</source><volume>17</volume><pub-id pub-id-type="doi">10.1038/nn.3776</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De la Torre</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A least-squares framework for component analysis</article-title><source>Pattern Analysis and Machine Intelligence, IEEE Transactions</source><volume>34</volume><fpage>1041</fpage><lpage>1055</lpage><pub-id pub-id-type="doi">10.1109/tpami.2011.184</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eldar</surname><given-names>E</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The effects of neural gain on attention and learning</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1146</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1038/nn.3428</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Feierstein</surname><given-names>CE</given-names></name><name><surname>Quirk</surname><given-names>MC</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Sosulski</surname><given-names>DL</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>Single-neuron spike train recordings from rat orbitofrontal cortex during an odor discrimination task</data-title><source>CRCNS.org</source><pub-id pub-id-type="doi">10.6080/K0QC01D2</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feierstein</surname><given-names>CE</given-names></name><name><surname>Quirk</surname><given-names>MC</given-names></name><name><surname>Uchida</surname> <given-names>N</given-names></name><name><surname>Sosulski</surname><given-names>DL</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Representation of spatial goals in rat orbitofrontal cortex</article-title><source>Neuron</source><volume>51</volume><fpage>495</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.06.032</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouvêa</surname><given-names>TS</given-names></name><name><surname>Monteiro</surname><given-names>T</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Ongoing behavior predicts perceptual report of interval duration</article-title><source>Frontiers in Neurorobotics</source><volume>8</volume><pub-id pub-id-type="doi">10.3389/fnbot.2014.00010</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>The Elements of Statistical Learning</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/b94608</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hernández</surname><given-names>A</given-names></name><name><surname>Nácher</surname><given-names>V</given-names></name><name><surname>, R</surname></name><name><surname>Zainos</surname><given-names>A</given-names></name><name><surname>Lemus</surname><given-names>L</given-names></name><name><surname>Alvarez</surname><given-names>M</given-names></name><name><surname>Vázquez</surname><given-names>Y</given-names></name><name><surname>Camarillo</surname> <given-names>L</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Decoding a perceptual decision process across cortex</article-title><source>Neuron</source><volume>66</volume><fpage>300</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.03.031</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huijbers</surname><given-names>W</given-names></name><name><surname>Pennartz</surname><given-names>CM</given-names></name><name><surname>Beldzik</surname><given-names>E</given-names></name><name><surname>Domagalik</surname><given-names>A</given-names></name><name><surname>Vinck</surname><given-names>M</given-names></name><name><surname>Hofman</surname><given-names>WF</given-names></name><name><surname>Cabeza</surname> <given-names>R</given-names></name><name><surname>Daselaar</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Respiration phase-locks to fast stimulus presentations: Implications for the interpretation of posterior midline &quot;deactivations&quot;</article-title><source>Human Brain Mapping</source><volume>35</volume><fpage>4932</fpage><lpage>4943</lpage><pub-id pub-id-type="doi">10.1002/hbm.22523</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izenman</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Reduced-rank regression for the multivariate linear model</article-title><source>Journal of Multivariate Analysis</source><volume>5</volume><fpage>248</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.1016/0047-259X(75)90042-1</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Izenman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>Modern Multivariate Statistical Techniques.</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-78189-1</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JK</given-names></name><name><surname>Miller</surname><given-names>P</given-names></name><name><surname>Hernández</surname><given-names>A</given-names></name><name><surname>Zainos</surname><given-names>A</given-names></name><name><surname>Lemus</surname><given-names>L</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Heterogenous population coding of a short-term memory and decision task</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>916</fpage><lpage>929</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2062-09.2010</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Zariwala</surname><given-names>HA</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural correlates, computation and behavioural impact of decision confidence</article-title><source>Nature</source><volume>455</volume><fpage> 227</fpage><lpage> 231</lpage><pub-id pub-id-type="doi">10.1038/nature07200</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machens</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Demixing population activity in higher cortical areas</article-title><source>Frontiers in Computational Neuroscience</source><volume>4</volume><pub-id pub-id-type="doi">10.3389/fncom.2010.00126</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Flexible control of mutual inhibition: A neural model of two-interval discrimination</article-title><source>Science</source><volume>307</volume><fpage>1121</fpage><lpage>1124</lpage><pub-id pub-id-type="doi">10.1126/science.1104171</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional, but not anatomical, separation of &quot;what&quot; and &quot;when&quot; in prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>350</fpage><lpage>360</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3276-09.2010</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname><given-names>EM</given-names></name><name><surname>Qi</surname><given-names>XL</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Incorporation of new information into prefrontal cortical activity after learning working memory tasks</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><volume>109</volume><fpage>4651</fpage><lpage>4656</lpage><pub-id pub-id-type="doi">10.1073/pnas.1201022109</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>T</given-names></name><name><surname>Qi</surname><given-names>XL</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Stimulus selectivity in dorsal and ventral prefrontal cortex after training in working memory tasks</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>6266</fpage><lpage>6276</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6798-10.2011</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagan</surname><given-names>M</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Quantifying the signals contained in heterogeneous neural responses and determining their relationships with task performance</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>1584</fpage><lpage>1598</lpage><pub-id pub-id-type="doi">10.1152/jn.00260.2014</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>IM</given-names></name><name><surname>Meister</surname><given-names>ML</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Encoding and decoding in parietal cortex during sensorimotor decision-making</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1395</fpage><lpage>1403</lpage><pub-id pub-id-type="doi">10.1038/nn.3800</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Pfau</surname><given-names>D</given-names></name><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Robust learning of low-dimensional dynamics from large neural ensembles</article-title><conf-name>Advances in Neural Information Processing Systems 26</conf-name><fpage>2391</fpage><lpage>2399</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qi</surname><given-names>XL</given-names></name><name><surname>Meyer</surname><given-names>T</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Changes in prefrontal neuronal activity after learning to perform a spatial working memory task</article-title><source>Cerebral Cortex</source><volume>21</volume><pub-id pub-id-type="doi">10.1093/cercor/bhr058</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qi</surname><given-names>XL</given-names></name><name><surname>Meyer</surname><given-names>T</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural correlates of a decision variable before learning to perform a match/non-match task</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>6161</fpage><lpage>6169</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6365-11.2012</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raposo</surname><given-names>D</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A category-free neural population supports evolving demands during decision-making</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1784</fpage><lpage>1792</lpage><pub-id pub-id-type="doi">10.1038/nn.3865</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Reinsel</surname><given-names>GC</given-names></name><name><surname>Velu</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Multivariate reduced-rank regression</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4757-2853-8</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Variability in neural activity and behavior</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>211</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.02.013</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>MR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Fusi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rishel</surname><given-names>CA</given-names></name><name><surname>Huang</surname> <given-names>G</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Independent category and spatial encoding in parietal cortex</article-title><source>Neuron</source><volume>77</volume><fpage>969</fpage><lpage>979</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.007</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez</surname><given-names>A</given-names></name><name><surname>Laio</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Machine learning. clustering by fast search and find of density peaks</article-title><source>Science</source><volume>344</volume><fpage> 1492</fpage><lpage>1496</lpage><pub-id pub-id-type="doi">10.1126/science.1242072</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Hernández</surname><given-names>A</given-names></name><name><surname>Lemus</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>Single-neuron spike train recordings from macaque prefrontal cortex during a somatosensory working memory task</data-title><source>CRCNS.org</source><pub-id pub-id-type="doi">10.6080/K0V40S4D</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Hernández</surname><given-names>A</given-names></name><name><surname>Lemus</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neuronal correlates of parametric working memory in the prefrontal cortex</article-title><source>Nature</source><volume>399</volume><fpage>470</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1038/20939</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Salinas</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Flutter discrimination: Neural codes, perception, memory and decision making</article-title><source>Nature Reviews. Neuroscience</source><volume>4</volume><fpage> 203</fpage><lpage> 218</lpage><pub-id pub-id-type="doi">10.1038/nrn1058</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rutherford</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Introducing Anova and Ancova: A Glm Approach</source><publisher-name>Sage</publisher-name></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sornborger</surname><given-names>A</given-names></name><name><surname>Yokoo</surname><given-names>T</given-names></name><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Sailstad</surname><given-names>C</given-names></name><name><surname>Sirovich</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Extraction of the average and differential dynamical response in stimulus-locked experimental data</article-title><source>Journal of Neuroscience Methods</source><volume>141</volume><fpage>223</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2004.06.012</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Zariwala</surname><given-names>HA</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>Single-neuron spike train recordings from rat orbitofrontal cortex during an odor classification task</data-title><source>CRCNS.org</source><pub-id pub-id-type="doi">10.6080/K0FT8HZ2</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Speed and accuracy of olfactory discrimination in the rat</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>1224</fpage><lpage>1229</lpage><pub-id pub-id-type="doi">10.1038/nn1142</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>AY</given-names></name><name><surname>Miura</surname> <given-names>K</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The dorsomedial striatum encodes net expected return, critical for energizing performance vigor</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>639</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1038/nn.3377</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wohrer</surname><given-names>A</given-names></name><name><surname>Humphries</surname><given-names>MD</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Population-wide distributions of neural activity during perceptual decision-making</article-title><source>Progress in Neurobiology</source><volume>103</volume><fpage>156</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2012.09.004</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Cunningham</surname><given-names>J</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>614</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1152/jn.90941.2008</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.10989.021</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>van Rossum</surname><given-names>Mark CW</given-names></name><role>Reviewing editor</role><aff id="aff10"><institution>University of Edinburgh</institution>, <country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your work entitled &quot;Demixed principal component analysis of neural population data&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Eve Marder as the Senior Editor.</p><p>The reviewers have discussed the reviews with one another and the Reviewing editor has drafted this decision to help you prepare a revised submission.</p><p>Essential revisions:</p><p>As you will see from the individual reports below, there were rather divergent views on your manuscript. While we are not asking for additional analysis, we are asking for editorial revisions to make the method and its novelty more apparent.</p><p>We are asking for a major rewrite that 1) defines the method better 2) clarifies the difference and similarities with earlier dPCA approaches, 3) the interpretation of the results. Being a methods paper we feel that the presentation is particularly important. Also publication/public presentation of the code is a necessary requirement for <italic>eLife</italic> publication (this can be any of a number of public and maintained sites).</p><p>We are treating this as a Tools and Resources paper, as it is really a data analysis Methods paper, and should have been submitted under the TR heading.</p><p>Reviewer #1:</p><p>This paper is about an algorithm to analyze neural activity in response to experimental manipulations of various factors such as time, stimulus property, and behavioral response. There is a new technique proposed and discussed in the context of previous methods. The results of the different methods do not look all that different.</p><p>The main issue I have with the paper is that the method is described 3 times in 3 separate sections (Results, Methods, supplement), with increasing level of detail, but seemingly inconsistently in concept and notation. It is therefore very hard to make sense of it all.</p><p>At the end of the day it seems that the novel idea advocated here is rather simple: find a set of projections of the raw data which explain most of the variance in the mean response (mean over all trials and factors while holding one factor fixed). This in turn seems to be simply solved, for each factor separately, by an existing algorithm known as &quot;reduced rank regression&quot;.</p><p>Complicating the description, the authors seem to have at least 3 algorithms which they call dPCA. First, one they proposed in 2011 under that exact title. Then, a second, apparently the new one, which is based on a cost function that was not entirely clear in the first reading in the Results section, but became clear only after reading the supplement. There is a third &quot;approximate&quot; version suggested, which concatenates the eigenvectors of the covariance for each mean response. (Is that what is called Naive demixing (<xref ref-type="fig" rid="fig12">Figure 12</xref>)?) They seem to argue at one point that this approximate method gives similar results to the superior new technique.</p><p>The methods seem to have all in common in their goal of explaining the covariance of the mean responses (mean across the factors, and at times, mean across trials) with a few dimensions. There is much talk about an additive model similar to what is used in conventional MANOVA, where the total data is explained as a sum of means across various combinations of factors. Though I don't see that this is necessary to motivate the dimensionality reduction proposed here, it does lead to comparison with yet one more technique. With the current presentation, I find it hard to keep it all straight.</p><p>In <xref ref-type="fig" rid="fig12">Figure 12</xref> the various methods are then finally compared to each other, but as stated below, the performance metric presented there left me confused, just when I thought I knew what is going on. Importantly, the traces presented there don't seem all that different to each other. So then, what is the main new contribution of this paper?</p><p>Reviewer #2:</p><p>This manuscript gives a discursive presentation of how a new version of remixed principal component analysis (dPCA-2015) may be used to analyze multivariate neural data. The presentation is, I believe, complete enough that a user could reconstruct, from zero, the analysis method and the particulars of data pre-processing, etc. It is well-written and logical. The method itself is a nice compromise between a principal component approach to data analysis and a MANOVA-like approach. In particular, components are easily visualizable and the manuscript figures do a nice job of showing, at a glance, the results from fairly sophisticated experiments.</p><p>Other than a number of small textual changes that I would suggest, and one reference, I think this manuscript is in publishable form.</p><p>Reviewer #3:</p><p>This paper describes a statistical method, demixed-PCA, for finding low-dimensional &quot;interpretable&quot; structure in trial based neural recordings. In many cases, neurons exhibit &quot;mixed selectivity&quot;, meaning that they exhibit tuning to multiple experimental variables (e.g., the visual stimulus and the upcoming decision). dPCA differs from PCA in that it tries to find a low-dimensional projection that separates ('demixes') the tuning to different kinds of variables. The paper applies dPCA to four different datasets from monkey PFC and rat OFC, and shows that it recovers structure consistent with previously reported findings using these datasets.</p><p>The paper is interesting and this technique is likely to be of considerable interest and importance to the field, particularly given recent interest in mixed selectivities in different brain areas. However, I have some concerns about novelty and about the intended contribution of this paper. A similar demixing approach has been described previously in Machens 2010 and Brendel 2011 (which was a NIPS paper, so perhaps shouldn't be counted against the novelty of this one since it's only a conference proceedings paper). But it would be helpful to describe a little bit more clearly the intended contribution. Is this just the journal-length writeup of the NIPS paper? How does the method described differ from that in the 2010 Frontiers paper?</p><p>It would also be nice to spell out a little bit more clearly what scientific insights the method is likely to provide (or indeed, provides for the datasets being analyzed here). It seemed that for each dataset, the paper mostly says that dPCA confirmed the findings in the original paper. But if that's the case, why do we need dPCA? What can we learn by analyzing data with this method that we didn't already have access to using the analysis methods from those papers?</p><p>I have two other high-level comments:</p><p>1) I think the authors don't do enough to describe how they decided which components to include when setting up the model. Presumably this is a choice made before running the method, i.e., how to map the elements of the experiment onto discrete conditions. How did the authors solve this problem, and how should potential users go about solving it when applying the method to new datasets. For example, why don't we have &quot;reward&quot; components for any of the datasets considered here? How did you decide which interaction terms to include in the first place? In the olfactory data, did you include different components for each mixture or just one for each mixing component? What are the keys to setting up the analysis? Are there any exploratory data analysis techniques that the authors used to make these decisions?</p><p>2) For a methods paper like this, providing code should be a mandatory requirement for publication. Please say something about where to find an implementation, so that would be users can try it out.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Demixed principal component analysis of neural population data&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Eve Marder (Senior editor), a Reviewing editor, and three reviewers. The manuscript has been greatly improved but there are some smaller remaining issues outlined below.</p><p>Reviewer #1 (General assessment and major comments (Required)):</p><p>This manuscript is like night and day compared to the previous version. I barely recognize the material any more. It is so much clearer that it makes me much more confident that this is all sound and well. I can hardly believe that these were the same authors.</p><p>Reviewer #1 (Minor Comments):</p><p>A few places where I got stuck were for example:</p><p>Equation 4 in subsection “Core dPCA: loss function and algorithm”, why can one separate the quadratic term like this? Also not clear why stefs 1-3 minimize this cost function, but I trust that all this is in the cited paper, though I could only retrieve the Izenman (1975) paper, which looked like yet more work to answer my questions.</p><p>Similarly, second equation in subsection “Unbalanced data”, why can one separate the square into two terms? and why can one replace X<sub>noise</sub> by C<sup>1/2</sup><sub>noise</sub>? and why is X<sub>PSTH</sub>=X tilde?</p><p>In general I have to say that, while the issue of balancing the data is intuitively clear, the corresponding math seems cumbersome, but again, this may be my lack of time.</p><p>Reviewer #2 (General assessment and major comments (Required)):</p><p>The revised manuscript is acceptable for publication.</p><p>Reviewer #3 (General assessment and major comments (Required)):</p><p>I thank the author for the detailed reply to the original review comments, and the revised manuscript is substantially improved. I have a few lingering technical comments and questions, but overall I think the paper is now suitable for publication.</p><p>Reviewer #3 (Minor Comments):</p><p><xref ref-type="fig" rid="fig1">Figure 1K</xref>: not entirely clear what principal components these are, i.e., why are there multiple traces per component? I guess this is because the authors have taken the principal components of the rows of the X matrix (i.e., each of which contains the PSTH of a single neuron for all components). This is a slightly odd choice: I would have thought one would take PCA of the PSTHs, which is closer in spirit to the dPCA and other methods discussed here, so that each component is just a length-T vector (i.e., where T is the number of time bins in the PSTH). I guess this is ok as is, but you should add something to the text or the caption to clarify what these are (e.g., each principal component is a PSTH across all conditions). This seems a little bit confusing, however, because the dPCA components you'll show later aren't the same size. (Or am I misinterpreting the figure, and what you're showing is instead the projection from all conditions on each PC?)</p><p>Indeed, the two components from <xref ref-type="fig" rid="fig1">Figure 1e</xref> explain only 23% of the total variance of the population firing rates and the two components from <xref ref-type="fig" rid="fig1">Figure 1h</xref> explain only 22% (see Methods). Consequently, a naive observer would not be able to infer from the components what the original neural activities looked like.</p><p>This seems a little bit uncharitable to the Mante et al. paper. After all, the regression model used by those authors did indeed have an &quot;untuned component&quot; for each neuron (referred to as Β-0), they simply didn't do anything with these components when it came time to construct a low-d projection that captured information about the stimulus and decision variables. But one could certainly have performed a dimensionality reduction of the Β_0's if one were interested in capturing information about time. So, in my view, while this passage is technically correct, I would encourage the authors to rephrase it to be slightly less disparaging to Mante et al. They simply weren't interested in the coding of time, so the fact that they don't capture those components is a choice about what variables to include more than a fundamental limitation of the method.</p><p>&quot;where averaging takes place over all irrelevant parameters.&quot; A bit unclear – could use an extra sentence unpacking what this means.</p><p>&quot;The overall variance explained by the dPCA components (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, red line) is very close to the overall variance explained by the PCA components (black line).&quot;: Why does PCA only get 80% of the variance? Is this because you've determined that the last 20% belongs to the noise component?</p><p><xref ref-type="fig" rid="fig3">Figure 3</xref> caption: &quot;Thick black lines show time intervals during which the respective task parameters can be reliably extracted from single-trial activity&quot;. Does this mean a pseudotrial with 832 neurons? Start of this section mentions the # of neurons but I think it's important to say it here in the Figure caption what this means. (Presumably, if you have enough neurons recorded then single-trial decoding becomes perfect?)</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.10989.022</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>Essential revisions:</p><p>As you will see from the individual reports below, there were rather divergent views on your manuscript. While we are not asking for additional analysis, we are asking for editorial revisions to make the method and its novelty more apparent.</p><p>We thank all the reviewers and the editors for their careful reading of our manuscript. The comments and suggestions were very helpful and have guided our efforts to clarify the manuscript and to improve the presentation. We provide a detailed rebuttal below.</p><p>Large portions of the text have been rewritten and rearranged, for that reason we found it difficult to provide a version of the manuscript with all changes tracked. The parts that are completely rewritten are (i) the presentation of dPCA method in the Results, (ii) the Methods. Please see the new manuscript file.</p><p>We are asking for a major rewrite that 1) defines the method better 2) clarifies the difference and similarities with earlier dPCA approaches, 3) the interpretation of the results. Being a methods paper we feel that the presentation is particularly important.</p><p>We rearranged, clarified and streamlined the presentation throughout the paper. To address point (1), we expanded and completely rewrote the presentation of the dPCA method in the Results section and also changed the accompanying figure. Furthermore, we merged the Supplementary Information with the Methods section, which now contains all the mathematical details. To address point (2), we now address these differences explicitly in both Introduction and Discussion. More importantly, we included a new section in the Methods that provides a detailed comparison between our older approaches and the current paper. To address point (3), we have clarified the description and layout of the figures taking into account all of the reviewers' comments. See below for more specific replies.</p><p>Also publication/public presentation of the code is a necessary requirement for eLife publication (this can be any of a number of public and maintained sites).</p><p>We fully support this policy. Indeed, we already published our code on github prior to the original submission. This was stated at the end of our Introduction.</p><p>“The dPCA code is available at <ext-link ext-link-type="uri" xlink:href="http://github.com/wielandbrendel/dPCA">http://github.com/wielandbrendel/dPCA</ext-link> for Matlab and Python.”</p><p>and we now re-iterate this information in the Methods section.</p><p>We are treating this as a Tools and Resources paper, as it is really a data analysis Methods paper, and should have been submitted under the TR heading.</p><p>We were not aware that <italic>eLife</italic> has a Tools and Resources section. We agree that this is where our manuscript belongs the best.</p><p>Reviewer #1:</p><p>This is a partial review only. I did not manage to understand all the pieces of this manuscript, and feel that without a few answers from the Authors, and a more streamlined presentation, I will struggle to provide a complete review. So I will list what I did and did not understand so far.</p><p>We thank the Reviewer for a thorough reading of our paper and for a lot of detailed comments. Based on the issues raised by the Reviewer, we have made a number of major changes in how we present the method and structure the text, see below.</p><p>This paper is about an algorithm to analyze neural activity in response to experimental manipulations of various factors such as time, stimulus property, and behavioral response. There is a new technique proposed and discussed in the context of previous methods. The results of the different methods do not look all that different.</p><p>We believe that this “lack of difference&quot; is a misunderstanding that was caused by a lack of clarity from our side. Indeed, dPCA should mainly be compared with the methods presented in our <xref ref-type="fig" rid="fig1">Figure 1, i</xref>.e. with the methods that are actually being used in neuroscience to analyze and visualize neurophysiological recordings. The difference between dPCA and all these methods is obvious.</p><p>In contrast, the “previous methods&quot; that the Reviewer mentions here probably refers to our original <xref ref-type="fig" rid="fig12">Figure 12</xref>, which compared our own preliminary approach dPCA-2011 (Brendel et al. 2011) to a variety of ad hoc demixing methods (such as MANOVA-based demixing&quot;, “naive demixing&quot;, etc.). However, none of these methods actually exist in the literature; rather, they were being presented here as alternative ways of doing something similar to dPCA with different, if sometimes subtle, trade-offs.</p><p>Our preliminary approach, dPCA-2011, was published as a conference proceedings paper, and is now substantially reformulated and improved (we extended our description in the Discussion of what are the differences and included a new section in the Methods); we do not claim that the difference between dPCA-2011 and dPCA-2015 is enormous, but we do argue that the improvement is quite noticeable and that the method, in its updated form and with an application to a variety of neural datasets, deserves a detailed journal-length treatment. Apart from changes in the method, we now provide an in-depth analysis of several experimental datasets, make a comparison between them, and clarify the relationship between dPCA and related methods such as LDA or the method of Mante et al. 2013. (Please see our largely updated presentation of LDA/dPCA in the Results and also the updated Discussion and Methods.)</p><p>All other approaches are ad hoc demixing methods; none of them has ever been used or even suggested for the analysis of neural data. We now see that what used to be our <xref ref-type="fig" rid="fig12">Figure 12</xref> only caused confusion, so we removed all mentions of MANOVA-based demixing&quot;,”naive demixing&quot;, etc. as these methods do not exist in the literature. See below.</p><p>The main issue I have with the paper is that the method is described 3 times in 3 separate sections (Results, Methods, supplement), with increasing level of detail, but seemingly inconsistently in concept and notation. It is therefore very hard to make sense of it all.</p><p>The reviewer has a point. We undertook the following measures in order to address this issue:</p><p>1. We have merged the Supplementary Information with the Materials &amp; Methods section.</p><p>2. We have largely expanded the presentation of dPCA in the Results, providing more geometrical intuitions, so that the main concepts are clearer.</p><p>3. We have reformatted our whole manuscript in LATEX which allowed us to use a consistent notation throughout the text.</p><p>The changes are too numerous to copy here entirely.</p><p>At the end of the day it seems that the novel idea advocated here is rather simple: find a set of projections of the raw data which explain most of the variance in the mean response (mean over all trials and factors while holding one factor fixed). This in turn seems to be simply solved, for each factor separately, by an existing algorithm known as &quot;reduced rank regression&quot;.</p><p>That is correct. In fact, we are quite happy that the method has such a simple formulation and solution; we consider it a strength and not a weakness. Note that most of the related linear dimensionality reduction methods such as PCA, LDA, CCA, etc. can be formulated and solved just as easily.</p><p>Complicating the description, the authors seem to have at least 3 algorithms which they call dPCA. First, one they proposed in 2011 under that exact title. Then, a second, apparently the new one, which is based on a cost function that was not entirely clear in the first reading in the Results section, but became clear only after reading the supplement. There is a third &quot;approximate&quot; version suggested, which concatenates the eigenvectors of the covariance for each mean response. (Is that what is called Naive demixing (<xref ref-type="fig" rid="fig12">Figure 12</xref>)?) They seem to argue at one point that this approximate method gives similar results to the superior new technique.</p><p>It is true that the method published in 2011 in the NIPS conference proceedings has the same name; we decided that in the long run it will be less confusing if we keep using the name “dPCA&quot; instead of inventing some other name. The core idea of the dPCA framework is to find linear projections of the full data that capture most of the variance and are demixed; dPCA-2011 and dPCA-2015 are two algorithmic approaches to solve this same task. DPCA-2015 supersedes the approach of 2011.</p><p>Note that this is not an uncommon situation; e.g. independent component analysis (ICA) is a name that encompasses a variety of related methods, and the same is true for sparse PCA or factor analysis (FA). In both cases there are various suggested implementations, but all of them are united by their goal.</p><p>We did not intend to advocate what we called “naive demixing&quot; as a “third version&quot;. Rather, we intended to briefly describe this procedure in order to provide additional intuitions about dPCA and to explain a common misperception that we have encountered when talking to experimentalists. Since this is a minor technical point, we have downgraded the discussion of this “version&quot; by eliminating it from the comparison figure, and by discussing it only briefly in a separate section in the methods (see section “Comparison of dPCA to PCA in each marginalization&quot;).</p><p>The methods seem to have all in common in their goal of explaining the covariance of the mean responses (mean across the factors, and at times, mean across trials) with a few dimensions. There is much talk about an additive model similar to what is used in conventional MANOVA, where the total data is explained as a sum of means across various combinations of factors. Though I don't see that this is necessary to motivate the dimensionality reduction proposed here, it does lead to comparison with yet one more technique. With the current presentation, I find it hard to keep it all straight.</p><p>We agree with the reviewer that the multitude of comparisons with other “methods&quot;, most of which do not even exist, was, with hindsight, more confusing than helpful. Essentially, all these comparisons were driven by conversations with e.g. someone claiming that what we are doing has been done by and is not different from e.g. MANOVA, so we felt we need to explain the difference. We have downgraded most of these discussions to just highlight the technical differences without giving each “ad hoc&quot; method the prominence it had in the original submission. To straighten the reference to MANOVA, we have rewritten both the Discussion and Methods section about LDA/MANOVA.</p><p>In <xref ref-type="fig" rid="fig12">Figure 12</xref> the various methods are then finally compared to each other, but as stated below, the performance metric presented there left me confused, just when I thought I knew what is going on. Importantly, the traces presented there don't seem all that different to each other. So then, what is the main new contribution of this paper?</p><p>We see the referee's point and we agree that <xref ref-type="fig" rid="fig12">Figure 12</xref> was quite unfortunate. First, we reemphasize that two of the “methods&quot; presented in <xref ref-type="fig" rid="fig12">Figure 12</xref> were just ad hoc methods invented by us to contrast against dPCA (e.g. naive demixing or MANOVA-based demixing). Since this is indeed confusing, we eliminated these methods from the plot, leaving the only two published methods, dPCA- 2011 and targeted dimensionality reduction by Mante et al., 2013 (<xref ref-type="fig" rid="fig14">Figure 14</xref> in the revised version). Second, we notice that there are differences between the remaining methods, even though they may appear subtle. dPCA-2011 fails to find one component; the method of Mante et al. does not produce any condition-independent components. The differences are not enormous, but noticeable and consistent.</p><p>It's especially strong where the encoding subspaces are not orthogonal. Third, we note that applying these methods to data is also not necessarily the best way of showing how they differ – too much depends on the specifics of the data set. To address these problems, we completely rewrote our explanation of dPCA and added additional geometric intutions that show how dPCA differs from PCA and LDA. We hope that these changes clarify the contributions of dPCA-2015: a method that is far simpler and more direct than any of the existing (or ad hoc) methods, and that achieves overall better demixing, although, as the reviewer noticed, the differences on real data are not necessarily huge.</p><p>Concerning the performance metric, we compare all methods using linear projections of the full data and the variance is the variance of this projection.</p><p>Reviewer #2:</p><p>I am not an experimentalist, so this review will have nothing to say about the details of the experimental design, etc. I was not able to view the video(s) with my QuickTime viewer, so I also have nothing to say about the video(s).</p><p>This manuscript gives a discursive presentation of how a new version of remixed principal component analysis (dPCA-2015) may be used to analyze multivariate neural data. The presentation is, I believe, complete enough that a user could reconstruct, from zero, the analysis method and the particulars of data pre-processing, etc. It is well-written and logical. The method itself is a nice compromise between a principal component approach to data analysis and a MANOVA-like approach. In particular, components are easily visualizable and the manuscript figures do a nice job of showing, at a glance, the results from fairly sophisticated experiments.</p><p>Other than a number of small textual changes that I would suggest, and one reference, I think this manuscript is in publishable form.</p><p>We thank the reviewer for these comments.</p><p>Reviewer #3:</p><p>This paper describes a statistical method, demixed-PCA, for finding low-dimensional &quot;interpretable&quot; structure in trial based neural recordings. In many cases, neurons exhibit &quot;mixed selectivity&quot;, meaning that they exhibit tuning to multiple experimental variables (e.g., the visual stimulus and the upcoming decision). dPCA differs from PCA in that it tries to find a low-dimensional projection that separates ('demixes') the tuning to different kinds of variables. The paper applies dPCA to four different datasets from monkey PFC and rat OFC, and shows that it recovers structure consistent with previously reported findings using these datasets.</p><p>The paper is interesting and this technique is likely to be of considerable interest and importance to the field, particularly given recent interest in mixed selectivities in different brain areas.</p><p>We thank the Reviewer for these comments.</p><p>However, I have some concerns about novelty and about the intended contribution of this paper. A similar demixing approach has been described previously in Machens 2010 and Brendel 2011 (which was a NIPS paper, so perhaps shouldn't be counted against the novelty of this one since it's only a conference proceedings paper). But it would be helpful to describe a little bit more clearly what the intended contribution. Is this just the journal-length writeup of the NIPS paper? How does the method described differ from that in the 2010 Frontiers paper?</p><p>We understand that this history of publications makes it look as if we are trying to publish the same method over and over again. That is not the case. Rather, we see the current paper as the culmination of our efforts in developing demixing dimensionality reduction. It is similar in spirit to the previous papers, but technically very different. Briefly, here is our reasoning: Machens et al. 2010 (JNeurosci) introduced a “difference of covariances&quot; method that can work with only two parameters, and has therefore rather limited applicability. Machens 2010 (Frontiers) papers offered one way to generalize that method to more parameters, but only in some specific cases; and it did not properly capture interaction terms. Indeed, this latter problem made its application to real data rather unattractive (unpublished observations). Finally, the Brendel et al. 2011 NIPS paper suggested a fully general way of splitting the data matrix and the covariance matrix into additive parts and then offered a probabilistic model to find the demixing projections. The latter paper also introduced the name “demixed principal component analysis&quot;.</p><p>The NIPS paper was a purely technical paper, and, back then, for us, the end of method development. At this point, we knew that dPCA worked very nicely on the Romo data, which is only briefly discussed within the NIPS paper. However, for a journal-length writeup, we intended to go beyond the Romo data to demonstrate the applicability of dPCA-2011 on a wider range of data sets. Alas, when we applied dPCA-2011 to other datasets, we saw that the demixing was not as good as in the Romo data. Further investigation of this problem showed that the orthogonality constraint of dPCA-2011, i.e., our emphasis on finding demixed components that are orthogonal to each other, posed a problem in some of the cases. Hence, we needed a more flexible approach, which caused us to revisit the method and design dPCA-2015.</p><p>This history is probably not very interesting for the general reader. However, we agree with the reviewer that it is important to explain to readers how the methods differ. We have now included a more explicit paragraph into the Introduction, extended a corresponding subsection in the Discussion, and wrote a new section in the Methods that explicitly contrasts the older methods with dPCA-2015.</p><p>It would also be nice to spell out a little bit more clearly what scientific insights the method is likely to provide (or indeed, provides for the datasets being analyzed here). It seemed that for each dataset, the paper mostly says that dPCA confirmed the findings in the original paper. But if that's the case, why do we need dPCA? What can we learn by analyzing data with this method that we didn't already have access to using the analysis methods from those papers?</p><p>The key problem that someone who has recorded 100s<sup>-1</sup>000s of neurons in a behavioral task faces is how to make sense of the incredible diversity of responses. The classical solutions are summarized in our <xref ref-type="fig" rid="fig1">Figure 1</xref>, and they essentially focus on extracting features of the data. Through clever combination of these classical techniques, researchers have always been able to extract important, or even key features in their data. The reviewer is correct in noting that dPCA is not guaranteed to discover anything beyond that, and, in the data sets we analyzed, we mostly confirmed the findings of the original studies (see below for some differences). So why dPCA?</p><p>There are several answers to this question. First, the classical techniques are actually quite laborious, and they require a lot of intuitions and work on the side of the researchers. Which part of the data to focus on first? Which time-periods or which parameters? dPCA is supposed to radically simplify this process by allowing researchers to boil down complex data sets and represent them in a few components, or, in essence, print out pretty much everything that's happening on a single page. First and foremost, dPCA is therefore a quick visualization method for the data, something that can serve as an entry point to analyzing certain aspects of the data in more depth and detail.</p><p>Second, dPCA is designed to not `lose' any important part of the data, where importance is measured in terms of neural variability, i.e., changes in _ring rates. Our emphasis on (approximately) loss-less dimensionality reduction or compression highlights several features of the data that have mostly been disregarded in the past (although they may be somewhat familiar at least to the people who have access to the raw data): these include the large percentage of neural variance falling into the condition independent components, the lack of separate cell classes, the precise dynamics of the embedding of the task parameters in the neural population response etc, see the section “Universal features of the PFC population activity&quot;. Of course, the question then is, why should we care about these features? That brings us to the next point:</p><p>Third, dPCA is designed to facilitate the comparison of population activity across data sets, which is one of the main emphasis of this paper. Since there is no unique way of applying the classical methods (<xref ref-type="fig" rid="fig1">Figure 1</xref>; statistical significance, regression, etc.) to neural population data, different studies usually rely on different combination of methods. In turn, it becomes essentially hard to impossible for a reader to appreciate the similarities and/or differences of monkey PFC data in different conditions, especially if the studies were carried out in different labs. dPCA provides a simple, standardized way of visualizing the data, which in turn does allow one to compare neural activities across data sets. In turn, if certain dominating aspects re-occur, such as the condition-independent components, or the dynamics of the representation of information, then maybe these aspects deserve special attention, because they tell us something about what these areas are doing.</p><p>I have two other high-level comments:</p><p>1) I think the authors don't do enough to describe how they decided which components to include when setting up the model. Presumably this is a choice made before running the method, i.e., how to map the elements of the experiment onto discrete conditions. How did the authors solve this problem, and how should potential users go about solving it when applying the method to new datasets. For example, why don't we have &quot;reward&quot; components for any of the datasets considered here? How did you decide which interaction terms to include in the first place? In the olfactory data, did you include different components for each mixture or just one for each mixing component? What are the keys to setting up the analysis? Are there any exploratory data analysis techniques that the authors used to make these decisions?</p><p>This is a very important issue, especially with respect to the goals of dPCA as explained above, and we updated both our Discussion and the methods (sections on balanced and unbalanced data and missing data) to cover it in more depth. To answer the questions above:</p><p>Why are there no “reward&quot; components? In the monkey datasets we analyzed only correct trials because we lacked data on error trials since the monkeys often did not make enough mistakes. Accordingly, reward category is not very meaningful. In the olfactory datasets, we analyzed both correct and incorrect trials. Here, however, the presence/absence of reward is fully determined by stimulus and decision, so the “reward&quot; label is superfluous given stimulus and decision labels. We note, however, that the activity related to reward shows up in the “interaction&quot; terms, precisely because reward is based on the interaction of stimulus and decision.</p><p>How did you decide which interaction terms to include? Since we had stimulus and decision labels in all datasets, there really is only one interaction term. This interaction term was always included. (Time plays a special role in neurophysiological data and is treated differently as a parameter, as explained in the dPCA section in the Results and Methods.)</p><p>In the olfactory data, did you include different components for each mixture or just one for each mixing component? In the Kepecs et al. olfactory dataset, different odour mixtures were simply treated as different stimuli.</p><p>What are the keys to setting up the analysis? Are there any exploratory data analysis techniques that the authors used to make these decisions? The question of what labels to include is indeed very meaningful. We can provide some guidelines, but not a precise step-by-step recipe, as this depends too much on the scientific question addressed by an experiment. For instance, one could e.g. have some extra label available (imagine that for each stimulus and decision there were several possible values of reaction time, or movement type, etc.). Should they be included in the dPCA analysis as another parameter? That depends on the question, and there are some tradeoffs involved. As the number of task parameters included increases, so does the number of parameters (decoders and encoders) in dPCA, especially since there will be more and more interaction terms. Hence, the statistical power decreases and so does the usefulness of dPCA. Here is our updated Discussion:</p><p>“Second, the number of neurons needs to be sufficiently high in order to obtain reliable estimates of the demixed components. In our datasets, we found that at least _100 neurons were needed to achieve satisfactory demixing. The number is likely to be higher if more than three task parameters are to be demixed, as the number of interaction terms grows exponentially with the number of parameters. This trade-off between model complexity and demixing feasibility should be kept in mind when deciding how many parameters to put into the dPCA procedure. In cases when there are many task parameters of interest, dPCA is likely to be less useful than the more standard parametric single-unit approaches (such as linear regression). As a trivial example, imagine that only N = 1 neuron has been recorded; it might have strong and significant tuning to various parameters of interest, but there is no way to demix (or decode) these parameters from the recorded population.&quot;</p><p>2) For a methods paper like this, providing code should be a mandatory requirement for publication. Please say something about where to find an implementation, so that would be users can try it out.</p><p>We published our code on github prior to the original submission, which is stated in the end of our Introduction. We now included a reminder of this in the Methods.</p><p>“The dPCA code is available at <ext-link ext-link-type="uri" xlink:href="http://github.com/wielandbrendel/dPCA">http://github.com/wielandbrendel/dPCA</ext-link> for Matlab and Python.”</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>The manuscript has been greatly improved but there are some smaller remaining issues outlined below.</p><p>Reviewer #1 (General assessment and major comments (Required)):</p><p>This manuscript is like night and day compared to the previous version. I barely recognize the material any more. It is so much clearer that it makes me much more confident that this is all sound and well. I can hardly believe that these were the same authors.</p><p>We are very happy to hear that and thank the reviewer for the valuable comments.</p><p>Reviewer #1 (Minor Comments):</p><p>A few places where I got stuck were for example:</p><p>Equation 4 in subsection “Core dPCA: loss function and algorithm”, why can one separate the quadratic term like this? Also not clear why stefs 1-3 minimize this cost function, but I trust that all this is in the cited paper, though I could only retrieve the Izenman 1975 paper, which looked like yet more work to answer my questions.</p><p>We updated this section to make it as self-contained as possible and to avoid sending readers to the original papers from the 1970s. Reduced-rank regression is quite obscure in our field and we have therefore tried to clarify all the steps as much as possible. Please let us know if something remains unclear here.</p><p>The updated paragraphs now read:</p><p>“We note that the loss function <italic>L<sub>Ø</sub></italic> is of the general form ||<bold>X</bold><sub>Ø</sub> – <bold>AX</bold>||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup>, with <bold>A</bold> = <bold>FD</bold>. For an arbitrary <italic>N x N</italic> matrix <bold>A</bold>, minimization of the loss function amounts to a classical regression problem with the well-known ordinary least squares (OLS) solution, A<sub>OLS</sub> =<bold>X</bold><sub>Ø</sub><bold>X</bold><sup>T</sup> (<bold>XX</bold><sup>T)-1</sup>. In our case, <bold>A</bold> = <bold>FD</bold> is an <italic>N x N</italic> matrix of rank <italic>q</italic>, which we will make explicit by writing <bold>A</bold><sub>q</sub>. The dPCA loss function therefore amounts to a linear regression problem with an additional rank constraint on the matrix of regression coefficients. This problem is known as reduced-rank regression (<italic>RRR</italic>) (Izenman, 1975; Reinsel and Velu, 1998; Izenman, 2008) and can be solved via the singular value decomposition.</p><p>To see this, we write <bold>X</bold><sub>Ø</sub> –<bold>A</bold><sub>q</sub><bold>X</bold> = (<bold>X</bold><sub>Ø</sub> –<bold>A</bold><sub>OLS</sub><bold>X</bold>) + (<bold>A</bold><sub>OLS</sub><bold>X</bold> – <bold>A</bold><sub>q</sub><bold>X</bold>). The first term, <bold>X</bold><sub>Ø</sub> –<bold>A</bold><sub>OLS</sub><bold>X</bold>, consists of the regression residuals that cannot be accounted for by any linear transformation of X. It is straightforward to verify that these regression residuals, <bold>X</bold><sub>Ø</sub> –<bold>A</bold><sub>OLS</sub><bold>X</bold>, are orthogonal to <bold>X</bold> (Hastie et al., 2009, Section 3.2) and hence also orthogonal to (<bold>A</bold><sub>OLS</sub> – <bold>A</bold><sub>q</sub>)X. This orthogonality allows us to split the loss function into two terms,</p><p>|| <bold>X</bold><sub>Ø</sub> –<bold>A</bold><sub>q</sub><bold>X</bold> ||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup> = || <bold>X</bold><sub>Ø</sub> –<bold>A</bold><sub>OLS</sub><bold>X</bold>||<bold><sup><xref ref-type="bibr" rid="bib2">2</xref></sup></bold>+|| <bold>A</bold><sub>OLS</sub><bold>X – A</bold><sub>q</sub><bold>X</bold>||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup></p><p>where the first term captures the (unavoidable) error of the least squares fit while the second term describes the additional loss suffered through the rank constraint. Since the first term does not depend on Aq, the problem reduces to minimizing the second term.</p><p>To minimize the second term, we note that the best rank-q approximation to <bold>A</bold><sub>OLS</sub><bold>X</bold></p><p>is given by its first <italic>q</italic> principal components (Eckart-Young-Mirsky theorem). Accordingly, if we write <bold><italic>U</italic></bold><italic><sub>q</sub></italic> for the matrix of the <italic>q</italic> leading principal directions (left singular vectors) <italic>u<sub>i</sub></italic> of <bold>A</bold><sub>OLS</sub><bold>X</bold>, then the best approximation is given by <bold>U</bold><sub>q</sub><bold>U</bold><sup>T</sup><sub>q</sub><bold>A</bold><sub>OLS</sub><bold>X</bold> and hence <bold>A</bold><sub>q</sub> =<bold>U</bold><sub>q</sub><bold>U</bold><sup>T</sup><sub>q</sub><bold>A</bold><sub>OLS</sub>.</p><p>To summarize, the reduced-rank regression problem posed above can be solved in a three-step procedure: […]”</p><p>Similarly, second equation in subsection “Unbalanced data, why can one separate the square into two terms? and why can one replace X<sub>noise</sub> by C<sup>1/2</sup><sub>noise</sub>? and why is X<sub>PSTH</sub>=X?</p><p>We have expanded the explanation of this bit as follows:</p><p>“In the balanced case, the dPCA loss function <italic>L<sub>Ø</sub></italic> can be rewritten as the sum of two terms with one term depending on the PSTHs and another term depending on the trial-to-trial variations,</p><p><italic>L<sub>Ø</sub></italic> = ||<bold>X</bold><italic><sub>Ø</sub></italic> – <bold>FDX</bold>||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup> = ||<bold>X</bold><italic><sub>Ø</sub> </italic>– <bold>FD (X</bold> – <bold>X</bold><sub>noise</sub>)||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup> + ||<bold>FDX</bold><sub>noise</sub>||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup>;</p><p>where we used the fact that <bold>X</bold><italic><sub>Ø</sub></italic> and <bold>X</bold> – <bold>X</bold><sub>noise</sub> are orthogonal to <bold>X</bold><sub>noise</sub> (see Appendix A). We now define <bold>X</bold><sub>PSTH</sub> = X – X<sub>noise</sub> which is simply a matrix of the same size as <bold>X</bold> with the activity of each trial replaced by the corresponding PSTH. In addition, we observe that the squared norm of any centered data matrix <bold>Y</bold> with <italic>n</italic> data points can be written in terms of its covariance matrix <bold>C</bold><sub>Y</sub> = <bold>YY</bold><sup>T</sup>/<italic>n</italic>, namely ||<bold>Y</bold>||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup> = tr[<bold>YY</bold><sup>T</sup>] = <italic>n</italic> tr[<bold>C</bold><sub>Y</sub>] = <italic>n</italic> tr[<bold>C</bold><sup>1/2</sup> <sub>Y</sub><bold>C</bold><sup>1/2</sup> <sub>Y</sub>] = <italic>n</italic>|| <bold>C</bold><sup>1/2</sup> <sub>Y</sub> ||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup>, and so</p><p><italic>L<sub>Ø</sub></italic> = ||<bold>X</bold><italic><sub>Ø</sub></italic> – <bold>FDX</bold><sub>PSTH</sub>||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup> + <italic>K SQT</italic>|| <bold>FCD</bold><sup>½</sup><sub>noise</sub>||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup></p><p>The first term consists of <italic>K</italic> replicated copies: <bold>X</bold><sub>PSTH</sub> contains <italic>K</italic> replicated copies of X (which we defined above as the matrix of PSTHs) and X<italic><sub>Ø</sub></italic> contains <italic>K</italic> replicated copies of X<italic><sub>Ø</sub> </italic>(which we take to be a marginalization of <bold>X</bold>, with <bold>X</bold> = Ʃ<italic><sub>Ø</sub> </italic>X<italic><sub>Ø</sub></italic>). We can eliminate the replications and drop the factor <italic>K</italic> to obtain</p><p>L<italic><sub>Ø</sub></italic> = ||X<italic><sub>Ø</sub> </italic>– <bold>FDX</bold> ||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup> + <italic>SQT</italic> ||<bold>FDC<sup><xref ref-type="bibr" rid="bib1">1</xref></sup></bold><sup>/2</sup><sub>noise</sub>||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup>”</p><p>In general I have to say that, while the issue of balancing the data is intuitively clear, the corresponding math seems cumbersome, but again, this may be my lack of time.</p><p>The balancing math can indeed appear somewhat confusing, however we should point out that this whole part is not essential for the understanding of our method. We included the full treatment of the balancing issue because it can be important in practical applications when the conditions are very unbalanced. Otherwise, we can expect the direct formulation of dPCA without any rebalancing modifications with ||<bold>X</bold><italic> <sub>Ø</sub> </italic>–<bold>FDX</bold>||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup> loss function to work fine, and the generalization to the loss function with PSTH matrices ||X<italic> <sub>Ø</sub></italic> – <bold>FDX</bold>||<sup><xref ref-type="bibr" rid="bib2">2</xref></sup> for the sequentially recorded datasets is intuitive.</p><p> <italic>Reviewer #2 (General assessment and major comments (Required)): The revised manuscript is acceptable for publication. Reviewer #2 (Minor Comments): No author was designated with author association superscript 6 – Harvard University.</italic></p><p>Author designation with superscript 2 is not provided.</p><p>- There's probably an error in the numbering of author associations.</p><p>We fixed the affiliation list.</p><p>Reviewer #3 (General assessment and major comments (Required)):</p><p>I thank the author for the detailed reply to the original review comments, and the revised manuscript is substantially improved. I have a few lingering technical comments and questions, but overall I think the paper is now suitable for publication.</p><p>Reviewer #3 (Minor Comments):</p><p><xref ref-type="fig" rid="fig1">Figure 1K</xref>: not entirely clear what principal components these are, i.e., why are there multiple traces per component? I guess this is because the authors have taken the principal components of the rows of the X matrix (i.e., each of which contains the PSTH of a single neuron for all components). This is a slightly odd choice: I would have thought one would take PCA of the PSTHs, which is closer in spirit to the dPCA and other methods discussed here, so that each component is just a length-T vector (i.e., where T is the number of time bins in the PSTH). I guess this is ok as is, but you should add something to the text or the caption to clarify what these are (e.g., each principal component is a PSTH across all conditions). This seems a little bit confusing, however, because the dPCA components you'll show later aren't the same size. (Or am I misinterpreting the figure, and what you're showing is instead the projection from all conditions on each PC?)</p><p>There is some confusion here. As it might partially be due to the terminology (“principal axes&quot; vs “principal components&quot;), we should clarify it here. The PSTH matrix <bold>X</bold> has <italic>N</italic> rows (<italic>N</italic> is the number of neurons) and <italic>SQT</italic> columns (number of data points in the PSTHs in all conditions, <italic>S</italic> is the number of stimuli, <italic>D</italic> the number of decisions, <italic>T</italic> the number of time-points). We consider this as <italic>SQT</italic> points in the <italic>N</italic>-dimensional space and perform PCA on these data. Covariance matrix is <italic>N x N</italic>, each eigenvector (a direction in the <italic>N</italic>-dimensional space) defines a principal axis, and the projection onto this axis we call “principal component&quot; and it has <italic>SQT</italic> points. These projections are shown on <xref ref-type="fig" rid="fig1">Figure 1K</xref>. We clarified this in the legend and also in the Methods section “Implementation of classical approaches&quot;.</p><p>(Perhaps by “PCA of the PSTHs&quot; the reviewer means reshaping these data as <italic>N SQ</italic> different PSTHs of length <italic>T</italic>? Then each PC would indeed be of length <italic>T</italic>; but we want to work in the <italic>N</italic>-dimensional (and not <italic>NSQ</italic>-dimensional) space of neurons because we want PCA/dPCA projections to be interpreted as linear readouts from the neural population.)</p><p>We furthermore emphasize that in the dPCA treatment we always work in the same <italic>N</italic>-dimensional space. In all subsequent figures (e.g. <xref ref-type="fig" rid="fig3">Figure 3</xref>), we always display all conditions in each panel, so we emphasize that the size of the PCA/dPCA components does not change throughout the manuscript. Indeed, although the condition-independent component may sometimes appear to consist of only one line, for instance, we are plotting one line for each condition, and they are just on top of each other (as they should be when the components are properly demixed).</p><p> <italic>Indeed, the two components from <xref ref-type="fig" rid="fig1">Figure 1e</xref> explain only 23% of the total variance of the population firing rates and the two components from <xref ref-type="fig" rid="fig1">Figure 1h</xref> explain only 22% (see Methods). Consequently, a naive observer would not be able to infer from the components what the original neural activities looked like. This seems a little bit uncharitable to the Mante</italic> et al. <italic>paper. After all, the regression model used by those authors did indeed have an &quot;untuned component&quot; for each neuron (referred to as Β-0), they simply didn't do anything with these components when it came time to construct a low-d projection that captured information about the stimulus and decision variables. But one could certainly have performed a dimensionality reduction of the Β_0's if one were interested in capturing information about time. So, in my view, while this passage is technically correct, I would encourage the authors to rephrase it to be slightly less disparaging to Mante</italic> et al.</p><p> <italic>They simply weren't interested in the coding of time, so the fact that they don't capture those components is a choice about what variables to include more than a fundamental limitation of the method.</italic> </p><p>This is a fair point. Strictly speaking, Mante et al. define their targeted dimensionality reduction (TDR) procedure (in the supplementary materials to their paper) by using only linear terms in the regression, and they disregard the <italic>β</italic>0-term. However, the reviewer is correct that one could treat _0 in the same way as other regression terms, and we thank the reviewer for pointing this out. More generally, as with any supervised or regression-based method, there are many possible extensions of TDR one could envision (e.g. including quadratic terms into the regression, using several time-points instead of choosing only one time-point, etc.). However, our point here was to contrast the trade-o_ between supervised and unsupervised methods, and so we chose to present TDR exactly as it was originally introduced.</p><p>To address the reviewer's comments, we have now updated our “Comparison of dPCA with targeted dimensionality reduction&quot; section in the Methods to be clear about the possibility of using <italic>β</italic>0, and updated our <xref ref-type="fig" rid="fig14">Figure 14</xref> to show these condition-independent components produced by TDR. We do not think the introductory Results section is the right place to elaborate on possible extensions of TDR, but we have slightly reworded the section to be more careful about our comparison of the supervised and unsupervised methods, and we now write:</p><p>“While such supervised approaches can be extended in various ways to produce more components and capture more variance, a more direct way to avoid this loss of information is to resort to unsupervised methods such as principal component analysis (PCA).”</p><p>To emphasize the importance of having more than one component per task parameter, we have now prepared another Video and included one additional paragraph in the Results section describing our first dataset:</p><p>“The first stimulus component (#5) looks similar to the stimulus components that we obtained with standard regression-based methods (<xref ref-type="fig" rid="fig1">Figure 1e, h</xref>) but now we have further components as well. Together they show how stimulus representation evolves in time. In particular, plotting the first two stimulus components against each other (see <xref ref-type="other" rid="media1">Video 1</xref>) illustrates how stimulus representation rotates in the neural space during the delay period so that the encoding subspaces at F1 and F2 periods are not the same (but far from orthogonal either).”</p><p>&quot;where averaging takes place over all irrelevant parameters.&quot; A bit unclear – could use an extra sentence unpacking what this means.</p><p>We slightly changed the formulation of this bit; what is meant here is illustrated in the next sentence using the toy example:</p><p>“First, we require that the compression and decompression steps reconstruct not the neural activity directly, but the neural activity averaged over trials and over some of the task parameters. In the toy example, the reconstruction target is the matrix of stimulus averages, <bold>X</bold>s, which has the same size as <bold>X</bold>, but in which every data point is replaced by the average neural activity for the corresponding stimulus, as shown in <xref ref-type="fig" rid="fig2">Figure 2h</xref>.”</p><p>This averaging is also explained more formally below and is illustrated on <xref ref-type="fig" rid="fig2">Figure 2g</xref>.</p><p> <italic>&quot;The overall variance explained by the dPCA components (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, red line) is very close to the overall variance explained by the PCA components (black line).&quot; Why does PCA only get 80% of the variance? Is this because you've determined that the last 20% belongs to the noise component?</italic> </p><p>The location of the legend was suboptimal on this figure panel and made it look as if “PCA&quot; label refers to the horizontal dashed line at 80%. Instead, “PCA&quot; label refers to the black line of cumulative explained variance that can be hardly seen because of the red dPCA line. So the correct interpretation is that PCA explained variance grows with the number of components and reaches 80% at 15 components. With more components it would of course exceed 80%. At the same time, the dashed horizontal line shows our estimate of the amount of “signal variance&quot;, i.e. we estimate that 20% of the variance is due to the noise in our PSTH estimates. This computation is explained in the Methods, but is not essential for anything else.</p><p>We have changed the legend location to remove the ambiguity.</p><p><italic><xref ref-type="fig" rid="fig3">Figure 3</xref> caption: &quot;Thick black lines show time intervals during which the respective task parameters can be reliably extracted from single-trial activity&quot;. Does this mean a pseudotrial with 832 neurons? Start of this section mentions the # of neurons but I think it's important to say it here in the Figure caption what this means. (Presumably, if you have enough neurons recorded then single-trial decoding becomes perfect?)</italic> </p><p>Yes, the decoding was done using pseudotrials with 832 neurons. The exact number does not seem to be important in the _gure caption: we simply used all available neurons. But the reviewer is right in that it is important to point out that the decoding was done using pseudotrials (and hence ignores noise correlations). We clarified the figure caption as follows:</p><p>“Thick black lines show time intervals during which the respective task parameters can be reliably extracted from single-trial activity (using pseudotrials with all recorded neurons)”</p></body></sub-article></article>