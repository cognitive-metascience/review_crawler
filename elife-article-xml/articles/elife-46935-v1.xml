<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">46935</article-id><article-id pub-id-type="doi">10.7554/eLife.46935</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Immunology and Inflammation</subject></subj-group></article-categories><title-group><article-title>Deep generative models for T cell receptor protein sequences</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-136705"><name><surname>Davidsen</surname><given-names>Kristian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3821-6902</contrib-id><email>krdav@uw.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-137372"><name><surname>Olson</surname><given-names>Branden J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1951-8822</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-137373"><name><surname>DeWitt</surname><given-names>William S</given-names><suffix>III</suffix></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6802-9139</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-134933"><name><surname>Feng</surname><given-names>Jean</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2041-3104</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-137374"><name><surname>Harkins</surname><given-names>Elias</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-106193"><name><surname>Bradley</surname><given-names>Philip</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0224-6464</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-113042"><name><surname>Matsen</surname><given-names>Frederick A</given-names><suffix>IV</suffix></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0607-6025</contrib-id><email>matsen@fredhutch.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Fred Hutchinson Cancer Research Center</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Weigel</surname><given-names>Detlef</given-names></name><role>Senior Editor</role><aff><institution>Max Planck Institute for Developmental Biology</institution><country>Germany</country></aff></contrib><contrib contrib-type="editor"><name><surname>Chakraborty</surname><given-names>Arup K</given-names></name><role>Reviewing Editor</role><aff><institution>Massachusetts Institute of Technology</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>05</day><month>09</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e46935</elocation-id><history><date date-type="received" iso-8601-date="2019-03-17"><day>17</day><month>03</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-08-21"><day>21</day><month>08</month><year>2019</year></date></history><permissions><copyright-statement>Â© 2019, Davidsen et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Davidsen et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-46935-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.46935.001</object-id><p>Probabilistic models of adaptive immune repertoire sequence distributions can be used to infer the expansion of immune cells in response to stimulus, differentiate genetic from environmental factors that determine repertoire sharing, and evaluate the suitability of various target immune sequences for stimulation via vaccination. Classically, these models are defined in terms of a probabilistic V(D)J recombination model which is sometimes combined with a selection model. In this paper we take a different approach, fitting variational autoencoder (VAE) models parameterized by deep neural networks to T cell receptor (TCR) repertoires. We show that simple VAE models can perform accurate cohort frequency estimation, learn the rules of VDJ recombination, and generalize well to unseen sequences. Further, we demonstrate that VAE-like models can distinguish between real sequences and sequences generated according to a recombination-selection model, and that many characteristics of VAE-generated sequences are similar to those of real sequences.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>T cell receptor</kwd><kwd>variational autoencoder</kwd><kwd>repertoire modeling</kwd><kwd>vaccine</kwd><kwd>T cell expansion</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 GM113246</award-id><principal-award-recipient><name><surname>Davidsen</surname><given-names>Kristian</given-names></name><name><surname>Olson</surname><given-names>Branden J</given-names></name><name><surname>DeWitt</surname><given-names>William S</given-names><suffix>III</suffix></name><name><surname>Matsen</surname><given-names>Frederick A</given-names><suffix>IV</suffix></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U19 AI117891</award-id><principal-award-recipient><name><surname>Matsen</surname><given-names>Frederick A</given-names><suffix>IV</suffix></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 AI120961</award-id><principal-award-recipient><name><surname>Harkins</surname><given-names>Elias</given-names></name><name><surname>Matsen</surname><given-names>Frederick A</given-names><suffix>IV</suffix></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><award-id>Faculty Scholar grant</award-id><principal-award-recipient><name><surname>Davidsen</surname><given-names>Kristian</given-names></name><name><surname>Olson</surname><given-names>Branden J</given-names></name><name><surname>DeWitt</surname><given-names>William S</given-names><suffix>III</suffix></name><name><surname>Matsen</surname><given-names>Frederick A</given-names><suffix>IV</suffix></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 AI146028</award-id><principal-award-recipient><name><surname>Olson</surname><given-names>Branden J</given-names></name><name><surname>Harkins</surname><given-names>Elias</given-names></name><name><surname>Matsen</surname><given-names>Frederick A</given-names><suffix>IV</suffix></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>5T32HG000035-23</award-id><principal-award-recipient><name><surname>DeWitt</surname><given-names>William S</given-names><suffix>III</suffix></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Deep learning improves estimation of T cell receptor cohort frequencies and learns the rules of VDJ recombination, potentially making it helpful for vaccine design.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>T cell receptors (TCRs) are composed of an <inline-formula><mml:math id="inf1"><mml:mi>Î±</mml:mi></mml:math></inline-formula> and a <inline-formula><mml:math id="inf2"><mml:mi>Î²</mml:mi></mml:math></inline-formula> protein chain, both originating from a random V(D)J recombination process, followed by selective steps that ensure functionality and limit auto-reactivity. To generate diverse and functional TCRs, T cells combine a stochastic process for choosing from a pool of V, D and J genes with a process for selecting for expression and MHC recognition. The process first occurs for the <inline-formula><mml:math id="inf3"><mml:mi>Î²</mml:mi></mml:math></inline-formula> chain, where first a D and a J gene are recombined using random trimming and joining with random nucleotides, then this DJ segment is recombined with a V gene via an analogous process. After the <inline-formula><mml:math id="inf4"><mml:mi>Î²</mml:mi></mml:math></inline-formula> chain has been generated, a small cell expansion occurs followed by a similar <inline-formula><mml:math id="inf5"><mml:mi>Î±</mml:mi></mml:math></inline-formula> chain recombination, although without a D gene. For detailed reviews ofÂ V(D)J recombination see <xref ref-type="bibr" rid="bib4">Bassing et al. (2002)</xref>,Â and <xref ref-type="bibr" rid="bib37">Schatz and Ji (2011)</xref>. The naive T cell population consists of T cells that have undergone V(D)J recombination and MHC selection but not yet encountered antigen. In a system known as the clonal selection mechanism of immune memory, T cells that bind antigen increase in frequency, thus increasing the frequency of their corresponding TCR sequences. The resulting ensemble of protein sequences thus summarizes each individualâs previous immune exposures and largely determines their resistance to various infections. One can consider these protein sequences as a sample from a probability distribution, whether it is the distribution of receptors within an individual, or the distribution of receptors in a population. This article concerns fitting such probability distributions on TCR <inline-formula><mml:math id="inf6"><mml:mi>Î²</mml:mi></mml:math></inline-formula> protein sequences (which will be called âTCR sequencesâ for the rest of the paper).</p><p>Probability estimates from these models can be used to draw important biological conclusions. For example, observing sequences that are amplified in a repertoire indicates that they perform important functions like targeting yellow fewer or cytomegalovirus (<xref ref-type="bibr" rid="bib33">Pogorelyy et al., 2018c</xref>; <xref ref-type="bibr" rid="bib34">Pogorelyy et al., 2018d</xref>; <xref ref-type="bibr" rid="bib16">Emerson et al., 2017</xref>). However, in order to properly define amplification, we must infer the frequency of such sequences appearing in the naive (i.e. post-selection but pre-amplification) repertoire so that we do not mistake an inherently probable recombination scenario with functional selection. As another application, (<xref ref-type="bibr" rid="bib14">Elhanati et al., 2018</xref>) used probability calculations to predict the frequency of shared TCR sequences between individuals, showing that biases of the V(D)J recombination process significantly explain the degree of sharing.</p><p>The appearance of a given TCR sequence in the blood of an individual means that it was generated by V(D)J recombination and subsequently passed thymic selection, which removes TCRs with improper binding to MHC as well as self-reactive TCRs. This series of two steps constitutes a sophisticated random process for generating protein sequences. Previous work (<xref ref-type="bibr" rid="bib13">Elhanati et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Pogorelyy et al., 2018c</xref>) approached the problem of inferring this process by calculating the probability of a sequenceâs V(D)J recombination using a probabilistic graphical model, multiplying this probability by a thymic selection factor <inline-formula><mml:math id="inf7"><mml:mi>Q</mml:mi></mml:math></inline-formula>, and scaling accordingly. Although breaking the process into generation and selection steps parallels the biological process, we can instead fit a distribution to a mature TCR repertoire directly, and assess the advantages of either approach. Indeed, these considerations raise the question of how to model the distribution of TCR protein sequences from a given source in order to answer meaningful immunological questions.</p><p>In this paper we develop variants of Variational Autoencoder (VAE) models (<xref ref-type="bibr" rid="bib20">Kingma et al., 2014b</xref>; <xref ref-type="bibr" rid="bib18">Higgins et al., 2017</xref>) to fit the distribution of TCR protein sequences. Recent work on deep generative models of proteins inspired our approach (<xref ref-type="bibr" rid="bib39">Sinai et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Riesselman et al., 2018</xref>). We find that these models can predict cohort frequency with high accuracy, learn the rules of VDJ recombination, generalize to unseen sequences, and generate sequences with similar characteristics to observed TCR sequences.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Methods overview</title><p>We briefly outline our methods in order to present results; further details can be found in the MaterialsÂ andÂ methods section. We model TCR sequences using simple variants of variational autoencoders (VAEs). Previous work using VAEs have found success when first, there is a vast amount of data available, and second, the data distribution is complicated, involving nonlinearities and interactions between covariates. There is indeed a vast amount of TCR repertoire data, and the TCR probability distributions are complex.</p><p>VAE models can be described as consisting of an <inline-formula><mml:math id="inf8"><mml:mi>n</mml:mi></mml:math></inline-formula>-dimensional latent space, a prior <inline-formula><mml:math id="inf9"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on that latent space, and probabilistic maps parameterized by two neural networks: an encoder <inline-formula><mml:math id="inf10"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and a decoder <inline-formula><mml:math id="inf11"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>Â (<xref ref-type="fig" rid="fig1">Figure 1</xref>; <xref ref-type="bibr" rid="bib20">Kingma et al., 2014b</xref>). For the models used in this paper the latent space is 20-dimensional, and we use the conventional choice of a standard multivariate normal prior for <inline-formula><mml:math id="inf12"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The encoder <inline-formula><mml:math id="inf13"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a multivariate normal distribution with mean and diagonal covariance determined by a neural network with input <inline-formula><mml:math id="inf14"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> (see MaterialsÂ andÂ methods for how TCR protein sequences are transformed into appropriate input for a neural network). This choice of a normal distribution is primarily for mathematical convenience rather than being part of a specific modeling design; the normal ânoiseâ in the latent space gets processed by a neural network which introduces non-linearities that ensure that the result is not normal. However, VAE variants do use other distributions in place of normal (<xref ref-type="bibr" rid="bib12">Dilokthanakul et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Davidson et al., 2018</xref>). The decoder <inline-formula><mml:math id="inf15"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a per-site categorical distribution over amino acids and gaps parameterized by a neural network with input <inline-formula><mml:math id="inf16"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula>.</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.46935.016</object-id><label>Figure 1.</label><caption><title>A cartoon of a variational autoencoder (VAE).</title><p>A VAE embeds objects of interest <inline-formula><mml:math id="inf17"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> (here TCR protein sequences) into an <inline-formula><mml:math id="inf18"><mml:mi>n</mml:mi></mml:math></inline-formula>-dimensional latent space, using a probabilistic encoder <inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and decoder <inline-formula><mml:math id="inf20"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that are both parametrized by deep neural networks. The VAE objective is to encode and decode objects with high fidelity (<inline-formula><mml:math id="inf21"><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>â</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>) while ensuring the encoder <inline-formula><mml:math id="inf22"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> distribution is close to a prior <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on that latent space, typically taken to be a standard multivariate normal distribution.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.46935.017</object-id><label>Figure 1âfigure supplement 1.</label><caption><title>The <monospace>basic</monospace> model decoder.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.46935.018</object-id><label>Figure 1âfigure supplement 2.</label><caption><title>The <monospace>count_match</monospace> model decoder.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig1-figsupp2-v1.tif"/></fig></fig-group><p>Once the VAE is trained, one can sample new sequences by âdecodingâ samples from <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, that is, drawing from <inline-formula><mml:math id="inf25"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, feeding those points through the decoder network, and then sampling from the resulting probabilities. In the case of TCRs, this final sampling step goes from categorical distributions on the TCR components (i.e. on the V gene, J gene, and the amino acids at the various positions) to an actual TCR sequence. One trains a VAE with a collection of observed sequences <inline-formula><mml:math id="inf26"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> via the encoder <inline-formula><mml:math id="inf27"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. VAE training has two goals, which are represented by two terms of the objective function: first, to be able to (probabilistically) encode and decode the sequences through the latent space with high fidelity, and second, to ensure that that the <inline-formula><mml:math id="inf28"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> map is close to the prior <inline-formula><mml:math id="inf29"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on average across <inline-formula><mml:math id="inf30"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula>. The second component of this objective encourages a structured mapping of input sequences to latent values, in hopes that the model learns meaningful sequence features rather than memorizing properties of the training data. The balance between these two components is important and is controlled by a parameter <inline-formula><mml:math id="inf31"><mml:mi>Î²</mml:mi></mml:math></inline-formula>Â (<xref ref-type="bibr" rid="bib18">Higgins et al., 2017</xref>). Once the VAE is trained (i.e. parameters <inline-formula><mml:math id="inf32"><mml:mi>Ï</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf33"><mml:mi>Î¸</mml:mi></mml:math></inline-formula> are optimized according to the objective with respect to a particular dataset), we can calculate the probability of generating a given sequence <inline-formula><mml:math id="inf34"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> via importance sampling.</p><p>We are interested in TCR <inline-formula><mml:math id="inf35"><mml:mi>Î²</mml:mi></mml:math></inline-formula> protein sequences, which due to the process of VDJ recombination are uniquely identified by triples consisting of V gene, J gene, and CDR3 protein sequence (<xref ref-type="bibr" rid="bib44">Woodsworth et al., 2013</xref>). We developed two VAE models for such protein sequences: a simple one, denoted <monospace>basic</monospace> and a more complex model, denoted <monospace>count_match</monospace>. The <monospace>basic</monospace> model does not have any information about theÂ contentÂ of germline genes built into the model and was trained according to a simple loss function (<xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref>). The <monospace>count_match</monospace> model brings in information about the protein sequence of the germline genes and has a more complex loss function involving CDR3 length and the degree to which the protein sequences on the ends of the CDR3 match the corresponding germline gene sequences (<xref ref-type="fig" rid="fig1s2">Figure 1âfigure supplement 2</xref>).</p><p>As a baseline for comparison, we combined OLGA, a sophisticated recombination model (<xref ref-type="bibr" rid="bib38">Sethna et al., 2018</xref>), with a simplified version of the selection model used in <xref ref-type="bibr" rid="bib13">Elhanati et al. (2014)</xref>, together which we will denote <monospace>OLGA.Q</monospace>. Our selection component <inline-formula><mml:math id="inf36"><mml:mi>Q</mml:mi></mml:math></inline-formula> is parameterized by triples consisting of V gene identity, J gene identity, and CDR3 length, resulting in a model with about 14,000 parameters. This is a simpler model than the general <xref ref-type="bibr" rid="bib13">Elhanati et al. (2014)</xref> model, which allows for selection based on CDR3 amino acid composition. However, it is a richer model than any models used by the same group since the publication of <xref ref-type="bibr" rid="bib13">Elhanati et al. (2014)</xref>, such as the one used to find condition-associated immune receptors in <xref ref-type="bibr" rid="bib32">Pogorelyy et al. (2018b)</xref>Â and <xref ref-type="bibr" rid="bib33">Pogorelyy et al. (2018c)</xref>. <xref ref-type="bibr" rid="bib38">Sethna et al. (2018)</xref> suggest probabilistically evaluating vaccine targets using OLGA directly and no selection model at all. In any case, aÂ software implementation of the general <xref ref-type="bibr" rid="bib13">Elhanati et al. (2014)</xref> model, for which training is highly involved, is not currently available.</p></sec><sec id="s2-2"><title>VAE models predict cohort frequency</title><p>We wished to understand the ability of <monospace>basic</monospace>, <monospace>count_match</monospace>, and <monospace>OLGA.Q</monospace> to estimate the frequency with which a TCR appears in a given cohort, both when the TCR is contained in the training set (âtrainâ) and when it is not (âtestâ). Here we define âcohort countâ for a collection of repertoires to be the number of times a given TCR amino acid sequence appeared in the output files from the ImmunoSEQ assay (Adaptive Biotechnologies, Seattle, WA, USA) for those repertoires (ignoring the template abundance column). Multiple nucleotide occurrences of a given TCR protein sequence contribute separately to this number. Define <inline-formula><mml:math id="inf37"><mml:mi>c</mml:mi></mml:math></inline-formula> to be the cohort count vector for the data set of <xref ref-type="bibr" rid="bib16">Emerson et al. (2017)</xref>, indexed by the TCR protein sequences with values being these cohort counts.</p><p>To assess out-of-sample performance, we first partitioned <inline-formula><mml:math id="inf38"><mml:mi>c</mml:mi></mml:math></inline-formula> into <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>c</mml:mi><mml:mi>train</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>c</mml:mi><mml:mi>test</mml:mi></mml:msub></mml:math></inline-formula> with a 50/50 split irrespective of abundance. We emphasize that there is no overlap between these collections of TCRs. To obtain a training set, we drew 200,000 sequences from the multinomial distribution induced by <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>c</mml:mi><mml:mi>train</mml:mi></mml:msub></mml:math></inline-formula>. We then trained <monospace>basic</monospace> and <monospace>OLGA.Q</monospace> using these sequences. The trained models yield per-sequence probability distributions: <inline-formula><mml:math id="inf42"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> for the <monospace>basic</monospace> VAE and <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> for <monospace>OLGA.Q</monospace>. We evaluated each of these probabilities as well as the cohort frequency for 10,000 sequences drawn multinomially from <inline-formula><mml:math id="inf44"><mml:msub><mml:mi>c</mml:mi><mml:mi>test</mml:mi></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf45"><mml:msub><mml:mi>c</mml:mi><mml:mi>train</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>We performed this procedure for the entire cohort, but also restricting the cohort for training to a randomly-selected, smaller number of subjects while still comparing to frequency estimates using the whole cohort.</p><p>We found that VAE models can predict cohort frequency for out-of-sample TCR sequences (<xref ref-type="fig" rid="fig2">Figure 2</xref>). As the number of samples increases, the scatter of points decreases and the difference between training and testing samples also decreases. With 666 samples, this results in an <inline-formula><mml:math id="inf46"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> value for the best-fit line on the log-log scale of 0.258 for <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> and an <inline-formula><mml:math id="inf48"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> value of 0.442 for <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> on the test set (<xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>). When we increased the number of training sequences five-fold to 1 million, <inline-formula><mml:math id="inf50"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>s increased slightly to 0.268 for <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> and 0.474 for <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula>. Recall that these correlation measures include the full scale of frequencies, including very noisy frequency estimates on the lower end of the scale. Also, we make no efforts to account for sequencing error above the methods used in <xref ref-type="bibr" rid="bib11">DeWitt et al. (2018)</xref>. We note that higher correlations have been observed for an <monospace>OLGA.Q</monospace>-type model when calculating probability of CDR3 only, restricting to sequences found in an epitope database, and smoothing using a single amino acid mismatch (<xref ref-type="bibr" rid="bib31">Pogorelyy et al., 2018a</xref>).</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.46935.002</object-id><label>Figure 2.</label><caption><title>Cohort frequency prediction with two probability estimators.</title><p>Plot shows the (natural) log frequency in the entire cohort, restricted to TCRs appearing in the subset of subjects, versus the probability according to <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> and <inline-formula><mml:math id="inf54"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> for the <monospace>basic</monospace> model. Results partitioned into when the TCR appeared in the training set (âtrainâ) and when it did not (âtestâ). Probabilities for each estimator normalized to sum to one across the collection of sequences represented in the plots.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.46935.003</object-id><label>Figure 2âfigure supplement 1.</label><caption><title>Comparison of R<sup>2</sup> values for cohort frequency estimation.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig2-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-3"><title>VAE models learn the rules of VDJ recombination</title><p>TCR <inline-formula><mml:math id="inf55"><mml:mi>Î²</mml:mi></mml:math></inline-formula> chains are generated via VDJ recombination, a process in which germline-encoded genes are randomly chosen from a pool, trimmed a random amount, and then joined together with random nucleotide insertions in between. This recombination process leads to important structural characteristics in the generated sequences. Specifically, because the beginning of the CDR3 region is encoded by the V gene, and the end by the J gene, there is a strong correlation between the V and J gene identities and the CDR3 sequence.</p><p>The probabilistic models considered here differ in the extent to which they explicitly model this process. On one end of the spectrum, the <monospace>OLGA.Q</monospace> model is built on an explicit model of nucleotide VDJ recombination which emulates this process quite carefully, using our knowledge of the germline TCR nucleotide sequences and recombination mechanism (<xref ref-type="bibr" rid="bib28">Murugan et al., 2012</xref>; <xref ref-type="bibr" rid="bib23">Marcou et al., 2018</xref>; <xref ref-type="bibr" rid="bib38">Sethna et al., 2018</xref>). The <monospace>count_match</monospace> model incorporates some of these aspects by making the germline V and J amino acid sequences for each input available to the decoder, and by scoring the degree to which the correct number of CDR3 amino acid positions of the reconstructed sequences match those of their corresponding V and J genes. The <monospace>basic</monospace> model predicts the germline genes and the CDR3 sequences as independent outputs of the VAE, and thus has no built-in prior information on the correlations between the germline genes and CDR3s.</p><p>We can understand the degree to which the models learn the VDJ recombination rules by evaluating them under the <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> recombination-selection model. If the VAE models respect the rules of VDJ recombination, they will generate sequences with a <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> comparable to that of real sequences, while if they do not respect these rules, they should get a low <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace>. This is a stringent criterion: a single amino acid change towards the <inline-formula><mml:math id="inf59"><mml:msup><mml:mn>3</mml:mn><mml:mo>â²</mml:mo></mml:msup></mml:math></inline-formula> end of the CDR3 can cause <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> to drop precipitously. For example, OLGA gives (TRBV5-1, TRBJ2-6, CASSFSGSGANVLTF) a relatively high probability, while the same TCR with a single T switched to Q (TRBV5-1, TRBJ2-6, CASSFSGSGANVL<bold>Q</bold>F) is assigned probability zero.</p><p>To test the modelsâ compliance with the rules of VDJ recombination, we used the data of <xref ref-type="bibr" rid="bib10">De Neuter et al. (2019)</xref>, which consists of TCR <inline-formula><mml:math id="inf61"><mml:mi>Î²</mml:mi></mml:math></inline-formula> sequences from 33 subjects, as follows. We randomly split the data so that the repertoires of 22 subjects were used for training, and the remaining 11 subjectsâ repertoires were used for testing (with one repertoire used for each subject). Each of the 22 training repertoires was randomly downsampled to 20,000 sequences to standardize the contribution of each repertoire to the training set; these samples were then pooled. 100,000 sequences from this pool were randomly selected to train the models, including the <inline-formula><mml:math id="inf62"><mml:mi>Q</mml:mi></mml:math></inline-formula> factor of <monospace>OLGA.Q</monospace>. We then evaluated the distribution of <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> on 10,000 sequences from each of the held-out test repertoires as well as 10,000 sequences generated from each of the three models.</p><p>We found evidence that the VAE models do indeed learn the rules of VDJ recombination (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Although there is slight left skew in the <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> distributions for VAE-generated sequences compared to the <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> distributions of experimental repertoires, the behavior of the VAE-generated distributions reasonably matches the behavior of the experimental distributions. In fact, the <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> distribution for <monospace>OLGA.Q</monospace>-generated sequences seems to exhibit more left skew than the <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> distributions for VAE-generated sequences, although the three distributions are for the most part comparable. Perhaps surprisingly, the <monospace>count_match</monospace> model that encodes germline amino acid information resulted in a very small improvement in terms of recombination probability compared to the <monospace>basic</monospace> model, which does not explicitly encode any dependence between a TCRâs germline gene usage and CDR3 sequence.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.46935.004</object-id><label>Figure 3.</label><caption><title>VAE models generate plausible recombinations according to the <monospace>OLGA.Q</monospace> model, which is built on a model of VDJ recombination.</title><p>Here we show the distribution of log-probability of generation according to the <monospace>OLGA.Q</monospace> model for a panel of sequences from 11 test repertoires (gray) as well as simulated sequences from the <monospace>basic</monospace>, <monospace>count_match</monospace>, and <monospace>OLGA.Q</monospace> models.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig3-v1.tif"/></fig></sec><sec id="s2-4"><title>VAE models generalize to unseen sequences and learn more than a simple <monospace>OLGA.Q</monospace></title><p>Next, we set out to determine whether the VAE models were simply memorizing and regurgitating training sequences. Such behavior is a persistent concern for deep generative models (<xref ref-type="bibr" rid="bib2">Arora et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Arora and Zhang, 2017</xref>). Although the close correspondence between test and train performance in the above frequency estimation suggests model generalization, it does not directly address this issue.</p><p>To evaluate out-of-sample probability estimation, we used the <xref ref-type="bibr" rid="bib10">De Neuter et al. (2019)</xref> data as in the previous section to evaluate <inline-formula><mml:math id="inf68"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> under the <monospace>basic</monospace> model rather than <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace>. If the VAE were regurgitating training sequences, it should consistently assign higher <inline-formula><mml:math id="inf70"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> to sequences it generates compared to held-out test sequences. Instead, we found that the <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> probabilities for VAE-generated sequences closely follow probabilities for test sequences (<xref ref-type="fig" rid="fig4">Figure 4</xref>), for both basic and <monospace>count_match</monospace>. We also observed that the <monospace>OLGA.Q</monospace>-generated sequences are consistently assigned a lower <inline-formula><mml:math id="inf72"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> on average than either test sequences or VAE-generated sequences, indicating the VAE learns characteristics of real sequences not captured by the formulation of <monospace>OLGA.Q</monospace> used here.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.46935.005</object-id><label>Figure 4.</label><caption><title>Sequences generated by the VAE models show a similar distribution of <inline-formula><mml:math id="inf73"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> compared to real sequences.</title><p>Here we show the distribution of the log probability of generation according to the <monospace>OLGA.Q</monospace> model for a panel of sequences from 11 test repertoires (gray) as well as simulated sequences from the <monospace>basic</monospace>, <monospace>count_match</monospace>, and <monospace>OLGA.Q</monospace> models.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig4-v1.tif"/></fig></sec><sec id="s2-5"><title>VAE models generate sequences with similar characteristics to real sequences</title><p>We next sought to quantify the similarity of model-generated sequences to real sequences, for each of the three models in consideration. To accomplish this task, we used the sumrep package (<xref ref-type="bibr" rid="bib29">Olson et al., 2019</xref>)Â (<ext-link ext-link-type="uri" xlink:href="https://github.com/matsengrp/sumrep/">https://github.com/matsengrp/sumrep/</ext-link>), a collaborative effort of the AIRR (<xref ref-type="bibr" rid="bib6">Breden et al., 2017</xref>; <xref ref-type="bibr" rid="bib36">Rubelt et al., 2017</xref>) software working group. This package calculates many summary statistics on immune receptor sequence repertoires and provides functions for comparing these summaries. While these summaries are not of direct interest for this application, they comprise simple and relevant means of summarizing the abstract, high-dimensional distribution of TCRs. Collectively, these summary comparisons allow for robust model validation without appealing to the models themselves for assessment. We found agreement between simulated and test repertoires in some respects, with the performance of the model depending on the summary statistic (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.46935.006</object-id><label>Figure 5.</label><caption><title>Divergences for summary statistics comparing model-generated sequences to held-out repertoire sequences on the <xref ref-type="bibr" rid="bib10">De Neuter et al. (2019)</xref> data set.</title><p>Each colored point represents the divergence of a summary distribution computed on a simulated pool of sequences to the distribution of the same summary on a set of sequences drawn from one of 11 repertoires (<xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>). Each black '+' represents a similar divergence but with a random selection from the training data rather than a simulated pool of sequences. A lower divergence means more similarity with respect to the given summary. The following summary statistics, applied to the CDR3 amino acid sequence, use Jensen-Shannon divergence: acidity, aliphatic index, aromaticity, basicity, bulkiness, length (in amino acids), charge, GRAVY index, nearest neighbor Levenshtein distance, pairwise Levenshtein distance, and polarity. The following summary statistics use <inline-formula><mml:math id="inf74"><mml:msub><mml:mi mathvariant="normal">â</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> divergence: CDR3 amino acid 2mer frequency, CDR3 amino acid frequency, J gene frequency, and V gene frequency.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.46935.007</object-id><label>Figure 5âfigure supplement 1.</label><caption><title>Nearest neighbor Levenshtein distributions on the <xref ref-type="bibr" rid="bib10">De Neuter et al. (2019)</xref> data set.</title><p>Nearest neighbor Levenshtein distance distributions for simulated sequences and test repertoire sequences. Each of the divergences in <xref ref-type="fig" rid="fig4">Figure 4</xref> calculate a divergence between one of the colored lines (a simulated collection of sequences) and one of the gray lines (test repertoire sequences).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.46935.008</object-id><label>Figure 5âfigure supplement 2.</label><caption><title>Summary statistics comparison on a multiple sclerosis data set.</title><p>Analysis as in <xref ref-type="fig" rid="fig4">Figure 4</xref> but instead on the multiple sclerosis samples from <xref ref-type="bibr" rid="bib15">Emerson et al. (2013)</xref>, combining CD4+ and CD8+ sorts. Training was performed on 16 repertoires, with nine repertoires for a held-out test set.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig5-figsupp2-v1.tif"/></fig></fig-group><p>Above we showed how the VAE model learns the rules of VDJ recombination assessed by <monospace>OLGA.Q</monospace> likelihood distributions. Another assessment is to look directly at summary statistics like V/J gene usage, amino acid frequencies and CDR3 length. All models succeeded on J gene frequencies, with the VAE models performing worse in terms of V gene frequency, in particular the <monospace>basic</monospace> model. The models performed similarly in terms of CDR3 summaries, with <monospace>OLGA.Q</monospace> perhaps doing better in terms of CDR3 length, and the VAEs getting the correct distribution of nearest-neighbor distances (<xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>). The amino acid frequencies for the VAE models did not match those of the training data as closely as expected, although in some respects they appear better than <monospace>OLGA.Q</monospace>. Results were broadly consistent when analyzing a second data set (<xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>).</p></sec><sec id="s2-6"><title>The latent space embedding</title><p>We wished to understand the factors that determine the position of TCRs within the latent embedding. To uncover these determinants, we performed standard principal components analysis (PCA) on De Neuter test data embedded in the VAE latent space. This reduces the 20-dimensional latent space embedding to the two dimensions which account for the largest variability in the data.</p><p>We found that this projection is structured according to V and J gene identity (<xref ref-type="fig" rid="fig6">Figure 6</xref>). In particular, the V gene determines one axis of the principal components projection, while the J gene determines another. In order to learn the next level of organization, we restricted the embedded TCR sequences to those using the most popular V and J genes â TCRBV30-01 and TCRBJ01-02 â and re-did the projection. This additional projection showed that CDR3 length was an important determinant of embedding location (<xref ref-type="fig" rid="fig6s1">Figure 6âfigure supplement 1</xref>).</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.46935.009</object-id><label>Figure 6.</label><caption><title>Principal components analysis (PCA) on the De Neuter test data embedded into the 20 dimensional latent space, colored by (<bold>a</bold>) V gene and (<bold>b</bold>) J gene.</title><p>Panel (<bold>a</bold>) is limited to the seven most popular V genes.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.46935.010</object-id><label>Figure 6âfigure supplement 1.</label><caption><title>PCA of the most popular V and J genes showing embedding of CDR3 length.</title><p>PCA projection of embedded TCR sequences using the most popular V and J genes: TCRBV30-01 and TCRBJ01-02. When we do such a restriction, we see CDR3 length showing up as determining an axis in the latent space.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig6-figsupp1-v1.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Probabilistic models of immune repertoires are powerful tools, with applications to finding disease-responsive TCRs (<xref ref-type="bibr" rid="bib33">Pogorelyy et al., 2018c</xref>) and analyzing the forces dictating TCR sharing (<xref ref-type="bibr" rid="bib14">Elhanati et al., 2018</xref>), among others. In this paper we applied deep learning to model TCR <inline-formula><mml:math id="inf75"><mml:mi>Î²</mml:mi></mml:math></inline-formula> repertoires, and used the resultant models to gain meaningful insights. Specifically, we use a semiparametric method that makes a single weak assumption: that there exists some small number of latent parameters that can be used to generate to the observed distribution. We make no assumptions about the function mapping from these parameters to the high-dimensional distribution space and learn it from the data. We have learned that this biology-agnostic approach can provide good results, even when compared to a previous approach that formalizes the considerable biological knowledge we have concerning the mechanism of VDJ recombination.</p><p>We find that these models have the following interesting features.</p><list list-type="order"><list-item><p>These models yield better in-sample and out-of-sample performance for cohort frequency estimation compared to an existing recombination and selection model.</p></list-item><list-item><p>They generalize well by learning features of real TCR repertoires, which allows them to differentiate between experimental repertoires and repertoires generated from the recombination and selection model.</p></list-item><list-item><p>They generate simulated repertoires that are similar to real TCR repertoires.</p></list-item><list-item><p>By leveraging powerful deep learning libraries, they can be expressed and implemented very simply with small amounts of specialized computer programming. The <monospace>basic</monospace> model, for example, is implemented in about 100 lines of Python code.</p></list-item></list><p>Furthermore, our efforts to inject biological knowledge into the deep learning framework did not significantly improve performance.</p><p>However, these models also have some important drawbacks. Most importantly, as is often the case for models parametrized by neural networks, these models are not directly interpretable. Although we have identified some structure in the latent space, further details may be difficult to ascertain. Besides the difficulty in interpreting the neural network weights, we did not engineer the model architectures with mechanism in mind. In addition, the models operate on amino acid sequences and thus cannot shed light on the VDJ recombination process, which operates at the nucleotide level. We also note that <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula>, which relies on importance sampling, is more expensive to compute than <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace>.</p><p>The results presented here offer some interesting lessons concerning future development of deep probabilistic models for immune repertoires. Our model that had no a priori information about germline gene sequence performs very similarly, even when evaluated in terms of VDJ recombination likelihood, to one that deliberately attempts to recapitulate the amount of matching between germline gene and CDR3 amino acid sequences and includes germline CDR3 sequences in the untrained model. This may indicate that, given the volume of sequence data available, we should focus our efforts on the abstract problem of density estimation on the set of TCRs, rather than incorporating biological knowledge into our deep learning models.</p><p>Although we performed a preliminary analysis of the latent embedding, this exclusively involved sequence characteristics directly available to the model. In future work, we hope to further unravel this embedding by comparing repertoires in the latent space, and by comparing sequences labeled with external characteristics. We also plan to deliver a pre-trained model that will enable biologists to evaluate the probability of seeing a naive B cell receptor (BCR) or TCR in a given population. Here we have restricted our attention to TCR <inline-formula><mml:math id="inf78"><mml:mi>Î²</mml:mi></mml:math></inline-formula> sequences, however, our methods apply with no modification to TCR <inline-formula><mml:math id="inf79"><mml:mi>Î±</mml:mi></mml:math></inline-formula> chains. Contrasting the <inline-formula><mml:math id="inf80"><mml:mi>Î±</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf81"><mml:mi>Î²</mml:mi></mml:math></inline-formula> chains may yield interesting insights on the differences between the two generation processes. The most interesting insights will come from jointly modeling the two chains using large-scale <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>â¢</mml:mo><mml:mi>Î²</mml:mi></mml:mrow></mml:math></inline-formula> paired TCR sequencing (<xref ref-type="bibr" rid="bib19">Howie et al., 2015</xref>), which is a more complex process.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Data</title><p>Our goal was to model probability distributions on TCR <inline-formula><mml:math id="inf83"><mml:mi>Î²</mml:mi></mml:math></inline-formula> chain protein sequences. By the process of VDJ recombination, these sequences are uniquely determined by V and J gene identities and CDR3 amino acid sequence. Thus, for the purpose of this paper, we exclusively used triples of V gene, J gene, CDR3 amino acid sequence to represent TCR protein sequences.</p><p>All data was downloaded from <ext-link ext-link-type="uri" xlink:href="https://clients.adaptivebiotech.com/immuneaccess">https://clients.adaptivebiotech.com/immuneaccess</ext-link>.Â We preprocessed the data to exclude sequences:</p><list list-type="order"><list-item><p>from an out-of-frame rearrangement</p></list-item><list-item><p>with a CDR3 that does not begin with the characteristic C or end with an F or YV</p></list-item><list-item><p>with a CDR3 longer than 30 amino acids</p></list-item><list-item><p>with an ambiguous V or J gene call.</p></list-item></list><p>We also excluded any TCRs with TCRBJ02-05, which the internal Adaptive pipeline annotates incorrectly, and TCRBJ02-07, to which the default OLGA model assigns artifactually low probabilities. Model design and parameter tuning, including the sizes of hidden layers and the dimension of the latent space, was performed using the data of <xref ref-type="bibr" rid="bib11">DeWitt et al. (2018)</xref>. On this data we endeavored to decrease model size without incurring loss on held-out data within this data set. We found that the model was relatively robust to parameter perturbations as long as the number of parameters was not too small. Model evaluation was performed using the data sets described in the Results section.</p></sec><sec id="s4-2"><title>Encoding TCR sequences</title><p>The CDR3 sequences were padded with gaps in the middle so that they are a fixed length of 30 amino acid/gap characters. Thus there is an equal number of amino acids on either end of the gaps for even length CDR3s, with one extra on the left side for odd length CDR3s. This resulting sequence is âone-hot encoded,â meaning that each amino acid at each site is represented with 0/1 for absence/presence, with an additional dimension for âgapâ to make a 21-dimensional space (<xref ref-type="fig" rid="fig7">Figure 7</xref>). V and J genes are similarly encoded in 67- and 13-dimensional vectors, respectively, and all of these vectors are concatenated into a single large encoding vector. This vector is mapped to a latent embedding via a linear transformation that is learned during training (<xref ref-type="bibr" rid="bib5">Biswas et al., 2018</xref>). In our case, there is one transformation for the V genes, one for the J genes, and one for amino acids. These transformations do not change dimension except for V gene identities, which are projected to a 30-dimensional space (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.46935.011</object-id><label>Figure 7.</label><caption><title>Encoding/transforming TCR sequences.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig7-v1.tif"/></fig></sec><sec id="s4-3"><title>Models</title><p>Here we describe the <monospace>basic</monospace> and <monospace>count_match</monospace> models in detail. They are not exactly VAEs as originally defined in <xref ref-type="bibr" rid="bib20">Kingma et al. (2014b)</xref> for two reasons. First, they are better categorized as <inline-formula><mml:math id="inf84"><mml:mi>Î²</mml:mi></mml:math></inline-formula>-VAEs since they include a weight on the Kullback-Leibler divergence term of the training objective (<xref ref-type="bibr" rid="bib18">Higgins et al., 2017</xref>). Namely, the loss is<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">â</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>Î²</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>KL</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt">â¥</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf85"><mml:mi class="ltx_font_mathcaligraphic">â</mml:mi></mml:math></inline-formula> is the reconstruction loss for <inline-formula><mml:math id="inf86"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> encoded as <inline-formula><mml:math id="inf87"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> (details below).</p><p>Second, they have multiple outputs that are scored by separate reconstruction loss functions. Our reconstruction loss is a linear combination of these loss functions. For example, the simplest â<monospace>basic</monospace>â model produces three outputs: one for the V gene, one for the J gene, and one for the CDR3 sequence. It has two densely-connected layers for the encoder and two for the decoder (<xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref>). The V and J gene identities are scored using categorical cross-entropy, while the CDR3 sequence is scored by the average categorical cross-entropy across sites.</p><p>The <monospace>count_match</monospace> model includes TCR germline information in the untrained model in such a way that it can count the number of V-germline-matching amino acids on the <inline-formula><mml:math id="inf88"><mml:msup><mml:mn>5</mml:mn><mml:mo>â²</mml:mo></mml:msup></mml:math></inline-formula> end of the CDR3 and the number of J-germline-matching amino acids on the <inline-formula><mml:math id="inf89"><mml:msup><mml:mn>3</mml:mn><mml:mo>â²</mml:mo></mml:msup></mml:math></inline-formula> end of the CDR3 (<xref ref-type="fig" rid="fig1s2">Figure 1âfigure supplement 2</xref>). Its loss function includes a component that scores these counts in terms of two-dimensional squared loss. This model also contains an explicit loss component for CDR3 length, which is also evaluated via squared loss.</p><p>We combine the multiple losses within each model into a weighted linear combination which yields a single overall reconstruction loss function for optimization. Weights were determined by multivariate linear regression, minimizing the squared difference between the log likelihood and this reconstruction loss on a validation set (see next section for definition of the validation set used in training). This resulted in a marginal improvement in performance on the (<xref ref-type="bibr" rid="bib11">DeWitt et al., 2018</xref>) data and fitting was not done again. Due to these modifications, our loss function cannot be interpreted in terms of the variational evidence lower bound (ELBO).</p></sec><sec id="s4-4"><title>Training</title><p>For the purposes of fitting, the training data was split into true-training and validation sets: the former was used for fitting, while the latter was used to assess error during training (which provided a stopping criterion). âTestâ data was completely held out from the training procedure.</p><p>Inspired by the work of <xref ref-type="bibr" rid="bib40">SÃ¸nderby et al. (2016)</xref>, we implemented a <inline-formula><mml:math id="inf90"><mml:mi>Î²</mml:mi></mml:math></inline-formula> schedule during training such that training begins with <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>Î²</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and then linearly increases every training epoch until its final value. We extended this procedure by implementing a collection of pre-training phases that start with randomized weights and train for a fixed number of epochs using the <inline-formula><mml:math id="inf92"><mml:mi>Î²</mml:mi></mml:math></inline-formula> schedule. The optimal weights, according to the validation loss, were used as the starting weights for a full optimization, which terminates when validation loss does not improve for a fixed number of epochs or until a maximum number of epochs is reached. Training was done using the Adam optimizer <xref ref-type="bibr" rid="bib21">Kingma and Ba (2014a)</xref> implemented in Keras (<xref ref-type="bibr" rid="bib7">Chollet, 2015</xref>).</p></sec><sec id="s4-5"><title>Picking <inline-formula><mml:math id="inf93"><mml:mi>Î²</mml:mi></mml:math></inline-formula></title><p>As described above, our complete loss function is a sum of a reconstruction loss plus <inline-formula><mml:math id="inf94"><mml:mi>Î²</mml:mi></mml:math></inline-formula> times a Kullback-Leibler (KL) divergence term <inline-formula><mml:math id="inf95"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>KL</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt">â¥</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> describing the divergence of the probabilistic encoder map <inline-formula><mml:math id="inf96"><mml:mi>q</mml:mi></mml:math></inline-formula> to the prior <inline-formula><mml:math id="inf97"><mml:mi>p</mml:mi></mml:math></inline-formula>. This KL divergence term regularizes the optimization by encouraging a structured embedding of TCRs.</p><p>We tested the <inline-formula><mml:math id="inf98"><mml:mi>Î²</mml:mi></mml:math></inline-formula> parameter in an evenly distributed range with seven values from 0.625 to 1 on the <xref ref-type="bibr" rid="bib11">DeWitt et al. (2018)</xref> data set. We found that <inline-formula><mml:math id="inf99"><mml:mi>Î²</mml:mi></mml:math></inline-formula> slightly impacted <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> when evaluated on test sequences, with larger values of <inline-formula><mml:math id="inf101"><mml:mi>Î²</mml:mi></mml:math></inline-formula> being slightly preferred (<xref ref-type="fig" rid="fig8">Figure 8</xref>). On the other hand, <inline-formula><mml:math id="inf102"><mml:mi>Î²</mml:mi></mml:math></inline-formula> strongly impacted the agreement of summary statistics of generated sequences with observed sequences in test repertoires (<xref ref-type="fig" rid="fig9">Figure 9</xref>). To balance these evaluative and generative objectives, we fixed <inline-formula><mml:math id="inf103"><mml:mi>Î²</mml:mi></mml:math></inline-formula> to be 0.75. This choice was confirmed by running the same analysis on the data of <xref ref-type="bibr" rid="bib15">Emerson et al. (2013)</xref> (<xref ref-type="fig" rid="fig9s1">Figure 9âfigure supplement 1</xref>) and <xref ref-type="bibr" rid="bib10">De Neuter et al. (2019)</xref> (<xref ref-type="fig" rid="fig9s2">Figure 9âfigure supplement 2</xref>), both of which yielded similar results.</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.46935.012</object-id><label>Figure 8.</label><caption><title>The effect of <inline-formula><mml:math id="inf104"><mml:mi>Î²</mml:mi></mml:math></inline-formula> on <inline-formula><mml:math id="inf105"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> evaluated on test sequences for the data of <xref ref-type="bibr" rid="bib11">DeWitt et al. (2018)</xref>, overall (<bold>a</bold>) and near the peak (<bold>b</bold>).</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig8-v1.tif"/></fig><fig-group><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.46935.013</object-id><label>Figure 9.</label><caption><title>The effect of <inline-formula><mml:math id="inf106"><mml:mi>Î²</mml:mi></mml:math></inline-formula> on summary divergences between generated sequences and observed test sequences as in <xref ref-type="fig" rid="fig4">Figure 4</xref>, using the data of <xref ref-type="bibr" rid="bib11">DeWitt et al. (2018)</xref>.</title><p><monospace>OLGA.Q</monospace> is also run separately for each <inline-formula><mml:math id="inf107"><mml:mi>Î²</mml:mi></mml:math></inline-formula> value; because <inline-formula><mml:math id="inf108"><mml:mi>Î²</mml:mi></mml:math></inline-formula> has no influence on <monospace>OLGA.Q</monospace>, the observed variation is simply due to differences between random samples.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig9-v1.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.46935.014</object-id><label>Figure 9âfigure supplement 1.</label><caption><title>Summary statistic divergences by <inline-formula><mml:math id="inf109"><mml:mi>Î²</mml:mi></mml:math></inline-formula> on the data of <xref ref-type="bibr" rid="bib15">Emerson et al. (2013)</xref>.</title><p>Analysis identical to that in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig9-figsupp1-v1.tif"/></fig><fig id="fig9s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.46935.015</object-id><label>Figure 9âfigure supplement 2.</label><caption><title>Summary statistic divergences for each on the <xref ref-type="bibr" rid="bib10">De Neuter et al. (2019)</xref>.</title><p>Analysis identical to that in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46935-fig9-figsupp2-v1.tif"/></fig></fig-group></sec><sec id="s4-6"><title>Importance sampling</title><p><inline-formula><mml:math id="inf110"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> denotes the probability <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the VAE generating <inline-formula><mml:math id="inf112"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> when decoding a sample from the prior in the latent space. In principle we could calculate this as the expectation of <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf114"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula> is drawn from <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, but this would be very inefficient.</p><p>Instead, we use importance sampling, calculating<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ð¼</mml:mi><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â¢</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here</p><list list-type="bullet"><list-item><p><inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a sample from a multivariate normal with mean and variance determined by the encoder</p></list-item><list-item><p><inline-formula><mml:math id="inf117"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability of generating a given sequence from the decoded version of <inline-formula><mml:math id="inf118"><mml:mi mathvariant="bold">ð³</mml:mi></mml:math></inline-formula>: a product of categorical probabilities</p></list-item><list-item><p><inline-formula><mml:math id="inf119"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð³</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the prior on the latent space</p></list-item></list><p>We found that 100 iterations of importance sampling yielded stable <inline-formula><mml:math id="inf120"><mml:msub><mml:mi>P</mml:mi><mml:mi>VAE</mml:mi></mml:msub></mml:math></inline-formula> estimates for our data, but used 500 iterations in the results presented here to ensure convergence.</p></sec><sec id="s4-7"><title>OLGA and selection model</title><p>We used OLGA (<xref ref-type="bibr" rid="bib38">Sethna et al., 2018</xref>) with its default model parameters to evaluate recombination probabilities. We layered a selection model on top of this recombination model via a multiplicative factor <inline-formula><mml:math id="inf121"><mml:mi>Q</mml:mi></mml:math></inline-formula>, parameterized in terms of triples consisting of V gene identity, J gene identity, and CDR3 length. The roughly 14,000 parameters of this selection model <inline-formula><mml:math id="inf122"><mml:mi>Q</mml:mi></mml:math></inline-formula> were estimated from the same training data used to train the VAE in each case. As derived in the supplementary material of <xref ref-type="bibr" rid="bib13">Elhanati et al. (2014)</xref>, the maximum likelihood estimate of <inline-formula><mml:math id="inf123"><mml:mi>Q</mml:mi></mml:math></inline-formula> for a given triple is the ratio of the empirical frequency of the triple in the data to the probability of observing the triple based on the recombination model. We truncated this ratio at 100 for numerical stability. We then used rejection sampling to sample from the corresponding <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><monospace><sub>OLGA.Q</sub></monospace> distribution. Code for estimating the <monospace>OLGA.Q</monospace> model parameters is included in our software package.</p></sec><sec id="s4-8"><title>Implementation and pipeline</title><p>We implemented our models in a modular fashion with extensive comments so that others can understand, reproduce, and build upon our work. Code and pipelines are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/matsengrp/vampire/">https://github.com/matsengrp/vampire/</ext-link>Â (<xref ref-type="bibr" rid="bib24">Matsen, 2019a</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/vampire">https://github.com/elifesciences-publications/vampire</ext-link>), while scripts and Jupyter notebooks (<xref ref-type="bibr" rid="bib22">Kluyver et al., 2016</xref>) specific to this paper are available atÂ <ext-link ext-link-type="uri" xlink:href="https://github.com/matsengrp/vampire-analysis-1/">https://github.com/matsengrp/vampire-analysis-1/</ext-link>Â (<xref ref-type="bibr" rid="bib25">Matsen, 2019b</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/vampire-analysis-1">https://github.com/elifesciences-publications/vampire-analysis-1</ext-link>). All models were implemented in Python 3.6 using Keras 2.2.4 (<xref ref-type="bibr" rid="bib7">Chollet, 2015</xref>) and the Tensorflow 1.11.0 backend (<xref ref-type="bibr" rid="bib1">Abadi et al., 2015</xref>). Our pipeline is written with SCons (<ext-link ext-link-type="uri" xlink:href="https://scons.org">https://scons.org</ext-link>) and nestly (<ext-link ext-link-type="uri" xlink:href="https://pythonhosted.org/nestly/">https://pythonhosted.org/nestly/</ext-link>; <xref ref-type="bibr" rid="bib26">McCoy et al., 2013</xref>). The sumrep package depends heavily on the Immcantation framework (<ext-link ext-link-type="uri" xlink:href="https://immcantation.readthedocs.io/">https://immcantation.readthedocs.io/</ext-link>; <xref ref-type="bibr" rid="bib17">Gupta et al., 2015</xref>).</p><p>The following tools were also especially helpful:</p><list list-type="bullet"><list-item><p>Biopython (<xref ref-type="bibr" rid="bib8">Cock et al., 2009</xref>)</p></list-item><list-item><p>cowplot (<xref ref-type="bibr" rid="bib43">Wilke, 2018</xref>)</p></list-item><list-item><p>ggplot2 (<xref ref-type="bibr" rid="bib42">Wickham, 2016</xref>)</p></list-item><list-item><p>GNU parallel (<xref ref-type="bibr" rid="bib41">Tange, 2018</xref>)</p></list-item><list-item><p>pandas (<xref ref-type="bibr" rid="bib27">McKinney, 2010</xref>)</p></list-item><list-item><p>scikit-learn (<xref ref-type="bibr" rid="bib30">Pedregosa et al., 2011</xref>).</p></list-item></list></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We would like to thank Sam Sinai and Cheng Zhang for helpful discussions, Thierry Mora, Aleks Walczak, and Zachary Sethna for assistance with OLGA and the Q model, the AIRR software working group for their contributions to sumrep, Fred Hutch scientific computing, especially Michael Gutteridge and Dirk Petersen, and Adaptive Biotechnologies for hosting and sharing TCR data.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Methodology, Writingâoriginal draft, Writingâreview and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Formal analysis, Visualization, Methodology, Writingâoriginal draft, Writingâreview and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Methodology, Writingâoriginal draft, Writingâreview and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Software, Methodology, Writingâoriginal draft, Writingâreview and editing</p></fn><fn fn-type="con" id="con5"><p>Software, Methodology</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Software, Formal analysis, Supervision, Methodology, Writingâoriginal draft, Writingâreview and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Visualization, Methodology, Writingâoriginal draft, Writingâreview and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.46935.019</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-46935-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Raw data (TCR sequences) is available at immuneACCESS: <ext-link ext-link-type="uri" xlink:href="https://clients.adaptivebiotech.com/pub/emerson-2013-jim">https://clients.adaptivebiotech.com/pub/emerson-2013-jim</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://clients.adaptivebiotech.com/pub/emerson-2017-natgen">https://clients.adaptivebiotech.com/pub/emerson-2017-natgen</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://clients.adaptivebiotech.com/pub/seshadri-2018-journalofimmunology">https://clients.adaptivebiotech.com/pub/seshadri-2018-journalofimmunology</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://clients.adaptivebiotech.com/pub/deneuter-2018-cmvserostatus">https://clients.adaptivebiotech.com/pub/deneuter-2018-cmvserostatus</ext-link>. Processed data is available through Zenodo: <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/2619576#.XKElTrfYphE">https://zenodo.org/record/2619576#.XKElTrfYphE</ext-link>. Code and instructions for reproducing figures is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/matsengrp/vampire-analysis-1">https://github.com/matsengrp/vampire-analysis-1</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/vampire-analysis-1">https://github.com/elifesciences-publications/vampire-analysis-1</ext-link>). Code to process data and run VAE is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/matsengrp/vampire/">https://github.com/matsengrp/vampire/</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/vampire">https://github.com/elifesciences-publications/vampire</ext-link>).</p><p>The following previously published datasets were used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Emerson</surname><given-names>R</given-names></name><name><surname>Sherwood</surname><given-names>A</given-names></name><name><surname>Desmarais</surname><given-names>C</given-names></name><name><surname>Malhotra</surname><given-names>S</given-names></name><name><surname>Phippard</surname><given-names>D</given-names></name><name><surname>Robins</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><data-title>Estimating the ratio of CD4+ to CD8+ T cells using high-throughput sequence data</data-title><source>immuneACCESS</source><pub-id assigning-authority="other" pub-id-type="doi">10.21417/B7H01M</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Emerson</surname><given-names>R</given-names></name><name><surname>DeWitt</surname><given-names>W</given-names></name><name><surname>Vignali</surname><given-names>M</given-names></name><name><surname>Gravley</surname><given-names>J</given-names></name><name><surname>Hu</surname><given-names>J</given-names></name><name><surname>Osborne</surname><given-names>E</given-names></name><name><surname>Desmarais</surname><given-names>C</given-names></name><name><surname>Klinger</surname><given-names>M</given-names></name><name><surname>Carlson</surname><given-names>C</given-names></name><name><surname>Hansen</surname><given-names>J</given-names></name><name><surname>Rieder</surname><given-names>M</given-names></name><name><surname>Robins</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Immunosequencing identifies signatures of cytomegalovirus exposure history and HLA-mediated effects on the T-cell repertoire</data-title><source>immuneACCESS</source><pub-id assigning-authority="other" pub-id-type="doi">10.21417/B7001Z</pub-id></element-citation></p><p><element-citation id="dataset3" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>DeWitt</surname><given-names>WS</given-names></name><name><surname>Yu</surname><given-names>KKQ</given-names></name><name><surname>Wilburn</surname><given-names>DB</given-names></name><name><surname>Sherwood</surname><given-names>A</given-names></name><name><surname>Vignali</surname><given-names>M</given-names></name><name><surname>Day</surname><given-names>CL</given-names></name><name><surname>Scriba</surname><given-names>TJ</given-names></name><name><surname>Robins</surname><given-names>HS</given-names></name><name><surname>Swanson</surname><given-names>WJ</given-names></name><name><surname>Emerson</surname><given-names>RO</given-names></name><name><surname>Bradley</surname><given-names>PH</given-names></name><name><surname>Seshadri</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>A diverse lipid antigen-specific T cell receptor repertoire is clonally expanded during active tuberculosis</data-title><source>immuneACCESS</source><pub-id assigning-authority="other" pub-id-type="doi">10.21417/B7QG66</pub-id></element-citation></p><p><element-citation id="dataset4" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>De</surname><given-names>Neuter N</given-names></name><name><surname>Bartholomeus</surname><given-names>E</given-names></name><name><surname>Elias</surname><given-names>G</given-names></name><name><surname>Keersmaekers</surname><given-names>N</given-names></name><name><surname>Suls</surname><given-names>A</given-names></name><name><surname>Jansens</surname><given-names>H</given-names></name><name><surname>Smits</surname><given-names>E</given-names></name><name><surname>Hens</surname><given-names>N</given-names></name><name><surname>Beutels</surname><given-names>P</given-names></name><name><surname>Van</surname><given-names>Damme P</given-names></name><name><surname>Mortier</surname><given-names>G</given-names></name><name><surname>Van</surname><given-names>Tendeloo V</given-names></name><name><surname>Laukens</surname><given-names>K</given-names></name><name><surname>Meysman</surname><given-names>P</given-names></name><name><surname>Ogunjimi</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Memory CD4+ T cell receptor repertoire data mining as a tool for identifying cytomegalovirus serostatus</data-title><source>immuneACCESS</source><pub-id assigning-authority="other" pub-id-type="doi">10.21417/B7R91W</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Abadi</surname> <given-names>M</given-names></name><name><surname>Agarwal</surname> <given-names>A</given-names></name><name><surname>Barham</surname> <given-names>P</given-names></name><name><surname>Brevdo</surname> <given-names>E</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Citro</surname> <given-names>C</given-names></name><name><surname>Corrado</surname> <given-names>GS</given-names></name><name><surname>Davis</surname> <given-names>A</given-names></name><name><surname>Dean</surname> <given-names>J</given-names></name><name><surname>Devin</surname> <given-names>M</given-names></name><name><surname>Ghemawat</surname> <given-names>S</given-names></name><name><surname>Goodfellow</surname> <given-names>I</given-names></name><name><surname>Harp</surname> <given-names>A</given-names></name><name><surname>Irving</surname> <given-names>G</given-names></name><name><surname>Isard</surname> <given-names>M</given-names></name><name><surname>Jia</surname> <given-names>Y</given-names></name><name><surname>Jozefowicz</surname> <given-names>R</given-names></name><name><surname>Kaiser</surname> <given-names>L</given-names></name><name><surname>Kudlur</surname> <given-names>M</given-names></name><name><surname>Levenberg</surname> <given-names>J</given-names></name><name><surname>ManÃ©</surname> <given-names>D</given-names></name><name><surname>Monga</surname> <given-names>R</given-names></name><name><surname>Moore</surname> <given-names>S</given-names></name><name><surname>Murray</surname> <given-names>D</given-names></name><name><surname>Olah</surname> <given-names>C</given-names></name><name><surname>Schuster</surname> <given-names>M</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Steiner</surname> <given-names>B</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Talwar</surname> <given-names>K</given-names></name><name><surname>Tucker</surname> <given-names>P</given-names></name><name><surname>Vanhoucke</surname> <given-names>V</given-names></name><name><surname>Vasudevan</surname> <given-names>V</given-names></name><name><surname>ViÃ©gas</surname> <given-names>F</given-names></name><name><surname>Vinyals</surname> <given-names>O</given-names></name><name><surname>Warden</surname> <given-names>P</given-names></name><name><surname>Wattenberg</surname> <given-names>M</given-names></name><name><surname>Wicke</surname> <given-names>M</given-names></name><name><surname>Yu</surname> <given-names>Y</given-names></name><name><surname>Zheng</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>TensorFlow: Large-scale machine learning on heterogeneous systems</data-title><ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">https://www.tensorflow.org/</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Arora</surname> <given-names>S</given-names></name><name><surname>Ge</surname> <given-names>R</given-names></name><name><surname>Liang</surname> <given-names>Y</given-names></name><name><surname>Ma</surname> <given-names>T</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Generalization and equilibrium in generative adversarial nets (GANs)</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1703.00573">https://arxiv.org/abs/1703.00573</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Arora</surname> <given-names>S</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Do GANs actually learn the distribution?Â An empirical study</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.08224">https://arxiv.org/abs/1706.08224</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bassing</surname> <given-names>CH</given-names></name><name><surname>Swat</surname> <given-names>W</given-names></name><name><surname>Alt</surname> <given-names>FW</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The mechanism and regulation of chromosomal V(D)J recombination</article-title><source>Cell</source><volume>109 Suppl</volume><fpage>S45</fpage><lpage>S55</lpage><pub-id pub-id-type="doi">10.1016/S0092-8674(02)00675-X</pub-id><pub-id pub-id-type="pmid">11983152</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Biswas</surname> <given-names>S</given-names></name><name><surname>Kuznetsov</surname> <given-names>G</given-names></name><name><surname>Ogden</surname> <given-names>PJ</given-names></name><name><surname>Conway</surname> <given-names>NJ</given-names></name><name><surname>Adams</surname> <given-names>RP</given-names></name><name><surname>Church</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Toward machine-guided design of proteins</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/337154</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breden</surname> <given-names>F</given-names></name><name><surname>Luning Prak</surname> <given-names>ET</given-names></name><name><surname>Peters</surname> <given-names>B</given-names></name><name><surname>Rubelt</surname> <given-names>F</given-names></name><name><surname>Schramm</surname> <given-names>CA</given-names></name><name><surname>Busse</surname> <given-names>CE</given-names></name><name><surname>Vander Heiden</surname> <given-names>JA</given-names></name><name><surname>Christley</surname> <given-names>S</given-names></name><name><surname>Bukhari</surname> <given-names>SAC</given-names></name><name><surname>Thorogood</surname> <given-names>A</given-names></name><name><surname>Matsen Iv</surname> <given-names>FA</given-names></name><name><surname>Wine</surname> <given-names>Y</given-names></name><name><surname>Laserson</surname> <given-names>U</given-names></name><name><surname>Klatzmann</surname> <given-names>D</given-names></name><name><surname>Douek</surname> <given-names>DC</given-names></name><name><surname>Lefranc</surname> <given-names>MP</given-names></name><name><surname>Collins</surname> <given-names>AM</given-names></name><name><surname>Bubela</surname> <given-names>T</given-names></name><name><surname>Kleinstein</surname> <given-names>SH</given-names></name><name><surname>Watson</surname> <given-names>CT</given-names></name><name><surname>Cowell</surname> <given-names>LG</given-names></name><name><surname>Scott</surname> <given-names>JK</given-names></name><name><surname>Kepler</surname> <given-names>TB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reproducibility and reuse of adaptive immune receptor repertoire data</article-title><source>Frontiers in Immunology</source><volume>8</volume><elocation-id>1418</elocation-id><pub-id pub-id-type="doi">10.3389/fimmu.2017.01418</pub-id><pub-id pub-id-type="pmid">29163494</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Chollet</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Keras</data-title><ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cock</surname> <given-names>PJ</given-names></name><name><surname>Antao</surname> <given-names>T</given-names></name><name><surname>Chang</surname> <given-names>JT</given-names></name><name><surname>Chapman</surname> <given-names>BA</given-names></name><name><surname>Cox</surname> <given-names>CJ</given-names></name><name><surname>Dalke</surname> <given-names>A</given-names></name><name><surname>Friedberg</surname> <given-names>I</given-names></name><name><surname>Hamelryck</surname> <given-names>T</given-names></name><name><surname>Kauff</surname> <given-names>F</given-names></name><name><surname>Wilczynski</surname> <given-names>B</given-names></name><name><surname>de Hoon</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Biopython: freely available Python tools for computational molecular biology and bioinformatics</article-title><source>Bioinformatics</source><volume>25</volume><fpage>1422</fpage><lpage>1423</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btp163</pub-id><pub-id pub-id-type="pmid">19304878</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Davidson</surname> <given-names>TR</given-names></name><name><surname>Falorsi</surname> <given-names>L</given-names></name><name><surname>De Cao</surname> <given-names>N</given-names></name><name><surname>Kipf</surname> <given-names>T</given-names></name><name><surname>Tomczak</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Hyperspherical variational Auto-Encoders</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1804.00891">http://arxiv.org/abs/1804.00891</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Neuter</surname> <given-names>N</given-names></name><name><surname>Bartholomeus</surname> <given-names>E</given-names></name><name><surname>Elias</surname> <given-names>G</given-names></name><name><surname>Keersmaekers</surname> <given-names>N</given-names></name><name><surname>Suls</surname> <given-names>A</given-names></name><name><surname>Jansens</surname> <given-names>H</given-names></name><name><surname>Smits</surname> <given-names>E</given-names></name><name><surname>Hens</surname> <given-names>N</given-names></name><name><surname>Beutels</surname> <given-names>P</given-names></name><name><surname>Van Damme</surname> <given-names>P</given-names></name><name><surname>Mortier</surname> <given-names>G</given-names></name><name><surname>Van Tendeloo</surname> <given-names>V</given-names></name><name><surname>Laukens</surname> <given-names>K</given-names></name><name><surname>Meysman</surname> <given-names>P</given-names></name><name><surname>Ogunjimi</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Memory CD4<sup>+</sup> T cell receptor repertoire data mining as a tool for identifying Cytomegalovirus serostatus</article-title><source>Genes &amp; Immunity</source><volume>20</volume><fpage>255</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1038/s41435-018-0035-y</pub-id><pub-id pub-id-type="pmid">29904098</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeWitt</surname> <given-names>WS</given-names></name><name><surname>Yu</surname> <given-names>KKQ</given-names></name><name><surname>Wilburn</surname> <given-names>DB</given-names></name><name><surname>Sherwood</surname> <given-names>A</given-names></name><name><surname>Vignali</surname> <given-names>M</given-names></name><name><surname>Day</surname> <given-names>CL</given-names></name><name><surname>Scriba</surname> <given-names>TJ</given-names></name><name><surname>Robins</surname> <given-names>HS</given-names></name><name><surname>Swanson</surname> <given-names>WJ</given-names></name><name><surname>Emerson</surname> <given-names>RO</given-names></name><name><surname>Bradley</surname> <given-names>PH</given-names></name><name><surname>Seshadri</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A diverse lipid Antigen-Specific TCR repertoire is clonally expanded during active tuberculosis</article-title><source>The Journal of Immunology</source><volume>201</volume><fpage>888</fpage><lpage>896</lpage><pub-id pub-id-type="doi">10.4049/jimmunol.1800186</pub-id><pub-id pub-id-type="pmid">29914888</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dilokthanakul</surname> <given-names>N</given-names></name><name><surname>Mediano</surname> <given-names>PAM</given-names></name><name><surname>Garnelo</surname> <given-names>M</given-names></name><name><surname>Lee</surname> <given-names>MCH</given-names></name><name><surname>Salimbeni</surname> <given-names>H</given-names></name><name><surname>Arulkumaran</surname> <given-names>K</given-names></name><name><surname>Shanahan</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep unsupervised clustering with gaussian mixture variational autoencoders</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1611.02648">http://arxiv.org/abs/1611.02648</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elhanati</surname> <given-names>Y</given-names></name><name><surname>Murugan</surname> <given-names>A</given-names></name><name><surname>Callan</surname> <given-names>CG</given-names></name><name><surname>Mora</surname> <given-names>T</given-names></name><name><surname>Walczak</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Quantifying selection in immune receptor repertoires</article-title><source>PNAS</source><volume>111</volume><fpage>9875</fpage><lpage>9880</lpage><pub-id pub-id-type="doi">10.1073/pnas.1409572111</pub-id><pub-id pub-id-type="pmid">24941953</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elhanati</surname> <given-names>Y</given-names></name><name><surname>Sethna</surname> <given-names>Z</given-names></name><name><surname>Callan</surname> <given-names>CG</given-names></name><name><surname>Mora</surname> <given-names>T</given-names></name><name><surname>Walczak</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Predicting the spectrum of TCR repertoire sharing with a data-driven model of recombination</article-title><source>Immunological Reviews</source><volume>284</volume><fpage>167</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1111/imr.12665</pub-id><pub-id pub-id-type="pmid">29944757</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emerson</surname> <given-names>R</given-names></name><name><surname>Sherwood</surname> <given-names>A</given-names></name><name><surname>Desmarais</surname> <given-names>C</given-names></name><name><surname>Malhotra</surname> <given-names>S</given-names></name><name><surname>Phippard</surname> <given-names>D</given-names></name><name><surname>Robins</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Estimating the ratio of CD4+ to CD8+ T cells using high-throughput sequence data</article-title><source>Journal of Immunological Methods</source><volume>391</volume><fpage>14</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jim.2013.02.002</pub-id><pub-id pub-id-type="pmid">23428915</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emerson</surname> <given-names>RO</given-names></name><name><surname>DeWitt</surname> <given-names>WS</given-names></name><name><surname>Vignali</surname> <given-names>M</given-names></name><name><surname>Gravley</surname> <given-names>J</given-names></name><name><surname>Hu</surname> <given-names>JK</given-names></name><name><surname>Osborne</surname> <given-names>EJ</given-names></name><name><surname>Desmarais</surname> <given-names>C</given-names></name><name><surname>Klinger</surname> <given-names>M</given-names></name><name><surname>Carlson</surname> <given-names>CS</given-names></name><name><surname>Hansen</surname> <given-names>JA</given-names></name><name><surname>Rieder</surname> <given-names>M</given-names></name><name><surname>Robins</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Immunosequencing identifies signatures of Cytomegalovirus exposure history and HLA-mediated effects on the T cell repertoire</article-title><source>Nature Genetics</source><volume>49</volume><fpage>659</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1038/ng.3822</pub-id><pub-id pub-id-type="pmid">28369038</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname> <given-names>NT</given-names></name><name><surname>Vander Heiden</surname> <given-names>JA</given-names></name><name><surname>Uduman</surname> <given-names>M</given-names></name><name><surname>Gadala-Maria</surname> <given-names>D</given-names></name><name><surname>Yaari</surname> <given-names>G</given-names></name><name><surname>Kleinstein</surname> <given-names>SH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Change-O: a toolkit for analyzing large-scale B cell immunoglobulin repertoire sequencing data</article-title><source>Bioinformatics</source><volume>31</volume><fpage>3356</fpage><lpage>3358</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btv359</pub-id><pub-id pub-id-type="pmid">26069265</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Higgins</surname> <given-names>I</given-names></name><name><surname>Matthey</surname> <given-names>L</given-names></name><name><surname>Pal</surname> <given-names>A</given-names></name><name><surname>Burgess</surname> <given-names>C</given-names></name><name><surname>Glorot</surname> <given-names>X</given-names></name><name><surname>Botvinick</surname> <given-names>M</given-names></name><name><surname>Mohamed</surname> <given-names>S</given-names></name><name><surname>Lerchner</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Î²-vae: learning basic visual concepts with a constrained variational framework</article-title><conf-name>International Conference on Learning Representations</conf-name><ext-link ext-link-type="uri" xlink:href="https://openreview.net/pdf?id=Sy2fzU9gl">https://openreview.net/pdf?id=Sy2fzU9gl</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howie</surname> <given-names>B</given-names></name><name><surname>Sherwood</surname> <given-names>AM</given-names></name><name><surname>Berkebile</surname> <given-names>AD</given-names></name><name><surname>Berka</surname> <given-names>J</given-names></name><name><surname>Emerson</surname> <given-names>RO</given-names></name><name><surname>Williamson</surname> <given-names>DW</given-names></name><name><surname>Kirsch</surname> <given-names>I</given-names></name><name><surname>Vignali</surname> <given-names>M</given-names></name><name><surname>Rieder</surname> <given-names>MJ</given-names></name><name><surname>Carlson</surname> <given-names>CS</given-names></name><name><surname>Robins</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>High-throughput pairing of T cell receptor Î± and Î² sequences</article-title><source>Science Translational Medicine</source><volume>7</volume><elocation-id>301ra131</elocation-id><pub-id pub-id-type="doi">10.1126/scitranslmed.aac5624</pub-id><pub-id pub-id-type="pmid">26290413</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Welling</surname> <given-names>M</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Lecun</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Auto-encoding variational bayes</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1312.6114v10">http://arxiv.org/abs/1312.6114v10</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kluyver</surname> <given-names>T</given-names></name><name><surname>Ragan-Kelley</surname> <given-names>B</given-names></name><name><surname>PÃ©rez</surname> <given-names>F</given-names></name><name><surname>Granger</surname> <given-names>B</given-names></name><name><surname>Bussonnier</surname> <given-names>M</given-names></name><name><surname>Frederic</surname> <given-names>J</given-names></name><name><surname>Kelley</surname> <given-names>K</given-names></name><name><surname>Hamrick</surname> <given-names>J</given-names></name><name><surname>Grout</surname> <given-names>J</given-names></name><name><surname>Corlay</surname> <given-names>S</given-names></name><name><surname>Ivanov</surname> <given-names>P</given-names></name><name><surname>Avila</surname> <given-names>D</given-names></name><name><surname>Abdalla</surname> <given-names>S</given-names></name><name><surname>Willing</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Jupyter Notebooks â A Publishing Format for Reproducible Computational Workflows</chapter-title><person-group person-group-type="editor"><name><surname>Loizides</surname> <given-names>F</given-names></name><name><surname>Schmidt</surname> <given-names>B</given-names></name></person-group><source>Positioning and Power in Academic Publishing: Players, Agents an Agendas</source><publisher-name>IOS Press</publisher-name><fpage>87</fpage><lpage>90</lpage></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marcou</surname> <given-names>Q</given-names></name><name><surname>Mora</surname> <given-names>T</given-names></name><name><surname>Walczak</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>High-throughput immune repertoire analysis with IGoR</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>561</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-02832-w</pub-id><pub-id pub-id-type="pmid">29422654</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Matsen</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2019">2019a</year><data-title>Deep generative models for TCR sequences</data-title><version designator="10aa639">10aa639</version><publisher-name>GitHub</publisher-name><ext-link ext-link-type="uri" xlink:href="https://github.com/matsengrp/vampire/">https://github.com/matsengrp/vampire/</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Matsen</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2019">2019b</year><data-title>Data analysis using the vampire models for immune cell receptor distributions</data-title><source>GitHub</source><version designator="72481ed">72481ed</version><ext-link ext-link-type="uri" xlink:href="https://github.com/matsengrp/vampire-analysis-1/">https://github.com/matsengrp/vampire-analysis-1/</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCoy</surname> <given-names>CO</given-names></name><name><surname>Gallagher</surname> <given-names>A</given-names></name><name><surname>Hoffman</surname> <given-names>NG</given-names></name><name><surname>Matsen</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Nestlyâa framework for running software with nested parameter choices and aggregating results</article-title><source>Bioinformatics</source><volume>388</volume><fpage>387</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts696</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>McKinney</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><data-title>Data structures for statistical computing in python</data-title><source>Proceedings of the 9th Python in Science</source><ext-link ext-link-type="uri" xlink:href="https://pdfs.semanticscholar.org/f6da/c1c52d3b07c993fe52513b8964f86e8fe381.pdf">https://pdfs.semanticscholar.org/f6da/c1c52d3b07c993fe52513b8964f86e8fe381.pdf</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murugan</surname> <given-names>A</given-names></name><name><surname>Mora</surname> <given-names>T</given-names></name><name><surname>Walczak</surname> <given-names>AM</given-names></name><name><surname>Callan</surname> <given-names>CG</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Statistical inference of the generation probability of T-cell receptors from sequence repertoires</article-title><source>PNAS</source><volume>109</volume><fpage>16161</fpage><lpage>16166</lpage><pub-id pub-id-type="doi">10.1073/pnas.1212755109</pub-id><pub-id pub-id-type="pmid">22988065</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Olson</surname> <given-names>BJ</given-names></name><name><surname>Moghimi</surname> <given-names>P</given-names></name><name><surname>Schramm</surname> <given-names>C</given-names></name><name><surname>Obraztsova</surname> <given-names>A</given-names></name><name><surname>Ralph</surname> <given-names>D</given-names></name><name><surname>Heiden</surname> <given-names>JAV</given-names></name><name><surname>Shugay</surname> <given-names>M</given-names></name><name><surname>Shepherd</surname> <given-names>A</given-names></name><name><surname>Lees</surname> <given-names>W</given-names></name><name><surname>Matsen</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sumrep: a summary statistic framework for immune receptor repertoire comparison and model validation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/727784</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>Varoquaux</surname> <given-names>G</given-names></name><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Michel</surname> <given-names>V</given-names></name><name><surname>Thirion</surname> <given-names>B</given-names></name><name><surname>Grisel</surname> <given-names>O</given-names></name><name><surname>Blondel</surname> <given-names>M</given-names></name><name><surname>Prettenhofer</surname> <given-names>P</given-names></name><name><surname>Weiss</surname> <given-names>R</given-names></name><name><surname>Dubourg</surname> <given-names>V</given-names></name><name><surname>Vanderplas</surname> <given-names>J</given-names></name><name><surname>Passos</surname> <given-names>A</given-names></name><name><surname>Cournapeau</surname> <given-names>D</given-names></name><name><surname>Brucher</surname> <given-names>M</given-names></name><name><surname>Perrot</surname> <given-names>M</given-names></name><name><surname>Duchesnay</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in Python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pogorelyy</surname> <given-names>MV</given-names></name><name><surname>Fedorova</surname> <given-names>AD</given-names></name><name><surname>McLaren</surname> <given-names>JE</given-names></name><name><surname>Ladell</surname> <given-names>K</given-names></name><name><surname>Bagaev</surname> <given-names>DV</given-names></name><name><surname>Eliseev</surname> <given-names>AV</given-names></name><name><surname>Mikelov</surname> <given-names>AI</given-names></name><name><surname>Koneva</surname> <given-names>AE</given-names></name><name><surname>Zvyagin</surname> <given-names>IV</given-names></name><name><surname>Price</surname> <given-names>DA</given-names></name><name><surname>Chudakov</surname> <given-names>DM</given-names></name><name><surname>Shugay</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Exploring the pre-immune landscape of antigen-specific T cells</article-title><source>Genome Medicine</source><volume>10</volume><elocation-id>68</elocation-id><pub-id pub-id-type="doi">10.1186/s13073-018-0577-7</pub-id><pub-id pub-id-type="pmid">30144804</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pogorelyy</surname> <given-names>MV</given-names></name><name><surname>Minervina</surname> <given-names>AA</given-names></name><name><surname>Chudakov</surname> <given-names>DM</given-names></name><name><surname>Mamedov</surname> <given-names>IZ</given-names></name><name><surname>Lebedev</surname> <given-names>YB</given-names></name><name><surname>Mora</surname> <given-names>T</given-names></name><name><surname>Walczak</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Method for identification of condition-associated public antigen receptor sequences</article-title><source>eLife</source><volume>7</volume><elocation-id>e33050</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.33050</pub-id><pub-id pub-id-type="pmid">29533178</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pogorelyy</surname> <given-names>MV</given-names></name><name><surname>Minervina</surname> <given-names>AA</given-names></name><name><surname>Shugay</surname> <given-names>M</given-names></name><name><surname>Chudakov</surname> <given-names>DM</given-names></name><name><surname>Lebedev</surname> <given-names>YB</given-names></name><name><surname>Mora</surname> <given-names>T</given-names></name><name><surname>Walczak</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018c</year><article-title>Detecting t-cell receptors involved in immune responses from single repertoire snapshots</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/375162</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pogorelyy</surname> <given-names>MV</given-names></name><name><surname>Minervina</surname> <given-names>AA</given-names></name><name><surname>Touzel</surname> <given-names>MP</given-names></name><name><surname>Sycheva</surname> <given-names>AL</given-names></name><name><surname>Komech</surname> <given-names>EA</given-names></name><name><surname>Kovalenko</surname> <given-names>EI</given-names></name><name><surname>Karganova</surname> <given-names>GG</given-names></name><name><surname>Egorov</surname> <given-names>ES</given-names></name><name><surname>Komkov</surname> <given-names>AY</given-names></name><name><surname>Chudakov</surname> <given-names>DM</given-names></name><name><surname>Mamedov</surname> <given-names>IZ</given-names></name><name><surname>Mora</surname> <given-names>T</given-names></name><name><surname>Walczak</surname> <given-names>A</given-names></name><name><surname>Lebedev</surname> <given-names>YB</given-names></name></person-group><year iso-8601-date="2018">2018d</year><article-title>Precise tracking of vaccine-responding t-cell clones reveals convergent and personalized response in identical twins</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/300343</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesselman</surname> <given-names>AJ</given-names></name><name><surname>Ingraham</surname> <given-names>JB</given-names></name><name><surname>Marks</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep generative models of genetic variation capture the effects of mutations</article-title><source>Nature Methods</source><volume>15</volume><fpage>816</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0138-4</pub-id><pub-id pub-id-type="pmid">30250057</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubelt</surname> <given-names>F</given-names></name><name><surname>Busse</surname> <given-names>CE</given-names></name><name><surname>Bukhari</surname> <given-names>SAC</given-names></name><name><surname>BÃ¼rckert</surname> <given-names>JP</given-names></name><name><surname>Mariotti-Ferrandiz</surname> <given-names>E</given-names></name><name><surname>Cowell</surname> <given-names>LG</given-names></name><name><surname>Watson</surname> <given-names>CT</given-names></name><name><surname>Marthandan</surname> <given-names>N</given-names></name><name><surname>Faison</surname> <given-names>WJ</given-names></name><name><surname>Hershberg</surname> <given-names>U</given-names></name><name><surname>Laserson</surname> <given-names>U</given-names></name><name><surname>Corrie</surname> <given-names>BD</given-names></name><name><surname>Davis</surname> <given-names>MM</given-names></name><name><surname>Peters</surname> <given-names>B</given-names></name><name><surname>Lefranc</surname> <given-names>MP</given-names></name><name><surname>Scott</surname> <given-names>JK</given-names></name><name><surname>Breden</surname> <given-names>F</given-names></name><name><surname>Luning Prak</surname> <given-names>ET</given-names></name><name><surname>Kleinstein</surname> <given-names>SH</given-names></name><collab>AIRR Community</collab></person-group><year iso-8601-date="2017">2017</year><article-title>Adaptive immune receptor repertoire community recommendations for sharing immune-repertoire sequencing data</article-title><source>Nature Immunology</source><volume>18</volume><fpage>1274</fpage><lpage>1278</lpage><pub-id pub-id-type="doi">10.1038/ni.3873</pub-id><pub-id pub-id-type="pmid">29144493</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schatz</surname> <given-names>DG</given-names></name><name><surname>Ji</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Recombination centres and the orchestration of V(D)J recombination</article-title><source>Nature Reviews Immunology</source><volume>11</volume><fpage>251</fpage><lpage>263</lpage><pub-id pub-id-type="doi">10.1038/nri2941</pub-id><pub-id pub-id-type="pmid">21394103</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sethna</surname> <given-names>Z</given-names></name><name><surname>Elhanati</surname> <given-names>Y</given-names></name><name><surname>Callan</surname> <given-names>CG</given-names></name><name><surname>Mora</surname> <given-names>T</given-names></name><name><surname>Walczak</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>OLGA: fast computation of generation probabilities of B- and t-cell receptor amino acid sequences and motifs</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1807.04425">http://arxiv.org/abs/1807.04425</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sinai</surname> <given-names>S</given-names></name><name><surname>Kelsic</surname> <given-names>E</given-names></name><name><surname>Church</surname> <given-names>GM</given-names></name><name><surname>Nowak</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Variational auto-encoding of protein sequences</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1712.03346">http://arxiv.org/abs/1712.03346</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>SÃ¸nderby</surname> <given-names>CK</given-names></name><name><surname>Raiko</surname> <given-names>T</given-names></name><name><surname>MaalÃ¸e</surname> <given-names>L</given-names></name><name><surname>SÃ¸nderby</surname> <given-names>SK</given-names></name><name><surname>Winther</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Ladder variational autoencoders</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1602.02282">http://arxiv.org/abs/1602.02282</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Tange</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>GNU Parallel</data-title><publisher-name>Zenodo</publisher-name><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1146014">https://doi.org/10.5281/zenodo.1146014</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wickham</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Ggplot2: Elegant Graphics for Data Analysis</source><publisher-loc>New York</publisher-loc><publisher-name>Springer-VerlagÂ </publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-98141-3</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Wilke</surname> <given-names>CO</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>cowplot: Streamlined Plot Theme and Plot Annotations for âggplot2â</data-title><version designator="0.9.3">r package version 0.9.3</version><ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=cowplot">https://CRAN.R-project.org/package=cowplot</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodsworth</surname> <given-names>DJ</given-names></name><name><surname>Castellarin</surname> <given-names>M</given-names></name><name><surname>Holt</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Sequence analysis of T-cell repertoires in health and disease</article-title><source>Genome Medicine</source><volume>5</volume><elocation-id>98</elocation-id><pub-id pub-id-type="doi">10.1186/gm502</pub-id><pub-id pub-id-type="pmid">24172704</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.46935.029</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Chakraborty</surname><given-names>Arup K</given-names></name><role>Reviewing Editor</role><aff><institution>Massachusetts Institute of Technology</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Huseby</surname><given-names>Eric</given-names> </name><role>Reviewer</role><aff><institution>University of Massachusetts</institution></aff></contrib><contrib contrib-type="reviewer"><name><surname>Callan</surname><given-names>Curtis</given-names> </name><role>Reviewer</role><aff><institution>Princeton University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Deep generative models for T cell receptor protein sequences&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by Arup Chakraborty as the Senior and Reviewing Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Eric Huseby (Reviewer #1); Curtis Callan (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The paper presents a method, based on a certain generic machine learning protocol, for using T cell receptor (TCR) sequence data to capture the probability distribution from which this sequence data is drawn. This is a nontrivial thing to do, since one can easily convince oneself that actual data is sparsely drawn from the underlying, very high-entropy, distribution on sequence space. To capture the true distribution from sparse data, one must make restrictive assumptions on the form of that distribution, and the heart of the machine learning protocol is the assumption that the distributions to be learned are Gaussian (or Bernoulli). This is a very restrictive assumption, but it apparently works very well in a number of contexts, such as computer vision. You ask whether this approach can capture the diversity of the immune system, and present evidence that the answer is &quot;yes&quot;. Therefore, we think this is a valuable contribution to our developing quantitative understanding of the stochastic nature of the adaptive immune system. We have concerns about the readability of the presentation (for the non-experts in machine learning), some technical issues, and some aspects of the way in which you present the significance of their work. These are noted below.</p><p>Essential revisions:</p><p>1) It would be helpful (in the Introduction, or at the relevant places in the Results) to describe the current understanding of the primary events of VDJ recombination. On the same line of discussion, it would be helpful to describe what events have been learned, and articulate clearly to a broad audience why this particular package is better than previous attempts to model TCR repertoires.</p><p>2) Few of the intended readers will be familiar with the Kingma and Welling (KW) approach and the conceptual context of that approach needs to be explained in much more concrete detail. To start with it has to be said explicitly that the encoder between data x and latent variables z is an explicit parametrized (Gaussian we think) probability distribution; similarly, it has to be said that the decoder from z to data variables x is also an explicit parametrized distribution (Bernoulli we think). It also has to be said that the prior on latent variables z is a very special distribution (isotropic unit variance on the z variable space, we think). Then it has to be said that the neural nets behind the encoder and decoder are actually maps from variables like x to the parameters of the various parametric distributions. As another specific clarity issue, the dimension of the latent space is not explicitly stated up front; one has to wait until somewhere deep in Materials and methods to realize that 20 is the chosen value (there is no discussion of why 20 as opposed to 200). More generally, There should be some discussion of why the KW method (statistics are Gaussian) has any reason to work in the context of understanding TCR statistics. An effort to rewrite the exposition so as to convey the conceptual heart of the method more clearly and explicitly would greatly enhance the utility of the paper to the quantitative biology readership.</p><p>3) It is argued that the VAE models can predict cohort frequencies at an R<sup>2</sup> value of ~0.45, whereas a previous OLGA model works at an R<sup>2</sup> value of ~0.25. In absolute terms, the VAE model is better, however, neither works particularly well; both have R<sup>2</sup> values less than a minimal cutoff of 0.5. Much more clarity is required in describing the comparison between the two methods.</p><p>OLGA is a different way of capturing TCR sequence statistics, and it relies on the idea that there are biological hidden variables (associated with the VDJ recombination process) whose statistics can be inferred from sequence data and then these statistics used to compute probabilities of finding individual TCR sequences in new data. OLGA doesn't include selection effects that happen post-VDJ recombination and that shape the statistics of observed in-frame TCR sequences. This is where the Q in OLGA.Q comes in. You construct a version of a selection model which is too simplistic to do a good job of capturing selection effects. Comparing the results of their machine learning approach to OLGA.Q to say that former captures &quot;more&quot; aspects of TCR sequence statistics than the latter doesn't seem very appropriate. Are TRAV rearrangements considered? TRBV/TRBJ rearrangement and usage is largely ignorant of selection context. TCRb rearrangement occurs prior to TCRa rearrangement and its usage is largely ligand independent. TCRa rearrangement is later, and subject to strong selective pressures. In particular, if TCRa rearrangement produces a non-signaling TCR, TCRa rearrangement occurs again with more 3' gene segments. Control mechanisms and the &quot;rules&quot; that govern TCRa processes are much less well understood. These aspects are not accounted for, and should be stated.</p><p>4) Because it assumes that Gaussian core distributions underlie the observed data, it is by no means obvious that the Kingma and Welling method is appropriate for TCR data. The most attractive feature of the KW approach is that it provides a method for computing the intrinsic probability of finding any specific TCR clone in a new data sample (what the authors call P<sub>VAE</sub>). The paper shows histograms of this quantity over various data sets, and these histograms have the striking feature that the probability values start small and range over more than ten orders of magnitude. Now the OLGA method, taking a totally different approach, can also calculate the one-shot probability (called P<sub>gen</sub>) of any specific clone being created in a single VDJ recombination event and one can plot the same sort of histogram of generation probabilities. What is interesting is that the two approaches produce very similar generation probability histograms. The further modifications to OLGA predictions due to selection might change probabilities by modest factors, but we are talking about probabilities that range over more than ten orders of magnitude, so the approximate compatibility of the two methods is evidence that the KW approach is doing a good job of capturing the stochastic effects of selection. The relatively poor fit of both methods may also reflect TCR sequencing errors (PCR amplification and sample limited constraints) as well as the more significant problem alluded to in the end of the Discussion. That is, how to deal with the error associated with point estimates of low and very low frequency TCR rearrangements. This is illustrated in Figure 2 by the triangle shape of the log-log plots, where there is a greater level of divergence at low versus high frequency TCRs. Some discussion or accounting for these issues would be help to the reader.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.46935.030</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) It would be helpful (in the Introduction, or at the relevant places in the Results) to describe the current understanding of the primary events of VDJ recombination.</p></disp-quote><p>We have added a condensed introduction to V(D)J recombination in the first paragraph of the Introduction:</p><p>âT cell receptors (TCRs) are composed of an <italic>Î±</italic> and a <italic>Î²</italic> protein chain, both originating from a random V(D)J recombination process, followed by selective steps that ensure functionality and limit auto-reactivity. [â¦] The naive T cell population consists of T cells that have undergone V(D)J recombination and MHC selection but not yet encountered antigen.â</p><p>This includes important information about the order of the events (noted in point 3 of these comments). We also now more consistently refer to V(D)J when referring to the general process, but use VDJ when discussing the <italic>Î²</italic> chain specifically.</p><disp-quote content-type="editor-comment"><p>On the same line of discussion, it would be helpful to describe what events have been learned, and articulate clearly to a broad audience why this particular package is better than previous attempts to model TCR repertoires.</p></disp-quote><p>We have now added the following material to the Discussion adding to our description of what has been learned.</p><p>âSpecifically, we use a semiparametric method that makes a single weak assumption: that there exists some small number of latent parameters that can be used to generate to the observed distribution. [â¦] The basic model, for example, is implemented in about 100 lines of Python code.â</p><p>We have also added another lesson learned:</p><p>âFurthermore, our efforts to inject biological knowledge into the deep learning framework did not significantly improve performance.â</p><disp-quote content-type="editor-comment"><p>2) Few of the intended readers will be familiar with the Kingma and Welling (KW) approach and the conceptual context of that approach needs to be explained in much more concrete detail. To start with it has to be said explicitly that the encoder between data x and latent variables z is an explicit parametrized (Gaussian we think) probability distribution; similarly, it has to be said that the decoder from z to data variables x is also an explicit parametrized distribution (Bernoulli we think). It also has to be said that the prior on latent variables z is a very special distribution (isotropic unit variance on the z variable space, we think). Then it has to be said that the neural nets behind the encoder and decoder are actually maps from variables like x to the parameters of the various parametric distributions. As another specific clarity issue, the dimension of the latent space is not explicitly stated up front; one has to wait until somewhere deep in Materials and methods to realize that 20 is the chosen value (there is no discussion of why 20 as opposed to 200). More generally, There should be some discussion of why the KW method (statistics are Gaussian) has any reason to work in the context of understanding TCR statistics. An effort to rewrite the exposition so as to convey the conceptual heart of the method more clearly and explicitly would greatly enhance the utility of the paper to the quantitative biology readership.</p></disp-quote><p>Thank you for these comments. We have now provided additional detail about the operation of the encoder and decoder, specifically calling out the distributions used:</p><p>âIn this paper the latent space is 20-dimensional, and we use the conventional choice of a standard multivariate normal prior for <italic>p<sub>Î¸</sub></italic>(<bold>z</bold>). [â¦] The decoder <italic>p<sub>Î¸</sub></italic><bold>xÌ|z</bold>) is a per-site categorical distribution over amino acids and gaps parameterized by a neural network with input <bold>z</bold>.â</p><p>We also describe why we thought that VAEs could be useful in the TCR context: âPrevious work using VAEs have found success when first, there is a vast amount of data available, and second, the data distribution is complicated, involving nonlinearities and interactions between covariates. There is indeed a vast amount of TCR repertoire data, and the TCR probability distributions are complex.â</p><p>We also clarify that the choice of a normal distribution is not a classical âmodel choiceâ based on a mental model of the underlying biology, but rather out of mathematical convenience:</p><p>âThis choice of a normal distribution is primarily for mathematical convenience rather than being part of a specific modeling design; the normal ânoiseâ in the latent space get processed by a neural network which introduces non-linearities that ensure that the result is not normal. However, VAE variants do use other distributions in place of normal (Dilokthanakul et al., 2016; Davidson et al., 2018).â This text also emphasizes that the neural network offers substantial flexibility, transforming the normal with non-linearities.</p><p>As described above, the conceptual heart of the method is simply that TCR repertoire distributions can be generated by latent distributions on a relatively small number of parameters. These methods have tremendous flexibility and generality: given the vast amount of TCR data now available, one can learn models that capture features of the distributions that we do not yet even conceptualize. Indeed, part of the message of the paper is that even very simple off-the-shelf neural network methods can compete with refined classical models, and that future developments on neural network design and training will enable much better models.â</p><p>Regarding 20 vs. 200 dimensions of the latent space, we have now added the following material:</p><p>âModel design and parameter tuning, including the sizes of hidden layers and the dimension of the latent space, was performed using the data of DeWitt et al., 2018). On this data we endeavored to decrease model size without incurring loss on held-out data within this data set. We found that the model was relatively robust to parameter perturbations as long as the number of parameters was not too small.â</p><p>We welcome any suggestions concerning how we can make these points more clear.</p><disp-quote content-type="editor-comment"><p>3) It is argued that the VAE models can predict cohort frequencies at an R<sup>2</sup> value of ~0.45, whereas a previous OLGA model works at an R<sup>2</sup> value of ~0.25. In absolute terms, the VAE model is better, however, neither works particularly well; both have R<sup>2</sup> values less than a minimal cutoff of 0.5. Much more clarity is required in describing the comparison between the two methods.</p></disp-quote><p>We have tamped down the âhigh accuracyâ claims. However, we do feel that these R<sup>2</sup> values show that our VAE has predictive power. Recall that R<sup>2</sup> can be interpreted as the proportion of variance explained. Although there is room for improvement, we feel that getting almost 50% variance explained for an out-of-sample frequency prediction applying a biology-agnostic model to just amino acid sequence across many orders of magnitudes of frequency is a significant achievement.</p><p>As suggested below, we have added two sentences describing additional challenges with the data we have at hand:</p><p>âRecall that these correlation measures include the full scale of frequencies, including very noisy frequency estimates on the lower end of the scale. Also, we make no efforts to account for sequencing error above the methods used in DeWitt et al., 2018.â</p><p>Regarding clarity of comparison description, see below and note that we will be opening up our analysis repository upon publication. This repository is composed of a series of Jupyter notebooks performing the analysis reproducibly.</p><disp-quote content-type="editor-comment"><p>OLGA is a different way of capturing TCR sequence statistics, and it relies on the idea that there are biological hidden variables (associated with the VDJ recombination process) whose statistics can be inferred from sequence data and then these statistics used to compute probabilities of finding individual TCR sequences in new data. OLGA doesn't include selection effects that happen post-VDJ recombination and that shape the statistics of observed in-frame TCR sequences. This is where the Q in OLGA.Q comes in. You construct a version of a selection model which is too simplistic to do a good job of capturing selection effects. Comparing the results of their machine learning approach to OLGA.Q to say that former captures &quot;more&quot; aspects of TCR sequence statistics than the latter doesn't seem very appropriate.</p></disp-quote><p>This is an important point that is addressed with the following sentences that introduce our version of the OLGA.Q model:</p><p>âThis is a simpler model than the general Elhanati et al., 2014 model, which allows for selection based on CDR3 amino acid composition. [â¦] In any case, an implementation of the general Elhanati et al., 2014 model, for which training is highly involved, is not currently available.â</p><p>Specifically, our model is richer than any of the models currently in use, including models from the same group of Elhanati et al., 2014, that model the underlying frequency of TCRs in the functional repertoire. We welcome further suggestions about how we can make this point more clear.</p><p>We also note that we have implemented the first publicly-available method for sampling sequences from the Ppost distribution of OLGA.Q.</p><disp-quote content-type="editor-comment"><p>Are TRAV rearrangements considered? TRBV/TRBJ rearrangement and usage is largely ignorant of selection context. TCRb rearrangement occurs prior to TCRa rearrangement and its usage is largely ligand independent. TCRa rearrangement is later, and subject to strong selective pressures. In particular, if TCRa rearrangement produces a non-signaling TCR, TCRa rearrangement occurs again with more 3' gene segments. Control mechanisms and the &quot;rules&quot; that govern TCRa processes are much less well understood. These aspects are not accounted for, and should be stated.</p></disp-quote><p>We do not consider TRAV in this paper, however, it is a very interesting topic for future work as we now note:</p><p>âHere we have restricted our attention to TCR <italic>Î²</italic> sequencing, however, our methods apply with no modification to TCR <italic>Î±</italic> chains. [â¦] The most interesting insights will come from jointly modeling the two chains using large-scale <italic>Î±Î²</italic> paired TCR sequencing (Howie et al., 2015), which is a more complex process.â</p><disp-quote content-type="editor-comment"><p>4) Because it assumes that Gaussian core distributions underlie the observed data, it is by no means obvious that the Kingma and Welling method is appropriate for TCR data. The most attractive feature of the KW approach is that it provides a method for computing the intrinsic probability of finding any specific TCR clone in a new data sample (what the authors call P<sub>VAE</sub>). The paper shows histograms of this quantity over various data sets, and these histograms have the striking feature that the probability values start small and range over more than ten orders of magnitude. Now the OLGA method, taking a totally different approach, can also calculate the one-shot probability (called P<sub>gen</sub>) of any specific clone being created in a single VDJ recombination event and one can plot the same sort of histogram of generation probabilities. What is interesting is that the two approaches produce very similar generation probability histograms. The further modifications to OLGA predictions due to selection might change probabilities by modest factors, but we are talking about probabilities that range over more than ten orders of magnitude, so the approximate compatibility of the two methods is evidence that the KW approach is doing a good job of capturing the stochastic effects of selection. The relatively poor fit of both methods may also reflect TCR sequencing errors (PCR amplification and sample limited constraints) as well as the more significant problem alluded to in the end of the Discussion. That is, how to deal with the error associated with point estimates of low and very low frequency TCR rearrangements. This is illustrated in Figure 2 by the triangle shape of the log-log plots, where there is a greater level of divergence at low versus high frequency TCRs. Some discussion or accounting for these issues would be help to the reader.</p></disp-quote><p>Regarding Gaussian distributions underlying the observed data, we hope it is clear from our response above that this is a Gaussian on the latent space before application of the neural network. A Gaussian transformed by a neural network is not Gaussian in general, and can be quite âfarâ from a Gaussian using even simple neural network transformations.</p><p>We now emphasize the difficulty posed by low frequency rearrangements or sequencing error when we introduce the R<sup>2</sup> results, as described above.</p></body></sub-article></article>