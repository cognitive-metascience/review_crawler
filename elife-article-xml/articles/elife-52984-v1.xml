<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">52984</article-id><article-id pub-id-type="doi">10.7554/eLife.52984</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Push-pull competition between bottom-up and top-down auditory attention to natural soundscapes</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-164767"><name><surname>Huang</surname><given-names>Nicholas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5993-8325</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-55873"><name><surname>Elhilali</surname><given-names>Mounya</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2597-738X</contrib-id><email>mounya@jhu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Laboratory for Computational Audio Perception, Department of Electrical Engineering, Johns Hopkins University</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing Editor</role><aff><institution>Peking University</institution><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>20</day><month>03</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e52984</elocation-id><history><date date-type="received" iso-8601-date="2019-10-23"><day>23</day><month>10</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-02-13"><day>13</day><month>02</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Huang and Elhilali</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Huang and Elhilali</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-52984-v1.pdf"/><abstract><p>In everyday social environments, demands on attentional resources dynamically shift to balance our attention to targets of interest while alerting us to important objects in our surrounds. The current study uses electroencephalography to explore how the push-pull interaction between top-down and bottom-up attention manifests itself in dynamic auditory scenes. Using natural soundscapes as distractors while subjects attend to a controlled rhythmic sound sequence, we find that salient events in background scenes significantly suppress phase-locking and gamma responses to the attended sequence, countering enhancement effects observed for attended targets. In line with a hypothesis of limited attentional resources, the modulation of neural activity by bottom-up attention is graded by degree of salience of ambient events. The study also provides insights into the interplay between endogenous and exogenous attention during natural soundscapes, with both forms of attention engaging a common fronto-parietal network at different time lags.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>When walking into a busy restaurant or café, our sense of hearing is bombarded with different sounds that our brain has to sort through to make sense of our surroundings. Our brain has to balance the desire to focus our attention on sounds we choose to listen to (such as the friend we are having a conversation with) and sounds that attract our attention (such as the sound of someone else’s phone ringing). Without the ability to be distracted, we might miss a noise that may or may not be crucial to our survival, like the engine roar of an approaching vehicle or a ping notifying us of an incoming email. However, it remains unclear what happens in our brains that enables us to shift our attention to background sounds.</p><p>To investigate this further, Huang and Elhilali asked 81 participants to focus their attention on a repeating sound while being exposed to background noises from everyday life, such as sounds from a busy café. The experiment showed that when a more noticeable sound happened in the background, such as a loud voice, the participants were more likely to lose attention on their task and miss changes in the tone of the repeating sound.</p><p>Huang and Elhilali then measured the brain activity of 12 participants as they counted the number of altered tones in a sequence of sounds, again with noise in the background. This revealed that brain waves synchronized with tones that the participants were concentrating on. However, once there was a noticeable event in the background, this tone synchronization was reduced and the brain waves aligned with the background noise. Huang and Elhilali found that distracting noises in the background activate the same region of the brain as sounds we choose to listen to. This demonstrates how background sounds are able to re-direct our attention.</p><p>These results are consistent with the idea that we have a limited capacity for attention, and that new sensory information can divert brain activity. Having a better understanding of how these processes work could help develop better communication aids for people with impaired hearing, and improve software for interpreting sounds with a noisy background.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>auditory</kwd><kwd>Attention</kwd><kwd>top-down</kwd><kwd>bottom-up</kwd><kwd>salience</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01HL133043</award-id><principal-award-recipient><name><surname>Elhilali</surname><given-names>Mounya</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U01AG058532</award-id><principal-award-recipient><name><surname>Elhilali</surname><given-names>Mounya</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1734744</award-id><principal-award-recipient><name><surname>Elhilali</surname><given-names>Mounya</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N000141912014</award-id><principal-award-recipient><name><surname>Elhilali</surname><given-names>Mounya</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N000141912689</award-id><principal-award-recipient><name><surname>Elhilali</surname><given-names>Mounya</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N000141712736</award-id><principal-award-recipient><name><surname>Elhilali</surname><given-names>Mounya</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Everyday soundscapes dynamically engage attention towards target sounds or salient ambient events, with both attentional forms engaging the same fronto-parietal network but in a push-pull competition for limited neural resources.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Attention is a selection mechanism that deploys our limited neural resources to the most relevant stimuli in the environment. Without such a process, the sights and sounds of everyday life would overwhelm our senses. Filtering out important information from our surroundings puts a constant demand on the cognitive system given the dynamic nature of everyday scenes. On the one hand, we attend to sounds, sights and smells that we choose based on what matches our behavioral goals and contextual expectations. At the same time, we have to balance perception of salient events and objects that we need to be alerted to both for survival as well as awareness of our ever changing surrounds. These various factors guide our attentional resources to dynamically shift in order to shape the representation of sensory information based on its behavioral relevance, and ultimately influence how we perceive the world around us.</p><p>How does the brain manage its executive attentional resources faced with these dynamic demands? Studies of voluntary (‘top-down’ or endogenous) attention have shown that cognitive feedback modulates the encoding of sensory cues in order to improve the signal-to-noise ratio of attended targets relative to irrelevant maskers or other objects in the environment (<xref ref-type="bibr" rid="bib4">Baluch and Itti, 2011</xref>; <xref ref-type="bibr" rid="bib18">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib67">Senkowski et al., 2005</xref>). Whether attending to someone’s voice amidst a crowd, or spotting that person in a busy street, or even identifying a particular smell among others, changes to neural encoding due to attention have been reported across all sensory modalities including auditory, visual, olfactory and somatosensory systems and appear to operate across multiple neural scales (<xref ref-type="bibr" rid="bib13">Corbetta and Shulman, 2002</xref>; <xref ref-type="bibr" rid="bib89">Zelano et al., 2005</xref>; <xref ref-type="bibr" rid="bib5">Bauer et al., 2006</xref>; <xref ref-type="bibr" rid="bib37">Knudsen, 2007</xref>). Response profiles of individual neurons, entire sensory circuits, and cross brain regions are modulated by attentional feedback to produce neural responses that reflect not only the physical properties of the stimulus but also the behavioral state and reward expectations of the system. Some of the hallmarks of selective attention are enhanced neural encoding in sensory cortex of attended features and dynamics (e.g. envelope of an attended speaker; <xref ref-type="bibr" rid="bib49">Mazziotta et al., 2001</xref>) as well as recruitment of dorsal fronto-parietal circuits mediated by boosted gamma oscillatory activity (<xref ref-type="bibr" rid="bib26">Fries, 2009</xref>; <xref ref-type="bibr" rid="bib4">Baluch and Itti, 2011</xref>).</p><p>In contrast, our understanding of the effects of involuntary (‘bottom-up’ or exogenous) attention has been mostly led by work in vision. There is a well established link between perceptual attributes of salience and their influence on attention deployment in visual scenes (<xref ref-type="bibr" rid="bib8">Borji et al., 2013</xref>; <xref ref-type="bibr" rid="bib78">Treue, 2003</xref>; <xref ref-type="bibr" rid="bib86">Wolfe and Horowitz, 2004</xref>). Studies of visual salience greatly benefited from natural behaviors such as eye gaze which facilitate tracking subjects’ attentional focus and allow the use of rich and naturalistic stimuli including still pictures and videos (<xref ref-type="bibr" rid="bib9">Borji, 2015</xref>; <xref ref-type="bibr" rid="bib11">Carmi and Itti, 2006</xref>; <xref ref-type="bibr" rid="bib47">Marius 't Hart et al., 2009</xref>). In parallel, exploration of brain networks implicated in visual salience revealed engagement of both subcortical and cortical circuits that balance the sensory conspicuity of a visual scene with more task-related information to shape attentional orienting to salient visual objects (<xref ref-type="bibr" rid="bib81">Veale et al., 2016</xref>). Bottom-up attention also engages ventral fronto-parietal circuits that orient subjects’ focus to salient stimuli outside the spotlight of voluntary attention (<xref ref-type="bibr" rid="bib13">Corbetta and Shulman, 2002</xref>; <xref ref-type="bibr" rid="bib25">Fox et al., 2006</xref>; <xref ref-type="bibr" rid="bib3">Asplund et al., 2010</xref>).</p><p>By comparison, the study of auditory bottom-up attention has proven more challenging owing to the difficulty of properly defining behavioral metrics that determine when attention is captured by a salient sound. Pupilometry has been explored in auditory salience studies with recent evidence suggesting a correlation between stimulus salience and pupil size (<xref ref-type="bibr" rid="bib41">Liao et al., 2016</xref>; <xref ref-type="bibr" rid="bib82">Wang et al., 2014</xref>; <xref ref-type="bibr" rid="bib90">Zhao et al., 2019</xref>). Still, there are various aspects of this correlation that remain ill understood, with some evidence suggesting that a large component of the pupillary response is driven by the sound’s loudness and its local context rather than a full account of its perceptual salience (<xref ref-type="bibr" rid="bib42">Liao et al., 2017</xref>; <xref ref-type="bibr" rid="bib32">Huang and Elhilali, 2017</xref>). In parallel, a large body of work has focused on deviance detection in auditory sequences, and has established neural markers associated with deviant or rare events. An example of such a response is mismatch negativity (MMN) which can be elicited pre-attentively, though this response is modulated by attention and associated with behavioral measures of distraction (<xref ref-type="bibr" rid="bib61">Ririe et al., 2017</xref>; <xref ref-type="bibr" rid="bib55">Näätänen et al., 2007</xref>). By using sequences with deviant tokens or snippets, studies of novelty processing are able to better control the acoustic parameters of the stimulus and the precise occurrence and nature of salient events. Other studies have extended oddball designs using richer sound structures including musical sequences or noise patterns, but still piecing together sound tokens to control presence of transient salient events in the acoustic signal (<xref ref-type="bibr" rid="bib22">Duangudom and Anderson, 2013</xref>; <xref ref-type="bibr" rid="bib36">Kayser et al., 2005</xref>; <xref ref-type="bibr" rid="bib35">Kaya and Elhilali, 2014</xref>; <xref ref-type="bibr" rid="bib77">Tordini et al., 2015</xref>). Nonetheless, this structure falls short of the natural intricacies of realistic sounds in everyday environments where salience can take on more nuanced manifestations. Similar to established results in vision, use of natural soundscapes could not only extend results observed with simpler oddball sequences; but also shed light on the privileged processing status of social signals that reflect attentional capture in everyday life (<xref ref-type="bibr" rid="bib21">Doherty et al., 2017</xref>).</p><p>In the current work, we explore dynamic deployment of attention using an unconstrained dataset of natural sounds. The collection includes a variety of contents and compositions, and spans samples from everyday situations taken from public databases such as YouTube and Freesound. It covers various settings such as a busy cafeteria, a ballgame at the stadium, a concert in a symphony hall, a dog park, and a protest in the streets. Concurrent with these everyday sounds, subjects’ attention is directed towards a controlled tone sequence where we examine effects of top-down attention to this rhythmic sequence as well as intermittent switches of attention elicited by salient events in background scenes. This paradigm tackles three key limitations in our understanding of the dynamic deployment of attentional resources in complex auditory scenes. First, the study investigates the relationship between auditory salience and bottom-up attentional capture beyond deviance detection paradigms. The stimulus probes how distracting events in a background scene organically compete for attentional resources already deployed toward a dynamic sound sequence. Unlike clearly defined ‘deviants’ typically used in oddball paradigms, the attention-grabbing nature of salient events in natural soundscapes is more behaviorally and cognitively nuanced and exhibits a wide range of dynamics both in strength and buildup. A salient event in a complex scene can vary from a momentary transient event (e.g. a phone ringing in an auditorium) to a gradually dynamic sound (e.g. a distinct voice steadily emerging from the cacophony in a busy cafeteria). Here, we are interested in probing whether this natural and nuanced capture of attention induces equally profound effects on brain responses as previously reported from top-down, task-related attention. The study leverages the organic nature of competition between bottom-up and top-down attention in natural soundscapes to not only test the hypothesis of limited resources shared between the two modes of attention, but also engagement of distinct but overlapping brain networks (<xref ref-type="bibr" rid="bib13">Corbetta and Shulman, 2002</xref>; <xref ref-type="bibr" rid="bib64">Salmi et al., 2009</xref>).</p><p>Second, employing dynamic scenes allows us to focus our analysis beyond event-related potentials (ERPs) which require precisely-aligned event onsets, hence often limiting paradigms to oddball designs or intermittent distractors. In the current study, the use of continuous scenes is anchored against a rhythmic attended sequence which provides a reference for temporal alignment of competing attentional states throughout the experiment. Third, while the study sheds lights on neural markers of auditory salience, it does so <italic>relative to</italic> a competing sequence in a controlled attentional task. As such, it balances the dichotomy often found in studies of auditory salience using either distraction or detection paradigms. A number of studies probe bottom-up attention using irrelevant stimuli presented to the subjects without necessarily competing for their attentional focus; or where subjects are informed or learn their value (see <xref ref-type="bibr" rid="bib38">Lavie, 2005</xref>). Here, we are interested in characterizing the dynamic effect of salient distractors on the encoding of attended targets. Ultimately, the current study aims to determine how well we can predict the existence of attention-grabbing events while subjects are engaged in a competing task.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Listeners perform an amplitude-modulation (AM) detection task by attending to a tone sequence and indicating presence of intermittent modulated target tones (orange note in <xref ref-type="fig" rid="fig1">Figure 1</xref>). Concurrently, a busy acoustic scene is presented in the background and subjects are asked to completely ignore it. Background scenes are taken from the JHU DNSS (Dichotic Natural Salience Soundscapes) database for which behavioral estimates of salience timing and strength have been previously collected (<xref ref-type="bibr" rid="bib32">Huang and Elhilali, 2017</xref>) (see Materials and methods for details). In a first experiment, easy and hard AM detection tasks are interleaved in experimental blocks by changing the modulation depth of the target note (easy: 0 dB, hard: −5 dB). As expected, subjects report a higher overall detection accuracy for the easy condition (75.4%) compared to the hard condition (48.2%). Moreover, target detection (in both easy and hard conditions) is disrupted by presence of a salient event in the ignored background scenes; and detection accuracy drops significantly over a period up to a second after onset of the salient event [drop in detection accuracy; hard task, t(62) = −5.25, p=1.96*10<sup>−6</sup>; easy task, t(62) = −5.62, p=4.92*10<sup>−7</sup>]. Salient events attract listeners’ attention away from the task at hand and cause a drop in detection accuracy that is proportional to the salience level of background distractors; especially for high and mid salience events [hard task - high salience event t(62) = −4.97, p=5.57*10<sup>−6</sup>; mid salience event t(62) = −3.70, p=4.54*10<sup>−4</sup>; low salience event t(62) = −0.75, p=0.46; easy task - high salience event t(62) = −4.20, p=8.54*10<sup>−5</sup>; mid salience event t(62) = −2.29, p=0.025; low salience event t(62) = −1.51, p=0.14]. In order to further explore neural underpinnings of changes in the attentional state of listeners, this paradigm is repeated with the easy task while neural activity is measured using Electroencephalography (EEG).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Stimulus paradigm during EEG recording.</title><p>Listeners are presented with two concurrent sounds in each each trial: (top stimulus) A recording of a natural audio clip, which subjects are asked to ignore; and (bottom stimulus) a rhythmic tone sequence, which subjects pay attention to and detect presence of occasional modulated tones (shown in orange). A segment of one trial neural recording is shown in the bottom. Analyses focus on changes in neural responses due to presence of salient events in the ambient scene or target tones in the attended scene.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-52984-fig1-v1.tif"/></fig><p>The attended tone sequence is presented at a regular tempo of 2.6 Hz and induces a strong overall phase-locked response around this frequency despite the concurrent presentation of a natural scene in the background. <xref ref-type="fig" rid="fig2">Figure 2A</xref> shows the grand average spectral profile of the neural response observed throughout the experiment. The plot clearly displays a strong energy at 2.6 Hz, with a left-lateralized fronto-central response, consistent with activation of Heschl’s gyrus and conforming to prior observations of precise phase-locking to relatively slow rates in core auditory cortex (<xref ref-type="bibr" rid="bib46">Lütkenhöner and Steinsträter, 1998</xref>; <xref ref-type="bibr" rid="bib43">Liégeois-Chauvel et al., 2004</xref>; <xref ref-type="bibr" rid="bib73">Stropahl et al., 2018</xref>). (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, inset).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Phase-locking results.</title><p>(<bold>A</bold>) Spectral density across all stimuli. The peak in energy at the tone presentation frequency is marked by a red arrow. Inset shows average normalized tone-locking energy for individual electrodes. (<bold>B</bold>) Spectral density around target tones (top) and salient events (bottom). Black lines show energy <italic>preceding</italic> the target or event, while colored lines depict energy <italic>following</italic>. Note that target tones are fewer throughout the experiment leading to lower resolution of the spectral profile. (<bold>C</bold>) Change in phase-locking energy across target tones, non-events, and salient events. (<bold>D</bold>) Change in tone-locking energy across high, mid, and low salience events. Error bars depict ±1 SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-52984-fig2-v1.tif"/></fig><p>Taking a closer look at this phase-locked activity aligned to the tone sequence, the response appears to change during the course of each trial, particularly when coinciding with task-specific AM tone targets, as well as when concurring with salient events in the background scene. Phase-locking near modulated-tone targets shows an increase in 2.6 Hz power relative to the average level, reflecting an expected increase in neural power induced by top-down attention (<xref ref-type="fig" rid="fig2">Figure 2B</xref>-top). The same phase-locked response is notably reduced when tones coincide with salient events in the background (<xref ref-type="fig" rid="fig2">Figure 2B</xref>-bottom - blue curve), indicating diversion of resources away from the attended sequence and potential markers of distraction caused by salient events in the ignored background.</p><p>We contrast variability of 2.6 Hz phase-locked energy over 3 windows of interest in each trial: (i) near AM tone targets, (ii) near salient events and (iii) near tones chosen randomly ‘away’ from either targets or salient events and used as control baseline responses. We compare activity in each of these windows relative to a preceding window (e.g. <xref ref-type="fig" rid="fig1">Figure 1</xref>, post vs. pre-event interval). <xref ref-type="fig" rid="fig2">Figure 2C</xref> shows that phase-locking to 2.6 Hz after target tones increases significantly [t(443)=4.65, p=4.43*10<sup>−6</sup>], whereas it decreases significantly following salient events [t(443)=−5.89, p &lt; 10<sup>−7</sup>], relative to preceding non-target tones. A random sampling of tones away from target tones or salient events does not show any significant variability [t(443)=−0.78, p=0.43, Bayes Factor 0.072] indicating a relatively stable phase-locked power in ‘control’ segments of the experiment away from task-relevant targets or bottom-up background events (2C, middle bar). Compared to each other, the top-down attentional effect due to target tones is significantly different from the inherent variability in phase-locked responses in ‘control’ segments [t(886)=3.81, p=1.48*10<sup>−4</sup>]; while distraction due to salient events induces a decrease in phase-locking that is significantly different from inherent variability in ‘control’ segments [t(886)=−3.58, p=3.66*10<sup>−3</sup>].</p><p>Interestingly, this salience-induced decrease is modulated in strength by the level of salience of background events. The decrease in phase-locked energy is strongest for events with a higher level of salience [t(443)=−3.78, p=1.8*10<sup>−4</sup>]. It is also significant for events with mid-level salience [t(443)=−2.57, p=0.01], but marginally reduced though not significant for events with the lowest salience [t(359)=1.33, p=0.20, Bayes Factor BF 0.14] (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). A one-way ANOVA did not show a significant difference between the mean suppression at the three salience levels [F(1329)=1.65, p=0.19].</p><p>A potential confound to reduced phase-locking due to distraction could be local acoustic variability associated with salient events instead of actual deployment of bottom-up attention that disrupts phase-locking to the attended sequence. While this possibility is unlikely given the significant effect of salient events on behavioral detection of targets, we further reassess loss of phase-locking to the attended rhythm near events by excluding salient events with the highest loudness which could cause energetic masking effects (<xref ref-type="bibr" rid="bib52">Moore, 2013</xref>). This analysis confirms that phase-locking to 2.6 Hz is still significantly reduced relative to non-event control moments [t(443)=−3.88, p &lt; 10<sup>−3</sup>]. A complementary measure of loudness is also explored by excluding events with the highest energy in one equivalent rectangular bandwidth (ERB) around the tone frequency at 440 Hz (<xref ref-type="bibr" rid="bib53">Moore and Glasberg, 1983</xref>). Excluding the loudest 25% events by this measure still yields a significant reduction in tone-locking [t(443)=−4.93, p=1.17*10<sup>−6</sup>]. In addition, we analyze acoustic attributes of all salient events in background scenes and compare their acoustic attributes to those of randomly selected intervals in non-salient segments. This comparison assesses whether salient events have unique acoustic attributes that are never observed at other moments in the scene. A Bhattacharyya coefficient -BC- (<xref ref-type="bibr" rid="bib34">Kailath, 1967</xref>) reveals that salient events share the <italic>same</italic> global acoustic attributes as non-salient moments in the ambient background across a wide range of features (BC for loudness 0.9655, brightness 0.9851, pitch 0.9867, harmonicity .9775 and scale 0.9868). Morever, the significant drop in phase locking is maintained when events are split by strength of low-level acoustic features such as harmonicity or brightness [High Harmonicity, t(443) = −3.75, p=1.97*10<sup>−4</sup>; Low Harmonicity, t(443) = −3.77, p=1.82*10<sup>−4</sup>; High Brightness, t(443) = −4.18, p=3.51*10<sup>−5</sup>; Low Brightness, t(443) = −3.26, p=1.21*10<sup>−3</sup>], further validating that the effect of salience is not solely due to low-level acoustic features.</p><p>The reduction of phase-locking to the attended sequences’ rhythm in presence of salient events raises the question whether these ‘attention-grabbing’ instances result in momentary increased neural entrainment to the background scene. While the ambient scene does not contain a steady rate to examine exact phase-locking, its dynamic nature as a natural soundscape allows us to explore the fidelity of encoding of the stimulus envelope before and after salient events. Generally, synchronization of ignored stimuli tends to be greatly suppressed (<xref ref-type="bibr" rid="bib20">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib27">Fuglsang et al., 2017</xref>). Nonetheless, we note a momentary enhancement in decoding accuracy after high salience events compared to a preceding period [paired t-test, t(102) = 2.18, p=0.03] though no such effects are observed in mid [t(113)=−1.09, p=0.28] and low salience [t(107)=0.24, p=0.81] events (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Reconstruction of ignored scene envelopes from neural responses before and after salient events for high, mid and low salience instances.</title><p>The accuracy quantifies the correlation between neural reconstructions and scene envelopes estimated using ridge regression (see Materials and methods). Error bars depict ±1 SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-52984-fig3-v1.tif"/></fig><p>Next, we probe other markers of attentional shift and focus particularly on the Gamma band energy in the neural response (<xref ref-type="bibr" rid="bib60">Ray et al., 2008</xref>). We contrast spectral profiles of neural responses after target tones, salient events and during ‘control’ tones. <xref ref-type="fig" rid="fig4">Figure 4A</xref> depicts a time-frequency profile of neural energy around modulated target tones (0 on the x-axis denotes the start of the target tone). A strong increase in Gamma activity occurs after the onset of target tones and spans a broad spectral bandwidth from 40 to 120 Hz. <xref ref-type="fig" rid="fig4">Figure 4B</xref> shows the same time-frequency profile of neural energy relative to attended tones closest to a salient event. The figure clearly shows a decrease in spectral power post-onset of attended tones nearest salient events which is also spectrally broad, though strongest in a high-Gamma range (∼60–120 Hz).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>High gamma band energy results.</title><p>(<bold>A</bold>) Time frequency spectrogram of neural responses aligned to onsets nearest modulated targets, averaged across central and frontal electrodes. Contours depict the highest 80% and 95% of the gamma response. (<bold>B</bold>) Time frequency spectrogram of tones nearest salient events in the background scene. Contours depict the lowest 80% and 95% of the gamma response. (<bold>C</bold>) Change in energy in the high gamma frequency band (70–110 Hz) across target tones, non-events, and salient events relative to a preceding time window. (<bold>D</bold>) Change in high gamma band energy across high, mid, and low salience events. Error bars depict ±1 SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-52984-fig4-v1.tif"/></fig><p><xref ref-type="fig" rid="fig4">Figure 4C</xref> quantifies the variations of Gamma energy relative to targets, salient events, and control tones as compared to a preceding time window. High-Gamma band energy increases significantly following target tones [t(443)=11.5, p &lt; 10<sup>−7</sup>]; while it drops significantly for attended tones near salient events [t(443)=−6.83, p &lt; 10<sup>−7</sup>]. Control non-event segments show no significant variations in Gamma energy [t(443)=1.5, p=0.13, Bayes factor 0.16] confirming a relatively stable Gamma energy throughout the experimental trials overall. The increase in spectral energy around the Gamma band is significantly different in a direct comparison between target and control tones [t(886)=10.3, p &lt; 10<sup>−7</sup>]. Similarly, the decrease in spectral energy around the Gamma band is significantly different when comparing salient events against control tones [t(886)=6.68, p &lt; 10<sup>−7</sup>]. As with the decrease in tone locking, the Gamma band energy drop is more prominent for higher salience events [t(443)=−7.72, p &lt; 10<sup>−7</sup>], is lower but still significant for mid-level salience events [t(443)=−3.64, p=3.02*10<sup>−4</sup>], but not significant for low salience events [t(443)=0.84, p=0.40, Bayes Factor 0.076] (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). A one-way ANOVA shows that the three levels of salience strength have significantly different changes in gamma power [F(1329)=20.79, p=1.29*10<sup>−9</sup>], with all levels found to be significantly different from each other based on a post-hoc Tukey test.</p><p>Furthermore, the modulation of gamma band energy by both bottom-up and top-down attention is further modulated by subjects’ behavior, quantified using signed error (defined as detected targets minus actual targets - see Materials and methods). Targets in scenes with negative signed error (suggesting that modulated targets were missed due to lower top-down attentional focus) show a smaller increase in gamma power than events in scenes with positive signed error. This difference is significant based on a two-sample t-test [t(886)=−3.96, p=8.06*10<sup>−5</sup>]. Conversely, salient events within negative signed error scenes showed significantly higher increase in gamma than those in positive signed error scenes [t(886)=4.32, p=1.74*10<sup>−5</sup>], suggesting that lower top-down attention indicated higher bottom-up attention, and vice versa. A qualitatively similar result is obtained by grouping subjects’ behavior by error size (absolute error) rather than signed error.</p><p>Given this push-pull competition between bottom-up and top-down attentional responses to tones in the attended rhythmic sequence, we examine similarities between neural loci engaged during these different phases of the neural response. Using the Brainstorm software package (<xref ref-type="bibr" rid="bib74">Tadel et al., 2011</xref>), electrode activations are mapped onto brain surface sources using standardized low resolution brain electromagnetic tomography (sLORETA, see Materials and methods for details). This analysis of localized Gamma activity across cortical voxels examines brain regions <italic>uniquely</italic> engaged while attending to target tones or distracted by a salient event (relative to background activity of control tones).</p><p>We correlate the topography of these top-down and bottom-up brain voxels using sparse canonical correlation analysis (sCCA) (<xref ref-type="bibr" rid="bib62">Roeber et al., 2003</xref>; <xref ref-type="bibr" rid="bib85">Witten and Tibshirani, 2009</xref>; <xref ref-type="bibr" rid="bib44">Lin et al., 2013</xref>) to estimate multivariate similarity between these brain networks at different time lags (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, see Materials and methods for details). Canonical correlation analysis (CCA) is a form of multivariate analysis of correlation where high-dimensional data are compared in order to discover interpretable associations (or correlations) represented as data projections -called canonical vectors- (<xref ref-type="bibr" rid="bib79">Uurtio et al., 2018</xref>). Imposing sparse constraints on this procedure improves interpretability of these projections by confining these mapping to constrained vectors. We cross-correlate brain activation maps at different time lags, and consider that similar brain networks are engaged if a statistically significant correlation emerges from the canonical analysis. <xref ref-type="fig" rid="fig5">Figure 5B</xref> shows that a significant correlation between Gamma activity in brain voxels is observed about 1 s after tone onset, with bottom-up attention to salient events engaging these circuits about 0.5 s earlier relative to activation by top-down attention. The contoured area denotes statistically significant canonical correlations with p &lt; 0.005, and highlights that the overlap in bottom-up and top-down brain networks is slightly offset in time (mostly off the diagonal axis) with an earlier activation by salient events. A closer look at canonical vectors resulting from this correlation analysis reveals the topography of brain networks most contributing to this correlation. Canonical vectors reflect the set of weights applied to each voxel map that results in maximal correlation, and can therefore be represented in voxel space. These canonical vectors show a stable pattern over time lags of significant correlation and reveal a topography with strong contributions of frontal and parietal brain. <xref ref-type="fig" rid="fig5">Figure 5C</xref> shows a representative profile of overlapped canonical vectors obtained from SCCA analysis corresponding to the the time lag shown with an asterisk in <xref ref-type="fig" rid="fig5">Figure 5B</xref> and reveals the engagement of inferior/middle frontal gyrus (IFG/MFG) as well as the superior parietal lobule (SPL).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Analysis of overlapping brain networks.</title><p>(<bold>A</bold>) Sparse canonical correlation analysis (SCCA) is applied to compare top-down (near target) <inline-formula><mml:math id="inf1"><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> and bottom-up (near salient event) <inline-formula><mml:math id="inf2"><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> activation maps. Activations at different time lags <inline-formula><mml:math id="inf3"><mml:msub><mml:mi>τ</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:msub><mml:mi>τ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> are compared using SCCA which yields a canonical correlation value <inline-formula><mml:math id="inf5"><mml:mi>q</mml:mi></mml:math></inline-formula> that maximizes the correlation between linear transformations of the original maps; <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix">max</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. A statistical significance (<inline-formula><mml:math id="inf7"><mml:mi>p</mml:mi></mml:math></inline-formula>-value) of the correlation value <inline-formula><mml:math id="inf8"><mml:mi>q</mml:mi></mml:math></inline-formula> is also estimated at each computation lag using a permutation-based approach (see Materials and methods). (<bold>B</bold>) Canonical correlation values <inline-formula><mml:math id="inf9"><mml:mi>q</mml:mi></mml:math></inline-formula> comparing neural activation patterns after tones near salient events (x-axis) and target tones (y-axis). The contour depicts all canonical correlations with statistical significance less than <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>C</bold>) Projection of canonical vector (mapping function) that yields maximal correlation between the response after salient events and the response after target tones (at the point shown with an asterisk in panel <bold>B</bold>). The red dashed lines are visual guides to highlight earliest point of observed significant correlation as well as time index of correlation point indicated by an asterisk. The overlap is right-lateralized and primarily located within the superior parietal lobule(SPL), the inferior frontal gyrus(IFG), and the medial frontal gyrus(MFG).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-52984-fig5-v1.tif"/></fig><p>Given the profound effects of bottom-up attention on neural responses, we examine the predictive power of changes in tone-locking and Gamma-energy modulations as biomarkers of auditory salience. We train a neural network classifier to infer whether a tone in the attended sequence is aligned with a salient distractor in the background scene or not. <xref ref-type="fig" rid="fig6">Figure 6</xref> shows that classification accuracy for each neural marker, measured by the area under the ROC curve. Both Gamma and tone-locking yield significant predictions above chance [Gamma energy: 68.5% accuracy, t(9) = 4.12, p &lt; 10<sup>−3</sup>; Tone-locking: 73% accuracy, t(9) = 6.03, p &lt; 10<sup>−5</sup>]. Interestingly, the best accuracy is achieved when including both features [79% accuracy, t(9) = 7.20, p &lt; 10<sup>−7</sup>], alluding to the fact that Gamma-band energy and phase-locking may contribute complementary information regarding the presence of attention-grabbing salient events in the background. Furthermore, an estimate of noise floor for this classification (see Materials and methods) yields a prediction range of 2% which is below the improvement in accuracy observed from combining both features. In addition, interaction information (IF) across these features was assessed. IF is an information theory metric that quantifies whether two features are complementary with respect to a class variable (<xref ref-type="bibr" rid="bib88">Yeung, 1991</xref>; <xref ref-type="bibr" rid="bib48">Matsuda, 2000</xref>; <xref ref-type="bibr" rid="bib69">Shuai and Elhilali, 2014</xref>). This measure results in greater mutual information I(F1,F2;S)=0.65 using both gamma energy and tone-locking than the combination of both measures I(F1;S)+I(F2;S)=0.23+0.27, again suggesting a possible complimentary role of both features as biomarkers of salience.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Event Prediction Accuracy.</title><p>A neural network classifier is used to detect whether a tone in the attended sequence coincides with a salient event or not. The figure quantifies the average prediction accuracy (area under the ROC curve) resulting from training (and testing) the classifier using only high gamma band energy, only tone-locking energy, and both features. Error bars depict ±1 SEM. The noise floor is computed by shuffling feature values and labels (coincidence with salient tone).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-52984-fig6-v1.tif"/></fig></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Selective attention heavily modulates brain activity (<xref ref-type="bibr" rid="bib4">Baluch and Itti, 2011</xref>; <xref ref-type="bibr" rid="bib37">Knudsen, 2007</xref>), and bottom-up auditory attention is no exception (<xref ref-type="bibr" rid="bib1">Ahveninen et al., 2013</xref>; <xref ref-type="bibr" rid="bib2">Alho et al., 2014</xref>; <xref ref-type="bibr" rid="bib64">Salmi et al., 2009</xref>). The current study reinforces the view that profound and dynamic changes to neural responses in both sensory and cognitive networks are induced by bottom-up auditory attention. It further demonstrates that these effects compete with top-down attention for neural resources, interrupting behavioral and neural encoding of the attended stimulus by engaging neural circuits that reflect the cognitive salience of ambient distractors. Modulation of both steady-state phase-locked activity in response to the attended stream as well as energy in the high gamma band is so profound that it can accurately identify moments in the neural response coinciding with these salient events with an accuracy of up to 79% relative to control -non salient- moments. The observed changes in both phase-locked activity as well as gamma oscillations dynamically change in <italic>opposing directions</italic> based on engagement of voluntary or stimulus-driven attention. This dichotomy strongly suggests shared limited resources devoted to tracking a sequence of interest, resulting in either enhancement or suppression of neural encoding of this attended target as a result of occasional competing objects (<xref ref-type="bibr" rid="bib38">Lavie, 2005</xref>; <xref ref-type="bibr" rid="bib65">Scalf et al., 2013</xref>). This push-pull action is strongly modulated by the salience of events in the ambient scene which not only reflect the dynamic acoustic profile of natural sounds, but also their higher-level perceptual and semantic representations, hence shedding light on dynamic reorienting mechanisms underlying the brain’s awareness of its surroundings in everyday social environments (<xref ref-type="bibr" rid="bib12">Corbetta et al., 2008</xref>; <xref ref-type="bibr" rid="bib21">Doherty et al., 2017</xref>). Further evidence of this push-pull interaction is also observed when accounting for subjects’ behavior, where trials with lower errors (suggesting higher attentional focus on the dynamic scene) result in higher enhancement of gamma power of targets and lower suppression by salient events; while trials with higher error (suggesting lower attentional focus) result in lower enhancement of targets and higher suppression by salient events.</p><p>The fidelity of the neural representation of an auditory stimulus can be easily quantified by the power of phase-locked responses to the driving rhythm. Enhancement of this phase-locking is accepted as one of the hallmarks of top-down attention and has been observed using a wide range of stimuli from simple tone sequences (similar to targets employed here) to complex signals (e.g. speech) (<xref ref-type="bibr" rid="bib24">Elhilali et al., 2009</xref>; <xref ref-type="bibr" rid="bib50">Mesgarani and Chang, 2012</xref>; <xref ref-type="bibr" rid="bib27">Fuglsang et al., 2017</xref>). By enhancing neural encoding of voluntarily-attended sensory inputs relative to other objects in the scene, attentional feedback effectively facilitates selection and tracking of objects of interest and suppression of irrelevant sensory information (<xref ref-type="bibr" rid="bib37">Knudsen, 2007</xref>; <xref ref-type="bibr" rid="bib68">Shamma and Fritz, 2014</xref>). In the current study, we observe that diverting attention away from the attended stream does in fact suppress the power of phase-locking relative to a baseline level, tapered by the degree of salience of the distracting event (<xref ref-type="fig" rid="fig2">Figure 2C–D</xref>). The locus of this modulated phase-locked activity is generally consistent with core auditory cortex, though no precise topography can be localized from scalp activity only. Still, engagement of core sensory cortex in both enhancement and suppression of phase-locked responses to the attended sequence concurs with a role of auditory cortex in auditory scene analysis and auditory object formation (<xref ref-type="bibr" rid="bib20">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib40">Leaver and Rauschecker, 2010</xref>). Moreover, the drop of steady-state following responses due to distractors coincides with a significant increase in the encoding of the background scene near highly-salient events, as reflected in the accuracy of the ambient scene reconstruction (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Generally, the ability to decode entrainment to a dynamic envelope is a non-trivial task often yielding rather low reconstruction accuracy values (<xref ref-type="bibr" rid="bib56">O'Sullivan et al., 2015</xref>; <xref ref-type="bibr" rid="bib80">Vanthornhout et al., 2019</xref>); and is even more challenging for background sources away from attentional focus (<xref ref-type="bibr" rid="bib20">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib27">Fuglsang et al., 2017</xref>). Still, neural decoding of ignored scenes results in boosted accuracy at specific moments coinciding with salient events. Such enhancement suggests that neural resources are indeed being diverted during those specific moments due to competition for neural representation between the attended target and the salient background object. Such diversion of resources is consistent with prior reports supporting load theory which posits that capacity limitations dictate the degree to which otherwise ignored sensory information can divert processing capability within and across modalities (<xref ref-type="bibr" rid="bib39">Lavie, 2010</xref>; <xref ref-type="bibr" rid="bib64">Salmi et al., 2009</xref>; <xref ref-type="bibr" rid="bib51">Molloy et al., 2018</xref>).</p><p>Further analysis of this push-pull modulation of phase-locked activity rules out an interpretation based on acoustic masking which reduces the target-to-noise ration during distracting events resulting in a weaker auditory stimulation. A comparison of acoustic features throughout the background scene shows that there are no global differences between moments deemed attention-grabbing vs. background segments in the JHU DNSS dataset. Specifically, salient events were not confined to simply louder moments in the scene. Rather, some dense scenes contained ongoing raucous activity that would be perceived as continuously loud but not necessarily salient, except for specific conspicuous moments (e.g. emergence of a human voice or occasional discernible background music in an otherwise busy cafeteria scene). As such, what makes certain events salient is not the instantaneous acoustic profile independent of context. Instead, it is often a <italic>relative</italic> change in the scene statistics reflecting not only acoustic changes but also perceptual and semantic manifestations that normally emerge during everyday social settings. Moreover, excluding the loudest salient events (using both envelope-level and spectral-based measures) still reveals a significant drop of phase-locked activity relative to baseline tones further confirming that this effect cannot be simply attributed to energetic masking of the attended sequence. An additional analysis grouping salient events based on various low-level acoustic attributes (harmonicity, brightness) shows no difference in the degree of suppressed phase-locking induced by different sub-groups of events. Finally, the significant drop in behavioral accuracy in the attended task further confirms the distraction effect likely due to disengagement of attentional focus away from the attended sequence. It should however be noted that the behavioral design of the EEG experiment did not place any target tones in the post-event region in order to avoid any contamination of the neural signal. As such, it was not possible to perform a direct comparison of target tones and salient events and all analyses were compared against preceding time windows to assess the <italic>relative</italic> modulation of neural phase-locked activity as the stimulus sequence unfolded.</p><p>The engagement of executive attention resources in the current paradigm is further observed in enhancements in gamma-band activity, which is also consistent with prior effects linking modulation of gamma-band activity and engagement of cognitive control, particularly with attentional networks. Specifically, enhancement of high-frequency gamma has been reported in conjunction with top-down attention in auditory, visual, and somatosensory tasks (<xref ref-type="bibr" rid="bib15">Debener et al., 2003</xref>; <xref ref-type="bibr" rid="bib75">Tallon-Baudry et al., 2005</xref>; <xref ref-type="bibr" rid="bib5">Bauer et al., 2006</xref>; <xref ref-type="bibr" rid="bib67">Senkowski et al., 2005</xref>). While enhanced gamma oscillations index effective sensory processing due to voluntary attention, they also interface with mnemonic processes, particularly encoding of sensory information in short-term memory (<xref ref-type="bibr" rid="bib66">Sederberg et al., 2003</xref>; <xref ref-type="bibr" rid="bib33">Jensen et al., 2007</xref>). Given the demands of the current paradigm to remember the number of modulated targets in the attended sequence, it is not surprising to observe that the enhancement of gamma energy extends over a period of a few seconds post target onset, in line with previously reported effects of memory consolidation (<xref ref-type="bibr" rid="bib76">Tallon-Baudry and Bertrand, 1999</xref>). In some instances, gamma band activity have also been reported to increase during attentional capture (<xref ref-type="bibr" rid="bib10">Buschman and Miller, 2007</xref>); though two distinctions with the current study design are worth noting: First, our analysis is explicitly aligned to the attended target sequence hence probing distraction effects on activity <italic>relative to</italic> the foreground sound. Second, the modulation of gamma-band activity is very much tied with task demands, the notion of salience, and how it relates to both the perceptual and cognitive load imposed by the task at hand (<xref ref-type="bibr" rid="bib38">Lavie, 2005</xref>). Importantly, the current paradigm emulates natural listening situations, which reaffirms the privileged status of social distractors previously reported in visual tasks (<xref ref-type="bibr" rid="bib21">Doherty et al., 2017</xref>).</p><p>While effects of enhancement of broadband gamma frequency synchronization reflecting an interface of attentional and memory processing have been widely reported, reduction in gamma-band energy due to distraction effects is not commonly observed. Few studies, encompassing data from animal or human subjects, have shown a potential link between modulation of gamma energy and impaired attention, in line with results observed here. Ririe and colleagues reported reduced Local Field Potential activity (including gamma band energy) during audiovisual distraction in medial prefrontal cortex in freely behaving rats (<xref ref-type="bibr" rid="bib61">Ririe et al., 2017</xref>). Bonnefond et al. used Magentoencephalography (MEG) in human listeners and reported reduced gamma for distractors during a memory task very much in line with observed effects in the current task (<xref ref-type="bibr" rid="bib7">Bonnefond and Jensen, 2013</xref>). Moreover, modulation of gamma activity has been ruled out as being associated with novel unexpected stimuli (<xref ref-type="bibr" rid="bib15">Debener et al., 2003</xref>) and is likely linked to ongoing shifts in attentional focus of listeners throughout the task reflecting the interaction between top-down executive control and sound-driven activity.</p><p>Importantly, the analysis of neural networks engaged during this push-pull effect points to overlapping networks spanning frontal and parietal areas involved during both bottom-up and top-down attention with a temporal offset for engagement of these networks (<xref ref-type="fig" rid="fig5">Figure 5</xref>). On the one hand, there is strong evidence in the literature dissociating dorsal and ventral networks of bottom-up and top-down attentional engagement, respectively (<xref ref-type="bibr" rid="bib13">Corbetta and Shulman, 2002</xref>). Such networks have been reported across sensory tasks suggesting a supramodel attentional circuitry that interfaces with sensory networks in guiding selection and tracking of targets of interest while maintaining sensitivity to salient objects in a dynamic scene (<xref ref-type="bibr" rid="bib72">Spagna et al., 2015</xref>). On the other hand, there are overlapping brain regions that underlie top-down as well as bottom-up attentional control particularly centered in the lateral prefrontal cortex, and interface between the two systems to guide behavioral responses that not only account for task-guided goals, but also environmentally-relevant stimuli (<xref ref-type="bibr" rid="bib3">Asplund et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Corbetta et al., 2008</xref>). Our analysis shows that there is a coordinated interaction between these two networks consistent with other reports of a possible shared circuitry, especially in the auditory modality (<xref ref-type="bibr" rid="bib2">Alho et al., 2014</xref>). The current study specifically reveals a consistent temporal offset in the activation of the two networks. A possible interpretation of this dynamic interaction could be that signals from bottom-up attentional networks interrupt activity in the top-down network, potentially reorienting the locus of attention (<xref ref-type="bibr" rid="bib1">Ahveninen et al., 2013</xref>; <xref ref-type="bibr" rid="bib12">Corbetta et al., 2008</xref>; <xref ref-type="bibr" rid="bib64">Salmi et al., 2009</xref>). Building on this interpretation, two possible hypotheses emerge from the earlier engagement of this common orienting network by bottom-up attention. One, that engagement of bottom-up attention sends an inhibitory reset signal in order to reorient attention to socially engaging salient events. Two, that earlier engagement of overlapped networks of attention by salient events reflects reduced memory consolidation due to a distraction effect, which is itself tied to diminished sensory encoding of the attended rhythm.</p><p>The ability to analyze the circuitry underlying the interaction between bottom-up and top-down networks in the current study was facilitated by a powerful multivariate regression technique named Canonical correlation analysis (CCA). This approach attempts to circumvent the limitations of comparisons across source topographies in electroencephalography (EEG) and leverages multivariate techniques to explore relationships between high-dimensional datasets which have been championed with great success in numerous applications including neuroimaging, pharmacological and genomic studies (<xref ref-type="bibr" rid="bib58">Parkhomenko et al., 2009</xref>; <xref ref-type="bibr" rid="bib83">Wang et al., 2015</xref>; <xref ref-type="bibr" rid="bib6">Bilenko and Gallant, 2016</xref>). While applying CCA directly to very high-dimensional data can be challenging or uninterpretable, the use of kernel-based regularizations with constraints such as sparsity as adopted in the current work allows mappings between high-dimensional brain images (<xref ref-type="bibr" rid="bib84">Witten et al., 2009</xref>; <xref ref-type="bibr" rid="bib28">Gao et al., 2015</xref>; <xref ref-type="bibr" rid="bib79">Uurtio et al., 2018</xref>). Here, we adapted the approach proposed by <xref ref-type="bibr" rid="bib63">Rosa et al. (2015)</xref> as it not only regularizes the correlation analysis over sparse constraints, but also optimally defines these constraints in a data-driven fashion. The permutation-based approach yields an optimal way to define statistical significance of observed correlation effects as well as constraints on optimization parameters much in line with cross-validation tests performed in statistical analyses, hence reducing bias by the experimenter in defining parameters (<xref ref-type="bibr" rid="bib85">Witten and Tibshirani, 2009</xref>; <xref ref-type="bibr" rid="bib44">Lin et al., 2013</xref>). The resulting sCCA coefficients delimit brain regions with statistically significant common effects. Consistent with previous findings, the analysis does reveal that areas with statistically significant activations lie in inferior parietal and frontal cortices, particularly inferior and medial temporal gyri as well as the superior parietal lobule (<xref ref-type="bibr" rid="bib2">Alho et al., 2014</xref>).</p><p>In conclusion, the use of a naturalistic experimental paradigm provides a range of nuanced ambient distractions similar to what one experiences in everyday life and demonstrates the dynamic competition for attentional resources between task and socially-relevant stimulus-driven cues. It not only sheds light on profound effects of both top-down and bottom-up attention in continuously shaping brain responses, but it also reveals a steady push-pull competition between these two systems for limited natural resources and active engagement by listeners.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Stimuli</title><p>Auditory stimuli consisted of scenes from a previous study of auditory salience in natural scenes (<xref ref-type="bibr" rid="bib32">Huang and Elhilali, 2017</xref>). This JHU DNSS (Dichotic Natural Salience Soundscapes) database includes twenty natural scenes, each roughly two minutes in length. All scenes were sampled at 22 kHz with a bit rate of 352 kbps, and converted to mono signals whenever applicable. Scenes were drawn from many sources (including Youtube, Freesound, and the BBC Sound Effects Library), and encompassed a wide range of sounds and scenarios. Some scenes were acoustically sparse, such as bowling sounds in an otherwise quiet bowling alley. Others were acoustically dense, such as an orchestra playing in a concert hall or a busy cafeteria.</p><p>Salience of events in each scene was measured using a dichotic listening behavioral paradigm (see [<xref ref-type="bibr" rid="bib32">Huang and Elhilali, 2017</xref>] for full details). Human listeners were presented with two simultaneous scenes, one in each ear, and asked to indicate which scene they were attending to in a continuous fashion. The proportion of subjects attending to a scene compared to all other scenes was defined as salience. Peaks in the derivative of this salience measure were defined as salient events. The strength of salient events was further defined as a linear combination of the salience slope and salience maximum peak in a four second window after event. This salience strength was used to rank the events as high salience, mid salience, or low salience, with one-third of the events falling within each group. Each group contained 117 events, with a total of 351 across all scenes.</p><p>In the current study, each scene was presented one at a time, concurrently and binaurally with a sequence of repeating tones; together forming a single trial (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The tone sequence consisted of repeated 440 Hz tones, each tone 300 ms long, with 10 ms cosine on and off ramps. Tones were presented at a rate of 2.5 Hz. A behavioral study was performed using Amazon’s Mechanical Turk to determine reasonable detection parameters for the tone sequence. Roughly 12.5% of the tones were amplitude modulated at 64 Hz to serve as targets for this behavioral modulation detection task; target tones were randomly positioned throughout the sequence. This experiment was developed using the jsPsych library (<xref ref-type="bibr" rid="bib14">de Leeuw, 2015</xref>) and the Psiturk framework (<xref ref-type="bibr" rid="bib30">Gureckis et al., 2016</xref>). Two modulation depths were tested: 0 dB (easy condition) and −5 dB (hard condition). During EEG recording sessions, tones were presented at a presentation rate of 2.6 Hz with a duration of 300 ms and 10 ms cosine on and off ramps. To avoid any confounds of neural effects between targets and salient events in background scenes, only three to five tones in each trial were amplitude modulated at 64 Hz with a modulation depth of 0 dB. Further, amplitude-modulated targets were constrained to be at least 1.5 s away from any salient event within the concurrent natural scene. A total of 79 modulated targets were present throughout the entire experiment. Stimuli (concurrent scene and tone sequence) were presented binaurally to both ears via a pair of ER-3A insert earphones.</p></sec><sec id="s4-2"><title>Participants and procedure</title><p>Eighty-one subjects (ages 22–60, twenty-seven female) participated in the behavioral study over Mechanical Turk. After a short training period, each subject performed ten trials, alternating between easy and hard conditions. The order of easy and hard conditions was counter-balanced across subjects. For each trial, a natural scene was presented concurrently with the tone sequence; subjects were instructed to devote their attention to the tone sequence and to press the space bar in response to each modulated target tone. Targets with a response between 200 and 800 ms after stimulus onset were considered hits; accuracy was calculated as the percentage of targets that were hits. In order to evaluate any distraction effects from salient events, we contrasted the detection accuracy of two groups of tones: (i) targets occurring between .25 and 1.25 s after an event (the period of the strongest event effect), (ii) targets occurring more than 4 s after any event (thus unlikely to be affected).</p><p>Twelve subjects (ages 18–28, nine female) with no reported history of hearing problems participated in the main EEG experiment. Subjects were tasked with counting the number of amplitude modulated tones within each sequence, and they reported this value at the end of the trial using a keyboard. Each subject heard each of the twenty scenes one time only. They were instructed to focus on the tone sequence and to treat the auditory scene as background noise. All experimental procedures (for both behavioral and EEG experiments) were approved by the Johns Hopkins University Homewood Institutional Review Board (IRB), and subjects were compensated for their participation. The sample size was powered to detect 30% difference in average phase-locking power to the stimulus rhythm between control (non-salient) and salient event epochs (power = 0.9, <inline-formula><mml:math id="inf11"><mml:mi>α</mml:mi></mml:math></inline-formula>=0.05).</p></sec><sec id="s4-3"><title>Electroencephalography</title><p>EEG measurements were obtained using a 128-electrode Biosemi Active Two system (Biosemi Inc, The Netherlands). Electrodes were placed at both left and right mastoids (for referencing) as well as below and lateral to each eye, in order to monitor eye blinks and movements. Data were initially sampled at 2048 Hz. 24 electrodes surrounding and including the Cz electrode were considered to be in the ‘Central’ area, while 22 electrodes surrounding and including the Fz electrode were considered to be in the ‘Frontal’ area (<xref ref-type="bibr" rid="bib69">Shuai and Elhilali, 2014</xref>). Eight electrodes were located in the intersection of these two sets; all statistical tests were performed using the union of the Central and Frontal electrodes.</p><sec id="s4-3-1"><title>Preprocessing</title><p>EEG data were analyzed using MATLAB (Mathworks Inc, MA), with both FieldTrip (<xref ref-type="bibr" rid="bib57">Oostenveld et al., 2011</xref>) and EEGLab (<xref ref-type="bibr" rid="bib16">Delorme and Makeig, 2004</xref>) analysis tools. Neural signals were first demeaned and detrended, then highpass filtered (3rd order Butterworth filter with 0.5 Hz cutoff frequency). Signals were then downsampled to 256 Hz. Power line noise at 60 Hz was removed using the Cleanline plugin for EEGLab (<xref ref-type="bibr" rid="bib54">Mullen, 2012</xref>). Outlier electrodes (around 2%) were removed by excluding channels exceeding 2.5 standard deviation of average energy in 20–40 Hz across all channels. Data were then re-referenced using a common average reference.</p></sec><sec id="s4-3-2"><title>Data analysis</title><p>For event-based analyses (steady-state tone locking and gamma band energy), EEG signals were divided into epochs centered at the onset of a tone of interest (onset of modulated target, tone nearest a salient event, or onset of a control tone away from either target or salient event). Control tones were selected at random with the constraint that no target tone or salient event were contained within the control epoch; in total, 245 such control tones were selected. Each ‘epoch’ was ten seconds in duration (±5 s relative to onset). Noisy epochs were excluded using a joint probability criterion on the amplitude of the EEG data, which rejects trials with improbably high electrode amplitude, defined using both a local (single electrode) threshold of 6 standard deviations away from the mean, as well as a global (all electrodes) threshold of 2 standard deviations. Data were then decomposed using Independent Component Analysis (ICA). Components stereotypical of eyeblink artifacts were manually identified and removed by an experimenter.</p><p>For the tone-locking analysis, epochs were further segmented into 2.3 s (six tones) before and after onsets of interest (target, salient event, and control tone). All 2.3 s segments for a given group were concatenated before taking the Fourier Transform. This concatenation was performed to achieve a higher frequency resolution (given short signal duration and low frequency of interest). The concatenated signal was also zero padded to a length corresponding to 260 event windows in order to maintain the same frequency resolution for all conditions. As the analysis focused on spectral energy around 2.6 Hz, edge effects were minimal at that frequency region. Moreover, any concatenation effects affected all 3 conditions of interest equally. Finally, a normalized peak energy was obtained by dividing the energy at 2.6 Hz by the average energy at surrounding frequencies (between 2.55 and 2.65 Hz excluding the point at 2.6 Hz). The change in tone-locking power was defined as the normalized power using post-onset segments minus the power at pre-onset segments. For illustrative purposes, a tone-locking analysis was also performed over the full scene without dividing the data into epochs. (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><p>High gamma band analysis was also performed by taking the Fourier Transform of the data in two-second windows around attended targets, salient events, or control tones. The energy at each frequency and each electrode was normalized by dividing by the mean power across the entire event window after averaging across trials, and then converted to decibels (dB). The average power between 70 and 110 Hz was taken as the energy in the high gamma band. The change in gamma-band power was defined as the high gamma energy in the window containing 1.5 and 3.5 s post-onset minus high gamma energy between 2.5 and 0.5 s pre-onset.</p><p>All statistical tests that compared pre- and post-onset activity were performed using a paired t-test analysis. Effects were considered statistically significant if <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> and shown with ∗ in figures. Effects with <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> are shown with ∗∗. For non significant values, an additional Bayesian hypothesis test was conducted; a Bayes Factor below 0.33 provides good confidence that the null hypothesis is true (<xref ref-type="bibr" rid="bib19">Dienes, 2014</xref>).</p><p>Events and tones were also divided into groups to test the influence of various conditions. Events were separated by salience strength into high, middle, and low salience events. Events were also divided based on low-level acoustic features, including loudness, harmonicity, and brightness (<xref ref-type="bibr" rid="bib32">Huang and Elhilali, 2017</xref>). Finally, events and tones were divided based on the scene’s signed error, defined as number of target tones detected by a subject minus the number of target tones present during the scene. Positive signed error indicated high false positives while negative errors indicated high misses. A similar analysis was replicated using absolute error (—detected-actual— targets) with qualitatively similar results (data not shown).</p></sec></sec><sec id="s4-4"><title>Envelope decoding</title><p>Decoding of the background scene envelope was performed by training a linear decoder, as described in <xref ref-type="bibr" rid="bib56">O'Sullivan et al. (2015)</xref>. Briefly, a set of weights (<inline-formula><mml:math id="inf14"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>) was calculated to obtain an estimate of the envelope of the natural scenes (<inline-formula><mml:math id="inf15"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐘</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>) from the EEG data (<inline-formula><mml:math id="inf16"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>) according to the equation <inline-formula><mml:math id="inf17"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐘</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐗𝐖</mml:mi></mml:mrow></mml:math></inline-formula>. Stimulus envelopes were extracted using an 8 Hz low-pass filter followed by a Hilbert transform. The EEG data itself was band-pass filtered between 2–8 Hz (both 4<sup>th</sup> order Butterworth filters). Each linear decoder was trained to reconstruct the corresponding stimulus envelope from the EEG data, using time lags of up to 250 ms on all good electrodes. In contrast to the paper by O’Sullivan et al. which used least squares estimation, these weights <inline-formula><mml:math id="inf18"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> were estimated using ridge regression (<xref ref-type="bibr" rid="bib27">Fuglsang et al., 2017</xref>; <xref ref-type="bibr" rid="bib87">Wong et al., 2018</xref>). Here, an additional regularization parameter (<inline-formula><mml:math id="inf19"><mml:mi>λ</mml:mi></mml:math></inline-formula>) is included to mitigate the effects of collinearity between independent variables (here the various EEG channels). The equation for estimating the weights is given by the equation <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐈</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐘</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib87">Wong et al., 2018</xref>). To avoid overfitting, a separate decoder was trained for each of the twenty scenes, using data from the remaining nineteen. The quality of these reconstructions was then evaluated over a range of regularization parameters <inline-formula><mml:math id="inf21"><mml:mi>λ</mml:mi></mml:math></inline-formula> by taking the correlation between the original stimulus envelope and the decoded envelope. A fixed high value of 2<sup>20</sup> was chosen to maximize this overall correlation value for a majority of subjects. Finally, the correlation between estimated and original envelopes within a one second sliding window was calculated as a local measure of attention to the natural scene, and reported as reconstruction accuracy as shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></sec><sec id="s4-5"><title>Gamma band source localization</title><p>Source localization was performed using the Brainstorm analysis software package for MATLAB (<xref ref-type="bibr" rid="bib74">Tadel et al., 2011</xref>) and its implementation of sLORETA (standardized low resolution brain electromagnetic tomography). In contrast to other methods for converting electrode recordings into brain sources, sLORETA is a linear imaging method that is unbiased and has zero localization error (<xref ref-type="bibr" rid="bib59">Pascual-Marqui, 2002</xref>). It is similar to minimum norm estimation, which minimizes the squared error of the estimate, but its estimation of source activities are standardized using the covariance of the data.</p><p>A surface head model was computed using the ICBM 152 brain template (<xref ref-type="bibr" rid="bib49">Mazziotta et al., 2001</xref>) and standard positions for the BioSemi 128 electrode cap, in lieu of individual MRI scans. Gamma band activity was computed by first taking the Fourier Transform of the data in 500 ms windows with 200 ms step size centered around salient events, targets tones, and control areas, then averaging over frequency bands between 70 and 110 Hz. Energy at each voxel on the cortical surface was computed from the EEG gamma band activity using sLORETA, then z-score normalized across time for each trial.</p></sec><sec id="s4-6"><title>Voxel activation correlation</title><p>In order to compare brain networks engaged by bottom-up and top-down attention, we selected voxels of interest from the surface models during target tones <inline-formula><mml:math id="inf22"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, salient events <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and control tones <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each subject. For each time instant <inline-formula><mml:math id="inf25"><mml:mi>t</mml:mi></mml:math></inline-formula>, target voxel activations (salient event activations, respectively) across trials were compared voxel-by-voxel to the control activations at the same relative point in time <inline-formula><mml:math id="inf26"><mml:mi>t</mml:mi></mml:math></inline-formula> using a paired t-test across all trials (significance at <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:math></inline-formula>). A Bonferroni correction was applied to select only voxels that were <italic>uniquely</italic> activated during target tones <inline-formula><mml:math id="inf28"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and salient events <inline-formula><mml:math id="inf29"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> but not controls (<xref ref-type="bibr" rid="bib71">Snedecor and Cochran, 1989</xref>). Using false discovery rates or random field theory to correct for multiple comparisons did not qualitatively change the final outcome of the correlation analysis (<xref ref-type="bibr" rid="bib31">Hsu, 1996</xref>; <xref ref-type="bibr" rid="bib23">Efron, 2010</xref>; <xref ref-type="bibr" rid="bib45">Lindquist and Mejia, 2015</xref>). Based on this analysis, all voxels that were not significantly different from control activations were set to zero, therefore maintaining only voxels that were <italic>uniquely</italic> activated during target and near-event tones. As such, we are excluding shared activity that emerges from sensory response to tone presentations.</p><p>Next, matrices <inline-formula><mml:math id="inf30"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf31"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> representing unique activations across subjects <inline-formula><mml:math id="inf32"><mml:mi>n</mml:mi></mml:math></inline-formula> and voxels <inline-formula><mml:math id="inf33"><mml:mi>v</mml:mi></mml:math></inline-formula> for target and salient event responses at each time instant <inline-formula><mml:math id="inf34"><mml:mi>t</mml:mi></mml:math></inline-formula> were constructed, and columns of each matrix were standardized to 0-mean and 1-variance. A sparse canonical correlation analysis (CCA) was performed for each time index <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>τ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>τ</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> to yield canonical vectors (or weights) <inline-formula><mml:math id="inf37"><mml:msub><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf38"><mml:msub><mml:mi>w</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> at times <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>τ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>τ</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula>. CCA effectively determines linear transformations of <inline-formula><mml:math id="inf41"><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf42"><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> that are maximally correlated with each other following the objective function <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>:</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mi>max</mml:mi><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>S</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>S</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib79">Uurtio et al., 2018</xref>). The weights <inline-formula><mml:math id="inf44"><mml:msub><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:msub><mml:mi>w</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> can be thought of as weights of the linear transformation from original voxel space that maximizes the correlation between the two datasets <inline-formula><mml:math id="inf46"><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf47"><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula>. Here, we specifically implemented a sparse version of CCA which optimizes the same objective function <inline-formula><mml:math id="inf48"><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:math></inline-formula> but imposes sparse, non-negative constraints on <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf50"><mml:msub><mml:mi>w</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> using a least absolute shrinkage and selection operator (L1) penalty function, following the approach proposed in <xref ref-type="bibr" rid="bib63">Rosa et al. (2015)</xref>. This analysis resulted in a similarity measure across the entire dataset (for each time lag pair), while providing a combined set of sparse weights (canonical vectors) that can are easier to interpret in voxel space. A permutation-based approach was used to choose the regularization parameters. This technique performs a bootstrapping test across choices of regularization parameters by independently permuting data samples and maximizing correlation values across all data shufflings (<xref ref-type="bibr" rid="bib63">Rosa et al., 2015</xref>). This same permutation also resulted in an overall significance metric for the final correlation values, by accepting correlation values <inline-formula><mml:math id="inf51"><mml:mi>q</mml:mi></mml:math></inline-formula> that are unlikely to be obtained through a random shuffling of the two datasets (<xref ref-type="bibr" rid="bib63">Rosa et al., 2015</xref>). We performed the sparse-CCA across all time lags relative to targets and salient events in a cross-correlation fashion and scored as significant only the time lags that yielded a correlation value <inline-formula><mml:math id="inf52"><mml:mi>q</mml:mi></mml:math></inline-formula> with statistical significance less than <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:math></inline-formula>. For all time lags for which a statistically significant correlation was obtained, we visually compared the significant canonical vectors <inline-formula><mml:math id="inf54"><mml:msub><mml:mi>w</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:msub><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> which represent the sparse weights that yield a maximal correlation between the two sets and noted common activation areas.</p></sec><sec id="s4-7"><title>Predictions</title><p>Event prediction was performed using an artificial neural network. The analysis was performed on a per-tone basis where tones starting between 1 and 2.5 s before an event were labeled as ‘before’, while tones starting between 1 and 2.5 s following an event were labeled as ‘after’. Any tones that qualified for both groups due to proximity of two salient events were removed from the analysis. Two features were used in the classification: energy at the tone presentation frequency and gamma band energy. Both measures were averaged across central and frontal electrodes for each of the twelve subjects, resulting in twenty-four features used for classification. Gamma band energy was calculated by performing a Fourier transform on the data within a 2 s window around the onset of the tone, and then averaging energy between 70–110 Hz. A longer window was required to capture energy at the lower tone-presentation frequency. Thus, tone-locked energy was calculated by performing a Fourier transform on data within a 5 s window centered around the tone. Then energy at 2.6 Hz was normalized by dividing by neighboring frequencies (between 2.5 and 2.7 Hz). Classification was performed using a 3-layer feed-forward back-propagation neural network with sigmoid and relu activations for the first and second layers respectively and a softmax for the final layer (<xref ref-type="bibr" rid="bib17">Deng, 2013</xref>; <xref ref-type="bibr" rid="bib29">Goodfellow et al., 2016</xref>). The network was trained to classify each tone as either occurring before an event or after an event. Ten-fold cross-validation was performed by randomly dividing all of the events across scenes into ten equal portions. During each of the ten iterations, one group of events was used as test data and the remainder used as training data. A receiver operating characteristic (ROC) curve was constructed by applying varying thresholds on the network’s outputs and prediction accuracy was calculated as the area under the ROC curve. In order to compute a noise-floor for classification, three separate networks were trained, using gamma energy, phase-locking and both features combined consecutively, by shuffling the salience label of the neural marker and its correspondence near or far from a salient event. 10% of the shuffled data was used for testing yielding a random floor of predictions that reflects any underlying noise correlations in the data analysis. In a parallel analysis, mutual information between feature values (<inline-formula><mml:math id="inf56"><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>: Gamma energy, <inline-formula><mml:math id="inf57"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>: tone locking) and salience label (<inline-formula><mml:math id="inf58"><mml:mi>S</mml:mi></mml:math></inline-formula>: near or far salient event) was computed. The metric quantifies interaction information I(F1,F2;S) which reflects whether two features are complementary with respect to a class variable (<xref ref-type="bibr" rid="bib88">Yeung, 1991</xref>; <xref ref-type="bibr" rid="bib48">Matsuda, 2000</xref>; <xref ref-type="bibr" rid="bib70">Singha and Shenoy, 2018</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research was supported by National Institutes of Health grants R01HL133043 and U01AG058532, National Science Foundation research grant 1734744 and Office of Naval Research grants N000141912014, N000141912689 and N000141712736.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Project administration</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All experimental procedures (for behavioral and EEG experiments) were approved by the Johns Hopkins University Homewood Institutional Review Board (IRB), under protocol reference CR00009687 HIRB00008173. Informed consent was obtained from all subjects taking part in the study, and all forms were approved by the Johns Hopkins IRB board. Participation in the study was voluntary.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-52984-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Analysis from all data generated during this study are included in the manuscript and supporting files.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahveninen</surname> <given-names>J</given-names></name><name><surname>Huang</surname> <given-names>S</given-names></name><name><surname>Belliveau</surname> <given-names>JW</given-names></name><name><surname>Chang</surname> <given-names>WT</given-names></name><name><surname>Hämäläinen</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic oscillatory processes governing cued orienting and allocation of auditory attention</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1926</fpage><lpage>1943</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00452</pub-id><pub-id pub-id-type="pmid">23915050</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alho</surname> <given-names>K</given-names></name><name><surname>Salmi</surname> <given-names>J</given-names></name><name><surname>Koistinen</surname> <given-names>S</given-names></name><name><surname>Salonen</surname> <given-names>O</given-names></name><name><surname>Rinne</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Top-down controlled and bottom-up triggered orienting of auditory attention to pitch activate overlapping brain networks</article-title><source>Brain Research</source><volume>1626</volume><fpage>136</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2014.12.050</pub-id><pub-id pub-id-type="pmid">25557401</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asplund</surname> <given-names>CL</given-names></name><name><surname>Todd</surname> <given-names>JJ</given-names></name><name><surname>Snyder</surname> <given-names>AP</given-names></name><name><surname>Marois</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A central role for the lateral prefrontal cortex in goal-directed and stimulus-driven attention</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>507</fpage><lpage>512</lpage><pub-id pub-id-type="doi">10.1038/nn.2509</pub-id><pub-id pub-id-type="pmid">20208526</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baluch</surname> <given-names>F</given-names></name><name><surname>Itti</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mechanisms of top-down attention</article-title><source>Trends in Neurosciences</source><volume>34</volume><fpage>210</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2011.02.003</pub-id><pub-id pub-id-type="pmid">21439656</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bauer</surname> <given-names>M</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Peeters</surname> <given-names>M</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Tactile spatial attention enhances gamma-band activity in somatosensory cortex and reduces low-frequency activity in parieto-occipital Areas</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>490</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5228-04.2006</pub-id><pub-id pub-id-type="pmid">16407546</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bilenko</surname> <given-names>NY</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Pyrcca: regularized kernel canonical correlation analysis in Python and its applications to neuroimaging</article-title><source>Frontiers in Neuroinformatics</source><volume>10</volume><elocation-id>49</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00049</pub-id><pub-id pub-id-type="pmid">27920675</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnefond</surname> <given-names>M</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The role of gamma and alpha oscillations for blocking out distraction</article-title><source>Communicative &amp; Integrative Biology</source><volume>6</volume><elocation-id>e22702</elocation-id><pub-id pub-id-type="doi">10.4161/cib.22702</pub-id><pub-id pub-id-type="pmid">23802042</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borji</surname> <given-names>A</given-names></name><name><surname>Sihite</surname> <given-names>DN</given-names></name><name><surname>Itti</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Quantitative analysis of human-model agreement in visual saliency modeling: a comparative study</article-title><source>IEEE Transactions on Image Processing</source><volume>22</volume><fpage>55</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1109/TIP.2012.2210727</pub-id><pub-id pub-id-type="pmid">22868572</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borji</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>What is a salient object? A dataset and a baseline model for salient object detection</article-title><source>IEEE Transactions on Image Processing</source><volume>24</volume><fpage>742</fpage><lpage>756</lpage><pub-id pub-id-type="doi">10.1109/TIP.2014.2383320</pub-id><pub-id pub-id-type="pmid">25532178</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buschman</surname> <given-names>TJ</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Top-down versus bottom-up control of attention in the prefrontal and posterior parietal cortices</article-title><source>Science</source><volume>315</volume><fpage>1860</fpage><lpage>1862</lpage><pub-id pub-id-type="doi">10.1126/science.1138071</pub-id><pub-id pub-id-type="pmid">17395832</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carmi</surname> <given-names>R</given-names></name><name><surname>Itti</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Visual causes versus correlates of attentional selection in dynamic scenes</article-title><source>Vision Research</source><volume>46</volume><fpage>4333</fpage><lpage>4345</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.08.019</pub-id><pub-id pub-id-type="pmid">17052740</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Patel</surname> <given-names>G</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The reorienting system of the human brain: from environment to theory of mind</article-title><source>Neuron</source><volume>58</volume><fpage>306</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.04.017</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title><source>Nature Reviews Neuroscience</source><volume>3</volume><fpage>201</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1038/nrn755</pub-id><pub-id pub-id-type="pmid">11994752</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Leeuw</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>jsPsych: a JavaScript library for creating behavioral experiments in a web browser</article-title><source>Behavior Research Methods</source><volume>47</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3758/s13428-014-0458-y</pub-id><pub-id pub-id-type="pmid">24683129</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Debener</surname> <given-names>S</given-names></name><name><surname>Herrmann</surname> <given-names>CS</given-names></name><name><surname>Kranczioch</surname> <given-names>C</given-names></name><name><surname>Gembris</surname> <given-names>D</given-names></name><name><surname>Engel</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Top-down attentional processing enhances auditory evoked gamma band activity</article-title><source>NeuroReport</source><volume>14</volume><fpage>683</fpage><lpage>686</lpage><pub-id pub-id-type="doi">10.1097/00001756-200304150-00005</pub-id><pub-id pub-id-type="pmid">12692463</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname> <given-names>A</given-names></name><name><surname>Makeig</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>Journal of Neuroscience Methods</source><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Deep learning: methods and applications</article-title><source>Foundations and Trends in Signal Processing</source><volume>7</volume><fpage>197</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1561/2000000039</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname> <given-names>R</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Neural mechanisms of selective visual attention</article-title><source>Annual Review of Neuroscience</source><volume>18</volume><fpage>193</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id><pub-id pub-id-type="pmid">7605061</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dienes</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Using Bayes to get the most out of non-significant results</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>781</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00781</pub-id><pub-id pub-id-type="pmid">25120503</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title><source>PNAS</source><volume>109</volume><fpage>11854</fpage><lpage>11859</lpage><pub-id pub-id-type="doi">10.1073/pnas.1205381109</pub-id><pub-id pub-id-type="pmid">22753470</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doherty</surname> <given-names>BR</given-names></name><name><surname>Patai</surname> <given-names>EZ</given-names></name><name><surname>Duta</surname> <given-names>M</given-names></name><name><surname>Nobre</surname> <given-names>AC</given-names></name><name><surname>Scerif</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The functional consequences of social distraction: Attention and memory for complex scenes</article-title><source>Cognition</source><volume>158</volume><fpage>215</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2016.10.015</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Duangudom</surname> <given-names>V</given-names></name><name><surname>Anderson</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Identifying salient sounds using dual-task experiments</article-title><conf-name>IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</conf-name><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/WASPAA.2013.6701865</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Efron</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Large Scale Inference</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511761362</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elhilali</surname> <given-names>M</given-names></name><name><surname>Xiang</surname> <given-names>J</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Interaction between attention and bottom-up saliency mediates the representation of foreground and background in an auditory scene</article-title><source>PLOS Biology</source><volume>7</volume><elocation-id>e1000129</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000129</pub-id><pub-id pub-id-type="pmid">19529760</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname> <given-names>MD</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Vincent</surname> <given-names>JL</given-names></name><name><surname>Raichle</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spontaneous neuronal activity distinguishes human dorsal and ventral attention systems</article-title><source>PNAS</source><volume>103</volume><fpage>10046</fpage><lpage>10051</lpage><pub-id pub-id-type="doi">10.1073/pnas.0604187103</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neuronal Gamma-Band synchronization as a fundamental process in cortical computation</article-title><source>Annual Review of Neuroscience</source><volume>32</volume><fpage>209</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135603</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuglsang</surname> <given-names>SA</given-names></name><name><surname>Dau</surname> <given-names>T</given-names></name><name><surname>Hjortkjær</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Noise-robust cortical tracking of attended speech in real-world acoustic scenes</article-title><source>NeuroImage</source><volume>156</volume><fpage>435</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.04.026</pub-id><pub-id pub-id-type="pmid">28412441</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname> <given-names>C</given-names></name><name><surname>Ma</surname> <given-names>Z</given-names></name><name><surname>Ren</surname> <given-names>Z</given-names></name><name><surname>Zhou</surname> <given-names>HH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Minimax estimation in sparse canonical correlation analysis</article-title><source>The Annals of Statistics</source><volume>43</volume><fpage>2168</fpage><lpage>2197</lpage><pub-id pub-id-type="doi">10.1214/15-AOS1332</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname> <given-names>I</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Courville</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Deep Learning</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gureckis</surname> <given-names>TM</given-names></name><name><surname>Martin</surname> <given-names>J</given-names></name><name><surname>McDonnell</surname> <given-names>J</given-names></name><name><surname>Rich</surname> <given-names>AS</given-names></name><name><surname>Markant</surname> <given-names>D</given-names></name><name><surname>Coenen</surname> <given-names>A</given-names></name><name><surname>Halpern</surname> <given-names>D</given-names></name><name><surname>Hamrick</surname> <given-names>JB</given-names></name><name><surname>Chan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>psiTurk: an open-source framework for conducting replicable behavioral experiments online</article-title><source>Behavior Research Methods</source><volume>48</volume><fpage>829</fpage><lpage>842</lpage><pub-id pub-id-type="doi">10.3758/s13428-015-0642-8</pub-id><pub-id pub-id-type="pmid">26428910</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hsu</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>Multiple Comparisons: Theory and Methods</source><publisher-name>Chapman &amp; Hall/CRC</publisher-name><pub-id pub-id-type="doi">10.1002/1097-0258(20000730)19:14&lt;1951::AID-SIM471&gt;3.0.CO;2-W</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>N</given-names></name><name><surname>Elhilali</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Auditory salience using natural soundscapes</article-title><source>The Journal of the Acoustical Society of America</source><volume>141</volume><fpage>2163</fpage><lpage>2176</lpage><pub-id pub-id-type="doi">10.1121/1.4979055</pub-id><pub-id pub-id-type="pmid">28372080</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>Kaiser</surname> <given-names>J</given-names></name><name><surname>Lachaux</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Human gamma-frequency oscillations associated with attention and memory</article-title><source>Trends in Neurosciences</source><volume>30</volume><fpage>317</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2007.05.001</pub-id><pub-id pub-id-type="pmid">17499860</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kailath</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>The divergence and Bhattacharyya distance measures in signal selection</article-title><source>IEEE Transactions on Communications</source><volume>15</volume><fpage>52</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1109/TCOM.1967.1089532</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaya</surname> <given-names>EM</given-names></name><name><surname>Elhilali</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Investigating bottom-up auditory attention</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>327</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00327</pub-id><pub-id pub-id-type="pmid">24904367</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Petkov</surname> <given-names>CI</given-names></name><name><surname>Lippert</surname> <given-names>M</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Mechanisms for allocating auditory attention: an auditory saliency map</article-title><source>Current Biology</source><volume>15</volume><fpage>1943</fpage><lpage>1947</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2005.09.040</pub-id><pub-id pub-id-type="pmid">16271872</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knudsen</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Fundamental components of attention</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>57</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.30.051606.094256</pub-id><pub-id pub-id-type="pmid">17417935</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavie</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Distracted and confused?: selective attention under load</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>75</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.12.004</pub-id><pub-id pub-id-type="pmid">15668100</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavie</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attention, distraction, and cognitive control under load</article-title><source>Current Directions in Psychological Science</source><volume>19</volume><fpage>143</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1177/0963721410370295</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leaver</surname> <given-names>AM</given-names></name><name><surname>Rauschecker</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical representation of natural complex sounds: effects of acoustic features and auditory object category</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>7604</fpage><lpage>7612</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0296-10.2010</pub-id><pub-id pub-id-type="pmid">20519535</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname> <given-names>HI</given-names></name><name><surname>Kidani</surname> <given-names>S</given-names></name><name><surname>Yoneya</surname> <given-names>M</given-names></name><name><surname>Kashino</surname> <given-names>M</given-names></name><name><surname>Furukawa</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Correspondences among pupillary dilation response, subjective salience of sounds, and loudness</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>412</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0898-0</pub-id><pub-id pub-id-type="pmid">26163191</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liao</surname> <given-names>HI</given-names></name><name><surname>Zhao</surname> <given-names>S</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name><name><surname>Kashino</surname> <given-names>M</given-names></name><name><surname>Furukawa</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How the eyes detect acoustic transitions: a study of pupillary responses to transitions between regular and random frequency patterns</article-title><conf-name>Association for Research in Otolaryngology</conf-name></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liégeois-Chauvel</surname> <given-names>C</given-names></name><name><surname>Lorenzi</surname> <given-names>C</given-names></name><name><surname>Trébuchon</surname> <given-names>A</given-names></name><name><surname>Régis</surname> <given-names>J</given-names></name><name><surname>Chauvel</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Temporal envelope processing in the human left and right auditory cortices</article-title><source>Cerebral Cortex</source><volume>14</volume><fpage>731</fpage><lpage>740</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh033</pub-id><pub-id pub-id-type="pmid">15054052</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname> <given-names>D</given-names></name><name><surname>Zhang</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>J</given-names></name><name><surname>Calhoun</surname> <given-names>VD</given-names></name><name><surname>Deng</surname> <given-names>HW</given-names></name><name><surname>Wang</surname> <given-names>YP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Group sparse canonical correlation analysis for genomic data integration</article-title><source>BMC Bioinformatics</source><volume>14</volume><elocation-id>245</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2105-14-245</pub-id><pub-id pub-id-type="pmid">23937249</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindquist</surname> <given-names>MA</given-names></name><name><surname>Mejia</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Zen and the art of multiple comparisons</article-title><source>Psychosomatic Medicine</source><volume>77</volume><fpage>114</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1097/PSY.0000000000000148</pub-id><pub-id pub-id-type="pmid">25647751</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lütkenhöner</surname> <given-names>B</given-names></name><name><surname>Steinsträter</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>High-precision neuromagnetic study of the functional organization of the human auditory cortex</article-title><source>Audiology and Neurotology</source><volume>3</volume><fpage>191</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1159/000013790</pub-id><pub-id pub-id-type="pmid">9575385</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marius 't Hart</surname> <given-names>B</given-names></name><name><surname>Vockeroth</surname> <given-names>J</given-names></name><name><surname>Schumann</surname> <given-names>F</given-names></name><name><surname>Bartl</surname> <given-names>K</given-names></name><name><surname>Schneider</surname> <given-names>E</given-names></name><name><surname>König</surname> <given-names>P</given-names></name><name><surname>Einhäuser</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gaze allocation in natural stimuli: comparing free exploration to head-fixed viewing conditions</article-title><source>Visual Cognition</source><volume>17</volume><fpage>1132</fpage><lpage>1158</lpage><pub-id pub-id-type="doi">10.1080/13506280902812304</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsuda</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Physical nature of higher-order mutual information: intrinsic correlations and frustration</article-title><source>Physical Review E</source><volume>62</volume><fpage>3096</fpage><lpage>3102</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.62.3096</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazziotta</surname> <given-names>J</given-names></name><name><surname>Toga</surname> <given-names>A</given-names></name><name><surname>Evans</surname> <given-names>A</given-names></name><name><surname>Fox</surname> <given-names>P</given-names></name><name><surname>Lancaster</surname> <given-names>J</given-names></name><name><surname>Zilles</surname> <given-names>K</given-names></name><name><surname>Woods</surname> <given-names>R</given-names></name><name><surname>Paus</surname> <given-names>T</given-names></name><name><surname>Simpson</surname> <given-names>G</given-names></name><name><surname>Pike</surname> <given-names>B</given-names></name><name><surname>Holmes</surname> <given-names>C</given-names></name><name><surname>Collins</surname> <given-names>L</given-names></name><name><surname>Thompson</surname> <given-names>P</given-names></name><name><surname>MacDonald</surname> <given-names>D</given-names></name><name><surname>Iacoboni</surname> <given-names>M</given-names></name><name><surname>Schormann</surname> <given-names>T</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name><name><surname>Palomero-Gallagher</surname> <given-names>N</given-names></name><name><surname>Geyer</surname> <given-names>S</given-names></name><name><surname>Parsons</surname> <given-names>L</given-names></name><name><surname>Narr</surname> <given-names>K</given-names></name><name><surname>Kabani</surname> <given-names>N</given-names></name><name><surname>Goualher</surname> <given-names>GL</given-names></name><name><surname>Boomsma</surname> <given-names>D</given-names></name><name><surname>Cannon</surname> <given-names>T</given-names></name><name><surname>Kawashima</surname> <given-names>R</given-names></name><name><surname>Mazoyer</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A probabilistic atlas and reference system for the human brain: international consortium for brain mapping (ICBM)</article-title><source>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</source><volume>356</volume><fpage>1293</fpage><lpage>1322</lpage><pub-id pub-id-type="doi">10.1098/rstb.2001.0915</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title><source>Nature</source><volume>485</volume><fpage>233</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature11020</pub-id><pub-id pub-id-type="pmid">22522927</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molloy</surname> <given-names>K</given-names></name><name><surname>Lavie</surname> <given-names>N</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Auditory figure-ground segregation is impaired by high visual load</article-title><source>The Journal of Neuroscience</source><volume>18</volume><elocation-id>2518-18</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2518-18.2018</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>BCJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>An Introduction to the Psychology of Hearing</source><edition>Sixth ed</edition><publisher-name>Brill</publisher-name></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>BCJ</given-names></name><name><surname>Glasberg</surname> <given-names>BR</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Suggested formulae for calculating auditory‐filter bandwidths and excitation patterns</article-title><source>The Journal of the Acoustical Society of America</source><volume>74</volume><fpage>750</fpage><lpage>753</lpage><pub-id pub-id-type="doi">10.1121/1.389861</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mullen</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>CleanLine EEGLAB plugin</article-title><conf-name>Neuroimaging Informatics Tools and Resources Clearinghouse (NITRC)</conf-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Näätänen</surname> <given-names>R</given-names></name><name><surname>Paavilainen</surname> <given-names>P</given-names></name><name><surname>Rinne</surname> <given-names>T</given-names></name><name><surname>Alho</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The mismatch negativity (MMN) in basic research of central auditory processing <italic>A review</italic></article-title><source>Clinical Neurophysiology</source><volume>12</volume><fpage>2544</fpage><lpage>2590</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2007.04.026</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Sullivan</surname> <given-names>JA</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Evidence for neural computations of temporal coherence in an auditory scene and their enhancement during active listening</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>7256</fpage><lpage>7263</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4973-14.2015</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Schoffelen</surname> <given-names>J-M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkhomenko</surname> <given-names>E</given-names></name><name><surname>Tritchler</surname> <given-names>D</given-names></name><name><surname>Beyene</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Sparse canonical correlation analysis with application to genomic data integration</article-title><source>Statistical Applications in Genetics and Molecular Biology</source><volume>8</volume><fpage>1</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.2202/1544-6115.1406</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascual-Marqui</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Standardized low-resolution brain electromagnetic tomography (sLORETA): technical details</article-title><source>Methods and Findings in Experimental and Clinical Pharmacology</source><volume>24 Suppl D</volume><fpage>5</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">12575463</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ray</surname> <given-names>S</given-names></name><name><surname>Niebur</surname> <given-names>E</given-names></name><name><surname>Hsiao</surname> <given-names>SS</given-names></name><name><surname>Sinai</surname> <given-names>A</given-names></name><name><surname>Crone</surname> <given-names>NE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>High-frequency gamma activity (80-150Hz) is increased in human cortex during selective attention</article-title><source>Clinical Neurophysiology</source><volume>119</volume><fpage>116</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2007.09.136</pub-id><pub-id pub-id-type="pmid">18037343</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ririe</surname> <given-names>DG</given-names></name><name><surname>Boada</surname> <given-names>MD</given-names></name><name><surname>Schmidt</surname> <given-names>BS</given-names></name><name><surname>Martin</surname> <given-names>SJ</given-names></name><name><surname>Kim</surname> <given-names>SA</given-names></name><name><surname>Martin</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Audiovisual distraction increases prefrontal cortical neuronal activity and impairs attentional performance in the rat</article-title><source>Journal of Experimental Neuroscience</source><volume>11</volume><elocation-id>117906951770308</elocation-id><pub-id pub-id-type="doi">10.1177/1179069517703080</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roeber</surname> <given-names>U</given-names></name><name><surname>Widmann</surname> <given-names>A</given-names></name><name><surname>Schröger</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Auditory distraction by duration and location deviants: a behavioral and event-related potential study</article-title><source>Cognitive Brain Research</source><volume>17</volume><fpage>347</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(03)00136-8</pub-id><pub-id pub-id-type="pmid">12880905</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosa</surname> <given-names>MJ</given-names></name><name><surname>Mehta</surname> <given-names>MA</given-names></name><name><surname>Pich</surname> <given-names>EM</given-names></name><name><surname>Risterucci</surname> <given-names>C</given-names></name><name><surname>Zelaya</surname> <given-names>F</given-names></name><name><surname>Reinders</surname> <given-names>AA</given-names></name><name><surname>Williams</surname> <given-names>SC</given-names></name><name><surname>Dazzan</surname> <given-names>P</given-names></name><name><surname>Doyle</surname> <given-names>OM</given-names></name><name><surname>Marquand</surname> <given-names>AF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Estimating multivariate similarity between neuroimaging datasets with sparse canonical correlation analysis: an application to perfusion imaging</article-title><source>Frontiers in Neuroscience</source><volume>9</volume><elocation-id>366</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2015.00366</pub-id><pub-id pub-id-type="pmid">26528117</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salmi</surname> <given-names>J</given-names></name><name><surname>Rinne</surname> <given-names>T</given-names></name><name><surname>Koistinen</surname> <given-names>S</given-names></name><name><surname>Salonen</surname> <given-names>O</given-names></name><name><surname>Alho</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Brain networks of bottom-up triggered and top-down controlled shifting of auditory attention</article-title><source>Brain Research</source><volume>1286</volume><fpage>155</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2009.06.083</pub-id><pub-id pub-id-type="pmid">19577551</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scalf</surname> <given-names>PE</given-names></name><name><surname>Torralbo</surname> <given-names>A</given-names></name><name><surname>Tapia</surname> <given-names>E</given-names></name><name><surname>Beck</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Competition explains limited attention and perceptual resources: implications for perceptual load and dilution theories</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>243</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00243</pub-id><pub-id pub-id-type="pmid">23717289</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sederberg</surname> <given-names>PB</given-names></name><name><surname>Kahana</surname> <given-names>MJ</given-names></name><name><surname>Howard</surname> <given-names>MW</given-names></name><name><surname>Donner</surname> <given-names>EJ</given-names></name><name><surname>Madsen</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Theta and gamma oscillations during encoding predict subsequent recall</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>10809</fpage><lpage>10814</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-34-10809.2003</pub-id><pub-id pub-id-type="pmid">14645473</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senkowski</surname> <given-names>D</given-names></name><name><surname>Talsma</surname> <given-names>D</given-names></name><name><surname>Herrmann</surname> <given-names>CS</given-names></name><name><surname>Woldorff</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multisensory processing and oscillatory gamma responses: effects of spatial selective attention</article-title><source>Experimental Brain Research</source><volume>166</volume><fpage>411</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-2381-z</pub-id><pub-id pub-id-type="pmid">16151775</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname> <given-names>S</given-names></name><name><surname>Fritz</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adaptive auditory computations</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>164</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.01.011</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shuai</surname> <given-names>L</given-names></name><name><surname>Elhilali</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Task-dependent neural representations of salient events in dynamic auditory scenes</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>203</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00203</pub-id><pub-id pub-id-type="pmid">25100934</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singha</surname> <given-names>S</given-names></name><name><surname>Shenoy</surname> <given-names>PP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An adaptive heuristic for feature selection based on complementarity</article-title><source>Machine Learning</source><volume>107</volume><fpage>2027</fpage><lpage>2071</lpage><pub-id pub-id-type="doi">10.1007/s10994-018-5728-y</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Snedecor</surname> <given-names>G</given-names></name><name><surname>Cochran</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1989">1989</year><source>Statistical Methods</source><publisher-loc>Ames</publisher-loc><publisher-name>Iowa State University Press</publisher-name></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spagna</surname> <given-names>A</given-names></name><name><surname>Mackie</surname> <given-names>MA</given-names></name><name><surname>Fan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Supramodal executive control of attention</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>35</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00065</pub-id><pub-id pub-id-type="pmid">25759674</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stropahl</surname> <given-names>M</given-names></name><name><surname>Bauer</surname> <given-names>AR</given-names></name><name><surname>Debener</surname> <given-names>S</given-names></name><name><surname>Bleichner</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Source-Modeling auditory processes of EEG data using EEGLAB and brainstorm</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>309</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00309</pub-id><pub-id pub-id-type="pmid">29867321</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadel</surname> <given-names>F</given-names></name><name><surname>Baillet</surname> <given-names>S</given-names></name><name><surname>Mosher</surname> <given-names>JC</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Leahy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Brainstorm: a User-Friendly application for MEG/EEG analysis</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1155/2011/879716</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tallon-Baudry</surname> <given-names>C</given-names></name><name><surname>Bertrand</surname> <given-names>O</given-names></name><name><surname>Hénaff</surname> <given-names>MA</given-names></name><name><surname>Isnard</surname> <given-names>J</given-names></name><name><surname>Fischer</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Attention modulates gamma-band oscillations differently in the human lateral occipital cortex and fusiform gyrus</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>654</fpage><lpage>662</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh167</pub-id><pub-id pub-id-type="pmid">15371290</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tallon-Baudry</surname> <given-names>C</given-names></name><name><surname>Bertrand</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Oscillatory gamma activity in humans and its role in object representation</article-title><source>Trends in Cognitive Sciences</source><volume>3</volume><fpage>151</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(99)01299-1</pub-id><pub-id pub-id-type="pmid">10322469</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tordini</surname> <given-names>F</given-names></name><name><surname>Bregman</surname> <given-names>AS</given-names></name><name><surname>Cooperstock</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The loud bird doesn’t (always) get the worm: Why computational salience also needs brightness and tempo</article-title><conf-name>Proceedings of the 21st International Conference on Auditory Display (ICAD 2015)</conf-name><fpage>236</fpage><lpage>243</lpage></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Visual attention: the where, what, how and why of saliency</article-title><source>Current Opinion in Neurobiology</source><volume>13</volume><fpage>428</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(03)00105-3</pub-id><pub-id pub-id-type="pmid">12965289</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uurtio</surname> <given-names>V</given-names></name><name><surname>Monteiro</surname> <given-names>JM</given-names></name><name><surname>Kandola</surname> <given-names>J</given-names></name><name><surname>Shawe-Taylor</surname> <given-names>J</given-names></name><name><surname>Fernandez-Reyes</surname> <given-names>D</given-names></name><name><surname>Rousu</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A tutorial on canonical correlation methods</article-title><source>ACM Computing Surveys</source><volume>50</volume><fpage>1</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1145/3136624</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanthornhout</surname> <given-names>J</given-names></name><name><surname>Decruy</surname> <given-names>L</given-names></name><name><surname>Francart</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Effect of task and attention on neural tracking of speech</article-title><source>Frontiers in Neuroscience</source><volume>13</volume><elocation-id>977</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2019.00977</pub-id><pub-id pub-id-type="pmid">31607841</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veale</surname> <given-names>R</given-names></name><name><surname>Hafed</surname> <given-names>ZM</given-names></name><name><surname>Yoshida</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How is visual salience computed in the brain? insights from behaviour, neurobiology and modelling</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><elocation-id>20160113</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0113</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>CA</given-names></name><name><surname>Boehnke</surname> <given-names>SE</given-names></name><name><surname>Itti</surname> <given-names>L</given-names></name><name><surname>Munoz</surname> <given-names>DP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Transient pupil response is modulated by contrast-based saliency</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>408</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3550-13.2014</pub-id><pub-id pub-id-type="pmid">24403141</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>YXR</given-names></name><name><surname>Jiang</surname> <given-names>K</given-names></name><name><surname>Feldman</surname> <given-names>LJ</given-names></name><name><surname>Bickel</surname> <given-names>PJ</given-names></name><name><surname>Huang</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Inferring gene–gene interactions and functional modules using sparse canonical correlation analysis</article-title><source>The Annals of Applied Statistics</source><volume>9</volume><fpage>300</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1214/14-AOAS792</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witten</surname> <given-names>DM</given-names></name><name><surname>Tibshirani</surname> <given-names>R</given-names></name><name><surname>Hastie</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis</article-title><source>Biostatistics</source><volume>10</volume><fpage>515</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1093/biostatistics/kxp008</pub-id><pub-id pub-id-type="pmid">19377034</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witten</surname> <given-names>DM</given-names></name><name><surname>Tibshirani</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Extensions of sparse canonical correlation analysis with applications to genomic data</article-title><source>Statistical Applications in Genetics and Molecular Biology</source><volume>8</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.2202/1544-6115.1470</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname> <given-names>JM</given-names></name><name><surname>Horowitz</surname> <given-names>TS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>What attributes guide the deployment of visual attention and how do they do it?</article-title><source>Nature Reviews Neuroscience</source><volume>5</volume><fpage>495</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1038/nrn1411</pub-id><pub-id pub-id-type="pmid">15152199</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname> <given-names>DDE</given-names></name><name><surname>Fuglsang</surname> <given-names>SA</given-names></name><name><surname>Hjortkjær</surname> <given-names>J</given-names></name><name><surname>Ceolini</surname> <given-names>E</given-names></name><name><surname>Slaney</surname> <given-names>M</given-names></name><name><surname>de Cheveigné</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A comparison of regularization methods in forward and backward models for auditory attention decoding</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>531</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00531</pub-id><pub-id pub-id-type="pmid">30131670</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeung</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>A new outlook on Shannon's information measures</article-title><source>IEEE Transactions on Information Theory</source><volume>37</volume><fpage>466</fpage><lpage>474</lpage><pub-id pub-id-type="doi">10.1109/18.79902</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zelano</surname> <given-names>C</given-names></name><name><surname>Bensafi</surname> <given-names>M</given-names></name><name><surname>Porter</surname> <given-names>J</given-names></name><name><surname>Mainland</surname> <given-names>J</given-names></name><name><surname>Johnson</surname> <given-names>B</given-names></name><name><surname>Bremner</surname> <given-names>E</given-names></name><name><surname>Telles</surname> <given-names>C</given-names></name><name><surname>Khan</surname> <given-names>R</given-names></name><name><surname>Sobel</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Attentional modulation in human primary olfactory cortex</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>114</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1038/nn1368</pub-id><pub-id pub-id-type="pmid">15608635</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname> <given-names>S</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name><name><surname>Dick</surname> <given-names>F</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Furukawa</surname> <given-names>S</given-names></name><name><surname>Liao</surname> <given-names>HI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pupil-linked phasic arousal evoked by violation but not emergence of regularity within rapid sound sequences</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>4030</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-12048-1</pub-id><pub-id pub-id-type="pmid">31492881</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.52984.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing Editor</role><aff><institution>Peking University</institution><country>China</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Grimault</surname><given-names>Nicolas</given-names> </name><role>Reviewer</role><aff><institution>Université Claude Bernard - Lyon</institution><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Everyday life is full of switch between top-down and bottom-up attention. Although top-down attention has been widely studied, the neural mechanism for the saliency-based attention capture and their dynamic interplay, particularly in natural auditory scenes, remains largely unknown. This paper employed an elegant design by using a stream of natural sounds as background and examined how different levels of saliency in the background could modulate the neural response to foreground tones. They provide convincing evidence that the bottom-up and top-down auditory attention have opposite effects and compete for attentional resources.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Push-pull competition between bottom-up and top-down auditory attention to natural soundscapes&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Nicolas Grimault (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This study examined the competing process between top-down and bottom-up auditory attention and the neural mechanisms using a very interesting naturalistic approach. Several methodologies as behavioral, EEG and models are used jointly to build a strong and robust demonstration. The experiments are carefully designed, and the evidence is clear. Meanwhile, there are several major issues brought up by the reviewers that need the authors to address and do additional analysis.</p><p>Essential revisions:</p><p>1) About the conclusion regarding competition between bottom-up and top-down attention. The authors build their conclusion based on the enhancement and inhibition by top-down and bottom-up attention respectively. Meanwhile, the modulated target during top-down attention is still a transient event accompanying a physical change (amplitude modulation) and thus could still be interpreted as reflecting stimulus-driven, salient target instead of pure top-down attentional effect. Therefore, to claim the competition between bottom-up and top-down attention, the authors should justify that the responses to the modulated target are indeed indicative of processing related to top-down attention by new analysis or a new experiment.</p><p>2) About loudness control analysis. Loudness is a subjective measure and reflects a wide range of spectrum. Therefore, the analysis here is not a critical test for the contribution of energetic masking. It would be more appropriate to compare the phase-locking with the energy of the background around the tone frequency (e.g., 1-ERB around 440 Hz), instead of with loudness.</p><p>3) About low-level control analysis. Although the authors have performed control analysis to support that the saliency-induced effect is not due to loudness by excluding events with the highest loudness, this seems not enough to control low-level acoustic features. Could the authors use another low-level index, for example, the overall loudness, and did the same analysis to compare response to tones close to high loudness and tones close to low loudness? This would be a better control analysis to exclude at least loudness factors.</p><p>4) About the neural source analysis. The bottom-up and top-down attention has been previously posited to originate in dissociated brain networks, but here do the results suggest shared neural network with different temporal lag? Please add more clarifications.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.52984.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) About the conclusion regarding competition between bottom-up and top-down attention. The authors build their conclusion based on the enhancement and inhibition by top-down and bottom-up attention respectively. Meanwhile, the modulated target during top-down attention is still a transient event accompanying a physical change (amplitude modulation) and thus could still be interpreted as reflecting stimulus-driven, salient target instead of pure top-down attentional effect. Therefore, to claim the competition between bottom-up and top-down attention, the authors should justify that the responses to the modulated target are indeed indicative of processing related to top-down attention by new analysis or a new experiment.</p></disp-quote><p>In order to confirm that the neural response to targets is largely driven by top-down attention, we conducted a new analysis that focused on behavioral responses at the end of each scene. Although subjects do not make an immediate response to each modulated target, they do input the total number of targets they detected at the end of each trial, which can indicate whether or not they missed targets during the scene. Thus, for each subject, scenes were grouped by signed error, calculated as the number of targets reported by the subjects (detected targets) minus the number of targets in the scene (actual targets). A negative signed error, indicates that more actual targets were missed, suggesting that top-down attention was more distracted during these trials. Using signed error to breakdown responses with positive versus negative errors, we find that energy at both the 2.6 tone presentation frequency and gamma band energy exhibited higher increases after modulated target tones during the scenes with positive signed error than negative. The difference between the changes in gamma energy was highly significant [t(886) = 3.96, p = 8.06e-5]. The difference in tone-locking was present but not significant (t(886) = 0.73, p = 0.47), perhaps owing to reduced ability to isolate that specific frequency with too few epochs. Still, the strongly reduced effect in the gamma band with lower top-down attention clearly indicates that the effect is not solely a product of bottom-up processing of acoustic changes in the modulated tone. We replicated the same analysis using absolute error (absolute value of detected minus actual targets) and found qualitatively similar results (albeit at smaller statistical power, though still significant).</p><p>Along the same lines, we examined related aspects to the push-pull nature of the interaction between bottom and top-down attention by examining salient events in negative vs. positive signed error scenes. This analysis complements the results from top-down attention (discussed earlier) and focuses on effects of salience. In this analysis, we note that salient events in negative error scenes (more distraction) showed significantly higher increase in gamma than those in positive signed error cases [t(886) = 4.32, p = 1.74e-5]; suggesting that lower top-down attention indicated higher bottom-up attention, and vice versa.</p><p>Together, the increase/decrease in gamma energy with top-down/bottom-up attention with subjects’ behavior provides further support to a push-pull interaction between both forms of cognitive demands on subjects. These new analyses have been added to the text.</p><disp-quote content-type="editor-comment"><p>2) About loudness control analysis. Loudness is a subjective measure and reflects a wide range of spectrum. Therefore, the analysis here is not a critical test for the contribution of energetic masking. It would be more appropriate to compare the phase-locking with the energy of the background around the tone frequency (e.g., 1-ERB around 440 Hz), instead of with loudness.</p></disp-quote><p>As suggested by the reviewer, the phase-locking analysis was repeated after removing events with the highest energy in a range of 1-ERB around 440 Hz (calculated from Moore and Glasberg, 1983). After removing the events with the highest energy (top 25%) in that band, there was still a very significant drop in tone-locking following the events [t(443) = -4.93, p = 1.17e-6]. This new analysis is now included in the text.</p><disp-quote content-type="editor-comment"><p>3) About low-level control analysis. Although the authors have performed control analysis to support that the saliency-induced effect is not due to loudness by excluding events with the highest loudness, this seems not enough to control low-level acoustic features. Could the authors use another low-level index, for example, the overall loudness, and did the same analysis to compare response to tones close to high loudness and tones close to low loudness? This would be a better control analysis to exclude at least loudness factors.</p></disp-quote><p>A new analysis was conducted by splitting the events into two groups based on low-level acoustic features. Since loudness was quantified in two ways (using an overall measure and spectral based ERB-centered measure, see point #2 above), we explored additional low-level attributes. Harmonicity (how strongly pitched the sound is) and brightness (the centroid of the frequency spectrum) were examined since they were deemed important features based on their contribution to auditory salience (Huang and Elhilali, 2017). For both features, high and low events both had significant decreases in phase-locking and gamma power after events.</p><p>High harmonicity, phase-locking, [t(443) = -3.75, p = 1.97e-4]</p><p>Low harmonicity, phase-locking, [t(443) = -3.77, p = 1.82e-4]</p><p>High harmonicity, gamma energy, [t(443) = -3.61, p = 3.42e-4]</p><p>Low harmonicity, gamma energy, [t(443) = -7.68, p = 1.03e-13]</p><p>High brightness, phase-locking, [t(443) = -4.18, p = 3.51e-5]</p><p>Low brightness, phase-locking, [t(443) = -3.26, p = 1.21e-3]</p><p>High brightness, gamma energy, [t(443) = -9.30, p = 6.35e-19]</p><p>Low brightness, gamma energy, [t(443) = -2.99, p = 2.92e-3]</p><p>We added the results of this analysis on phase-locking to the text, in order to strengthen the claim that modulation of neural responses was not due to specific acoustic attributes.</p><disp-quote content-type="editor-comment"><p>4) About the neural source analysis. The bottom-up and top-down attention has been previously posited to originate in dissociated brain networks, but here do the results suggest shared neural network with different temporal lag? Please add more clarifications.</p></disp-quote><p>While there is indeed evidence in favor of distinct brain networks activated by top-down and bottom-up attention, a number of studies have already established some common activation areas from both forms of attention. For instance, Alho et al., 2015, have suggested that auditory attention may engage more overlap in bottom-up and top-down networks than visual attention, spanning temporal, parietal and frontal areas. This observation is in fact not unique to audition. Asplund et al., 2010, have used a visual paradigm and found convergent activations of stimulus driven and goal-directed attention in lateral prefrontal cortex. As such, the general observation of an overlapping activation is not surprising. The novelty with the SCCA analysis is to provide a better insight on the time lag of this common activation as well as pinpoint dynamics and span of this common activation. It is important to note that we confined the CCA to ‘correlated’ regions by design, after accounting for common activations due to the sensory drive (by removing overlap with control tones). Before applying CCA on voxel activations, we did observe that bottom-up attention elicited by salient events shows more activation in the lateral sulcus, presumably associated with primary auditory cortex; conversely, the top-down attention directed towards targets shows more activation in the superior parietal / frontal areas consistent with sustained attention. These are observations that were not included in the manuscript as we feel that proper statistical analyses are required in order to properly define these regions, which is not easily achieved with the current paradigm design and using EEG. Instead, SCCA offers the possibility of making statistically robust conclusions about underlying correlations in the data. We have edited the Discussion to better nuance the observation of existence of overlapped regions between bottom-up and top-down attention, as we feel this is an interesting conclusion that merits further work across modalities in order to better understand its implications.</p></body></sub-article></article>