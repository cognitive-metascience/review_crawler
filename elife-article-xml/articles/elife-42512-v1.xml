<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">42512</article-id><article-id pub-id-type="doi">10.7554/eLife.42512</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Short Report</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Image content is more important than Bouma’s Law for scene metamers</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-122742"><name><surname>Wallis</surname><given-names>Thomas SA</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7431-4852</contrib-id><email>thomas.wallis@uni-tuebingen.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-122743"><name><surname>Funke</surname><given-names>Christina M</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-21210"><name><surname>Ecker</surname><given-names>Alexander S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2392-5105</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-20405"><name><surname>Gatys</surname><given-names>Leon A</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/><xref ref-type="fn" rid="pa1">‡</xref></contrib><contrib contrib-type="author" id="author-122744"><name><surname>Wichmann</surname><given-names>Felix A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2592-634X</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-122748"><name><surname>Bethge</surname><given-names>Matthias</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6417-7812</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Werner Reichardt Center for Integrative Neuroscience</institution><institution>Eberhard Karls Universität Tübingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>Bernstein Center for Computational Neuroscience</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Center for Neuroscience and Artificial Intelligence</institution><institution>Baylor College of Medicine</institution><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Institute for Theoretical Physics</institution><institution>Eberhard Karls Universität Tübingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Neural Information Processing Group, Faculty of Science</institution><institution>Eberhard Karls Universität Tübingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff6"><label>6</label><institution>Max Planck Institute for Biological Cybernetics</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="editor"><name><surname>Herzog</surname><given-names>Michael</given-names></name><role>Reviewing Editor</role><aff><institution>EPFL</institution><country>Switzerland</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>‡</label><p>Apple Inc, Cupertino, United States</p></fn><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>30</day><month>04</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e42512</elocation-id><history><date date-type="received" iso-8601-date="2018-10-03"><day>03</day><month>10</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2019-03-09"><day>09</day><month>03</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Wallis et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Wallis et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-42512-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.42512.001</object-id><p>We subjectively perceive our visual field with high fidelity, yet peripheral distortions can go unnoticed and peripheral objects can be difficult to identify (crowding). Prior work showed that humans could not discriminate images synthesised to match the responses of a mid-level ventral visual stream model when information was averaged in receptive fields with a scaling of about half their retinal eccentricity. This result implicated ventral visual area V2, approximated ‘Bouma’s Law’ of crowding, and has subsequently been interpreted as a link between crowding zones, receptive field scaling, and our perceptual experience. However, this experiment never assessed natural images. We find that humans can easily discriminate real and model-generated images at V2 scaling, requiring scales at least as small as V1 receptive fields to generate metamers. We speculate that explaining why scenes look as they do may require incorporating segmentation and global organisational constraints in addition to local pooling.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.42512.002</object-id><title>eLife digest</title><p>As you read this digest, your eyes move to follow the lines of text. But now try to hold your eyes in one position, while reading the text on either side and below: it soon becomes clear that peripheral vision is not as good as we tend to assume. It is not possible to read text far away from the center of your line of vision, but you can see ‘something’ out of the corner of your eye. You can see that there is text there, even if you cannot read it, and you can see where your screen or page ends. So how does the brain generate peripheral vision, and why does it differ from what you see when you look straight ahead?</p><p>One idea is that the visual system averages information over areas of the peripheral visual field. This gives rise to texture-like patterns, as opposed to images made up of fine details. Imagine looking at an expanse of foliage, gravel or fur, for example. Your eyes cannot make out the individual leaves, pebbles or hairs. Instead, you perceive an overall pattern in the form of a texture. Our peripheral vision may also consist of such textures, created when the brain averages information over areas of space.</p><p>Wallis, Funke et al. have now tested this idea using an existing computer model that averages visual input in this way. By giving the model a series of photographs to process, Wallis, Funke et al. obtained images that should in theory simulate peripheral vision. If the model mimics the mechanisms that generate peripheral vision, then healthy volunteers should be unable to distinguish the processed images from the original photographs. But in fact, the participants could easily discriminate the two sets of images. This suggests that the visual system does not solely use textures to represent information in the peripheral visual field. Wallis, Funke et al. propose that other factors, such as how the visual system separates and groups objects, may instead determine what we see in our peripheral vision.</p><p>This knowledge could ultimately benefit patients with eye diseases such as macular degeneration, a condition that causes loss of vision in the center of the visual field and forces patients to rely on their peripheral vision.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual perception</kwd><kwd>scene appearance</kwd><kwd>texture perception</kwd><kwd>crowding</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id><institution>Bundesministerium für Bildung und Forschung</institution></institution-wrap></funding-source><award-id>FKZ: 01GQ1002</award-id><principal-award-recipient><name><surname>Wichmann</surname><given-names>Felix A</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>BE 3848/2-1</award-id><principal-award-recipient><name><surname>Bethge</surname><given-names>Matthias</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>Priority program 1527</award-id><principal-award-recipient><name><surname>Wichmann</surname><given-names>Felix A</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>276693517; SFB 1233</award-id><principal-award-recipient><name><surname>Wallis</surname><given-names>Thomas SA</given-names></name><name><surname>Funke</surname><given-names>Christina M</given-names></name><name><surname>Wichmann</surname><given-names>Felix A</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100005156</institution-id><institution>Alexander von Humboldt-Stiftung</institution></institution-wrap></funding-source><award-id>Thomas Wallis</award-id><principal-award-recipient><name><surname>Wallis</surname><given-names>Thomas SA</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Peripheral appearance models emphasising pooling processes that depend on retinal eccentricity will instead need to explore input-dependent grouping and segmentation.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Vision science seeks to understand why things look as they do (<xref ref-type="bibr" rid="bib47">Koffka, 1935</xref>). Typically, our entire visual field looks subjectively crisp and clear. Yet our perception of the scene falling onto the peripheral retina is actually limited by at least three distinct sources: the optics of the eye, retinal sampling, and the mechanism(s) giving rise to crowding, in which our ability to identify and discriminate objects in the periphery is limited by the presence of nearby items (<xref ref-type="bibr" rid="bib11">Bouma, 1970</xref>; <xref ref-type="bibr" rid="bib66">Pelli and Tillman, 2008</xref>). Many other phenomena also demonstrate striking ‘failures’ of visual perception, for example change blindness (<xref ref-type="bibr" rid="bib70">Rensink et al., 1997</xref>; <xref ref-type="bibr" rid="bib62">O'Regan et al., 1999</xref>) and inattentional blindness (<xref ref-type="bibr" rid="bib55">Mack and Rock, 1998</xref>), though there is some discussion as to what extent these are distinct from crowding (<xref ref-type="bibr" rid="bib75">Rosenholtz, 2016</xref>). Whatever the case, it is clear that we can be insensitive to significant changes in the world despite our rich subjective experience.</p><p>Visual crowding has been characterised as compulsory texture perception (<xref ref-type="bibr" rid="bib64">Parkes et al., 2001</xref>; <xref ref-type="bibr" rid="bib51">Lettvin, 1976</xref>) and compression (<xref ref-type="bibr" rid="bib6">Balas et al., 2009</xref>; <xref ref-type="bibr" rid="bib73">Rosenholtz et al., 2012a</xref>). This idea entails that we cannot perceive the precise structure of the visual world in the periphery. Rather, we are aware only of some set of summary statistics or ensemble properties of visual displays, such as the average size or orientation of a group of elements (<xref ref-type="bibr" rid="bib3">Ariely, 2001</xref>; <xref ref-type="bibr" rid="bib25">Dakin and Watt, 1997</xref>). One of the appeals of the summary statistic idea is that it can be directly motivated from the perspective of efficient coding as a form of compression. Image-computable texture summary statistics have been shown to be correlated with human performance in various tasks requiring the judgment of peripheral information, such as crowding and visual search (<xref ref-type="bibr" rid="bib73">Rosenholtz et al., 2012a</xref>; <xref ref-type="bibr" rid="bib6">Balas et al., 2009</xref>; <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="bib75">Rosenholtz, 2016</xref>; <xref ref-type="bibr" rid="bib27">Ehinger and Rosenholtz, 2016</xref>). Recently, it has even been suggested that summary statistics underlie our rich phenomenal experience itself—in the absence of focussed attention, we perceive only a texture-like visual world (<xref ref-type="bibr" rid="bib18">Cohen et al., 2016</xref>).</p><p>Across many tasks, summary statistic representations seem to capture aspects of peripheral vision when the scaling of their pooling regions corresponds to ‘Bouma’s Law’ (<xref ref-type="bibr" rid="bib73">Rosenholtz et al., 2012a</xref>; <xref ref-type="bibr" rid="bib6">Balas et al., 2009</xref>; <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="bib92">Wallis and Bex, 2012</xref>; <xref ref-type="bibr" rid="bib27">Ehinger and Rosenholtz, 2016</xref>). Bouma’s Law states that objects will crowd (correspondingly, statistics will be pooled) over spatial regions corresponding to approximately half the retinal eccentricity (<xref ref-type="bibr" rid="bib11">Bouma, 1970</xref>; <xref ref-type="bibr" rid="bib66">Pelli and Tillman, 2008</xref>; though see <xref ref-type="bibr" rid="bib72">Rosen et al., 2014</xref>). While the precise value of Bouma’s law can vary substantially even over different visual quadrants within an individual (see e.g. <xref ref-type="bibr" rid="bib67">Petrov and Meleshkevich, 2011</xref>), we refer here to the broader notion that summary statistics are pooled over an area that increases linearly with eccentricity, rather than the exact factor of this increase (the exact factor becomes important in the paragraph below). If the visual system does indeed represent the periphery using summary statistics, then Bouma’s scaling implies that as retinal eccentricity increases, increasingly large regions of space are texturised by the visual system. If a model captured these statistics and their pooling, and the model was amenable to being run in a generative mode, then images could be created that are indistinguishable from the original despite being physically different (metamers). These images would be equivalent to the model and to the human visual system (<xref ref-type="bibr" rid="bib32">Freeman and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="bib90">Wallis et al., 2016</xref>; <xref ref-type="bibr" rid="bib68">Portilla and Simoncelli, 2000</xref>; <xref ref-type="bibr" rid="bib46">Koenderink et al., 2017</xref>).</p><p><xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref> developed a model (hereafter, FS-model) in which texture-like summary statistics are pooled over spatial regions inspired by the receptive fields in primate visual cortex. The size of neural receptive fields in ventral visual stream areas increases as a function of retinal eccentricity, and as one moves downstream from V1 to V2 and V4 at a given eccentricity. Each visual area therefore has a signature scale factor, defined as the ratio of the receptive field diameter to retinal eccentricity (<xref ref-type="bibr" rid="bib32">Freeman and Simoncelli, 2011</xref>). Similarly, the pooling regions of the FS-model also increase with retinal eccentricity with a definable scale factor. New images can be synthesised that match the summary statistics of original images at this scale factor. As scale factor increases, texture statistics are pooled over increasingly large regions of space, resulting in more distorted synthesised images relative to the original (that is, more information is discarded).</p><p>The maximum scale factor for which the images remain indistinguishable (the critical scale) characterises perceptually-relevant compression in the visual system’s representation. If the scale factor of the model corresponded to the scaling of the visual system in the responsible visual area, and information in upstream areas was irretrievably lost, then the images synthesised by the model should be indistinguishable while discarding as much information as possible. That is, we seek the maximum compression that is perceptually lossless:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>crit</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mi>max</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo rspace="5.3pt">:</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>s</mml:mi></mml:msub><mml:mo rspace="5.3pt">,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf1"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>crit</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the critical scale for an image <inline-formula><mml:math id="inf2"><mml:mi>I</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf3"><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> is a synthesised image at scale <inline-formula><mml:math id="inf4"><mml:mi>s</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mi>d</mml:mi></mml:math></inline-formula> is a perceptual distance. Larger scale factors discard more information than the relevant visual area and therefore the images should look different. Smaller scale factors preserve information that could be discarded without any perceptual effect.</p><p>Crucially, it is the <italic>minimum</italic> critical scale over images that is important for the scaling theory. If the visual system computes summary statistics over fixed (image-independent) pooling regions in the same way as the model, then the model must be able to produce metamers for all images. While images may vary in their individual critical scales, the image with the smallest critical scale determines the maximum compression for appearance to be matched by the visual system in general, assuming an image-independent representation:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>system</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mi>min</mml:mi><mml:mi>I</mml:mi></mml:munder><mml:mo>⁡</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>crit</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Freeman and Simoncelli showed that the largest scale factor for which two synthesised images could not be told apart was approximately 0.5, or pooling regions of about half the eccentricity. This scaling matched the signature of area V2, and also matched the approximate value of Bouma’s Law. Subsequently, this result has been interpreted as demonstrating a link between receptive field scaling, crowding, and our rich phenomenal experience (e.g. <xref ref-type="bibr" rid="bib10">Block, 2013</xref>; <xref ref-type="bibr" rid="bib18">Cohen et al., 2016</xref>, <xref ref-type="bibr" rid="bib50">Landy, 2013</xref>, <xref ref-type="bibr" rid="bib60">Movshon and Simoncelli, 2014</xref>, <xref ref-type="bibr" rid="bib77">Seth, 2014</xref>). These interpretations imply that the FS-model creates metamers for natural scenes. However, observers in Freeman and Simoncelli’s experiment never saw the original scenes, but only compared synthesised images to each other. Showing that two model samples are indiscriminable from each other could yield trivial results. For example, two white noise samples matched to the mean and contrast of a natural scene would be easy to discriminate from the scene but hard to discriminate from each other. Furthemore, since synthesised images represent a specific subset of images, and the system critical scale <inline-formula><mml:math id="inf6"><mml:msub><mml:mi>s</mml:mi><mml:mi>system</mml:mi></mml:msub></mml:math></inline-formula> is the minimum over all possible images, the <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>s</mml:mi><mml:mi>system</mml:mi></mml:msub></mml:math></inline-formula> estimated in <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref> is likely to be an overestimate.</p><p>No previous paper has estimated <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>s</mml:mi><mml:mi>system</mml:mi></mml:msub></mml:math></inline-formula> for the FS-model using natural images. <xref ref-type="bibr" rid="bib90">Wallis et al., 2016</xref> tested the related <xref ref-type="bibr" rid="bib68">Portilla and Simoncelli (2000)</xref> model textures, and found that observers could easily discriminate these textures from original images in the periphery. However, the Portilla and Simoncelli model makes no explicit connection to neural receptive field scaling. In addition, relative to the textures tested by <xref ref-type="bibr" rid="bib90">Wallis et al., 2016</xref>, the pooling region overlap used in the FS-model provides a strong constraint on the resulting syntheses, making the images much more similar to the originals. It is therefore still possible that the FS-model produces metamers for natural scenes for scale factors of 0.5.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Measuring critical scale in the FS-model</title><p>We tested whether the FS-model can produce metamers using an oddity design in which the observer had to pick the odd image out of three successively shown images (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). In a three-alternative oddity paradigm, performance for metamerism would lie at 1/3 (dashed horizontal line, <xref ref-type="fig" rid="fig1">Figure 1F</xref>). We used two comparison conditions: either observers compared two model syntheses to each other (synth vs synth; as in <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli, 2011</xref>) or the original image to a model synthesis (orig vs synth). As in the original paper (<xref ref-type="bibr" rid="bib32">Freeman and Simoncelli, 2011</xref>) we measured the performance of human observers for images synthesised with different scale factors (using Freeman and Simoncelli’s code, see Materials and methods). To quantify the critical scale factor we fit the same nonlinear model as Freeman and Simoncelli, which parameterises sensitivity as a function of critical scale and gain, but using a mixed-effects model with random effects of participant and image (see Materials and methods).</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.003</object-id><label>Figure 1.</label><caption><title>Two texture pooling models fail to match arbitrary scene appearance.</title><p>We selected ten scene-like (<bold>A</bold>) and ten texture-like (<bold>B</bold>) images from the MIT 1003 dataset (<xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>, <ext-link ext-link-type="uri" xlink:href="https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html</ext-link>) and synthesised images to match them using the Freeman and Simoncelli model (FS scale 0.46 shown) or a model using CNN texture features (CNN 32; example scene and texture-like stimuli shown in (<bold>C</bold>) and (<bold>D</bold>) respectively). Images reproduced under a CC-BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">https://creativecommons.org/licenses/by/3.0/</ext-link>) with changes as described in the Methods. (<bold>E</bold>): The oddity paradigm. Three images were presented in sequence, with two being physically-identical and one being the oddball. Participants indicated which image was the oddball (1, 2 or 3). On 'orig vs synth’ trials participants compared real and synthesised images, whereas on 'synth vs synth’ trials participants compared two images synthesised from the same model. (<bold>F</bold>): Performance as a function of scale factor (pooling region diameter divided by eccentricity) in the Freeman-Simoncelli model (circles) and for the CNN 32 model (triangles; arbitrary x-axis location). Points show grand mean <inline-formula><mml:math id="inf9"><mml:mrow><mml:mo>±</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> SE over participants; faint lines link individual participant performance levels (FS-model) and faint triangles show individual CNN 32 performance. Solid curves and shaded regions show the fit of a nonlinear mixed-effects model estimating the critical scale and gain. Participants are still above chance for scene-like images in the original vs synth condition for the lowest scale factor of the FS-model we could generate, and for the CNN 32 model, indicating that neither model succeeds in producing metamers. (<bold>G</bold>): When comparing original and synthesised images, estimated critical scales (scale at which performance rises above chance) are lower for scene-like than for texture-like images. Points with error bars show population mean and 95% credible intervals. Triangles show posterior means for participants; diamonds show posterior means for images. Black squares show critical scale estimates of the four participants from <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref> (x-position jittered to reduce overplotting); shaded regions denote the receptive field scaling of V1 and V2 estimated by <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref>. Data reproduced from <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref> using WebPlotDigitizer v. 4.0.0 (Rohatgi, A., software under the GNU Affero General Public License v3, <ext-link ext-link-type="uri" xlink:href="https://www.gnu.org/licenses/agpl-3.0.en.html">https://www.gnu.org/licenses/agpl-3.0.en.html</ext-link>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.42512.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>The ten scene-like and ten texture-like images used in our main experiments, along with example syntheses from the FS-0.46 and CNN 32 models (best viewed with zoom).</title><p>Images reproduced from the MIT 1003 Database (<xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>, <ext-link ext-link-type="uri" xlink:href="https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html</ext-link>) under a CC-BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">https://creativecommons.org/licenses/by/3.0/</ext-link>) with changes as described in the Methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-fig1-figsupp1-v1.tif"/></fig></fig-group><p>We used 20 images to test the FS model. These images are split into two classes of ten images each, which we labelled ‘scene-like’ and ‘texture-like’. The distinction of these two classes is based on the results of a pilot experiment with a model we developed, which is inspired by the FS model but based on a different set of image features (those extracted by a convolutional neural network; see Materials and methods and <xref ref-type="fig" rid="app1fig1">Appendix 2—figure 1</xref>). In this pilot experiment, we found that some images are easier to discriminate than others (<xref ref-type="fig" rid="app2fig7">Appendix 2—figure 7—figure 9</xref>). Easily-discriminable images tended to contain larger areas of inhomogenous structure, long edges, borders between different surfaces or objects, and angled edges providing perspective cues (‘scene-like’). Difficult images tended to contain more visual textures: homogenous structure, patterned content, or materials (‘texture-like’“). For example, images from the first class tended to contain more structure such as faces, text, skylines, buildings, and clearly segmented objects or people, whereas images from the second class tended to contain larger areas of visual texture such as grass, leaves, gravel, or fur. A similar distinction could also be made along the lines of ‘human-made’ versus ‘natural’ image structure, but we suspect the visual structure itself rather than its origin is of causal importance and so used that level of description.</p><p>While our labelling of images in this way is debatable (for example, ‘texture-like’ regions contain some ‘scene-like’ content and vice versa) and to some degree based on subjective judgment, we hypothesised that this classification distinguishes the types of image content that are critical. If the visual system indeed created a texture-like summary in the periphery and the FS-model was a sufficient approximation of that process, then we should observe no difference in the average critical scale factor of images in each group (because image content would be irrelevant to the distribution of <inline-formula><mml:math id="inf10"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>crit</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>We start by considering the condition where participants compared synthesised images to each other—as in <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref>. Under this condition, there was little evidence that the critical scale depended on the image content (see curves in <xref ref-type="fig" rid="fig1">Figure 1F</xref>, synth vs synth). The critical scale (posterior mean with 95% credible interval quantiles) for scene-like images was 0.28, 95% CI [0.21, 0.36] and the critical scale for texture-like images was 0.37, 95% CI [0.27, 0.5] (<xref ref-type="fig" rid="fig1">Figure 1G</xref>). Though these critical scales are lower than those reported by <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref>, they are within the range of other reported critical scale factors (<xref ref-type="bibr" rid="bib33">Freeman and Simoncelli, 2013</xref>). There was weak evidence for a difference in critical scale between texture-like and scene-like images, with the posterior distribution of scale differences being 0.09, 95% CI [−0.03, 0.24], <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.078</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (where <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the posterior probability of the difference being negative; symmetrical posterior distributions centered on zero would have <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). However, this evidence should be interpreted cautiously: because asymptotic performance never reaches high values, critical scale estimates are more uncertain than in the orig vs synth condition below (<xref ref-type="fig" rid="fig1">Figure 1G</xref>). This poor asymptotic performance may be because we used more images in our experiment than Freeman and Simoncelli, so participants were less familiar with the distortions that could appear. To make sure this difference did not arise due to different experimental paradigms (oddity vs. ABX), we repeated the experiment using the same ABX task as in Freeman and Simoncelli (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). This experiment again showed poor asymptotic performance, and furthermore demonstrated no evidence for a critical scale difference between the scene- and texture-like images. Taken together, our synth vs synth results are somewhat consistent with Freeman and Simoncelli, who reported no dependency of <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>crit</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> on image. It seems likely that this is because comparing synthesised images to each other means that the model has removed higher-order structure that might allow discrimination. All images appear distorted, and the task becomes one of identifying a specific distortion pattern.</p><p>Comparing the original image to model syntheses yielded a different pattern of results. First, participants were able to discriminate the original images from their FS-model syntheses at scale factors of 0.5 (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). Performance lay well above chance for all participants. This result held for both scene-like and texture-like images. Furthermore, there was evidence that critical scale depended on the image type. Model syntheses matched the texture-like images on average with scale factors of 0.36, 95% CI [0.29, 0.43]. In contrast, the scene-like images were quite discriminable from their model syntheses even at the smallest scale we could generate (0.25). The critical scale estimated for scene-like images was 0.22, 95% CI [0.18, 0.27]. Texture-like images had higher critical scales than scene-like images on average (scale difference = 0.13, 95% CI [0.06, 0.22], <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>This difference in critical scale was not attributable to differences in the success of the synthesis procedure between scene-like and texture-like images. Scene-like images had higher final loss (distance between the original and synthesised images in model space) than texture-like images on average (see Materials and methods). This is a corollary of the importance of image content: since a texture summary model is a poor description of scene-like content, the model’s optimisation procedure is also more likely to find local minima with relatively high loss. We checked that our main result was not explained by this difference by performing a control analysis in which we refit the model after equating the average loss in the two groups by excluding images with highest final loss until the groups were matched (resulting in four scene-like images being excluded; see Materials and methods). The remaining scene-like images had a critical scale of 0.24, 95% CI [0.2, 0.28] in the orig vs synth condition, texture-like images again showed a critical scale of 0.36, 95% CI [0.3, 0.42] and the difference distribution had a mean of 0.12, 95% CI [0.06, 0.19], <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, differences in synthesis loss do not explain our findings.</p><p>As noted above, the image with the minimum critical scale determines the largest compression that can be applied for the scaling model to hold (<inline-formula><mml:math id="inf17"><mml:msub><mml:mi>s</mml:mi><mml:mi>system</mml:mi></mml:msub></mml:math></inline-formula>). For two images (<xref ref-type="fig" rid="fig2">Figure 2A and E</xref>) the nonlinear mixed-effects model estimated critical scales of approximately 0.14 (see <xref ref-type="fig" rid="fig1">Figure 1G</xref>, diamonds; the minimum critical scale after excluding high-loss images in the control analysis reported above was 0.19). However, examining the individual data for these images (<xref ref-type="fig" rid="fig2">Figure 2D and H</xref>) reveals that these critical scale estimates are largely determined by the hierarchical nature of the mixed-effects model, not the data itself. Both images were easy to discriminate from the original even for the lowest scale factor we could generate. This suggests that the true scale factor required to generate metamers may be even lower than estimated by the mixed-effects model.</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.005</object-id><label>Figure 2.</label><caption><title>The two images with smallest critical scale estimates are highly discriminable even for the lowest scale factor we could generate.</title><p>(<bold>A</bold>) The original image. (<bold>B</bold>) An example FS synthesis at scale factor 0.25. (<bold>C</bold>) An example FS synthesis at scale factor 0.46. Images in B and C reproduced from the MIT 1003 Database (<xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>), <ext-link ext-link-type="uri" xlink:href="https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html</ext-link>) under a CC-BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">https://creativecommons.org/licenses/by/3.0/</ext-link>) with changes as described in the Methods. (<bold>D</bold>) The average data for this image. Points and error bars show grand mean and <inline-formula><mml:math id="inf18"><mml:mrow><mml:mo>±</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> SE over participants, solid curve and shaded area show posterior mean and 95% credible intervals from the mixed-effects model. Embedded text shows posterior mean and 95% credible interval on the critical scale estimate for this image. (<bold>E–H</bold>) Same as A–D for the image with the second-lowest critical scale. Note that in both cases the model is likely to overestimate critical scale.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.42512.006</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Images with the highest and lowest critical scale estimates within the scene-like and texture-like categories for the orig vs synth comparison.</title><p>(<bold>A</bold>) Scene-like image with the lowest critical scale estimate. Points and error bars show grand mean and ±2 SE over participants, solid curve and shaded area show posterior mean and 95Inline text shows posterior mean and 95B: Scene-like image with the highest critical scale estimate. (<bold>C</bold>) Texture-like image with the lowest critical scale estimate. (<bold>D</bold>) Texture-like image with the highest critical scale estimate. Images reproduced from the MIT 1003 Database (<xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>, <ext-link ext-link-type="uri" xlink:href="https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html</ext-link>) under a CC-BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">https://creativecommons.org/licenses/by/3.0/</ext-link>) with changes as described in the Methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-fig2-figsupp1-v1.tif"/></fig></fig-group><p>Our results show that smaller pooling regions are required to make metamers for scene-like images than for texture-like images. Human observers can reliably detect relatively small distortions produced by the FS-model at scale factors of 0.25 in scene-like image content (compare <xref ref-type="fig" rid="fig2">Figure 2B and F</xref> at scale 0.25 and <xref ref-type="fig" rid="fig2">C</xref> and <xref ref-type="fig" rid="fig2">G</xref> at scale 0.46 to images <xref ref-type="fig" rid="fig2">A</xref> and <xref ref-type="fig" rid="fig2">B</xref>). Thus, syntheses at these scales are not metamers for natural scenes.</p></sec><sec id="s2-2"><title>Local image structure determines the visibility of texture-like distortions</title><p>In our first experiment we found that scene-like images yielded lower critical scales than texture-like images. However, this categorisation is crude: ‘texture-ness’ in photographs of natural scenes is a property of local regions of the image rather than the image as a whole. In addition, the classification of images above was based in part on the difficulty of these images in a pilot experiment.</p><p>We therefore ran a second experiment to test the importance of local image structure more directly (<xref ref-type="bibr" rid="bib8">Bex, 2010</xref>; <xref ref-type="bibr" rid="bib46">Koenderink et al., 2017</xref>; <xref ref-type="bibr" rid="bib83">Valsecchi et al., 2018</xref>; <xref ref-type="bibr" rid="bib92">Wallis and Bex, 2012</xref>), using a set of images whose selection was not based on pilot discrimination results. Participants detected a localised texture-like distortion (generated by the texture model of <xref ref-type="bibr" rid="bib34">Gatys et al., 2015</xref>) blended into either a scene-like or texture-like region (<xref ref-type="fig" rid="fig3">Figure 3A–C</xref>). These image regions were classified by author CF (non-authors showed high agreement with this classification—see Materials and methods). The patches were always centered at an eccentricity of six degrees, and we varied the radius of the circular patch (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). This is loosely analogous to creating summary statistics in a single pooling region (<xref ref-type="bibr" rid="bib90">Wallis et al., 2016</xref>). Participants discriminated between the original image and an image containing a local distortion in a 2IFC paradigm (<xref ref-type="fig" rid="fig3">Figure 3E</xref>).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.007</object-id><label>Figure 3.</label><caption><title>Sensitivity to local texture distortions depends on image content.</title><p>(<bold>A</bold>) A circular patch of an image was replaced with a texture-like distortion. In different experimental conditions the radius of the patch was varied. (<bold>B</bold>) Two example images in which a ’scene-like’ or inhomogenous region is distorted (red cross). (<bold>C</bold>) Two example images in which a ’texture-like’ or homogenous region is distorted (red cross). (<bold>D</bold>) Examples of an original image and the four distortion sizes used in the experiment. Images in B–D reproduced from the MIT 1003 Database (<xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>), <ext-link ext-link-type="uri" xlink:href="https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html</ext-link>) under a CC-BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">https://creativecommons.org/licenses/by/3.0/</ext-link>) with changes as described in the Methods. (<bold>E</bold>) Depiction of the 2IFC task, in which the observer reported whether the first or second image contained the distortion. (<bold>F</bold>) Proportion correct as a function of distortion radius in scene-like (blue) and texture-like (red) image regions. Lines link the performance of each observer (each point based on a median of 51.5 trials; min 31, max 62). Points show mean of observer means, error bars show <inline-formula><mml:math id="inf19"><mml:mrow><mml:mo>±</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-fig3-v1.tif"/></fig><p>The results showed that the visibility of texture-like distortions depended strongly on the underlying image content. Participants were quite insensitive to even large texture-like distortions occurring in texture-like image regions (<xref ref-type="fig" rid="fig3">Figure 3F</xref>). Performance for distortions of nearly five degrees radius (i.e. nearly entering the foveal fixation point) was still close to chance. Conversely, distorting scene-like regions is readily detectable for the three largest distortion patch sizes.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>It is a popular idea that the appearance of scenes in the periphery is described by summary statistic textures captured at the scaling of V2 neural populations. In contrast, here we show that humans are very sensitive to the difference between original and model-matched images at this scale (<xref ref-type="fig" rid="fig1">Figure 1</xref>). A recent preprint (<xref ref-type="bibr" rid="bib26">Deza et al., 2017</xref>) finds a similar result in a set of 50 images, and our results are also consistent with the speculations made by Wallis et al. based on their experiments with Portilla and Simoncelli textures (<xref ref-type="bibr" rid="bib90">Wallis et al., 2016</xref>). Together, these results show that the pooling of texture-like features in the FS-model at the scaling of V2 receptive fields does not explain the appearance of natural images.</p><p>One exciting aspect of <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref> was the promise of inferring a critical brain region via a receptive field size prediction derived from psychophysics. Indeed, aspects of this promise have since received empirical support: the presence of texture-like features can discriminate V2 neurons from V1 neurons (<xref ref-type="bibr" rid="bib31">Freeman et al., 2013</xref>; <xref ref-type="bibr" rid="bib105">Ziemba et al., 2016</xref>; see also <xref ref-type="bibr" rid="bib63">Okazawa et al., 2015</xref>). Discarding all higher-order structure not captured by the candidate model by comparing syntheses to each other, thereby isolating only features that change, may be a useful way to distinguish the feedforward component of sequential processing stages in neurons.</p><p>While texture-like representations may therefore be important for understanding neural encoding (<xref ref-type="bibr" rid="bib60">Movshon and Simoncelli, 2014</xref>), our results call into question the link between receptive field scaling and scene appearance. If the peripheral appearance of visual scenes is explained by image-independent pooling of texture-like features, then the pooling regions must be small. Consider that participants in our experiment could easily discriminate the images in <xref ref-type="fig" rid="fig2">Figure 2B and F</xref> from those in <xref ref-type="fig" rid="fig2">Figure 2A and E</xref> respectively. Therefore, images synthesised at a truly metameric scaling must remain extremely close to the original: <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>s</mml:mi><mml:mi>system</mml:mi></mml:msub></mml:math></inline-formula> must be at least as small as V1 neurons, and perhaps even lower (<xref ref-type="fig" rid="fig2">Figure 2</xref>). This may even be consistent with scaling in precortical visual areas. For example, the scaling of retinal ganglion cell receptive fields at the average eccentricity of our stimuli (six degrees) is approximately 0.08 for the surround (<xref ref-type="bibr" rid="bib21">Croner and Kaplan, 1995</xref>) and 0.009 for the centre (<xref ref-type="bibr" rid="bib22">Dacey and Petersen, 1992</xref>). It becomes questionable how much is learned about compression in the ventral pathway using such an approach, beyond the aforementioned, relatively well-studied limits of optics and retinal sampling (e.g. <xref ref-type="bibr" rid="bib94">Wandell, 1995</xref>; <xref ref-type="bibr" rid="bib95">Watson, 2014</xref>).</p><p>A second main finding from our paper is that the ability of the FS-model to synthesise visual metamers at a given scale factor depends on image content. Images containing predominantly ‘scene-like’ content tended to be more difficult to match (requiring lower scale factors in the case of the FS-model) than images containing ‘texture-like’ content (<xref ref-type="fig" rid="fig1">Figure 1F and G</xref>). In a second experiment measuring the visibility of local texture distortions, we found that people can be quite insensitive to even large texture-like distortions so long as these fall on texture-like regions of the input image (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This confirms the importance of the distinction between ‘things’ (scene-like content) and ‘stuff’ (texture-like content; <xref ref-type="bibr" rid="bib1">Adelson, 2001</xref>) for peripheral scene appearance.</p><p>This result can be experienced via simple demonstration. The ‘China Lane’ sign in <xref ref-type="fig" rid="fig4">Figure 4A</xref> has been distorted in <xref ref-type="fig" rid="fig4">Figure 4B</xref> (using local texture distortions as in <xref ref-type="fig" rid="fig3">Figure 3</xref>), and is readily visible in the periphery (with central fixation on the circular bullseye). The same type of distortion in a texture-like region of the image is far less visible (the brickwork in the image centre; FS-model result <xref ref-type="fig" rid="fig4">Figure 4C</xref>), despite appearing in the parafovea. It is the image content, not retinal eccentricity, that is the primary determinant of the visibility of at least some summary statistic distortions. Requiring information to be preserved at V1 or smaller scaling would therefore be inefficient from the standpoint of compression: small scale factors will preserve texture-like structure that could be compressed without affecting appearance.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.008</object-id><label>Figure 4.</label><caption><title>The visibility of texture-like distortions depends on image content.</title><p>(<bold>A</bold>) 'Geotemporal Anomaly’ by Pete Birkinshaw (2010: <ext-link ext-link-type="uri" xlink:href="https://www.flickr.com/photos/binaryape/5203086981">https://www.flickr.com/photos/binaryape/5203086981</ext-link>, re-used under a CC-BY 2.0 license: <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/2.0/uk/">https://creativecommons.org/licenses/by/2.0/uk/</ext-link>). The image has been resized and a circular bullseye has been added to the centre. (<bold>B</bold>) Two texture-like distortions have been introduced into circular regions of the scene in A (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for higher resolution). The distortion in the upper-left is quite visible, even with central fixation on the bullseye, because it breaks up the high-contrast contours of the text. The second distortion occurs on the brickwork centered on the bullseye, and is more difficult to see (you may not have noticed it until reading this caption). The visibility of texture-like distortions can depend more on image content than on retinal eccentricity (see also <xref ref-type="fig" rid="fig3">Figure 3</xref>). (<bold>C</bold>) Results synthesised from the FS-model at scale 0.46 for comparison. Pooling regions depicted for one angular meridian as overlapping red circles; real pooling regions are smooth functions tiling the whole image. Pooling in this fashion reduces large distortions compared to B, but our results show that this is insufficient to match appearance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.42512.009</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Higher-resolution versions of the images from <xref ref-type="fig" rid="fig4">Figure 4</xref>.</title><p>(<bold>A</bold>) 'Geotemporal Anomaly' by Pete Birkinshaw (2010: <ext-link ext-link-type="uri" xlink:href="https://www.flickr.com/photos/binaryape/5203086981">https://www.flickr.com/photos/binaryape/5203086981</ext-link>, re-used under a CC-BY 2.0 license: <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/2.0/uk/">https: //creativecommons.org/licenses/by/2.0/uk/</ext-link>). The image has been resized and a circu- lar bullseye has been added to the centre. (<bold>B</bold>) The image from A with two circular texture-like distortions (circles enclose distorted area) created using the <xref ref-type="bibr" rid="bib34">Gatys et al. (2015)</xref> texture synthesis algorithm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-fig4-figsupp1-v1.tif"/></fig></fig-group><p>It may seem trivial that a texture statistic model better captures the appearance of textures than non-textures. However, if the human visual system represents the periphery as a texture-like summary, and these models are sufficient approximations of this representation, then image content should not matter—because scene-like retinal inputs in the periphery are transformed into textures by the visual system.</p><p>Perhaps the V2 scaling theory holds but the FS-model texture features are insufficient to capture natural scene appearance. To test whether improved texture features (<xref ref-type="bibr" rid="bib34">Gatys et al., 2015</xref>) could help in matching appearance for scenes, we developed a new model (CNN-model; see Materials and methods and <xref ref-type="fig" rid="app2fig1">Appendix 2—figures 1</xref>–<xref ref-type="fig" rid="app2fig4">4</xref>) that was inspired by the FS-model but uses the texture features of a convolutional neural network (VGG-19, <xref ref-type="bibr" rid="bib78">Simonyan and Zisserman, 2015</xref>) that have previously been shown to better capture the appearance of some textures than the Portilla and Simoncelli texture features (<xref ref-type="bibr" rid="bib91">Wallis et al., 2017</xref>). As for the FS-model, discrimination performance becomes poorer as pooling region sizes become smaller (<xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3</xref>). The CNN 32 model shows very similar behaviour to the FS-model such that human performance for scene-like images is higher than for texture-like images (triangles in <xref ref-type="fig" rid="fig1">Figure 1D</xref> and <xref ref-type="fig" rid="fig2">Figure 2</xref>). Thus, the syntheses from both models are not metamers for natural scenes. Nevertheless, our results cannot rule out that a hereto unknown summary statistic model exists that will create metamers for all images at V2 scales or higher. However, that two additional summary statistic models (the CNN-model and the NeuroFovea model of <xref ref-type="bibr" rid="bib26">Deza et al., 2017</xref>) also fail to capture scene appearance and show dependence on image content adds some generality to our claim that these models are insufficient descriptions of peripheral visual scene appearance.</p><p>If this claim was correct, this begs the question: what is the missing ingredient that could capture appearance while compressing as much information as possible? Through the Gestalt tradition, it has long been known that the appearance of local image elements can crucially depend on the context in which they are placed and their interpretation in the scene (for overviews of recent work, see <xref ref-type="bibr" rid="bib40">Jäkel et al., 2016</xref>; <xref ref-type="bibr" rid="bib88">Wagemans et al., 2012a</xref>; <xref ref-type="bibr" rid="bib89">Wagemans et al., 2012b</xref>). We speculate that mechanisms of perceptual organisation (such as segmentation and grouping) need to be considered if one wants to capture appearance in general—yet current models that texturise local regions do not explicitly include these mechanisms (<xref ref-type="bibr" rid="bib38">Herzog et al., 2015</xref>; <xref ref-type="bibr" rid="bib17">Clarke et al., 2014</xref>). If segmentation and grouping processes are critical for efficiently matching scene appearance, then uniformly computing summary statistics without including these processes will require preserving much of the original image structure by making pooling regions very small. A parsimonious model capable of compressing as much information as possible might need to adapt either the size and arrangement of pooling regions or the feature representations to the image content.</p><sec id="s3-1"><title>Local vs global mechanisms</title><p>These segmentation and grouping mechanisms could be mediated by local interactions between nearby image features, global properties of the scene, or both. The present results do not allow us to distinguish these alternatives.</p><p>In favour of the importance of local interactions, studies of contour integration in Gabor fields show that the arrangement of local orientation structure can influence the discrimination of contour shape (<xref ref-type="bibr" rid="bib24">Dakin and Baruch, 2009</xref>) and contour localisation (<xref ref-type="bibr" rid="bib71">Robol et al., 2012</xref>), and that these effects are consistent with crowding (<xref ref-type="bibr" rid="bib71">Robol et al., 2012</xref>). In these stimuli, crowding between nearby contour elements is the primary determinant of global contour judgments (see also <xref ref-type="bibr" rid="bib23">Dakin et al., 2009</xref>). Specifically, contours consisting of parallel Gabor elements (‘snakes’) were more easily perceived when adjacent Gabor elements were oriented perpendicularly to the main contour. A related study (<xref ref-type="bibr" rid="bib84">Van der Burg et al., 2017</xref>) used an evolutionary algorithm to select dense line element displays that maximally alleviated crowding in an orientation discrimination task. Displays evolved using human responses showed that a substantial reduction of crowding was obtained by orienting the two line segments nearest the target (separated by only <inline-formula><mml:math id="inf21"><mml:msup><mml:mn>0.75</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> at <inline-formula><mml:math id="inf22"><mml:msup><mml:mn>6</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> eccentricity) to be perpendicular to the target’s mean orientation (forming ‘T’ and/or ‘I’ junctions). In contrast, simulations based on Bouma’s Law predicted that much larger areas of the display (relative to the human data) would need to be adjusted. These results are consistent with our finding that humans can be far more sensitive to image structure in the periphery than predicted by Bouma-like scaling.</p><p>The studies above suggest the possibility that T-junctions may be critical local cues to segmentation in the periphery. The potential importance of different junction types in segmentation and grouping has long been noted (<xref ref-type="bibr" rid="bib9">Biederman, 1987</xref>). In real scenes, T-junctions usually signal occlusion edges between rigid surfaces, whereas Y-, L- and arrow-junctions are created by projecting the corners of 3D objects into 2D. Histograms of junction distributions are diagnostic of scene category (<xref ref-type="bibr" rid="bib93">Walther and Shen, 2014</xref>), with human-made scenes such as city streets and offices tending to contain more T-junctions than more natural environments like beaches and mountains. A recent study also highlights the importance of local contour symmetry for scene categorisation (<xref ref-type="bibr" rid="bib100">Wilder et al., 2019</xref>). Finally, <xref ref-type="bibr" rid="bib54">Loschky et al. (2010)</xref> found that participants were extremely poor at classifying scene category from <xref ref-type="bibr" rid="bib68">Portilla and Simoncelli (2000)</xref> global textures of scene images. These results suggest that the Portilla and Simoncelli texture statistics (used in the FS-model) do not adequately preserve junction information.</p><p>Taken together, these studies give rise to the following hypothesis: images with more junctions (particularly T-junctions; <xref ref-type="bibr" rid="bib84">Van der Burg et al., 2017</xref>) will require smaller pooling regions to match and thus will show lower critical scale estimates in the FS-model. We applied the junction detection algorithm of <xref ref-type="bibr" rid="bib101">Xia et al. (2014)</xref> to each of the 20 original images used in our first experiment. Consistent with the (post-hoc) hypothesis above, lower critical scales were associated with more frequent junctions, particularly if ‘less meaningful’ junctions (defined by the algorithm) were excluded (T-junction correlation <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.54</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; L-junctions <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.63</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>). If confirmed by a targeted experiment (and dissociated from general edge density), this relationship would suggest a clear avenue for future improvement of scene appearance models: they must successfully capture junction information in images.</p><p>Other evidence supports the role of global information (the arrangement and organisation of objects over large retinal areas) in segmentation and grouping. In crowding, <xref ref-type="bibr" rid="bib58">Manassi et al. (2013)</xref> found that configurations of stimuli well outside the region of Bouma’s law could modulate the crowding effectiveness of the same flankers (see also <xref ref-type="bibr" rid="bib57">Manassi et al., 2012</xref>; <xref ref-type="bibr" rid="bib76">Saarela et al., 2009</xref>; <xref ref-type="bibr" rid="bib87">Vickery et al., 2009</xref>; <xref ref-type="bibr" rid="bib52">Levi and Carney, 2009</xref>). <xref ref-type="bibr" rid="bib61">Neri (2017)</xref> reported evidence from a variety of experiments in support of a fast segmentation process, operating over large regions of space, that can strongly modulate the perceptual interpretation of—and sensitivity to—local edge elements in a scene according to the figure-ground organisation of the scene (see also <xref ref-type="bibr" rid="bib81">Teufel et al., 2018</xref>). Our findings could be explained by the fact that the texture summary statistic models we examine here do not include any such global segmentation processes. The importance of these mechanisms could be examined in future studies, and potentially dissociated from the local information discussed above, by using image manipulations thought to disrupt the activity of global grouping mechanisms such as polarity inversion or image two-toning (<xref ref-type="bibr" rid="bib61">Neri, 2017</xref>; <xref ref-type="bibr" rid="bib7">Balas, 2012</xref>; <xref ref-type="bibr" rid="bib81">Teufel et al., 2018</xref>).</p></sec><sec id="s3-2"><title>Summary statistics, performance and phenomenology</title><p>Our results do not undermine the considerable empirical support for the periphery-as-summary-statistic theory as a description of visual performance. Humans can judge summary statistics of visual displays (<xref ref-type="bibr" rid="bib3">Ariely, 2001</xref>; <xref ref-type="bibr" rid="bib25">Dakin and Watt, 1997</xref>), summary statistics can influence judgments where other information is lost (<xref ref-type="bibr" rid="bib29">Fischer and Whitney, 2011</xref>; <xref ref-type="bibr" rid="bib28">Faivre et al., 2012</xref>), and the information preserved by summary statistic stimuli may offer an explanation for performance in various visual tasks (<xref ref-type="bibr" rid="bib74">Rosenholtz et al., 2012b</xref>; <xref ref-type="bibr" rid="bib6">Balas et al., 2009</xref>; <xref ref-type="bibr" rid="bib73">Rosenholtz et al., 2012a</xref>; <xref ref-type="bibr" rid="bib44">Keshvari and Rosenholtz, 2016</xref>; <xref ref-type="bibr" rid="bib16">Chang and Rosenholtz, 2016</xref>; <xref ref-type="bibr" rid="bib104">Zhang et al., 2015</xref>; <xref ref-type="bibr" rid="bib96">Whitney et al., 2014</xref>; <xref ref-type="bibr" rid="bib53">Long et al., 2016</xref>; though see <xref ref-type="bibr" rid="bib2">Agaoglu and Chung, 2016</xref>; <xref ref-type="bibr" rid="bib38">Herzog et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Francis et al., 2017</xref>). Texture-like statistics may even provide the primitives from which form is constructed (<xref ref-type="bibr" rid="bib51">Lettvin, 1976</xref>)—after appropriate segmentation, grouping and organisation. However, one additional point merits further discussion. The studies by Rosenholtz and colleagues primarily test summary statistic representations by showing that performance with summary statistic stimuli viewed foveally is correlated with peripheral performance with real stimuli. This means that the summary statistics preserve sufficient information to explain the performance of tasks in the periphery. Our results show that these summary statistics are insufficient to match scene appearance, at least under the pooling scheme used in the Freeman and Simoncelli model at computationally feasible scales. This shows the usefulness of scene appearance matching as a test: a parsimonious model that matches scene appearance would be expected to also preserve enough information to show correlations with peripheral task performance; the converse does not hold.</p><p>While it may be useful to consider summary statistic pooling in accounts of visual performance, to say that summary statistics can account for phenomenological experience of the visual periphery (<xref ref-type="bibr" rid="bib18">Cohen et al., 2016</xref>; see also <xref ref-type="bibr" rid="bib10">Block, 2013</xref>; <xref ref-type="bibr" rid="bib77">Seth, 2014</xref>) seems premature in light of our results (see also <xref ref-type="bibr" rid="bib37">Haun et al., 2017</xref>). <xref ref-type="bibr" rid="bib18">Cohen et al. (2016)</xref> additionally posit that focussed spatial attention can in some cases overcome the limitations imposed by a summary statistic representation. We instead find little evidence that participants’ ability to discriminate real from synthesised images is improved by cueing spatial attention, at least in our experimental paradigm and for our CNN-model (<xref ref-type="fig" rid="app2fig6">Appendix 2—figure 6</xref>).</p></sec><sec id="s3-3"><title>Conclusion</title><p>Our results show that the appearance of scenes in the periphery cannot be captured by the <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref> summary statistic model at receptive field scalings similar to V2. We suggest that peripheral appearance models emphasising pooling processes that depend on retinal eccentricity will instead need to explore input-dependent grouping and segmentation. We speculate that mechanisms of perceptual organisation (either local or global) are critical to explaining visual appearance and efficient peripheral encoding. Models of the visual system that assume image content is processed in feedforward, fixed pooling regions—including current convolutional neural networks—lack these mechanisms.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All stimuli, data and code to reproduce the figures and statistics reported in this paper are available at <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.1475111">http://dx.doi.org/10.5281/zenodo.1475111</ext-link>. This document was prepared using the knitr package (<xref ref-type="bibr" rid="bib102">Xie, 2013</xref>; <xref ref-type="bibr" rid="bib103">Xie, 2016</xref>) in the R statistical environment (<xref ref-type="bibr" rid="bib69">R Core Team, 2017</xref>; <xref ref-type="bibr" rid="bib99">Wickham and Francois, 2016</xref>; <xref ref-type="bibr" rid="bib97">Wickham, 2009</xref>, <xref ref-type="bibr" rid="bib98">Wickham, 2011</xref>; <xref ref-type="bibr" rid="bib5">Auguie, 2016</xref>; <xref ref-type="bibr" rid="bib4">Arnold, 2016</xref>) to improve its reproducibility.</p><sec id="s4-1"><title>Participants</title><p>Eight observers participated in the first experiment (<xref ref-type="fig" rid="fig1">Figure 1</xref>): authors CF and TW, a research assistant unfamiliar with the experimental hypotheses, and five naïve participants recruited from an online advertisement pool who were paid 10 Euro per hr for two one-hour sessions. An additional naïve participant was recruited but showed insufficient eyetracking accuracy (see below). Four observers participated in the second experiment (<xref ref-type="fig" rid="fig3">Figure 3</xref>); authors CF and TW plus two naïve observers paid 10 Euro per hour. All participants signed a consent form prior to participating. Participants reported normal or corrected-to-normal visual acuity. All procedures conformed to Standard 8 of the American Psychological Association’s ‘Ethical Principles of Psychologists and Code of Conduct’ (2010).</p></sec><sec id="s4-2"><title>Stimuli</title><p>Images were taken from the MIT 1003 scene dataset (<xref ref-type="bibr" rid="bib43">Judd et al., 2012</xref>; <xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>). A square was cropped from the center of the original image and downsampled to 512 × 512 px. The images were converted to grayscale and standardized to have a mean gray value of 0.5 (scaled [0,1]) and an RMS contrast (<inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>/</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:math></inline-formula>) of 0.3. For the first experiment, images were selected as described in the Results and <xref ref-type="fig" rid="app2fig7">Appendix 2—figure 7—figure 9</xref>.</p><sec id="s4-2-1"><title>Freeman and Simoncelli syntheses</title><p>We synthesised images using the FS-model (<xref ref-type="bibr" rid="bib32">Freeman and Simoncelli, 2011</xref>, code available from <ext-link ext-link-type="uri" xlink:href="https://github.com/freeman-lab/metamers">https://github.com/freeman-lab/metamers</ext-link>). Four unique syntheses were created for each source image at each of eight scale factors (0.25, 0.36, 0.46, 0.59, 0.7, 0.86, 1.09, 1.45), using 50 gradient steps as in Freeman and Simoncelli’s main experiment. Pilot experiments with stimuli generated with 100 gradient steps produced similar results. <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref> computed the final loss between original and synthesised images as ‘mean squared error, normalized by the parameter variance’. We take this to mean the following: for a matrix of model parameters from an original image <inline-formula><mml:math id="inf26"><mml:msub><mml:mi mathvariant="normal">X</mml:mi><mml:mi>orig</mml:mi></mml:msub></mml:math></inline-formula> (rows are parameters and columns are pooling regions) and the corresponding parameters for the synthesised image <inline-formula><mml:math id="inf27"><mml:msub><mml:mi mathvariant="normal">X</mml:mi><mml:mi>synth</mml:mi></mml:msub></mml:math></inline-formula>, we compute the normalised MSE as <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>MSE</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>mean</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">X</mml:mi><mml:mi>orig</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="normal">X</mml:mi><mml:mi>synth</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mi>Var</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">X</mml:mi><mml:mi>orig</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Freeman and Simoncelli report that this metric was 0.01 ± 0.015 (mean ± s.d.) across all images and scales in their experiment. For our experiment, the same metric across all images and scales was 0.06 ± 0.2. These higher final loss values were driven by the scene-like images, which had a mean loss of 0.11 ± 0.27 compared to the texture-like images (0.01 ± 0.05). Excluding the four highest-loss images (all scene-like) reduced the average loss of the scene-like category to 0.01 ± 0.02, which is similar to the range of the syntheses used by <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref> and to the texture-like images. A control analysis showed the difference in critical scale between the image categories remained after matching the average loss (Results).</p><p>To successfully synthesise images at scale factors of 0.25 and 0.36 it was necessary to increase the central region of the image in which the original pixels were perfectly preserved (pooling regions near the fovea become too small to compute correlation matrices). Scales of 0.25 used a central radius of 32 px (0.8 dva in our viewing conditions) and scales 0.36 used 16 px (0.4 dva). This change should, if anything, make syntheses even harder to discriminate from the original image. All other parameters of the model were as in Freeman and Simoncelli. Synthesising an image with scale factor 0.25 took approximately 35 hr, making a larger set of syntheses or source images infeasible. It was not possible to reliably generate images with scale factors lower than 0.25 using the code above.</p></sec><sec id="s4-2-2"><title>CNN model syntheses</title><p>The CNN pooling model (triangles in <xref ref-type="fig" rid="fig1">Figure 1</xref>) was inspired by the model of Freeman and Simoncelli, with two primary differences: first, we replaced the <xref ref-type="bibr" rid="bib68">Portilla and Simoncelli (2000)</xref> texture features with the texture features derived from a convolutional neural network (<xref ref-type="bibr" rid="bib34">Gatys et al., 2015</xref>), and second, we simplified the ‘foveated’ pooling scheme for computational reasons. Specifically, for the CNN 32 model presented above, the image was divided up into 32 angular regions and 28 radial regions, spanning the outer border of the image and an inner radius of 64 px. Within each of these regions we computed the mean activation of the feature maps from a subset of the VGG-19 network layers (conv1_1, conv2_1, conv3_1, conv4_1, conv5_1). To better capture long-range correlations in image structure, we computed these radial and angular regions over three spatial scales, by computing three networks over input sizes 128, 256 and 512 px. Using this multiscale radial and angular pooling representation of an image, we synthesised new images to match the representation of the original image via iterative gradient descent (<xref ref-type="bibr" rid="bib34">Gatys et al., 2015</xref>). Specifically, we minimised the mean-squared distance between the original and a target image, starting from Gaussian noise outside the central 64 px region, using the L-BFGS optimiser as implemented in scipy (<xref ref-type="bibr" rid="bib41">Jones et al., 2001</xref>) for 1000 gradient steps, which we found in pilot experiments was sufficient to produce small (but not zero) loss. Further details, including tests of other variants of this model, are provided in Appendix 2.</p></sec><sec id="s4-2-3"><title>Local distortion experiment</title><p>We identified local regions that were scene-like or texture-like, whose centre-of-mass was approximately 128 px (±5 px; approximately 6 degrees) from the centre of the image. Because we are not aware of any algorithmic method to distinguish these types of image structure, these were chosen based on our definition of scene-like and texture-like image content (see Results) by author CF. Specifically, a Python script was used to display the 1003 images of the MIT database with a circle of radius 128 px superimposed. CF clicked on a point on the circle that lay in a texture- or scene-like region; if no such region was identified this image was discarded. The coordinates of this point as well as its classification were stored. This procedure resulted in 389 unique images, of which 229 contained a ‘scene-like’ region and 160 contained a ‘texture-like’ region.</p><p>Non-authors generally agreed with this classification. We conducted a pilot experiment to measure agreement in five participants. Participants were shown each of the 389 images above with a circle (of radius 100 px) superimposed over the region defined by CF. They were instructed to classify the circled region as ‘scene-like’ (defined as ‘tend to contain larger areas of inhomogenous structure, long edges, borders between different surfaces or objects, and angled edges providing perspective cues’) or ‘texture-like’ (defined as ‘homogenous structure, patterned content, or materials’) in a single-interval binary response task. We found a mean agreement of 88.6% with CF’s classification (individual accuracies of 74.8, 90.2, 92.5, 92.8, 92.8%, mean <inline-formula><mml:math id="inf29"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> = 2.81, with a mean bias to respond ‘scene-like’, <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:math></inline-formula> = −1.39). In this experiment (conducted approximately two years after the initial classification), CF showed a retest agreement of 97.4%.</p><p>For each image we perturbed a circular patch in the center of the texture/object region using the texture model of <xref ref-type="bibr" rid="bib34">Gatys et al. (2015)</xref>. Note that this is the texture model not the CNN-model using radial and angular pooling regions. For each original image, we generated new images containing distortions of different sizes (radii of 40, 70, 85 and 100 px, corresponding to approximately 2, 3.4, 4.1 and 4.9 dva). The local texture features were computed as the (square) Gram matrices in the same VGG-19 layers as used in the CNN-model over an area equal to the radius plus 24 px (square side length <inline-formula><mml:math id="inf31"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>24</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). Texture synthesis was then performed via gradient descent as in the CNN-model, with the exception that the loss function included a circular cosine spatial windowing function which ramped between the synthesised and original pixels over a region of 12 px, in order to smoothly blend the texture distortion with the surrounding image structure. Some example images are shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. In total we therefore used 389 unique images and 389*4 synthesised images as stimuli in this experiment.</p></sec></sec><sec id="s4-3"><title>Equipment</title><p>Stimuli were displayed on a VIEWPixx 3D LCD (VPIXX Technologies Inc, Saint-Bruno-de-Montarville, Canada; spatial resolution 1920 × 1080 pixels, temporal resolution 120 Hz, operating with the scanning backlight turned off in normal colour mode). Outside the stimulus image the monitor was set to mean grey. Participants viewed the display from 57 cm (maintained via a chinrest) in a darkened chamber. At this distance, pixels subtended approximately 0.025 degrees on average (approximately 40 pixels per degree of visual angle). The monitor was linearised (maximum luminance 260 <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>cd</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>) using a Konica-Minolta LS-100 (Konica-Minolta Inc, Tokyo, Japan). Stimulus presentation and data collection was controlled via a desktop computer (Intel Core i5-4460 CPU, AMD Radeon R9 380 GPU) running Ubuntu Linux (16.04 LTS), using the Psychtoolbox Library (version 3.0.12, <xref ref-type="bibr" rid="bib12">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib45">Kleiner et al., 2007</xref>; <xref ref-type="bibr" rid="bib65">Pelli, 1997</xref>), the Eyelink toolbox (<xref ref-type="bibr" rid="bib19">Cornelissen et al., 2002</xref>) and our internal iShow library (<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.34217">http://dx.doi.org/10.5281/zenodo.34217</ext-link>) under MATLAB (The Mathworks Inc, Natick MA, USA; R2015b). Participants’ gaze position was monitored by an Eyelink 1000 (SR Research) video-based eyetracker.</p></sec><sec id="s4-4"><title>Procedure</title><p>In the first experiment, participants were shown three images in succession on each trial. Two images were identical, one image was different (the ‘oddball’, which could occur first, second or third with equal probability). The oddball could be either a synthesised or a natural image (in the orig vs synth condition; counterbalanced), whereas the other two images were physically the same as each other and from the opposite class as the oddball. In the synth vs synth condition (as used in Freeman and Simoncelli), both oddball and foil images were (physically different) model synths. The participant identified the temporal position of the oddball image via button press. Participants were told to fixate on a central point (<xref ref-type="bibr" rid="bib82">Thaler et al., 2013</xref>) presented in the center of the screen. The images were centred around this spot and displayed with a radius of 512 pixels (i.e. images were upsampled by a factor of two for display), subtending ≈12.8° at the eye. Images were windowed by a circular cosine, ramping the contrast to zero in the space of 52 pixels. The stimuli were presented for 200 ms, with an inter-stimulus interval of 1000 ms (making it unlikely participants could use motion cues to detect changes), followed by a 1200 ms response window. Feedback was provided by a 100 ms change in fixation cross brightness. Gaze position was recorded during the trial. If the participant moved the eye more than 1.5 degrees away from the fixation spot, the trial immediately ended and no response was recorded; participants saw a feedback signal (sad face image) indicating a fixation break. Prior to the next trial, the state of the participant’s eye position was monitored for 50 ms; if the eye position was reported as more than 1.5 degrees away from the fixation spot a recalibration was triggered. The inter-trial interval was 400 ms.</p><p>Scene-like and texture-like images were compared under two comparison conditions (orig vs synth and synth vs synth; see main text). Image types and scale factors were randomly interleaved within a block of trials (with a minimum of one trial from another image in between) whereas comparison condition was blocked. Participants first practiced the task and fixation control in the orig vs synth comparison condition (scales 0.7, 0.86 and 1.45); the same images used in the experiment were also used in practice to familiarise participants with the images. Participants performed at least 60 practice trials, and were required to achieve at least 50% correct responses and fewer than 20% fixation breaks before proceeding (as noted above, one participant failed). Following successful practice, participants performed one block of orig vs synth trials, which consisted of five FS-model scale factors (0.25, 0.36, 0.46, 0.59, 0.86) plus the CNN 32 model, repeated once for each image to give a total of 120 trials. The participant then practiced the synth vs synth condition for at least one block (30 trials), before continuing to a normal synth vs synth block (120 trials; scale factors of 0.36, 0.46, 0.7, 0.86, 1.45). Over two one-hour sessions, naïve participants completed a total of four blocks of each comparison condition in alternating order (except for one participant who ran out of time to complete the final block). Authors performed more blocks (total 11).</p><p>In the second experiment, observers discriminated which image contained the distortion in a 2IFC paradigm. Each image was presented for 200 ms with a 1000 ms inter-stimulus interval, after which the observer had 1200 ms to respond. The original, unmodified image could appear either first or second; the other image was the same but contained the circular distortion. Observers fixated a spot (<xref ref-type="bibr" rid="bib82">Thaler et al., 2013</xref>) in the centre of the screen. Feedback was provided, and eyetracking was not used. All observers performed 389 trials. To avoid effects of familiarity with the distortion region, each observer saw each original image only once (that is, each original image was randomly assigned to one of the four distortion scales for each observer). While authors were familiar with the images, naïve observers were not. The consistency of effects between authors and naïves suggests that familiarity does not play a major role in this experiment.</p></sec><sec id="s4-5"><title>Data analysis</title><p>In the first experiment, we discarded trials for which participants made no response (N = 66) and broke fixation (N = 239), leaving a total of 7555 trials for further analysis. The median number of responses for each image at each scale for each subject in each condition was 4 trials (min 1, max 7). The individual observer data for the FS-model averaged over images (faint lines in <xref ref-type="fig" rid="fig1">Figure 1F</xref>) were based on a median of 39 trials (min 20, max 70) for each scale in each condition. The individual observer performance as a function of condition (each psychometric function of FS-scale) was based on a median of 192.5 responses (min 136, max 290).</p><p>In the second experiment we discarded trials with no response (N = 8), and did not record eye movements, leaving 1548 trials for further analysis.</p><p>To quantify the critical scale as a function of the scale factor <inline-formula><mml:math id="inf33"><mml:mi>s</mml:mi></mml:math></inline-formula>, we used the same 2-parameter function for discriminability <inline-formula><mml:math id="inf34"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> fitted by Freeman and Simoncelli:<disp-formula id="equ3"><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>s</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>consisting of the critical scale <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> (below which the participant cannot discriminate the stimuli) and a gain parameter <inline-formula><mml:math id="inf36"><mml:mi>α</mml:mi></mml:math></inline-formula> (asymptotic performance level in units of <inline-formula><mml:math id="inf37"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>). This <inline-formula><mml:math id="inf38"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> value was transformed to proportion correct using a Weibull function as in <xref ref-type="bibr" rid="bib90">Wallis et al., 2016</xref>:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>correct</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="inf39"><mml:mi>m</mml:mi></mml:math></inline-formula> set to three (the number of alternatives), and scale <inline-formula><mml:math id="inf40"><mml:mi>λ</mml:mi></mml:math></inline-formula> and shape <inline-formula><mml:math id="inf41"><mml:mi>k</mml:mi></mml:math></inline-formula> parameters chosen by minimising the squared difference between the Weibull and simulated results for oddity as in <xref ref-type="bibr" rid="bib20">Craven (1992)</xref>. The posterior distribution over model parameters (<inline-formula><mml:math id="inf42"><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf43"><mml:mi>α</mml:mi></mml:math></inline-formula>) was estimated in a nonlinear mixed-effects model with fixed effects for the experimental conditions (comparison and image type) and random effects for participant (crossed with comparison and image type) and image (crossed with comparison, nested within image type), assuming binomial variability. Note that <inline-formula><mml:math id="inf44"><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> here is shorthand for a population-level critical scale and group-level offsets estimated for each participant and image; <inline-formula><mml:math id="inf45"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>crit</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the image-specific <inline-formula><mml:math id="inf46"><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> estimate. Estimates were obtained by a Markov Chain Monte Carlo (MCMC) procedure implemented in the Stan language (version 2.16.2, <xref ref-type="bibr" rid="bib80">Stan Development Team, 2017</xref>; <xref ref-type="bibr" rid="bib39">Hoffman and Gelman, 2014</xref>), with the model wrapper package brms (version 1.10.2, <xref ref-type="bibr" rid="bib14">Bürkner, 2017</xref>; <xref ref-type="bibr" rid="bib15">Bürkner, 2018</xref>) in the R statistical environment. MCMC sampling was conducted with four chains, each with 20,000 iterations (10,000 warmup), resulting in 40,000 post-warmup samples in total. Convergence was assessed using the <inline-formula><mml:math id="inf47"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> statistic (<xref ref-type="bibr" rid="bib13">Brooks and Gelman, 1998</xref>) and by examining traceplots. The model parameters were given weakly-informative prior distributions, which provide information about the plausible scale of parameters but do not bias the direction of inference. Specifically, both critical scale and gain were estimated on the natural logarithmic scale; the mean log critical scale (intercept) was given a Gaussian distribution prior with mean −0.69 (corresponding to a critical scale of approximately 0.5—that is centred on the result from Freeman and Simoncelli) and sd 1, other fixed-effect coefficients were given Gaussian priors with mean 0 and sd 0.5, and the group-level standard deviation parameters were given positive-truncated Cauchy priors with mean 0 and sd 0.1. Priors for the log gain parameter were the same, except the intercept prior had mean 1 (linear gain estimate of 2.72 in <inline-formula><mml:math id="inf48"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> units) and sd 1. The posterior distribution represents the model’s beliefs about the parameters given the priors and data. This distribution is summarised above as posterior mean, 95% credible intervals and posterior probabilities for the fixed-effects parameters to be negative (the latter computed via the empirical cumulative distribution of the relevant MCMC samples).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgments</title><p>Funded by the German Federal Ministry of Education and Research (BMBF) through the Bernstein Computational Neuroscience Program Tübingen (FKZ: 01GQ1002), the German Excellency Initiative through the Centre for Integrative Neuroscience Tübingen (EXC307), and the Deutsche Forschungsgemeinschaft (DFG; priority program 1527, BE 3848/2–1 and Projektnummer 276693517 – SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, TP03). We acknowledge support by the Deutsche Forschungsgemeinschaft and Open Access Publishing Fund of University of Tübingen. We thank Wiebke Ringels for assistance with data collection, Heiko Schütt, Matthias Kümmerer and Corey Ziemba for helpful comments on an earlier draft, Andrew Haun and Ben Balas for helpful comments on Twitter, and reviewer John Cass for suggesting the importance of junction information in explaining our results. TSAW was supported in part by an Alexander von Humboldt Postdoctoral Fellowship. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Christina Funke.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>This author now works for Apple, Inc. The author's contributions to this article were prior to commencing employment at Apple.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Software, Formal analysis, Validation, Investigation, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Methodology, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Resources, Software, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants provided informed consent to participate in the study and for their anonymised data to be made publicly available. The study adhered to Standard 8 of the American Psychological Association's &quot;Ethical Principles of Psychologists and Code of Conduct&quot; (2010). The experiments were approved by the Ethics Commission of the University Clinics Tübingen (Nr. 222/2011B02).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.42512.010</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-42512-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All raw data, processed data, model files, stimulus materials, and analysis code are provided for download in a Zenodo database at <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.1475111">http://dx.doi.org/10.5281/zenodo.1475111</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Wallis</surname><given-names>TSA</given-names></name><name><surname>Funke</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Materials to reproduce Wallis, Funke et al. &quot;Image content is more important than Bouma's Law for scene metamers&quot;</data-title><source>Zenodo</source><pub-id assigning-authority="Zenodo" pub-id-type="doi">10.5281/zenodo.1475111</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname> <given-names>EH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>On seeing stuff: the perception of materials by humans and machines</article-title><source>Human Vision and Electronic Imaging</source><volume>4299</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1117/12.429489</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agaoglu</surname> <given-names>MN</given-names></name><name><surname>Chung</surname> <given-names>ST</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Can (should) theories of crowding be unified?</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/16.15.10</pub-id><pub-id pub-id-type="pmid">27936273</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ariely</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Seeing sets: representation by statistical properties</article-title><source>Psychological Science</source><volume>12</volume><fpage>157</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00327</pub-id><pub-id pub-id-type="pmid">11340926</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Arnold</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>ggthemes: Extra Themes, Scales and Geoms for 'ggplot2'</data-title><source>4.0</source><ext-link ext-link-type="uri" xlink:href="https://rdrr.io/cran/ggthemes/">https://rdrr.io/cran/ggthemes/</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Auguie</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>gridExtra: Miscellaneous Functions for &quot;Grid&quot; Graphics</data-title><version designator="2.3">2.3</version><ext-link ext-link-type="uri" xlink:href="https://rdrr.io/cran/gridExtra/">https://rdrr.io/cran/gridExtra/</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balas</surname> <given-names>B</given-names></name><name><surname>Nakano</surname> <given-names>L</given-names></name><name><surname>Rosenholtz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A summary-statistic representation in peripheral vision explains visual crowding</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1167/9.12.13</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balas</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Contrast negation and texture synthesis differentially disrupt natural texture appearance</article-title><source>Frontiers in Psychology</source><volume>3</volume><fpage>29</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00515</pub-id><pub-id pub-id-type="pmid">23181049</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bex</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>(In) Sensitivity to spatial distortion in natural scenes</article-title><source>Journal of Vision</source><volume>10</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1167/10.2.23</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Recognition-by-components: a theory of human image understanding</article-title><source>Psychological Review</source><volume>94</volume><fpage>115</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.94.2.115</pub-id><pub-id pub-id-type="pmid">3575582</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Block</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Seeing and windows of integration</article-title><source>Thought: A Journal of Philosophy</source><volume>2</volume><fpage>29</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1002/tht3.62</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouma</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Interaction effects in parafoveal letter recognition</article-title><source>Nature</source><volume>226</volume><fpage>177</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/226177a0</pub-id><pub-id pub-id-type="pmid">5437004</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brooks</surname> <given-names>SP</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>General methods for monitoring convergence of iterative simulations</article-title><source>Journal of Computational and Graphical Statistics</source><volume>7</volume><elocation-id>434</elocation-id><pub-id pub-id-type="doi">10.2307/1390675</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bürkner</surname> <given-names>P-C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Brms: an R package for bayesian multilevel models using stan</article-title><source>Journal of Statistical Software</source><volume>80</volume><fpage>1</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.18637/jss.v080.i01</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bürkner</surname> <given-names>P-C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Advanced bayesian multilevel modeling with the R package brms</article-title><source>The R Journal</source><volume>10</volume><fpage>395</fpage><lpage>411</lpage><pub-id pub-id-type="doi">10.32614/RJ-2018-017</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>H</given-names></name><name><surname>Rosenholtz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Search performance is better predicted by tileability than presence of a unique basic feature</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1167/16.10.13</pub-id><pub-id pub-id-type="pmid">27548090</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname> <given-names>AM</given-names></name><name><surname>Herzog</surname> <given-names>MH</given-names></name><name><surname>Francis</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visual crowding illustrates the inadequacy of local vs. global and feedforward vs. feedback distinctions in modeling visual perception</article-title><source>Frontiers in Psychology</source><volume>5</volume><pub-id pub-id-type="doi">10.3389/fpsyg.2014.01193</pub-id><pub-id pub-id-type="pmid">25374554</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MA</given-names></name><name><surname>Dennett</surname> <given-names>DC</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What is the bandwidth of perceptual experience?</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>324</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.03.006</pub-id><pub-id pub-id-type="pmid">27105668</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cornelissen</surname> <given-names>FW</given-names></name><name><surname>Peters</surname> <given-names>EM</given-names></name><name><surname>Palmer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The eyelink toolbox: eye tracking with MATLAB and the psychophysics toolbox</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><volume>34</volume><fpage>613</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.3758/BF03195489</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craven</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>A table of d' for M-alternative odd-man-out forced-choice procedures</article-title><source>Perception &amp; Psychophysics</source><volume>51</volume><fpage>379</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.3758/BF03211631</pub-id><pub-id pub-id-type="pmid">1603651</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Croner</surname> <given-names>LJ</given-names></name><name><surname>Kaplan</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Receptive fields of P and M ganglion cells across the primate retina</article-title><source>Vision Research</source><volume>35</volume><fpage>7</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)E0066-T</pub-id><pub-id pub-id-type="pmid">7839612</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dacey</surname> <given-names>DM</given-names></name><name><surname>Petersen</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Dendritic field size and morphology of midget and parasol ganglion cells of the human retina</article-title><source>PNAS</source><volume>89</volume><fpage>9666</fpage><lpage>9670</lpage><pub-id pub-id-type="doi">10.1073/pnas.89.20.9666</pub-id><pub-id pub-id-type="pmid">1409680</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dakin</surname> <given-names>SC</given-names></name><name><surname>Bex</surname> <given-names>PJ</given-names></name><name><surname>Cass</surname> <given-names>JR</given-names></name><name><surname>Watt</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dissociable effects of attention and crowding on orientation averaging</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>28</elocation-id><pub-id pub-id-type="doi">10.1167/9.11.28</pub-id><pub-id pub-id-type="pmid">20053091</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dakin</surname> <given-names>SC</given-names></name><name><surname>Baruch</surname> <given-names>NJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Context influences contour integration</article-title><source>Journal of Vision</source><volume>9</volume><fpage>13</fpage><pub-id pub-id-type="doi">10.1167/9.2.13</pub-id><pub-id pub-id-type="pmid">19271923</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dakin</surname> <given-names>SC</given-names></name><name><surname>Watt</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The computation of orientation statistics from visual texture</article-title><source>Vision Research</source><volume>37</volume><fpage>3181</fpage><lpage>3192</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(97)00133-8</pub-id><pub-id pub-id-type="pmid">9463699</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Deza</surname> <given-names>A</given-names></name><name><surname>Jonnalagadda</surname> <given-names>A</given-names></name><name><surname>Eckstein</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Towards metamerism via foveated style transfer</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.10041">https://arxiv.org/abs/1705.10041</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehinger</surname> <given-names>KA</given-names></name><name><surname>Rosenholtz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A general account of peripheral encoding also predicts scene perception performance</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1167/16.2.13</pub-id><pub-id pub-id-type="pmid">27893077</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faivre</surname> <given-names>N</given-names></name><name><surname>Berthet</surname> <given-names>V</given-names></name><name><surname>Kouider</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Nonconscious influences from emotional faces: a comparison of visual crowding, masking, and continuous flash suppression</article-title><source>Frontiers in Psychology</source><volume>3</volume><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00129</pub-id><pub-id pub-id-type="pmid">22563325</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname> <given-names>J</given-names></name><name><surname>Whitney</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Object-level visual information gets through the bottleneck of crowding</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>1389</fpage><lpage>1398</lpage><pub-id pub-id-type="doi">10.1152/jn.00904.2010</pub-id><pub-id pub-id-type="pmid">21676930</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname> <given-names>G</given-names></name><name><surname>Manassi</surname> <given-names>M</given-names></name><name><surname>Herzog</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural dynamics of grouping and segmentation explain properties of visual crowding</article-title><source>Psychological Review</source><volume>124</volume><fpage>483</fpage><lpage>504</lpage><pub-id pub-id-type="doi">10.1037/rev0000070</pub-id><pub-id pub-id-type="pmid">28437128</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname> <given-names>J</given-names></name><name><surname>Ziemba</surname> <given-names>CM</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A functional and perceptual signature of the second visual area in primates</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>974</fpage><lpage>981</lpage><pub-id pub-id-type="doi">10.1038/nn.3402</pub-id><pub-id pub-id-type="pmid">23685719</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname> <given-names>J</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Metamers of the ventral stream</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1195</fpage><lpage>1201</lpage><pub-id pub-id-type="doi">10.1038/nn.2889</pub-id><pub-id pub-id-type="pmid">21841776</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname> <given-names>J</given-names></name><name><surname>Simoncelli</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The radial and tangential extent of spatial metamers</article-title><source>Journal of Vision</source><volume>13</volume><fpage>573</fpage><pub-id pub-id-type="doi">10.1167/13.9.573</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gatys</surname> <given-names>LA</given-names></name><name><surname>Ecker</surname> <given-names>AS</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Texture synthesis using convolutional neural networks</article-title><conf-name>2016 23rd International Conference on Pattern Recognition (ICPR)</conf-name></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Geirhos</surname> <given-names>R</given-names></name><name><surname>Rubisch</surname> <given-names>P</given-names></name><name><surname>Michaelis</surname> <given-names>C</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Wichmann</surname> <given-names>FA</given-names></name><name><surname>Brendel</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title><italic>ImageNet-Trained CNNs are biased towards texture; Increasing shape bias Improves Accuracy and robustness</italic></article-title><conf-name>International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Hwang</surname> <given-names>J</given-names></name><name><surname>Vehtari</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Understanding predictive information criteria for bayesian models</article-title><source>Statistics and Computing</source><volume>24</volume><fpage>997</fpage><lpage>1016</lpage><pub-id pub-id-type="doi">10.1007/s11222-013-9416-2</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haun</surname> <given-names>AM</given-names></name><name><surname>Tononi</surname> <given-names>G</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Tsuchiya</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Are we underestimating the richness of visual experience?</article-title><source>Neuroscience of Consciousness</source><volume>2017</volume><pub-id pub-id-type="doi">10.1093/nc/niw023</pub-id><pub-id pub-id-type="pmid">30042833</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herzog</surname> <given-names>MH</given-names></name><name><surname>Sayim</surname> <given-names>B</given-names></name><name><surname>Chicherov</surname> <given-names>V</given-names></name><name><surname>Manassi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Crowding, grouping, and object recognition: a matter of appearance</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>5</elocation-id><pub-id pub-id-type="doi">10.1167/15.6.5</pub-id><pub-id pub-id-type="pmid">26024452</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname> <given-names>MD</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The No-U-Turn sampler: adaptively setting path lengths in hamiltonian monte carlo</article-title><source>Journal of Machine Learning Research</source><volume>15</volume><fpage>1593</fpage><lpage>1623</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jäkel</surname> <given-names>F</given-names></name><name><surname>Singh</surname> <given-names>M</given-names></name><name><surname>Wichmann</surname> <given-names>FA</given-names></name><name><surname>Herzog</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An overview of quantitative approaches in gestalt perception</article-title><source>Vision Research</source><volume>126</volume><fpage>3</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2016.06.004</pub-id><pub-id pub-id-type="pmid">27353224</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>E</given-names></name><name><surname>Oliphant</surname> <given-names>T</given-names></name><name><surname>Peterson</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><data-title><italic>SciPy: Open Source Scientific Tools for Python</italic></data-title><ext-link ext-link-type="uri" xlink:href="https://www.researchgate.net/publication/213877848_SciPy_Open_Source_Scientific_Tools_for_Python">https://www.researchgate.net/publication/213877848_SciPy_Open_Source_Scientific_Tools_for_Python</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Judd</surname> <given-names>T</given-names></name><name><surname>Ehinger</surname> <given-names>KA</given-names></name><name><surname>Durand</surname> <given-names>F</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Learning to predict where humans look</article-title><conf-name>IEEE 12th International Conference on Computer Vision</conf-name><fpage>2106</fpage><lpage>2113</lpage><ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/document/5459462">https://ieeexplore.ieee.org/document/5459462</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Judd</surname> <given-names>T</given-names></name><name><surname>Durand</surname> <given-names>F</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>A Benchmark of Computational Models of Saliency to Predict Human Fixations</source><publisher-name>CSAIL Technical Reports</publisher-name></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshvari</surname> <given-names>S</given-names></name><name><surname>Rosenholtz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Pooling of continuous features provides a unifying account of crowding</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>39</elocation-id><pub-id pub-id-type="doi">10.1167/16.3.39</pub-id><pub-id pub-id-type="pmid">26928055</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname> <given-names>M</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name><name><surname>Pelli</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s New in Psychtoolbox-3</article-title><source>Perception</source><volume>36</volume></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname> <given-names>J</given-names></name><name><surname>Valsecchi</surname> <given-names>M</given-names></name><name><surname>van Doorn</surname> <given-names>A</given-names></name><name><surname>Wagemans</surname> <given-names>J</given-names></name><name><surname>Gegenfurtner</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Eidolons: novel stimuli for vision research</article-title><source>Journal of Vision</source><volume>17</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.1167/17.2.7</pub-id><pub-id pub-id-type="pmid">28245489</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Koffka</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1935">1935</year><source>Principles of Gestalt Psychology</source><publisher-loc>Oxford, UK</publisher-loc><publisher-name>Harcourt Brace</publisher-name></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kruschke</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan</source><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubilius</surname> <given-names>J</given-names></name><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Op de Beeck</surname> <given-names>HP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep neural networks as a computational model for human shape sensitivity</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004896</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004896</pub-id><pub-id pub-id-type="pmid">27124699</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Landy</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>The New Visual Neurosciences</chapter-title><person-group person-group-type="editor"><name><surname>Werner</surname> <given-names>J. S</given-names></name><name><surname>Chalupa</surname> <given-names>L. M</given-names></name></person-group><source>Texture Analysis and Perception</source><publisher-name>MIT Press</publisher-name><fpage>639</fpage><lpage>652</lpage></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lettvin</surname> <given-names>JY</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>On seeing sidelong</article-title><source>The Sciences</source><volume>16</volume><fpage>10</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1002/j.2326-1951.1976.tb01231.x</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levi</surname> <given-names>DM</given-names></name><name><surname>Carney</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Crowding in peripheral vision: why bigger is better</article-title><source>Current Biology</source><volume>19</volume><fpage>1988</fpage><lpage>1993</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.09.056</pub-id><pub-id pub-id-type="pmid">19853450</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>B</given-names></name><name><surname>Konkle</surname> <given-names>T</given-names></name><name><surname>Cohen</surname> <given-names>MA</given-names></name><name><surname>Alvarez</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mid-level perceptual features distinguish objects of different real-world sizes</article-title><source>Journal of Experimental Psychology: General</source><volume>145</volume><fpage>95</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1037/xge0000130</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loschky</surname> <given-names>LC</given-names></name><name><surname>Hansen</surname> <given-names>BC</given-names></name><name><surname>Sethi</surname> <given-names>A</given-names></name><name><surname>Pydimarri</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The role of higher order image statistics in masking scene gist recognition</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>72</volume><fpage>427</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.3758/APP.72.2.427</pub-id><pub-id pub-id-type="pmid">20139457</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mack</surname> <given-names>A</given-names></name><name><surname>Rock</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Inattentional Blindness</source><volume>33</volume><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/3707.001.0001</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Macmillan</surname> <given-names>NA</given-names></name><name><surname>Creelman</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Detection Theory: A User’s Guide</source><publisher-loc>Mahwah, NJ</publisher-loc><publisher-name>Lawrence Erlbaum</publisher-name></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manassi</surname> <given-names>M</given-names></name><name><surname>Sayim</surname> <given-names>B</given-names></name><name><surname>Herzog</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Grouping, pooling, and when bigger is better in visual crowding</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1167/12.10.13</pub-id><pub-id pub-id-type="pmid">23019118</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manassi</surname> <given-names>M</given-names></name><name><surname>Sayim</surname> <given-names>B</given-names></name><name><surname>Herzog</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>When crowding of crowding leads to uncrowding</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/13.13.10</pub-id><pub-id pub-id-type="pmid">24213598</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McElreath</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</source><publisher-loc>New York</publisher-loc><publisher-name>CRC Press, Taylor &amp; Francis Group</publisher-name></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Movshon</surname> <given-names>JA</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Representation of naturalistic image structure in the primate visual cortex</article-title><source>Cold Spring Harbor Symposia on Quantitative Biology</source><volume>79</volume><fpage>115</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1101/sqb.2014.79.024844</pub-id><pub-id pub-id-type="pmid">25943766</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neri</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Object segmentation controls image reconstruction from natural scenes</article-title><source>PLOS Biology</source><volume>15</volume><elocation-id>e1002611</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002611</pub-id><pub-id pub-id-type="pmid">28827801</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Regan</surname> <given-names>JK</given-names></name><name><surname>Rensink</surname> <given-names>RA</given-names></name><name><surname>Clark</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Change-blindness as a result of 'mudsplashes'</article-title><source>Nature</source><volume>398</volume><fpage>34</fpage><pub-id pub-id-type="doi">10.1038/17953</pub-id><pub-id pub-id-type="pmid">10078528</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okazawa</surname> <given-names>G</given-names></name><name><surname>Tajima</surname> <given-names>S</given-names></name><name><surname>Komatsu</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Image statistics underlying natural texture selectivity of neurons in macaque V4</article-title><source>PNAS</source><volume>112</volume><fpage>E351</fpage><lpage>E360</lpage><pub-id pub-id-type="doi">10.1073/pnas.1415146112</pub-id><pub-id pub-id-type="pmid">25535362</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkes</surname> <given-names>L</given-names></name><name><surname>Lund</surname> <given-names>J</given-names></name><name><surname>Angelucci</surname> <given-names>A</given-names></name><name><surname>Solomon</surname> <given-names>JA</given-names></name><name><surname>Morgan</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Compulsory averaging of crowded orientation signals in human vision</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>739</fpage><lpage>744</lpage><pub-id pub-id-type="doi">10.1038/89532</pub-id><pub-id pub-id-type="pmid">11426231</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname> <given-names>DG</given-names></name><name><surname>Tillman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The uncrowded window of object recognition</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1129</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1038/nn.2187</pub-id><pub-id pub-id-type="pmid">18828191</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petrov</surname> <given-names>Y</given-names></name><name><surname>Meleshkevich</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Asymmetries and idiosyncratic hot spots in crowding</article-title><source>Vision Research</source><volume>51</volume><fpage>1117</fpage><lpage>1123</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2011.03.001</pub-id><pub-id pub-id-type="pmid">21439309</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portilla</surname> <given-names>J</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A parametric texture model based on joint statistics of complex wavelet coefficients</article-title><source>International Journal of Computer Vision</source><volume>40</volume><fpage>49</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1023/A:1026553619983</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="software"><person-group person-group-type="author"><collab>R Core Team</collab></person-group><year iso-8601-date="2017">2017</year><data-title>R: A Language and Environment for Statistical Computing</data-title><source>R Foundation for Statistical Computing</source><publisher-loc>Vienna, Austria</publisher-loc></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rensink</surname> <given-names>RA</given-names></name><name><surname>O'Regan</surname> <given-names>JK</given-names></name><name><surname>Clark</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>To see or not to see: the need for attention to perceive changes in scenes</article-title><source>Psychological Science</source><volume>8</volume><fpage>368</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.1997.tb00427.x</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robol</surname> <given-names>V</given-names></name><name><surname>Casco</surname> <given-names>C</given-names></name><name><surname>Dakin</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The role of crowding in contextual influences on contour integration</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1167/12.7.3</pub-id><pub-id pub-id-type="pmid">22776847</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname> <given-names>S</given-names></name><name><surname>Chakravarthi</surname> <given-names>R</given-names></name><name><surname>Pelli</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The bouma law of crowding, revised: critical spacing is equal across parts, not objects</article-title><source>Journal of Vision</source><volume>14</volume><fpage>10</fpage><pub-id pub-id-type="doi">10.1167/14.6.10</pub-id><pub-id pub-id-type="pmid">25502230</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenholtz</surname> <given-names>R</given-names></name><name><surname>Huang</surname> <given-names>J</given-names></name><name><surname>Ehinger</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Rethinking the role of top-down attention in vision: effects attributable to a lossy representation in peripheral vision</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00013</pub-id><pub-id pub-id-type="pmid">22347200</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenholtz</surname> <given-names>R</given-names></name><name><surname>Huang</surname> <given-names>J</given-names></name><name><surname>Raj</surname> <given-names>A</given-names></name><name><surname>Balas</surname> <given-names>BJ</given-names></name><name><surname>Ilie</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>A summary statistic representation in peripheral vision explains visual search</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/12.4.14</pub-id><pub-id pub-id-type="pmid">22523401</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenholtz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Capabilities and limitations of peripheral vision</article-title><source>Annual Review of Vision Science</source><volume>2</volume><fpage>437</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035733</pub-id><pub-id pub-id-type="pmid">28532349</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saarela</surname> <given-names>TP</given-names></name><name><surname>Sayim</surname> <given-names>B</given-names></name><name><surname>Westheimer</surname> <given-names>G</given-names></name><name><surname>Herzog</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Global stimulus configuration modulates crowding</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>5</elocation-id><pub-id pub-id-type="doi">10.1167/9.2.5</pub-id><pub-id pub-id-type="pmid">19271915</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seth</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A predictive processing theory of sensorimotor contingencies: explaining the puzzle of perceptual presence and its absence in synesthesia</article-title><source>Cognitive Neuroscience</source><volume>5</volume><fpage>97</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1080/17588928.2013.877880</pub-id><pub-id pub-id-type="pmid">24446823</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname> <given-names>K</given-names></name><name><surname>Zisserman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Very deep convolutional networks for Large-Scale image recognition</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</ext-link></element-citation></ref><ref id="bib79"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Stan Development</collab></person-group><year iso-8601-date="2015">2015</year><source>Stan Modeling Language Users Guide and Reference Manual</source><version designator="2.10.0">2.10.0</version></element-citation></ref><ref id="bib80"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Stan Development Team</collab></person-group><year iso-8601-date="2017">2017</year><data-title>Stan: A C++ Library for Probability and Sampling</data-title><version designator="2.14.0">2.14.0</version></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teufel</surname> <given-names>C</given-names></name><name><surname>Dakin</surname> <given-names>SC</given-names></name><name><surname>Fletcher</surname> <given-names>PC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Prior object-knowledge sharpens properties of early visual feature-detectors</article-title><source>Scientific Reports</source><volume>8</volume><pub-id pub-id-type="doi">10.1038/s41598-018-28845-5</pub-id><pub-id pub-id-type="pmid">30022033</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thaler</surname> <given-names>L</given-names></name><name><surname>Schütz</surname> <given-names>AC</given-names></name><name><surname>Goodale</surname> <given-names>MA</given-names></name><name><surname>Gegenfurtner</surname> <given-names>KR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>What is the best fixation target? the effect of target shape on stability of fixational eye movements</article-title><source>Vision Research</source><volume>76</volume><fpage>31</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.10.012</pub-id><pub-id pub-id-type="pmid">23099046</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valsecchi</surname> <given-names>M</given-names></name><name><surname>Koenderink</surname> <given-names>J</given-names></name><name><surname>van Doorn</surname> <given-names>A</given-names></name><name><surname>Gegenfurtner</surname> <given-names>KR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Prediction shapes peripheral appearance</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>21</elocation-id><pub-id pub-id-type="doi">10.1167/18.13.21</pub-id><pub-id pub-id-type="pmid">30593064</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Burg</surname> <given-names>E</given-names></name><name><surname>Olivers</surname> <given-names>CN</given-names></name><name><surname>Cass</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evolving the keys to visual crowding</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>43</volume><fpage>690</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1037/xhp0000337</pub-id><pub-id pub-id-type="pmid">28182476</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vehtari</surname> <given-names>A</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Gabry</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Practical bayesian model evaluation using Leave-One-Out Cross-Validation and WAIC</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1507.04544">https://arxiv.org/abs/1507.04544</ext-link></element-citation></ref><ref id="bib86"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Venables</surname> <given-names>WN</given-names></name><name><surname>Ripley</surname> <given-names>BD</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Modern Applied Statistics with S.</source><edition>Fourth Edition</edition><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-21706-2</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vickery</surname> <given-names>TJ</given-names></name><name><surname>Shim</surname> <given-names>WM</given-names></name><name><surname>Chakravarthi</surname> <given-names>R</given-names></name><name><surname>Jiang</surname> <given-names>YV</given-names></name><name><surname>Luedeman</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Supercrowding: weakly masking a target expands the range of crowding</article-title><source>Journal of Vision</source><volume>9</volume><fpage>12</fpage><pub-id pub-id-type="doi">10.1167/9.2.12</pub-id><pub-id pub-id-type="pmid">19271922</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagemans</surname> <given-names>J</given-names></name><name><surname>Elder</surname> <given-names>JH</given-names></name><name><surname>Kubovy</surname> <given-names>M</given-names></name><name><surname>Palmer</surname> <given-names>SE</given-names></name><name><surname>Peterson</surname> <given-names>MA</given-names></name><name><surname>Singh</surname> <given-names>M</given-names></name><name><surname>von der Heydt</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>A century of gestalt psychology in visual perception: I. perceptual grouping and figure-ground organization</article-title><source>Psychological Bulletin</source><volume>138</volume><fpage>1172</fpage><lpage>1217</lpage><pub-id pub-id-type="doi">10.1037/a0029333</pub-id><pub-id pub-id-type="pmid">22845751</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagemans</surname> <given-names>J</given-names></name><name><surname>Feldman</surname> <given-names>J</given-names></name><name><surname>Gepshtein</surname> <given-names>S</given-names></name><name><surname>Kimchi</surname> <given-names>R</given-names></name><name><surname>Pomerantz</surname> <given-names>JR</given-names></name><name><surname>van der Helm</surname> <given-names>PA</given-names></name><name><surname>van Leeuwen</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>A century of gestalt psychology in visual perception: ii. conceptual and theoretical foundations</article-title><source>Psychological Bulletin</source><volume>138</volume><fpage>1218</fpage><lpage>1252</lpage><pub-id pub-id-type="doi">10.1037/a0029334</pub-id><pub-id pub-id-type="pmid">22845750</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname> <given-names>TSA</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Wichmann</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Testing models of peripheral encoding using metamerism in an oddity paradigm</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.1167/16.2.4</pub-id><pub-id pub-id-type="pmid">26968866</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname> <given-names>TSA</given-names></name><name><surname>Funke</surname> <given-names>CM</given-names></name><name><surname>Ecker</surname> <given-names>AS</given-names></name><name><surname>Gatys</surname> <given-names>LA</given-names></name><name><surname>Wichmann</surname> <given-names>FA</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A parametric texture model based on deep convolutional features closely matches texture appearance for humans</article-title><source>Journal of Vision</source><volume>17</volume><elocation-id>5</elocation-id><pub-id pub-id-type="doi">10.1167/17.12.5</pub-id><pub-id pub-id-type="pmid">28983571</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname> <given-names>TSA</given-names></name><name><surname>Bex</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Image correlates of crowding in natural scenes</article-title><source>Journal of Vision</source><volume>12</volume><fpage>6</fpage><pub-id pub-id-type="doi">10.1167/12.7.6</pub-id><pub-id pub-id-type="pmid">22798053</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname> <given-names>DB</given-names></name><name><surname>Shen</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Nonaccidental properties underlie human categorization of complex natural scenes</article-title><source>Psychological Science</source><volume>25</volume><fpage>851</fpage><lpage>860</lpage><pub-id pub-id-type="doi">10.1177/0956797613512662</pub-id><pub-id pub-id-type="pmid">24474725</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="1995">1995</year><source>Foundations of Vision</source><publisher-name>Sinauer Associates</publisher-name></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A formula for human retinal ganglion cell receptive field density as a function of visual field location</article-title><source>Journal of Vision</source><volume>14</volume><fpage>15</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1167/14.7.15</pub-id><pub-id pub-id-type="pmid">24982468</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Whitney</surname> <given-names>D</given-names></name><name><surname>Haberman</surname> <given-names>J</given-names></name><name><surname>Sweeny</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>From textures to crowds: multiple levels of summary statistical perception</chapter-title><source>The New Visual Neurosciences</source><publisher-name>MIT Press</publisher-name><fpage>695</fpage><lpage>710</lpage></element-citation></ref><ref id="bib97"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wickham</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Ggplot2: Elegant Graphics for Data Analysis</source><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-98141-3</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wickham</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The Split-Apply-Combine strategy for data analysis</article-title><source>Journal of Statistical Software</source><volume>40</volume><fpage>1</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.18637/jss.v040.i01</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Wickham</surname> <given-names>H</given-names></name><name><surname>Francois</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Dplyr: A Grammar of Data Manipulation</source><ext-link ext-link-type="uri" xlink:href="https://rdrr.io/cran/dplyr/">https://rdrr.io/cran/dplyr/</ext-link></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilder</surname> <given-names>J</given-names></name><name><surname>Rezanejad</surname> <given-names>M</given-names></name><name><surname>Dickinson</surname> <given-names>S</given-names></name><name><surname>Siddiqi</surname> <given-names>K</given-names></name><name><surname>Jepson</surname> <given-names>A</given-names></name><name><surname>Walther</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Local contour symmetry facilitates scene categorization</article-title><source>Cognition</source><volume>182</volume><fpage>307</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2018.09.014</pub-id><pub-id pub-id-type="pmid">30415132</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xia</surname> <given-names>G-S</given-names></name><name><surname>Delon</surname> <given-names>J</given-names></name><name><surname>Gousseau</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Accurate junction detection and characterization in natural images</article-title><source>International Journal of Computer Vision</source><volume>106</volume><fpage>31</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1007/s11263-013-0640-1</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xie</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>Knitr: A Comprehensive Tool for Reproducible Research in R. BT - Implementing Reproducible Computational Research</chapter-title><person-group person-group-type="editor"><name><surname>Stodden</surname> <given-names>V</given-names></name><name><surname>Leisch</surname> <given-names>F</given-names></name><name><surname>Peng</surname> <given-names>R. D</given-names></name></person-group><source>Implementing Reproducible Computational Research</source><publisher-name>Chapman &amp; Hall/CRC</publisher-name></element-citation></ref><ref id="bib103"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xie</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Dynamic Documents with R and Knitr</source><publisher-name>Chapman &amp; Hall/CRC</publisher-name><pub-id pub-id-type="doi">10.1201/b15166</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Huang</surname> <given-names>J</given-names></name><name><surname>Yigit-Elliott</surname> <given-names>S</given-names></name><name><surname>Rosenholtz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cube search, revisited</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.1167/15.3.9</pub-id><pub-id pub-id-type="pmid">25780063</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziemba</surname> <given-names>CM</given-names></name><name><surname>Freeman</surname> <given-names>J</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Selectivity and tolerance for visual texture in macaque V2</article-title><source>PNAS</source><volume>113</volume><fpage>E3140</fpage><lpage>E3149</lpage><pub-id pub-id-type="doi">10.1073/pnas.1510847113</pub-id><pub-id pub-id-type="pmid">27173899</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.42512.011</object-id><sec id="s8" sec-type="appendix"><title>Additional experiments with the Freeman and Simoncelli model</title><sec id="s8-1"><title>Stimulus artifact control</title><p>During the course of our testing we noticed that synthesised images generated with the code from <ext-link ext-link-type="uri" xlink:href="http://github.com/freeman-lab/metamers">http://github.com/freeman-lab/metamers</ext-link> contained an artifact, visible as a wedge in the lower-left quadrant of the synthesised images in which the phases of the surrounding image structure were incorrectly matched (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>). The angle and extent of the wedge changed with the scale factor, and corresponded to the region where angular pooling regions wrapped from 0–2π (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B–C</xref>). The visibility of the artifact depended on image structure, but was definitely due to the synthesis procedure itself because it also occurred when synthesising matches to a white noise source image (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1D–E</xref>). The artifact was not peculiar to our hardware or implementation because it is also visible in the stimuli shown in <xref ref-type="bibr" rid="bib26">Deza et al. (2017)</xref>.</p><fig id="app1fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.012</object-id><label>Appendix 1—figure 1.</label><caption><title>Our results do not depend on an artifact in the synthesis procedure.</title><p>(<bold>A</bold>) During our pilot testing, we noticed a wedge-like artifact in the synthesis procedure of Freeman and Simoncelli (highlighted in red wedge; image from <ext-link ext-link-type="uri" xlink:href="https://github.com/freeman-lab/metamers">https://github.com/freeman-lab/metamers</ext-link> and shared under a CC-BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">https://creativecommons.org/licenses/by/3.0/</ext-link>)). The artifact occurred where the angular pooling regions wrapped from 0 to <inline-formula><mml:math id="inf49"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula> (<bold>B</bold>) pooling region contours shown with increasing greyscale to wrap point, (<bold>C</bold>) overlayed on scene with artifact. (<bold>D</bold>) The artifact was not driven by image content, because it also occurred when synthesising to match white noise (shown with enhanced contrast in (<bold>E</bold>)). If participants’ good performance at small scale factors was due to taking advantage of this wedge, removing it by masking out that image region should drop performance to chance. (<bold>F</bold>) Performance at the two smallest scale factors replotted from the main experiment (left) and with a wedge mask overlayed (right) in the orig vs synth comparison. Points show average (±2SE) over participants; faint lines link individual participant means. Performance remains above chance for the scene-like images, indicating that the low critical scales we observed were not due to the wedge artifact.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app1-fig1-v1.tif"/></fig><p>Participants in our experiment could have learned to use the artifact to help discriminate images, particularly synthesised images from original images (since only synthesised images contain the artifact). This may have boosted their sensitivity more than might be expected from the model described by Freeman and Simoncelli, leading to the lower critical scales we observed. To control for this, we re-ran the original vs synth condition with the same participants, with the exception that the lower-left quadrant of the image containing the artifact was masked by a grey wedge (in both original and synthesised images) with angular subtense of 60 degrees. We used only the lowest two scale factors from the main experiment, and participants completed this control experiment after the main experiment reported in the paper. We discarded trials for which participants made no response (N = 9) or broke fixation (N = 57), leaving a total of 1014 trials for further analysis. If the high sensitivity at low scale factors we observed above were due to participants using the artifact, then their performance with the masked stimuli should fall to chance for low scale factors.</p><p>This is not what we observed: while performance with the wedge was slightly worse (perhaps because a sizable section of the image was masked), the scene-like images remained above chance performance for the lowest two scale factors (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1F</xref>). This shows that the low critical scale factors we observed in the main experiment are not due to the wedge artifact.</p><p>We performed one additional artifact control experiment. The FS algorithm preserves a small central region of the image exactly in order to match foveal appearance. If there is any image artifact produced by the synthesis procedure at the border of this region, participants could have used this artifact to discriminate the stimuli in the original vs synth condition. Authors TW and CF performed new oddity discrimination trials in which a grey annular occluding zone (inner radius 0.4 deg, outer radius 1.95 deg) was presented over all images. If the low scale factors we find are because participants used a stimulus artifact, then performance at the low scales should drop to chance.</p><p>The results of this additional experiment are shown in Figure (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). Both participants can still discriminate real and synthesised scene-like images better than chance even after superposition of the occluding annulus, indicating that any central artifact is not a crucial determinant of discriminability.</p><fig id="app1fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.013</object-id><label>Appendix 1—figure 2.</label><caption><title>Our results do not depend on any potential annular artifact resulting from the synthesis procedure.</title><p>Performance at the three smallest scale factors replotted from the main experiment (left) and with an annular mask overlayed (right) in the orig vs synth comparison for authors TW and CF. Points show average performance (error bars show 95% beta distribution confidence limits). Performance remains above chance for the scene-like images, indicating that the low critical scales we observed were not due to a potential annular artifact.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app1-fig2-v1.tif"/></fig></sec><sec id="s8-2"><title>Junctions in original images</title><p>For each of the 20 original images used in our first experiment, we used the junction detection algorithm of <xref ref-type="bibr" rid="bib101">Xia et al. (2014)</xref> to identify junctions in the image (with algorithm parameter <inline-formula><mml:math id="inf50"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>36</mml:mn></mml:mrow></mml:math></inline-formula>). We subdivided all three-edge junctions into T-, Y- and arrow-junctions according to the angle criteria used in <xref ref-type="bibr" rid="bib93">Walther and Shen (2014)</xref>, and excluded all junctions that fell outside the circular region of the image shown in our experiment.</p><p>We find that scene-like images tend to contain more junctions than texture-like images (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3A</xref>). This relationship became stronger when we excluded ‘less meaningful’ junctions (using a ‘meaningfulness’ cutoff of <inline-formula><mml:math id="inf51"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>NFA</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <xref ref-type="bibr" rid="bib101">Xia et al. (2014)</xref>; <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3C</xref>). Images with smaller critical scales are associated with the presence of junctions (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3B</xref>), and this association gets stronger when small and weak junctions are excluded (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3D</xref>).</p><p>If junction information is important for scene appearance and the FS-model fails to adequately capture this information, we would expect such a negative relationship between junctions and critical scales. Of course, the analysis above does not support a specific causal role for junction information: for example, it may be correlated with simple edge density. Future studies could confirm (or reject) this relationship using a larger and more diagnostic image set.</p><fig id="app1fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.014</object-id><label>Appendix 1—figure 3.</label><caption><title>The number of junctions present in original images may be related to critical scale estimates.</title><p>(<bold>A</bold>) Distribution of arrow (A), L-, T-, X- and Y-junctions at all scales and all levels of ‘meaningfulness’ (<xref ref-type="bibr" rid="bib101">Xia et al., 2014</xref>) in scene-like and texture-like images. Each small point is one image; larger points with error bars show mean ±2 SE. Points have been jittered to aid visibility. (<bold>B</bold>) Correlations between number of junctions of each type with critical scale estimates for that image from the main experiment. Grey line shows linear model fit with shaded region showing 95% confidence area. Pearson correlation coefficient shown below. Note that the x-axis scales in the subplots differ. (<bold>C</bold>) Same as A but for junctions defined with a more strict ‘meaningfulness’ cutoff of <inline-formula><mml:math id="inf52"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>NFA</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib101">Xia et al., 2014</xref>). (<bold>D</bold>) Same as B for more ‘meaningful’ junctions as in C.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app1-fig3-v1.tif"/></fig></sec><sec id="s8-3"><title>ABX replication</title><p>Participants in our experiment showed poor performance in the synth vs synth condition even for large scale factors (highest accuracy for a participant at the largest scale of 1.45 was 0.8, average accuracy 0.58), leading to relatively flat psychometric functions (<xref ref-type="fig" rid="fig2">Figure 2F</xref> of main manuscript). In contrast, most participants in <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref> achieved accuracies above 90% correct for the highest scale factor they test (1.45 as in our experiment). One difference between our experiment and <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref> is that they used an ABX task, in which participants saw two images A and B, followed by image X, and had to report whether image X was the same as A or B. Perhaps our oddity task is simply harder: due to greater memory load or the cognitive demands of the comparison, participants in our experiment were unable to perform consistently well.</p><p>To assess whether the use of an oddity task lead to our finding of lower critical scales and/or poorer asymptotic performance in the synth vs synth condition, we re-ran our experiment as an ABX task. We used the same timing parameters as in Freeman and Simoncelli. Six participants participated in the experiment, including a research assistant (the same as in the main experiment), four naÃ¯ve participants and author AE (who only participated in the synth vs synth condition). We discarded trials for which participants made no response (N = 61) or broke fixation (N = 442), leaving a total of 7537 trials for further analysis. The predicted proportion correct in the ABX task was derived from <inline-formula><mml:math id="inf53"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> using the link function given by <xref ref-type="bibr" rid="bib56">Macmillan and Creelman (2005)</xref>, (229–33) for a differencing model in a roving design:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>correct</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mn>6</mml:mn></mml:msqrt></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:msqrt><mml:mn>6</mml:mn></mml:msqrt></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the standard cumulative Normal distribution.</p><p>As in our main experiment with the oddity task, we find that participants could easily discriminate scene-like syntheses from their original at all scales we could generate (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). Critical scale factor estimates were similar to those in the main experiment, indicating that the ABX task did not make a large difference to these results. Critical scale estimates were slightly larger, but much more uncertain, in the synth vs synth condition. This uncertainty is largely driven by the even poorer asymptotic performance than in the main experiment. This shows that the results we report in the primary manuscript are not particular to the oddity task.</p><fig id="app1fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.015</object-id><label>Appendix 1—figure 4.</label><caption><title>Results from the main paper replicated under an ABX task.</title><p>(<bold>A</bold>) Performance in the ABX task as a function of scale factor. Points show grand mean ±2 SE over participants; faint lines link individual participant performance levels. Solid curves and shaded regions show the fit of a nonlinear mixed-effects model estimating the critical scale and gain. (<bold>B</bold>) When comparing original and synthesised images, estimated critical scales (scale at which performance rises above chance) are lower for scene-like than for texture-like images. Points with error bars show population mean and 95% credible intervals. Triangles show posterior means for participants; diamonds show posterior means for images. Black squares show critical scale estimates of the four participants from Freeman and Simoncelli reproduced from that paper (x-position jittered to reduce overplotting); shaded regions denote the receptive field scaling of V1 and V2 estimated by Freeman and Simoncelli.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app1-fig4-v1.tif"/></fig><p>What explains the discrepancy between asymptotic performance in our experiment vs Freeman and Simoncelli? One possibility is that the participants in Freeman and Simoncelli’s experiment were more familiar with the images shown, and that good asymptotic performance in the synth vs synth condition requires strong familiarity. Freeman and Simoncelli used four original (source) images, and generated three unique synthesised images for each source image at each scale, compared to our 20 source images with four syntheses.</p></sec></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.42512.016</object-id><sec id="s9" sec-type="appendix"><title>CNN scene appearance model</title><p>Here we describe the CNN scene appearance model presented in the paper in more detail, as well as additional experiments concerning this model.</p><p>To create a summary statistic model using CNN features, we compute the mean activation in a subset of CNN layers over a number of radial and angular spatial regions (see <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref>). Increasing the number of pooling regions (reducing the spatial area over which CNN features are pooled) preserves more of the structure of the original image. New images can be synthesised by minimising the difference between the model features for a given input image and a white noise image via an iterative gradient descent procedure (see below). This allows us to synthesise images that are physically different to the original but approximately the same according to the model. We did this for each of four pooling region sizes, named model 4, 8, 16 and 32 respectively after the number of angular pooling regions. These features were matched over three spatial scales, which we found improved the model’s ability to capture long-range correlations.</p><fig id="app2fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.017</object-id><label>Appendix 2—figure 1.</label><caption><title>Methods for the CNN scene appearance model.</title><p>(<bold>A</bold>) The average activations in a subset of CNN feature maps were computed over non-overlapping radial and angular pooling regions that increase in area away from the image centre (not to scale), for three spatial scales. Increasing the number of pooling regions (CNN 4 and CNN 8 shown in this example) increases the fidelity of matching to the original image, restricting the range over which distortions can occur. Higher-layer CNN receptive fields overlap the pooling regions, ensuring smooth transitions between regions. The central 3° of the image (grey fill) is fixed to be the original. (<bold>B</bold>) The image radius subtended 12.5°. (<bold>C</bold>) An original image from the MIT1003 dataset. (<bold>D</bold>) Synthesised image matched to the image from C by the CNN 8 pooling model. (<bold>E</bold>) Synthesised image matched to the image from E by the CNN 32 pooling model. Fixating the central bullseye, it should be apparent that the CNN 32 model preserves more information than the CNN 8 model, but that the periphery is nevertheless significantly distorted relative to the original. Images from the MIT 1003 dataset (<xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>), (<ext-link ext-link-type="uri" xlink:href="https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html</ext-link>) and reproduced under a CC-BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">https://creativecommons.org/licenses/by/3.0/</ext-link>) with changes as described in the Materials and methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app2-fig1-v1.tif"/></fig><p>In Experiment 1, we tested the discriminability of syntheses generated from the four pooling models in a set of 400 images that were novel to the participants. Experiment 2 examines the effect of image familiarity by repeatedly presenting a small number of source images. Experiment 3 tested the effect of cueing spatial attention on performance.</p><sec id="s9-1"><title>CNN model methods</title><sec id="s9-1-1"><title>Radial and angular pooling</title><p>In the texture synthesis approach of <xref ref-type="bibr" rid="bib34">Gatys et al. (2015)</xref>, spatial information is removed from the raw CNN activations by computing summary statistics (the Gram matrices of correlations between feature maps) over the whole image. In the ‘foveated’ pooling model we present here, we compute and match the mean of the feature maps (i.e. not the full Gram matrices) over local image regions by dividing the image into a number of radial and angular pooling regions (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref>). The radius defining the border between each radial pooling region is based on a given number of angular regions <inline-formula><mml:math id="inf55"><mml:msub><mml:mi>N</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula> (which divide the circle evenly) and given by<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>α</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the radius of each region <inline-formula><mml:math id="inf57"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is the outermost radius (set to be half the image size), and <inline-formula><mml:math id="inf59"><mml:mi>α</mml:mi></mml:math></inline-formula> is the ratio between the radial and angular difference. Radial regions were created for all <inline-formula><mml:math id="inf60"><mml:mi>i</mml:mi></mml:math></inline-formula> for which <inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math></inline-formula>~px, corresponding to the preserved central region of the image (see below). We set <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> because at this ratio <inline-formula><mml:math id="inf63"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf64"><mml:msub><mml:mi>N</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> is the number of radial regions) for most <inline-formula><mml:math id="inf65"><mml:msub><mml:mi>N</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula>. The value of <inline-formula><mml:math id="inf66"><mml:msub><mml:mi>N</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula> corresponds to the model name used in the paper (e.g. ‘CNN 4’ uses <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>).</p><p>We now apply these pooling regions to the activations of the VGG-19 deep CNN (<xref ref-type="bibr" rid="bib78">Simonyan and Zisserman, 2015</xref>). For a subset of VGG-19 layers (conv1_1, conv2_1, conv3_1, conv4_1, conv5_1) we compute the mean activation for each feature map <inline-formula><mml:math id="inf68"><mml:mi>j</mml:mi></mml:math></inline-formula> in each layer <inline-formula><mml:math id="inf69"><mml:mi>l</mml:mi></mml:math></inline-formula> within each (radial or angular) pooling region <inline-formula><mml:math id="inf70"><mml:mi>p</mml:mi></mml:math></inline-formula> as<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where is <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula> the size of the feature map of layer l in pixels and <inline-formula><mml:math id="inf72"><mml:mi>k</mml:mi></mml:math></inline-formula> is the (vectorised) spatial position in feature map <inline-formula><mml:math id="inf73"><mml:msubsup><mml:mi>F</mml:mi><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:math></inline-formula>. The set of all <inline-formula><mml:math id="inf74"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:math></inline-formula> provides parameters that specify the foveated image at a given scale. Note that while the radial and angular pooling region responses are computed separately, because they are added together to the loss function during optimisation (see below) they effectively overlap (as depicted in <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref>).</p><p>In addition, while the borders of the pooling regions are hard-edged (i.e. pooling regions are non-overlapping), the receptive fields of CNN units (area of pixels in the image that can activate a given unit in a feature map) can span multiple pooling regions. This means that the model parameters of a given pooling region will depend on image structure lying outside the pooling region (particularly for feature maps in the higher network layers). This encourages smooth transitions between pooling regions in the synthesised images.</p></sec><sec id="s9-1-2"><title>Multiscale model</title><p>In the VGG-19 network, receptive fields of the units are squares of a certain size, and this size is independent of the input size of the image. That is, given a hypothetical receptive field centred in the image of size 128 ~ px square, the unit will be sensitive to one quarter of the image for input size 512 ~ px but half the image for input size 256. Therefore, the same unit in the network can receive image structure at a different scale by varying the input image size, and in the synthesis process the low (high) frequency content can be reproduced with greater fidelity by using a small (large) input size.</p><p>We leverage this relationship to better capture long-range correlations in image structure (caused by for example edges that extend across large parts of the image) by computing and matching the model statistics over three spatial scales. This is not a controversial idea: for example, the model of <xref ref-type="bibr" rid="bib32">Freeman and Simoncelli (2011)</xref> also computes features in a multiscale framework. How many scales is sufficient?</p><p>We evaluated the degree to which the number and combination of scales affected appearance in a psychophysical experiment on authors TW and CF. We matched 100 unique images using seven different models: four single-scale models corresponding to input sizes of 64, 128, 256 and 512 pixels, and three multiscale models in which features were matched at multiple scales ([256, 512 , 128, 256, 512] and [64, 128, 256, 512]). The foveated pooling regions corresponded to the CNN 32 model. Output images were upsampled to the final display resolution as appropriate. We discarded trials for which participants made no response (N = 2) or broke fixation (N = 5), leaving a total of 1393 trials for further analysis.</p><p><xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2</xref> shows that participants are sensitive to the difference between model syntheses and original images when features are matched at only a single scale. However, using two or three scales appears to be sufficient to match appearance on average. As a compromise between fidelity and computational tractability, we therefore used three scales for all other experiments on the CNN appearance model. The final model used three networks consisting of the same radial and angular regions described above, computed over sizes 128, 256 and 512 ~ px square. The final model representation <inline-formula><mml:math id="inf75"><mml:mi>W</mml:mi></mml:math></inline-formula> therefore consists of the pooled feature map activations over three scales: <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mn>128</mml:mn></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mn>256</mml:mn></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mn>512</mml:mn></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><fig id="app2fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.018</object-id><label>Appendix 2—figure 2.</label><caption><title>Performance for discriminating model syntheses and original scenes for single- and multi-scale models (all with pooling regions corresponding to CNN 32) for participants CF and TW.</title><p>Points show participant means (error bars show ±2 SEM), dashed line shows chance performance. The multiscale model with three scales produces close-to-chance performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app2-fig2-v1.tif"/></fig></sec><sec id="s9-1-3"><title>Gradient descent</title><p>As in <xref ref-type="bibr" rid="bib34">Gatys et al. (2015)</xref>, synthesised images are generated using iterative gradient descent, in which the mean-squared distance between the averaged feature maps of the original image and the synthesis is minimized. If <inline-formula><mml:math id="inf77"><mml:mi>T</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf78"><mml:mi>W</mml:mi></mml:math></inline-formula> are the model representations for the synthesis and the original image respectively, then the loss for each layer is given by<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf79"><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf80"><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> are the vectorised pixels of the original and new image respectively, <inline-formula><mml:math id="inf81"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of feature maps for layer <inline-formula><mml:math id="inf82"><mml:mi>l</mml:mi></mml:math></inline-formula> in scale <inline-formula><mml:math id="inf83"><mml:mi>s</mml:mi></mml:math></inline-formula>. A circular area in the middle of the image (radius 64 ~ px) is preserved to be the original image. Tiling pooling regions even for the centre of the image created reasonable syntheses but is prohibitively costly in generation time. To preserve the pixels in the circular area, the initialisation image of the gradient descent is identical to the original image. Outside the central area the gradient descent is initialised with Gaussian noise. The gradient descent used the L-BFGS optimiser (scipy implementation, <xref ref-type="bibr" rid="bib41">Jones et al., 2001</xref>) for 1000 iterations.</p></sec></sec><sec id="s9-2"><title>Experiment 1: Discriminability of CNN model syntheses for 400 unique images</title><p>This experiment measured whether any of the variants of the CNN scene appearance model could synthesise images that humans could not discriminate from their natural source images, and if so, identify the simplest variant of this model producing metamers. We chose a set of 400 images and had participants discriminate original and model-generated images in a temporal oddity paradigm.</p><sec id="s9-2-1"><title>Methods</title><p>The methods for this and the following psychophysical experiments were the same as in the main paper unless otherwise noted.</p><p>Participants</p><p>Thirteen participants participated in this experiment. Of these, ten participants were recruited via online advertisements and paid 15 Euro for a 1.5 hr testing session; the other three participants were authors AE, TW and CF. One session comprised one experiment using unique images (35 mins) followed by and one of repeated images (see below; 25 mins). All participants signed a consent form prior to participating. Participants reported normal or corrected-to-normal visual acuity. All procedures conformed to Standard 8 of the American Psychological Association’s ‘Ethical Principles of Psychologists and Code of Conduct’ (2010).</p><p>Stimuli</p><p>We used 400 images (two additional images for authors, see below) from the MIT 1003 database (<xref ref-type="bibr" rid="bib43">Judd et al., 2012</xref>; <xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>). One of the participants (TW) was familiar with the images in this database due to previous experiments. New images were synthesised using the multiscaled (512 px, 256 px, 128 px) foveated model described above, for four pooling region complexities (4, 8, 16 and 32). An image was synthesised for each of the 400 original images from each model (giving a total stimulus set including originals of 2000).</p><p>Procedure</p><p>Participants viewed the display from 60 cm; at this distance, pixels subtended approximately 0.024 degrees on average (approximately 41 pixels per degree of visual angle) – note that this is slightly further away than the experiment reported in the primary paper (changed to match the angular subtense used by Freeman and Simoncelli). Images therefore subtended ≈12.5° at the eye. As in the main paper, the stimuli were presented for 200 ms, with an inter-stimulus interval of 1000 ms, followed by a 1200 ms response window. Feedback was provided by a 100 ms change in fixation cross brightness. Gaze position was recorded during the trial. If the participant moved the eye more than 1.5 degrees away from the fixation spot, feedback signifying a fixation break appeared for 200 ~ ms after the response feedback. Prior to the next trial, the state of the participant’s eye position was monitored for 50 ms; if the eye position was reported as more than 1.5 degrees away from the fixation spot a recalibration was triggered. The inter-trial interval was 400 ms.</p><p>Each unique image was assigned to one of the four models for each participant (counterbalanced). That is, a given image might be paired with a CNN 4 synthesis for one participant and a CNN 8 synthesis for another. Showing each unique image only once ensures that the participants cannot become familiar with the images. For authors, images were divided into only CNN 8, CNN 16 and CNN 32 (making 134 images for each model and 402 trials in total for these participants). To ensure that the task was not too hard for naïve participants we added the easier CNN 4 model (making 100 images for each model version and 400 trials in total). The experiment was divided into six blocks consisting of 67 trials (65 trials for the last block). After each block a break screen was presented telling the participant their mean performance on the previous trials. During the breaks the participants were free to leave the testing room to take a break and to rest their eyes. At the beginning of each block the eyetracker was recalibrated. Naïve participants were trained to do the task, first using a slower practice of 6 trials and second a correct-speed practice of 30 trials (using five images not part of the stimulus set for the main experiment).</p><p>Data analysis</p><p>We discarded trials for which participants made no response (N = 81) or broke fixation (N = 440), leaving a total of 4685 trials for further analysis.</p><p>Performance at each level of CNN model complexity was quantified using a logistic mixed-effects model. Correct responses were assumed to arise from a fixed effect factor of CNN model (with four levels) plus the random effects of participant and image. The model (in lme4-style notation) was<code xml:space="preserve">correct - model + (model | subj) + (model | im_code)</code></p><p>with <monospace>family = Bernoulli(‘logit’)</monospace>, and using <monospace>contr.sdif</monospace> coding for the CNN model factor (<xref ref-type="bibr" rid="bib86">Venables and Ripley, 2002</xref>).</p><p>The posterior distribution over model parameters was estimated using weakly-informative priors, which provide scale information about the setting of the model but do not bias effect estimates above or below zero. Specifically, fixed effect coefficients were given Cauchy priors with mean zero and SD 1, random effect standard deviations were given bounded Cauchy priors with mean 0.2 (indicating that we expect some variance between the random effect levels) and SD 1, with a lower-bound of 0 (variances cannot be negative), and correlation matrices were given LKJ(2) priors, reflecting a weak bias against strong correlations (<xref ref-type="bibr" rid="bib79">Stan Development, 2015</xref>). The model posterior was estimated using MCMC implemented in the Stan language (version 2.16.2, <xref ref-type="bibr" rid="bib80">Stan Development Team, 2017</xref>; <xref ref-type="bibr" rid="bib39">Hoffman and Gelman, 2014</xref>), with the model wrapper package brms (version 1.10.2, <xref ref-type="bibr" rid="bib14">Bürkner, 2017</xref>) in the R statistical environment. We computed four chains of 15,000 steps, of which the first 5000 steps were used to tune the sampler; to save disk space we only saved every 5th sample.</p></sec><sec id="s9-2-2"><title>Results and discussion</title><p>The CNN 32 model came close to matching appearance on average for a set of 400 images. Discrimination performance for ten naïve participants and three authors is shown in <xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3</xref> (lines link individual participant means, based on at least 64 trials, median 94). All participants achieve above-chance performance for the simplest model (CNN 4), indicating that they understood and could perform the task. Performance deteriorates as models match the structure of the original image more precisely.</p><fig id="app2fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.019</object-id><label>Appendix 2—figure 3.</label><caption><title>The CNN model comes close to matching appearance on average.</title><p>Oddity performance as a function of the CNN image model. Points show mean over participants (error bars ±2 SEM), coloured lines link the mean performance of each participant for each pooling model. For most participants, performance falls to approximately chance (dashed horizontal line) for the CNN 32 model. Black line and shaded regions show the mean and 95% credible intervals on the population mean derived from a mixed-effects model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app2-fig3-v1.tif"/></fig><p>To quantify the data, we estimated the posterior distribution of a logistic mixed-effects model with a population-level (fixed-effect) factor of CNN model, whose effect was nested within participants and image (i.e. random effects of participant and image). Regression coefficients coded the difference between successive CNN models, expressed using sequential difference coding from the MASS package (<xref ref-type="bibr" rid="bib86">Venables and Ripley, 2002</xref>), and are presented below as the values of the linear predictor (corresponding to log odds in a logistic model). Mean performance had a greater than 0.99 posterior probability of being lower for CNN 8 than CNN 4 (-0.48, 95% CI [−0.74,–0.23], <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0.999</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), and for CNN 16 being lower than CNN 8 (-0.43, 95% CI [−0.68,–0.18], <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>); whereas the difference between the 16 and 32 models was somewhat smaller (−0.17, 95% CI [−0.37, 0.03], <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.951</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Most participants performed close to chance for the CNN 32 model (excluding authors, the population mean estimate had a 0.88 probability of being greater than chance; including authors this value was 0.96). Therefore, the model is capable of synthesising images that are indiscriminable from a large set of arbitrary scenes in our experimental conditions, on average, for naïve participants. However, one participant (author AE) performs noticably better than the others, even for the CNN 32 model. AE had substantial experience with the type of distortions produced by the model but had never seen this set of original images before. Therefore, the images produced by the model are not true metamers, because they do not encapsulate the limits of visible structure for all humans.</p></sec></sec><sec id="s9-3"><title>Experiment 2: Image familiarity and learning tested by repeated presentation</title><p>It is plausible that familiarity with the images played a role in the results above. That is, the finding that images become difficult on average to discriminate with the CNN 32 model may depend in part on participants having never seen the images before. To investigate the role that familiarity with the source images might play, the same participants as in the experiment above performed a second experiment in which five of the original images from the first experiment were presented 60 times, using 15 unique syntheses per image generated with the CNN 32 model (<xref ref-type="fig" rid="app2fig4">Appendix 2—figure 4A</xref>).</p><fig id="app2fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.020</object-id><label>Appendix 2—figure 4.</label><caption><title>Familiarity with original image content did not improve discrimination performance.</title><p>(<bold>A</bold>) Five original images (top) were repeated 60 times (interleaved over 4 blocks), and observers discriminated them from CNN 32 model syntheses (bottom). (<bold>B</bold>) Proportion of correct responses for each image from A. Some images are easier than others, even for the CNN 32 model. (<bold>C</bold>) Performance as a function of each 75-trial session reveals little evidence that performance improves with repeated exposure. Points show grand mean (error bars show bootstrapped 95% confidence intervals), lines link the mean performance of each observer for each pooling model (based on at least 5 trials; median 14). Black line and shaded region shows the posterior mean and 95% credible intervals of a logistic mixed-effects model predicting the population mean performance for each image. Images from the MIT 1003 dataset (<xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>, <ext-link ext-link-type="uri" xlink:href="https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html</ext-link>) and reproduced under a CC-BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">https://creativecommons.org/licenses/by/3.0/</ext-link>) with changes as described in the Materials and methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app2-fig4-v1.tif"/></fig><sec id="s9-3-1"><title>Methods</title><p>Participants</p><p>The same thirteen participants participated as in Experiment 1.</p><p>Stimuli</p><p>We selected five images from the set of 400 and generated 15 new syntheses for each of these images from the CNN 32 model (yielding a stimulus set of 80 images).</p><p>Procedure</p><p>Each pairing of unique image (5) and synthesis (15) was shown in one block of 75 trials (pseudo-random order with the restriction that trials from the same source image could never follow one another). Participants performed four such blocks, yielding 300 trials in total (60 repetitions of each original image).</p><p>Data analysis</p><p>We discarded trials for which participants made no response (N = 63) or broke fixation (N = 294), leaving a total of 3543 trials for further analysis. Model fitting was as for Experiment 1 above, except that the final posterior was based on four chains of 16,000 steps, of which the first 8000 steps were used to tune the sampler; to save disk space we only saved every 4th sample.</p><p>The intercept-only model (assuming only random effects variation but no learning) was specified as<code xml:space="preserve">correct - 1 + (1 | subj) + (1 | im_name)</code></p><p>and the learning model was specified as<code xml:space="preserve">correct - session + (session | subj) + (session | im_name)</code></p><p>We compare models using an information criterion (LOOIC, <xref ref-type="bibr" rid="bib85">Vehtari et al., 2016</xref>; see also <xref ref-type="bibr" rid="bib36">Gelman et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">McElreath, 2016</xref>) that estimates of out-of-sample prediction error on the deviance scale.</p></sec><sec id="s9-3-2"><title>Results and discussion</title><p>While some images (e.g. House) could be discriminated quite well by most participants (<xref ref-type="fig" rid="app2fig4">Appendix 2—figure 4B</xref>), others (e.g. Graffiti) were almost indiscriminable from the model image for all participants (posterior probability that the population mean was above chance performance was 0.61 for Graffiti, 0.93 for Market, and greater than 0.99 for all other images). This image dependence shows that even the CNN 32 model is insufficient to produce metamers for arbitrary scenes.</p><p>Furthermore, there was little evidence that participants learned over the course of sessions (<xref ref-type="fig" rid="app2fig4">Appendix 2—figure 4C</xref>). The population-level linear slope of session number was 0.02, 95% CI [−0.1, 0.15], <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.326</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the LOOIC comparison between the intercept-only model and the model containing a learning term indicated equivocal evidence if random-effects variance was included (LOOIC difference 3.3 in favour of the learning model, SE = 6.1) but strongly favoured the intercept model if only fixed-effects were considered (LOOIC difference −23.3 in favour of the intercept model, SE = 1.7). The two images with the most evidence for learning were Children (median slope 0.04, 95% CI [−0.08, 0.17], <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.247</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and Sailboat (0.04, 95% CI [−0.08, 0.17], <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.269</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Two authors showed some evidence of learning: AE (0.17, 95% CI [−0.03, 0.37], <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.047</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), and CF (0.22, 95% CI [0.03, 0.44], <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.008</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Overall, these results show that repeated image exposures with response feedback did not noticably improve performance.</p></sec></sec><sec id="s9-4"><title>Experiment 3: Spatial cueing of attention</title><p>The experiment presented in the primary paper showed that the discriminability of model syntheses depended on the source images, with scene-like images being easier to discriminate from model syntheses than texture-like images for a given image model. This finding was replicated in an ABX paradigm (above) and the general finding of source-image-dependence was corroborated by the data with repeated images (<xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3</xref>). One possible reason for this image-dependence could be that participants found it easier to know where to attend in some images than in others, creating an image-dependence not due to the summary statistics per se. Relatedly, <xref ref-type="bibr" rid="bib18">Cohen et al. (2016)</xref> suggest that the limits imposed by an ensemble statistic representation can be mitigated by the deployment of spatial attention to areas of interest. Can the discriminability of images generated by our model be influenced by focused spatial attention?</p><p>To probe this possibility we cued participants to a spatial region of the scene before the trial commenced. We computed the mean squared error (MSE) between the original and synthesised images within 12 partially-overlapping wedge-like regions subtending 60°. We computed MSE in both the pixel space (representing the physical difference between the two images) and in the feature space of the fifth convolutional layer (conv5_1) of the VGG-19 network, with the hypothesis that this might represent more perceptually relevant information, and thus be a more informative cue.</p><p>We pre-registered the following hypotheses for this experiment (available at <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.17605/OSF.IO/MBGSQ">http://dx.doi.org/10.17605/OSF.IO/MBGSQ</ext-link>; click on ‘View Registration Form’). For the overall effect of cueing (the primary outcome of interest), we hypothesised that</p><list list-type="bullet"><list-item><p>performance in the Valid:Conv5 condition would be higher than the Uncued condition and</p></list-item><list-item><p>performance in the Invalid condition would be lower than the Uncued condition</p></list-item></list><p>These findings would be consistent with the account that spatial attention can be used to overcome ensemble statistics in the periphery, providing that it is directed to an informative location. This outcome also assumes that our positive cues (Conv5 and Pixels) identify informative locations.</p><p>Alternative possibilities are</p><list list-type="bullet"><list-item><p>if focussed spatial attention cannot influence the ‘resolution’ of the periphery in this task, then performance in the Valid:Conv5 and Invalid conditions will be equal to the Uncued condition.</p></list-item><list-item><p>if observers use a global signal (‘gist’) to perform the task, performance in the Uncued condition would be higher than the Valid:Conv5 and Invalid conditions. That is, directing spatial attention interferes with a gist cue.</p></list-item></list><p>Our secondary hypothesis concerns the difference between Valid:Conv5 and Valid:Pixel cues. A previous analysis at the image level (see below) found that conv5 predicted image difficultly slightly better than the pixel space. We therefore predicted that Valid spatial cues based on Conv5 features (Valid:Conv5) should be more effective cues, evoking higher performance, than Valid:Pixel cues.</p><sec id="s9-4-1"><title>Methods</title><p>Participants</p><p>We pre-registered (<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.17605/OSF.IO/MBGSQ">http://dx.doi.org/10.17605/OSF.IO/MBGSQ</ext-link>) the following data collection plan with a stopping rule that depended on the precision (<xref ref-type="bibr" rid="bib48">Kruschke, 2015</xref>). Specifically, we collected data from a minimum of 10 and a maximum of 30 participants, planning to stop in the intermediate range if the 95% credible intervals for the two parameters of interest (population fixed-effect difference between Valid and Uncued, and population fixed-effect difference between Invalid and Uncued) spanned a width of 0.3 or less on the linear predictor scale.</p><p>This value was determined as 75% of the width of our ‘Region of Practical Equivalence’ to zero effect (ROPE), pre-registered as [−0.2, 0.2] on the linear predictor scale (this corresponds to odds ratios of [0.82, 1.22]). We deemed any difference smaller than this value as being too small to be practically important.</p><p>As an example, if the performance in one condition is 0.5, then an increase of 0.2 in the linear predictor corresponds to a performance of 0.55. The target for precision was then determined as 75% of the ROPE width, in order to give a reasonable chance for the estimate to lie within the ROPE (<xref ref-type="bibr" rid="bib48">Kruschke, 2015</xref>).</p><p>We tested these conditions by fitting the data model (see below) after every participant after the 10th, stopping if the above conditions were met. However, as shown in <xref ref-type="fig" rid="app2fig5">Appendix 2—figure 5</xref>, this precision was not met with our maximum of 30 participants, and so we ceased data collection at 30, deeming further data collection beyond our resources for the experiment. Thus our data should be interpreted with the caveat that the desired precision was not reached (though we got close).</p><fig id="app2fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.021</object-id><label>Appendix 2—figure 5.</label><caption><title>Parameter precision as a function of number of participants.</title><p>(<bold>A</bold>) Width of the 95% credible interval on three model parameters as a function of the number of participants tested. Points show model fit runs (the model was not re-estimated after every participant due to computation time required). We aimed to achieve a width of 0.3 (dashed horizontal line) on the linear predictor scale, or stop after 30 participants. The Uncued - Invalid parameter failed to reach the desired precision after 30 participants. Lines show fits of a quadratic polynomial as a visual guide.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app2-fig5-v1.tif"/></fig><p>An additional five participants were recruited but showed insufficient eyetracking accuracy or training performance (criteria pre-registered). Of the 30, three were lab members unfamiliar with the purpose of the study, the other 27 were recruited online; all were paid 15 Euro for the 1.5 hr testing session. Of these, three participants did not complete the full session due to late arrival, and eyetracking calibration failed in the second last trial block for an additional participant.</p><p>Stimuli</p><p>This experiment used the same 400 source images and CNN 8 model syntheses as Experiment 1.</p><p>Procedure</p><p>The procedure for this experiment was as in Experiment 1 with the following exceptions. The same 400 original images were used as in Experiment 1, all with syntheses from the CNN 8 model. A trial began with the presentation of a bright wedge (60 degree angle, Weber contrast 0.25) or circle (radius 2 dva) for 400 ~ ms, indicating a spatial cue (85% of trials) or Uncued trial (15%) respectively (<xref ref-type="fig" rid="app2fig6">Appendix 2—figure 6A</xref>). A blank screen with fixation spot was presented for 800 ms before the oddity paradigm proceeded as above. On spatial cue trials, participants were cued to the wedge region containing either the largest pixel MSE between the original and synthesised images (35% of all trials), the largest conv5 MSE (35%), or the <italic>smallest</italic> pixel MSE (an invalid cue, shown on 15% of all trials). Thus, 70% of all trials were valid cues, encouraging participants to make use of the cues rather than learning to ignore them. Participants were also instructed to attend to the cued region on trials where a wedge was shown. For Uncued trials they were instructed to attend globally over the image. Cueing conditions were interleaved and randomly assigned to each unique image for each participant. The experiment was divided into eight blocks of 50 trials. Before the experiment we introduced participants to the task and fixation control with repeated practice sessions of 30 trials (using 30 images not used in the main experiment and with the CNN 4 model syntheses). Participants saw at least 60 and up to 150 practice trials, until they were able to get at least 50% correct and with 20% or fewer trials containing broken fixations or blinks.</p><fig id="app2fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.022</object-id><label>Appendix 2—figure 6.</label><caption><title>Cueing spatial attention has little effect on performance.</title><p>(<bold>A</bold>) Covert spatial attention was cued to the area of the largest difference between the images (70% of trials; half from conv5 feature MSE; half from pixel MSE) via a wedge stimulus presented before the trial. On 15% of trials the wedge cued an invalid location (smallest pixel MSE), and on 15% of trials no cue was provided (circle stimulus). (<bold>B</bold>) Performance as a function of cueing condition for 30 participants. Points show grand mean (error bars show <inline-formula><mml:math id="inf92"><mml:mrow><mml:mo>±</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> SE), lines link the mean performance of each observer for each pooling model (based on at least 30 trials; median 65). Blue lines and shaded area show the population mean estimate and 95% credible intervals from the mixed-effects model. Triangle in the Uncued condition replots the average performance from CNN 8 in <xref ref-type="fig" rid="fig3">Figure 3</xref> for comparison. Images from the MIT 1003 dataset (<xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>, <ext-link ext-link-type="uri" xlink:href="https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html</ext-link>) and reproduced under a CC-BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">https://creativecommons.org/licenses/by/3.0/</ext-link>) with changes as described in the Materials and methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app2-fig6-v1.tif"/></fig><p>Data analysis</p><p>We discarded trials for which participants made no response (N = 141) or broke fixation (N = 1398), leaving a total of 10261 trials for further analysis.</p><p>This analysis plan was pre-registered and is available at <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.17605/OSF.IO/MBGSQ">http://dx.doi.org/10.17605/OSF.IO/MBGSQ</ext-link> (click on ‘view registration form’). We seek to estimate three performance differences:</p><list list-type="order"><list-item><p>The difference between Invalid and Uncued</p></list-item><list-item><p>The difference between Valid:Conv5 and Uncued</p></list-item><list-item><p>The difference between Valid:Conv5 and Valid:Pixels</p></list-item></list><p>The model formula (in lme4-style formula notation) is<code xml:space="preserve">correct - cue + (cue | subj) + (cue | im_code)</code></p><p>with <monospace>family = Bernoulli(‘logit’)</monospace>. The ‘cue’ factor uses custom contrast coding (design matrix) to test the hypotheses of interest. Specifically, the design matrix for the model above was specified as</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th/><th><inline-formula><mml:math id="inf93"><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf94"><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf95"><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf96"><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula></th></tr></thead><tbody><tr><td>Invalid</td><td>1</td><td>-1</td><td>0</td><td>0</td></tr><tr><td>Uncued</td><td>1</td><td>1</td><td>-1</td><td>0</td></tr><tr><td>Valid:Conv5</td><td>1</td><td>0</td><td>1</td><td>1</td></tr><tr><td>Valid:Pixels</td><td>1</td><td>0</td><td>0</td><td>-1</td></tr></tbody></table></table-wrap><p>Therefore, <inline-formula><mml:math id="inf97"><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> codes Uncued - Invalid, <inline-formula><mml:math id="inf98"><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> codes Valid:Conv5 - Uncued, <inline-formula><mml:math id="inf99"><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> codes Valid:Conv5 - Valid:Pixels and <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> codes the Intercept (average performance). Note that the generalised inverse of this matrix was passed to brms (<xref ref-type="bibr" rid="bib86">Venables and Ripley, 2002</xref>).</p><p>Each of these population fixed-effects is offset by the random effects of participant (<monospace>subj</monospace>) and image (<monospace>im_code</monospace>). We also assume that the offsets for each fixed effect can be correlated (denoted by the single pipe character |). The model thus estimates:</p><list list-type="order"><list-item><p>Four fixed-effect coefficients. The coefficients coding Valid:Conv5 – Uncued and Uncued – Invalid constitute the key outcome measures of the study. The final coefficient is the analysis of secondary interest.</p></list-item><list-item><p>Eight random-effects standard-deviations (four for each fixed-effect, times two for the two random effects).</p></list-item><list-item><p>Twelve correlations (six for each pairwise relationship between the fixed-effects, times two for the two random effects).</p></list-item></list><p>These parameters were given weakly-informative prior distributions as for Experiment 1 (above): fixed-effects had Cauchy(0, 1) priors, random effect SDs had bounded Cauchy(0.2, 1) priors, and correlation matrices had LKJ(2) priors.</p><p>To judge the study outcome we pre-defined a region of practical equivalence (ROPE) around zero effect (0) of [−0.2, 0.2] on the linear predictor scale. This corresponds to odds ratios of [0.82, 1.22]. Our decision rules were then:</p><list list-type="bullet"><list-item><p>If the 95% credible interval of the parameter value falls outside the ROPE, we consider there to be a credible difference between the conditions.</p></list-item><list-item><p>If the 95% credible interval of the parameter value falls fully within the ROPE, we consider there to be no practical difference between the conditions. This does not mean that there is no effect, but only that it is unlikely to be large.</p></list-item><list-item><p>If the 95% credible interval overlaps the ROPE, the data are ambiguous as to the conclusion for our hypothesis. This does not mean that the data give no insight into the direction and magnitude of any effect, but only that they are ambiguous with respect to our decision criteria.</p></list-item></list><p>For more discussion of this approach to hypothesis testing, see (<xref ref-type="bibr" rid="bib48">Kruschke, 2015</xref>).</p></sec><sec id="s9-4-2"><title>Results and discussion</title><p>The results of this experiment are shown in <xref ref-type="fig" rid="app2fig6">Appendix 2—figure 6B</xref>. While mean performance across conditions was in the expected direction for all effects, no large differences were observed. Specifically, the population-level coefficient estimate on the linear predictor scale for the difference between the Valid:Conv5 cueing condition and the uncued condition was 0.09, 95% CI [−0.05, 0.22], <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Given our decision rules above, the coefficient does not fall wholely within the ROPE and therefore this result is somewhat inconclusive; in general the difference is rather small and so large ‘true’ effects of spatial cueing are quite unlikely. Similarly, we find no large difference between uncued performance and the invalid cues (0.09, 95% CI [−0.07, 0.25], <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.141</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Based on our pre-registered cutoff for a meaningful effect size we conclude that cueing spatial attention in this paradigm results in effectively no performance change.</p><p>We further hypothesised that the conv5 cue would be more informative (resulting in a larger performance improvement) than the pixel MSE cue. Note that for 269 of 400 images the conv5 and pixel MSE cued the same or neighbouring wedges, meaning that the power of this experiment to detect differences between these conditions is limited. Consistent with this and contrary to our hypothesis, we find no practical difference between the Valid:Conv5 and Valid:Pixels conditions, 0.04, 95% CI [−0.07, 0.14], <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.253</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note that for this comparison, the 95% credible intervals for the parameter fall entirely within the ROPE, leading us to conclude that there is no practical difference between these conditions in our experiment.</p><p>To conclude, our results here suggest that if cueing spatial attention improves the ‘resolution’ of the periphery, then the effect is very small. <xref ref-type="bibr" rid="bib18">Cohen et al. (2016)</xref> have suggested that an ensemble representation serves to create phenomenal experience of a rich visual world, and that spatial attention can be used to gain more information about the environment beyond simple summary statistics. The results here are contrary to this idea, at least for the specific task and setting we measure here.</p><p>Note however that other experimental paradigms may in general be more suitable for assessing the influence of spatial attention than a temporal oddity paradigm. For example, in temporal oddity participants may choose to reallocate spatial attention after the first interval is presented (e.g. on invalid trials pointing at regions of sky). In this respect a single-interval yes-no design (indicating original/synthesis) might be preferable. However, analysis of such data with standard signal detection theory would need to assume that the participants’ decision criteria remain constant over trials, whereas it seems likely that decision criteria would depend strongly on the image. To remain consistent with our earlier experiments we nevertheless employed a three-alternative temporal oddity task here; future work could assess whether our finding of minimal influence of spatial cueing depends on this choice.</p></sec></sec><sec id="s9-5"><title>Selection of scene- and texture-like images</title><p>As discussed in the main paper, we used the results of a pilot experiment (Experiment 3, above) to help select images to provide a strong test of the FS-model. Briefly, 30 observers discriminated 400 images from syntheses produced by the CNN 8 model. Each image was paired with only one unique synthesis (see Experiment 3 above for further details on the experiment).</p><p>In an exploratory analysis of that data, we found that there was a large range of difficulty for individual images (as in Experiment 2, above). <xref ref-type="fig" rid="app2fig7">Appendix 2—figure 7</xref> shows the image-specific intercepts estimated by the model described above. We examine this rather than the raw data because cueing conditions were randomly assigned to each image for each subject, meaning that the mean performance of the images will depend on this randomisation (though, given our results, the effects are likely to be small). The image-specific intercept from the model estimates the difficulty of each image, statistically marginalising over cueing condition. While the posterior means for some images were close to chance, and the 95% credible intervals associated with about 100 images overlapped chance performance, approximately 30 images were easily discriminable from their model syntheses, lying above the mean performance for all images with the CNN 8 model.</p><fig id="app2fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.024</object-id><label>Appendix 2—figure 7.</label><caption><title>Estimated difficulty of each image in Experiment 3 (syntheses with the CNN 8 model) and the images chosen to form the texture- and scene-like categories in the main experiment.</title><p>Solid black line links model estimates of each image’s difficulty (the posterior mean of the image-specific model intercept, plotted on the performance scale). Shaded region shows 95% credible intervals. Dashed horizontal line shows chance performance; solid blue horizontal line shows mean performance. Red and blue points denote the images chosen as texture- and scene-like images in the main experiment respectively. The red point near the middle of the range is the ‘graffiti’ image from the experiments above.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app2-fig7-v1.tif"/></fig><fig id="app2fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.025</object-id><label>Appendix 2—figure 8.</label><caption><title>The 50 easiest images from <xref ref-type="fig" rid="app2fig7">Appendix 2—figure 7</xref> where difficulty increases left-to-right, top-to-bottom.</title><p>Images chosen for the main experiment as ‘scene-like’ are circled in blue. Images from the MIT 1003 dataset (<xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>, <ext-link ext-link-type="uri" xlink:href="https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html</ext-link>) and reproduced under a CC-BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">https://creativecommons.org/licenses/by/3.0/</ext-link>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app2-fig8-v1.tif"/></fig><p>Our final selection of ten images per category was made by examining the easiest and hardest images from this experiment (<xref ref-type="fig" rid="app2fig7">Appendix 2—figure 7</xref>) and selecting ten images we subjectively judged to contain scene-like or texture-like content. The final images used in the first experiment of the main paper are shown in <xref ref-type="fig" rid="app2fig7">Appendix 2—figure 7</xref> as coloured points. The 50 easiest and 50 hardest images are shown in <xref ref-type="fig" rid="app2fig8">Appendix 2—figures 8</xref>,<xref ref-type="fig" rid="app2fig9">9</xref> respectively.</p><fig id="app2fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.026</object-id><label>Appendix 2—figure 9.</label><caption><title>The 50 hardest images from <xref ref-type="fig" rid="app2fig7">Appendix 2—figure 7</xref> where difficulty decreases left-to-right, top-to-bottom.</title><p>Images chosen for the main experiment as ‘texture-like’ are circled in red. Images from the MIT 1003 dataset (<xref ref-type="bibr" rid="bib42">Judd et al., 2009</xref>, <ext-link ext-link-type="uri" xlink:href="https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html</ext-link>) and reproduced under a CC-BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">https://creativecommons.org/licenses/by/3.0/</ext-link>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app2-fig9-v1.tif"/></fig></sec><sec id="s9-6"><title>Predicting the difficulty of individual images</title><p>As shown above, some images are easier than others. We assessed whether an image-based metric considering the difference between original and synthesised images could predict difficulty at the image level. Specifically, we asked whether the mean squared-error (MSE) between the original and synthesised images in two feature spaces (conv5 and pixels) could predict the relative difficulty of the source images. Note that we performed this analysis first on the results of Experiment 1 (<xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3</xref>), and that these results were used to inform the hypothesis regarding the usefulness of conv5 vs pixel cues presented in Experiment 3, above. We subsequently performed the same analysis on the data from Experiment 3. We present both analyses concurrently here for ease of reading, but the reader should be aware of the chronological order.</p><sec id="s9-6-1"><title>Methods</title><p>We computed the mean squared error between the original and synthesised images in two feature spaces. First, the MSE in the pixel space was used to represent the physical difference at all spatial scales. Second, the difference in feature activations in the conv5 layer of the VGG network was used as an abstracted feature space which may correspond to aspects of human perception (e.g. <xref ref-type="bibr" rid="bib49">Kubilius et al., 2016</xref>, see also <xref ref-type="bibr" rid="bib35">Geirhos et al., 2019</xref>). Both are also correlated with the final value of the loss function from our synthesis procedure. As a baseline we fit a mixed-effects logistic regression containing fixed-effects for the levels of the CNN model and a random effect of observer on all fixed effect terms. As a ‘saturated’ model (a weak upper bound) we added a random effect for image to the baseline model (that is, each image is uniquely predicted given the available data). Using the scale defined by the baseline and saturated models, we then compared models in which the image-level predictor (pixel or conv5 MSE, standardised to have zero mean and unit variance within each CNN model level) was added as an additional linear covariate to the baseline model. That is, each image was associated with a scalar value of pixel/conv5 MSE with each synthesis. Additional image-level predictors were compared but are not reported here because they performed similarly or worse than the conv5 or pixel MSE.</p><p>As above, we compared the models using the LOOIC information criterion that estimates out-of-sample prediction error on the deviance scale. Qualitatively similar results were found using ten-fold crossvalidation for models fit with penalised maximum-likelihood in lme4.</p></sec><sec id="s9-6-2"><title>Results</title><p>For the dataset from Experiment 1, the LOOIC favoured the model containing conv5 MSE over the pixel MSE (LOOIC difference 18.2, SE = 8.3) and the pixel MSE over the baseline model (LOOIC difference 25.3, SE = 10.9)—see <xref ref-type="fig" rid="app2fig10">Appendix 2—figure 10A</xref>. The regression weight of the standardised pixel MSE feature fit to all the data was 0.04 (95% credible interval = 0.15–0.07), and the weight of the standardised conv5 feature was 0.04 (0.2–0.11; presented as odds ratios in <xref ref-type="fig" rid="app2fig10">Appendix 2—figure 10C</xref>). Therefore, a one standard deviation increase in the conv5 feature produced a slightly larger increase in the linear predictor (and thus the expected probability) than the pixel MSE, in agreement with the model comparison.</p><fig id="app2fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.42512.027</object-id><label>Appendix 2—figure 10.</label><caption><title>Predicting image difficulty using image-based metrics.</title><p>(<bold>A</bold>) Expected prediction improvement over a baseline model for models fit to the data from Experiment 1 (<xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3</xref>), as estimated by the LOOIC (<xref ref-type="bibr" rid="bib85">Vehtari et al., 2016</xref>). Values in deviance units (−2 * log likelihood; higher is better). Error bars show ±2 1 SE. Percentages are expected prediction improvement relative to the saturated model. (<bold>B</bold>) Same as A but for the data from Experiment 3 (<xref ref-type="fig" rid="app2fig6">Appendix 2—figure 6</xref>). (<bold>C</bold>) Odds of a success for a one SD increase in the image predictor for data from Experiment 1. Points show mean and 95% credible intervals on odds ratio (exponentiated logistic regression weight). (<bold>D</bold>) As for C for Experiment 3.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-42512-app2-fig10-v1.tif"/></fig><p>Applying this analysis to the data from Experiment 3 lead to similar results (<xref ref-type="fig" rid="app2fig10">Appendix 2—figure 10B,D</xref>). The LOOIC favoured the model containing conv5 MSE over the pixel MSE (LOOIC difference 49.9, SE = 13.3) and the pixel MSE over the baseline model (LOOIC difference 62.4, SE = 16.2). Note that the worse performance of the image metric models relative to the saturated model (compared to <xref ref-type="fig" rid="app2fig10">Appendix 2—figure 10A</xref>) is because the larger data mass in this experiment provides a better constraint for the random effects estimates of image. The regression weight of the standardised pixel MSE feature fit to all the data was 0.03 (95% credible interval = 0.14–0.08), and the weight of the standardised conv5 feature was 0.03 (0.21–0.15).</p><p>These results show that the difficulty of a given image can be to some extent predicted from the pixel differences or conv5 differences, suggesting these might prove useful full-reference metrics, at least with respect to the distortions produced by our CNN model.</p></sec></sec></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.42512.030</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Herzog</surname><given-names>Michael</given-names></name><role>Reviewing Editor</role><aff><institution>EPFL</institution><country>Switzerland</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Herzog</surname><given-names>Michael</given-names> </name><role>Reviewer</role><aff><institution>EPFL</institution><country>Switzerland</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Caas</surname><given-names>John</given-names> </name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Image content is more important than Bouma's Law for scene metamers&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Michael Herzog as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Timothy Behrens as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: John Caas (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>The authors show that summary statistics models cannot easily explain crowding. In previous studies, only &quot;metameric&quot; images were compared with each other by human observers. In the present study, the original image was compared with two images created by either the FS-model or a deep network. For all scale factors tested, observers could well tell the original from the other images arguing against summary statistic approaches.</p><p>The topic is timely, the experiment was conducted in an expert fashion, and the presentation is clear and well structured. However, there are aspects which need to be taken care before a final decision can be made.</p><p>Essential revisions:</p><p>The failure of FS images to metameric to regular scenes could be a function of many things: The specific size of the pooling regions as a function of eccentricity, the statistics that are measured inside pooling regions, or the contribution of mechanisms for perceptual organization that are not explicitly modeled in the FS approach. The demonstration that metamerism breaks down is important, but the current study does not allow us to clearly distinguish between these various alternatives. Please consider discussing these various accounts of the data in more depth, especially with regard to how we might differentiate between these explanations.</p><p>The authors note, &quot;It is the image content, not retinal eccentricity, that is the primary determinant of the visibility of at least some summary statistic distortions.&quot; The reviewers agree that global processes may well contribute to the higher spatial resolution observed in this study in response to the real-world images. Exactly what these might be is not specified, nor is the possibility that local interactions may also be at play. Two papers (Robol, Casco and Dakin, 2012; and Van der Burg, Cass and Olivers, 2017) showing that local orthogonal structure is capable of driving perceptual segmentation speak to this. One of the reviewers wonders whether such local interactions might at least partially explain performance differences between the different classes of stimuli (real vs. synthetic; texture-like vs. scene-like)?</p><p>Could the authors provide a more detailed description of the criteria they used to categorise the real-world images as either texture-like or scene-like? How and why might metametric sensitivity depend upon this classification criterion?</p><p><italic>Reviewer #1:</italic> </p><p>1) The TTM model was used by Freeman and Simoncelli to show that different textures generated from the same image are metamers (i.e., cannot be differentiated by subjects).</p><p>2) Wallis et al. correctly claim: if the summary statistics in this model truly mimic the statistics extracted by the brain, subjects should not be able to differentiate texturized images from the original image.</p><p>3) However, they show that this is not the case with natural images: subjects can well discriminate between the original and associated textures (using both Simoncelli statistics and statistics from CNN simulations).</p><p>4) They claim that the global visual scene must be taken into account and perceptual organization is important. Compressibility by summary statistics depends on image content.</p><p>5) They link their results to the debate about the richness of experience. Cohen, Dennett and Kanwisher proposed summary statistics to explain the richness in the periphery. Here, they discuss that summary statistics in fact doesn't seem to be able to do the job.</p><p>Although it is not the most groundbreaking experiment ever, I still quite like this paper because:</p><p>1) It is good to highlight that texturized images are in fact <italic>not</italic> metamers of the original images.</p><p>2) The discussion is interesting bringing together crowding and the debate on using summary statistics to explain the richness of perception in the periphery.</p><p><italic>Reviewer #2:</italic> </p><p>The authors describe a single experiment designed to test the hypothesis that natural scenes will be metameric to synthetic scenes created using a model that implements a summary-statistic model of texture-like pooling of visual information in peripheral vision (what the authors call the Freeman-Simoncelli or FS model). Critically, the model allows the scale of pooling regions to increase at different rates as a function of eccentricity, making it possible to examine the specific hypothesis that Bouma's Law (which predicts a scale factor of 0.5) governs pooling across the visual field. The authors rightly point out that the key comparison between natural scenes and synthetic scenes has not been done, and the key contribution here is to do just that. Also, the authors examine performance for scenes that they classify as &quot;texture-like&quot; and &quot;scene-like,&quot; which should be equally subject to the metameric treatment if the strong FS-model hypothesis is correct. Briefly, they find that scene type does change the critical scale at which participants can reliably tell natural scenes from synthetic ones, and that the critical scale for scene-like images appears to be a good bit lower than the 0.5 value predicted by Bouma's Law.</p><p>The task the authors use is appropriate (a 3AFC oddball judgment applied to images presented in sequence), and I don't have any concerns about the implementation of the FS model. I don't love the fact that the entire stimulus set was comprised of only 20 images (10 per scene type), however. I understand the practical constraints the authors mention regarding the long rendering time for synthetic images, but there's a real concern here: Do these results generalize? The authors emphasize that if we find any image that can be discriminated with a scale factor that's lower than 0.5, then that determines the minimal scale of pooling. If we accept this logic, we have no need to worry about generalizability because all we're looking for is the existence of any image that fits the bill. I think I'd feel better about accepting this logic if the authors' task didn't involve distinguishing two physically identical images from an oddball. While an ideal version of the FS-model would (I think) predict that there should be no measurements at scales coarser than the critical scale that would support discrimination of these images, practically, the model one actually runs is not ideal, and thus the images it produces might not completely meet this standard. To be clear, I'm not saying I disagree with the authors' main conclusions, but I do think it's worth qualifying some of the stronger statements about what you can conclude from 10 images per condition produced by a model that doesn't have clear convergence properties.</p><p>More broadly, while I think the discrimination task is a good place to start, I also thought that extending the task beyond the identical-foils version of the design could provide some useful context. For example, suppose instead of 3AFC oddball detection, the authors included a version of the task that was a 2AFC real/synthetic judgment? This obviously is no longer a test for metamerism, but whether or not individuals can reliably tell if a synthesized image looks strange is an important indicator of whether or not they can solve your task without actually trying to discriminate the images (are there two strange images and a non-strange one?). Really what I'd like is a more thorough discussion of how observers might try to accomplish the task when distractors are physically identical, and what that might imply about the strength of the conclusions the authors can draw regarding spatial pooling in the model.</p><p>Finally, I like the authors discussion of gestalt properties a great deal, but there's also a competing argument that I think is hard to dismiss: What if the critical scale of 0.5 really is sufficient to make metamers out of natural scenes, but the set of statistics being computed within those regions in the FS-model is inadequate? This is a &quot;God-of-the-gaps&quot; argument to be sure, but again, the issue is that we can't hold a specific implementation of a model to the standard of an ideal. I tend to agree with the authors that there are probably grouping and segmentation processes (or global processes) that contribute to perceptual appearance, but it's an open question whether some of these properties might fall out of a model that incorporated different summary statistics that more closely approximate our own perceptual codes in peripheral vision. Again, this is a place where what I'm looking for is some more nuance – perceptual organization is an interesting thing to think about, but refining our ideas about what texture statistics might be computed in the periphery is also interesting to pursue.</p><p><italic>Reviewer #3:</italic> </p><p>This paper uses a 3AFC peripheral distortion detection task to measure the human visual system's spatial resolution for images of real world scenes and synthesised versions of these images. Results indicate that the critical scale with which participants were able to reliably detect distortions was significantly smaller for real world images. A further comparison of the results pertaining to ten &quot;scene-like&quot; and ten &quot;texture-like&quot; images was conducted. Results of this analysis demonstrate that distortions in &quot;scene-like&quot; images (&quot;those containing inhomogeneous structures&quot;) could be made at significantly finer scales than are distortions in &quot;texture-like&quot; images (&quot;those containing more homogenous or periodically patterned content&quot;).</p><p>The authors explain that the critical spatial scale implied by performance regarding the synthesised images is broadly consistent with Freeman and Simoncelli, (2011) V2-like texture pooling model of peripheral visual performance. Given the higher spatial resolution performance implied by performance with the real-world images, and most strikingly, the scene-like images, a convincing argument is mounted that performance in these conditions must be based on either additional information, or at least finer grained information, not available in the FS synthesised images.</p><p>These results contribute to the burgeoning literature demonstrating that the deleterious effects of visual clutter can – under certain image-based circumstances – be much smaller than is predicted by Bouma-like pooling. As the authors note, &quot;It is the image content, not retinal eccentricity, that is the primary determinant of the visibility of at least some summary statistic distortions.&quot;</p><p>This begs the question of 'what types of image content enable the relative high spatial resolution shown here and elsewhere?'. The authors cite some studies relevant to this issue (Saarela et al., 2009; Manassi et al., 2013; Vickery et al., 2009; Herzog et al., 2015), and suggest that &quot;early global segmentation processes influence local perceptual sensitivity&quot; and that &quot;global scene organisation needs to be considered if one wants to capture appearance-yet current models that texturise local regions do not explicitly include perceptual organisation (Herzog et al., 2015).&quot;</p><p>Whilst I agree that global processes may well contribute to the higher spatial resolution observed in this study in response to the real-world images, this overlooks the recent finding by Van der Burg, Olivers and Cass (2017) showing that local geometric interactions can profoundly improve peripheral performance (i.e. to break crowding) in densely cluttered heterogenous displays. Specifically, they find that the presence of local T-junction-like orthogonal structure reduces the effects of deleterious pooling to almost zero. I would suggest that these or similar high-order local interactions may be at play here. That result and the possibility that local interactions might play a role here should at least be proposed. Given the (somewhat vaguely defined) criteria used by the authors to categorise the real-world images as either texture-like or scene-like, I wonder what specific information the authors believe the visual system might use extract peripheral real world image content.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.42512.031</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The failure of FS images to metameric to regular scenes could be a function of many things: The specific size of the pooling regions as a function of eccentricity, the statistics that are measured inside pooling regions, or the contribution of mechanisms for perceptual organization that are not explicitly modeled in the FS approach. The demonstration that metamerism breaks down is important, but the current study does not allow us to clearly distinguish between these various alternatives. Please consider discussing these various accounts of the data in more depth, especially with regard to how we might differentiate between these explanations.</p></disp-quote><p>It is correct that our results as presented do not distinguish between possible causes for the failure to match scene appearance of either texture pooling model tested here. We have reworded and expanded our Discussion section to more clearly delineate alternative accounts. Our results do not rule out that some configuration of statistics, pooling or optimisation may exist that succeeds in matching appearance. In fact, we do not see how to falsify this possibility, beyond pointing out that a model using fixed pooling and fixed texture features will not be the most parsimonious description for image content (Discussion section).</p><p>To discuss how different explanations might be differentiated, we suggest that future experiments could seek to disrupt or reduce the activity of mechanisms of perceptual organisation using image manipulations such as polarity inversion or image two-toning (subsection “Summary statistics, performance and phenomenology”).</p><disp-quote content-type="editor-comment"><p>The authors note, &quot;It is the image content, not retinal eccentricity, that is the primary determinant of the visibility of at least some summary statistic distortions.&quot; The reviewers agree that global processes may well contribute to the higher spatial resolution observed in this study in response to the real-world images. Exactly what these might be is not specified, nor is the possibility that local interactions may also be at play. Two papers (Robol, Casco and Dakin, 2012; and Van der Burg, Cass and Olivers, 2017) showing that local orthogonal structure is capable of driving perceptual segmentation speak to this. One of the reviewers wonders whether such local interactions might at least partially explain performance differences between the different classes of stimuli (real vs. synthetic; texture-like vs. scene-like)?</p></disp-quote><p>We thank the reviewers for pointing us to this relevant literature and highlighting the important distinction between local and global mechanisms that could contribute to our results. We now discuss these papers and the possibility that segmentation and grouping may be critically determined by local interactions in a new subsection “Local vs. global mechanisms”. In addition, we have conducted an analysis of our stimuli in which we used a junction detection algorithm to label different junctions in our images (e.g. L-junctions and T-junctions). We find that junction information is negatively correlated with critical scale, as might be predicted from the local interaction account suggested by reviewer 3 (see Appendix 1—figure 3). However, we leave this analysis as a pilot for future work, which would need to test this in a larger and more controlled image set, as well as testing whether this is specific to junctions per se, or rather (e.g.) to edge density more generally. We thank the reviewers for pointing out this interesting avenue for future work.</p><disp-quote content-type="editor-comment"><p>Could the authors provide a more detailed description of the criteria they used to categorise the real-world images as either texture-like or scene-like? How and why might metametric sensitivity depend upon this classification criterion?</p></disp-quote><p>We have expanded our description of this assignment in both the Results section and Appendix 2—figures 7, 8 and 9. Briefly, we used our subjective judgment of image content combined with the results from a pilot experiment using the CNN 8 model to select images with “scene-like” and “texture-like” content that were easy and hard (respectively) for the CNN 8 model to match. This purposeful selection of images provides a strong test of the FS-model: if the same images were relatively easy and hard for both the FS-model and the CNN-model to match, this would provide evidence that certain image structure is problematic for summary statistic models.</p><p>Additional major change:</p><p>All reviewers wonder what specifically are the image features that cause the distinction we see between the image classes. We wished to test the role of scene-like and texture-like features more specifically: since these are really properties of local regions of images (and the scale of content therein) rather than natural image photographs as a whole, our classification into these categories at the image level is rather crude.</p><p>We therefore included a second experiment (previously reported in the Appendix only) in to the main manuscript (see new Figure 3). We added local, circular texture-like distortions into image regions chosen to be “scene-like” or “texture-like” by author CF, not based on any pilot discrimination results. A control experiment showed that naïve participants showed reasonable agreement with CF’s classification (mean agreement 88.6%; see subsection “Stimuli”). We find that texture-like distortions added to scene-like regions are highly visible, whereas the same types of distortions in texture regions are very difficult to detect (see new Figure 3). This result confirms the importance of “scene-like” vs. “texture-like” structure.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>1) The TTM model was used by Freeman and Simoncelli to show that different textures generated from the same image are metamers (i.e., cannot be differentiated by subjects).</p><p>2) Wallis et al. correctly claim: if the summary statistics in this model truly mimic the statistics extracted by the brain, subjects should not be able to differentiate texturized images from the original image.</p><p>3) However, they show that this is not the case with natural images: subjects can well discriminate between the original and associated textures (using both Simoncelli statistics and statistics from CNN simulations).</p><p>4) They claim that the global visual scene must be taken into account and perceptual organization is important. Compressibility by summary statistics depends on image content.</p><p>5) They link their results to the debate about the richness of experience. Cohen, Dennett and Kanwisher proposed summary statistics to explain the richness in the periphery. Here, they discuss that summary statistics in fact doesn't seem to be able to do the job.</p><p>Although it is not the most groundbreaking experiment ever, I still quite like this paper because:</p><p> <italic>1) It is good to highlight that texturized images are in fact</italic> not <italic>metamers of the original images.</italic> </p><p>2) The discussion is interesting bringing together crowding and the debate on using summary statistics to explain the richness of perception in the periphery.</p></disp-quote><p>We thank the reviewer for their positive comments and their time in assessing the paper.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The authors describe a single experiment designed to test the hypothesis that natural scenes will be metameric to synthetic scenes created using a model that implements a summary-statistic model of texture-like pooling of visual information in peripheral vision (what the authors call the Freeman-Simoncelli or FS model). Critically, the model allows the scale of pooling regions to increase at different rates as a function of eccentricity, making it possible to examine the specific hypothesis that Bouma's Law (which predicts a scale factor of 0.5) governs pooling across the visual field. The authors rightly point out that the key comparison between natural scenes and synthetic scenes has not been done, and the key contribution here is to do just that. Also, the authors examine performance for scenes that they classify as &quot;texture-like&quot; and &quot;scene-like,&quot; which should be equally subject to the metameric treatment if the strong FS-model hypothesis is correct. Briefly, they find that scene type does change the critical scale at which participants can reliably tell natural scenes from synthetic ones, and that the critical scale for scene-like images appears to be a good bit lower than the 0.5 value predicted by Bouma's Law.</p><p>The task the authors use is appropriate (a 3AFC oddball judgment applied to images presented in sequence), and I don't have any concerns about the implementation of the FS model. I don't love the fact that the entire stimulus set was comprised of only 20 images (10 per scene type), however. I understand the practical constraints the authors mention regarding the long rendering time for synthetic images, but there's a real concern here: Do these results generalize?</p></disp-quote><p>We agree that it would be nice to test generalisability, but as the reviewer notes, the computation time to generate a number of scale factors for a large number of images is infeasible. However, as we mention in the paper (Discussion section), a recent preprint provides some evidence that at least part of this result is not limited to only our 20 images. Deza et al. (2017) reported percent correct performance in an ABX task for FS syntheses generated at scale factors of 0.5 for 50 original images. For all six of their naive observers, performance in discriminating original and synthesised images lay above chance (50%), and for four of six observers performance lay above 70%. This result corroborates our finding that FS syntheses can be discriminated from original images at scale factors of 0.5, but does not allow the estimation of critical scales as we do here.</p><disp-quote content-type="editor-comment"><p>The authors emphasize that if we find any image that can be discriminated with a scale factor that's lower than 0.5, then that determines the minimal scale of pooling. If we accept this logic, we have no need to worry about generalizability because all we're looking for is the existence of any image that fits the bill. I think I'd feel better about accepting this logic if the authors' task didn't involve distinguishing two physically identical images from an oddball. While an ideal version of the FS-model would (I think) predict that there should be no measurements at scales coarser than the critical scale that would support discrimination of these images, practically, the model one actually runs is not ideal, and thus the images it produces might not completely meet this standard.</p></disp-quote><p>The reviewer is correct that there is a potential discrepancy between idealised predictions and models in practice. The idealised FS logic would predict that if a hypothetical image had a critical scale of 0.3, then performance should remain at chance for any lower scale factors (e.g. 0.15), but the images should be discriminable for larger (coarser) scales. It is possible that there are implementational details in the models that might cause artifacts that violate these assumptions (for example, if the image synthesis produces images with stronger artifacts at low scale factors). We have run control experiments (Appendix 1—figure 1 and Appendix 1—figure 2) ruling out two possible artifacts as being sole explanations for our result. We agree with the general point that an idealised model can never be tested. Our experiments cannot rule out the possibility that a better implementation of the model may succeed in creating metamers, but we speculate that more will be required. We now try to be clearer on this point in the manuscript (see below).</p><disp-quote content-type="editor-comment"><p>To be clear, I'm not saying I disagree with the authors' main conclusions, but I do think it's worth qualifying some of the stronger statements about what you can conclude from 10 images per condition produced by a model that doesn't have clear convergence properties.</p></disp-quote><p>In response to this comment and the reviewer’s call for more nuance (below), we have been more explicit that our result cannot rule out the existence of some pooling model that could match appearance for all images at V2 scaling (Discussion section).</p><disp-quote content-type="editor-comment"><p>More broadly, while I think the discrimination task is a good place to start, I also thought that extending the task beyond the identical-foils version of the design could provide some useful context. For example, suppose instead of 3AFC oddball detection, the authors included a version of the task that was a 2AFC real/synthetic judgment? This obviously is no longer a test for metamerism, but whether or not individuals can reliably tell if a synthesized image looks strange is an important indicator of whether or not they can solve your task without actually trying to discriminate the images (are there two strange images and a non-strange one?). Really what I'd like is a more thorough discussion of how observers might try to accomplish the task when distractors are physically identical, and what that might imply about the strength of the conclusions the authors can draw regarding spatial pooling in the model.</p></disp-quote><p>We are not sure that we understand the reviewer’s point here and would be happy to re-address the concern if clarification is required. To summarise the point as we understand it: the reviewer suggests it is plausible that synthesised images would be “acceptable” as real in a single-interval task (i.e. with no direct discrimination). That is, in a task where one image is shown per trial (either an original or a synthesised image) and observers label that image as “real” or “synthesised”, the reviewer suggests that observers may have low sensitivity to the true class label but have a bias to report “real”.</p><p>This is an interesting question which speaks to acceptable distortion quality. Potentially it is the case that synthesised images that can be told apart from real images in a discrimination task (e.g. our oddity design) might appear “non-strange” when no other reference is given. Such a result would suggest that a certain level of pooling discards an “acceptable” amount of information. However, what is an “acceptable” level of distortion is a separate question from the one we are concerned with here: what information is irretrievably discarded by the visual system. A discrimination experiment is a more direct test of the latter.</p><disp-quote content-type="editor-comment"><p>Finally, I like the authors discussion of gestalt properties a great deal, but there's also a competing argument that I think is hard to dismiss: What if the critical scale of 0.5 really is sufficient to make metamers out of natural scenes, but the set of statistics being computed within those regions in the FS-model is inadequate? This is a &quot;God-of-the-gaps&quot; argument to be sure, but again, the issue is that we can't hold a specific implementation of a model to the standard of an ideal.</p></disp-quote><p>We agree that our results cannot exclude the possibility that a summary statistic model exists that could yield scalings of 0.5. We now make this clearer in our revised Discussion section. However, that three of the known and image-computable summary statistic models (FS, our own CNN model, and the NeuroFovea model of Deza et al., 2017) fail to match appearance supports our claim that summary statistic models with fixed pooling regions may be inadequate to match appearance.</p><p>Thus, we cannot rule out that such a model exists, only that none of the known ones work. We clearly label our discussion of Gestalt properties “speculation” (Discussion section) for this reason.</p><disp-quote content-type="editor-comment"><p>I tend to agree with the authors that there are probably grouping and segmentation processes (or global processes) that contribute to perceptual appearance, but it's an open question whether some of these properties might fall out of a model that incorporated different summary statistics that more closely approximate our own perceptual codes in peripheral vision. Again, this is a place where what I'm looking for is some more nuance – perceptual organization is an interesting thing to think about, but refining our ideas about what texture statistics might be computed in the periphery is also interesting to pursue.</p></disp-quote><p>See our response to the editor’s summary letter, above. We have reworded our Discussion section to make it clearer that we cannot rule out some unknown texture statistics as an explanation, and we have expanded our discussion of local and global mechanisms. However, we also re-iterate our general speculation that no fixed texture representation computed over fixed pooling regions can parsimoniously account for image appearance, for it will have to preserve information that could be discarded (Discussion section).</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>This paper uses a 3AFC peripheral distortion detection task to measure the human visual system's spatial resolution for images of real world scenes and synthesised versions of these images. Results indicate that the critical scale with which participants were able to reliably detect distortions was significantly smaller for real world images. A further comparison of the results pertaining to ten &quot;scene-like&quot; and ten &quot;texture-like&quot; images was conducted. Results of this analysis demonstrate that distortions in &quot;scene-like&quot; images (&quot;those containing inhomogeneous structures&quot;) could be made at significantly finer scales than are distortions in &quot;texture-like&quot; images (&quot;those containing more homogenous or periodically patterned content&quot;).</p><p>The authors explain that the critical spatial scale implied by performance regarding the synthesised images is broadly consistent with Freeman and Simoncelli, (2011) V2-like texture pooling model of peripheral visual performance. Given the higher spatial resolution performance implied by performance with the real-world images, and most strikingly, the scene-like images, a convincing argument is mounted that performance in these conditions must be based on either additional information, or at least finer grained information, not available in the FS synthesised images.</p><p>These results contribute to the burgeoning literature demonstrating that the deleterious effects of visual clutter can – under certain image-based circumstances – be much smaller than is predicted by Bouma-like pooling. As the authors note, &quot;It is the image content, not retinal eccentricity, that is the primary determinant of the visibility of at least some summary statistic distortions.&quot;</p><p>This begs the question of 'what types of image content enable the relative high spatial resolution shown here and elsewhere?'. The authors cite some studies relevant to this issue (Saarela et al., 2009; Manassi et al., 2013; Vickery et al., 2009; Herzog et al., 2015), and suggest that &quot;early global segmentation processes influence local perceptual sensitivity&quot; and that &quot;global scene organisation needs to be considered if one wants to capture appearance-yet current models that texturise local regions do not explicitly include perceptual organisation (Herzog et al., 2015).&quot;</p><p>Whilst I agree that global processes may well contribute to the higher spatial resolution observed in this study in response to the real-world images, this overlooks the recent finding by Van der Burg, Olivers and Cass (2017) showing that local geometric interactions can profoundly improve peripheral performance (i.e. to break crowding) in densely cluttered heterogenous displays. Specifically, they find that the presence of local T-junction-like orthogonal structure reduces the effects of deleterious pooling to almost zero. I would suggest that these or similar high-order local interactions may be at play here. That result and the possibility that local interactions might play a role here should at least be proposed. Given the (somewhat vaguely defined) criteria used by the authors to categorise the real-world images as either texture-like or scene-like, I wonder what specific information the authors believe the visual system might use extract peripheral real world image content.</p></disp-quote><p>We thank the reviewer for these insightful comments and for pointing us to this relevant literature. Please see our comments to the main revisions: we have now included a greatly expanded discussion of these issues, as well as a pilot analysis of junction information in our images. This analysis shows that, consistent with the reviewer’s suggestion, images with lower critical scales tended to contain more junction information. This result will be interesting to follow up more systematically in future work.</p></body></sub-article></article>