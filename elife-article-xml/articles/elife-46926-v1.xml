<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">46926</article-id><article-id pub-id-type="doi">10.7554/eLife.46926</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Population adaptation in efficient balanced networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-127716"><name><surname>Gutierrez</surname><given-names>Gabrielle J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2350-1559</contrib-id><email>ellag9@uw.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">†</xref></contrib><contrib contrib-type="author" id="author-4559"><name><surname>Denève</surname><given-names>Sophie</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa2">‡</xref></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Applied Mathematics</institution><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Group for Neural Theory</institution><institution>École Normale Supérieure</institution><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Department of Applied Mathematics, University of Washington, Seattle, United States of America</p></fn><fn fn-type="present-address" id="pa2"><label>‡</label><p>Département D'Études Cognitives, École normale supérieure, Paris, France</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>24</day><month>09</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e46926</elocation-id><history><date date-type="received" iso-8601-date="2019-03-16"><day>16</day><month>03</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-08-27"><day>27</day><month>08</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Gutierrez and Denève</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Gutierrez and Denève</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-46926-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.46926.001</object-id><p>Adaptation is a key component of efficient coding in sensory neurons. However, it remains unclear how neurons can provide a stable representation of external stimuli given their history-dependent responses. Here we show that a stable representation is maintained if efficiency is optimized by a population of neurons rather than by neurons individually. We show that spike-frequency adaptation and E/I balanced recurrent connectivity emerge as solutions to a global cost-accuracy tradeoff. The network will redistribute sensory responses from highly excitable neurons to less excitable neurons as the cost of neural activity increases. This does not change the representation at the population level despite causing dynamic changes in individual neurons. By applying this framework to an orientation coding network, we reconcile neural and behavioral findings. Our approach underscores the common mechanisms behind the diversity of neural adaptation and its role in producing a reliable representation of the stimulus while minimizing metabolic cost.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.46926.002</object-id><title>eLife digest</title><p>Humans see, hear, feel, taste and smell the world as spiking electrical signals in the brain encoded by sensory neurons. Sensory neurons learn from experience to adjust their activity when exposed repeatedly to the same stimuli. A loud noise or that strange taste in your mouth might be alarming at first but soon sensory neurons dial down their response as the sensations become familiar, saving energy.</p><p>This neural adaptation has been observed experimentally in individual cells, but it raises questions about how the brain deciphers signals from sensory neurons. How do downstream neurons learn whether the reduced activity from sensory neurons is a result of getting used to a feeling, or a signal encoding a weaker stimulus? The energy saved through neural adaptation cannot come at the expense of sensing the world less accurately. Neural networks in our brain have evidently evolved to code information in a way that is both efficient and accurate, and computational neuroscientists want to know how. There has been great interest in reproducing neural networks for machine learning, but computer models have not yet captured the mechanisms of neural coding with the same eloquence as the brain.</p><p>Gutierrez and Denève used computational models to test how networks of sensory neurons encode a sensible signal whilst adapting to new or repeated stimuli. The experiments showed that optimal neural networks are highly cooperative and share the load when encoding information. Individual neurons are more sensitive to certain stimuli but the information is encoded across the network so that if one neuron becomes fatigued, others receptive to the same stimuli can respond. In this way, the network is both responsive and reliable, producing a steady output which can be readily interpreted by downstream neurons.</p><p>Exploring how stimuli are encoded in the brain, Gutierrez and Denève have shown that the activity of one neuron does not represent the whole picture of neural adaptation. The brain has evolved to adapt to continuous stimuli for efficiency at both the level of individual neurons and across balanced networks of interconnected neurons. It takes many neurons to accurately represent the world, but only as a network can the brain sustain a steady picture.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual encoding</kwd><kwd>neural tuning</kwd><kwd>perception</kwd><kwd>efficient encoding</kwd><kwd>spike-frequency adaptation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Denève</surname><given-names>Sophie</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>Predispike</award-id><principal-award-recipient><name><surname>Denève</surname><given-names>Sophie</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-10-LABX-0087</award-id><principal-award-recipient><name><surname>Denève</surname><given-names>Sophie</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural populations may depend on balanced recurrent connectivity to produce an efficient stimulus representation while also maintaining an accurate stimulus encoding despite the variability introduced by adapting neural responses.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The range of firing rates that a sensory neuron can maintain is limited by biophysical constraints and available metabolic resources. Yet, these same neurons represent sensory inputs whose strength varies by orders of magnitude. Seminal work by <xref ref-type="bibr" rid="bib2">Barlow (1961)</xref> and <xref ref-type="bibr" rid="bib28">Laughlin (1981)</xref> demonstrated that sensory neurons in early processing stages adapt their response threshold and gain to the range of inputs that they recently received. A particularly striking example of such gain modulation at the single cell level has been shown in the fly H1 neuron (<xref ref-type="bibr" rid="bib8">Brenner et al., 2000</xref>). Gain adaptation has been observed in other early sensory circuits (<xref ref-type="bibr" rid="bib5">Blakemore and Campbell, 1969</xref>; <xref ref-type="bibr" rid="bib14">Fairhall et al., 2001</xref>; <xref ref-type="bibr" rid="bib45">Solomon and Kohn, 2014</xref>; <xref ref-type="bibr" rid="bib48">Wark et al., 2007</xref>), such as in the retina (<xref ref-type="bibr" rid="bib27">Kastner and Baccus, 2014</xref>), auditory hair cells (<xref ref-type="bibr" rid="bib35">Nagel and Doupe, 2006</xref>; <xref ref-type="bibr" rid="bib51">Wen et al., 2009</xref>) and is also present in later sensory stages (<xref ref-type="bibr" rid="bib1">Adibi et al., 2013</xref>; <xref ref-type="bibr" rid="bib47">Wainwright, 1999</xref>). Moreover, cortical neurons acquire this property during development (<xref ref-type="bibr" rid="bib32">Mease et al., 2013</xref>).</p><p>The work of Laughlin and Barlow was instrumental in uncovering a principle of neural adaption as maximizing information transfer. However, the natural follow-up question concerns the decoding of such neural responses after they have been subject to adaptation. Indeed, such changes in neural gains may result in profound changes of the mapping of neural responses to stimuli in a history-dependent manner. This raises the issue of how such adapting responses are interpreted by downstream sensory areas (<xref ref-type="bibr" rid="bib43">Seriès et al., 2009</xref>; <xref ref-type="bibr" rid="bib50">Webster, 2011</xref>).</p><p>One possibility, of course, is that downstream areas do not change their decoding strategy, thus introducing systematic biases in perception. This has been interpreted as the source of perceptual illusions such as the tilt after-effect or the waterfall illusion (<xref ref-type="bibr" rid="bib3">Barlow and Hill, 1963</xref>; <xref ref-type="bibr" rid="bib47">Wainwright, 1999</xref>; <xref ref-type="bibr" rid="bib11">Clifford, 2014</xref>; <xref ref-type="bibr" rid="bib20">He and MacLeod, 2001</xref>; <xref ref-type="bibr" rid="bib42">Schwartz et al., 2009</xref>). Such illusions are classically triggered by long presentations of particularly strong or repetitive stimuli (<xref ref-type="bibr" rid="bib30">Maffei et al., 1973</xref>). However, adaptation deeply affects neural responses even at short time scales or after only one repetition of the same stimulus (<xref ref-type="bibr" rid="bib40">Patterson et al., 2013</xref>).</p><p>Adaptation could make it impossible to recognize a visual object independently of the stimuli presented previously. An example is given in <xref ref-type="fig" rid="fig1">Figure 1</xref> where we present successive visual patterns to a population of randomly connected leaky integrate-and-fire (LIF) neurons. For simplicity and for the sake of illustration, the network takes a 7-dimensional time-varying input interpreted as a spatio-temporal sequence of digital numbers (<xref ref-type="fig" rid="fig1">Figure 1b</xref>, top row). An optimal linear decoder was trained to reconstruct the patterns from the spike counts during the presentation of the patterns. Not surprisingly, the decoder could reconstruct the patterns accurately, regardless of their place in the sequence (<xref ref-type="fig" rid="fig1">Figure 1b</xref>, 2nd row). We then tested the network in the presence of spike-based adaptation in the LIF neurons. Spike-based adaptation was induced by temporarily hyperpolarizing the neurons after each spike. The time scale of this adaptation was chosen to be long enough to cover several visual patterns. When subjected to this spike-time dependent adaptation, the responses became strongly history dependent, resulting in a highly inaccurate decoding (<xref ref-type="fig" rid="fig1">Figure 1b</xref>, 3rd row). This would suggest that activity in downstream areas and perceptual interpretations should be based not only on the current sensory responses, but also on the recent history of neural activity (<xref ref-type="bibr" rid="bib14">Fairhall et al., 2001</xref>; <xref ref-type="bibr" rid="bib7">Borst et al., 2005</xref>). In this study, we show that this is not necessarily the case. Recurrent connections can be tuned such that spike-dependent adaptation will not impair the stability of the representation (<xref ref-type="fig" rid="fig1">Figure 1b</xref>, bottom row).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.46926.003</object-id><label>Figure 1.</label><caption><title>Digital number encoding network.</title><p>(<bold>a</bold>) Schematic of a 7-dimensional input (one dimension for each bar position of a digital interface) being presented to a random recurrent network that sends input to a readout layer (here represented by a single neuron). (<bold>b</bold>) Top, a sequence of digits that serve as stimuli (presented for 200 ms each, spaced by 100 ms between digits). Second row, decoded output of random recurrent network with optimal decoder (trained on 100 samples of completely random patterns). Third row, decoded output of same random recurrent network as above but with adapting neuron responses. Bottom row, balanced network with adaptation derived from efficient coding framework. [All rows: 400 neurons, <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for neural responses integrated by decoder; 3rd and 4th row: <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn><mml:mo>;</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2000</mml:mn><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for the adaptive firing rates].</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46926-fig1-v1.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Neural network solving a global cost-accuracy tradeoff</title><p>We will start from an objective function quantifying the efficiency of a population of spiking neurons in representing a time varying sensory stimulus, <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We will then show that appropriate recurrent connections between the neurons, namely connections that maintain a tight balance between the excitation and inhibition received by each neuron, will minimize this objective function and thus, maximize the efficiency of the neural code. For the sake of illustration, we hereby assume that the stimulus is unidimensional and positive, as for luminance or color saturation (see Materials and methods for multidimensional stimuli), and the stimulus has arbitrary units. The stimulus will be decoded from the firing activity of the neurons by summing their responses with their respective readout weights , <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The neural response, <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, is defined as the spike train integrated at a short time scale,<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> corresponds to the spike train of neuron <inline-formula><mml:math id="inf7"><mml:mi>i</mml:mi></mml:math></inline-formula>. The readout weight of neuron <inline-formula><mml:math id="inf8"><mml:mi>i</mml:mi></mml:math></inline-formula> is denoted as <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and it is a fixed parameter. One may choose to include a wide range of readout weights in the network. The output estimate, <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, can be interpreted as a postsynaptic integration of the output spike trains of the population, weighted by synaptic weights <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>We wish to construct a network that will minimize the difference between <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf13"><mml:mrow><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, ensuring an accurate representation of the stimulus. Additionally, we wish to impose, not only accuracy, but also cost efficiency in the neural representation. For biological neurons, spiking comes with inherent metabolic costs. For example, resources are expended after each spike and neurons or neural populations may need some time to recover from a period of strong activity. Albeit many different types of cost can be incorporated into our approach, here we summarize these constraints as a cost term representing the sum of all squared firing rates. Thus, we define an objective function composed of two terms, one representing the precision of the representation, and the other the cost of neural activity (<xref ref-type="bibr" rid="bib6">Boerlin et al., 2013</xref>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>(t) is the firing history of neuron <inline-formula><mml:math id="inf15"><mml:mi>i</mml:mi></mml:math></inline-formula> and the parameter µ weights the relative contributions of error and metabolic costs. The firing history, <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>(t), is defined as the spike train integrated with a time constant, <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula>.<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Typically, the adaptation time scale is assumed to be significantly longer than the decoder time scale (<inline-formula><mml:math id="inf18"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>≫</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula>). A short <inline-formula><mml:math id="inf19"><mml:mi>τ</mml:mi></mml:math></inline-formula> (e.g. of the order of 10 ms) ensures that fast changes in the stimulus can be represented accurately. However, the metabolic cost of spiking accumulates and recovers at slower time scales (e.g. <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> corresponds to hundreds of ms). The underlying assumption is that the dynamics allowing metabolic resources to be replenished are slower than the time scale at which neural populations transmit information. The sum of squared firing history will encourage, not only low activity at the level of the population, but also low activity in single neurons. As a result, neurons will share the burden of the representation.</p><p>From these assumptions, we derive a prescription for the voltage dynamics of leaky integrate-and-fire (LIF) neurons performing a greedy minimization of the objective function, <inline-formula><mml:math id="inf21"><mml:mi>E</mml:mi></mml:math></inline-formula> (see Materials and methods for full derivation). Our framework revolves around the assumption that a neuron spikes only when doing so reduces the decoding error. This condition can be expressed in terms of the objective function as <inline-formula><mml:math id="inf22"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+2.2pt"><mml:mi>o</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> where a spike is justified if the objective is minimized relative to having no spike at that time step. Using <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, we obtain a new expression from this inequality that embodies a condition for spiking and that we interpret as a voltage expression and a threshold (see Materials and methods for derivation details) such that <inline-formula><mml:math id="inf23"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and voltage is:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="160%" minsize="160%">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="160%" minsize="160%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Taking the derivative of the voltage expression produces the voltage equation below:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the gain of neuron <inline-formula><mml:math id="inf25"><mml:mi>i</mml:mi></mml:math></inline-formula>,<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>and the lateral connections are given by <inline-formula><mml:math id="inf26"><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the delta function (equals one only if <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>, zero otherwise) and <inline-formula><mml:math id="inf29"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mi>τ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The form of the voltage equation is amenable to being interpreted as a set of currents to a neuron embedded in a recurrent network with all-to-all connectivity. Neurons are connected by mutually inhibitory synapses determined by their decoding weights. The final term corresponds to an adaptation current that depresses the voltage as a function of its recent activity (see <xref ref-type="fig" rid="fig2">Figure 2c</xref>). This indicates that spike-frequency adaptation in single neurons is part of the solution to the cost-accuracy tradeoff. However, we will show that it cannot work alone; it needs to be associated with appropriately tuned recurrent connections.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.46926.004</object-id><label>Figure 2.</label><caption><title>Intrinsic model neuron properties.</title><p>(<bold>a</bold>) High gain neurons (light blue) are intrinsically excitable and due to their small decoding weights they are precise while low gain neurons (dark blue) are less excitable and less precise. An arbitrary input, <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, elicits distinct responses from the two neurons (spikes train <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf32"><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>, respectively). Neurons send a filtered response, <inline-formula><mml:math id="inf33"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf34"><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>, to the decoder weighted by <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>, respectively. (<bold>b</bold>) Relationship between gain <inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, feedforward gain <inline-formula><mml:math id="inf38"><mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and decoding weight <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>c</bold>) Different gains give neurons distinct adaptation dynamics. Instantaneous spiking rates in response to a constant input are plotted over time for three model neurons with different decoding weights (light blue, w = 1; medium blue, w = 5; dark blue, w = 9). High gain neurons have the steepest adaptation (light blue) whereas low gain neurons (dark blue) do not adapt as rapidly given the same input. Inset shows the voltage trace, <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and spike train, <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, for each example neuron.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46926-fig2-v1.tif"/></fig><p>It is easier to interpret the network function if we consider that the membrane potentials are effectively proportional to the global coding error penalized by the past activity of the neuron, as seen in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>. A neuron that reaches the firing threshold is guaranteed to contribute a decrease of the error term in the objective function (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>). As a whole, the population performs a greedy minimization of the objective function, or, in other terms, a greedy maximization of the coding efficiency.</p><p>Finally, we note that since the integrated excitatory input, <inline-formula><mml:math id="inf43"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is cancelled as precisely as possible (except for the cost penalty) by the recurrent inhibition, <inline-formula><mml:math id="inf44"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the network can be considered as balancing feedforward excitation and recurrent inhibition (see <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). The second ingredient for population efficiency (in addition to spike-based adaptation) is thus to maintain a tight E/I balance in the network. In other words, we show that a memoryless decoder will be able to reconstruct the stimulus from the output spike trains of an E/I balanced population of adapting neurons. This is shown in the bottom row of <xref ref-type="fig" rid="fig1">Figure 1b</xref>. Before we investigate the network dynamics and performance, we first describe the properties of single neurons and the relationship between their gain and their coding precision.</p></sec><sec id="s2-2"><title>Single neuron properties</title><p>Let us first consider the case without quadratic cost (i.e. <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). In that case, each neuron effectively has identical voltage and spiking dynamics. Neurons are differentiated only by their gain, <inline-formula><mml:math id="inf46"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, and their decoding weight, <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The strength of the feedforward gain is inversely related to the strength of the output weight for each neuron. As a result, neurons with the smallest decoding weights (and thus, the highest precision in representing the input) tend to respond most strongly to the stimulus (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). We will refer to these costly but reliable neurons as 'strongly excitable’. In contrast, neurons with large decoding weights and small input weights (thus 'low gain’ neurons) bring less precision to the estimate but are metabolically efficient. We will refer to these neurons as 'weakly excitable’.</p><p>Note that if <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the cost is not taken into account by the network. Thus, it will always favor precision over cost. In that case, only the most excitable neuron (with the smallest decoding weight) will respond to the stimulus while completely inhibiting the other neurons. However, with the addition of a cost (<inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), adaptive currents contribute to the voltage dynamics, penalizing neurons with large firing rates. Moreover, the feedforward gain, <inline-formula><mml:math id="inf50"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, does not necessarily decrease monotonically with the decoding weight (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). For very small decoding weights, <inline-formula><mml:math id="inf51"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>≪</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:math></inline-formula>, neurons with decoding weights smaller in magnitude than <inline-formula><mml:math id="inf52"><mml:msqrt><mml:mi>μ</mml:mi></mml:msqrt></mml:math></inline-formula> are penalized. These neurons would simply be too costly to participate meaningfully in the cost/accuracy tradeoff solved by the population. Model neurons in isolation (i.e. without any contribution from recurrent connections) would respond to a step-like input with a rate that decreases exponentially in time before reaching a plateau (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), a classic signature of activity-dependent suppression. The time constant of this adaptation is determined by <inline-formula><mml:math id="inf53"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula>, while the strength of this adaptation increases with the gain. Highly excitable neurons adapt strongly, while less excitable neurons adapt weakly.</p><p>However, these intrinsic properties of single neurons will be deeply affected by the dynamics introduced by recurrent connections. To gain a better understanding of population adaptation, we investigate how inhibitory connections orchestrate the relative contributions of different neurons over the duration of a long stimulus.</p></sec><sec id="s2-3"><title>Network activity is distributed on a manifold in neural activity space</title><p>We first illustrate the effect of recurrent connections with an example network composed of only two neurons (<xref ref-type="fig" rid="fig3">Figure 3</xref>). The two neurons are reciprocally connected with inhibitory connections, as prescribed in the derivation (schematized in <xref ref-type="fig" rid="fig3">Figure 3a</xref>). They receive a constant stimulus, but have different input weights.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.46926.005</object-id><label>Figure 3.</label><caption><title>Two-neuron network.</title><p>(<bold>a</bold>) Schematic of recurrently connected two-neuron network derived from efficient coding framework. Neuron 1 is strongly excitable (<inline-formula><mml:math id="inf54"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>), while neuron 2 is weakly excitable (<inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>b</bold>) Spikes from neuron 1 (light blue) and neuron 2 (dark blue) show the transient response of the strongly excitable neuron and the delayed, but sustained response of the weakly excitable neuron (top) in response to a constant stimulus. Postsynaptic activity, <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (bottom) [<inline-formula><mml:math id="inf57"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>25</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1000</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>]. (<bold>c</bold>) The balanced network with adaptation follows a linear manifold (left), whereas the network without recurrent connections but with adaptation cannot be linearly decoded (right). (<bold>d</bold>) The cost (<inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mi>f</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, yellow) accumulates steeply until neuron one adapts and neuron two is recruited and the cost increases at a slower rate. The network representation (orange) is maintained despite the redistribution of activity among the neurons.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46926-fig3-v1.tif"/></fig><p>During sustained stimulation, the response of each neuron fluctuates dynamically despite the fact that the stimulus is constant (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). This would be expected given their spike-time dependent adaptation. However, if one removes the recurrent connections and plots the response of one neuron as a function of the other (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, right), we discover that the population response wanders from the iso-coding line, <inline-formula><mml:math id="inf59"><mml:mrow><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (i.e. the manifold in activity space where the stimulus would be decoded properly). In contrast, the intact network with its recurrent connections coordinates the two neurons such that the weighted sum of their responses remains accurate. The movement of the activity along the manifold defined by the constant stimulus and the decoding weights reflects a progressive redistribution of activity to satisfy the unfolding cost-accuracy tradeoff, as the cost slowly accumulates (<xref ref-type="fig" rid="fig3">Figure 3d</xref>).</p><p>While to a naive observer, the high gain neuron may appear to adapt while the low gain neuron has a sustained response and a longer delay, in fact both contribute to population adaptation because both neurons coordinate and adapt their activity to limit the metabolic cost of the representation while maintaining its accuracy. Recurrent connections deeply affect the dynamics of each neuron. For example, the inhibition from the strongly excitable neuron is responsible for the response delay of the weakly excitable neuron.</p></sec><sec id="s2-4"><title>Coordinated adaptation in a neural population</title><p>Within a network with many neurons (<xref ref-type="fig" rid="fig4">Figure 4</xref>), recurrent connections interact with the intrinsic properties of the neurons in a similar manner as in the previous example. The first neurons to be recruited are strongly excitable and provide an initially very precise representation of the signal.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.46926.006</object-id><label>Figure 4.</label><caption><title>Adapting population of heterogeneous neurons.</title><p>(<bold>a</bold>) Spike raster of all 10 neurons in a balanced network with adaptation in response to a pulse stimulus (<inline-formula><mml:math id="inf60"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝒘</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>). Neurons are ordered from weakly excitable (top, dark blue) to highly excitable (bottom, light blue). (<bold>b</bold>) Both the error (<inline-formula><mml:math id="inf61"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, blue) and cost (<inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mi>f</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, orange) accumulate over time. (<bold>c</bold>) The network estimate (<inline-formula><mml:math id="inf63"><mml:mrow><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, orange) tracks the stimulus (<inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, gray) with increasing variance. (<bold>d</bold>) The smoothed network estimate (blue line) shows a biased estimate with increasing variance (blue shade, standard deviation). (<bold>e</bold>) Instantaneous spiking rates of 3 example neurons in the network. Inset shows the voltage trace, <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and spike train, <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, for each example neuron. (<bold>f</bold>) Schematic of 10-neuron balanced network showing only connections to and from the middle neuron. Excitatory connections are shown as triangles and in this particular network are only found in the feedforward and output connections. Inhibitory connections are shown with small circles and make up only the recurrent connections.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46926-fig4-v1.tif"/></fig><p>These neurons inhibit the less excitable neurons, preventing them from firing early in this stimulation period. As the cost accumulates, however, the response of the high gain neurons decays due to spike-frequency adaptation. This is compensated by weakly excitable neurons that become disinhibited, fire, and then adapt in their turn. The less excitable a neuron is, the later it will be recruited, resulting in strong response delays. The dynamic response properties of individual neurons are thus dominated by network interactions and are markedly different from their intrinsic adaptive properties (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><p>Because the disinhibition of weakly excitable neurons automatically compensates for the decay in strongly excitable neural responses, the stimulus representation remains stable during the whole period (<xref ref-type="fig" rid="fig4">Figure 4d</xref>). However, note that its precision degrades as more low gain neurons contribute to the representation. As a result, the bias and standard deviation of the representation increases as imposed by the global cost/accuracy tradeoff.</p></sec><sec id="s2-5"><title>Coordinated adaptation of tuning curves</title><p>To illustrate what coordinated adaptation would predict for tuning curves measured experimentally, we constructed a population of neurons that code for visual orientation; V1 simple cells. The input to the network takes the form of a two-dimensional signal with a cosine and a sine of the presented orientation (see Materials and methods). Each neuron has a preferred orientation that is given by the combination of input weight strengths in the two input dimensions. In turn, the network orientation estimate can be decoded from the population (see Materials and methods).</p><p>The lateral connections derived from the model maximally inhibit neurons with similar preferred orientations and excite neurons with orthogonal orientations (see schematic in <xref ref-type="fig" rid="fig5">Figure 5a</xref>) due to the choice of decoding weights which can be positive or negative, or some combination. To observe the effects of adaptation on a diverse population of neurons, we constructed our network so that neurons have equally spaced preferred orientations and a partner neuron that shares the same preference but has a different gain. There is a high gain and a low gain neuron among each pair of neurons that code for the same orientation.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.46926.007</object-id><label>Figure 5.</label><caption><title>Orientation coding-network.</title><p>(<bold>a</bold>) Schematic showing the dual-ring structure of the network of high gain (light blue) and low gain (dark blue) neurons. Some of the recurrent connections from the outlined light blue neuron are illustrated to show that a neuron inhibits its neighbors most strongly and excites neurons with opposing preferences (inhibitory connections are shown as circles, excitatory connections are shown as chevrons). (<bold>b</bold>) Spike raster (top) of population activity showing the evolution of the population response during a prolonged stimulus presentation of a constant orientation. Rasters are displayed in order of neuron orientation preferences. The decoded orientation is steady while the variance increases over time (bottom). Arrow indicates the stimulus orientation. (<inline-formula><mml:math id="inf67"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2000</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, stimulus magnitude C = 50, 200 neurons).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46926-fig5-v1.tif"/></fig><p><xref ref-type="fig" rid="fig5">Figure 5b</xref> illustrates the spiking response of the network to a prolonged oriented stimulus. As seen in the simpler model from <xref ref-type="fig" rid="fig4">Figure 4</xref>, high gain neurons respond first, then adapt. As the responses of those strongly excitable neurons decay, weakly excitable neurons are recruited, maintaining the representation. This results in systematic changes in the tuning curves from early in the response to later in the response (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Highly excitable neurons are suppressed relative to their early responses (<xref ref-type="fig" rid="fig6">Figure 6</xref>, top). In contrast, weakly excitable neurons see their tuning curves widen when the adapting stimulus is similar to their preferred orientation (<xref ref-type="fig" rid="fig6">Figure 6</xref>, bottom). At the flank of the adapting orientation, low gain neurons see an increase in their responsiveness. Here, the network interactions override the intrinsic adapting currents in the weakly excitable neural population. In other words, the disinhibition from strongly excitable neurons combined with the constant feedforward drive to these low gain neurons results in facilitated activity rather than the suppressed activity one would expect to be caused by adaptation. Finally, the tuning curves for the most excitable neurons are broader than those for weakly excitable neurons. These neurons are more likely to fire first in response to oriented stimuli that are near their preferred orientation and prevent the low gain neurons from doing the same.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.46926.008</object-id><label>Figure 6.</label><caption><title>Population adaptation tuning curves show neuron responses to a full range of test orientations (x-axis) after adaptation to a single orientation (black dashed line).</title><p>Top, tuning curves for strongly excitable neurons before adaptation (light blue) are broad. After adaptation (orange), tuning curves near the adaptor are suppressed. Bottom, tuning curves for weakly excitable neurons before adaptation (dark blue) show less activation than for high gain neurons and more specific tuning. After adaptation (red), flanking curves are facilitated and shifted toward adaptor. [<inline-formula><mml:math id="inf69"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2000</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, stimulus magnitude C = 50, 200 neurons].</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46926-fig6-v1.tif"/></fig></sec><sec id="s2-6"><title>Heterogeneity in a more plausible model</title><p>The same qualitative effects are observed in a more realistic network where the preferred orientations of different neurons and their decoding weights are taken from a random distribution (<xref ref-type="fig" rid="fig7">Figure 7</xref>), rather than regularly spaced with two levels of excitability. The tuning curves are more heterogeneous not because of noise but because of the randomness of the decoding weights. Tuning curves can be either facilitated or suppressed by adaptation. When the adapted stimulus falls on the flank of the tuning curve, it can be accompanied by a shift toward or away from the adapting stimulus. The effect of adaptation on single neurons is variable not because of noise (we did not introduce any) but because of local heterogeneity in the competition they receive from other neurons, itself due to the random choices of weights. In fact, adaptation in one neuron would be impossible to predict quantitatively without observing the rest of the network.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.46926.009</object-id><label>Figure 7.</label><caption><title>Selected tuning curves from orientation network with random decoder weights (and thus random neuron gains).</title><p>Blue curves, before adaptation; red curves, after weak adaptation; yellow curves, after strong adaptation. Some neuron responses are suppressed after adaptation while others are facilitated, and some tuning curves shift laterally after adaptation. Dashed lines indicate adaptor orientation. [<inline-formula><mml:math id="inf71"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1000</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, weak stimulus magnitude C = 10, strong stimulus magnitude C = 50, test stimulus magnitude C = 10, 200 neurons].</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46926-fig7-v1.tif"/></fig></sec><sec id="s2-7"><title>Perceptual adapation</title><p>We have stressed the accuracy of the stimulus representation in the face of time-varying activity due to adaptation. While this kind of activity could be interpreted as leading to a stable percept in spite of adaptation, we acknowledge that perceptual errors and biases are abundant in the natural world. Our network is capable of emulating these errors and it is able to do so in a manner that is consistent with experimental findings. The network is designed to negotiate the tradeoff between accuracy and efficiency and it will prioritize the production of a stable representation if µ is small. If µ is large, the network will favor cost over accuracy. Thus, strong adaptation and a prolonged stimulus presentation can produce a representation that degrades over time. This degradation can lead to a bias in the decoder. In <xref ref-type="fig" rid="fig8">Figure 8</xref>, an oriented, strong, adapting stimulus is presented for 2 seconds followed by a test orientation (this is schematized in <xref ref-type="fig" rid="fig8">Figure 8a</xref>). An example of the resulting network activity is shown in <xref ref-type="fig" rid="fig8">Figure 8b</xref>.</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.46926.010</object-id><label>Figure 8.</label><caption><title>Tilt illusion.</title><p>(<bold>a</bold>) Schematic of tilt adaptation protocol. (<bold>b</bold>) Network activity in response to an adapting stimulus followed by a test stimulus. Rasters are ordered by neurons’ orientation preferences. Black arrow, neurons that prefer adapting orientation; red arrow, neurons that prefer test orientation (<inline-formula><mml:math id="inf72"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2000</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, 200 neurons, adaptor C = 50, test C = 25). (<bold>c</bold>) Examples of tilt bias: (left) no bias before adaptation, (middle) network estimate is biased away from test stimulus and adaptor when adaptor is near test orientation, (right) estimate is biased towards adaptor when adaptor is at large angle to test stimulus (red arrow, test orientation; grey arrow, adaptor; blue arrow, decoded orientation to test orientation after adaptation). (<bold>d</bold>) Estimate bias is repulsive for near adaptation and attractive for oblique adaptation. Adaptor is presented for 2 s and test orientation is presented for 250 ms (<inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, adaptor C = 25, test C = 5).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46926-fig8-v1.tif"/></fig><p>Before adaptation takes hold, the adapting stimulus activates the high gain neurons that have preferences at or near the stimulus orientation. Because the adapting stimulus is strong, high gain neurons with similar preferences are quickly recruited. As the stimulus persists, the most strongly activated high gain neurons fatigue and the low gain neurons with matching preferences are recruited. After the presentation of the adapting orientation, a weaker peripherally oriented test stimulus is delivered. The response distribution and dynamics are markedly different. Instead of a widely-tuned response, the weaker test stimulus produces a more narrowly distributed response. The decoded orientation is offset from the test stimulus orientation, indicating a bias in the perceived orientation.</p><p>A classical study of perceptual bias is the tilt illusion (<xref ref-type="bibr" rid="bib18">Gibson and Radner, 1937</xref>; <xref ref-type="bibr" rid="bib11">Clifford, 2014</xref>). In the tilt illusion, the orientation of a test grating is perceived incorrectly after adaptation to a differently oriented stimulus. Experimental studies report that the perceived orientation is often repulsed away from the adapted orientation, the effect being maximal for adapting stimuli tilted around 15–20 degrees from the test stimulus. If the adapting stimulus is oriented around 60 degrees from the test stimulus, a repulsive effect is observed instead. This effect has been confirmed in the visual cortex (<xref ref-type="bibr" rid="bib25">Jin et al., 2005</xref>; <xref ref-type="bibr" rid="bib20">He and MacLeod, 2001</xref>). Our model replicates this effect (<xref ref-type="fig" rid="fig8">Figure 8c,d</xref>). The test stimulus is decoded at an orientation that is repulsed from its actual orientation away from the adaptor when the adaptor is approximately 15 degrees from vertical (<xref ref-type="fig" rid="fig8">Figure 8c</xref>, middle). However, when the adaptor is obliquely oriented from the test orientation, the test stimulus is perceived to be oriented in a direction that is attracted to the adaptor (<xref ref-type="fig" rid="fig8">Figure 8c</xref>, right). Test stimuli within a range of 0–45 degrees difference from the adaptor orientation are repulsed whereas test stimuli with a greater than 45 degree difference from the adaptor orientation are attracted (<xref ref-type="fig" rid="fig8">Figure 8d</xref>). In accordance with experimental findings, the repulsion effect has a greater amplitude than the attraction effect.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Sensory neurons in cortex are embedded in highly recurrent networks with each cell receiving strong inhibitory currents that co-vary with excitatory currents (<xref ref-type="bibr" rid="bib19">Graupner and Reyes, 2013</xref>) and are thus E/I balanced. Here, we show that in balanced networks, heterogeneous sensory neurons with activity dependent suppression solve a global cost/accuracy tradeoff rather than a local tradeoff at the level of each neuron. In our case, adaptation at the level of individual neurons co-exists with a largely stable representation at the population level. Rather than being globally suppressed by adaptation, E/I balance indirectly ensures that the neural activity is redistributed from highly responsive neurons to less responsive neurons without changing the interpretation of this activity by downstream areas.</p><p>Our approach suggests that, given adaptation, neural coding cannot be understood at the level of a single neuron, except in cases where a unique sensory feature is solely encoded by a single neuron, such as the H1 neuron (<xref ref-type="bibr" rid="bib8">Brenner et al., 2000</xref>). In areas containing large numbers of interconnected neurons with redundant selectivity, many questions about neural coding and adaptation are only meaningful when applied to whole populations. We show that the adapting tuning curves of a single neuron can reflect a collective, flexible solution found by the network in particular contexts. Other studies have used predictive coding and efficient coding frameworks to construct models of adapting neural responses (<xref ref-type="bibr" rid="bib10">Chopin and Mamassian, 2012</xref>; <xref ref-type="bibr" rid="bib33">Młynarski and Hermundstad, 2018</xref>; <xref ref-type="bibr" rid="bib31">May and Zhaoping, 2016</xref>) , however, our model incorporates a biologically plausible spike-frequency adaptation mechanism.</p><p>Another problem arises in the trial-to-trial variability produced by adaptation that is observed at the single neuron level. As mentioned in the introduction, the history-dependence caused by adaptation begs the question of how a consistent representation can be decoded from a network in which all, or most, neurons are subject to adaptation. Our study shows that the potentially harmful effects of adaptation on the individual neuron’s ability to encode a stimulus can be mitigated by a coordinated population response. Other studies that have addressed this issue propose updating the decoder (<xref ref-type="bibr" rid="bib4">Benucci et al., 2009</xref>) or have considered divisive gain control mechanisms (<xref ref-type="bibr" rid="bib42">Schwartz et al., 2009</xref>) as well as synaptic plasticity mechanisms (<xref ref-type="bibr" rid="bib23">Hosoya et al., 2005</xref>). Our study offers an alternative, plausible framework for resolving the cost-accuracy tradeoff on a shorter time scale than the operating time scale for synaptic plasticity. Instead of updating the weights to better represent stimuli over several iterations, as is done for the perceptron and convolutional neural networks (<xref ref-type="bibr" rid="bib16">Fukushima, 1980</xref>; <xref ref-type="bibr" rid="bib29">LeCun et al., 1999</xref>; <xref ref-type="bibr" rid="bib39">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib41">Rosenblatt, 1958</xref>), we derive a prescription for the voltage dynamics so that the network neurons can produce a reconstruction of any stimulus with fixed decoder and recurrent weights.</p><p>Our model is developed from a normative encoding framework (<xref ref-type="bibr" rid="bib6">Boerlin et al., 2013</xref>; <xref ref-type="bibr" rid="bib13">Druckmann and Chklovskii, 2012</xref>; <xref ref-type="bibr" rid="bib39">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib46">Spratling, 2010</xref>) in which we enforce efficiency in the encoder and accuracy in the decoder. The new contribution compared to <xref ref-type="bibr" rid="bib6">Boerlin et al. (2013)</xref> is to allow more flexibility in the form that the metabolic cost can take. In particular, the time scales of the cost and of the representation are disassociated which leads to distinct dynamics for the cost and for the neural firing rate. This approach can be generalized to many other types of cost, arbitrary weights, and number of neurons.</p><sec id="s3-1"><title>Single neuron coding is dynamic rather than a static property</title><p>Our model suggests that diverse adaptation properties within a population can be an asset. The variability of adaptation effects has been observed in V1 neurons (<xref ref-type="bibr" rid="bib24">Jeyabalaratnam et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Nemri et al., 2009</xref>; <xref ref-type="bibr" rid="bib17">Ghisovan et al., 2009</xref>). A heterogeneous population of neurons is able to better distribute the cost to maximize efficiency in different contexts. Studies in the retina show that retinal ganglion cells with different adaptive properties complement each other such that sensitizing cells can improve the encoding of weak signals when fatiguing cells adapt (<xref ref-type="bibr" rid="bib26">Kastner and Baccus, 2011</xref>). This arrangement is particularly advantageous for encoding contrast decrements which would be difficult to distinguish from the prior stimulus distribution if only suppressive adaptation prevailed. At the same time, these heterogeneities contribute to complex dynamics in the neural spike trains (<xref ref-type="bibr" rid="bib12">Dragoi et al., 2000</xref>; <xref ref-type="bibr" rid="bib38">Okun et al., 2015</xref>; <xref ref-type="bibr" rid="bib37">Nirenberg et al., 2010</xref>; <xref ref-type="bibr" rid="bib34">Mohar et al., 2013</xref>; <xref ref-type="bibr" rid="bib52">Wissig and Kohn, 2012</xref>), obscuring the relationship between neural activity and neural coding for an observer of single neuron activity. We make the prediction that neurophysiological studies where single neuron activity is recorded may exhibit an experimental bias that results in highly responsive neurons being overrepresented in the sample.</p><p>Moreover, our study challenges the notion that tuning is a static characteristic of neurons. Experiments increasingly reveal that neurons change their tuning dynamically with changing stimulus statistics (<xref ref-type="bibr" rid="bib21">Hollmann et al., 2015</xref>; <xref ref-type="bibr" rid="bib22">Hong et al., 2008</xref>; <xref ref-type="bibr" rid="bib23">Hosoya et al., 2005</xref>; <xref ref-type="bibr" rid="bib35">Nagel and Doupe, 2006</xref>; <xref ref-type="bibr" rid="bib44">Smirnakis et al., 1997</xref>; <xref ref-type="bibr" rid="bib45">Solomon and Kohn, 2014</xref>; <xref ref-type="bibr" rid="bib48">Wark et al., 2007</xref>; <xref ref-type="bibr" rid="bib49">Wark et al., 2009</xref>). In the visual cortex, it has been shown that the tilt after effect is not only an effect of response suppression but that it also has the effect of shifting the tuning curves of neurons away from their preferred orientations (<xref ref-type="bibr" rid="bib25">Jin et al., 2005</xref>; <xref ref-type="bibr" rid="bib17">Ghisovan et al., 2009</xref>; <xref ref-type="bibr" rid="bib12">Dragoi et al., 2000</xref>). While it may be possible to predict some aspect of the tuning change from measurements of intrinsic neuron properties, our study shows that a great deal of the change may be a network effect rather than an intrinsic neuronal effect. Thus, the extent of adaptation for a single neuron may be difficult to predict without considering the properties of the rest of the network (<xref ref-type="bibr" rid="bib15">Fairhall, 2014</xref>). Such unpredictable adaptation could be a problem for the interpretation by downstream readouts, however, we show that when the network is considered as a whole, the adaptive effects in one neuron can be compensated for by another neuron that reports to the same readout. In other words, the apparently complex adaptation at the single neuron level is not an impediment to the network but rather an indicator of the manner in which the signal is encoded by the network as a whole.</p></sec><sec id="s3-2"><title>Validating the framework experimentally</title><p>Our model applies at the level of relatively densely connected, and thus local, populations. Observing the organized transfer of responses between neurons through adaptation and E/I balance would require one to record a significant proportion of these neurons locally (neurons that are likely to be interconnected directly or through interneurons). Recent experimental techniques render such recordings possible (<xref ref-type="bibr" rid="bib9">Buzsáki, 2004</xref>), bringing an experimental validation of this framework within grasp. These recordings could be compared before and after adaptation, over the duration of prolonged stimuli, or over many repetitions of the same stimulus. What we expect to see is a generalization of the effect illustrated in <xref ref-type="fig" rid="fig4">Figure 4b,c</xref> to larger neural populations. First of all, there should exist a decoder of neural activity, independent of stimulus history that can detect the stimulus despite large changes in neural activity over time. Second of all, shuffling the neural responses, for example between the early and latter part of the responses to a prolonged stimulus, should have detrimental effects on such stable decoding. And finally, over the course of adaptation, the activity of the different neurons should not vary independently. For example, if we performed a dimensionality reduction (such as a principle component analysis) of the neural population activity during a prolonged stimulus presentation, we might be able to observe that neural responses over time are constrained on a subspace where the stimulus representation is stable. Another, more direct way of testing our framework would be to activate or inactivate a part of the neural population. This could be done optogenetically, for example (<xref ref-type="bibr" rid="bib38">Okun et al., 2015</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All simulations were done in Matlab using code that we developed from the spiking predictive coding model that is mathematically derived below.</p><sec id="s4-1"><title>Digital number encoding network</title><p>The network used in <xref ref-type="fig" rid="fig1">Figure 1</xref> is a generic recurrent network of 400 neurons with random recurrent and feedforward weights. The feedforward weights are a 7 × 400 matrix of values drawn from a uniform distribution in the [−1,1] range. The recurrent weights are drawn from a Gaussian distribution with mean = 0, std = 0.87 (close to 1) and are a 400 × 400 matrix, however, all neurons had an autapse that was the sum of the negative squares of its feedforward weights. The network was trained on 100 stimulus examples of 300 ms each that were generated randomly from a uniform distribution of arbitrary input values between 0 and 4. An optimal linear decoder was obtained from this training by taking the inverse of the responses and multiplying them by the stimulus training examples: decoder = pseudoinverse <inline-formula><mml:math id="inf74"><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The trained network was then presented with a sequence of 8 digitized patterns for 200 ms each separated by 100 ms of no stimulus input. To demonstrate the effect of adaptation, the trained network was run on the same stimulus sequence and with the same linear decoder but this time the spiking threshold was dynamically regulated by past spiking activity such that the threshold was <inline-formula><mml:math id="inf75"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf76"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. For the example of the balanced network with adaptation, the network was derived using the framework described below using the following parameters: <inline-formula><mml:math id="inf77"><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2000</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-2"><title>Network model</title><p>We provide here a brief description of the network structure and the objective function it minimizes. A detailed and closely related version of this derivation is found in <xref ref-type="bibr" rid="bib6">Boerlin et al. (2013)</xref>. The innovation in our present study is the incorporation of a variable for spiking history in the derivation. We consider a spiking neural network composed of N neurons that encodes a set of M sensory signals, <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi mathvariant="bold-italic">ϕ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Estimates of these input signals, <inline-formula><mml:math id="inf79"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">ϕ</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, are decoded by applying a set of decoding weights, <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, to the filtered spike train of neuron <inline-formula><mml:math id="inf81"><mml:mi>i</mml:mi></mml:math></inline-formula> so that <inline-formula><mml:math id="inf82"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">ϕ</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). The filtered spike train, <inline-formula><mml:math id="inf83"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, corresponds to a leaky integration of its spikes, <inline-formula><mml:math id="inf84"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, while the spike history, <inline-formula><mml:math id="inf85"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, filters the spike train on a longer time scale so that <inline-formula><mml:math id="inf86"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula>.<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle><mml:mspace linebreak="newline"/></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>with <inline-formula><mml:math id="inf87"><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> the spike time of the <inline-formula><mml:math id="inf88"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> spike in neuron <inline-formula><mml:math id="inf89"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf90"><mml:mi>τ</mml:mi></mml:math></inline-formula> the time scale of the decoder. As we will see, <inline-formula><mml:math id="inf91"><mml:mi>τ</mml:mi></mml:math></inline-formula> is the membrane time constant of the model neurons and <inline-formula><mml:math id="inf92"><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> is the adaptation time constant.</p><p>The decoding weights <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are chosen a priori. They determine the selectivity and gain of the model neurons. We want to construct a neural network that represents the signals most efficiently, given the fixed decoding weights. Efficiency is defined as the minimization of an objective function composed of two terms, one penalizing coding errors, and the other penalizing firing rates:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">ϕ</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>µ is a positive constant regulating the cost/accuracy tradeoff. In order to minimize this objective function, we define a spiking rule that performs a greedy minimization. Thus, neuron <inline-formula><mml:math id="inf94"><mml:mi>i</mml:mi></mml:math></inline-formula> fires as soon as this results in a minimization of the cost, that is as soon as <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. A spike in neuron <inline-formula><mml:math id="inf96"><mml:mi>i</mml:mi></mml:math></inline-formula> contributes a decaying exponential kernel, <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, to its firing rate so that<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>→</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>→</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>→</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf98"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a more slowly decaying exponential kernel than <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The spiking condition, <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, can be expressed as:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>h</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The i-th element in the Euclidean basis vector, <inline-formula><mml:math id="inf101"><mml:msub><mml:mi mathvariant="bold-italic">𝒆</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, is one while all other entries are zero. Algebraically rearranging this expression leads to the following spiking rule: neuron <inline-formula><mml:math id="inf102"><mml:mi>i</mml:mi></mml:math></inline-formula> spikes if:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">(</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝒘</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>With <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> being the 'gain’ of neuron <inline-formula><mml:math id="inf104"><mml:mi>i</mml:mi></mml:math></inline-formula>. We interpret the left-hand side of <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> as the membrane potential, <inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, of neuron <inline-formula><mml:math id="inf106"><mml:mi>i</mml:mi></mml:math></inline-formula>, and the right-hand side as its firing threshold. Membrane potentials are normalized such that each neuron has a threshold equal to 1/2 and reset potential equal to −1/2. The membrane potential dynamics are obtained by taking the derivative of the voltage expression with respect to time (where <inline-formula><mml:math id="inf107"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mi>τ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>):<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Notice that the lateral connection between neuron <inline-formula><mml:math id="inf108"><mml:mi>i</mml:mi></mml:math></inline-formula> and neuron <inline-formula><mml:math id="inf109"><mml:mi>j</mml:mi></mml:math></inline-formula> is equal to <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, the lateral connections measure to what extent the feed-forward connections of two neurons are correlated, and they remove those correlations to obtain the most efficient code.</p></sec><sec id="s4-3"><title>Orientation model</title><p>The orientation-coding network follows the same derivation as outlined for the generic network model. It has two input dimensions and 200 neurons. There are two subpopulations of neurons such that 100 neurons are high gain neurons and the remaining 100 neurons are low gain neurons. Both populations span the unit circle evenly such that one low gain and one high gain neuron share the same preferred orientation.</p><p>More precisely, we endowed each neuron with a decoding vector , <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="normal">Θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="normal">Θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>&lt;</mml:mo><mml:msub><mml:mi mathvariant="normal">Θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf112"><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> equals three for high gain neurons and nine for low gain neurons and <inline-formula><mml:math id="inf113"><mml:msub><mml:mi mathvariant="normal">Θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the preferred orientation of neuron <inline-formula><mml:math id="inf114"><mml:mi>i</mml:mi></mml:math></inline-formula>. Feedforward inputs correspond to two time-varying inputs, <inline-formula><mml:math id="inf115"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the stimulus magnitude and <inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the stimulus orientation at time <inline-formula><mml:math id="inf118"><mml:mi>t</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf119"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>θ</mml:mi><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>). The orientation estimate , <inline-formula><mml:math id="inf120"><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is decoded from the population as<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The spiking threshold includes an additional term, <inline-formula><mml:math id="inf121"><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, that ensures that neurons with opposing preferences will not be activated to spike so easily by the excitation from opposing neurons.</p><p>Tuning curves in <xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig7">7</xref> were generated by presenting the network with a full range of stimulus orientations Each orientation was presented for 250 ms after reinitializing the network. Neuron responses were centered on their preferred orientation and the mean was taken for each subpopulation. Tuning curves after adaptation were made by lining up neuron responses to an adapting stimulus that corresponded with its preferred orientation. Adapting stimuli were presented for 1.5 s. Standard deviations were computed on these centered data. The random gain network was identical to the above with the exception that the feedforward weight gains, <inline-formula><mml:math id="inf122"><mml:mi>γ</mml:mi></mml:math></inline-formula>, were randomly selected from a uniform distribution with values ranging from 3 to 9.</p><p>The tilt illusion was generated by presenting the network with an adaptor orientation (duration of 2 s) and a subsequent test orientation (250 ms). The perceived angle was decoded from the mean network output over the 250 ms presentation of the test stimulus. The adaptor had a stimulus magnitude of 25 while the test stimulus had a magnitude of 5.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Validation, Investigation, Visualization, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Methodology, Writing—original draft, Writing—review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.46926.011</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-46926-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Source code for model frameworks and all data generated therefrom have been provided for all figures in our GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/gabrielle9/BalancedNetworkAdaptation">https://github.com/gabrielle9/BalancedNetworkAdaptation</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/BalancedNetworkAdaptation">https://github.com/elifesciences-publications/BalancedNetworkAdaptation</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adibi</surname> <given-names>M</given-names></name><name><surname>McDonald</surname> <given-names>JS</given-names></name><name><surname>Clifford</surname> <given-names>CW</given-names></name><name><surname>Arabzadeh</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Adaptation improves neural coding efficiency despite increasing correlations in variability</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>2108</fpage><lpage>2120</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3449-12.2013</pub-id><pub-id pub-id-type="pmid">23365247</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname> <given-names>HB</given-names></name></person-group><year iso-8601-date="1961">1961</year><chapter-title>Possible principles underlying the transformation of sensory messages</chapter-title><person-group person-group-type="editor"><name><surname>Rosenblith</surname> <given-names>WA</given-names></name></person-group><source>Sensory Communication</source><publisher-name>MIT Press</publisher-name><fpage>217</fpage><lpage>234</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname> <given-names>HB</given-names></name><name><surname>Hill</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Evidence for a physiological explanation of the waterfall phenomenon and figural after-effects</article-title><source>Nature</source><volume>200</volume><fpage>1345</fpage><lpage>1347</lpage><pub-id pub-id-type="doi">10.1038/2001345a0</pub-id><pub-id pub-id-type="pmid">14098503</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benucci</surname> <given-names>A</given-names></name><name><surname>Ringach</surname> <given-names>DL</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Coding of stimulus sequences by population responses in visual cortex</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1317</fpage><lpage>1324</lpage><pub-id pub-id-type="doi">10.1038/nn.2398</pub-id><pub-id pub-id-type="pmid">19749748</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blakemore</surname> <given-names>C</given-names></name><name><surname>Campbell</surname> <given-names>FW</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>On the existence of neurones in the human visual system selectively sensitive to the orientation and size of retinal images</article-title><source>The Journal of Physiology</source><volume>203</volume><fpage>237</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1969.sp008862</pub-id><pub-id pub-id-type="pmid">5821879</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boerlin</surname> <given-names>M</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name><name><surname>Denève</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003258</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003258</pub-id><pub-id pub-id-type="pmid">24244113</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borst</surname> <given-names>A</given-names></name><name><surname>Flanagin</surname> <given-names>VL</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Adaptation without parameter change: dynamic gain control in motion detection</article-title><source>PNAS</source><volume>102</volume><fpage>6172</fpage><lpage>6176</lpage><pub-id pub-id-type="doi">10.1073/pnas.0500491102</pub-id><pub-id pub-id-type="pmid">15833815</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brenner</surname> <given-names>N</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>de Ruyter van Steveninck</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Adaptive rescaling maximizes information transmission</article-title><source>Neuron</source><volume>26</volume><fpage>695</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)81205-2</pub-id><pub-id pub-id-type="pmid">10896164</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Large-scale recording of neuronal ensembles</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>446</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1038/nn1233</pub-id><pub-id pub-id-type="pmid">15114356</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chopin</surname> <given-names>A</given-names></name><name><surname>Mamassian</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Predictive properties of visual adaptation</article-title><source>Current Biology</source><volume>22</volume><fpage>622</fpage><lpage>626</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.02.021</pub-id><pub-id pub-id-type="pmid">22386314</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clifford</surname> <given-names>CW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The tilt illusion: phenomenology and functional implications</article-title><source>Vision Research</source><volume>104</volume><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2014.06.009</pub-id><pub-id pub-id-type="pmid">24995379</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dragoi</surname> <given-names>V</given-names></name><name><surname>Sharma</surname> <given-names>J</given-names></name><name><surname>Sur</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Adaptation-induced plasticity of orientation tuning in adult visual cortex</article-title><source>Neuron</source><volume>28</volume><fpage>287</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)00103-3</pub-id><pub-id pub-id-type="pmid">11087001</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Druckmann</surname> <given-names>S</given-names></name><name><surname>Chklovskii</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neuronal circuits underlying persistent representations despite time varying activity</article-title><source>Current Biology</source><volume>22</volume><fpage>2095</fpage><lpage>2103</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.08.058</pub-id><pub-id pub-id-type="pmid">23084992</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname> <given-names>AL</given-names></name><name><surname>Lewen</surname> <given-names>GD</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>de Ruyter Van Steveninck</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Efficiency and ambiguity in an adaptive neural code</article-title><source>Nature</source><volume>412</volume><fpage>787</fpage><lpage>792</lpage><pub-id pub-id-type="doi">10.1038/35090500</pub-id><pub-id pub-id-type="pmid">11518957</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The receptive field is dead. long live the receptive field?</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><pub-id pub-id-type="doi">10.1016/j.conb.2014.02.001</pub-id><pub-id pub-id-type="pmid">24618227</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukushima</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Neocognitron: a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</article-title><source>Biological Cybernetics</source><volume>36</volume><fpage>193</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1007/BF00344251</pub-id><pub-id pub-id-type="pmid">7370364</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghisovan</surname> <given-names>N</given-names></name><name><surname>Nemri</surname> <given-names>A</given-names></name><name><surname>Shumikhina</surname> <given-names>S</given-names></name><name><surname>Molotchnikoff</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Long adaptation reveals mostly attractive shifts of orientation tuning in cat primary visual cortex</article-title><source>Neuroscience</source><volume>164</volume><fpage>1274</fpage><lpage>1283</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2009.09.003</pub-id><pub-id pub-id-type="pmid">19747528</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname> <given-names>JJ</given-names></name><name><surname>Radner</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1937">1937</year><article-title>Adaptation, after-effect and contrast in the perception of tilted lines. I. quantitative studies</article-title><source>Journal of Experimental Psychology</source><volume>20</volume><fpage>453</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1037/h0059826</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graupner</surname> <given-names>M</given-names></name><name><surname>Reyes</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Synaptic input correlations leading to membrane potential decorrelation of spontaneous activity in cortex</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>15075</fpage><lpage>15085</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0347-13.2013</pub-id><pub-id pub-id-type="pmid">24048838</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname> <given-names>S</given-names></name><name><surname>MacLeod</surname> <given-names>DI</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Orientation-selective adaptation and tilt after-effect from invisible patterns</article-title><source>Nature</source><volume>411</volume><fpage>473</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1038/35078072</pub-id><pub-id pub-id-type="pmid">11373679</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hollmann</surname> <given-names>V</given-names></name><name><surname>Lucks</surname> <given-names>V</given-names></name><name><surname>Kurtz</surname> <given-names>R</given-names></name><name><surname>Engelmann</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adaptation-induced modification of motion selectivity tuning in visual tectal neurons of adult zebrafish</article-title><source>Journal of Neurophysiology</source><volume>114</volume><fpage>2893</fpage><lpage>2902</lpage><pub-id pub-id-type="doi">10.1152/jn.00568.2015</pub-id><pub-id pub-id-type="pmid">26378206</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname> <given-names>S</given-names></name><name><surname>Lundstrom</surname> <given-names>BN</given-names></name><name><surname>Fairhall</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Intrinsic gain modulation and adaptive neural coding</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000119</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000119</pub-id><pub-id pub-id-type="pmid">18636100</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosoya</surname> <given-names>T</given-names></name><name><surname>Baccus</surname> <given-names>SA</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Dynamic predictive coding by the retina</article-title><source>Nature</source><volume>436</volume><fpage>71</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1038/nature03689</pub-id><pub-id pub-id-type="pmid">16001064</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeyabalaratnam</surname> <given-names>J</given-names></name><name><surname>Bharmauria</surname> <given-names>V</given-names></name><name><surname>Bachatene</surname> <given-names>L</given-names></name><name><surname>Cattan</surname> <given-names>S</given-names></name><name><surname>Angers</surname> <given-names>A</given-names></name><name><surname>Molotchnikoff</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Adaptation shifts preferred orientation of tuning curve in the mouse visual cortex</article-title><source>PLOS ONE</source><volume>8</volume><fpage>e64294</fpage><lpage>e64298</lpage><pub-id pub-id-type="doi">10.1371/journal.pone.0064294</pub-id><pub-id pub-id-type="pmid">23717586</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname> <given-names>DZ</given-names></name><name><surname>Dragoi</surname> <given-names>V</given-names></name><name><surname>Sur</surname> <given-names>M</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Tilt aftereffect and adaptation-induced changes in orientation tuning in visual cortex</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>4038</fpage><lpage>4050</lpage><pub-id pub-id-type="doi">10.1152/jn.00571.2004</pub-id><pub-id pub-id-type="pmid">16135549</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname> <given-names>DB</given-names></name><name><surname>Baccus</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Coordinated dynamic encoding in the retina using opposing forms of plasticity</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1317</fpage><lpage>1322</lpage><pub-id pub-id-type="doi">10.1038/nn.2906</pub-id><pub-id pub-id-type="pmid">21909086</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname> <given-names>DB</given-names></name><name><surname>Baccus</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Insights from the retina into the diverse and general computations of adaptation, detection, and prediction</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>63</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.11.012</pub-id><pub-id pub-id-type="pmid">24709602</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>A simple coding procedure enhances a neuron's Information Capacity</article-title><source>Zeitschrift Für Naturforschung C</source><volume>36</volume><fpage>910</fpage><lpage>912</lpage><pub-id pub-id-type="doi">10.1515/znc-1981-9-1040</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Haffner</surname> <given-names>P</given-names></name><name><surname>Bottou</surname> <given-names>L</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1999">1999</year><chapter-title>Object Recognition with Gradient-Based Learning</chapter-title><source>Shape, Contour and Grouping in Computer Vision</source><publisher-loc>Berlin</publisher-loc><publisher-name>Springer</publisher-name><fpage>319</fpage><lpage>345</lpage></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maffei</surname> <given-names>L</given-names></name><name><surname>Fiorentini</surname> <given-names>A</given-names></name><name><surname>Bisti</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Neural correlate of perceptual adaptation to gratings</article-title><source>Science</source><volume>182</volume><fpage>1036</fpage><lpage>1038</lpage><pub-id pub-id-type="doi">10.1126/science.182.4116.1036</pub-id><pub-id pub-id-type="pmid">4748674</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>May</surname> <given-names>KA</given-names></name><name><surname>Zhaoping</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Efficient coding theory predicts a tilt aftereffect from viewing untilted patterns</article-title><source>Current Biology</source><volume>26</volume><fpage>1571</fpage><lpage>1576</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.04.037</pub-id><pub-id pub-id-type="pmid">27291055</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mease</surname> <given-names>RA</given-names></name><name><surname>Famulare</surname> <given-names>M</given-names></name><name><surname>Gjorgjieva</surname> <given-names>J</given-names></name><name><surname>Moody</surname> <given-names>WJ</given-names></name><name><surname>Fairhall</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Emergence of adaptive computation by single neurons in the developing cortex</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>12154</fpage><lpage>12170</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3263-12.2013</pub-id><pub-id pub-id-type="pmid">23884925</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Młynarski</surname> <given-names>WF</given-names></name><name><surname>Hermundstad</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Adaptive coding for dynamic sensory inference</article-title><source>eLife</source><volume>7</volume><elocation-id>e32055</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32055</pub-id><pub-id pub-id-type="pmid">29988020</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohar</surname> <given-names>B</given-names></name><name><surname>Katz</surname> <given-names>Y</given-names></name><name><surname>Lampl</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Opposite adaptive processing of stimulus intensity in two major nuclei of the somatosensory brainstem</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>15394</fpage><lpage>15400</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1886-13.2013</pub-id><pub-id pub-id-type="pmid">24068807</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagel</surname> <given-names>KI</given-names></name><name><surname>Doupe</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Temporal processing and adaptation in the songbird auditory forebrain</article-title><source>Neuron</source><volume>51</volume><fpage>845</fpage><lpage>859</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.08.030</pub-id><pub-id pub-id-type="pmid">16982428</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nemri</surname> <given-names>A</given-names></name><name><surname>Ghisovan</surname> <given-names>N</given-names></name><name><surname>Shumikhina</surname> <given-names>S</given-names></name><name><surname>Molotchnikoff</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Adaptive behavior of neighboring neurons during adaptation-induced plasticity of orientation tuning in VI</article-title><source>BMC Neuroscience</source><volume>10</volume><elocation-id>147</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-10-147</pub-id><pub-id pub-id-type="pmid">20003453</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nirenberg</surname> <given-names>S</given-names></name><name><surname>Bomash</surname> <given-names>I</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Victor</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Heterogeneous response dynamics in retinal ganglion cells: the interplay of predictive coding and adaptation</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>3184</fpage><lpage>3194</lpage><pub-id pub-id-type="doi">10.1152/jn.00878.2009</pub-id><pub-id pub-id-type="pmid">20357061</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okun</surname> <given-names>M</given-names></name><name><surname>Steinmetz</surname> <given-names>N</given-names></name><name><surname>Cossell</surname> <given-names>L</given-names></name><name><surname>Iacaruso</surname> <given-names>MF</given-names></name><name><surname>Ko</surname> <given-names>H</given-names></name><name><surname>Barthó</surname> <given-names>P</given-names></name><name><surname>Moore</surname> <given-names>T</given-names></name><name><surname>Hofer</surname> <given-names>SB</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Diverse coupling of neurons to populations in sensory cortex</article-title><source>Nature</source><volume>521</volume><fpage>511</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1038/nature14273</pub-id><pub-id pub-id-type="pmid">25849776</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname> <given-names>CA</given-names></name><name><surname>Wissig</surname> <given-names>SC</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Distinct effects of brief and prolonged adaptation on orientation tuning in primary visual cortex</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>532</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3345-12.2013</pub-id><pub-id pub-id-type="pmid">23303933</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblatt</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1958">1958</year><article-title>The perceptron: a probabilistic model for information storage and organization in the brain</article-title><source>Psychological Review</source><volume>65</volume><fpage>386</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1037/h0042519</pub-id><pub-id pub-id-type="pmid">13602029</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname> <given-names>O</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Perceptual organization in the tilt illusion</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.1167/9.4.19</pub-id><pub-id pub-id-type="pmid">19757928</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seriès</surname> <given-names>P</given-names></name><name><surname>Stocker</surname> <given-names>AA</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Is the homunculus &quot;aware&quot; of sensory adaptation?</article-title><source>Neural Computation</source><volume>21</volume><fpage>3271</fpage><lpage>3304</lpage><pub-id pub-id-type="doi">10.1162/neco.2009.09-08-869</pub-id><pub-id pub-id-type="pmid">19686064</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smirnakis</surname> <given-names>SM</given-names></name><name><surname>Berry</surname> <given-names>MJ</given-names></name><name><surname>Warland</surname> <given-names>DK</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Adaptation of retinal processing to image contrast and spatial scale</article-title><source>Nature</source><volume>386</volume><fpage>69</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1038/386069a0</pub-id><pub-id pub-id-type="pmid">9052781</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solomon</surname> <given-names>SG</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Moving sensory adaptation beyond suppressive effects in single neurons</article-title><source>Current Biology</source><volume>24</volume><fpage>R1012</fpage><lpage>R1022</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.09.001</pub-id><pub-id pub-id-type="pmid">25442850</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spratling</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Predictive coding as a model of response properties in cortical area V1</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>3531</fpage><lpage>3543</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4911-09.2010</pub-id><pub-id pub-id-type="pmid">20203213</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wainwright</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Visual adaptation as optimal information transmission</article-title><source>Vision Research</source><volume>39</volume><fpage>3960</fpage><lpage>3974</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(99)00101-7</pub-id><pub-id pub-id-type="pmid">10748928</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wark</surname> <given-names>B</given-names></name><name><surname>Lundstrom</surname> <given-names>BN</given-names></name><name><surname>Fairhall</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sensory adaptation</article-title><source>Current Opinion in Neurobiology</source><volume>17</volume><fpage>423</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.07.001</pub-id><pub-id pub-id-type="pmid">17714934</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wark</surname> <given-names>B</given-names></name><name><surname>Fairhall</surname> <given-names>A</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Timescales of inference in visual adaptation</article-title><source>Neuron</source><volume>61</volume><fpage>750</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.019</pub-id><pub-id pub-id-type="pmid">19285471</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webster</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Adaptation and visual coding</article-title><source>Journal of Vision</source><volume>11</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1167/11.5.3</pub-id><pub-id pub-id-type="pmid">21602298</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname> <given-names>B</given-names></name><name><surname>Wang</surname> <given-names>GI</given-names></name><name><surname>Dean</surname> <given-names>I</given-names></name><name><surname>Delgutte</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dynamic range adaptation to sound level statistics in the auditory nerve</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>13797</fpage><lpage>13808</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5610-08.2009</pub-id><pub-id pub-id-type="pmid">19889991</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wissig</surname> <given-names>SC</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The influence of surround suppression on adaptation effects in primary visual cortex</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>3370</fpage><lpage>3384</lpage><pub-id pub-id-type="doi">10.1152/jn.00739.2011</pub-id><pub-id pub-id-type="pmid">22423001</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.46926.013</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Population adaptation in efficient balanced networks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This is a little jewel of a paper. It shows how adaptation in neurons and excitation-inhibition balance in the network can be viewed as arising from an optimisation principle. The simple cost function to be minimised is the sum of a term representing the error in signal transmission and one representing the metabolic cost of neuronal firing. The authors describe simple examples, working up to a model for orientation tuning, where they show how it naturally explains the main features of some classic observed perceptual adaptation effects. They show how understanding the phenomenon requires considering the whole local network, not just single neurons. Clearly and elegantly written, the paper was a pleasure to read, and it lays a good foundation for both further theoretical elaboration and experimental investigations.</p><p>Note that some of these are suggestions, designed to improve the paper; the authors should use their best judgment as to whether or not to include them.</p><p>1) After Equation 2, δ<sub>j</sub> should be δ<sub>ij</sub>. (It doesn't make sense to have δ<sub>j</sub> depend on i.)</p><p>2) We would suggest taking some of the inline equations and making them displayed. In particular, the equations for ŝ, r<sub>i</sub>, f<sub>i</sub>, g<sub>i</sub> and Ω<sub>ij</sub>. As a reader, we find it much easier to find relevant variables if we don't have to crawl through lines of text.</p><p>3) The same applies to the inline equations in Materials and methods (e.g., o<sub>i</sub>).</p><p>4) I would also suggest putting just a little more detail of the derivation in the main text. I know these equations have been derived in a number of places, but for those who haven't memorized the derivations, even the Materials and methods will be tough to follow (especially since you switch to the multi-stimulus case). Why not just say, in the main text,</p><p>Neuron i spikes when</p><p>(s(t) – ŝ(t) – w_i)<sup>2</sup> + mu sum<sub>j</sub> (f<sub>j</sub> + δ<sub>ij</sub>)<sup>2</sup> &lt;</p><p>(s(t) – ŝ(t))^2 + mu sum<sub>j</sub> f<sub>j</sub><sup>2</sup>.</p><p>Define</p><p>V<sub>i</sub> = g<sub>i</sub> (sum<sub>j</sub> w<sub>j</sub> (s<sub>j</sub> – ŝ<sub>j</sub>) – mu f<sub>i</sub>),</p><p>with a spike emitted when V<sub>i</sub> &gt; 1/2, at which point it is reset to -1/2. That, and a small amount of algebra, gives you Equation 2.</p><p>5) Along the same lines, we would strongly suggest that the authors expand the description in the network model section a bit. I think many readers would find the path from Equation 3 to Equation 5 a bit magical. I know the details are there in the 2013 Boerlin et al. paper, but, because of the importance of the present paper, I think filling them in here would complete the story presented here and make its message more accessible.</p><p>6) Materials and methods has a fair number of typos (we think): it's a mix of the one stimulus case (as in the main text) and the multi-stimulus case. Please check very carefully. The things we noticed:</p><p>- above Equation 3: W<sub>ij</sub> should be w<sub>ij</sub>.</p><p>- Equation 3: r<sub>i</sub> should be f<sub>i</sub>.</p><p>- one of the w's should be a transpose.</p><p>7) It would help to use Greek letters to label stimulus.</p><p><italic>Reviewer #1:</italic></p><p>While neuronal adaptation is useful for a number of reasons, it would seem to make it hard for downstream neurons to decode responses, since they would have to know the state of adaptation. The authors provide an elegant solution: they write down a cost function that explicitly takes metabolic cost into account, then apply a technique that Sophie Deneve pioneered about a decade ago to derive the optimal network. Through magic that to this day I don't fully understand, everything works, and downstream neurons don't have to know anything about the state of adaptation to decode near optimally. On top of that, an explanation of the famous tilt illusion falls naturally out of their formalism -- something that is hard to explain by Bayesian methods.</p><p><italic>Reviewer #2:</italic></p><p>This is a little jewel of a paper. It shows how adaptation in neurons and excitation-inhibition balance in the network can be viewed as arising from an optimisation principle. The simple cost function to be minimised is the sum of a term representing the error in signal transmission and one representing the metabolic cost of neuronal firing. The authors describe simple examples, working up to a model for orientation tuning, where they show how it naturally explains the main features of some classic observed perceptual adaptation effects. They show how understanding the phenomenon requires considering the whole local network, not just single neurons. Clearly and elegantly written, the paper was a pleasure to read, and it lays a good foundation for both further theoretical elaboration and experimental investigations.</p><p>I would only ask the authors to expand the description in the network model section a bit. I think many readers would find the path from Equation 3 to Equation 5 a bit magical. I know the details are there in the 2013 Boerlin et al. paper, but, because of the importance of the present paper, I think filling them in here would complete the story presented here and make its message more accessible.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.46926.014</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Summary:</p><p>This is a little jewel of a paper. It shows how adaptation in neurons and excitation-inhibition balance in the network can be viewed as arising from an optimisation principle. The simple cost function to be minimised is the sum of a term representing the error in signal transmission and one representing the metabolic cost of neuronal firing. The authors describe simple examples, working up to a model for orientation tuning, where they show how it naturally explains the main features of some classic observed perceptual adaptation effects. They show how understanding the phenomenon requires considering the whole local network, not just single neurons. Clearly and elegantly written, the paper was a pleasure to read, and it lays a good foundation for both further theoretical elaboration and experimental investigations.</p><p>Note that some of these are suggestions, designed to improve the paper; the authors should use their best judgment as to whether or not to include them.</p></disp-quote><p><italic>1) After Equation 2,</italic> δ<italic><sub>j</sub> should be</italic> δ<italic><sub>ij</sub>. (It doesn't make sense to have</italic> δ<italic><sub>j</sub> depend on i.)</italic></p><p>We have made that change to agree with the convention.</p><p><italic>2) We would suggest taking some of the inline equations and making them displayed. In particular, the equations for</italic> ŝ<italic>, r<sub>i</sub>, f<sub>i</sub>, g<sub>i</sub> and</italic> Ω<italic><sub>ij</sub>. As a reader, we find it much easier to find relevant variables if we don't have to crawl through lines of text.</italic></p><p>These equations are now displayed on their own line.</p><disp-quote content-type="editor-comment"><p>3) The same applies to the inline equations in Materials and methods (e.g., o<sub>i</sub>).</p></disp-quote><p>Important equations in Materials and methods are now displayed.</p><disp-quote content-type="editor-comment"><p>4) I would also suggest putting just a little more detail of the derivation in the main text. I know these equations have been derived in a number of places, but for those who haven't memorized the derivations, even the Materials and methods will be tough to follow (especially since you switch to the multi-stimulus case). Why not just say, in the main text,</p><p>Neuron i spikes when</p></disp-quote><p><italic>(s(t) –</italic> ŝ <italic>(t) – w_i)<sup>2</sup> + mu sum<sub>j</sub> (f<sub>j</sub> +</italic> δ<italic><sub>ij</sub>)<sup>2</sup> &lt;</italic></p><p><italic>(s(t) –</italic> ŝ <italic>(t))^2 + mu sum<sub>j</sub> f<sub>j</sub><sup>2</sup>.</italic></p><disp-quote content-type="editor-comment"><p>Define</p><p>V<sub>i</sub> = g<sub>i</sub> (sum<sub>j</sub> w<sub>j</sub> (s<sub>j</sub> – ŝ<sub>j</sub>) – mu f<sub>i</sub>),</p><p>with a spike emitted when V<sub>i</sub> &gt; 1/2, at which point it is reset to -1/2. That, and a small amount of algebra, gives you Equation 2.</p></disp-quote><p>We have included more detail in the Results section and have elaborated on the derivation in the Materials and methods.</p><disp-quote content-type="editor-comment"><p>5) Along the same lines, we would strongly suggest that the authors expand the description in the network model section a bit. I think many readers would find the path from Equation 3 to Equation 5 a bit magical. I know the details are there in the 2013 Boerlin et al. paper, but, because of the importance of the present paper, I think filling them in here would complete the story presented here and make its message more accessible.</p></disp-quote><p>The derivation for the network model has been expanded, particularly in the Materials and methods section.</p><disp-quote content-type="editor-comment"><p>6) Materials and methods has a fair number of typos (we think): it's a mix of the one stimulus case (as in the main text) and the multi-stimulus case. Please check very carefully.</p></disp-quote><p>Materials and methods section (and remainder of manuscript) was checked for typos and other errors.</p><disp-quote content-type="editor-comment"><p>The things we noticed:</p><p>- above Equation 3: W<sub>ij</sub> should be w<sub>ij</sub>.</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>- Equation 3: r<sub>i</sub> should be f<sub>i</sub>.</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>- one of the w's should be a transpose.</p></disp-quote><p>We used the summation notation instead of the linear algebra expression here.</p><disp-quote content-type="editor-comment"><p>7) It would help to use Greek letters to label stimulus.</p></disp-quote><p>The stimulus is now labeled with the Greek letter phi.</p></body></sub-article></article>