<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">18205</article-id><article-id pub-id-type="doi">10.7554/eLife.18205</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Transformation of temporal sequences in the zebra finch auditory system</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-60228"><name><surname>Lim</surname><given-names>Yoonseob</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-59693"><name><surname>Lagoy</surname><given-names>Ryan</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-19576"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5096-5914</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-58741"><name><surname>Gardner</surname><given-names>Timothy J</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1744-3970</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Cognitive and Neural Systems</institution>, <institution>Boston University</institution>, <addr-line><named-content content-type="city">Boston</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Convergence Research Center for Diagnosis, Treatment, and Care System for Dementia</institution>, <institution>Korea Institute of Science and Technology</institution>, <addr-line><named-content content-type="city">Seoul</named-content></addr-line>, <country>Korea</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Electrical and Computer Engineering</institution>, <institution>Boston University</institution>, <addr-line><named-content content-type="city">Boston</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Biomedical Engineering</institution>, <institution>Boston University</institution>, <addr-line><named-content content-type="city">Boston</named-content></addr-line>, <country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Department of Biology</institution>, <institution>Boston University</institution>, <addr-line><named-content content-type="city">Boston</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Raymond</surname><given-names>Jennifer L</given-names></name><role>Reviewing editor</role><aff id="aff6"><institution>Stanford University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>timothyg@bu.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>29</day><month>11</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e18205</elocation-id><history><date date-type="received"><day>26</day><month>05</month><year>2016</year></date><date date-type="accepted"><day>22</day><month>11</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Lim et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Lim et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-18205-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.18205.001</object-id><p>This study examines how temporally patterned stimuli are transformed as they propagate from primary to secondary zones in the thalamorecipient auditory pallium in zebra finches. Using a new class of synthetic click stimuli, we find a robust mapping from temporal sequences in the primary zone to distinct population vectors in secondary auditory areas. We tested whether songbirds could discriminate synthetic click sequences in an operant setup and found that a robust behavioral discrimination is present for click sequences composed of intervals ranging from 11 ms to 40 ms, but breaks down for stimuli composed of longer inter-click intervals. This work suggests that the analog of the songbird auditory cortex transforms temporal patterns to sequence-selective population responses or ‘spatial codes', and that these distinct population responses contribute to behavioral discrimination of temporally complex sounds.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.001">http://dx.doi.org/10.7554/eLife.18205.001</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>zebra finch</kwd><kwd>temporal sequence processing</kwd><kwd>auditory</kwd><kwd>songbird</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>National Research Council of Science and Technology</institution></institution-wrap></funding-source><award-id>CRC-15-04-KIST</award-id><principal-award-recipient><name><surname>Lim</surname><given-names>Yoonseob</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003621</institution-id><institution>Ministry of Science, ICT and Future Planning</institution></institution-wrap></funding-source><award-id>Institute for Information and Communications Technology Promotion R&amp;D program, R0126-16-1119</award-id><principal-award-recipient><name><surname>Lim</surname><given-names>Yoonseob</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>NSF OMA-0835976</award-id><principal-award-recipient><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><name><surname>Gardner</surname><given-names>Timothy J</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NIH R01NS089679</award-id><principal-award-recipient><name><surname>Gardner</surname><given-names>Timothy J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Songbirds discriminate synthetic sounds composed of temporal patterns of clicks, which they transform into distinct ensemble or spatial patterns in successive stages of neural auditory processing.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A highly developed auditory network supports auditory-vocal behavior in songbirds. The core of the auditory processing system consists of anatomical areas named Field L, NCM (caudomedial nidopallium), and CM (caudomedial mesopalium) (<xref ref-type="bibr" rid="bib46">Vates et al., 1996</xref>) (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). These areas and other associated auditory areas are directly and indirectly connected with the song motor pathway (<xref ref-type="bibr" rid="bib46">Vates et al., 1996</xref>; <xref ref-type="bibr" rid="bib27">Mandelblat-Cerf et al., 2013</xref>). Field L, the primary thalamorecipient area, is composed of four different sub-regions (L2a, L2b, L1, and L3) that are reciprocally connected (<xref ref-type="bibr" rid="bib46">Vates et al., 1996</xref>). Among these sub-regions, L2a receives the strongest input from the core of Ov (nucleus ovoidalis), the primary auditory thalamus (<xref ref-type="bibr" rid="bib30">Müller and Leppelsack, 1985</xref>; <xref ref-type="bibr" rid="bib39">Rübsamen and Dörrscheidt, 1986</xref>; <xref ref-type="bibr" rid="bib20">Hose et al., 1987</xref>). Secondary auditory areas — L2b, L3, and L1 — receive feed-forward input from L2a and thalamus, but also receive feedback from higher cortical areas such as CM. These hierarchically and reciprocally connected auditory areas are thought to be analogous to the early stages of mammalian auditory cortex, but the details of the homologies remain a subject of debate (<xref ref-type="bibr" rid="bib21">Jarvis et al., 2005</xref>; <xref ref-type="bibr" rid="bib47">Wang et al., 2010</xref>; <xref ref-type="bibr" rid="bib8">Calabrese and Woolley, 2015</xref>).<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.18205.002</object-id><label>Figure 1.</label><caption><title>Neural responses in primary and secondary auditory areas to birdsongs.</title><p>(<bold>a</bold>) Example of neural responses in primary (blue) and secondary auditory areas (red and black) to birdsongs. Syllable responses were extracted from playback of whole songs. Individual cells in this figure were recorded in different birds. Numbers on the right correspond to the bird indices shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. Cells in the primary auditory area, L2a, respond more synchronously than cells in the secondary area. Red and black colors in the raster denote two classes of cells in secondary auditory areas defined by spike-width. (For red, spike width is less than 250 µs , and for black, greater than 250 µs.) The scale bar is 50 ms. (<bold>b</bold>) Sagittal section located at 1.5 mm lateral of the midline with estimated electrode shank positions (dotted white line). Physiological locations are confirmed by the anatomy (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). (<bold>c</bold>) Schematic of a sagittal section of male zebra finch brain. (<bold>d</bold>) Response similarity scores between all pairs of cells in the secondary auditory area are lower than similarity scores in the primary auditory area. (Secondary auditory responses to song are more diverse across neurons.)</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.002">http://dx.doi.org/10.7554/eLife.18205.002</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig1-v2"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.003</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Estimated recording location of units.</title><p>Cells were colored by their classification as primary or secondary cells based on response latency and similarity scores (<xref ref-type="fig" rid="fig1">Figures 1d</xref> and <xref ref-type="fig" rid="fig3">3b</xref>). This figure shows that for each bird, the primary and secondary cells were spatially separable, providing independent confirmation that the classification as primary and secondary cortical neurons was accurate. On each graph, estimated spatial positions of primary (blue star) and secondary (red circles) units are shown. Positions were approximated based on the configuration of electrode and recording coordinates.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.003">http://dx.doi.org/10.7554/eLife.18205.003</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig1-figsupp1-v2"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.004</object-id><label>Figure 1—figure supplement 2.</label><caption><title>Song syllable discriminability analysis.</title><p>ROC analysis shows increased discriminability of song syllables in secondary auditory areas, L2b and L3, relative to primary auditory area, L2a (n = 13 syllables).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.004">http://dx.doi.org/10.7554/eLife.18205.004</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig1-figsupp2-v2"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.005</object-id><label>Figure 1—figure supplement 3.</label><caption><title>Peri-stimulus time histogram (PSTH) of song responses.</title><p>The PSTH of neurons in primary auditory area, L2a, reveal synchronous responses to song (bin size: 5 ms). In this figure, the average PSTH of all neurons is shown.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.005">http://dx.doi.org/10.7554/eLife.18205.005</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig1-figsupp3-v2"/></fig></fig-group></p><p>For zebra finches and other songbirds, temporal cues in song provide reliable information about the identity of the singer and are used for perceptual discrimination of songs (<xref ref-type="bibr" rid="bib15">Gentner and Margoliash, 2003</xref>; <xref ref-type="bibr" rid="bib14">Gentner et al., 2006</xref>; <xref ref-type="bibr" rid="bib17">Grace et al., 2003</xref>; <xref ref-type="bibr" rid="bib42">Shaevitz and Theunissen, 2007</xref>). The songbird auditory processing stream is well adapted to this information-processing task and reliably relays temporal information in conspecific song. In the zebra finch auditory system, there are neurons from midbrain to the highest levels of auditory association areas that respond with precise spike times to playback of conspecific song. This is true for both dense-spiking neurons and the highly selective, sparse-firing neurons recently described in the high-level auditory area, NCM (<xref ref-type="bibr" rid="bib41">Schneider and Woolley, 2013</xref>), as well as in the auditory-motor association area, HVC (high vocal center) (<xref ref-type="bibr" rid="bib37">Prather et al., 2008</xref>). Using a spectrotemporal receptive field (STRF) analysis, the effective temporal integration window of neurons in L2a, the first thalamorecipient zone, was observed to be very brief compared with responses one step further from the periphery in areas L1 and L3 (<xref ref-type="bibr" rid="bib22">Kim and Doupe, 2011</xref>). The secondary areas (including L2b, L1, and L3) but not the primary zone (L2a) are recipients of significant feedback from high-order auditory areas (<xref ref-type="bibr" rid="bib46">Vates et al., 1996</xref>). In combination, the results of these few studies suggest that an interesting transformation of temporal sequences could take place between primary and secondary zones in Field L.</p><p>Here, we developed new experimental paradigms to examine how temporally patterned auditory stimuli are transformed in the transition from the primary thalamorecipient zone, L2a, to the secondary auditory processing areas, L2b and L3. We first demonstrate that neurons responding to song in secondary areas, L2b and L3, become less synchronous in their relative response times yet more informative about the identity of specific syllables when compared to those in the primary area, L2a.</p><p>To zero in more closely on the nature of the transformation, we examined responses to a set of simplified auditory stimuli consisting of click sequences. The chosen stimuli were akin to ‘Morse code’ — the sounds differed only in the temporal ordering of intervals between clicks. These intervals were drawn from a distribution similar to the intervals between sub-syllabic acoustic transitions in zebra finch song (<xref ref-type="bibr" rid="bib12">Gardner et al., 2001</xref>; <xref ref-type="bibr" rid="bib4">Amador et al., 2013</xref>). For click sequence listening, a distinctive transformation of auditory responses was found between primary and secondary auditory zones. In the primary zone, each click elicited a similar low-latency response in all recorded neurons, and the structure of this response was largely insensitive to the temporal context of the click. One synapse further from the periphery, in secondary auditory areas, L2b and L3, neurons responded asynchronously and selectively, depending on the temporal context of the click. In effect, temporal sequences are transformed to distinct population vectors in the transition from primary to secondary auditory areas. In this process, temporal patterns come to be represented in a format that could directly form the basis of perceptual discriminations based on simple thresholds.</p><p>We next tested whether songbirds could discriminate different temporal click sequence patterns in an operant-training paradigm. A novel ‘restart-go’ operant paradigm, which we found effective for particularly challenging discrimination tests in zebra finches, was developed for this purpose. Using this training procedure, zebra finches rapidly learned to discriminate click sequences that were composed of song-like intervals. When the stimulus set was slowed by a factor of two, the strength of the temporal to spatial transformation in the secondary auditory was reduced, and there was a corresponding degradation of behavioral discrimination.</p><p>Taken together, these results indicate that the ascending auditory pathway in zebra finches transforms temporal sequences into distinct population vectors. This transformation applied to click sequences consisting of intervals that overlap with sub-syllabic acoustic structure in song, and may provide an important substrate for song perception and discrimination in sub-syllabic time-scales.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>General note: the electrophysiological recordings reported here were gathered using four-shank, 32 channel silicon electrodes. From each bird, we recorded activity simultaneously from the primary thalamorecipient zone in the auditory area, Field L2a, and neighboring auditory areas in L2b and L3 (<xref ref-type="fig" rid="fig1">Figure 1a and b</xref>). All stimuli were presented in an interleaved fashion, and each animal was recorded acutely, with all data gathered in a single session. All data presented in figures and quantified below were gathered from well-sorted single-unit responses — a minority of recordings (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). The only exception to this rule is <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, which includes a few channels of high SNR multi-unit traces that did not satisfy our criterion for single-unit isolation. These traces are marked with an asterisk. For additional details, see Methods.</p><sec id="s2-1"><title>Transformation of song responses in the auditory hierarchy</title><p>We first compared the temporal coding of song in primary (L2a) and secondary auditory areas (L2b and L3) of unanesthetized songbirds. Our intent was not to thoroughly catalog song responses, but rather to calibrate responses in order to design a set of synthetic stimuli that could be used for the remainder of the study. Primary and secondary recording sites were distinguished on the basis of the distinct response profiles found in the two areas (<xref ref-type="fig" rid="fig1">Figures 1a</xref> and 3a). This classification was confirmed by spatial mapping of the recording sites (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), showing that the primary cells were spatially segregated from the secondary neurons. Due to small anatomical and surgical variations and the small scale of the primary zone, this area could not be reliably identified by spatial coordinates alone.</p><p>Precise spike timing could be found in both primary and secondary areas in response to song. Focusing first on responses in the primary auditory area, L2a, we found a surprising degree of response synchrony across neurons and across birds (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). The population peri-stimulus temporal histogram (PSTH) for each song was deeply modulated for neurons in L2a (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>, <xref ref-type="fig" rid="fig2">Figure 2a</xref> is the histogram of inter-peak intervals in this population PSTH). By contrast, neurons in secondary auditory areas, L2b and L3, showed a broader repertoire of response profiles. This increase in the diversity of response timing leads to a decrease in the magnitude of the cross-correlation between the PSTHs of individual neurons in the secondary auditory areas relative to a similar cross-correlation performed in primary area, L2a (<xref ref-type="fig" rid="fig1">Figure 1d</xref>).<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.18205.006</object-id><label>Figure 2.</label><caption><title>Timescales of neural responses in the primary auditory area, L2a.</title><p>(<bold>a</bold>) Interval histogram of peaks in the PSTH of neurons in the primary auditory area, L2a, in response to bird songs. The population PSTH contains intervals distributed from 10 ms to 40 ms. (<bold>b</bold>,<bold>c</bold>) Interval histogram of peaks in the PSTH of neurons in the primary auditory area, L2a, in response to click sequences. For the click patterns, we applied two different timescales for the click intervals. In the first timescale, the click sequence evokes PSTH intervals in the range of 10–40 ms. The slower set of stimuli evokes PSTH intervals in the range of 20–80 ms.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.006">http://dx.doi.org/10.7554/eLife.18205.006</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig2-v2"/></fig></p></sec><sec id="s2-2"><title>Transformation of click-sequence responses in the auditory hierarchy</title><p>Our next objective was to examine whether a similar transformation from synchronous to asynchronous coding could be seen for more elementary stimuli consisting of irregularly spaced clicks. This synthetic stimulus would allow us to probe whether the sequence transformation from the primary to the secondary auditory areas requires complex spectral content. If secondary auditory neurons have more complex or more selective spectral receptive fields, the emergence of asynchronous coding in the secondary auditory areas could be explained on the basis of this acoustic selectivity alone. However, if the transformation from synchronous primary response to asynchronous secondary responses could be reproduced with click trains, the result would indicate that the auditory processing pathways contain intrinsic temporal dynamics that transform temporal sequences independent of spectral selectivity.</p><p>The chosen synthetic stimuli were three seconds long and composed of clicks separated by ten specific inter-click intervals (11, 14, 16, 20, 23, 26, 29, 34, 36 and 40 ms). We chose these intervals on the basis of the timescale of neural responses to birdsongs in L2a (<xref ref-type="fig" rid="fig2">Figure 2a and b</xref>). The inter-peak intervals of the population PSTH in response to these click sequences was similar to inter-peak intervals in response to natural song. In effect, we chose click patterns that, in the primary auditory area, elicited a temporal response that loosely overlapped with the natural song response. We note that the selected inter-click intervals are also similar to intervals between sub-syllabic acoustic transitions found in zebra finch song (<xref ref-type="bibr" rid="bib4">Amador et al., 2013</xref>; <xref ref-type="bibr" rid="bib32">Norton and Scharff, 2016</xref>). For comparison, <xref ref-type="fig" rid="fig2">Figure 2</xref> also shows the L2a PSTH inter-peak interval histogram for click sequences slowed by a factor of two.</p><p>The duration of all ten click intervals summed together is 249 ms. The longer three-second sequences were built from 249 ms blocks, in which each block contains a permutation of the ten click intervals. In some stimuli, the blocks were repeating and in others non-repeating. For all sequences, the stimuli differed only in the ordering of click intervals. On timescales longer than the block duration, the statistical properties of all stimuli were equivalent. The set of stimuli used in this study can be seen in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. (Sample audio files are also provided. See <xref ref-type="supplementary-material" rid="SD3-data">supplementary file 1</xref>).</p><p>Raster plots for single units in primary and secondary auditory areas are shown in <xref ref-type="fig" rid="fig3">Figure 3a</xref>. (Example spike waveforms of single units and corresponding rasters are shown in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>.) Raster plots for the full ensemble of single and multi-units are shown in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, including a breakdown of secondary cells into narrow (red) and broad-spiking (black) neuron waveforms. Only narrow units were found in the primary auditory area. (This figure is the only time in the paper that poorly sorted units, or ‘multi-units’ are included.) A distinct change in the temporal response to click sequences can be found in the transition from primary to secondary areas. In the primary auditory area, the click responses are fairly insensitive to the local context – to first approximation, each click evoked a synchronous, low latency response across channels, whereas secondary auditory areas were characterized by sparser and less synchronous responses that were more sensitive to the sequence context of the click (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplements 4</xref> and <xref ref-type="fig" rid="fig3s5">5</xref>). The click sequence, by definition, contains no significant spectral cues for frequencies above 100 Hz (the shortest interval in the click set was 11 ms, thus below the 100 Hz cutoff). Zebra finch hearing thresholds for pure tones are attenuated by about 20 dB relative to humans at 100 Hz (<xref ref-type="bibr" rid="bib34">Okanoya and Dooling, 1987</xref>; <xref ref-type="bibr" rid="bib29">Moore, 2007</xref>), and the fundamental frequency of conspecific song is typically 500 Hz or higher in zebra finches.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.18205.007</object-id><label>Figure 3.</label><caption><title>Neural responses to click sequences in primary and secondary auditory areas.</title><p>(<bold>a</bold>) Example of neural responses in primary and secondary auditory areas. Units from individual birds are grouped (black vertical bars and corresponding bird indices are shown on the right of the rasters). Red and black rasters mark two classes of cells in secondary auditory areas that are defined by spike-width. For red rasters, the spike width is less than 250 µs and for black, greater than 250 µs). Blue rasters are cells in the primary auditory area, L2a. (<bold>b</bold>) Histogram of cross-correlation scores between the click stimulus and the PSTH response. The discrimination line between two peaks (at 0.5 similarity score) also segregates the cells spatially (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), confirming the classification of neurons as residing in spatially separated areas – either L2a or L3/L2b. </p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.007">http://dx.doi.org/10.7554/eLife.18205.007</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig3-v2"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.008</object-id><label>Figure 3—figure supplement 1.</label><caption><title>All click sequences used for neural recordings and operant training.</title><p>Click sequences were repeating or non-repeating temporal patterns. Each temporal pattern is 249 ms long and the total length of the sequence is 3 s. For sequences 1, 3, 6, 7, 8, and 9, a single fixed temporal pattern repeats 11 times; the other sequences are composed of 11 different non-repeating patterns. We also built some sequences in reverse order (Seq. 1 vs Seq. 3, Seq. 2 vs Seq. 4, Seq. 7 vs Seq. 8). Sequences 1–8 were used for neural recording and sequences 1, 2, and 9 were used for the operant-training experiment. An audio file for each click sequence is provided (<xref ref-type="supplementary-material" rid="SD3-data">Supplementary file 1</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.008">http://dx.doi.org/10.7554/eLife.18205.008</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig3-figsupp1-v2"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.009</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Combined single and multi-unit responses to sequence 1 and sequence 2.</title><p>Responses in the primary auditory area, L2a, are shown in blue and those in secondary areas, L2b/L3, are shown in red and black. Multi-unit responses, as opposed to single-unit responses, are indicated by asterisk marks on the left. Responses from a single bird are grouped by a black vertical bar, with the corresponding bird index on the right. Two different classes of neurons in the secondary auditory areas (red and black rasters) are classified based on the peak-to-peak width of spike waveform following the conventions of <xref ref-type="fig" rid="fig3">Figure 3a</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.009">http://dx.doi.org/10.7554/eLife.18205.009</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig3-figsupp2-v2"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.010</object-id><label>Figure 3—figure supplement 3.</label><caption><title>Example spike waveforms corresponding to click responses shown in raster form.</title><p>Each row of the raster plot represents the single-unit responses to a click sequence (sequence 2); the corresponding spike waveform is shown on the right. The shaded error bars represent the standard deviation of waveforms. Primary L2a neurons are shown in blue. Narrow- and broad-spiking units in L2b or L3 are shown in red and black, respectively.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.010">http://dx.doi.org/10.7554/eLife.18205.010</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig3-figsupp3-v2"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.011</object-id><label>Figure 3—figure supplement 4.</label><caption><title>Population PSTH of neurons in response to click sequences.</title><p>The combined population PSTH of neurons in the primary auditory area, L2a, is deeply modulated, a result of synchronous responses to the click sequence (blue trace, bin size: 5 ms). The combined population PSTH of neurons in secondary areas (L2b and L3) is shown in red. The bottom tick marks show the waveform of the click stimulus (click sequence 1).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.011">http://dx.doi.org/10.7554/eLife.18205.011</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig3-figsupp4-v2"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.012</object-id><label>Figure 3—figure supplement 5.</label><caption><title>Latency of neural responses to click sequences in the primary auditory area, L2a.</title><p>To calculate the latency in the primary auditory area, a click-triggered histogram of single-unit responses is generated. The origin of this plot corresponds to the onset time of each click. The solid line represents the mean latency histogram and the shaded error bar is standard deviation of latency.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.012">http://dx.doi.org/10.7554/eLife.18205.012</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig3-figsupp5-v2"/></fig></fig-group></p><p>As for song responses, the transition from primary to secondary thalamorecipient areas reveals a desynchronizing transformation that maps temporal click sequences onto distinct neuronal ensembles. For the click sequences used here, this transformation is even more apparent than for song responses. The diversification of neuronal responses increases the information about the preceding temporal context of a given click that the population vector contains. To demonstrate this, we computed phase-space trajectories of the population vector in response to click sequences, and then quantified the Euclidean distance between these phase-space trajectories. In this analysis, every neuron recorded defines a direction in a phasespace hypercube, and the average firing rate of the cell defines a position along the respective axis.</p><p>The phase-space trajectory for three cells in the primary auditory area and three cells in the secondary auditory areas during playback of two distinct sequences are shown in <xref ref-type="fig" rid="fig4">Figure 4a</xref>. In the primary auditory area, L2a, the phase-space trajectories of distinct stimuli overlap for all time points, meaning that the pattern of active cells contains little population-vector information that can distinguish the stimuli. By contrast, in secondary auditory areas, specific points in the phase-space trajectory diverge from one another in a stimulus-dependent manner. That is, the pattern of cell responses in secondary auditory areas contains information about one or more intervals preceding the click. To summarize simply – there are particular configurations of active cells that occur only during playback of one stimulus or another — a useful feature for a system that is tuned to make fine discriminations about temporal sequence patterns.<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.18205.013</object-id><label>Figure 4.</label><caption><title>Temporal sequences are transformed to distinct population vectors in the secondary auditory areas, L2b and L3.</title><p>(<bold>a</bold>) For different stimuli, ensemble state-space trajectories are discriminable in secondary auditory areas but not in the primary auditory area, L2a. For each trace, the bin size for the ensemble state space was 5 ms. Each trace is smoothed by rectangular windows (10 ms) for visualization. (<bold>b</bold>) Receiver operating characteristic (ROC) analysis reveals enhanced discriminability of click sequences in secondary auditory areas, L2b and L3, relative to those in the primary auditory area, L2a. </p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.013">http://dx.doi.org/10.7554/eLife.18205.013</ext-link></p><p><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.18205.014</object-id><label>Figure 4—source data 1.</label><caption><title>Source data for ROC curve.</title><p>This zip file contains spike-timing data used for the ROC analysis shown in <xref ref-type="fig" rid="fig4">Figure 4b</xref>. Spike times of 10 different cells recorded in primary or secondary auditory areas are included in folders with corresponding names. For simple visualization of spike rasters, Matlab source code (DataLoad.m) is also provided.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.014">http://dx.doi.org/10.7554/eLife.18205.014</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-18205-fig4-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.015</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Short click-sequence discriminability analysis.</title><p>ROC analysis shows that the sequence discriminability in secondary auditory areas is maintained even when considering only the first 500 ms of the neural response.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.015">http://dx.doi.org/10.7554/eLife.18205.015</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig4-figsupp1-v2"/></fig></fig-group></p><p>To quantify the degree to which the click stimuli can be distinguished on the basis of the neural responses, we defined a simple decoding mechanism based on the population vector of the ensemble response (see Methods for details). In this decoding, the discriminability of the sequence at a particular time is given by the distance in phase space to the nearest trajectory belonging to a different stimulus. The power of this ‘spatial’ code for sequence discrimination is quantified through an ROC (receiver operating characteristic) analysis in <xref ref-type="fig" rid="fig4">Figure 4b</xref>. We analyzed coding in primary and secondary areas using the ROC analysis, using a fixed number of single unit recordings (n = 10) in both cases. In the secondary auditory areas, but not the primary thalamorecipient area, temporal sequences are mapped onto distinct population patterns, revealing a better sequence decoding in the ROC analysis (spike times of the units used in this analysis are also provided in <xref ref-type="supplementary-material" rid="SD1-data">Figure 4—source data 1</xref>). We repeated this analysis for just the first 500 ms of the stimulus, and still found a high degree of sequence discriminability in the secondary auditory areas (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). This shorter analysis is more directly relevant to the behavioral discriminations reported below, as trained birds performing behavioral discriminations typically respond within this time frame (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). To further validate this approach, we applied the same analysis to the PSTH of the song syllable responses (n = 13 syllables, <xref ref-type="fig" rid="fig1">Figure 1a</xref>) and found an increase in syllable discriminability in the secondary auditory area (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Given the rich spectral content of song relative to clicks, the primary area, L2a, already shows a high degree of response selectivity, better than that in the response to the click sequences.</p><p>We next repeated the click electrophysiology using a stimulus set composed of intervals twice as long as those in the first stimulus set (22–80 ms, rather than 11–40 ms, <xref ref-type="fig" rid="fig2">Figure 2c</xref>). This change in stimulus timescale had a minimal impact on spike rate in the secondary auditory cortex (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), but resulted in a significant change in the power of the temporal to spatial transformation. Using the same phase-plane ROC analysis, we found that the timescale dilation led to reduced sequence discrimination in secondary auditory areas (<xref ref-type="fig" rid="fig5">Figure 5</xref>).<fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.18205.016</object-id><label>Figure 5.</label><caption><title>Neural sequence discriminability depends on the timescale of the click sequence.</title><p>ROC analysis reveals that the discriminability of the click sequences is constrained by the interval distribution of the click stimuli. When the sequence is slowed by a factor of two, the discriminability of click sequences is lost in the secondary auditory area (shown in green).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.016">http://dx.doi.org/10.7554/eLife.18205.016</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig5-v2"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.017</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Spike rate of cells in response to click sequences with different timescales.</title><p>Slower click sequences evoke a lower spike rate in primary and secondary auditory areas. For the secondary auditory areas, this reduction in spike rate is relatively small. This analysis was based on data used in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.017">http://dx.doi.org/10.7554/eLife.18205.017</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig5-figsupp1-v2"/></fig></fig-group></p></sec><sec id="s2-3"><title>Behavioral recognition of click sequences</title><p>The preceding electrophysiology experiments demonstrated a transformation of click responses to distinct population vectors in the secondary auditory areas of unanesthetized zebra finches. As a result, areas downstream of secondary auditory areas could, in principle, solve a click-sequence classification problem by applying a simple summation and threshold to subsets of secondary cell inputs. Given the robust transformation of temporal click sequences in zebra finch auditory areas, we next sought to determine whether songbirds could be trained to behaviorally discriminate this class of artificial stimuli, and whether or not properties of the electrophysiological responses correlated with behavioral discriminations.</p><p>Songbirds were trained using a new operant-training procedure developed for this study (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). We call this automated training ‘reset-go'. A detailed description of the training procedure can be found in the Methods. In essence, a bird can demonstrate learning through two behaviors — by interrupting the playback of a non-rewarding stimulus to ‘request’ the reset of an unfavorable trial or by accessing the water port during playback of rewarding stimuli. In all experiments, two sounds were presented — a rewarded stimulus (click sequence 2 from <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) and non-rewarded stimulus (click sequence 1 or 9 from <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Zebra finches in this task were mildly water restricted, and worked for 1–5 μl drops of water, routinely performing a thousand trials in a five-hour training session.</p><p><xref ref-type="fig" rid="fig6">Figure 6a</xref> reveals the time-course of discrimination learning for one bird. Ten days after the initiation of training, this bird would interrupt the playback of the unrewarded stimulus (sequence 1) within three seconds and access the water port while the rewarded stimulus (sequence 2) was presented. <xref ref-type="fig" rid="fig6">Figure 6b</xref> shows summary statistics for learning in eight birds trained to discriminate sequence 1 vs. sequence 2. <xref ref-type="supplementary-material" rid="SD2-data">Figure 6—source data 1</xref> documents the groups of birds trained and <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref> shows the time-course of learning for the various groups. The detailed training procedure is described in the Methods. Within a population of trained birds (n = 53), a majority (n = 35 birds) showed high levels of performance (d’ &gt; 1) within 14 days of training onset, revealing that songbirds could readily learn to discriminate the fast temporal click sequences used in this study.<fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.18205.018</object-id><label>Figure 6.</label><caption><title>Operant training with click sequences.</title><p>(<bold>a</bold>) Example of training by the single-stage behavioral-shaping method. The probability distribution of accessing trial port and water port is illustrated on a log scale. The white dotted line represents the start of sequence playback and the white solid line is the termination time of the stimulus. We show two stimuli back to back with mirrored time axes. Asymmetry between the solid lines in this image indicates learning. Over the course of training, this bird started to interrupt playback of the non-rewarding sequence by accessing the trial port before sequence 1 (the unrewarded sequence) stopped playing. The bird also learned to access the water port selectively during the playback of the rewarded sequence (sequence 2). (<bold>b</bold>) Learning curve for birds exposed to the single-stage training method (n = 8 birds). With the single-stage training method, most birds start to show differentiated responses (d’ is around 1) after two weeks of training; that is, they interrupt and reset sequence 1 playback and access the water port for sequence 2 playback. (<bold>c</bold>) When the click intervals are slowed by a factor of two, all trained birds (n = 11 in the single-stage method) were unable to discriminate the temporal sequences; d’ is around 0.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.018">http://dx.doi.org/10.7554/eLife.18205.018</ext-link></p><p><supplementary-material id="SD2-data"><object-id pub-id-type="doi">10.7554/eLife.18205.019</object-id><label>Figure 6—source data 1.</label><caption><title>Summary of training.</title><p>The success of operant training was determined on the basis of the d-prime score. When d’ is greater than 1, the bird was deemed successful in learning the task. In this table, the number of birds that succeeded in operant training for click sequence discrimination (d’ &gt; 1) out of the total number of birds is shown. For example, 8 out of 10 birds succeeded in two-stage training to discriminate sequence 9 and 2.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.019">http://dx.doi.org/10.7554/eLife.18205.019</ext-link></p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-18205-fig6-data1-v2.docx"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig6-v2"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.020</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Operant training setup.</title><p>There are two infrared switches, a green LED (trial indicator) and a water spout in the training cage. An Arudino microprocessor monitors the timing of port access, plays stimuli, and delivers water rewards. The water reservoir is located 24 inches above the floor of cage. The water valve is opened for a fixed duration, just long enough to produce a drop of water that is consistently 1–5 μl in volume. During operant training, data collected by the Arduino is sent to another computer over Ethernet and analyzed in real-time.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.020">http://dx.doi.org/10.7554/eLife.18205.020</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig6-figsupp1-v2"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.18205.021</object-id><label>Figure 6—figure supplement 2.</label><caption><title>Result of operant training.</title><p>(<bold>a</bold>,<bold>b</bold>,<bold>c</bold>) Two-stage training, example of a bird learning sequence 1. (<bold>a</bold>) The probability distribution of the bird accessing the trial port is shown for the entire training period of the first stage of training (left). The white dotted line represents the start of sequence playback and the white solid line shows the termination time of the stimulus. Any asymmetry between the dotted and solid lines indicates learning (asymmetry implies different behaviors for rewarded and non-rewarded sequences). This bird started to interrupt non-rewarding trials around day 5. Individual rows (specific days in panel (<bold>a</bold>)) are plotted to the right to illustrate detail. (<bold>b</bold>) Learning curve for the first stage. Mean d-prime (± s.d.) after ten days of training is shown (n = 8 birds). (<bold>c</bold>) Learning curve after the passive reward is switched off (the second stage of training). This transition resulted in a minimal change in behavior. (<bold>d</bold>,<bold>e</bold>,<bold>f</bold>) Example of two-stage training for another bird learning a distinct sequence (sequence 9). (<bold>d</bold>) The probability of accessing the trial port during the first stage of training (left) and three sample days (right). (<bold>e</bold>) Learning curve at the first stage (n = 8 birds). (<bold>f</bold>) Learning curve at the second stage (n = 8 birds). (<bold>g</bold>,<bold>h</bold>) Example of two-stage training for a sequence whose intervals were slowed by a factor of two. (<bold>g</bold>) Probability distribution of accessing the trial port during the first stage of training. This bird usually reinitiated trials immediately after the presentation of the click sequence or after drinking water for rewarded trials (note the increased probability of accessing trial port around 10 s). The absence of asymmetry between the dotted and solid lines indicates an absence of learning. (<bold>h</bold>) Learning curve during the first stage of training. No birds (n = 4 birds in two-stage training) learned to discriminate the slowed click sequences over the course of 40 days of training.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.021">http://dx.doi.org/10.7554/eLife.18205.021</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig6-figsupp2-v2"/></fig></fig-group></p></sec><sec id="s2-4"><title>Catch trials probe the nature of auditory discrimination</title><p>To probe the underlying nature of the auditory discrimination, we examined catch trials for two conditions. For time-reversed click stimuli, behavior fell to chance levels (<xref ref-type="fig" rid="fig7">Figure 7a</xref>), indicating that the ordering of the click intervals was critical to the behavioral discrimination. The next test examined cyclic permutations of the training stimuli. Rather than beginning playback at the normal starting interval of each sequence, the cyclic permutation initiated each stimulus at a random click interval in the three second stimulus – effectively a phase shift in the stimulus. For this group of catch trials, a small decrease in performance was found when the cyclic stimulus was first introduced, but within four days, performance returned to baseline (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). The conclusion from this is that the birds are listening for patterned sequences of intervals irrespective of their absolute time of occurrence relative to the onset of the trial.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.18205.022</object-id><label>Figure 7.</label><caption><title>Catch-trial analysis.</title><p>(<bold>a</bold>) During catch-trial analysis, for 10% of non-rewarding trials, we presented reverse patterns to eight birds. The birds did not show any recognition of the reverse pattern (catch trials). Only the familiar non-rewarded sequence led to the adaptive behavior of resetting playback. Mean ± s.d. of trial interruption ratio is shown. (<bold>b</bold>) In this cyclic permutation catch-trial analysis, playback of the click sequence started at a random interval in the repeating sequence on each trial (a phase shift in the stimulus order); all birds (n = 11) maintained performance. This indicates that discriminations were based on patterns of click intervals regardless of the absolute time of any specific click relative to trial onset.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.022">http://dx.doi.org/10.7554/eLife.18205.022</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig7-v2"/></fig></p></sec><sec id="s2-5"><title>Breakdown of behavioral recognition</title><p>Since the click sequence contains no spectral structure above 100 Hz, stretching the click sequence is a manipulation that has no impact on the frequency content of the sound in the spectral range of zebra finch syllables (&gt;500 Hz). We found that birds trained to discriminate fast sequences failed to respond above chance levels when the timescale of the clicks was slowed by a factor of two. The slow sequences were truncated at three seconds to match the original stimulus duration.</p><p>We next examined whether naive birds could learn to discriminate the slower click sequences if they were exclusively trained on the slower sequences from the outset. Eleven birds were trained in a single-stage training and four birds were trained on the first stage of a two-stage training procedure that is also documented in the Methods section. In contrast to the high success rate for faster click sequences, no bird developed a discrimination ability for the slower click sequences (<xref ref-type="fig" rid="fig6">Figure 6c</xref>, <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2g and h</xref>). The ability to discriminate click stimuli was found only for the faster click sequences.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Songbirds form detailed auditory memories for complex songs, and these memories serve to guide imitative vocal learning (<xref ref-type="bibr" rid="bib33">Nottebohm, 1972</xref>; <xref ref-type="bibr" rid="bib6">Brainard and Doupe, 2002</xref>; <xref ref-type="bibr" rid="bib5">Bolhuis and Gahr, 2006</xref>; <xref ref-type="bibr" rid="bib13">Gardner et al., 2005</xref>). In parallel, a range of songbird species can perform at high levels in operant tasks involving song and synthetic stimulus discrimination (<xref ref-type="bibr" rid="bib16">Gess et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Sturdy and Weisman, 2006</xref>; <xref ref-type="bibr" rid="bib9">Cynx and Nottebohm, 1992</xref>; <xref ref-type="bibr" rid="bib40">Scharff et al., 1998</xref>; <xref ref-type="bibr" rid="bib43">Stripling et al., 2003</xref>; <xref ref-type="bibr" rid="bib2">Abe and Watanabe, 2011</xref>). While songbird auditory performance has been well documented, the network mechanisms underlying song discriminations have not been studied. In particular, one of the least understood aspects of auditory sequence processing concerns the transformations applied to complex temporal sequences (<xref ref-type="bibr" rid="bib28">Mauk and Buonomano, 2004</xref>).</p><p>The present study provides insight into the processing of a simple class of temporal sequences composed of irregularly spaced clicks. We find that after the stimulus passes through the primary thalamorecipient zone — L2a, L2b, and L3 — these temporal sequences are transformed into distinct population vectors or ‘spatial codes'. The mapping of temporal patterns to spatial patterns or ensemble codes provides an opportunity for downstream neurons to perform stimulus discrimination using simple linear classifiers to act on the population vector. For the click stimuli used in this study, reliable discriminations could be made on the basis of the distinct population vectors that arise in L2b and L3, binned in 5 ms time bins.</p><p>Operant training revealed that songbirds readily learn to discriminate the Morse-code like click stimuli. The fast-click sequences were behaviorally discriminable with high accuracy for a majority of trained birds. Surprisingly, no animals learned to discriminate click sequences that were slowed by a factor of two, even though secondary auditory areas responded with similar spike rates to the slower stimulus. The slowed sequences evoked inter-peak intervals in primary auditory area PSTH that were longer than the typical intervals between peaks in the PSTH during natural song exposure. We suggest that the ascending auditory pathway in the transition from L2a to L2b and L3 is tuned to process temporal events on the faster timescale (11–40 ms) in a manner that is particularly useful for song memorization and discrimination.</p><p>We mention two caveats in the present study. First, the high-pass cutoff frequency of the loudspeakers was 3 kHz. (High frequency tweeters were used for stimulus delivery limiting the spectral content of each click.) We do not know how the spectral content of the click impacts the behavioral discrimination of the slower sequences. In another prior study, zebra finches were able to discriminate sequences of beeps spaced by intervals of up to 300 ms — intervals much longer than those used in our study (<xref ref-type="bibr" rid="bib45">van der Aa et al., 2015</xref>). It is likely that brief clicks and longer tones tap into auditory processing pathways with distinct temporal dynamics, explaining the performance difference. In addition, many details of the temporal discrimination tasks were different in the two studies, and the distinct results may also relate to these task differences. Additional tests will be needed to determine whether or not the spectral content of each click impacts the behavioral performance. Opportunities also exist to further examine the ability of the zebra finch to generalize temporal pattern recognition through time-dilations (<xref ref-type="bibr" rid="bib31">Nagel et al., 2010</xref>).</p><p>The second caveat is that the single-unit ensembles studied here were ‘virtual ensembles’ recorded in different animals; noise correlations within animals could further impact discrimination in ways that were not addressed here (<xref ref-type="bibr" rid="bib48">Zohary et al., 1994</xref>; <xref ref-type="bibr" rid="bib1">Abbott and Dayan, 1999</xref>). While we did not acquire enough high-quality single-unit data to perform the ROC analysis for individual animals, enough units were recorded simultaneously to reveal the transformation qualitatively from primary to secondary responses in summary raster plots (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplements 2</xref> and <xref ref-type="fig" rid="fig3s3">3</xref>). These rasters support the view that the sequence transformation described for virtual ensembles will also hold for ensembles of neurons in individual birds.</p><p>Much theoretical interest has focused on the question of how brains composed of neurons with short intrinsic timescales can process long-timescale stimuli and generate long-timescale behaviors (<xref ref-type="bibr" rid="bib23">Lashely, 2004</xref>). For temporal stimuli composed of identical units such as clicks, intrinsic cellular or circuit mechanisms must bridge intervals of time from one interval to the next in order to create sequence-specific population responses. To encode the history of the stimulus in the present state of the network, synfire chains, avalanches, or more complex transient dynamics in recurrent networks have all been proposed (<xref ref-type="bibr" rid="bib3">Abeles, 1991</xref>; <xref ref-type="bibr" rid="bib19">Grossberg, 1969</xref>; <xref ref-type="bibr" rid="bib25">Maass et al., 2002</xref>). In other models, persistent currents in single cells bridge intervals of time (<xref ref-type="bibr" rid="bib10">Egorov et al., 2002</xref>). In each of these models, intrinsic dynamics of cortical cells or circuits are used to transfer information about past events into the network responses at a given time.</p><p>One effective way of transferring information about prior events into current responses is through feedback connections. The primary auditory area (L2a) in songbirds reportedly receives no feedback from higher-level auditory zones (<xref ref-type="bibr" rid="bib46">Vates et al., 1996</xref>), and the synchronous, low-latency responses in this region may reflect a feed-forward response to thalamic drive. By contrast, all other areas, including the secondary auditory zones examined here (L2b and L3), are more densely interconnected both with each other and with higher-level auditory areas. This anatomical distinction raises the possibility that L2b and L3, but not the primary auditory area, L2a, can sustain reverberant activity that could underlie the temporal sequence transformation observed in L2b and L3. Relevant theoretical constructs for this model include liquid state machine theories (<xref ref-type="bibr" rid="bib25">Maass et al., 2002</xref>). By way of illustration, <xref ref-type="fig" rid="fig8">Figure 8</xref> reveals the output of a simple reverberant model that recapitulates key features of the observed dynamics. In this case, the model is simply a linear dynamical system driven by click sequences and additive noise, and tick marks represent time points when the vector <inline-formula><mml:math id="inf1"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> crosses arbitrary threshold amplitude.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>M</mml:mi><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:math></disp-formula><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.18205.023</object-id><label>Figure 8.</label><caption><title>Sequence-selective responses in a critically tuned linear dynamical system.</title><p>Each blue row represents simulated neural responses in a simple linear model. The input stimulus (red) has a temporal pattern similar to the click sequences used in this study. This toy model illustrates a temporal to spatial transformation arising from simple linear dynamics in a recurrent system.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.023">http://dx.doi.org/10.7554/eLife.18205.023</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-18205-fig8-v2"/></fig></p><p>In this example, matrix <inline-formula><mml:math id="inf2"><mml:mi>M</mml:mi></mml:math></inline-formula> is a random anti-symmetric matrix, with all imaginary eigenvalues, and <inline-formula><mml:math id="inf3"><mml:mi>η</mml:mi></mml:math></inline-formula> is a random noise term. By choosing the time-constants <inline-formula><mml:math id="inf4"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mi>γ</mml:mi></mml:math></inline-formula> appropriately, the model can produce patterns that resemble spike rasters observed in L2b and L3. <xref ref-type="fig" rid="fig8">Figure 8</xref>, generated by this linear system, simply illustrates the point that even the simplest recurrent dynamical systems have the capability to transform click-sequence information into distinct population vectors when properly tuned. In this example, the anti-symmetric matrix, <inline-formula><mml:math id="inf6"><mml:mi>M</mml:mi></mml:math></inline-formula>, provides a form of ‘critical tuning’ in which multiple oscillatory timescales are equally excitable, providing richer temporal dynamics than would occur for a typical nonsymmetric random connectivity matrix (<xref ref-type="bibr" rid="bib26">Magnasco et al., 2009</xref>). While the hypothesis that recurrence explains the auditory sequence transformation is appealing, new experimental studies are needed to examine the role of recurrent dynamics in temporal sequence processing in the auditory pallium of songbirds.</p><p>While the reverberant models provide an attractive explanation for the sequence transformation observed in L2b and L3, the behavioral discriminations that the birds exhibited here cannot be taken as evidence that supports the reverberant model, strictly speaking. A purely feed-forward counter-hypothesis is that the neurons in secondary auditory areas could demonstrate biophysical integration timescales that solve the sequence discrimination through single-cell properties. To illustrate this hypothesis, we first smoothed the click sequences used for behavior training with three different rectangular windows of timescale <inline-formula><mml:math id="inf7"><mml:mi>T</mml:mi></mml:math></inline-formula> or shorter and built phase-plane traces of these hypothetical ‘smoothing units’ in 3D space. In this case, three different smoothing windows correspond to hypothetical units with different integration timescales. We then analyzed the minimal time-scale <inline-formula><mml:math id="inf8"><mml:mi>T</mml:mi></mml:math></inline-formula> for which the behaviorally trained sequences could be perfectly segregated in the ROC analysis performed earlier for actual neural sequences. From this analysis, we found that phase plane traces used in the behavioral studies can be perfectly separated if the width of the longest rectangular window was 100 ms or greater. To state this more simply, while the sequences presented to the birds were 3 s long, click rates measured in time bins as short as 100 ms provide a population vector that is adequate for sequence discrimination. This 100 ms timescale cannot rule out either feed-forward single-cell biophysics or recurrent dynamics as contributors to the sequence transformation.</p><p>While the circuit mechanisms remain to be established, this study serves to demonstrate both a distinctive transformation of temporal sequences in the transition from L2a to higher-order areas, L2b and L3, and a behavioral capacity of zebra finches to discriminate synthetic click sequences. The transformation of temporal sequences to distinct population vectors may underlie the songbird’s advanced discrimination abilities for temporally structured conspecific song.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All procedures were approved by the Institutional Animal Care and Use Committee of Boston University.</p><sec id="s4-1"><title>In vivo experimental preparation</title><sec id="s4-1-1"><title>Subjects</title><p>For the neural recordings, we examined a total of 11 different adult male zebra finches (<italic>Taeniopygia guttata</italic>).</p></sec><sec id="s4-1-2"><title>Stimulus</title><p>The artificial stimulus set used for electrophysiology consists of nine click sequences with different interval ordering (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Sequence 9 was used only for a subset of operant training tests. The natural song stimulus set consisted of three conspecific songs (n = 13 syllables). During electrophysiological recordings, a neural data acquisition system (RZ-5, Tucker-Davis Technology) triggered a pulse generator to create rectangular pulses (100 µs width) with different intervals, or played the conspecific bird songs. All stimuli were presented in free-field (60~65 dB peak amplitude) over a loudspeaker (bird song) (Companion 2, BOSE Corporation, Framingham, MA, USA) or a tweeter (for clicks) (PLWT4, Pyle Audio, frequency response range: 3–30 kHz).</p></sec><sec id="s4-1-3"><title>Neural recording</title><p>Prior to the electrophysiological recording, the birds were injected with the anti-inflammatory analgesic Meloxicam, via intramuscular injection, and anesthetized (1–2% isoflurane in 0.6–0.8 ℓ/min O<sub>2</sub>) for a preparatory surgical procedure to implant a custom-made head-post. Local scalp anesthetic (bupivicaine) was administered subcutaneously (40 μl, 4 mg/kg) and a small (0.18 g) head plate affixed to the skull through light-bonded dental acrylic. This was attached so that the head could later be held at a fixed 55 degree angle during unanesthetized auditory recordings. After recovery from general anesthetic (two hours), the bird was given a booster dose of bupivicaine along the margins of the scalp, placed in a foam restraint, and transferred to a double-walled sound proof chamber (40A-Series, Industrial Acoustics Company, Bronx, NY, USA), facing a loudspeaker or a tweeter. The sound source was located 25 cm away from the bird beak.</p><p>We used a four-shank multichannel silicon probe (Impedance: 1–2 MΩ, A4×8–5 mm-50-200-177-A32, Neuronexus, Ann Arbor, MI, USA) to record extracellular spikes. The coordinates for recording were 1.5 mm lateral and 0.8 mm anterior to the bifurcation point of the mid-sagittal sinus. The probe was advanced slowly at the speed of 1–2 µm/s using a motorized manipulator (MP-285, Sutter Instrument Company, Novato, CA, USA) until the tip of electrode was located 1.0–1.6 mm below the surface of the brain. Recordings lasted for 4–5 hr. At the end of the recording, an electrolytic lesion was made at the location of one of the silicon shank tips using a tungsten electrode (10 µA for 10 s). Following this, the birds were deeply anesthetized (110 µl, sodium pentobarbital [250 mg/kg]) and perfused. The extracted brains were stored in 4% paraformaldehyde solution for histology. On the next day after perfusion, parasagittal 100 µm sections of the brains were prepared (Vibratome Series 1000, Technical Products International, St. Louis, MO, USA) and stained with cresyl violet. Electrode placement was verified by comparing electrolytic lesions to histological landmarks that define the boundaries of Field L (<xref ref-type="bibr" rid="bib11">Fortune and Margoliash, 1992</xref>).</p></sec><sec id="s4-1-4"><title>Spike sorting</title><p>To isolate single units, the extracellular voltage traces were high-pass filtered at 500 Hz (third order Butterworth filter) and putative spikes were detected if the voltage traces crossed the positive- and negative-going threshold (<xref ref-type="bibr" rid="bib38">Quiroga et al., 2004</xref>). Then, spikes were re-aligned to the negative peak after resampling up to 250 kHz using the cubic spline interpolation method. Features of the aligned spikes were composed of the first three principal components and wavelet coefficients of spike waveforms (<xref ref-type="bibr" rid="bib38">Quiroga et al., 2004</xref>). A mixture of Gaussian models were fitted to the spike features using an Expectation Maximization (EM) algorithm to build distinctive clusters of spikes with similar spike waveforms (<xref ref-type="bibr" rid="bib35">Pham et al., 2005</xref>). Unit quality was then assessed by signal-to-noise ratio (SNR) and refractory period violations to select well-isolated single units (<xref ref-type="bibr" rid="bib24">Ludwig et al., 2009</xref>). All analyses for spike sorting were performed using custom software written in MATLAB (The Mathworks Inc. Natick, MA, USA).</p></sec><sec id="s4-1-5"><title>Spike-pattern classification</title><p>We classified spike patterns into primary and secondary responses on the basis of cross-correlations between spike trains and click-sequence stimuli. The similarity score was defined as the maximum cross-correlation value of normalized PSTH (bin size: 5 ms) with the normalized click stimulus. A unit was classified as primary If the similarity score was above 0.5 and secondary if the score was below 0.5. Physiological classification was validated by histology (<xref ref-type="fig" rid="fig1">Figure 1b</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), which revealed that although exact coordinates differed in different animals, primary neurons formed a contiguous island within the surrounding zone of secondary-like responses. The continuity and scale of these islands of primary responses were consistent with the known anatomy and location of primary thalamoricipient zone L2a.</p></sec><sec id="s4-1-6"><title>Timescales of neural responses</title><p>The timescales of ensemble responses to songs and click sequences in the primary auditory area L2a were characterized by the distribution of intervals between neighboring peaks of the smoothed PSTH (5 ms bin). To smooth the PSTH, we filtered the PSTH with an FIR band-pass filter (Kaiser window, passband: 5–110 Hz, number of coefficient: 2233, sampling rate: 1 kHz, passband ripple is 5% and stopband attenuation is 40 dB). The filtered PSTH was then normalized so that the values were distributed between 0 and 1. Local peaks of normalized PSTH are selected on the basis of three conditions: distance between peak and valley &gt;0.01, peak value &gt;0.3, and peak height relative to the neighboring valley &gt;0.05.</p></sec><sec id="s4-1-7"><title>Phase space trajectory</title><p>After dividing responses into two groups (primary or secondary), we built a population vector array that contained all PSTHs of units for different stimuli (bin size: 5 ms). Each vector had <inline-formula><mml:math id="inf9"><mml:mi>n</mml:mi></mml:math></inline-formula> dimensions of data, where <inline-formula><mml:math id="inf10"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of neurons. To visualize the behavior of multiple neurons (<xref ref-type="fig" rid="fig4">Figure 4a</xref>), we applied principal component analysis (PCA) on the population vector arrays using functions from MATLAB’s Statistical Toolbox.</p></sec><sec id="s4-1-8"><title>Stimulus discriminability</title><p>We defined discriminability of neural responses as the minimum Euclidean distance between two different population vector arrays in response to distinct sounds. Before calculating distances, each spike rate trace in a population vector was smoothed with a 30 ms rectangular window. Then, we divided the recording session into two groups (odd vs. even numbered trials) and obtained the distribution of distances in population vector space built from either the same stimulus or across different stimuli. To build the ROC curve, we calculated the true and false positive ratio for discriminating two different stimuli while changing the decision-boundary position.</p></sec></sec><sec id="s4-2"><title>Auditory operant-training preparation</title><p>Here we describe a method for auditory-operant training that is useful for training zebra finches on challenging discriminations with little shaping procedure. The proposed method uses water reward rather than seed reward (<xref ref-type="bibr" rid="bib36">Picardo et al., 2015</xref>). Zebra finches are adapted to arid conditions and can survive for months in a laboratory setting without access to water (<xref ref-type="bibr" rid="bib7">Cade et al., 1965</xref>), yet they remain highly motivated to work for water. The quantity of water provided in each reward can be as low as 1–5 μl. With this reward quantity, birds work for hundreds or thousands of trials per day.</p><sec id="s4-2-1"><title>Subjects</title><p>In the operant task, we trained 53 adult (&gt;90 days post-hatch on the first day of training) male zebra finches (<italic>Taeniopygia guttata</italic>). All birds were housed in the same aviary room and were experimentally naive at the start of training. Once a bird entered the training cage, he remained in the training cage 24 hours a day until the end of training period.</p></sec><sec id="s4-2-2"><title>Food and water</title><p>Dehydrated seed (100–110F<sup>o</sup> for 12 hr, D-5 Dehydrator, TSM Products) was supplied every two days (seed is dehydrated the day before it is provided in the cage). Soft food (ABBA 97 Ultimate nestling food, ABBASEED) was available once per week. Birds had unlimited access to water on the weekends and every day access to grit. Birds were not exposed to water deprivation conditions prior to training. On a single day of training, birds normally initiated around 800–1300 trials (with a maximum of 4000 trials for one individual). This corresponded to 300–1000 μl of water consumption during training. We provided additional water (0.5–1 ml) after the training if the total volume of water consumption for two days was less than 1 ml. The birds usually drank 0.5–1 ml of water over night when this supplement was provided. In total, through reward and supplement, the experimental birds received an average of 1–1.5 ml of water every day, a number that corresponds to 50% of normal water consumption for zebra finch under certain environmental conditions (<xref ref-type="bibr" rid="bib7">Cade et al., 1965</xref>).</p></sec><sec id="s4-2-3"><title>Operant chamber</title><p>In this experiment, 12 identical operant-training cages were used. The training cages (11 inch wide x eight inch high) were kept inside sound attenuation chambers (22 inch wide x 14.5 inch high x 16 inch deep, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). All inside surfaces of the chambers were lined with embossed acoustic foam (PROSPEC Composite, Pinta acoustics inc). Inside each training cage, there were two infrared switches (OPB815WZ, OPTEK Technology): one for trial initiation (called the trial port) and one for water reward (called the water port). The water reservoir was located 24 inches above the cage floor and the water valve (EW-01540–02, Cole-Parmer) was placed between the reservoir and spout. The water spout was located in the middle of the infrared switch assembly (water port), so that whenever the bird accessed the water spout, he broke the infrared beam automatically. We used two different sizes of incompressible plastic tubes to make water flow slow enough to create a proper drop size (1–5 μl). An illustration of the tubing is shown in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. The sound stimulus was presented through the same tweeter used for the electrophysiology study (PLWT4, Pyle Audio). A microprocessor dedicated to each cage (Arduino Mega 2560, Arduino) controlled stimulus presentation, water delivery, and infrared switches. Individual clicks generated by the Arduino microprocessor were 100 μs long rectangular pulses. Using this microprocessor, the mean jitter in the click interval was 93 μs (data are not shown). Every time the bird tried a new trial, data from the previous trial was transmitted by ethernet to a central data processing computer in the lab and analyzed in real time by a custom made Matlab program (Mathworks, Natick MA, USA). Training ran for five hours per day from Tuesday to Friday each week. The behavior of all birds was monitored through USB webcams in each chamber (Webcam Pro 9000, Logitech).</p></sec></sec><sec id="s4-3"><title>Auditory operant-training procedure</title><p>In this procedure, a bird can demonstrate learning through two behaviors — by interrupting ongoing playback of a non-rewarding stimulus to reset the trial, or by accessing the water port selectively for rewarding stimuli. We trained birds with two different methods: a two-stage method and a single-stage method. In all experiments, two sounds were presented — a rewarded stimulus (click sequence 2) and a non-rewarded stimulus (click sequence 1 or 9).</p><sec id="s4-3-1"><title>Training procedure during stage 1 of two-stage training</title><p>This training starts with only one infrared switch (for trial initiation, on the left side of the cage, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). The bird can start a new trial or interrupt playback of the stimulus by breaking the infrared beam any time 200 ms after the start of the stimulus playback. The water spout is on the right side of the cage and the water reward is passively given at the end of the rewarded stimuli, which constitutes 20% of total trials. In this setup, the bird learns to be ‘impatient’ and to interrupt stimuli that are not followed by reward. In a sense, the bird is ‘foraging’ for a low-probability rewarded sound. On each day of training, we monitored the latency of trial initiations to two different sequences. During the first one or two days, birds simply explored the training environment and explored the trial port randomly. Gradually, birds realized the existence of passive water reward and started to reinitiate trials earlier on non-rewarding trials than on rewarding trials (right middle panel of <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2a, and 2d</xref>. Note the bump of red curve around 5–6 s). In 1–2 weeks of training, birds could re-initiate trials only for non-rewarded trials, and wait for water reward on the rewarded trials.</p></sec><sec id="s4-3-2"><title>Training procedure during stage 2 of two-stage training</title><p>Once birds showed significant learning in stage 1, another infrared switch was activated on the water delivery port. Water was no longer delivered passively, but only if the water port was accessed during or just after the playback of the rewarding stimulus. This period, during which reward port access leads to release of water, is called the ‘response time-window'. This window was 7 s long from the end of a sequence. If the water port was accessed at any time during non-rewarding trials, or outside of the 7 s response window, a 10 s time-out period ensued, during which the green LED was turned off. Introducing another infrared switch in this stage did not alter the trial reset behavior that was acquired in the first stage of training (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2c and f</xref>).</p></sec><sec id="s4-3-3"><title>Training procedure for single-stage training</title><p>In single-stage training, the bird begins training with both infrared switch-contingencies active from the beginning. However, to jumpstart the process, water was also delivered passively at the end of the rewarded stimulus if the bird did not access the water port during playback of the rewarded stimulus. Once the bird learned to initiate trials and encounter water at the water port location, the passive water delivery was shut off. Other than this brief passive delivery period, this method involved no shaping or staging. Birds learned strategies for the use of both infrared switches through exploration (re-initiating trials within 3 s when the non-rewarded pattern was presented and accessing the water port during playback of the rewarding stimulus, <xref ref-type="fig" rid="fig6">Figure 6a</xref>).</p></sec><sec id="s4-3-4"><title>Operant task behavior evaluation</title><p>We used a d-prime measure to estimate the progress of learning:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>'</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where H is the proportion of correct responses (hit rate) and F is the proportion of incorrect responses (false alarm rate) (<xref ref-type="bibr" rid="bib18">Green and Swets, 1966</xref>).</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by CELEST, a National Science Foundation Science of Learning Center (NSF OMA-0835976), by the National Institute of Health (NIH R01NS089679), the National Research Council of Science and Technology (NIST) grant from the Korea government (MSIP) (No.CRC-15–04-KIST), and the Ministry of Science, ICT and Future Planning/Institute for Information &amp; communications Technology Promotion as part of the ICT R&amp;D program (R0126-16-1119). We thank Frederic Theunissen, Luke Remage-Healey, Kathy Nordeen, Ofer Tchernichovski, Sarah Bottjer, Elizabeth Regan, and Richard Hanhloser for providing zebra finch song samples from their colonies. We would also like to thank Aniruddh Patel for comments that improved the analysis and manuscript.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf2"><p>BGS-C: Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="conflict" id="conf1"><p>The other authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>YL, Designed the study, Collected data, Wrote the paper and analyzed the data</p></fn><fn fn-type="con" id="con2"><p>RL, Collected data</p></fn><fn fn-type="con" id="con3"><p>BGS-C, Designed the study</p></fn><fn fn-type="con" id="con4"><p>TJG, Designed the study, Wrote the paper</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to approved institutional animal care and use committee (IACUC) protocols (Protocol Number: 11-027) of the Boston University, operating under AALAC registration 000197, OLAW assurance A3316-01 and USDA 14-R-0017. All surgery was performed under isoflurane anesthesia, and every effort was made to minimize suffering.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="SD3-data"><object-id pub-id-type="doi">10.7554/eLife.18205.024</object-id><label>Supplementary file 1.</label><caption><title>Click-sequence audio files.</title><p>We provide audio files of all the click sequences used in this study in .wav format. The last number of the file name corresponds to the index of click sequence. For example, Clk_Sequence_1.wav contains audio data for sequence 1.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.18205.024">http://dx.doi.org/10.7554/eLife.18205.024</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-18205-supp1-v2.zip"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The effect of correlated variability on the accuracy of a population code</article-title><source>Neural Computation</source><volume>11</volume><fpage>91</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1162/089976699300016827</pub-id><pub-id pub-id-type="pmid">9950724</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abe</surname><given-names>K</given-names></name><name><surname>Watanabe</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Songbirds possess the spontaneous ability to discriminate syntactic rules</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1067</fpage><lpage>1074</lpage><pub-id pub-id-type="doi">10.1038/nn.2869</pub-id><pub-id pub-id-type="pmid">21706017</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Abeles</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1991">1991</year><source>Corticonics</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511574566</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amador</surname><given-names>A</given-names></name><name><surname>Perl</surname><given-names>YS</given-names></name><name><surname>Mindlin</surname><given-names>GB</given-names></name><name><surname>Margoliash</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Elemental gesture dynamics are encoded by song premotor cortical neurons</article-title><source>Nature</source><volume>495</volume><fpage>59</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1038/nature11967</pub-id><pub-id pub-id-type="pmid">23446354</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bolhuis</surname><given-names>JJ</given-names></name><name><surname>Gahr</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural mechanisms of birdsong memory</article-title><source>Nature Reviews. Neuroscience</source><volume>7</volume><fpage>347</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1038/nrn1904</pub-id><pub-id pub-id-type="pmid">16760915</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>MS</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>What songbirds teach us about learning</article-title><source>Nature</source><volume>417</volume><fpage>351</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1038/417351a</pub-id><pub-id pub-id-type="pmid">12015616</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cade</surname><given-names>TJ</given-names></name><name><surname>Tobin</surname><given-names>CA</given-names></name><name><surname>Gold</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1965">1965</year><article-title>Water economy and metabolism of two estrildine finches</article-title><source>Physiological Zoology</source><volume>38</volume><fpage>9</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1086/physzool.38.1.30152342</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calabrese</surname><given-names>A</given-names></name><name><surname>Woolley</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Coding principles of the canonical cortical microcircuit in the avian brain</article-title><source>PNAS</source><volume>112</volume><fpage>3517</fpage><lpage>3522</lpage><pub-id pub-id-type="doi">10.1073/pnas.1408545112</pub-id><pub-id pub-id-type="pmid">25691736</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cynx</surname><given-names>J</given-names></name><name><surname>Nottebohm</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Testosterone facilitates some conspecific song discriminations in castrated zebra finches (Taeniopygia guttata)</article-title><source>PNAS</source><volume>89</volume><fpage>1376</fpage><lpage>1378</lpage><pub-id pub-id-type="doi">10.1073/pnas.89.4.1376</pub-id><pub-id pub-id-type="pmid">1741392</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egorov</surname><given-names>AV</given-names></name><name><surname>Hamam</surname><given-names>BN</given-names></name><name><surname>Fransén</surname><given-names>E</given-names></name><name><surname>Hasselmo</surname><given-names>ME</given-names></name><name><surname>Alonso</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Graded persistent activity in entorhinal cortex neurons</article-title><source>Nature</source><volume>420</volume><fpage>173</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nature01171</pub-id><pub-id pub-id-type="pmid">12432392</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fortune</surname><given-names>ES</given-names></name><name><surname>Margoliash</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Cytoarchitectonic organization and morphology of cells of the field L complex in male zebra finches (Taenopygia guttata)</article-title><source>The Journal of Comparative Neurology</source><volume>325</volume><fpage>388</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1002/cne.903250306</pub-id><pub-id pub-id-type="pmid">1447407</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>T</given-names></name><name><surname>Cecchi</surname><given-names>G</given-names></name><name><surname>Magnasco</surname><given-names>M</given-names></name><name><surname>Laje</surname><given-names>R</given-names></name><name><surname>Mindlin</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Simple motor gestures for birdsongs</article-title><source>Physical Review Letters</source><volume>87</volume><fpage>208101</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.87.208101</pub-id><pub-id pub-id-type="pmid">11690514</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>TJ</given-names></name><name><surname>Naef</surname><given-names>F</given-names></name><name><surname>Nottebohm</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Freedom and rules: the acquisition and reprogramming of a bird's learned song</article-title><source>Science</source><volume>308</volume><fpage>1046</fpage><lpage>1049</lpage><pub-id pub-id-type="doi">10.1126/science.1108214</pub-id><pub-id pub-id-type="pmid">15890887</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gentner</surname><given-names>TQ</given-names></name><name><surname>Fenn</surname><given-names>KM</given-names></name><name><surname>Margoliash</surname><given-names>D</given-names></name><name><surname>Nusbaum</surname><given-names>HC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Recursive syntactic pattern learning by songbirds</article-title><source>Nature</source><volume>440</volume><fpage>1204</fpage><lpage>1207</lpage><pub-id pub-id-type="doi">10.1038/nature04675</pub-id><pub-id pub-id-type="pmid">16641998</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gentner</surname><given-names>TQ</given-names></name><name><surname>Margoliash</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neuronal populations and single cells representing learned auditory objects</article-title><source>Nature</source><volume>424</volume><fpage>669</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1038/nature01731</pub-id><pub-id pub-id-type="pmid">12904792</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gess</surname><given-names>A</given-names></name><name><surname>Schneider</surname><given-names>DM</given-names></name><name><surname>Vyas</surname><given-names>A</given-names></name><name><surname>Woolley</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Automated auditory recognition training and testing</article-title><source>Animal Behaviour</source><volume>82</volume><fpage>285</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2011.05.003</pub-id><pub-id pub-id-type="pmid">21857717</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grace</surname><given-names>JA</given-names></name><name><surname>Amin</surname><given-names>N</given-names></name><name><surname>Singh</surname><given-names>NC</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Selectivity for conspecific song in the zebra finch auditory forebrain</article-title><source>Journal of Neurophysiology</source><volume>89</volume><fpage>472</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1152/jn.00088.2002</pub-id><pub-id pub-id-type="pmid">12522195</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Green</surname><given-names>DM</given-names></name><name><surname>Swets</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1966">1966</year><source>Signal Detection Theory and Psychophysics</source><publisher-loc>New York</publisher-loc><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossberg</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>On learning of spatiotemporal patterns by networks with ordered sensory and motor components 1. Excitatory components of the cerebellum</article-title><source>Studies in Applied Mathematics</source><volume>48</volume><fpage>105</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1002/sapm1969482105</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hose</surname><given-names>B</given-names></name><name><surname>Langner</surname><given-names>G</given-names></name><name><surname>Scheich</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Topographic representation of periodicities in the forebrain of the mynah bird: one map for pitch and rhythm?</article-title><source>Brain Research</source><volume>422</volume><fpage>367</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(87)90946-2</pub-id><pub-id pub-id-type="pmid">3676796</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jarvis</surname><given-names>ED</given-names></name><name><surname>Güntürkün</surname><given-names>O</given-names></name><name><surname>Bruce</surname><given-names>L</given-names></name><name><surname>Csillag</surname><given-names>A</given-names></name><name><surname>Karten</surname><given-names>H</given-names></name><name><surname>Kuenzel</surname><given-names>W</given-names></name><name><surname>Medina</surname><given-names>L</given-names></name><name><surname>Paxinos</surname><given-names>G</given-names></name><name><surname>Perkel</surname><given-names>DJ</given-names></name><name><surname>Shimizu</surname><given-names>T</given-names></name><name><surname>Striedter</surname><given-names>G</given-names></name><name><surname>Wild</surname><given-names>JM</given-names></name><name><surname>Ball</surname><given-names>GF</given-names></name><name><surname>Dugas-Ford</surname><given-names>J</given-names></name><name><surname>Durand</surname><given-names>SE</given-names></name><name><surname>Hough</surname><given-names>GE</given-names></name><name><surname>Husband</surname><given-names>S</given-names></name><name><surname>Kubikova</surname><given-names>L</given-names></name><name><surname>Lee</surname><given-names>DW</given-names></name><name><surname>Mello</surname><given-names>CV</given-names></name><name><surname>Powers</surname><given-names>A</given-names></name><name><surname>Siang</surname><given-names>C</given-names></name><name><surname>Smulders</surname><given-names>TV</given-names></name><name><surname>Wada</surname><given-names>K</given-names></name><name><surname>White</surname><given-names>SA</given-names></name><name><surname>Yamamoto</surname><given-names>K</given-names></name><name><surname>Yu</surname><given-names>J</given-names></name><name><surname>Reiner</surname><given-names>A</given-names></name><name><surname>Butler</surname><given-names>AB</given-names></name><collab>Avian Brain Nomenclature Consortium</collab></person-group><year iso-8601-date="2005">2005</year><article-title>Avian brains and a new understanding of vertebrate brain evolution</article-title><source>Nature Reviews. Neuroscience</source><volume>6</volume><fpage>151</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1038/nrn1606</pub-id><pub-id pub-id-type="pmid">15685220</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>G</given-names></name><name><surname>Doupe</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Organized representation of spectrotemporal features in songbird auditory forebrain</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>16977</fpage><lpage>16990</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2003-11.2011</pub-id><pub-id pub-id-type="pmid">22114268</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lashely</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2004">2004</year><chapter-title>The probelm of serial order in behavior</chapter-title><source>First Language Acquisition: The Essential Readings</source><fpage>316</fpage><lpage>334</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludwig</surname><given-names>KA</given-names></name><name><surname>Miriani</surname><given-names>RM</given-names></name><name><surname>Langhals</surname><given-names>NB</given-names></name><name><surname>Joseph</surname><given-names>MD</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kipke</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Using a common average reference to improve cortical neuron recordings from microelectrode arrays</article-title><source>Journal of Neurophysiology</source><volume>101</volume><fpage>1679</fpage><lpage>1689</lpage><pub-id pub-id-type="doi">10.1152/jn.90989.2008</pub-id><pub-id pub-id-type="pmid">19109453</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Natschläger</surname><given-names>T</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title><source>Neural Computation</source><volume>14</volume><fpage>2531</fpage><lpage>2560</lpage><pub-id pub-id-type="doi">10.1162/089976602760407955</pub-id><pub-id pub-id-type="pmid">12433288</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnasco</surname><given-names>MO</given-names></name><name><surname>Piro</surname><given-names>O</given-names></name><name><surname>Cecchi</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Self-tuned critical anti-Hebbian networks</article-title><source>Physical Review Letters</source><volume>102</volume><fpage>258102</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.102.258102</pub-id><pub-id pub-id-type="pmid">19659122</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mandelblat-Cerf</surname><given-names>Y</given-names></name><name><surname>Las</surname><given-names>L</given-names></name><name><surname>Denissenko</surname><given-names>N</given-names></name><name><surname>Fee</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A role for descending auditory cortical projections in songbird vocal learning</article-title><source>eLife</source><volume>3</volume><elocation-id>e02152</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.02152</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mauk</surname><given-names>MD</given-names></name><name><surname>Buonomano</surname><given-names>DV</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The neural basis of temporal processing</article-title><source>Annual Review of Neuroscience</source><volume>27</volume><fpage>307</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144247</pub-id><pub-id pub-id-type="pmid">15217335</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>BCJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><edition>5 ed</edition><source>An Introduction to the Psychology of Hearing</source><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>CM</given-names></name><name><surname>Leppelsack</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Feature extraction and tonotopic organization in the avian auditory forebrain</article-title><source>Experimental Brain Research</source><volume>59</volume><fpage>587</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1007/BF00261351</pub-id><pub-id pub-id-type="pmid">2993015</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagel</surname><given-names>KI</given-names></name><name><surname>McLendon</surname><given-names>HM</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Differential influence of frequency, timing, and intensity cues in a complex acoustic categorization task</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>1426</fpage><lpage>1437</lpage><pub-id pub-id-type="doi">10.1152/jn.00028.2010</pub-id><pub-id pub-id-type="pmid">20610781</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norton</surname><given-names>P</given-names></name><name><surname>Scharff</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>&quot;Bird Song Metronomics&quot;: Isochronous Organization of Zebra Finch Song Rhythm</article-title><source>Frontiers in Neuroscience</source><volume>10</volume><elocation-id>309</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2016.00309</pub-id><pub-id pub-id-type="pmid">27458334</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nottebohm</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>The origins of vocal learning</article-title><source>The American Naturalist</source><volume>106</volume><fpage>116</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1086/282756</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okanoya</surname><given-names>K</given-names></name><name><surname>Dooling</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Hearing in passerine and psittacine birds: a comparative study of absolute and masked auditory thresholds</article-title><source>Journal of Comparative Psychology</source><volume>101</volume><fpage>7</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1037/0735-7036.101.1.7</pub-id><pub-id pub-id-type="pmid">3568610</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pham</surname><given-names>DT</given-names></name><name><surname>Dimov</surname><given-names>SS</given-names></name><name><surname>Nguyen</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Selection of K in K-means clustering</article-title><source>Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science</source><volume>219</volume><fpage>103</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1243/095440605X8298</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Picardo</surname><given-names>MA</given-names></name><name><surname>Katlowitz</surname><given-names>KA</given-names></name><name><surname>Okobi</surname><given-names>DE</given-names></name><name><surname>Benezra</surname><given-names>SE</given-names></name><name><surname>Clary</surname><given-names>RC</given-names></name><name><surname>Merel</surname><given-names>J</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Long</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>Analyzing the population dynamics underlying a complex motor act. Program No. 181.01. 2015</chapter-title><source>Neuroscience Meeting Planner</source><publisher-loc>Chicago, IL</publisher-loc><publisher-name>Society for Neuroscience, Online</publisher-name></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prather</surname><given-names>JF</given-names></name><name><surname>Peters</surname><given-names>S</given-names></name><name><surname>Nowicki</surname><given-names>S</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Precise auditory-vocal mirroring in neurons for learned vocal communication</article-title><source>Nature</source><volume>451</volume><fpage>305</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1038/nature06492</pub-id><pub-id pub-id-type="pmid">18202651</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>RQ</given-names></name><name><surname>Nadasdy</surname><given-names>Z</given-names></name><name><surname>Ben-Shaul</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Unsupervised spike detection and sorting with wavelets and superparamagnetic clustering</article-title><source>Neural Computation</source><volume>16</volume><fpage>1661</fpage><lpage>1687</lpage><pub-id pub-id-type="doi">10.1162/089976604774201631</pub-id><pub-id pub-id-type="pmid">15228749</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rübsamen</surname><given-names>R</given-names></name><name><surname>Dörrscheidt</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Tonotopic organization of the auditory forebrain in a songbird, the European starling</article-title><source>Journal of Comparative Physiology A</source><volume>158</volume><fpage>639</fpage><lpage>646</lpage><pub-id pub-id-type="doi">10.1007/BF00603820</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scharff</surname><given-names>C</given-names></name><name><surname>Nottebohm</surname><given-names>F</given-names></name><name><surname>Cynx</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Conspecific and heterospecific song discrimination in male zebra finches with lesions in the anterior forebrain pathway</article-title><source>Journal of Neurobiology</source><volume>36</volume><fpage>81</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-4695(199807)36:1&lt;81::AID-NEU7&gt;3.0.CO;2-6</pub-id><pub-id pub-id-type="pmid">9658340</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>DM</given-names></name><name><surname>Woolley</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Sparse and background-invariant coding of vocalizations in auditory scenes</article-title><source>Neuron</source><volume>79</volume><fpage>141</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.038</pub-id><pub-id pub-id-type="pmid">23849201</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shaevitz</surname><given-names>SS</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Functional connectivity between auditory areas field L and CLM and song system nucleus HVC in anesthetized zebra finches</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>2747</fpage><lpage>2764</lpage><pub-id pub-id-type="doi">10.1152/jn.00294.2007</pub-id><pub-id pub-id-type="pmid">17898149</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stripling</surname><given-names>R</given-names></name><name><surname>Milewski</surname><given-names>L</given-names></name><name><surname>Kruse</surname><given-names>AA</given-names></name><name><surname>Clayton</surname><given-names>DF</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Rapidly learned song-discrimination without behavioral reinforcement in adult male zebra finches (Taeniopygia guttata)</article-title><source>Neurobiology of Learning and Memory</source><volume>79</volume><fpage>41</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/S1074-7427(02)00005-9</pub-id><pub-id pub-id-type="pmid">12482678</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sturdy</surname><given-names>CB</given-names></name><name><surname>Weisman</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Rationale and methodology for testing auditory cognition in songbirds</article-title><source>Behavioural Processes</source><volume>72</volume><fpage>265</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1016/j.beproc.2006.03.007</pub-id><pub-id pub-id-type="pmid">16616437</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Aa</surname><given-names>J</given-names></name><name><surname>Honing</surname><given-names>H</given-names></name><name><surname>ten Cate</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The perception of regularity in an isochronous stimulus in zebra finches (Taeniopygia guttata) and humans</article-title><source>Behavioural Processes</source><volume>115</volume><fpage>37</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1016/j.beproc.2015.02.018</pub-id><pub-id pub-id-type="pmid">25725348</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vates</surname><given-names>GE</given-names></name><name><surname>Broome</surname><given-names>BM</given-names></name><name><surname>Mello</surname><given-names>CV</given-names></name><name><surname>Nottebohm</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Auditory pathways of caudal telencephalon and their relation to the song system of adult male zebra finches</article-title><source>The Journal of Comparative Neurology</source><volume>366</volume><fpage>613</fpage><lpage>642</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(19960318)366:4&amp;amp;lt;613::AID-CNE5&amp;amp;gt;3.0.CO;2-7</pub-id><pub-id pub-id-type="pmid">8833113</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Brzozowska-Prechtl</surname><given-names>A</given-names></name><name><surname>Karten</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Laminar and columnar auditory cortex in avian brain</article-title><source>PNAS</source><volume>107</volume><fpage>12676</fpage><lpage>12681</lpage><pub-id pub-id-type="doi">10.1073/pnas.1006645107</pub-id><pub-id pub-id-type="pmid">20616034</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zohary</surname><given-names>E</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Correlated neuronal discharge rate and its implications for psychophysical performance</article-title><source>Nature</source><volume>370</volume><fpage>140</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1038/370140a0</pub-id><pub-id pub-id-type="pmid">8022482</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.18205.025</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Raymond</surname><given-names>Jennifer L</given-names></name><role>Reviewing editor</role><aff id="aff7"><institution>Stanford University</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Transformation of temporal sequences in zebra finch auditory cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Shihab Shamma (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This manuscript describes experiments using fast and slow click trains to probe the neural mechanisms of temporal sequence discrimination in auditory areas of the zebra finch brain. More specifically, the authors describe a transformation from a temporal code to a spatial code in the higher auditory areas, which may enable birds to discriminate temporal sequences in the temporal range relevant for birdsong.</p><p>1) For fast click trains (interclick intervals in the 10 to 40 ms range), there is a transformation of the auditory responses between lower and higher areas. Whereas in the primary auditory area, neurons are highly synchronized, in higher areas the neural responses are less synchronous and evolve through a complex high-dimensional trajectory that carries more information about the sequence of click intervals.</p><p>2) For slower click trains (interclick intervals in the 20 to 80 ms range), there is less discrimination of different click sequences in higher auditory areas.</p><p>3) At the behavioral level, zebra finches are capable of learning sequences of clicks from the &quot;fast&quot; click trains but not the &quot;slow&quot; click trains. Finally, using and interesting and novel operant conditioning paradigm, the authors show that zebra finches can discriminate fast click sequences but not slow click sequences.</p><p>Strengths include the use of click trains to eliminate spectral information and isolate temporal processing, and the integration across multiple levels-primary and higher auditory areas and behavior. The experiments and analysis appear well designed and executed, and the claims of the paper are exciting. However, more thorough and, in some cases, more straightforward analysis of the data is needed, and the clarity and precision of the writing should be improved.</p><p>Essential revisions:</p><p>1) It seems that the essential claim is that, in the transformation from primary to secondary auditory cortex, the higher area integrates information from the lower areas over some timescale (say 20-40ms) to allow the discrimination of different sequences. The time-constants in the secondary auditory cortex are presumed to be long. This would be consistent with the observation that the slower click sequences (in which fewer of the clicks fall within the integration time) produce less discriminable spike patterns in the higher areas, and are less discriminable behaviorally. It seems that it should be possible to simply measure something akin to integration time-constants of these secondary auditory cells. Such analysis would likely be much more intuitive in the context of the paper, and could supplement or replace the more indirect analyses provided.</p><p>2) Similarly, it would be helpful to include a straightforward analysis of the integration window required to discriminate the primary auditory spike trains. It is true that the longer time constant can help conceptually to convert a temporal sequence into a spatial pattern through integration. But it is evident that the time constants in the secondary are not much longer than 2 or 3 clicks. So how would the spatial patters form for a whole sequence? Or is a sequence for these birds just 2 or 3 click patterns? If the birds need, say, 5-10 clicks to discriminate a sequence from another, then there must be some kind of integration that is commensurate with that sequence. Otherwise, the secondary patterns of activity are just a sequence of spatial patterns, and we are still back to the same question: how do we encode a sequence as one spatial pattern? The authors do not address this with respect to their data, since they seem to argue that for these birds, the secondary ACX is sufficient to account for all their behavioral performance. Also, it is not clear why the discriminability is largely eliminated by reducing the click rate by a factor of two. The fast and slow click sequences share click intervals in the range of 20ms to 40ms. And yet these shared intervals do not seem sufficient to yield discriminability in the slow sequence. Why is this? Is it possible that only the shortest click intervals (i.e. between 10ms and 20ms) provide discriminability?</p><p>3) A strong argument is made that the inter-peak intervals in the spike trains for their fast click sequences match those observed for natural song, while those for the slow click sequences do not. Given the plots in <xref ref-type="fig" rid="fig2">Figure 2</xref>, the reviewers did not find this argument convincing. The interval distribution for natural song appears to be exponentially distributed, while that for the fast click sequence is sharply limited below 40ms. The use of fast and slow click sequences is interesting and important, but the authors should find a less distracting way to motivate and describe their choice of click intervals.</p><p>4) The description of what was actually done (particularly regarding physiology) is hard to follow and scattered throughout the paper. When did they use units versus sites, when did they use simultaneous recordings versus pooled recordings, when did (or didn't) they do nested analyses to account for within-bird or within-site variability?</p><p>5) The terminology is often misleading; this paper advances our understanding of auditory processing in the bird telencephalon but does not tell us anything about how the cortex works – in fact the majority of the discussion relates to &quot;primary&quot; L2a's lack of recurrent/feedback connections, which is one of the ways in which it is most clearly dissimilar from any mammalian neocortex.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.18205.026</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions:</italic> </p><p><italic>1) It seems that the essential claim is that, in the transformation from primary to secondary auditory cortex, the higher area integrates information from the lower areas over some timescale (say 20-40ms) to allow the discrimination of different sequences. The time-constants in the secondary auditory cortex are presumed to be long. This would be consistent with the observation that the slower click sequences (in which fewer of the clicks fall within the integration time) produce less discriminable spike patterns in the higher areas, and are less discriminable behaviorally. It seems that it should be possible to simply measure something akin to integration time-constants of these secondary auditory cells. Such analysis would likely be much more intuitive in the context of the paper, and could supplement or replace the more indirect analyses provided.</italic> </p><p>We agree the hypothesis of longer cellular time-constants was over-emphasized given that there is no direct data such as whole-cell recordings in our study, or in prior studies.</p><p>The text has been modified to de-emphasize this unsupported idea in the Introduction and Discussion sections.</p><p>We now provide more balance in the Discussion, including equal emphasis on other possible mechanisms, including the presence of recurrent feedback in secondary areas that could extend the effective integration timescale in a way that doesn’t necessary imply a change in membrane biophysics.</p><p><italic>2) Similarly, it would be helpful to include a straightforward analysis of the integration window required to discriminate the primary auditory spike trains. It is true that the longer time constant can help conceptually to convert a temporal sequence into a spatial pattern through integration. But it is evident that the time constants in the secondary are not much longer than 2 or 3 clicks. So how would the spatial patters form for a whole sequence? Or is a sequence for these birds just 2 or 3 click patterns? If the birds need, say, 5-10 clicks to discriminate a sequence from another, then there must be some kind of integration that is commensurate with that sequence. Otherwise, the secondary patterns of activity are just a sequence of spatial patterns, and we are still back to the same question: how do we encode a sequence as one spatial pattern? The authors do not address this with respect to their data, since they seem to argue that for these birds, the secondary ACX is sufficient to account for all their behavioral performance. Also, it is not clear why the discriminability is largely eliminated by reducing the click rate by a factor of two. The fast and slow click sequences share click intervals in the range of 20ms to 40ms. And yet these shared intervals do not seem sufficient to yield discriminability in the slow sequence. Why is this? Is it possible that only the shortest click intervals (i.e. between 10ms and 20ms) provide discriminability?</italic> </p><p>This was an important suggestion. We now explain in the manuscript that the sequences used in the behavioral training can be discriminated with smoothing windows on the timescale of 100ms or shorter. As explained in the Discussion, to do this, we first smoothed the click sequences used for behavior training with three different rectangular windows of timescale T or shorter and built phase plane traces of a click sequence in 3D space. In this case, three different smoothing windows correspond to hypothetical units with different integration timescales. We then analyzed in the phase space the minimal time-scale T for which the behaviorally trained sequences could be perfectly segregated. From this analysis, we found that phase plane traces can be separated if the width of rectangular window is greater than 100ms.</p><p>Next, we examined whether the neural response over short timescales could discriminate the sequences. (This parallels the ROC analysis we reported for the full 3 second sequence. When applied to just the first 500ms of neural response, the story still held – separation in secondary but not primary areas.) This shorter timescale roughly matches the behavioral response times.</p><p><italic>3) A strong argument is made that the inter-peak intervals in the spike trains for their fast click sequences match those observed for natural song, while those for the slow click sequences do not. Given the plots in <xref ref-type="fig" rid="fig2">Figure 2</xref>, the reviewers did not find this argument convincing. The interval distribution for natural song appears to be exponentially distributed, while that for the fast click sequence is sharply limited below 40ms. The use of fast and slow click sequences is interesting and important, but the authors should find a less distracting way to motivate and describe their choice of click intervals.</italic> </p><p>This is a good point – birdsong includes many significant behavioral timescales ranging from 20ms to multiple seconds, and the slowed timescale clicks still overlap with many significant timescales in behavior. We keep the observation that the time to space transformation is tuned to a timescale that is common in birdsong, but now show the histograms for the spike sequences for reference only. We no longer argue that the factor of two slowing takes us “out of birdsong” timescale.</p><p><italic>4) The description of what was actually done (particularly regarding physiology) is hard to follow and scattered throughout the paper. When did they use units versus sites, when did they use simultaneous recordings versus pooled recordings, when did (or didn't) they do nested analyses to account for within-bird or within-site variability?</italic> </p><p>This has been clarified with a new introduction section to the Results.</p><p>In this study, we have recorded single and “sorted” multi-unit responses from primary and secondary auditory areas of 11 different birds. For all the analysis including click sequence discrimination analysis, song response analysis, and PSTH, we used only well sorted single units, and now clarify this in the text.</p><p>The only exception is in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>: here, we have shown ensembles of all the units recorded in this study. In this figure, we have indicated with an asterix multi-units on the left side of raster that did not pass the high SNR criteria for single units.</p><p>Also in the figure, we indicate which recordings came from each bird. All the numbers on the right side of raster correspond to the indices shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>.</p><p><italic>5) The terminology is often misleading; this paper advances our understanding of auditory processing in the bird telencephalon but does not tell us anything about how the cortex works – in fact the majority of the discussion relates to &quot;primary&quot; L2a's lack of recurrent/feedback connections, which is one of the ways in which it is most clearly dissimilar from any mammalian neocortex.</italic> </p><p>Our use of the term cortical is a shorthand that is common in the field, but needed further explanation to simplify, we removed all the terminology that may mislead readers such as cortex and cortical and added terminologies for aviary anatomy.</p></body></sub-article></article>