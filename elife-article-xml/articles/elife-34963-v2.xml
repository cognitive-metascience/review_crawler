<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">34963</article-id><article-id pub-id-type="doi">10.7554/eLife.34963</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A resource-rational theory of set size effects in human visual working memory</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-44196"><name><surname>van den Berg</surname><given-names>Ronald</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7353-5960</contrib-id><email>ronald.vandenberg@psyk.uu.se</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-15827"><name><surname>Ma</surname><given-names>Wei Ji</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9835-9083</contrib-id><email>weijima@nyu.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Psychology</institution><institution>University of Uppsala</institution><addr-line><named-content content-type="city">Uppsala</named-content></addr-line><country>Sweden</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Center for Neural Science and Department of Psychology</institution><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie</given-names></name><role>Reviewing Editor</role><aff><institution>University of Chicago</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>07</day><month>08</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e34963</elocation-id><history><date date-type="received" iso-8601-date="2018-01-10"><day>10</day><month>01</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2018-07-28"><day>28</day><month>07</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, van den Berg et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>van den Berg et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-34963-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.34963.001</object-id><p>Encoding precision in visual working memory decreases with the number of encoded items. Here, we propose a normative theory for such set size effects: the brain minimizes a weighted sum of an error-based behavioral cost and a neural encoding cost. We construct a model from this theory and find that it predicts set size effects. Notably, these effects are mediated by probing probability, which aligns with previous empirical findings. The model accounts well for effects of both set size and probing probability on encoding precision in nine delayed-estimation experiments. Moreover, we find support for the prediction that the total amount of invested resource can vary non-monotonically with set size. Finally, we show that it is sometimes optimal to encode only a subset or even none of the relevant items in a task. Our findings raise the possibility that cognitive &quot;limitations&quot; arise from rational cost minimization rather than from constraints.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.34963.002</object-id><title>eLife digest</title><p>You can read this sentence from beginning to end without losing track of its meaning thanks to your working memory. This system temporarily stores information relevant to whatever task you are currently performing. However, the more items you try to hold in working memory at once, the poorer the quality of each of the resulting memories.</p><p>It has long been argued that this phenomenon – known as the set size effect – occurs because the brain devotes a fixed amount of neural resources to working memory. But this theory struggles to account for certain experimental results. It also fails to explain why the brain would not simply recruit more resources whenever it has more items to remember. After all, your heart does something similar by beating faster whenever you increase your physical activity.</p><p>Van den Berg and Ma break with the idea that working memory resources are fixed. They propose that resource allocation is flexible and driven by two conflicting goals: maximize memory performance, but use as few neural resources as necessary. Indeed, a computer simulation that follows this strategy mimics the set size effects seen in healthy volunteers. In the model, the items most relevant for a task are stored more accurately than less important ones, a phenomenon also observed in participants. Lastly, the simulation predicts that the total amount of resources devoted to working memory will vary with the number of items to be remembered. This too is consistent with the results of previous experiments.</p><p>Working memory thus appears to be more flexible than previously thought. The amount of resources that the brain allocates to working memory is not fixed but could be the result of balancing resource cost against cognitive performance. If this is confirmed, it may be possible to improve working memory by offering rewards, or by increasing the perceived importance of a task.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual working memory</kwd><kwd>set size effects</kwd><kwd>resource rationality</kwd><kwd>cost function</kwd><kwd>normative models</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004359</institution-id><institution>Vetenskapsrådet</institution></institution-wrap></funding-source><award-id>2015-00371</award-id><principal-award-recipient><name><surname>van den Berg</surname><given-names>Ronald</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Marie Skłodowska-Curie Actions, Cofund</institution></institution-wrap></funding-source><award-id>INCA 600398</award-id><principal-award-recipient><name><surname>van den Berg</surname><given-names>Ronald</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EY020958</award-id><principal-award-recipient><name><surname>Ma</surname><given-names>Wei Ji</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Set size effects in visual working memory are explained as a resource-rational trade-off between an error-based behavioral cost and a neural encoding cost.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A well-established property of visual working memory (VWM) is that the precision with which items are encoded decreases with the number of encoded items (<xref ref-type="bibr" rid="bib56">Ma et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Luck and Vogel, 2013</xref>). A common way to explain this set size effect has been to assume that there is a fixed amount of resource available for encoding: the more items, the less resource per item and, therefore, the lower the precision per item. Different forms have been proposed for this encoding resource, such as samples (<xref ref-type="bibr" rid="bib64">Palmer, 1994</xref>; <xref ref-type="bibr" rid="bib72">Sewell et al., 2014</xref>), Fisher information (<xref ref-type="bibr" rid="bib87">van den Berg et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Keshvari et al., 2013</xref>), and neural firing rate (<xref ref-type="bibr" rid="bib16">Bays, 2014</xref>). Models with a fixed amount of resource generally predict that the encoding precision per item (defined as inverse variance of the encoding error) is inversely proportional to set size. This prediction is often inconsistent with empirical data, which is the reason that more recent studies instead use a power law to describe set size effects (<xref ref-type="bibr" rid="bib14">Bays et al., 2009</xref>; <xref ref-type="bibr" rid="bib15">Bays and Husain, 2008</xref>; <xref ref-type="bibr" rid="bib87">van den Berg et al., 2012</xref>; <xref ref-type="bibr" rid="bib86">van den Berg et al., 2014</xref>; <xref ref-type="bibr" rid="bib28">Devkar et al., 2015</xref>; <xref ref-type="bibr" rid="bib31">Elmore et al., 2011</xref>; <xref ref-type="bibr" rid="bib57">Mazyar et al., 2012</xref>; <xref ref-type="bibr" rid="bib94">Wilken and Ma, 2004</xref>; <xref ref-type="bibr" rid="bib29">Donkin et al., 2016</xref>; <xref ref-type="bibr" rid="bib44">Keshvari et al., 2013</xref>). In these power-law models, the total amount of resource across all items is no longer fixed, but instead decreases or increases monotonically with set size. These models tend to provide excellent fits to experimental data, but they have been criticized for lacking a principled motivation (<xref ref-type="bibr" rid="bib59">Oberauer et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Oberauer and Lin, 2017</xref>): they accurately describe <italic>how</italic> memory precision depends on set size, but not <italic>why</italic> these effects are best described by a power law – or why they exist at all. In the present study, we seek a normative answer to these fundamental questions.</p><p>While previous studies have used normative theories to account for certain aspects of VWM, none of them has accounted for set size effects in a principled way. Examples include our own previous work on change detection (<xref ref-type="bibr" rid="bib43">Keshvari et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Keshvari et al., 2013</xref>), change localization (<xref ref-type="bibr" rid="bib87">van den Berg et al., 2012</xref>), and visual search (<xref ref-type="bibr" rid="bib57">Mazyar et al., 2012</xref>). In those studies, we modelled the decision stage using optimal-observer theory, but assumed an ad hoc power law to model the relation between encoding precision and set size. Another example is the work by Sims and colleagues, who developed a normative framework in which working memory is conceptualized as an optimally performing information channel (<xref ref-type="bibr" rid="bib82">Sims, 2016</xref>; <xref ref-type="bibr" rid="bib80">Sims et al., 2012</xref>). Their information-theoretic framework offers parsimonious explanations for the relation between stimulus variability and encoding precision (<xref ref-type="bibr" rid="bib80">Sims et al., 2012</xref>) and the non-Gaussian shape of encoding noise (<xref ref-type="bibr" rid="bib81">Sims, 2015</xref>). However, it does not offer a normative explanation of set size effects. In their early work (<xref ref-type="bibr" rid="bib80">Sims et al., 2012</xref>), they accounted for these effects by assuming that total information capacity is fixed, which is similar to other fixed-resource models and predicts an inverse proportionality between encoding precision and set size. In their later work (<xref ref-type="bibr" rid="bib62">Orhan et al., 2014</xref>; <xref ref-type="bibr" rid="bib82">Sims, 2016</xref>), they add to this the assumption that there is an inefficiency in distributing capacity across items and fit capacity as a free parameter at each set size. Neither of these assumptions has a normative motivation. Finally, Nassar and colleagues have proposed a normative model in which a strategic trade-off is made between the number of encoded items and their precision: when two items are very similar, they are encoded as a single item, such that there is more resource available per encoded item (<xref ref-type="bibr" rid="bib58">Nassar et al., 2018</xref>). They showed that this kind of &quot;chunking&quot; is rational from an information-theoretical perspective, because it minimizes the observer’s expected estimation error. However, just as in much of the work discussed above, this theory assumes a fixed resource budget for item encoding, which is not necessarily optimal when resource usage is costly.</p><p>The approach that we take here aligns with the recent proposal that cognitive systems are &quot;resource-rational,&quot; that is, trade off the cost of using resources against expected task performance (<xref ref-type="bibr" rid="bib41">Griffiths et al., 2015</xref>). The starting point of our theory is the principle that neural coding is costly (<xref ref-type="bibr" rid="bib9">Attwell and Laughlin, 2001</xref>; <xref ref-type="bibr" rid="bib48">Lennie, 2003</xref>; <xref ref-type="bibr" rid="bib84">Sterling and Laughlin, 2015</xref>), which may have pressured the brain to trade off the behavioral benefits of high precision against the cost of the resource invested in stimulus encoding (<xref ref-type="bibr" rid="bib66">Pestilli and Carrasco, 2005</xref>; <xref ref-type="bibr" rid="bib48">Lennie, 2003</xref>; <xref ref-type="bibr" rid="bib55">Ma and Huang, 2009</xref>; <xref ref-type="bibr" rid="bib23">Christie and Schrater, 2015</xref>). We hypothesize that set size effects – and limitations in VWM in general – may be the result of making this trade-off near-optimally. We next formalize this hypothesis in a general model that can be applied to a broad range of tasks, analyze the theoretical predictions of this model, and fit it to data from nine previous delayed-estimation experiments.</p><sec id="s1-1"><title>Theory</title><sec id="s1-1-1"><title>General theoretical framework: trade-off between behavioral and neural cost</title><p>We define a vector <bold>Q</bold>={<italic>Q</italic><sub>1</sub>,…, <italic>Q<sub>N</sub></italic>} that specifies the amount of resource with which each of <italic>N</italic> task-relevant items is encoded. We postulate that <bold>Q</bold> affects two types of cost: an expected behavioral cost <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> induced by task errors and an expected neural cost <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> induced by spending neural resources on encoding. The <italic>expected total cost</italic> is a weighted combination, <disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where the weight <italic>λ</italic>≥0 represents the importance of the neural cost relative to the behavioral cost. Generally, increasing the amount of resource spent on encoding will reduce the expected behavioral cost, but simultaneously increase the expected neural cost.</p><p>The key novelty of our theory is that instead of assuming that there is a fixed resource budget for stimulus encoding (a hard constraint), we postulate that the brain – possibly on a trial-by-trial basis – chooses its resource vector <bold>Q</bold> in a manner that minimizes the expected total cost. We denote the vector that yields this minimum by <bold>Q</bold><sub>optimal</sub>:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Under this policy, the total amount of invested resource – the sum of the elements of <bold>Q</bold><sub>optimal</sub> – does not need to be fixed: when it is &quot;worth it&quot; (i.e. when investing more resource reduces the expected behavioral cost more than it increases the expected neural cost), more resource may be invested.</p><p><xref ref-type="disp-formula" rid="equ1 equ2">Equations (1) and (2)</xref> specify the theory at the most general level. To derive testable predictions, we next propose specific formalizations of resource and of the two expected cost functions.</p></sec><sec id="s1-1-2"><title>Formalization of resource</title><p>As in our previous work (<xref ref-type="bibr" rid="bib43">Keshvari et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Keshvari et al., 2013</xref>; <xref ref-type="bibr" rid="bib57">Mazyar et al., 2012</xref>; <xref ref-type="bibr" rid="bib87">van den Berg et al., 2012</xref>; <xref ref-type="bibr" rid="bib86">van den Berg et al., 2014</xref>), we quantify encoding precision as Fisher information, <italic>J</italic>. This measure provides a lower bound on the variance of any unbiased estimator (<xref ref-type="bibr" rid="bib26">Cover and Thomas, 2005</xref>; <xref ref-type="bibr" rid="bib53">Ly et al., 2017</xref>) and is a common tool in the study of theoretical limits on stimulus coding and discrimination (<xref ref-type="bibr" rid="bib1">Abbott and Dayan, 1999</xref>). Moreover, we assume that there is item-to-item and trial-to-trial variation in precision (<xref ref-type="bibr" rid="bib35">Fougnie et al., 2012</xref>; <xref ref-type="bibr" rid="bib87">van den Berg et al., 2012</xref>; <xref ref-type="bibr" rid="bib86">van den Berg et al., 2014</xref>; <xref ref-type="bibr" rid="bib44">Keshvari et al., 2013</xref>; <xref ref-type="bibr" rid="bib88">van den Berg et al., 2017</xref>). Following our previous work, we model this variability using a gamma distribution with a mean <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and shape parameter <italic>τ</italic> ≥0 (larger <italic>τ</italic> means more variability); we denote this distribution by gamma <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>J</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>We specify resource vector <bold>Q</bold> as the vector with mean encoding precisions, <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, such that the general theory specified by <xref ref-type="disp-formula" rid="equ1 equ2">Equations (1) and (2)</xref> modifies to<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>;</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>and<disp-formula id="equ4"><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>;</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In this formulation, it is assumed that the brain has control over resource vector <inline-formula><mml:math id="inf6"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:math></inline-formula>, but not over the variability in how much resource is actually assigned to an item. It should be noted, however, that our choice to incorporate variability in <italic>J</italic> is empirically motivated and not central to the theory: parameter <italic>τ</italic> mainly affects the kurtosis of the predicted estimation error distributions, not their variance or the way that the variance depends on set size (which is the focus of this paper). We will show that the theory also predicts set size effects when there is no variability in <italic>J</italic>.</p></sec><sec id="s1-1-3"><title>Formalization of expected neural cost</title><p>To formalize the neural cost function, we make two general assumptions. First, we assume that the expected neural cost induced by encoding a set of <italic>N</italic> items is the sum of the expected neural cost associated with each of the individual items. Second, we assume that each of these “local” neural costs has the same functional dependence on the amount of allocated resource: if two items are encoded with the same amount of resource, they induce equal amounts of neural cost. Combining these assumptions, the expected neural cost induced by encoding a set of <italic>N</italic> items with resource <inline-formula><mml:math id="inf7"><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> takes the form<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where we introduced the convention to denote local costs (associated with a single item) with small <italic>c</italic>, to distinguish them from the global costs (associated with the entire set of encoded items), which we denote with capital <italic>C</italic>.</p><p>We denote by <italic>c</italic><sub>neural</sub>(<italic>J</italic>) the neural cost induced by investing an amount of resource <italic>J</italic>. The expected neural cost induced by encoding an item with resource <inline-formula><mml:math id="inf8"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> is obtained by integrating over <italic>J</italic>,<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>J</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext>Gamma</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>J</mml:mi><mml:mo>;</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>J</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The theory is agnostic about the exact nature of the cost function <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>: it could include spiking and non-spiking components (<xref ref-type="bibr" rid="bib48">Lennie, 2003</xref>), be associated with activity in both sensory and non-sensory areas, and include other types of cost that are linked to “mental effort” in general (<xref ref-type="bibr" rid="bib75">Shenhav et al., 2017</xref>). </p><p>To motivate a specific form of this function, we consider the case that the neural cost is incurred by spiking activity. For many choices of spike variability, including the common one of Poisson-like variability (<xref ref-type="bibr" rid="bib54">Ma et al., 2006</xref>), Fisher information <italic>J</italic> of a stimulus encoded in a neural population is proportional to the trial-averaged neural spiking rate (<xref ref-type="bibr" rid="bib65">Paradiso, 1988</xref>; <xref ref-type="bibr" rid="bib71">Seung and Sompolinsky, 1993</xref>). If we further assume that each spike has a fixed cost, we find that the local neural cost induced by each item is proportional to <italic>J</italic>,<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>J</mml:mi><mml:mo>;</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>J</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where α is the amount of neural cost incurred by a unit increase in resource. Combining <xref ref-type="disp-formula" rid="equ5 equ6 equ7">Equations (5–7)</xref> yields<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mo>;</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Hence, the global expected neural cost is proportional to the total amount of invested resource and independent of the amount of variability in <italic>J</italic>. Although we use this linear expected neural cost function throughout the paper, we show in Appendix 1 that the key model prediction – a decrease of the optimal resource per item with set size – generalizes to a broad range of choices.</p></sec><sec id="s1-1-4"><title>Formalization of expected behavioral cost for local tasks</title><p>Before we specify the expected behavioral cost function, we introduce a distinction between two classes of tasks. First, we define a task as &quot;local&quot; if the observer’s response depends on only one of the encoded items. Examples of local tasks are single-probe delayed-estimation (<xref ref-type="bibr" rid="bib18">Blake et al., 1997</xref>; <xref ref-type="bibr" rid="bib67">Prinzmetal et al., 1998</xref>; <xref ref-type="bibr" rid="bib94">Wilken and Ma, 2004</xref>), single-probe change detection (<xref ref-type="bibr" rid="bib85">Todd and Marois, 2004</xref>; <xref ref-type="bibr" rid="bib51">Luck and Vogel, 1997</xref>), and single-probe change discrimination (<xref ref-type="bibr" rid="bib45">Klyszejko et al., 2014</xref>). By contrast, when the task response depends on all memorized items, we define the task as &quot;global.&quot; Examples of global tasks are whole-display change detection (<xref ref-type="bibr" rid="bib51">Luck and Vogel, 1997</xref>; <xref ref-type="bibr" rid="bib44">Keshvari et al., 2013</xref>), change localization (<xref ref-type="bibr" rid="bib87">van den Berg et al., 2012</xref>), and delayed visual search (<xref ref-type="bibr" rid="bib57">Mazyar et al., 2012</xref>). The theory that we developed up to this point – <xref ref-type="disp-formula" rid="equ1 equ2 equ3 equ4 equ5 equ6 equ7 equ8">Equations (1–8)</xref> – applies to both global and local tasks. However, from here on, we develop our theory in the context of local tasks only; we will come back to global tasks at the end of the Results.</p><p>As in local tasks only one item gets probed, the expected behavioral cost across all items is a weighted average,<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>p<sub>i</sub></italic> is the experimentally determined probing probability of the <italic>i</italic><sup>th</sup> item and <inline-formula><mml:math id="inf10"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the local expected behavioral cost associated with reporting the <italic>i</italic><sup>th</sup> item. We will refer to the product <inline-formula><mml:math id="inf11"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the 'expected behavioral cost per item'. The only remaining step is to specify <inline-formula><mml:math id="inf12"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This function is task-specific and we will specify it after we have described the task to which we apply the model.</p></sec><sec id="s1-1-5"><title>A resource-rational model for local tasks</title><p>Combining <xref ref-type="disp-formula" rid="equ3 equ8 equ9">Equations 3, 8, and 9</xref> yields the following expected total cost function for local tasks:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mo>;</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>λ</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As parameters <italic>α</italic> and <italic>λ</italic> have interchangeable effects on the model predictions, we will fix <italic>α</italic> = 1 and only treat <italic>λ</italic> as a free parameter.</p><p>We recognize that the right-hand side of <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> is a sum of independent terms. Therefore, each element of <inline-formula><mml:math id="inf13"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, can be computed independently of the other elements, by minimizing the expected total cost per item,<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>optimal</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This completes the specification of the general form of our resource-rational model for local tasks. Its free parameters are <italic>λ</italic> and <italic>τ</italic>.</p></sec><sec id="s1-1-6"><title>Set size effects result from cost minimization and are mediated by probing probability</title><p>To obtain an understanding of the model predictions, we analyze how <inline-formula><mml:math id="inf14"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> depends on probing probability and set size. We perform this analysis under two general assumptions about the local expected behavioral cost function: first, that it monotonically decreases with <inline-formula><mml:math id="inf15"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> (i.e. increasing resource reduces the expected behavioral cost) and, second, that it satisfies a law of diminishing returns (i.e. the reductions per unit increase of resource decrease with the total amount of already invested resource). It can be proven (see Appendix 1) that under these assumptions, the domain of probing probability <italic>p<sub>i</sub></italic> consists of three potential regimes, each with a different optimal encoding strategy (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). First, there might exist a regime 0≤<italic>p<sub>i</sub></italic>&lt;<italic>p</italic><sub>0</sub> in which it is optimal to not encode an item, <inline-formula><mml:math id="inf16"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. In this regime, the probing probability of an item is so low that investing any amount of resource can never reduce the local expected behavioral cost by more than it increases the expected neural cost. Second, there might exist a regime <italic>p</italic><sub>0</sub>≤<italic>p<sub>i</sub></italic>&lt;<italic>p</italic><sub>∞</sub> in which it is optimal to encode an item with a finite amount of resource, <inline-formula><mml:math id="inf17"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In this regime, <inline-formula><mml:math id="inf18"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> increases as a function of <italic>p<sub>i</sub></italic>. Finally, there may be a regime <italic>p</italic><sub>∞</sub>≤<italic>p<sub>i</sub></italic>≤1 in which the optimal strategy is to encode the item with an infinite amount of resource, <inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>. This last regime will only exist in extreme cases, such as when there is no neural cost associated with encoding. The threshold <italic>p</italic><sub>0</sub> depends on the importance of the neural cost, <italic>λ</italic>, and on the derivative of the local expected behavioral cost evaluated at <inline-formula><mml:math id="inf20"><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>; specifically, <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> . The threshold <italic>p</italic><sub>∞</sub> depends on <italic>λ</italic> and on the derivative of the local expected behavioral cost evaluated at <inline-formula><mml:math id="inf22"><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>; specifically, <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> . If <italic>p</italic><sub>∞</sub>&gt;1, then the third regime does not exist, whereas if <italic>p</italic><sub>0</sub> &gt;1, only the first regime exists.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.003</object-id><label>Figure 1.</label><caption><title>Effects of probing probability and set size on <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> in the resource-rational model for local tasks.</title><p>(<bold>A</bold>) The model has three different optimal solutions depending on probing probability <italic>p<sub>i</sub></italic>: invest no resource when <italic>p<sub>i</sub></italic> is smaller than some threshold value <italic>p</italic><sub>0</sub>, invest infinite resource when <italic>p<sub>i</sub></italic> is larger than <italic>p</italic><sub>∞</sub>, and invest a finite amount of resource when <italic>p</italic><sub>0</sub><italic> &lt;p<sub>i</sub></italic> &lt; p<sub>∞</sub>. The thresholds <italic>p</italic><sub>0</sub> and <italic>p</italic><sub>∞</sub> depend on weight <italic>λ</italic> (see <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref>) and on the derivative of the local expected behavioral cost function evaluated at 0 and ∞, respectively. If <italic>p</italic><sub>0</sub> &gt;1, then only the first regime exists; if <italic>p</italic><sub>0</sub> &lt;1 &lt; p<sub>∞</sub> then only the first two regimes exist. (<bold>B</bold>) If, in addition, <italic>p<sub>i</sub></italic> = 1/<italic>N</italic>, then the domain of <italic>N</italic> partitions in a similar manner.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34963-fig1-v2"/></fig><p>We next turn to set size effects. An interesting property of the model is that <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> depends only on the probing probability, <italic>p<sub>i</sub></italic>, and on the model parameters – it does <italic>not</italic> explicitly depend on set size, <italic>N</italic>. Therefore, the only way in which the model can predict set size effects is through a coupling between <italic>N</italic> and <italic>p<sub>i</sub></italic>. Such a coupling exists in most studies that use a local task. For example, in delayed-estimation tasks, each item is usually equally likely to be probed such that <italic>p<sub>i</sub></italic> = 1/<italic>N</italic>. For those experiments, the above partitioning of the domain of <italic>p<sub>i</sub></italic> translates to a similar partitioning of the domain of <italic>N</italic> (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Then, a set size <italic>N</italic><sub>∞</sub>≥0 may exist below which it is optimal to encode items with infinite resource, a region <italic>N</italic><sub>∞</sub>≤<italic>N</italic> &lt; <italic>N</italic><sub>0</sub> in which it is optimal to encode items with a finite amount of resource, and a region <italic>N</italic>&gt;<italic>N</italic><sub>0</sub> in which it is optimal to not encode items at all.</p></sec></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Model predictions for delayed-estimation tasks</title><p>To test the predictions of the model against empirical data, we apply it to the delayed-estimation task (<xref ref-type="bibr" rid="bib94">Wilken and Ma, 2004</xref>; <xref ref-type="bibr" rid="bib18">Blake et al., 1997</xref>; <xref ref-type="bibr" rid="bib67">Prinzmetal et al., 1998</xref>), which is currently one of the most widely used paradigms in VWM research. In this task, the observer briefly holds a set of items in memory and then reports their estimate of a randomly probed target item (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Set size effects manifest as a widening of the estimation error distribution as the number of items is increased (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), which suggests a decrease in the amount of resource per item (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.004</object-id><label>Figure 2.</label><caption><title>A resource-rational model for delayed-estimation tasks.</title><p>(<bold>A</bold>) Example of a trial in a delayed-estimation experiment. The subject is briefly presented with a set of stimuli and, after a short delay, reports the value of the item at a randomly chosen location (here indicated with thick circle). (<bold>B</bold>) The distribution of estimation errors in delayed-estimation experiments typically widens with set size (data from Experiment E5 in <xref ref-type="table" rid="table1">Table 1</xref>). (<bold>C</bold>) This suggests that the amount of resource per encoded item decreases with set size. The estimated amount of resource per item was computed using the same non-parametric model as the one underlying <xref ref-type="fig" rid="fig3">Figure 3C</xref>. (<bold>D</bold>) Expected cost per item as a function of the amount of invested resource (model parameters: <italic>λ</italic> = 0.01, <italic>β</italic> = 2, τ↓0). <italic>Left</italic>: The expected behavioral cost per item (colored curves) decreases with the amount of invested resource, while the expected neural cost per item increases (black line). <italic>Center</italic>: The sum of these two costs has a unique minimum, whose location (arrows) depends on probing probability <italic>p<sub>i</sub>. Right</italic>: The optimal amount of resource per item increases with the probability that the item will be probed. (<bold>E</bold>) Expected cost across all items, when each item is probed with a probability <italic>p<sub>i</sub></italic> = 1/<italic>N</italic>; the model parameters are the same as in <bold>D</bold> and the set sizes correspond with the values of <italic>p<sub>i</sub></italic> in <bold>D</bold>. The predicted set size effect (right panel) is qualitatively similar to set size effects observed in empirical data (cf. panel C). (<bold>D</bold>) and (<bold>E</bold>) are alternative illustrations of the same optimization problem; the right panel of (<bold>E</bold>) could also be obtained by replotting the right panel of (<bold>D</bold>) as a function of <italic>N</italic> = 1/<italic>p<sub>i</sub></italic>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34963-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34963.006</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Fits to the three delayed-estimation benchmark data sets that were excluded from the main analyses.</title><p>Circular variance (top) and circular kurtosis (bottom) of the estimation error distributions as a function of set size, split by experiment. Error bars and shaded areas represent 1 s.e.m. of the mean across subjects. The first three data sets were excluded from the main analyses on the grounds that they were published in papers that were later retracted (<xref ref-type="bibr" rid="bib6">Anderson and Awh, 2012</xref>; <xref ref-type="bibr" rid="bib7">Anderson et al., 2011</xref>). The data set from the study by <xref ref-type="bibr" rid="bib68">Rademaker et al. (2012</xref>) was excluded from the main analyses because it contains only two set sizes, which makes it less suitable for a fine-grained study of the relationship between encoding precision and set size.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34963-fig2-figsupp1-v2"/></fig></fig-group><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.005</object-id><label>Table 1.</label><caption><title>Overview of experimental datasets.</title><p>Experiments E5 and E6 differed in the way that subjects provided their responses (E5: color wheel; E6: scroll).</p></caption><table frame="hsides" rules="groups"><thead><tr valign="top"><th>Exp. ID</th><th>Reference</th><th>Feature</th><th>Set size(s)</th><th>Probing probability</th><th>Number of subjects</th></tr></thead><tbody><tr valign="top"><td>E1</td><td><xref ref-type="bibr" rid="bib94">Wilken and Ma (2004)</xref></td><td>Color</td><td>1, 2, 4, 8</td><td>Equal</td><td>15</td></tr><tr valign="top"><td>E2</td><td><xref ref-type="bibr" rid="bib96">Zhang and Luck (2008)</xref></td><td>Color</td><td>1, 2, 3, 6</td><td>Equal</td><td>8</td></tr><tr valign="top"><td>E3</td><td><xref ref-type="bibr" rid="bib14">Bays et al. (2009)</xref></td><td>Color</td><td>1, 2, 4, 6</td><td>Equal</td><td>12</td></tr><tr valign="top"><td>E4</td><td><xref ref-type="bibr" rid="bib87">van den Berg et al. (2012)</xref></td><td>Orientation</td><td>1-8</td><td>Equal</td><td>6</td></tr><tr valign="top"><td>E5</td><td><xref ref-type="bibr" rid="bib87">van den Berg et al. (2012)</xref></td><td>Color</td><td>1-8</td><td>Equal</td><td>13</td></tr><tr valign="top"><td>E6</td><td><xref ref-type="bibr" rid="bib87">van den Berg et al. (2012)</xref></td><td>Color</td><td>1-8</td><td>Equal</td><td>13</td></tr><tr valign="top"><td>E7</td><td><xref ref-type="bibr" rid="bib14">Bays et al. (2009)</xref></td><td>Orientation</td><td>2,4,8</td><td>Unequal</td><td>7</td></tr><tr valign="top"><td>E8</td><td><xref ref-type="bibr" rid="bib32">Emrich et al. (2017)</xref></td><td>Color</td><td>4</td><td>Unequal</td><td>20</td></tr><tr valign="top"><td>E9</td><td><xref ref-type="bibr" rid="bib32">Emrich et al. (2017)</xref></td><td>Color</td><td>6</td><td>Unequal</td><td>20</td></tr></tbody></table></table-wrap><p>To apply our model to this task, we express the expected local behavioral cost as an expected value of the behavioral cost with respect to the error distribution,<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mtext>behavioral</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ε</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ε</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>ε</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the behavioral cost function <italic>c</italic><sub>behavioral<italic>,i</italic></sub>(<italic>ε</italic>) maps an encoding error <italic>ε</italic> to a cost and <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ε</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the predicted distribution of <italic>ε</italic> for an item encoded with resource <inline-formula><mml:math id="inf27"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. We first specify <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ε</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and then turn to <italic>c</italic><sub>behavioral<italic>,i</italic></sub>(<italic>ε</italic>). As the task-relevant feature in delayed-estimation experiments is usually a circular variable (color or orientation), we make the common assumption that <italic>ε</italic> follows a Von Mises distribution. We denote this distribution by VM(<italic>ε;J</italic>), where <italic>J</italic> is one-to-one related to the distribution’s concentration parameter <italic>κ</italic> (Appendix 1). The distribution of <italic>ε</italic> for a stimulus encoded with resource <inline-formula><mml:math id="inf29"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is found by integrating over <italic>J,</italic> <disp-formula id="equ13"><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ε</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mtext>VM</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ε</mml:mi><mml:mo>;</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>Gamma</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>J</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>J</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Finally, we specify the behavioral cost function <inline-formula><mml:math id="inf30"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mtext>behavioral</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ε</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>, which maps an estimation error <italic>ε</italic> to a behavioral cost. As in most psychophysical experiments, human subjects tend to perform well on delayed-estimation tasks even when the reward is independent of their performance. This suggests that the behavioral cost function is strongly determined by internal incentives. A recent paper (<xref ref-type="bibr" rid="bib81">Sims, 2015</xref>) has attempted to measure this mapping and proposed a two-parameter function. We will test that proposal later, but for the moment we assume a simpler, one-parameter power-law function, <inline-formula><mml:math id="inf31"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mtext>behavioral</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ε</mml:mi><mml:mo>;</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>|</mml:mo> <mml:mi>ε</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mi>β</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, where power <italic>β</italic> is a free parameter.</p><p>To obtain an intuition for the predictions of this model, we plot in <xref ref-type="fig" rid="fig2">Figure 2D</xref> for a specific set of parameters the two expected costs per item and their sum, <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>, as a function of <inline-formula><mml:math id="inf32"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>. The expected behavioral cost per item depends on <italic>p<sub>i</sub></italic> and decreases with <inline-formula><mml:math id="inf33"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> (colored curves in left panel), while the expected neural cost per item is independent of <italic>p<sub>i</sub></italic> and increases (black line in left panel). The expected total cost per item has a unique minimum (middle panel). The value of <inline-formula><mml:math id="inf34"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> corresponding to this minimum, <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, increases with <italic>p<sub>i</sub></italic> (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, right). Hence, in this example, the optimal amount of resource per item is an increasing function of its probing probability.</p><p>We next consider the special case in which each item is equally likely to be probed, that is, <italic>p<sub>i</sub></italic> = 1/<italic>N</italic>. The values of <italic>p<sub>i</sub></italic> in <xref ref-type="fig" rid="fig2">Figure 2D</xref> then correspond to set sizes 1, 2, 4, and 8. When replotting <inline-formula><mml:math id="inf36"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as a function of <italic>N</italic>, we find a set size effect (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, right panel) that is qualitatively similar to the empirical result in <xref ref-type="fig" rid="fig2">Figure 2C</xref>. An alternative way to understand this predicted set size effect is by considering how the three expected costs across all items, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, depend on <inline-formula><mml:math id="inf37"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>. Substituting <italic>p<sub>i</sub></italic> = 1/<italic>N</italic> in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, we find that the expected behavioral cost across all items is independent of set size (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, left panel, black curve). Moreover, when all items are encoded with the same amount of resource (which is necessarily the optimal solution when <italic>p<sub>i</sub></italic> is identical across items), the expected neural cost across all items equals <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>N</mml:mi><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and therefore scales linearly with set size (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, left panel, colored lines). The sum of these terms has a unique minimum <inline-formula><mml:math id="inf39"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, center panel), which monotonically decreases with set size (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, right panel). The costs plotted in <xref ref-type="fig" rid="fig2">Figure 2E</xref> can be considered as obtained by multiplying the corresponding costs in <xref ref-type="fig" rid="fig2">Figure 2D</xref> by <italic>N</italic>.</p><p>The model thus predicts set size effects in delayed-estimation tasks that are fully mediated by individual-item probing probability. The latter notion is consistent with empirical observations. <xref ref-type="bibr" rid="bib63">Palmer et al. (1993</xref>) reported that &quot;relevant set size&quot; (where irrelevance means <italic>p<sub>i</sub></italic> = 0) acts virtually identically to actual set size. <xref ref-type="bibr" rid="bib32">Emrich et al. (2017</xref>) independently varied probing probability and set size in their experiment, and found that the former was a better predictor of performance than the latter. Based on this, they hypothesized that set size effects are mediated by probing probability. The predictions of our model are qualitatively consistent with these findings.</p></sec><sec id="s2-2"><title>Model fits to data from delayed-estimation experiments with equal probing probabilities</title><p>To examine how well the model accounts for set size effects in empirical data, we fit it to data from six experiments that are part of a previously published benchmark set (E1-E6 in <xref ref-type="table" rid="table1">Table 1</xref>). We use a Bayesian optimization method (<xref ref-type="bibr" rid="bib3">Acerbi and Ma, 2017</xref>) to estimate the maximum-likelihood parameter values, separately for each individual data set (see <xref ref-type="table" rid="table2">Table 2</xref> for a summary of these estimates). The model accounts well for the subject-level error distributions (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) and the two statistics that summarize these distributions (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The original benchmark set (<xref ref-type="bibr" rid="bib86">van den Berg et al., 2014</xref>) contained four more data sets, but three of those were published in papers that were later retracted and another one contains data at only two set sizes. Although we decided to leave those four datasets out of our main analyses, the model accounts well for them too (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.007</object-id><label>Figure 3.</label><caption><title>Model fits to data from six delayed-estimation experiments with equal probing probabilities.</title><p>(<bold>A</bold>) Maximum-likelihood fits to raw data of the worst-fitting and best-fitting subjects (subjects S10 in E6 and S4 in E4, respectively). Goodness of fit was measured as <italic>R</italic><sup>2</sup>, computed for each subject by concatenating histograms across set sizes. (<bold>B</bold>) Subject-averaged circular variance and kurtosis of the estimation error, as a function of set size and split by experiment. The maximum-likelihood fits of the model account well for the trends in these statistics. (<bold>C</bold>) Estimated amounts of resource per item in the resource-rational model scattered against the estimates in the non-parametric model. Each dot represents estimates from a single subject. (<bold>D</bold>) Estimated amount of resource per item (red) and total resource (black) plotted against set size. Here and in subsequent figures, error bars and shaded areas represent 1 s.e.m. of the mean across subjects.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34963-fig3-v2"/></fig><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.008</object-id><label>Table 2.</label><caption><title>Subject-averaged parameter estimates of the resource-rational model fitted to data from nine previously published experiments.</title><p>See <xref ref-type="table" rid="table1">Table 1</xref> for details about the experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr valign="top"><th>Experiment</th><th><italic>β</italic></th><th><italic>λ</italic></th><th><italic>τ</italic></th></tr></thead><tbody><tr valign="top"><td>E1</td><td>1.87 ± 0.29</td><td>(4.8 ± 1.2)·10<sup>−2</sup></td><td>17.9±2.5</td></tr><tr valign="top"><td>E2</td><td>(1.33 ± 0.30)·10<sup>−2</sup></td><td>(4.27 ± 0.83)·10<sup>−4</sup></td><td>14.8±1.1</td></tr><tr valign="top"><td>E3</td><td>0.138 ± 0.042</td><td>(2.78 ± 0.87) ·10<sup>−3</sup></td><td>19.1±2.6</td></tr><tr valign="top"><td>E4</td><td>0.106 ± 0.052</td><td>(3.2 ± 1.4)·10<sup>−3</sup></td><td>8.2±1.8</td></tr><tr valign="top"><td>E5</td><td>0.356 ± 0.085</td><td>(5.8 ± 1.1)·10<sup>−3</sup></td><td>18.1±2.8</td></tr><tr valign="top"><td>E6</td><td>0.61 ± 0.15</td><td>(8.8 ± 1.5)·10<sup>−3</sup></td><td>7.4±1.3</td></tr><tr valign="top"><td>E7</td><td>1.19 ± 0.51</td><td>(9.5 ± 6.6)·10<sup>−2</sup></td><td>5.7±1.5</td></tr><tr valign="top"><td>E8</td><td>0.58 ± 0.19</td><td>(1.58 ± 0.66)·10<sup>−2</sup></td><td>27.0±3.7</td></tr><tr valign="top"><td>E9</td><td>0.93 ± 0.25</td><td>(3.0 ± 1.0)·10<sup>−2</sup></td><td>23.7±2.3</td></tr></tbody></table></table-wrap><p>We next compare the goodness of fit of the resource-rational model to that of a descriptive variant in which the amount of resource per item, <inline-formula><mml:math id="inf40"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>, is assumed to be a power-law function of set size (all other aspects of the model are kept the same). This variant is identical to the VP-A model in our earlier work, which is one of the most accurate descriptive models currently available (<xref ref-type="bibr" rid="bib86">van den Berg et al., 2014</xref>). Model comparison based on the Akaike Information Criterion (AIC) (<xref ref-type="bibr" rid="bib5">Akaike, 1974</xref>) indicates that the data provide similar support for both models, with a small advantage for the resource-rational model (ΔAIC = 5.27 ± 0.70; throughout the paper, X ± Y indicates mean ±s.e.m. across subjects). Hence, the resource-rational model provides a principled explanation of set size effects without sacrificing quality of fit compared to one of the best available descriptive models of VWM. We find that the resource-rational model also fits better than a model in which the total amount of resource is fixed and divided equally across items (ΔAIC = 13.9 ± 1.4).</p><p>So far, we have assumed that there is random variability in the actual amount of resource assigned to an item. Next, we test an equal-precision variant of the resource-rational model, by fixing parameter <italic>τ</italic> to a very small value (10<sup>−3</sup>). Consistent with the results obtained with the variable-precision model, we find that the rational model has a substantial AIC advantage over a fixed-resource model (ΔAIC = 43.0 ± 6.8) and is on equal footing with the power-law model (ΔAIC = 2.0 ± 1.7 in favor of the power-law model). However, all three equal-precision models (fixed resource, power law, rational) are outperformed by their variable-precision equivalents by over 100 AIC points. Therefore, we will only consider variable-precision models in the remainder of the paper.</p><p>To get an indication of the absolute goodness of fit of the resource-rational model, we next examine how much room for improvement there is in the fits. We do this by fitting a non-parametric model variant in which resource <inline-formula><mml:math id="inf41"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> is a free parameter at each set size, while keeping all other aspects of the model the same. We find a marginal AIC difference, suggesting that the fits of the rational model cannot be improved much further without overfitting the data (ΔAIC = 3.49 ± 0.93, in favor of the non-parametric model). An examination of the fitted parameter values corroborates this finding: the estimated resource values in the non-parametric model closely match the optimal values in the rational model (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>So far, we have assumed that behavioral cost is a power-law function of the absolute estimation error, <italic>c</italic><sub>behavioral</sub>(<italic>ε</italic>)=|<italic>ε</italic>|<italic><sup>β</sup></italic>. To evaluate the necessity of a free parameter in this function, we also test three parameter-free choices: |<italic>ε</italic>|, <italic>ε</italic><sup>2</sup>, and −cos(<italic>ε</italic>). Model comparison favors the original model with AIC differences of 14.0 ± 2.8, 24.4 ± 4.1, and 19.5 ± 3.5, respectively. While there may be other parameter-free functions that give better fits, we expect that a free parameter is unavoidable here, as the error-to-cost mapping may differ across experiments (because of differences in external incentives) and also across subjects within an experiment (because of differences in intrinsic motivation). Finally, we also test a two-parameter function that was proposed recently (Equation (5) in <xref ref-type="bibr" rid="bib81">Sims [2015</xref>]). The main difference with our original choice is that this alternative function allows for saturation effects in the error-to-cost mapping. However, this extra flexibility does not increase the goodness of fit sufficiently to justify the additional parameter, as the original model outperforms this variant with an AIC difference of 5.3 ± 1.8.</p><p>Finally, we use five-fold cross validation to verify the AIC-based results reported in this section. We find that they are all consistent (<xref ref-type="table" rid="table3">Table 3</xref>).</p><table-wrap id="table3" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.009</object-id><label>Table 3.</label><caption><title>Comparing two metrics for model comparison: AIC and five-fold cross-validated log likelihood.</title><p>Each comparison is between the main version of the resource-rational model, <xref ref-type="disp-formula" rid="equ11">Equation (11)</xref>, and the model listed in the first column of the table. Negative AIC differences and positive cross-validated log likelihood differences indicate an advantage of the resource-rational model over the alternative model. In all comparisons, these differences have opposite signs, which means that the AIC-based results are consistent with the cross-validation results.</p></caption><table frame="hsides" rules="groups"><thead><tr valign="top"><th>Model with which the main model is compared</th><th>AIC difference</th><th>Cross-validation log likelihood difference</th></tr></thead><tbody><tr valign="top"><td>Descriptive power-law model</td><td>−5.27±0.70</td><td>2.59±0.39</td></tr><tr valign="top"><td>Descriptive fixed-resource model</td><td>−13.9±1.4</td><td>8.4±1.0</td></tr><tr valign="top"><td>Descriptive unconstrained model</td><td>3.49±0.93</td><td>−1.26±0.49</td></tr><tr valign="top"><td>Rational model variant: equal precision</td><td>−110±10</td><td>56±4.7</td></tr><tr valign="top"><td>Rational model variant: <italic>c</italic><sub>behavioral</sub>=|<italic>ε</italic>|</td><td>−14±2.8</td><td>7.1±1.4</td></tr><tr valign="top"><td>Rational model variant: <italic>c</italic><sub>behavioral</sub>=<italic>ε</italic><sup>2</sup></td><td>−24.4±4.1</td><td>12.2±2.0</td></tr><tr valign="top"><td>Rational model variant: <italic>c</italic><sub>behavioral</sub>=−cos(ε)</td><td>−19.5±3.5</td><td>9.8±1.8</td></tr><tr valign="top"><td>Rational model variant: <italic>c</italic><sub>behavioral</sub> as in Sims (2015)</td><td>−5.3±1.8</td><td>4.7±0.74</td></tr></tbody></table></table-wrap></sec><sec id="s2-3"><title>Non-monotonic relation between total resource and set size</title><p>One quantitative feature that sets the resource-rational theory apart from previous theories is its predicted relation between set size and the total amount of invested resource, <inline-formula><mml:math id="inf42"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This quantity is by definition constant in fixed-resource models, and in power-law models it varies monotonically with set size. By contrast, we find that in the fits to several of the experiments, <inline-formula><mml:math id="inf43"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> varies <italic>non-monotonically</italic> with set size (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, gray curves). To examine whether there is evidence for non-monotonic trends in the subject data, we next compute an &quot;empirical&quot; estimate <inline-formula><mml:math id="inf44"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the best-fitting resource estimates in the non-parametric model. We find that these estimates show evidence of similar non-monotonic relations in some of the experiments (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, black circles). To quantify this evidence, we perform Bayesian paired t-tests in which we compare the estimates of <inline-formula><mml:math id="inf46"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> at set size 3 with the estimates at set sizes 1 and 6 in the experiments that included these three set sizes (E2 and E4-E6). These tests reveal strong evidence that the total amount of resource is higher at set size 3 than at set sizes 1 (BF<sub>+0</sub>=1.05·10<sup>7</sup>) and 6 (BF<sub>+0</sub>=4.02·10<sup>2</sup>). We next compute for each subject the set size at which <inline-formula><mml:math id="inf47"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is largest, which we denote by <italic>N</italic><sub>peak</sub>, and find a subject-averaged value of 3.52 ± 0.18. Altogether, these findings suggest that the total amount of resource that subjects spend on item encoding varies non-monotonically with set size, which is consistent with predictions from the resource-rational model, but not with any of the previously proposed models. To the best of our knowledge, evidence for a possible non-monotonicity in the relation between set size and total encoding resource has not been reported before.</p></sec><sec id="s2-4"><title>Predicted effects of probing probability</title><p>As we noted before, the model predictions do not explicitly depend on set size, <italic>N</italic>. Yet, we found that the model accounts well for set size effects in the experiments that we considered so far (E1-E6). This happens because in all those experiments, <italic>N</italic> was directly coupled with probing probability <italic>p<sub>i</sub></italic>, through <italic>p<sub>i</sub></italic> = 1/<italic>N</italic>. This coupling makes it impossible to determine whether changes in subjects’ encoding precision are the result of changes in <italic>N</italic> or changes in <italic>p<sub>i</sub></italic>. Therefore, we will next consider experiments in which individual probing probabilities and set size were varied independently of each other (E7-E9 in <xref ref-type="table" rid="table1">Table 1</xref>). According to our model, the effects of <italic>N</italic> that we found in E1-E6 were really effects of <italic>p<sub>i</sub></italic>. Therefore, we should be able to make predictions about effects of <italic>p<sub>i</sub></italic> in E7-E9 by recasting the effects of <italic>N</italic> in E1-E6 as effects of <italic>p<sub>i</sub></italic> = 1/<italic>N</italic>. Given that the amount of resource per item in E1-E6 decreases with <italic>N</italic>, a first prediction is that it should increase as a function of <italic>p<sub>i</sub></italic> in E7-E9. A second and particularly interesting prediction is that the estimated total amount of invested resource should vary non-monotonically with <italic>p<sub>i</sub></italic> and peak at a value <italic>p</italic><sub>peak</sub> that is close to 1/<italic>N</italic><sub>peak</sub> found in E1-E6 (see previous section). Based on the values of <italic>N</italic><sub>peak</sub> in experiments E1-E6, we find a prediction <italic>p</italic><sub>peak</sub> = 0.358 ± 0.026.</p></sec><sec id="s2-5"><title>Model fits to data from delayed-estimation experiments with unequal probing probabilities</title><p>To test the predictions presented in the previous section and, more generally, to evaluate how well our model accounts for effects of <italic>p<sub>i</sub></italic> on encoding precision, we fit it to data from three experiments in which probing probability was varied independently of set size (E7-E9 in <xref ref-type="table" rid="table1">Table 1</xref>).</p><p>In the first of these experiments (E7), seven subjects performed a delayed-estimation task at set sizes 2, 4, and 8. On each trial, one of the items – indicated with a cue – was three times more likely to be probed than any of the other items. Hence, the probing probabilities for the cued and uncued items were 3/4 and 1/4 at <italic>N</italic> = 2, respectively, 1/2 and 1/6 at <italic>N</italic> = 4, and 3/10 and 1/10 at <italic>N</italic> = 8. The subject data show a clear effect of <italic>p<sub>i</sub></italic>: the higher the probing probability of an item, the more precise the subject responses (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, top row, black circles). We find that the resource-rational model, <xref ref-type="disp-formula" rid="equ11">Equation (11)</xref>, accounts well for this effect (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, top row, curves) and does so by increasing the amount of resource as a function of probing probability <italic>p<sub>i</sub></italic> (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, left panel, red curves).</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.010</object-id><label>Figure 4.</label><caption><title>Model fits to data from three delayed-estimation experiments with unequal probing probabilities.</title><p>(<bold>A</bold>) Fits of the resource-rational model (curves) to the data (black circles) of experiments E7-E9. (<bold>B</bold>) Estimated amount of resource per item as a function of probing probability (red) and the corresponding estimated total amount of resource that the subject would spend on encoding a display filled with items with equal probing probabilities (black). (<bold>C</bold>) Error histograms and a plot of <inline-formula><mml:math id="inf48"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as a function of <italic>p<sub>i</sub></italic> for a single subject (S4 in E9). The estimated value of <italic>p</italic><sub>0</sub> was 0.18 for this subject, which was larger than the smallest probing probability in the experiment. The error histograms for items with the four lowest probing probabilities appear to be uniform for this subject, which is indicative of guessing (p&gt;0.23 in Kolgomorov-Smirnov tests for uniformity on these four distributions).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34963-fig4-v2"/></fig><p>In the other two experiments (E8 and E9), the number of cued items and cue validity were varied between conditions, while set size was kept constant at 4 or 6. For example, in one of the conditions of E8, three of the four items were cued with 100% validity, such that <italic>p<sub>i</sub></italic> was 1/3 for each cued item and 0 for the uncued item; in another condition of the same experiment, two of the four items were cued with 66.7% validity, meaning that <italic>p<sub>i</sub></italic> was 1/3 for each cued item and 1/6 for each uncued item. The unique values of <italic>p<sub>i</sub></italic> across all conditions were {0, 1/6, 2/9, 1/4, 1/3, 1/2, 1} in E8 and {0, 1/12, 1/10, 2/15, 1/6, 1/3, 1/2, and 1} in E9. As in E7, responses become more precise with increasing <italic>p<sub>i</sub></italic> and the model accounts well for this (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), again by increasing the amount of resource assigned to an item with <italic>p<sub>i</sub></italic> (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). </p><p>We next examine how our model compares to the models proposed in the papers that originally published these three data sets. In contrast to our model, both <xref ref-type="bibr" rid="bib16">Bays (2014</xref>) and <xref ref-type="bibr" rid="bib32">Emrich et al. (2017</xref>) proposed that the total amount of invested resource is fixed. However, while Bays proposed that the distribution of this resource is in accordance with minimization of a behavioral cost function (as in our model), Emrich et al. postulated that the resource is distributed in proportion to each item’s probing probability. Hence, while our model optimizes both the amount of invested resource and its distribution, Bays’ model only optimizes the distribution, and Emrich et al.’s model does not explicitly optimize anything. To examine how the three proposals compare in terms of how well they account for the data, we fit two variants of our model that encapsulate the main assumptions of these two earlier proposals. In the first variant, we compute <inline-formula><mml:math id="inf49"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="inf50"><mml:mrow><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow></mml:mstyle><mml:mrow><mml:mi>b</mml:mi><mml:mi>f</mml:mi><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> under the constraint <inline-formula><mml:math id="inf51"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which is consistent with Bays’ proposal. Hence, in this variant, the neural cost function is removed and parameter <italic>λ</italic> is replaced by a parameter <inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> – otherwise, all aspects of the model are the same as in our main model. In the variant that we use to test Emrich et al.’s proposal, we compute <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for each item as <inline-formula><mml:math id="inf54"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <italic>p<sub>i</sub></italic> is the probing probability and <inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is again a free parameter that represents the total amount of resource. Fitting the models to the data from all 47 subjects in E7-E9, we find a substantial advantage of our model over the proposal by Emrich et al., with an AIC difference of 18.0 ± 3.9. However, our model cannot reliably be distinguished from the proposal by Bays: either model is preferred in about half of the subjects (our model: 27; Bays: 20) and the subject-averaged AIC difference is negligible (1.8 ± 2.5 in favor of our model). Hence, the model comparison suggests quite convincingly that subjects distribute their resource near-optimally across items with unequal probing probabilities, but it is inconclusive regarding the question of whether the total amount of invested resource is fixed or optimized.</p><p>As an alternative way to address the question of whether the total amount of resource is fixed, we again fit a non-parametric model to obtain “empirical” estimates of the total amount of invested resource. To this end, we define <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the best-fitting values in a non-parametric model, such that <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the estimated total amount of resource that a subject would invest to encode a display filled with items that all have probing probability <italic>p<sub>i</sub></italic>. We find that these estimates show signs of a non-monotonicity as a function of <italic>p<sub>i</sub></italic> (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, black points), which are captured reasonably well by the resource-rational model (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, black curves). Averaged across all subjects in E7-E9, the value of <italic>p<sub>i</sub></italic> at which <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is largest is 0.384 ± 0.037, which is close to the predicted value of 0.358 ± 0.026 (see previous section). Indeed, a Bayesian independent-samples t-test supports the null hypothesis that there is no difference (BF<sub>01</sub> = 4.27). Hence, while the model comparison results in the previous paragraph were inconclusive regarding the question of whether the total amount of invested resource is fixed or optimized, the present analysis provides evidence against fixed-resource models and confirms a prediction made by our own model.</p><p>In summary, the results in this section show that effects of probing probability in E7-E9 are well accounted for by the same model as we used to explain effects of set size in E1-E6. Regardless of whether total resource is fixed or optimized, this finding provides further support for the suggestion that set size effects are mediated by probing probability (<xref ref-type="bibr" rid="bib32">Emrich et al., 2017</xref>) or, more generally, by item relevance (<xref ref-type="bibr" rid="bib63">Palmer et al., 1993</xref>).</p></sec><sec id="s2-6"><title>Is it ever optimal to not encode an item?</title><p>There is an ongoing debate about the question of whether a task-relevant item is sometimes completely left out of working memory (<xref ref-type="bibr" rid="bib4">Adam et al., 2017</xref>; <xref ref-type="bibr" rid="bib52">Luck and Vogel, 2013</xref>; <xref ref-type="bibr" rid="bib56">Ma et al., 2014</xref>; <xref ref-type="bibr" rid="bib70">Rouder et al., 2008</xref>). Specifically, slot models predict that this happens when set size exceeds the number of slots (<xref ref-type="bibr" rid="bib96">Zhang and Luck, 2008</xref>). In resource models, the possibility of complete forgetting has so far been an added ingredient separate from the core of the model (<xref ref-type="bibr" rid="bib86">van den Berg et al., 2014</xref>). Our normative theory allows for a reinterpretation of this question: are there situations in which it is optimal to assign zero resource to the encoding of an item? We already established that this could happen in delayed-estimation tasks: whenever the probing probability is lower than a threshold value <inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo> <mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>, the optimal amount of resource to invest on encoding the item is zero (see Theory). But what values does <italic>p</italic><sub>0</sub> take in practice? Considering the expected behavioral cost function of a fixed-precision model (a variable-precision model with <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>↓</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), we can prove that <italic>p</italic><sub>0</sub> = 0, that is, it is never optimal to invest no resource (Appendix 1). For the expected behavioral cost function of the variable-precision model, however, simulations indicate that <italic>p</italic><sub>0</sub> can be greater than 0 (we were not able to derive this result analytically). We next examine whether this ever happens under parameter values that are representative for human subjects. Using the maximum-likelihood parameters obtained from the data in E7-E9, we estimate that <italic>p</italic><sub>0</sub> (expressed as a percentage) equals 8.86 ± 0.54%. Moreover, we find that for 8 of the 47 subjects, <italic>p</italic><sub>0</sub> is larger than the lowest probing probability in the experiment, which suggests that these subjects sometimes entirely ignored one or more of the items. For these subjects, the error distributions on items with <italic>p<sub>i</sub></italic>&lt;<italic>p</italic><sub>0</sub> look uniform (see <xref ref-type="fig" rid="fig4">Figure 4C</xref> for an example) and Kolmogorov-Smirnov tests for uniformity did not reject the null hypothesis in any of these cases (<italic>p</italic>&gt;0.05 in all tests).</p><p>These results suggest that there might be a principled reason why people sometimes leave task-relevant items out of visual working memory in delayed-estimation experiments. However, our model cannot explain all previously reported evidence for this. In particular, when probing probabilities are equal for all items, the model makes an &quot;all or none&quot; prediction: all items are encoded when <italic>p<sub>i</sub></italic>&gt;<italic>p</italic><sub>0</sub> and none are encoded otherwise. Hence, the model cannot explain why subjects in tasks with equal probing probabilities sometimes seem to encode a subset of task-relevant items. For example, a recent study reported that in a whole-report delayed-estimation experiment (<italic>p<sub>i</sub></italic> = 1 for all items), subjects encoded about half of the six presented items on each trial (<xref ref-type="bibr" rid="bib4">Adam et al., 2017</xref>). Unless additional assumptions are made, our model cannot account for this finding.</p></sec><sec id="s2-7"><title>Predictions for a global task: whole-display change detection</title><p>The results so far show that the resource-rational model accounts well for data in a variety of delayed-estimation experiments. To examine how its predictions generalize to other tasks, we next consider a change detection task, which is another widely used paradigm in research on VWM. In this task, the observer is sequentially presented with two sets of items and reports if any one of them changed (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). In the variant that we consider here, a change is present on exactly half of the trials and is equally likely to occur in any of the items. We construct a model for this task by combining <xref ref-type="disp-formula" rid="equ3 equ4 equ8">Equations 3, 4, and 8</xref> with an expected behavioral cost function based on the Bayesian decision rule for this task (see Appendix 1), which yields</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.011</object-id><label>Figure 5.</label><caption><title>A resource-rational model for change detection tasks.</title><p>(<bold>A</bold>) Example of a trial in a change detection task with a set size of 2. The subject is sequentially presented with two sets of stimuli and reports whether there was a change at any of the item locations. (<bold>B</bold>) Simulated expected total cost in the resource-rational cost function applied to a task with a set size of 2 and a reward of 0.05 (left), 0.20 (center), or 0.35 (right) units per correct trial. The red dot indicates the location of minimum cost, that is the resource-optimal combination of <inline-formula><mml:math id="inf62"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (note that the expected cost function in the central panel has a minimum at two distinct locations). When reward is low (left), the optimal strategy is to encode neither of the two stimuli. When reward is high (right), the optimal strategy is to encode both stimuli with equal amounts of resource. For intermediate reward (center), the optimal strategy is to encode one of the two items, but not the other one. (<bold>C</bold>) Model predictions as a function of trial rewards at <italic>N</italic> = 2. Left: The amount of resource assigned to the two items for a range of reward values. Right: the corresponding optimal number of encoded items (top) and optimal amount of resource per encoded item (bottom) as a function of reward. (<bold>D</bold>) Model predictions as a function of set size (trial reward = 1.5). The model predicts set size effects in both the number of encoded items (left, top) and the amount of resource with which these items are encoded (left, bottom). Moreover, the model produces response data (right) that are qualitatively similar to human data (see, for example, <xref ref-type="fig" rid="fig2">Figure 2C</xref> in <xref ref-type="bibr" rid="bib44">Keshvari et al., 2013</xref>). The parameter values used in all simulations were <italic>λ</italic> = 0.01 and τ↓0.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34963-fig5-v2"/></fig><p><disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow></mml:mstyle><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:munder><mml:mtext> </mml:mtext><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>error</mml:mtext><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>error</mml:mtext><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the expected behavioral cost function, which in this case specifies the probability of an error response when a set of items is encoded with resource <inline-formula><mml:math id="inf65"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:math></inline-formula>.</p><p>In contrast to local tasks, the expected total cost in global tasks cannot be written as a sum of expected costs per item, because the expected behavioral cost – such as <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>error</mml:mtext><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ14">Equation (14)</xref> – can only be computed globally, not per item. Consequently, the elements of <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> in global tasks cannot be computed separately for each item. This makes resource optimization computationally much more demanding, because it requires solving an <italic>N</italic>-dimensional minimization problem instead of <italic>N</italic> one-dimensional problems.</p><p>We perform a simulation at <italic>N</italic> = 2 (which is still tractable) to get an intuition of the predictions that follow from <xref ref-type="disp-formula" rid="equ14">Equation (14)</xref>. For practical convenience, we assume in this simulation that there is no variability in precision, <italic>τ</italic>↓0, such that <italic>λ</italic> is the only model parameter. The results (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) show that the cost-minimizing strategy is to encode neither of the items when the amount of reward per correct trial is very low (left panel) and encode them both when reward is high (right panel). However, interestingly, there is also an intermediate regime in which the optimal strategy is to encode one of the two items, but not the other one (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, central panel). Hence, just as in the delayed-estimation task, there are conditions in which it is optimal to encode only a subset of items. An important difference, however, is that in the delayed-estimation task this only happens when items have unequal probing probabilities, while in this change detection task it even happens when all items are equally likely to change.</p><p>Simulations at larger set sizes quickly become computationally intractable, because of the reason mentioned above. However, the results at <italic>N</italic> = 2 suggest that if two items are encoded, the optimal solution is to encode them with the same amount of resource (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Therefore, we conjecture that all non-zero values in <inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are identical, which would mean that the entire vector can be summarized by two values: the number of encoded items, which we denote by <italic>K</italic><sub>optimal</sub>, and the amount of resource assigned to each encoded item, which we denote by <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Using this conjecture (which we have not yet been able to prove), we are able to efficiently compute predictions at an arbitrary set size. Simulation results show that the model then predicts that both <italic>K</italic><sub>optimal</sub> and <inline-formula><mml:math id="inf70"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> depend on set size (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, left) and produces response data that are qualitatively similar to human data (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, right).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Summary</title><p>Descriptive models of visual working memory (VWM) have evolved to a point where there is little room for improvement in how well they account for experimental data. Nevertheless, the basic finding that VWM precision depends on set size still lacks a principled explanation. Here, we examined a normative proposal in which expected task performance is traded off against the cost of spending neural resource on encoding. We used this principle to construct a resource-rational model for &quot;local&quot; VWM tasks and found that set size effects in this model are fully mediated by the probing probabilities of the individual items; this is consistent with suggestions from earlier empirical work (<xref ref-type="bibr" rid="bib32">Emrich et al., 2017</xref>; <xref ref-type="bibr" rid="bib63">Palmer et al., 1993</xref>). From the perspective of our model, the interpretation is that as more items are added to a task, the relevance of each individual item decreases, which makes it less cost-efficient to spend resource on its encoding. We also found that in this model it is sometimes optimal to encode only a subset of task-relevant items, which implies that resource rationality could serve as a principled bridge between resource and slot-based models of VWM. We tested the model on data from nine previous delayed-estimation experiments and found that it accounts well for effects of both set size and probing probability, despite having relatively few parameters. Moreover, it accounts for a non-monotonicity that appears to exist between set size and the total amount of resource that subjects invest in item encoding. The broader implication of our findings is that VWM limitations – and cognitive limitations in general – may be driven by a mechanism that minimizes a cost, instead of by a fixed constraint on available encoding resource.</p></sec><sec id="s3-2"><title>Limitations</title><p>Our theory makes a number of assumptions that need further investigation. First, we have assumed that the expected behavioral cost decreases indefinitely with the amount of invested resource, such that in the limit of infinite resource there is no encoding error and no behavioral cost. However, encoding precision in VWM is fundamentally limited by the precision of the sensory input, which is itself limited by irreducible sources of neural noise – such as Johnson noise and Poisson shot noise (<xref ref-type="bibr" rid="bib33">Faisal et al., 2008</xref>; <xref ref-type="bibr" rid="bib83">Smith, 2015</xref>) – and suboptimalities in early sensory processing (<xref ref-type="bibr" rid="bib17">Beck et al., 2012</xref>). One way to incorporate this limitation is by assuming that there is a resource value <inline-formula><mml:math id="inf71"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>input</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> beyond which the expected behavioral cost no longer decreases as a function of <inline-formula><mml:math id="inf72"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>. In this variant, <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>input</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the quality of the input and <inline-formula><mml:math id="inf74"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> will never exceed this value, because any additional resource would increase the expected neural cost without decreasing the expected behavioral cost.</p><p>Moreover, our theory assumes that there is no upper limit on the total amount of resource available for encoding: cost is the only factor that matters. However, as the brain is a finite entity, the total amount of resource must obviously have an upper limit. This limit can be incorporated by optimizing <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>J</mml:mi></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> under the constraint <inline-formula><mml:math id="inf76"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the maximum amount of resource that can be invested. While an upper limit certainly exists, it may be much higher than the average amount of resource needed to encode information with the same fidelity as the sensory input. If that is the case, then <inline-formula><mml:math id="inf78"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>input</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> would be the constraining factor and <inline-formula><mml:math id="inf79"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> would have no effect.</p><p>Similarly, our theory assumes that there is no lower limit on the amount of resource available for encoding. However, there is evidence that task-irrelevant stimuli are sometimes automatically encoded (<xref ref-type="bibr" rid="bib95">Yi et al., 2004</xref>; <xref ref-type="bibr" rid="bib76">Shin and Ma, 2016</xref>), perhaps because in natural environments few stimuli are ever completely irrelevant. This would mean that there is a lower limit to the amount of resource spent on encoding. In contradiction to the predictions of our model, such a lower limit would prevent subjects from sometimes encoding nothing at all. For local tasks, such a lower limit can be incorporated by assuming that probing probability <italic>p<sub>i</sub></italic> is never zero.</p><p>We have fitted our model only to data from delayed-estimation experiments. However, it applies without modification to other local tasks, such as single-probe change detection (<xref ref-type="bibr" rid="bib51">Luck and Vogel, 1997</xref>; <xref ref-type="bibr" rid="bib85">Todd and Marois, 2004</xref>) and single-probe change discrimination (<xref ref-type="bibr" rid="bib45">Klyszejko et al., 2014</xref>). Further work is needed to examine how well the model accounts for empirical data of such tasks. Moreover, it should further examine how the theory generalizes to global tasks. One such task could be whole-report change detection; we presented simulation results for this task but the theory remains to be further worked out and fitted to the data.</p><p>A final limitation is that our theory assumes that items are uniformly distributed and uncorrelated. Although this is correct for most experimental settings, items in more naturalistic settings are often correlated and can take non-uniform distributions. In such environments, the expected total cost can probably be further minimized by taking into account statistical regularities (<xref ref-type="bibr" rid="bib62">Orhan et al., 2014</xref>). Moreover, recent work has suggested that even when items are uncorrelated and uniformly distributed, the expected estimation error can sometimes be reduced by using a &quot;chunking&quot; strategy, that is, encoding similar items as one (<xref ref-type="bibr" rid="bib58">Nassar et al., 2018</xref>). However, as Nassar et al. assumed a fixed total resource and did not take neural encoding cost into account in their optimization, it remains to be seen whether chunking is also optimal in the kind of model that we proposed. We speculate that this is likely to be the case, because encoding multiple items as one will reduce the expected neural cost (fewer items to encode), while the increase in expected behavioral cost will be negligible if the items are very similar. Hence, it seems worthwhile to examine models that combine resource rationality with chunking.</p></sec><sec id="s3-3"><title>Variability in resource assignment</title><p>Throughout the paper, we have assumed that there is variability in resource assignment. Part of this variability is possibly a result of stochastic factors, but part of it may also be systematic – for example, particular colors and orientations may be encoded with higher precision than others (<xref ref-type="bibr" rid="bib10">Bae et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Girshick et al., 2011</xref>). Whereas the systematic component could have a rational basis (e.g. higher precision for colors and orientations that occur more frequently in natural scenes [<xref ref-type="bibr" rid="bib36">Ganguli and Simoncelli, 2010</xref>; <xref ref-type="bibr" rid="bib92">Wei and Stocker, 2015</xref>]), this is unlikely to be true for the random component. Indeed, when we jointly optimize <inline-formula><mml:math id="inf80"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>and <italic>τ</italic> in <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>, we find estimates of <italic>τ</italic> that consistently approach 0, meaning that any variability in encoding precision is suboptimal under our proposed cost function. One way to reconcile this apparent suboptimality with the otherwise normative theory is to postulate that maintaining exactly equal resource assignment across cortical regions may itself be a costly process; under such a cost, it could be optimal to allow for some variability in resource assignment. Another possibility is that there are unavoidable imperfections in mental inference (<xref ref-type="bibr" rid="bib30">Drugowitsch et al., 2016</xref>) that make it impossible to compute <inline-formula><mml:math id="inf81"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> without error, such that the outcome of the computation will vary from trial to trial even when the stimuli are identical.</p></sec><sec id="s3-4"><title>Experimental predictions of incentive manipulations</title><p>In the present study, we have focused on effects of set size and probing probability on encoding precision. However, our theory also makes predictions about effects of incentive manipulations on encoding precision, because such manipulations affect the expected behavioral cost function.</p><p>Incentives can be experimentally manipulated in a variety of ways. One method used in at least two previously published delayed-estimation experiments is to make the feedback binary (&quot;correct,&quot; &quot;error&quot;) and vary the value of the maximum error allowed to receive positive feedback (<xref ref-type="bibr" rid="bib97">Zhang and Luck, 2011</xref>; <xref ref-type="bibr" rid="bib58">Nassar et al., 2018</xref>). In both studies, subjects in a &quot;low precision&quot; condition received positive feedback whenever their estimation error was smaller than a threshold value of π/3. Subjects in the &quot;high precision&quot; condition, however, received positive feedback only when the error was smaller than π/12 (<xref ref-type="bibr" rid="bib97">Zhang and Luck, 2011</xref>) or π/8 (<xref ref-type="bibr" rid="bib58">Nassar et al., 2018</xref>). Neither of the two studies found evidence for a difference in encoding precision between the low- and high-precision conditions. At first, this may seem to be at odds with the predictions of our model, as one may expect that it should assign more resource to items in the high-precision condition. To test whether this is the case, we simulated this experimental manipulation using a behavioral cost function <italic>c</italic><sub>behavioral,<italic>i</italic></sub>(<italic>ε</italic>) that maps values of |<italic>ε</italic>| smaller than the feedback threshold to 0 and larger values to 1. The results reveal that the model predictions are not straightforward and that it can actually account for the absence of an effect (<xref ref-type="fig" rid="fig6">Figure 6</xref>). In particular, the simulation results suggest that the experimental manipulations in the studies by Zhang and Luck and Nassar et al. may not have been strong enough to measure an effect. Indeed, another study has criticized the study by Zhang and Luck on exactly this point and did find an effect when using an experimental design with stronger incentives (<xref ref-type="bibr" rid="bib34">Fougnie et al., 2016</xref>).</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.012</object-id><label>Figure 6.</label><caption><title>Model predictions for a delayed-estimation task with binary feedback (<italic>N</italic> = 5).</title><p>In this experiment, the observer receives positive feedback (e.g. &quot;correct&quot;) when their estimation error is smaller than the positive feedback threshold and negative feedback (e.g. &quot;error&quot;) otherwise. We modelled this using a behavioral cost function that maps errors below the feedback threshold to a cost of 0 and errors larger than this threshold to a cost equal to 1. The model predicts that subjects do not invest any resource when the feedback threshold is very small (extremely difficult tasks) or very large (extremely easy tasks), such that the expected absolute estimation error is π/2 (guessing). In an intermediate regime, the prediction is U-shaped and contains a region in which the predicted estimation error barely changes as a function of feedback threshold. In this region, any performance benefit from increasing the amount of invested resource is almost exactly outdone by the added neural cost. The dashed lines show the feedback thresholds corresponding to the &quot;high precision&quot; and &quot;low precision&quot; conditions in the experiment by <xref ref-type="bibr" rid="bib58">Nassar et al. (2018</xref>). Under the chosen parameter settings (<italic>λ</italic> = 0.08, <italic>τ</italic> = 30), the model predicts that the average absolute estimation errors in these two conditions (black circles) are very similar to each other.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34963-fig6-v2"/></fig><p>Another method to manipulate incentives is to vary the amount of potential reward across items within a display. For example, Klyszejko and colleagues performed a local change discrimination experiment in which the monetary reward for a correct response depended on which item was probed (<xref ref-type="bibr" rid="bib45">Klyszejko et al., 2014</xref>). They found a positive relation between the amount of reward associated with an item and response accuracy, which indicates that subjects spent more resource on encoding items with larger potential reward. This incentive manipulation can be implemented by multiplying the behavioral cost function with an item-dependent factor <italic>u<sub>i</sub></italic>, which modifies <xref ref-type="disp-formula" rid="equ11">Equation (11)</xref> to <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>;</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The coefficients <italic>u<sub>i</sub></italic> and <italic>p<sub>i</sub></italic> can be combined into a single &quot;item relevance&quot; coefficient <italic>r<sub>i</sub></italic> = <italic>u<sub>i</sub>p<sub>i</sub></italic>, and all theoretical results and predictions that we derived for <italic>p<sub>i</sub></italic> now apply to <italic>r<sub>i</sub></italic>.</p><p>A difference between the two discussed methods is that the former varied incentives within a trial and the latter across trials. However, both methods can be applied in both ways. A within-trial variant of the experiments by <xref ref-type="bibr" rid="bib97">Zhang and Luck (2011</xref>) and <xref ref-type="bibr" rid="bib58">Nassar et al. (2018</xref>) would be a <italic>N</italic> = 2 task in which one of the items always has a low positive feedback threshold and the other a high one. Similarly, a between-trial variant of the experiment by <xref ref-type="bibr" rid="bib45">Klyszejko et al. (2014</xref>) would be to scale the behavioral cost function of items with a factor that varies across trials or blocks, but is constant within a trial. Our model can be used to derive predictions for these task variants, which to our knowledge have not been previously reported in the published literature.</p></sec><sec id="s3-5"><title>Neural mechanisms and timescale of optimization</title><p>Our results raise the question of what neural mechanism could implement the optimal allocation policy that forms the core of our theory. Some form of divisive normalization (<xref ref-type="bibr" rid="bib16">Bays, 2014</xref>; <xref ref-type="bibr" rid="bib20">Carandini and Heeger, 2012</xref>) would be a likely candidate, which is already a key operation in neural models of attention (<xref ref-type="bibr" rid="bib69">Reynolds and Heeger, 2009</xref>) and visual working memory (<xref ref-type="bibr" rid="bib16">Bays, 2014</xref>; <xref ref-type="bibr" rid="bib93">Wei et al., 2012</xref>). The essence of this mechanism is that it lowers the gain when set size is larger, without requiring explicit knowledge of the set size prior to the presentation of the stimuli. Consistent with the predictions of this theory, empirical work has found that the neural activity associated with the encoding of an item decreases with set size, as observed in for example the lateral intraparietal cortex (<xref ref-type="bibr" rid="bib24">Churchland et al., 2008</xref>; <xref ref-type="bibr" rid="bib11">Balan et al., 2008</xref>) and superior colliculus (<xref ref-type="bibr" rid="bib13">Basso and Wurtz, 1998</xref>). Moreover, the work by <xref ref-type="bibr" rid="bib16">Bays (2014</xref>) has shown that a modified version of divisive normalization can account for the near-optimal distribution of resources across items with unequal probing probabilities. As set size effects in our model are mediated by probing probability, its predicted set size effects can probably be accounted for by a similar mechanism.</p><p>Another question concerns the timescale at which the optimization takes place. In all experimental data that we considered here, the only factors that changed from trial to trial were set size (E1-E7) and probing probability (E7-E9). When we fitted the model, we assumed that the expected total cost in these experiments was minimized on a trial-by-trial basis: whenever set size or probing probability changed from one trial to the next, the computation of <inline-formula><mml:math id="inf83"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>J</mml:mi></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> followed this change. This assumption accounted well for the data and, as discussed above, previous work has shown that divisive normalization can accommodate trial-by-trial changes in set size and probing probability. However, can the same mechanism also accommodate changes in the optimal resource policy changes driven by other factors, such as the behavioral cost function, <italic>c</italic><sub>behavioral</sub>(<italic>ε</italic>)? From a computational standpoint, divisive normalization is a mapping from an input vector of neural activities to an output vector, and the shape of this mapping depends on the parameters of the mechanism (such as gain, weighting factors, and a power on the input). As the mapping is quite flexible, we expect that it can accommodate a near-optimal allocation policy for most experimental conditions. However, top-down control and some form of learning (e.g. reinforcement learning) are likely required to adjust the parameters of the normalization mechanism, which would prohibit instantaneous optimality after a change in the experimental conditions.</p></sec><sec id="s3-6"><title>Neural prediction</title><p>The total amount of resource that subjects spend on item encoding may vary non-monotonically with set size in our model. At the neural level, this translates to a prediction of a non-monotonic relation between population-level spiking activity and set size. We are not aware of any studies that have specifically addressed this prediction, but it can be tested using neuroimaging experiments similar to previously conducted experiments. For example, Balan et al. used single-neuron recording to estimate neural activity per item for set sizes 2, 4, and 6 in a visual search task (<xref ref-type="bibr" rid="bib11">Balan et al., 2008</xref>). To test for the existence of the predicted non-monotonicity, the same recoding techniques can be used in a VWM task with a more fine-grained range of set sizes. Even though it is practically impossible to directly measure population-level activity, reasonable estimates may be obtained by multiplying single-neuron recordings with set size (under the assumption that an increase in resource translates to an increase in firing rate and not an increase of neurons used to encode an item). A similar method can also assess the relation between an item’s probing probability and the spiking activity related to its neural encoding.</p></sec><sec id="s3-7"><title>Extensions to other domains</title><p>Our theory might apply beyond working memory tasks. In particular, it has been speculated that the selectivity of attention arises from a need to balance performance against the costs associated with spiking (<xref ref-type="bibr" rid="bib66">Pestilli and Carrasco, 2005</xref>; <xref ref-type="bibr" rid="bib48">Lennie, 2003</xref>). Our theory provides a normative formalism to test this speculation and may thus explain set size effects in attention tasks (<xref ref-type="bibr" rid="bib49">Lindsay et al., 1968</xref>; <xref ref-type="bibr" rid="bib73">Shaw, 1980</xref>; <xref ref-type="bibr" rid="bib55">Ma and Huang, 2009</xref>).</p><p>Furthermore, developmental studies have found that that working memory capacity estimates change with age (<xref ref-type="bibr" rid="bib77">Simmering and Perone, 2012</xref>; <xref ref-type="bibr" rid="bib78">Simmering, 2012</xref>). Viewed from the perspective of our proposed theory, this raises the question of why the optimal trade-off between behavioral and neural cost would change with age. A speculative answer is that a subject's coding efficiency – formalized by the reciprocal of parameter <italic>α</italic> in <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> – may improve during childhood: an increase in coding efficiency reduces the neural cost per unit of precision, which shifts the optimal amount of resource to use for encoding to larger values. Neuroimaging studies might provide insight into whether and how coding efficiency changes with age, for example by estimating the amount of neural activity required per unit of precision in memory representations.</p></sec><sec id="s3-8"><title>Broader context</title><p>Our work fits into a broader tradition of normative theories in psychology and neuroscience (<xref ref-type="table" rid="table4">Table 4</xref>). The main motivation for such theories is to reach a deeper level of understanding by analyzing a system in the context of the ecological needs and constraints under which it evolved. Besides work on ideal-observer decision rules (<xref ref-type="bibr" rid="bib40">Green and Swets, 1966</xref>; <xref ref-type="bibr" rid="bib46">KordingKörding, 2007</xref>; <xref ref-type="bibr" rid="bib37">Geisler, 2011</xref>; <xref ref-type="bibr" rid="bib74">Shen and Ma, 2016</xref>) and on resource-limited approximations to optimal inference (<xref ref-type="bibr" rid="bib38">Gershman et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Griffiths et al., 2015</xref>; <xref ref-type="bibr" rid="bib90">Vul and Pashler, 2008</xref>; <xref ref-type="bibr" rid="bib91">Vul, 2009</xref>), normative approaches have also been used at the level of neural coding. For example, properties of receptive fields (<xref ref-type="bibr" rid="bib89">Vincent et al., 2005</xref>; <xref ref-type="bibr" rid="bib50">Liu et al., 2009</xref>; <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1996</xref>), tuning curves (<xref ref-type="bibr" rid="bib8">Attneave, 1954</xref>; <xref ref-type="bibr" rid="bib12">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib36">Ganguli and Simoncelli, 2010</xref>), neural architecture (<xref ref-type="bibr" rid="bib21">Cherniak, 1994</xref>; <xref ref-type="bibr" rid="bib22">Chklovskii et al., 2002</xref>), receptor performance (<xref ref-type="bibr" rid="bib47">Laughlin, 2001</xref>), and neural network modularity (<xref ref-type="bibr" rid="bib25">Clune et al., 2013</xref>) have been explained as outcomes of optimization under either a cost or a hard constraint (on total neural firing, sparsity, or wiring length), and are thus mathematically closely related to the theory presented here. However, a difference concerns the timescale at which the optimization takes place: while optimization in the context of neural coding is typically thought to take place at the timescale over which the statistics of the environment change or a developmental timescale, the theory that we presented here could optimize on a trial-by-trial basis to follow changes in task properties.</p><table-wrap id="table4" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.013</object-id><label>Table 4.</label><caption><title>Examples of resource-rationality concepts in neuroscience, psychology, and economics.</title></caption><table frame="hsides" rules="groups"><thead><tr valign="top"><th>Study</th><th>Optimized quantity</th><th>Performance term</th><th>Resource cost/constraint</th></tr></thead><tbody><tr><td colspan="4" valign="top">Efficient coding in neural populations</td></tr><tr valign="top"><td><xref ref-type="bibr" rid="bib36">Ganguli and Simoncelli (2010</xref>)</td><td>Tuning curve spacing and width</td><td>Fisher information or discriminability</td><td>Neural activity (constraint)</td></tr><tr valign="top"><td><xref ref-type="bibr" rid="bib61">Olshausen and Field (1996</xref>)</td><td>Receptive field specificity</td><td>Information</td><td>Sparsity</td></tr><tr><td colspan="4" valign="top">Capacity “limitations” in attention and memory</td></tr><tr valign="top"><td><xref ref-type="bibr" rid="bib80">Sims et al. (2012</xref>)</td><td>Information channel bit allocation</td><td>Channel distortion (e.g. squared error)</td><td>Channel capacity (constraint)</td></tr><tr valign="top"><td>Van den Berg and Ma (present study)</td><td>Mean encoding precision</td><td>Behavioral task accuracy</td><td>Neural activity (cost)</td></tr><tr><td colspan="4">Rational inattention in consumer choice</td></tr><tr valign="top"><td><xref ref-type="bibr" rid="bib79">Sims (2003</xref>)</td><td>Distribution of attention</td><td>Channel distortion (e.g. squared error)</td><td>Channel capacity (constraint)</td></tr></tbody></table></table-wrap><p>We already mentioned the information-theory models of working memory developed by Chris R. Sims et al. A very similar framework has been proposed by Chris A. Sims in behavioral economics, who used information theory to formalize his hypothesis of &quot;rational inattention,&quot; that is, the hypothesis that consumers make optimal decisions under a fixed budget of attentional resources that can be allocated to process economic data (<xref ref-type="bibr" rid="bib79">Sims, 2003</xref>). The model presented here differs from these two approaches in two important ways. First, similar to early models of visual working memory limitations, they postulate a fixed total amount of resources (formalized as channel capacity), which is a constraint rather than a cost. Second, even if it had been a cost, it would have been the expected value of a log probability ratio. Unlike neural spike count, a log probability ratio does not obviously map to a biologically meaningful cost on a single-trial level. Nevertheless, recent work has attempted to bridge rational inattention and attention in a psychophysical setting (<xref ref-type="bibr" rid="bib19">Caplin et al., 2018</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Data and code sharing</title><p>Data from experiments E1-E7 (Table 1) and Matlab code for model fitting and simulations are available at <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5061/dryad.nf5dr6c">http://dx.doi.org/10.5061/dryad.nf5dr6c</ext-link>.</p></sec><sec id="s4-2"><title>Statistical analyses</title><p>Bayesian t-tests were performed using the JASP software package (<xref ref-type="bibr" rid="bib42">JASP Team, 2017</xref>) with the scale parameter of the Cauchy prior set to its default value of 0.707.</p></sec><sec id="s4-3"><title>Model fitting</title><p>We used a Bayesian optimization method (<xref ref-type="bibr" rid="bib3">Acerbi and Ma, 2017</xref>) to find the parameter vector <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that maximizes the log likelihood function, <inline-formula><mml:math id="inf85"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mstyle><mml:mtext> </mml:mtext><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ε</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic>n</italic> is the number of trials in the subject’s data set, <italic>ε<sub>i</sub></italic> the estimation error on the <italic>i</italic><sup>th</sup> trial, and <italic>p<sub>i</sub></italic> the probing probability of the probed item on that trial. To reduce the risk of converging into a local maximum, initial parameter estimates were chosen based on a coarse grid search over a large range of parameter values. The predicted estimation error distribution for a given parameter vector <bold>θ</bold> and probing probability <italic>p<sub>i</sub></italic> was computed as follows. First, <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was computed by applying Matlab's fminsearch function to <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>. Thereafter, the gamma distribution over <italic>J</italic> (with mean <inline-formula><mml:math id="inf87"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and shape parameter <italic>τ</italic>) was discretized into 50 equal-probability bins. The predicted (Von Mises) estimation error distribution was then computed under the central value of each bin. Finally, these 50 predicted distributions were averaged. We verified that increasing the number of bins used in the numerical approximation of the integral over <italic>J</italic> did not substantially affect the results.</p></sec><sec id="s4-4"><title>Model comparison using cross-validation</title><p>In the cross-validation analysis, we fitted the models in the same way as described above, but using only 80% of the data. We did this five times, each time leaving out a different subset of 20% of the data (in the first run we left out trials 1, 6, 11; in the second run we left out trials 2, 7, 12, etc.). At the end of each run, we used the maximum-likelihood parameter estimates to compute the log likelihood of the 20% of trials that were left out. These log likelihood values were then combined across the five runs to give an overall cross-validated log likelihood value for each model.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgments</title><p>This work was funded by grant R01EY020958 from the National Institutes of Health, grant 2015–00371 by the Swedish Resarch Council, and grant INCA 600398 by Marie Sklodowska Curie Actions. We thank all authors of the papers listed in Table 1 for making their data available.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Formal analysis, Supervision, Visualization, Methodology, Writing—original draft</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.34963.014</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-34963-transrepform-v2.pdf"/></supplementary-material><sec id="s10" sec-type="data-availability"><title>Data availability</title><p>Data from experiments E1-E7 (Table 1) and Matlab code for model fitting and simulations are available at <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5061/dryad.nf5dr6c">http://dx.doi.org/10.5061/dryad.nf5dr6c</ext-link>.</p><p>The following dataset was generated:</p><p><related-object content-type="generated-dataset" id="dataset1" source-id="http://dx.doi.org/10.5061/dryad.nf5dr6c" source-id-type="uri"><collab collab-type="author">Ronald van den Berg</collab><collab collab-type="author">Wei Ji Ma</collab><year>2018</year><source>Data from: A resource-rational theory of set size effects in human visual working memory</source><ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5061/dryad.nf5dr6c">http://dx.doi.org/10.5061/dryad.nf5dr6c</ext-link><comment>Available at Dryad Digital Repository under a CC0 Public Domain Dedication</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The effect of correlated variability on the accuracy of a population code</article-title><source>Neural Computation</source><volume>11</volume><fpage>91</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1162/089976699300016827</pub-id><pub-id pub-id-type="pmid">9950724</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Abramowitz</surname> <given-names>M</given-names></name><name><surname>Stegun</surname> <given-names>IA</given-names></name></person-group><year iso-8601-date="1972">1972</year><source>Handbook of Mathematical Functions: With Formulas, Graphs, and Mathematical Tables</source><volume>Vol. 55</volume><publisher-loc>New York</publisher-loc><publisher-name>Dover Publications</publisher-name><fpage>886</fpage></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Acerbi</surname> <given-names>L</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical bayesian optimization for model fitting with bayesian adaptive direct search</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.04405">https://arxiv.org/abs/1705.04405</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adam</surname> <given-names>KCS</given-names></name><name><surname>Vogel</surname> <given-names>EK</given-names></name><name><surname>Awh</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Clear evidence for item limits in visual working memory</article-title><source>Cognitive Psychology</source><volume>97</volume><fpage>79</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2017.07.001</pub-id><pub-id pub-id-type="pmid">28734172</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akaike</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>A new look at the statistical model identification</article-title><source>IEEE Transactions on Automatic Control</source><volume>19</volume><fpage>716</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1109/TAC.1974.1100705</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>DE</given-names></name><name><surname>Awh</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>RETRACTED ARTICLE: the plateau in mnemonic resolution across large set sizes indicates discrete resource limits in visual working memory</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>74</volume><fpage>891</fpage><lpage>910</lpage><pub-id pub-id-type="doi">10.3758/s13414-012-0292-1</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>DE</given-names></name><name><surname>Vogel</surname> <given-names>EK</given-names></name><name><surname>Awh</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Precision in visual working memory reaches a stable plateau when individual item limits are exceeded</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>1128</fpage><lpage>1138</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4125-10.2011</pub-id><pub-id pub-id-type="pmid">21248137</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attneave</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Some informational aspects of visual perception</article-title><source>Psychological Review</source><volume>61</volume><fpage>183</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1037/h0054663</pub-id><pub-id pub-id-type="pmid">13167245</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attwell</surname> <given-names>D</given-names></name><name><surname>Laughlin</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An energy budget for signaling in the grey matter of the brain</article-title><source>Journal of Cerebral Blood Flow &amp; Metabolism</source><volume>21</volume><fpage>1133</fpage><lpage>1145</lpage><pub-id pub-id-type="doi">10.1097/00004647-200110000-00001</pub-id><pub-id pub-id-type="pmid">11598490</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bae</surname> <given-names>GY</given-names></name><name><surname>Olkkonen</surname> <given-names>M</given-names></name><name><surname>Allred</surname> <given-names>SR</given-names></name><name><surname>Wilson</surname> <given-names>C</given-names></name><name><surname>Flombaum</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Stimulus-specific variability in color working memory with delayed estimation</article-title><source>Journal of Vision</source><volume>14</volume><fpage>7</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1167/14.4.7</pub-id><pub-id pub-id-type="pmid">24715329</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balan</surname> <given-names>PF</given-names></name><name><surname>Oristaglio</surname> <given-names>J</given-names></name><name><surname>Schneider</surname> <given-names>DM</given-names></name><name><surname>Gottlieb</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neuronal correlates of the set-size effect in monkey lateral intraparietal area</article-title><source>PLoS Biology</source><volume>6</volume><elocation-id>e158</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060158</pub-id><pub-id pub-id-type="pmid">18656991</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname> <given-names>HBH</given-names></name></person-group><year iso-8601-date="1961">1961</year><chapter-title>Possible principles underlying the transformation of sensory messages</chapter-title><source>Sensory Communication</source><fpage>217</fpage><lpage>234</lpage></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basso</surname> <given-names>MA</given-names></name><name><surname>Wurtz</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Modulation of neuronal activity in superior colliculus by changes in target probability</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>7519</fpage><lpage>7534</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-18-07519.1998</pub-id><pub-id pub-id-type="pmid">9736670</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname> <given-names>PM</given-names></name><name><surname>Catalao</surname> <given-names>RFG</given-names></name><name><surname>Husain</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The precision of visual working memory is set by allocation of a shared resource</article-title><source>Journal of Vision</source><volume>9</volume><fpage>7</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1167/9.10.7</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname> <given-names>PM</given-names></name><name><surname>Husain</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamic shifts of limited working memory resources in human vision</article-title><source>Science</source><volume>321</volume><fpage>851</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1126/science.1158023</pub-id><pub-id pub-id-type="pmid">18687968</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Noise in neural populations accounts for errors in working memory</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>3632</fpage><lpage>3645</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3204-13.2014</pub-id><pub-id pub-id-type="pmid">24599462</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname> <given-names>JM</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Pitkow</surname> <given-names>X</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Not noisy, just wrong: the role of suboptimal inference in behavioral variability</article-title><source>Neuron</source><volume>74</volume><fpage>30</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.016</pub-id><pub-id pub-id-type="pmid">22500627</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blake</surname> <given-names>R</given-names></name><name><surname>Cepeda</surname> <given-names>NJ</given-names></name><name><surname>Hiris</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Memory for visual motion</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>23</volume><fpage>353</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.23.2.353</pub-id><pub-id pub-id-type="pmid">9103999</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Caplin</surname> <given-names>A</given-names></name><name><surname>Csaba</surname> <given-names>D</given-names></name><name><surname>Leahy</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rational inattention and psychometrics</article-title><ext-link ext-link-type="uri" xlink:href="https://18798-presscdn-pagely.netdna-ssl.com/andrewcaplin/wp-content/uploads/sites/8350/2018/03/Rational-Inattention-and-Psychometrics.pdf">https://18798-presscdn-pagely.netdna-ssl.com/andrewcaplin/wp-content/uploads/sites/8350/2018/03/Rational-Inattention-and-Psychometrics.pdf</ext-link><date-in-citation iso-8601-date="2018-07">July 2018</date-in-citation></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cherniak</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Component placement optimization in the brain</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>2418</fpage><lpage>2427</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-04-02418.1994</pub-id><pub-id pub-id-type="pmid">8158278</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chklovskii</surname> <given-names>DB</given-names></name><name><surname>Schikorski</surname> <given-names>T</given-names></name><name><surname>Stevens</surname> <given-names>CF</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Wiring optimization in cortical circuits</article-title><source>Neuron</source><volume>34</volume><fpage>341</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00679-7</pub-id><pub-id pub-id-type="pmid">11988166</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christie</surname> <given-names>ST</given-names></name><name><surname>Schrater</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cognitive cost as dynamic allocation of energetic resources</article-title><source>Frontiers in Neuroscience</source><volume>9</volume><elocation-id>289</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2015.00289</pub-id><pub-id pub-id-type="pmid">26379482</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname> <given-names>AK</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision-making with multiple alternatives</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>693</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1038/nn.2123</pub-id><pub-id pub-id-type="pmid">18488024</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clune</surname> <given-names>J</given-names></name><name><surname>Mouret</surname> <given-names>J-B</given-names></name><name><surname>Lipson</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The evolutionary origins of modularity</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>280</volume><elocation-id>20122863</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2012.2863</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cover</surname> <given-names>TM</given-names></name><name><surname>Thomas</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Elements of Information Theory</source><publisher-name>Wiley-Blackwell</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>de Silva</surname> <given-names>N</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Optimal allocation of attentional resource to multiple items with unequal relevance</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.06456">https://arxiv.org/abs/1802.06456</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devkar</surname> <given-names>DT</given-names></name><name><surname>Wright</surname> <given-names>AA</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The same type of visual working memory limitations in humans and monkeys</article-title><source>Journal of Vision</source><volume>15</volume><fpage>13</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1167/15.16.13</pub-id><pub-id pub-id-type="pmid">26720277</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donkin</surname> <given-names>C</given-names></name><name><surname>Kary</surname> <given-names>A</given-names></name><name><surname>Tahir</surname> <given-names>F</given-names></name><name><surname>Taylor</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Resources masquerading as slots: flexible allocation of visual working memory</article-title><source>Cognitive Psychology</source><volume>85</volume><fpage>30</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2016.01.002</pub-id><pub-id pub-id-type="pmid">26794368</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname> <given-names>J</given-names></name><name><surname>Wyart</surname> <given-names>V</given-names></name><name><surname>Devauchelle</surname> <given-names>AD</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational precision of mental inference as critical source of human choice suboptimality</article-title><source>Neuron</source><volume>92</volume><fpage>1398</fpage><lpage>1411</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.11.005</pub-id><pub-id pub-id-type="pmid">27916454</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elmore</surname> <given-names>LC</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Magnotti</surname> <given-names>JF</given-names></name><name><surname>Leising</surname> <given-names>KJ</given-names></name><name><surname>Passaro</surname> <given-names>AD</given-names></name><name><surname>Katz</surname> <given-names>JS</given-names></name><name><surname>Wright</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual short-term memory compared in rhesus monkeys and humans</article-title><source>Current Biology</source><volume>21</volume><fpage>975</fpage><lpage>979</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.04.031</pub-id><pub-id pub-id-type="pmid">21596568</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emrich</surname> <given-names>SM</given-names></name><name><surname>Lockhart</surname> <given-names>HA</given-names></name><name><surname>Al-Aidroos</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention mediates the flexible allocation of visual working memory resources</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>43</volume><fpage>1454</fpage><lpage>1465</lpage><pub-id pub-id-type="doi">10.1037/xhp0000398</pub-id><pub-id pub-id-type="pmid">28368161</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faisal</surname> <given-names>AA</given-names></name><name><surname>Selen</surname> <given-names>LP</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Noise in the nervous system</article-title><source>Nature Reviews Neuroscience</source><volume>9</volume><fpage>292</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/nrn2258</pub-id><pub-id pub-id-type="pmid">18319728</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fougnie</surname> <given-names>D</given-names></name><name><surname>Cormiea</surname> <given-names>SM</given-names></name><name><surname>Kanabar</surname> <given-names>A</given-names></name><name><surname>Alvarez</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Strategic trade-offs between quantity and quality in working memory</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>42</volume><fpage>1231</fpage><lpage>1240</lpage><pub-id pub-id-type="doi">10.1037/xhp0000211</pub-id><pub-id pub-id-type="pmid">26950383</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fougnie</surname> <given-names>D</given-names></name><name><surname>Suchow</surname> <given-names>JW</given-names></name><name><surname>Alvarez</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Variability in the quality of visual working memory</article-title><source>Nature Communications</source><volume>3</volume><elocation-id>1229</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms2237</pub-id><pub-id pub-id-type="pmid">23187629</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguli</surname> <given-names>D</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Implicit encoding of prior probabilities in optimal neural populations</article-title><source>Advances in Neural Information Processing Systems</source><volume>2010</volume><fpage>658</fpage><lpage>666</lpage><pub-id pub-id-type="pmid">25356064</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contributions of ideal observer theory to vision research</article-title><source>Vision Research</source><volume>51</volume><fpage>771</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.09.027</pub-id><pub-id pub-id-type="pmid">20920517</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Horvitz</surname> <given-names>EJ</given-names></name><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Computational rationality: a converging paradigm for intelligence in brains, minds, and machines</article-title><source>Science</source><volume>349</volume><fpage>273</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1126/science.aac6076</pub-id><pub-id pub-id-type="pmid">26185246</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girshick</surname> <given-names>AR</given-names></name><name><surname>Landy</surname> <given-names>MS</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1038/nn.2831</pub-id><pub-id pub-id-type="pmid">21642976</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname> <given-names>DM</given-names></name><name><surname>Swets</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Signal detection theory and psychophysics</article-title><source>Society</source><volume>1</volume><elocation-id>521</elocation-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname> <given-names>TL</given-names></name><name><surname>Lieder</surname> <given-names>F</given-names></name><name><surname>Goodman</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rational use of cognitive resources: levels of analysis between the computational and the algorithmic</article-title><source>Topics in Cognitive Science</source><volume>7</volume><fpage>217</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1111/tops.12142</pub-id><pub-id pub-id-type="pmid">25898807</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="software"><person-group person-group-type="author"><collab>JASP Team</collab></person-group><year iso-8601-date="2017">2017</year><data-title>JASP</data-title><version designator="0.8.2">0.8.2</version></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshvari</surname> <given-names>S</given-names></name><name><surname>van den Berg</surname> <given-names>R</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Probabilistic computation in human perception under variability in encoding precision</article-title><source>PLoS One</source><volume>7</volume><elocation-id>e40216</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0040216</pub-id><pub-id pub-id-type="pmid">22768258</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshvari</surname> <given-names>S</given-names></name><name><surname>van den Berg</surname> <given-names>R</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>No evidence for an item limit in change detection</article-title><source>PLoS Computational Biology</source><volume>9</volume><elocation-id>e1002927</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002927</pub-id><pub-id pub-id-type="pmid">23468613</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klyszejko</surname> <given-names>Z</given-names></name><name><surname>Rahmati</surname> <given-names>M</given-names></name><name><surname>Curtis</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attentional priority determines working memory precision</article-title><source>Vision Research</source><volume>105</volume><fpage>70</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2014.09.002</pub-id><pub-id pub-id-type="pmid">25240420</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Decision theory: what &quot;should&quot; the nervous system do?</article-title><source>Science</source><volume>318</volume><fpage>606</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1126/science.1142998</pub-id><pub-id pub-id-type="pmid">17962554</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Energy as a constraint on the coding and processing of sensory information</article-title><source>Current Opinion in Neurobiology</source><volume>11</volume><fpage>475</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(00)00237-3</pub-id><pub-id pub-id-type="pmid">11502395</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lennie</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The cost of cortical computation</article-title><source>Current Biology</source><volume>13</volume><fpage>493</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1016/S0960-9822(03)00135-0</pub-id><pub-id pub-id-type="pmid">12646132</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindsay</surname> <given-names>PH</given-names></name><name><surname>Taylor</surname> <given-names>MM</given-names></name><name><surname>Forbes</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Attention and multidimensional discrimination1</article-title><source>Perception &amp; Psychophysics</source><volume>4</volume><fpage>113</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.3758/BF03209520</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>YS</given-names></name><name><surname>Stevens</surname> <given-names>CF</given-names></name><name><surname>Sharpee</surname> <given-names>TO</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Predictable irregularities in retinal receptive fields</article-title><source>PNAS</source><volume>106</volume><fpage>16499</fpage><lpage>16504</lpage><pub-id pub-id-type="doi">10.1073/pnas.0908926106</pub-id><pub-id pub-id-type="pmid">19805327</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname> <given-names>SJ</given-names></name><name><surname>Vogel</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The capacity of visual working memory for features and conjunctions</article-title><source>Nature</source><volume>390</volume><fpage>279</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1038/36846</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname> <given-names>SJ</given-names></name><name><surname>Vogel</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visual working memory capacity: from psychophysics and neurobiology to individual differences</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>391</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.006</pub-id><pub-id pub-id-type="pmid">23850263</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ly</surname> <given-names>A</given-names></name><name><surname>Marsman</surname> <given-names>M</given-names></name><name><surname>Verhagen</surname> <given-names>J</given-names></name><name><surname>Grasman</surname> <given-names>RPPP</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A tutorial on Fisher information</article-title><source>Journal of Mathematical Psychology</source><volume>80</volume><fpage>40</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2017.05.006</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Beck</surname> <given-names>JM</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian inference with probabilistic population codes</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1432</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1038/nn1790</pub-id><pub-id pub-id-type="pmid">17057707</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Huang</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>No capacity limit in attentional tracking: evidence for probabilistic inference under a resource constraint</article-title><source>Journal of Vision</source><volume>9</volume><fpage>3.1</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1167/9.11.3</pub-id><pub-id pub-id-type="pmid">20053066</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Husain</surname> <given-names>M</given-names></name><name><surname>Bays</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Changing concepts of working memory</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>347</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1038/nn.3655</pub-id><pub-id pub-id-type="pmid">24569831</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazyar</surname> <given-names>H</given-names></name><name><surname>van den Berg</surname> <given-names>R</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Does precision decrease with set size?</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/12.6.10</pub-id><pub-id pub-id-type="pmid">22685337</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname> <given-names>MR</given-names></name><name><surname>Helmers</surname> <given-names>JC</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Chunking as a rational strategy for lossy data compression in visual working memory</article-title><source>Psychological Review</source><volume>125</volume><fpage>486</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.1037/rev0000101</pub-id><pub-id pub-id-type="pmid">29952621</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oberauer</surname> <given-names>K</given-names></name><name><surname>Farrell</surname> <given-names>S</given-names></name><name><surname>Jarrold</surname> <given-names>C</given-names></name><name><surname>Lewandowsky</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What limits working memory capacity?</article-title><source>Psychological Bulletin</source><volume>142</volume><fpage>758</fpage><lpage>799</lpage><pub-id pub-id-type="doi">10.1037/bul0000046</pub-id><pub-id pub-id-type="pmid">26950009</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oberauer</surname> <given-names>K</given-names></name><name><surname>Lin</surname> <given-names>HY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An interference model of visual working memory</article-title><source>Psychological Review</source><volume>124</volume><fpage>21</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1037/rev0000044</pub-id><pub-id pub-id-type="pmid">27869455</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orhan</surname> <given-names>AE</given-names></name><name><surname>Sims</surname> <given-names>CR</given-names></name><name><surname>Jacobs</surname> <given-names>RA</given-names></name><name><surname>Knill</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The adaptive nature of visual working memory</article-title><source>Current Directions in Psychological Science</source><volume>23</volume><fpage>164</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1177/0963721414529144</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname> <given-names>J</given-names></name><name><surname>Ames</surname> <given-names>CT</given-names></name><name><surname>Lindsey</surname> <given-names>DT</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Measuring the effect of attention on simple visual search</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>19</volume><fpage>108</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.19.1.108</pub-id><pub-id pub-id-type="pmid">8440980</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Set-size effects in visual search: the effect of attention is independent of the stimulus for simple tasks</article-title><source>Vision Research</source><volume>34</volume><fpage>1703</fpage><lpage>1721</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)90128-7</pub-id><pub-id pub-id-type="pmid">7941377</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paradiso</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>A theory for the use of visual orientation information which exploits the columnar structure of striate cortex</article-title><source>Biological Cybernetics</source><volume>58</volume><fpage>35</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1007/BF00363954</pub-id><pub-id pub-id-type="pmid">3345319</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pestilli</surname> <given-names>F</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Attention enhances contrast sensitivity at cued and impairs it at Uncued locations</article-title><source>Vision Research</source><volume>45</volume><fpage>1867</fpage><lpage>1875</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2005.01.019</pub-id><pub-id pub-id-type="pmid">15797776</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prinzmetal</surname> <given-names>W</given-names></name><name><surname>Amiri</surname> <given-names>H</given-names></name><name><surname>Allen</surname> <given-names>K</given-names></name><name><surname>Edwards</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Phenomenology of attention: I. color, location, orientation, and spatial frequency</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>24</volume><fpage>261</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.24.1.261</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname> <given-names>RL</given-names></name><name><surname>Tredway</surname> <given-names>CH</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Introspective judgments predict the precision and likelihood of successful maintenance of visual working memory</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>21</elocation-id><pub-id pub-id-type="doi">10.1167/12.13.21</pub-id><pub-id pub-id-type="pmid">23262153</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname> <given-names>JH</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The normalization model of attention</article-title><source>Neuron</source><volume>61</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id><pub-id pub-id-type="pmid">19186161</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouder</surname> <given-names>JN</given-names></name><name><surname>Morey</surname> <given-names>RD</given-names></name><name><surname>Cowan</surname> <given-names>N</given-names></name><name><surname>Zwilling</surname> <given-names>CE</given-names></name><name><surname>Morey</surname> <given-names>CC</given-names></name><name><surname>Pratte</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>An assessment of fixed-capacity models of visual working memory</article-title><source>PNAS</source><volume>105</volume><fpage>5975</fpage><lpage>5979</lpage><pub-id pub-id-type="doi">10.1073/pnas.0711295105</pub-id><pub-id pub-id-type="pmid">18420818</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname> <given-names>HS</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Simple models for reading neuronal population codes</article-title><source>PNAS</source><volume>90</volume><fpage>10749</fpage><lpage>10753</lpage><pub-id pub-id-type="doi">10.1073/pnas.90.22.10749</pub-id><pub-id pub-id-type="pmid">8248166</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sewell</surname> <given-names>DK</given-names></name><name><surname>Lilburn</surname> <given-names>SD</given-names></name><name><surname>Smith</surname> <given-names>PL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An information capacity limitation of visual short-term memory</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>40</volume><fpage>2214</fpage><lpage>2242</lpage><pub-id pub-id-type="doi">10.1037/a0037744</pub-id><pub-id pub-id-type="pmid">25222469</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shaw</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="1980">1980</year><chapter-title>Identifying attentional and decision-making components in information processing</chapter-title><person-group person-group-type="editor"><name><surname>Nickerson</surname> <given-names>R. S</given-names></name></person-group><source>Attention and Performance VIII</source><publisher-loc>Hillsdale</publisher-loc><publisher-name>Erlbaum</publisher-name><fpage>277</fpage><lpage>296</lpage></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname> <given-names>S</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A detailed comparison of optimality and simplicity in perceptual decision making</article-title><source>Psychological Review</source><volume>123</volume><fpage>452</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1037/rev0000028</pub-id><pub-id pub-id-type="pmid">27177259</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenhav</surname> <given-names>A</given-names></name><name><surname>Musslick</surname> <given-names>S</given-names></name><name><surname>Lieder</surname> <given-names>F</given-names></name><name><surname>Kool</surname> <given-names>W</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Toward a rational and mechanistic account of mental effort</article-title><source>Annual Review of Neuroscience</source><volume>40</volume><fpage>99</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031526</pub-id><pub-id pub-id-type="pmid">28375769</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname> <given-names>H</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Crowdsourced single-trial probes of visual working memory for irrelevant features</article-title><source>Journal of Vision</source><volume>16</volume><fpage>10</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1167/16.5.10</pub-id><pub-id pub-id-type="pmid">26974056</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmering</surname> <given-names>VR</given-names></name><name><surname>Perone</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Working memory capacity as a dynamic process</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>567</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00567</pub-id><pub-id pub-id-type="pmid">23335902</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmering</surname> <given-names>VR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The development of visual working memory capacity during early childhood</article-title><source>Journal of Experimental Child Psychology</source><volume>111</volume><fpage>695</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1016/j.jecp.2011.10.007</pub-id><pub-id pub-id-type="pmid">22099167</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sims</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Implications of rational inattention</article-title><source>Journal of Monetary Economics</source><volume>50</volume><fpage>665</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1016/S0304-3932(03)00029-1</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sims</surname> <given-names>CR</given-names></name><name><surname>Jacobs</surname> <given-names>RA</given-names></name><name><surname>Knill</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An ideal observer analysis of visual working memory</article-title><source>Psychological Review</source><volume>119</volume><fpage>807</fpage><lpage>830</lpage><pub-id pub-id-type="doi">10.1037/a0029856</pub-id><pub-id pub-id-type="pmid">22946744</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sims</surname> <given-names>CR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The cost of misremembering: inferring the loss function in visual working memory</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.1167/15.3.2</pub-id><pub-id pub-id-type="pmid">25740875</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sims</surname> <given-names>CR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rate-distortion theory and human perception</article-title><source>Cognition</source><volume>152</volume><fpage>181</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2016.03.020</pub-id><pub-id pub-id-type="pmid">27107330</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>PL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The Poisson shot noise model of visual short-term memory and choice response time: normalized coding by neural population size</article-title><source>Journal of Mathematical Psychology</source><volume>66</volume><fpage>41</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2015.03.007</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sterling</surname> <given-names>P</given-names></name><name><surname>Laughlin</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Principles of Neural Design</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todd</surname> <given-names>JJ</given-names></name><name><surname>Marois</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Capacity limit of visual short-term memory in human posterior parietal cortex</article-title><source>Nature</source><volume>428</volume><fpage>751</fpage><lpage>754</lpage><pub-id pub-id-type="doi">10.1038/nature02466</pub-id><pub-id pub-id-type="pmid">15085133</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Berg</surname> <given-names>R</given-names></name><name><surname>Awh</surname> <given-names>E</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Factorial comparison of working memory models</article-title><source>Psychological Review</source><volume>121</volume><fpage>124</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1037/a0035234</pub-id><pub-id pub-id-type="pmid">24490791</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Berg</surname> <given-names>R</given-names></name><name><surname>Shin</surname> <given-names>H</given-names></name><name><surname>Chou</surname> <given-names>WC</given-names></name><name><surname>George</surname> <given-names>R</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Variability in encoding precision accounts for visual short-term memory limitations</article-title><source>Proceedings of the National Academy of Sciences</source><volume>109</volume><fpage>8780</fpage><lpage>8785</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117465109</pub-id><pub-id pub-id-type="pmid">22582168</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Berg</surname> <given-names>R</given-names></name><name><surname>Yoo</surname> <given-names>AH</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fechner's law in metacognition: A quantitative model of visual working memory confidence</article-title><source>Psychological Review</source><volume>124</volume><fpage>197</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1037/rev0000060</pub-id><pub-id pub-id-type="pmid">28221087</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vincent</surname> <given-names>BT</given-names></name><name><surname>Baddeley</surname> <given-names>RJ</given-names></name><name><surname>Troscianko</surname> <given-names>T</given-names></name><name><surname>Gilchrist</surname> <given-names>ID</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Is the early visual system optimised to be energy efficient?</article-title><source>Network: Computation in Neural Systems</source><volume>16</volume><fpage>175</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1080/09548980500290047</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vul</surname> <given-names>E</given-names></name><name><surname>Pashler</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Measuring the crowd within</article-title><source>Psychological Science</source><volume>19</volume><fpage>645</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2008.02136.x</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vul</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</article-title><source>Advances in Neural Information Processing Systems</source><volume>22</volume><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname> <given-names>XX</given-names></name><name><surname>Stocker</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A bayesian observer model constrained by efficient coding can explain 'anti-Bayesian' percepts</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1509</fpage><lpage>1517</lpage><pub-id pub-id-type="doi">10.1038/nn.4105</pub-id><pub-id pub-id-type="pmid">26343249</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname> <given-names>Z</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name><name><surname>Wang</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>From distributed resources to limited slots in multiple-item working memory: a spiking network model with normalization</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>11228</fpage><lpage>11240</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0735-12.2012</pub-id><pub-id pub-id-type="pmid">22895707</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilken</surname> <given-names>P</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A detection theory account of change detection</article-title><source>Journal of Vision</source><volume>4</volume><fpage>11</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1167/4.12.11</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname> <given-names>DJ</given-names></name><name><surname>Woodman</surname> <given-names>GF</given-names></name><name><surname>Widders</surname> <given-names>D</given-names></name><name><surname>Marois</surname> <given-names>R</given-names></name><name><surname>Chun</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neural fate of ignored stimuli: dissociable effects of perceptual and working memory load</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>992</fpage><lpage>996</lpage><pub-id pub-id-type="doi">10.1038/nn1294</pub-id><pub-id pub-id-type="pmid">15286791</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>W</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Discrete fixed-resolution representations in visual working memory</article-title><source>Nature</source><volume>453</volume><fpage>233</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1038/nature06860</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>W</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The number and quality of representations in working memory</article-title><source>Psychological Science</source><volume>22</volume><fpage>1434</fpage><lpage>1441</lpage><pub-id pub-id-type="doi">10.1177/0956797611417006</pub-id><pub-id pub-id-type="pmid">21987693</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s7" sec-type="appendix"><title>Relation between Fisher information <italic>J</italic> and concentration parameter <italic>κ</italic></title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.34963.015</object-id><p>As we are only considering stimuli with circular domains, we assume that memory encoding errors follow a Von Mises distribution with a concentration parameter <italic>κ</italic>, <disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ε</mml:mi><mml:mo>|</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>κ</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ε</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>I</italic><sub>0</sub> is the modified Bessel function of the first kind of order 0. We measure encoding precision as Fisher information, <italic>J</italic>, which measures the performance of the best possible unbiased decoder. Substituting <xref ref-type="disp-formula" rid="equ15">Equation 15</xref> into the definition of Fisher information, we find that <italic>J</italic> and <italic>κ</italic> are one-to-one related through <disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mi>κ</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Encoding precision <italic>J</italic> is a monotonically increasing function of <italic>κ</italic> and therefore invertible. However, the inverse is not analytic, so we use numerical inversion to compute the mapping from <italic>J</italic> to <italic>κ</italic> when fitting models.</p></boxed-text></sec><sec id="s8" sec-type="appendix"><title>Mathematical proofs of some properties of the resource-rational model for local tasks</title><boxed-text><p>In this section, we prove three properties of the general model that we presented for &quot;local&quot; tasks, that is, tasks in which responses depend on a single item. This model is characterized by <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>,<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow></mml:mstyle><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:munder><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf88"><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf89"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and we left out the dependence on the parameter τ for notational convenience. We will also use the derivative of the local expected total cost,<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf91"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the derivative of the expected behavioral cost.</p><p>We will now prove that the following three claims hold under rather general assumptions about the shape of the expected behavioral cost function in this model:</p><p><italic>Claim 1</italic>. When neural coding is costly (<italic>λ</italic> &gt;0), it is optimal to encode items with a finite amount of resource;</p><p><italic>Claim 2</italic>. It is sometimes optimal not to encode a task-relevant item;</p><p><italic>Claim 3</italic>. When each item is equally likely to be probed, <italic>p<sub>i</sub></italic> = 1/<italic>N</italic>, the optimal amount of resource per item decreases with set size.</p><sec id="s8-1"><title>Assumptions about the expected behavioral cost</title><p>We construct our proofs under two intuitive and general assumptions about the expected behavioral cost function <inline-formula><mml:math id="inf92"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:</p><p><italic>Assumption 1.</italic> Expected behavioral cost is a monotonically decreasing function of resource: whenever more resource is invested, the expected behavioral cost is lower. This means that <inline-formula><mml:math id="inf93"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf94"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>.</p><p><italic>Assumption 2.</italic> A law of diminishing returns: when adding a bit of extra resource, the resulting decrease in <inline-formula><mml:math id="inf95"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is lower in magnitude when <inline-formula><mml:math id="inf96"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> is higher. This means that <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is monotonically increasing, that is, <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf99"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>. As a consequence, <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> takes its lowest value at <inline-formula><mml:math id="inf101"><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and its largest as <inline-formula><mml:math id="inf102"><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Both assumptions are satisfied by the behavioral cost function that we used for fitting human data, namely <italic>c</italic><sub>behavioral</sub>(<italic>ε;β</italic>)=|<italic>ε</italic>|<italic><sup>β</sup></italic>. Examples of the expected behavioral cost function under this choice and its first and second derivative are presented in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>.</p><fig id="app1fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.016</object-id><label>Appendix 1—figure 1.</label><caption><title>Examples of the expected behavioral cost function and its first and second derivative under a behavioral cost function <italic>c</italic><sub>behavioral</sub>(<italic>ε</italic>)=|<italic>ε</italic>|<italic><sup>β</sup></italic>.</title><p>Different colors represent different choices of parameters <italic>β</italic> and <italic>τ</italic> (randomly drawn).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34963-app1-fig1-v2"/></fig></sec><sec id="s8-2"><title>Three scenarios</title><p>We now return to the problem of calculating <inline-formula><mml:math id="inf103"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ17">Equation 17</xref>. We are interested in the value <inline-formula><mml:math id="inf104"><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>∞</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> that minimizes the expected total cost, <inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We separately consider the following three scenarios: the minimum lies on the left boundary (0), on the right boundary (∞), or in between.</p></sec><sec id="s8-3"><title><italic>Scenario 1: <inline-formula><mml:math id="inf106"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is monotonically decreasing across the domain of <inline-formula><mml:math id="inf107"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>, so</italic> <inline-formula><mml:math id="inf108"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>.</title><p>When does this happen? The monotonic decrease means that <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf110"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>, or equivalently, <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf112"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>. As we assume <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to be monotonically increasing (Assumption 2), its largest value is attained at <inline-formula><mml:math id="inf114"><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>. Therefore, <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is equivalent to <inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>∞</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula>, or (using Assumption 1) <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mi>λ</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> This means that it is optimal to invest infinite resource when <italic>p<sub>i</sub></italic> exceeds a critical value <italic>p</italic><sub>∞</sub>:<disp-formula id="equ19"><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The condition <italic>p<sub>i</sub></italic> <italic>≥p∞</italic> is satisfied when <italic>λ</italic> = 0. This makes sense: when neural cost plays no role, there is no reason not to invest more. Other than that, the condition will rarely if ever be satisfied, as every expected behavioral cost function that we can think of has the property <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>: as the amount of invested resource approaches infinity, there is no behavioral benefit in investing more resource (note that <italic>p</italic><sub>∞</sub> has a domain [0,∞), not [0,1]). Therefore, unless neural cost plays no role, we do not expect it to be optimal to invest an infinite amount of resource in an item.</p><p>In tasks where <italic>p<sub>i</sub></italic> is one-to-one related to set size, the above result can be reformulated in terms of set size. In particular, when probing probabilities are equal, <inline-formula><mml:math id="inf119"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>, the above result implies that there exists a set size <italic>N</italic><sub>∞</sub> (in general not an integer) below which it is optimal to invest infinite resource in each item:<disp-formula id="equ20"><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>N</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mi>λ</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s8-4"><title><italic>Scenario 2: <inline-formula><mml:math id="inf120"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is monotonically increasing across the domain of <inline-formula><mml:math id="inf121"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>, so</italic> <inline-formula><mml:math id="inf122"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</title><p>The monotonic increase means that <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf124"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>, or equivalently, <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf126"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>. As we assume <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to be monotonically increasing (Assumption 2), its smallest value is attained at <inline-formula><mml:math id="inf128"><mml:mrow><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>. Therefore, <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is equivalent to <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, or (using Assumption 1) <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This means that it is optimal to invest no resource when <italic>p<sub>i</sub></italic> is smaller than or equal to a critical value <italic>p</italic><sub>0</sub>:<disp-formula id="equ21"><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>A similar condition was derived in our earlier work (<xref ref-type="bibr" rid="bib27">de Silva and Ma, 2018</xref>) for the case of a fixed total amount of resource (hard constraint).</p><p>The condition <italic>p<sub>i</sub></italic>≤ <italic>p</italic><sub>0</sub> is satisfied when <italic>p<sub>i</sub></italic> = 0. This makes sense: when an item never gets probed, one should not invest any resource. More generally, when probing probability is sufficiently low, the behavioral cost function is sufficiently shallow at 0, and neural cost is sufficiently important, it is not worth investing any resource on encoding. The expression for <italic>p</italic><sub>0</sub> also makes clear that the optimal amount of resource is never 0 when the slope of the behavioral cost function at 0 approaches −∞.</p><p>In tasks where <italic>p<sub>i</sub></italic> is one-to-one related to set size, the above result can be reformulated in terms of set size. In particular, when probing probabilities are equal, <inline-formula><mml:math id="inf132"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>, the above result implies that there exists a set size <italic>N</italic><sub>0</sub> (in general not an integer) beyond which it is optimal to not invest any resource in any item:<disp-formula id="equ22"><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>N</mml:mi><mml:mo>≥</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mi>λ</mml:mi></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Intuitively, this means that when set size is too large, the chances of success are too low and one should not even try.</p></sec><sec id="s8-5"><title><italic>Scenario 3: <inline-formula><mml:math id="inf133"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has a stationary point, so</italic> <inline-formula><mml:math id="inf134"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> <italic>is finite and nonzero.</italic></title><p>We will now consider the remaining scenario, which is the complement of Scenarios 1 and 2; in particular, we can take <italic>λ</italic> &gt;0 and <italic>p<sub>i</sub></italic> &gt;0. The stationary point of <inline-formula><mml:math id="inf135"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> will always be a minimum, as the second derivative <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is equal to <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is always positive (Assumption 2). At the minimum, we have <inline-formula><mml:math id="inf138"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, from which it follows that <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>a</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> at the minimum. As the left-hand side is monotonically increasing as a function of <inline-formula><mml:math id="inf140"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> (Assumption 2), the minimum is either a single point or a single interval, but there cannot be multiple disjoint minima. Graphically, this equation describes the intersection between <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is a monotonically increasing function, and a flat line at a value <inline-formula><mml:math id="inf142"><mml:mrow><mml:mo>−</mml:mo><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mi>λ</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). The value of at which this intersection occurs necessarily increases with <italic>p<sub>i</sub></italic>.</p><fig id="app1fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.34963.017</object-id><label>Appendix 1–figure 2.</label><caption><title>Graphical illustration of the solution to the cost-minimization problem that determines the value of <inline-formula><mml:math id="inf143"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</title><p>The cost-minimizing value of solution of <inline-formula><mml:math id="inf144"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> lies at the intersection between the derivative of the expected behavioral cost function (black curve) and a flat line at a value –<italic>λ</italic>/<italic>p<sub>i</sub></italic> (colored lines). This value (indicated with arrows) necessarily increases with <italic>p<sub>i</sub></italic>. The parameter values used in this simulation were the same as those used to generate <xref ref-type="fig" rid="fig2">Figure 2D and E</xref> (<italic>λ</italic> = 0.01, <italic>β</italic> = 2, τ↓0).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34963-app1-fig2-v2"/></fig></sec><sec id="s8-6"><title>Three regimes for probing probability</title><p>So far, we have assumed a given probing probability <italic>p<sub>i</sub></italic>. Now suppose that for a given <inline-formula><mml:math id="inf145"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and a given <italic>λ</italic>, we increase <italic>p<sub>i</sub></italic> from 0 to 1:</p><list list-type="bullet"><list-item><p>The first regime is <italic>p<sub>i</sub></italic>≤ <italic>p</italic><sub>0</sub>. There, Scenario 2 applies and <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>: the item does not get encoded at all.</p></list-item><list-item><p>The second regime is <italic>p</italic><sub>0</sub> &lt;<italic>p<sub>i</sub></italic> &lt; p<sub>∞</sub>; there, Scenario 3 applies and <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> monotonically increases with <italic>p<sub>i</sub></italic>.</p></list-item><list-item><p>The third regime is <italic>p<sub>i</sub></italic> <italic>≥p</italic><sub>∞</sub>. There, Scenario 1 applies and <inline-formula><mml:math id="inf148"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>: the item gets encoded with infinite resource.</p></list-item></list><p>Even though not all regimes might exist for every parameter combination, the model generally predicts that there is a regime in which <inline-formula><mml:math id="inf149"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> increases monotonically with <italic>p<sub>i</sub></italic> (<xref ref-type="fig" rid="fig1">Figure 1D</xref>).</p></sec><sec id="s8-7"><title>Three regimes for set size</title><p>We can similarly examine the experimentally important special case of equal probing probabilities, <inline-formula><mml:math id="inf150"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>:</p><p>The first regime is <italic>N</italic> ≤ <italic>N</italic><sub>∞</sub>. There, Scenario 1 applies and <inline-formula><mml:math id="inf151"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>: all items are encoded with infinite resource.</p><p>The second regime is <italic>N</italic><sub>∞</sub>&lt;<italic>N</italic> &lt;<italic>N</italic><sub>0</sub>. There, Scenario 3 applies and <inline-formula><mml:math id="inf152"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> monotonically decreases with <italic>N</italic>.</p><p>The third regime is <italic>N</italic>≥ <italic>N</italic><sub>0</sub>. There, Scenario 2 applies and <inline-formula><mml:math id="inf153"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>: no items are encoded at all.</p><p>Even though not all regimes might exist for every parameter combination, the model generally predicts that there is a regime in which <inline-formula><mml:math id="inf154"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> decreases monotonically with <italic>N</italic> (<xref ref-type="fig" rid="fig1">Figure 1E</xref>).</p></sec><sec id="s8-8"><title>Conclusion</title><p>In conclusion, given <xref ref-type="disp-formula" rid="equ17">Equation (17)</xref> and two additional assumptions, we have proven the following:</p><list list-type="bullet"><list-item><p>Investing infinite resource in an item is only optimal when <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. In practice, this might only happen when neural cost is unimportant (<italic>λ</italic> = 0). This proves Claim 1.</p></list-item><list-item><p>Investing no resource in an item is optimal when <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This can happen even when the probing probability <italic>p<sub>i</sub></italic> is nonzero. This proves Claim 2.</p></list-item><list-item><p><inline-formula><mml:math id="inf157"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a monotonically increasing function of <italic>p<sub>i</sub></italic>. In particular, if <inline-formula><mml:math id="inf158"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>, then <inline-formula><mml:math id="inf159"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a monotonically decreasing function of <italic>N</italic>. This proves Claim 3.</p></list-item></list><p>All three results hold more generally than we have shown here: we can replace the neural cost term <inline-formula><mml:math id="inf160"><mml:mrow><mml:mi>λ</mml:mi><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ17">Equation (17)</xref> by any function <inline-formula><mml:math id="inf161"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> whose derivative is positive and monotonically increasing. The proofs proceed along the same lines (see below).</p></sec><sec id="s8-9"><title>Special case: fixed-precision model</title><p>For the fixed-precision model (variable-precision model with <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo stretchy="false">↓</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), <xref ref-type="disp-formula" rid="equ12">Equation (12)</xref> in the main text takes the form<disp-formula id="equ23"><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>J</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>V</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo>;</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>ε</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We wish to evaluate <inline-formula><mml:math id="inf163"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>≡</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo> <mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. First, we evaluate the derivative of <inline-formula><mml:math id="inf164"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>J</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> using the chain rule: <disp-formula id="equ24"><label>(19)</label><mml:math id="m24"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>J</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>J</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Using <xref ref-type="disp-formula" rid="equ15">Equation (15)</xref>, the first factor is<disp-formula id="equ25"><label>(20)</label><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>κ</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>ε</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>κ</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>ε</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>κ</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mi>κ</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>ε</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>κ</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mi>κ</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>ε</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where in the last line we used <inline-formula><mml:math id="inf165"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see Eq. 9.6.27 in <xref ref-type="bibr" rid="bib2">Abramowitz and Stegun [1972</xref>]).</p><p>We next evaluate the second factor in <xref ref-type="disp-formula" rid="equ24">Equation (19)</xref> using <xref ref-type="disp-formula" rid="equ16">Equation (16)</xref>:<disp-formula id="equ26"><label>(21)</label><mml:math id="m26"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>J</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>κ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>ą</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mi>κ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where in the third equality, we used <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>κ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (see Eq. 9.6.28 in [Abramowitz &amp; Stegun, 1972]). We now combine <xref ref-type="disp-formula" rid="equ25">Equation (20)</xref> and <xref ref-type="disp-formula" rid="equ26">Equation (21)</xref> into <xref ref-type="disp-formula" rid="equ24">Equation (19)</xref> and the result in turn in the expression for <italic>p</italic><sub>0</sub>. We also realize that the limit <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">↓</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is, using <xref ref-type="disp-formula" rid="equ16">Equation (16)</xref>, equivalent to the limit 0. Putting everything together, we find</p><p><inline-formula><mml:math id="inf168"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mi>κ</mml:mi><mml:mo>↓</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mrow><mml:mo>|</mml:mo> <mml:mrow><mml:mfrac><mml:mrow><mml:mi>κ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:mrow><mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mi>π</mml:mi></mml:munderover></mml:mrow></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ε</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>κ</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>ε</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mi>e</mml:mi><mml:mi>κ</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>ε</mml:mi></mml:mrow></mml:mfrac></mml:mrow> <mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>We conclude that in our theory for delayed-estimation, assuming the expected behavioral cost function from the fixed-precision model, it is only optimal to invest no resource at all into an item when that item has zero probability of being probed.</p></sec><sec id="s8-10"><title>Generalization to other neural cost functions</title><p>So far, we have assumed that the expected neural cost is linear in resource, <xref ref-type="disp-formula" rid="equ8">Equation (8)</xref>. Relaxing this assumption, <xref ref-type="disp-formula" rid="equ17">Equation (17)</xref> for local tasks becomes <disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>optimal</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow></mml:mstyle><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:munder><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The derivative of the local expected total cost becomes <disp-formula id="equ28"><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The three claims above still hold if we modify the two assumptions to</p><p><italic>Assumption 1’.</italic> <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf170"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>.</p><p><italic>Assumption 2’.</italic> <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> is monotonically increasing for all <inline-formula><mml:math id="inf172"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>.</p><p>The proofs are completely analogous, with <inline-formula><mml:math id="inf173"><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> replaced by <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>behavioral</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>neural</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec></boxed-text></sec><sec id="s9" sec-type="appendix"><title>Optimal decision rule for the change detection task</title><boxed-text><p>In our simulation of the change detection task, we assume that observers use a Bayesian decision rule. This rule is to report &quot;change&quot; whenever the posterior ratio of change presence over change absence exceeds 1,<disp-formula id="equ29"><mml:math id="m29"><mml:mrow><mml:mstyle scriptlevel="+1"><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>changepresent</mml:mtext><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>changeabsent</mml:mtext><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <bold>x</bold> and <bold>y</bold> denote the vectors of noisy measurements of the items in the first and second displays, respectively. Under the Von Mises noise assumption, and assuming a flat prior on change presence, this decision rule evaluates to (<xref ref-type="bibr" rid="bib44">Keshvari et al., 2013</xref>)<disp-formula id="equ30"><mml:math id="m30"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>y</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>y</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>κ</italic><sub>x,<italic>i</italic></sub> and <italic>κ</italic><sub>y,<italic>i</italic></sub> denote the concentration parameters of the Von Mises distributions associated with the observations of the items at the <italic>i</italic><sup>th</sup> location in the first and second displays, respectively. The predicted probability of a correct response for a given resource vector, <inline-formula><mml:math id="inf175"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>error</mml:mtext><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>J</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is not analytic, but can easily be computed using Monte Carlo simulations.</p></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.34963.022</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie</given-names></name><role>Reviewing Editor</role><aff><institution>University of Chicago</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Ecological rationality in human working memory and attention&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The following individual involved in review of your submission has agreed to reveal her identity: Jacqueline Gottlieb (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>In this manuscript, van den Berg and Ma propose a normative theory of a phenomenon called set-size effect – the fact that attention and working memory performance degrades as a function of the number of items. The set-size effect is fundamental in attention and working memory research, and many previous models have successfully described it by using ad hoc assumptions about the relation between precision and set-size. The authors' main innovation is to explain this relation in a normative framework. Specifically, they propose that the degradation in precision as a function of set size reflects a tradeoff between the benefits of precise encoding and the neural costs that this encoding demands. The authors develop the model, use it to fit several existing data sets, and offer an extensive discussion of the limitations of their model and its relation to the previous theoretical and empirical literatures.</p><p>The paper incorporates a genuinely new idea and is clearly written and quite thorough in its analysis and discussion. The Results section details a substantive amount of work, in which the model was fit to data from several experiments where working memory precision was measured in the context of local delayed-estimation, global estimation, change detection, change localization and present/absent visual search, and was quantitatively evaluated against other (often equally good) prediction schemes.</p><p>Some major revisions are requested to clarify situations in which not all items are stored, to answer questions about the model parameter τ, and to expand the discussion about how the flexibility of resource allocation in the model could be mechanistically realized in the brain.</p><p>Essential revisions:</p><p>1) The number of encoded items: The paper mentions a possible hard constraint on the number of items encoded. Depending on set size and cost functions, when (if ever) is it &quot;optimal&quot; not to encode some of the items? Some new modeling results should be shown here to shore this up and a longer discussion of this point should be added to the Results and Discussion.</p><p>Furthermore, please expand the discussion of the costs of encoding more items versus fewer. Intuitively, it is obvious that it is more costly to encode 8 items than 2, but there are numerous reasons why this could be the case. Are there any experimental data in support of this assumption? In particular, are there any experimental data showing that encoding more items results in higher firing rates (at the population level)?</p><p>Michael Frank and his group have recently made a very principled attempt to characterize the nature of the cost in these tasks, in terms of how participants might group or chunk the items. It may be beyond the scope of the current paper to attempt to outline a theory that explains these results, but perhaps readers would enjoy some more elaborate discussion of this issue.</p><p>2) τ: Please provide additional model results that show how the fits look when τ = 0. In particular, please show the goodness of fit of the rational model (with τ = 0) as compared to the fit of a model with a &quot;hard&quot; constraint on resources? In general, it was confusing that the theory is initially described in absence of τ, while τ is used for actual fits to data. It would have been easier to understand if the theory had been evaluated in presence of τ, and its effect studied within that theoretical framework.</p><p>3) The speed of policy update: An assumption of the model that may be problematic is that people must almost instantaneously optimize their encoding precision when set-sizes change unpredictably from trial to trial. The theory predicts that, when a trial contains 2 stimuli and the next trial contains 4 stimuli, the participant instantaneously lowers the encoding precision to the new (near-) optimal level. This process sounds pretty demanding itself, especially considering that humans may also play an active role in determining their intrinsic motivation or deciding which items to memorize, which may further slow down the adjustment process. The flexibility in allocating resources that is implied by this model seems to be at odds with the slowness of cognitive control, well-documented by task switching costs. The authors touch on this point in the very last line of the paper, where they note that divisive normalization can provide a rapid adjustment mechanism. Even though the discussion is already long, it would be good to hear more about this point, and a comparison between a hardwired allocation mechanism and slower but more flexible cognitive control strategies.</p><p>Furthermore, in the Frank paper described above, they use a task in which there is binary feedback that depends on the liberal vs. conservative error criterion (and they don't report major differences in performance as a function of this). One might suspect that participants would fail to adapt their policy even in an incentive-compatible version of this task which systematically varied these behavioral costs, and this would present a challenge to the authors' theory as described here. Please add some text to the Discussion addressing this point.</p><p>4) Total precision vs. set-size: A novel and interesting prediction of the model is the non-monotonic relation between set-size and total precision (Figure 3B). Although the authors state that this point requires more empirical documentation, are the model results consistent with a non-monotonic encoding of target location that was reported by Balan et al. (2008) in monkey area LIP? That study found that, in a covert visual search task with different set sizes, the fidelity of target location encoding by area LIP was higher at set size 4 than at set size 2 or set size 6 (see Figure 5) – a non-linearity that was puzzling at the time but may gain new significance in light of this paper. Please add some discussion of this point to the manuscript.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.34963.023</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The number of encoded items: The paper mentions a possible hard constraint on the number of items encoded. Depending on set size and cost functions, when (if ever) is it &quot;optimal&quot; not to encode some of the items? Some new modeling results should be shown here to shore this up and a longer discussion of this point should be added to the Results and Discussion.</p></disp-quote><p>This is an interesting question, because its answer can possibly provide a principled bridge between slot-based and resource models of VWM. We now address this question in three different places. First, a mathematical analysis of the general conditions under which it is optimal to not encode an item is provided in Appendix 1. Second, the question is addressed in the context of delayed-estimation tasks in the new Results section “Is it ever optimal to not encode an item?”.Third, for the change-detection task, the question is addressed in the new section “Predictions for a global task: whole-display change detection”.</p><disp-quote content-type="editor-comment"><p>Furthermore, please expand the discussion of the costs of encoding more items versus fewer. Intuitively, it is obvious that it is more costly to encode 8 items than 2, but there are numerous reasons why this could be the case. Are there any experimental data in support of this assumption? In particular, are there any experimental data showing that encoding more items results in higher firing rates (at the population level)?</p></disp-quote><p>For many choices of spike variability, the total precision of a set of stimuli encoded in a neural population is proportional to the trial-averaged neural spiking rate (e.g., Paradiso, 1988; Seung and Sompolinsky, 1993; Ma et al., 2006). Based on this theoretical argument, it is expected that it is more costly (in terms of neural spiking) to encode 8 items compared to 2, <italic>if they are encoded with the same precision.</italic> </p><p>However, it is important to keep in mind that our model does not predict that the total spiking rate will increase with set size, because it generally predicts the precision per item (i.e., spike rate per item) to decrease with set size, which is consistent with physiological evidence (e.g., Churchland et al., 2008; Balan et al., 2008; Basso and Wurtz, 1998). The maximum-likelihood fits suggest that the total amount of invested resource varies non-monotonically with set size, which predicts that the population-level spiking activity also varies non-monotonically with set size. We are not aware of any work that strongly supports or rejects this prediction (see also our response below to the point about the Balan et al. paper). We address this point as follows in a new discussion section “Neural prediction”.</p><disp-quote content-type="editor-comment"><p>Michael Frank and his group have recently made a very principled attempt to characterize the nature of the cost in these tasks, in terms of how participants might group or chunk the items. It may be beyond the scope of the current paper to attempt to outline a theory that explains these results, but perhaps readers would enjoy some more elaborate discussion of this issue.</p></disp-quote><p>We assume that this comment refers to the recent paper by Nassar, Helmers, and Frank. If we understand correctly, this paper is currently in press, so we base our response to this comment on the preprint that is available on bioRxiv.</p><p>We agree that this paper has several connections with our own study, and we now refer to it at two different places. First, in the Introduction:</p><p>“Finally, Nassar and colleagues have proposed a normative model in which a strategic trade-off is made between the number of encoded items and their precision: when two items are very similar, they are encoded as a single item, such that there is more resource available per encoded item (Nassar et al., 2018). […] However, just as in much of the work discussed above, this theory assumes a fixed resource budget for item encoding, which is not necessarily optimal when resource usage is costly.”</p><p>And then again in the “Limitations” section in the Discussion:</p><p>“A final limitation is that our theory assumes that items are uniformly distributed and uncorrelated. […] Hence, it seems worthwhile to examine models that combine resource rationality with chunking.”</p><disp-quote content-type="editor-comment"><p>2) τ: Please provide additional model results that show how the fits look when τ = 0. In particular, please show the goodness of fit of the rational model (with τ = 0) as compared to the fit of a model with a &quot;hard&quot; constraint on resources?</p></disp-quote><p>We have added this analysis:</p><p>“So far, we have assumed that there is random variability in the actual amount of resource assigned to an item. […] Therefore, we will only consider variable-precision models in the remainder of the paper.”</p><p>As in the variable-precision model, the optimal amount of resource per item decreases with set size in the equal-precision variant of the rational model:</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.34963.019</object-id><label>Author response image 1.</label><caption/><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-34963-resp-fig1-v2"/></fig><p>Since we think that focusing too much on equal-precision results distracts a bit from the main story, we decided not to include this plot in the paper. As we explain in response to a later comment, the difference between the equal-precision and variable-precision models is mainly in the predicted kurtosis (“peakiness”) of the error distribution, not in the variance of these distributions (let alone in how the variance changes with set size). Hence, the equal-precision vs.variable-precision question is orthogonal to our main question.</p><disp-quote content-type="editor-comment"><p>In general, it was confusing that the theory is initially described in absence of τ, while τ is used for actual fits to data. It would have been easier to understand if the theory had been evaluated in presence of τ, and its effect studied within that theoretical framework.</p></disp-quote><p>Sorry, this was indeed confusing. In the rewritten “Theory” section, we explicitly indicate which equations depend on τ, by including it in the function arguments.</p><disp-quote content-type="editor-comment"><p>3) The speed of policy update: An assumption of the model that may be problematic is that people must almost instantaneously optimize their encoding precision when set-sizes change unpredictably from trial to trial. The theory predicts that, when a trial contains 2 stimuli and the next trial contains 4 stimuli, the participant instantaneously lowers the encoding precision to the new (near-) optimal level. This process sounds pretty demanding itself, especially considering that humans may also play an active role in determining their intrinsic motivation or deciding which items to memorize, which may further slow down the adjustment process. The flexibility in allocating resources that is implied by this model seems to be at odds with the slowness of cognitive control, well-documented by task switching costs. The authors touch on this point in the very last line of the paper, where they note that divisive normalization can provide a rapid adjustment mechanism. Even though the discussion is already long, it would be good to hear more about this point, and a comparison between a hardwired allocation mechanism and slower but more flexible cognitive control strategies.</p></disp-quote><p>This is an important issue, which we now discuss in the new Discussion section “Neural mechanisms and timescale of optimization”.</p><disp-quote content-type="editor-comment"><p>Furthermore, in the Frank paper described above, they use a task in which there is binary feedback that depends on the liberal vs. conservative error criterion (and they don't report major differences in performance as a function of this). One might suspect that participants would fail to adapt their policy even in an incentive-compatible version of this task which systematically varied these behavioral costs, and this would present a challenge to the authors' theory as described here. Please add some text to the Discussion addressing this point.</p></disp-quote><p>The experiment by Frank et al. used feedback threshold of π/3 (“low precision” condition) and π/8 (“high precision” condition) and found no difference in absolute estimation error between these two conditions. This would be at odds with any model that predicts that encoding precision is higher in the “high precision” condition, which is what one may expect to happen in our model. However, it turns out that the predictions for this experiment are not that straightforward and that the model can actually account for the lack of an effect. The short explanation is that there is a threshold region in which the prediction barely changes as a function of threshold, due to the performance benefit of adding extra resource is almost exactly outdone by the added neural cost. For a more detailed explanation, we refer to the new Figure 6.</p><p>This point is now also discussed in a new Discussion section “Experimental predictions of incentive manipulations”.</p><disp-quote content-type="editor-comment"><p>4) Total precision vs. set-size: A novel and interesting prediction of the model is the non-monotonic relation between set-size and total precision (Figure 3B). Although the authors state that this point requires more empirical documentation, are the model results consistent with a non-monotonic encoding of target location that was reported by Balan et al. (2008) in monkey area LIP? That study found that, in a covert visual search task with different set sizes, the fidelity of target location encoding by area LIP was higher at set size 4 than at set size 2 or set size 6 (see Figure 5) – a non-linearity that was puzzling at the time but may gain new significance in light of this paper. Please add some discussion of this point to the manuscript.</p></disp-quote><p>We thank the reviewer for the reference, as we were not aware of that paper. However, after a careful study of the results reported in that paper, we don’t see how the non-monotonic trend in Figure 3 can be linked to the predicted non-monotonicity in the total amount of invested resource. The non-monotonicity in the Balan paper shows that the stimulus identity (target/distractor) can be decoded more accurately from neural data in N=4 trials compared to N=2 and N=6 trials. However, we do not see how decoding accuracy of a single item relates to the total amount of resource invested in all items. Although it would have been nice if the Balan paper backs up the non-monotonicity prediction, we believe that linking our prediction to their result would be a bit misleading, so we decided to not include this point. (However, if we misunderstood the reviewer’s suggestion, we would of course be happy to have another look at it after some clarification).</p><p>Nevertheless, the Balan paper is relevant to our work for other reasons and we now cite it at two different places in the Discussion. First, in the Discussion section about experimental predictions and, second, in the rewritten part about Neural mechanisms (see responses to previous comments).</p></body></sub-article></article>