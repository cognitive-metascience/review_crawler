<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">31557</article-id><article-id pub-id-type="doi">10.7554/eLife.31557</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Sensory cortex is optimized for prediction of future input</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-98202"><name><surname>Singer</surname><given-names>Yosef</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4480-0574</contrib-id><email>yosef.singer@stcatz.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-68508"><name><surname>Teramoto</surname><given-names>Yayoi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3419-0351</contrib-id><email>yayoi.teramoto@balliol.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-38220"><name><surname>Willmore</surname><given-names>Ben DB</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2969-7572</contrib-id><email>benjamin.willmore@dpag.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-98209"><name><surname>Schnupp</surname><given-names>Jan WH</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-14601"><name><surname>King</surname><given-names>Andrew J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5180-7179</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-19854"><name><surname>Harper</surname><given-names>Nicol S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7851-4840</contrib-id><email>nicol.harper@dpag.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Physiology, Anatomy and Genetics</institution><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Biomedical Sciences</institution><institution>City University of Hong Kong</institution><addr-line><named-content content-type="city">Kowloon Tong</named-content></addr-line><country>Hong Kong</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gallant</surname><given-names>Jack L</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Kastner</surname><given-names>Sabine</given-names></name><role>Senior Editor</role><aff><institution>Princeton University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>18</day><month>06</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e31557</elocation-id><history><date date-type="received" iso-8601-date="2017-10-04"><day>04</day><month>10</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-06-16"><day>16</day><month>06</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Singer et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Singer et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-31557-v3.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.31557.001</object-id><p>Neurons in sensory cortex are tuned to diverse features in natural scenes. But what determines which features neurons become selective to? Here we explore the idea that neuronal selectivity is optimized to represent features in the recent sensory past that best predict immediate future inputs. We tested this hypothesis using simple feedforward neural networks, which were trained to predict the next few moments of video or audio in clips of natural scenes. The networks developed receptive fields that closely matched those of real cortical neurons in different mammalian species, including the oriented spatial tuning of primary visual cortex, the frequency selectivity of primary auditory cortex and, most notably, their temporal tuning properties. Furthermore, the better a network predicted future inputs the more closely its receptive fields resembled those in the brain. This suggests that sensory processing is optimized to extract those features with the most capacity to predict future input.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.31557.002</object-id><title>eLife digest</title><p>A large part of our brain is devoted to processing the sensory inputs that we receive from the world. This allows us to tell, for example, whether we are looking at a cat or a dog, and if we are hearing a bark or a meow. Neurons in the sensory cortex respond to these stimuli by generating spikes of activity. Within each sensory area, neurons respond best to stimuli with precise properties: those in the primary visual cortex prefer edge-like structures that move in a certain direction at a given speed, while neurons in the primary auditory cortex favour sounds that change in loudness over a particular range of frequencies.</p><p>Singer et al. sought to understand why neurons respond to the particular features of stimuli that they do. Why do visual neurons react more to moving edges than to, say, rotating hexagons? And why do auditory neurons respond more to certain changing sounds than to, say, constant tones? One leading idea is that the brain tries to use as few spikes as possible to represent real-world stimuli. Known as sparse coding, this principle can account for much of the behaviour of sensory neurons.</p><p>Another possibility is that sensory areas respond the way they do because it enables them to best predict future sensory input. To test this idea, Singer et al. used a computer to simulate a network of neurons and trained this network to predict the next few frames of video clips using the previous few frames. When the network had learned this task, Singer et al. examined the neurons’ preferred stimuli. Like neurons in primary visual cortex, the simulated neurons typically responded most to edges that moved over time.</p><p>The same network was also trained in a similar way, but this time using sound. As for neurons in primary auditory cortex, the simulated neurons preferred sounds that changed in loudness at particular frequencies. Notably, for both vision and audition, the simulated neurons favoured recent inputs over those further into the past. In this way and others, they were more similar to real neurons than simulated neurons that used sparse coding.</p><p>Both artificial networks trained to foretell sensory input and the brain therefore favour the same types of stimuli: the ones that are good at helping to grasp future information. This suggests that the brain represents the sensory world so as to be able to best predict the future.</p><p>Knowing how the brain handles information from our senses may help to understand disorders associated with sensory processing, such as dyslexia and tinnitus. It may also inspire approaches for training machines to process sensory inputs, improving artificial intelligence.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>prediction</kwd><kwd>cortex</kwd><kwd>ferret</kwd><kwd>auditory</kwd><kwd>normative</kwd><kwd>model</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Clarendon Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Singer</surname><given-names>Yosef</given-names></name><name><surname>Teramoto</surname><given-names>Yayoi</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT10525/Z/14/Z</award-id><principal-award-recipient><name><surname>Teramoto</surname><given-names>Yayoi</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT076508AIA</award-id><principal-award-recipient><name><surname>Willmore</surname><given-names>Ben DB</given-names></name><name><surname>King</surname><given-names>Andrew J</given-names></name><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT108369/Z/2015/Z</award-id><principal-award-recipient><name><surname>Willmore</surname><given-names>Ben DB</given-names></name><name><surname>King</surname><given-names>Andrew J</given-names></name><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>WT082692</award-id><principal-award-recipient><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000769</institution-id><institution>University Of Oxford</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000703</institution-id><institution>Action on Hearing Loss</institution></institution-wrap></funding-source><award-id>PA07</award-id><principal-award-recipient><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/H008608/1</award-id><principal-award-recipient><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Prediction of future input explains diverse neural tuning properties in sensory cortex.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Sensory inputs guide actions, but such actions necessarily lag behind these inputs due to delays caused by sensory transduction, axonal conduction, synaptic transmission, and muscle activation. To strike a cricket ball, for example, one must estimate its future location, not where it is now (<xref ref-type="bibr" rid="bib56">Nijhawan, 1994</xref>). Prediction has other fundamental theoretical advantages: a system that parsimoniously predicts future inputs from their past, and that generalizes well to new inputs, is likely to contain representations that reflect their underlying causes (<xref ref-type="bibr" rid="bib12">Bialek et al., 2001</xref>). This is important because ultimately, we are interested in these causes (e.g. flying cricket balls), not the raw images or sound waves incident on the sensory receptors. Furthermore, much of sensory processing involves discarding irrelevant information, such as that which is not predictive of the future, to arrive at a representation of what is important in the environment for guiding action (<xref ref-type="bibr" rid="bib12">Bialek et al., 2001</xref>).</p><p>Previous theoretical studies have suggested that many neural representations can be understood in terms of efficient coding of natural stimuli in a short time window at or just before the present (<xref ref-type="bibr" rid="bib6">Attneave, 1954</xref>; <xref ref-type="bibr" rid="bib8">Barlow, 1959</xref>; <xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>). Such studies generally built a network model of the brain, which was trained to represent stimuli subject to some set of constraints. One pioneering such study trained a network to efficiently represent static natural images using a sparse, generative model (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>). More recent studies have used related ideas to model the representation of moving (rather than static) images (<xref ref-type="bibr" rid="bib86">van Hateren and Ruderman, 1998a</xref>; <xref ref-type="bibr" rid="bib11">Berkes and Wiskott, 2005</xref>; <xref ref-type="bibr" rid="bib10">Berkes et al., 2009</xref>) and other sensory stimuli (<xref ref-type="bibr" rid="bib44">Klein et al., 2003</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib91">Zhao and Zhaoping, 2011</xref>; <xref ref-type="bibr" rid="bib45">Kozlov and Gentner, 2016</xref>; <xref ref-type="bibr" rid="bib23">Cusack and Carlyon, 2004</xref>). In contrast, we built a network model that was optimized not for efficient representation of the recent past, but for efficient prediction of the immediate future of the stimulus, which we will refer to as the temporal prediction model. The timescale of prediction considered for our model is in the range of tens to hundreds of milliseconds. Conduction delays to cortex and very fast motor responses are on this timescale (<xref ref-type="bibr" rid="bib13">Bixler et al., 1967</xref>; <xref ref-type="bibr" rid="bib90">Yeomans and Frankland, 1995</xref>; <xref ref-type="bibr" rid="bib14">Bizley et al., 2005</xref>).</p><p>The idea that prediction is an important component of perception dates at least as far back as Helmholtz (<xref ref-type="bibr" rid="bib34">Helmholtz, 1962</xref>; <xref ref-type="bibr" rid="bib84">Sutton and Barton, 1981</xref>), although what is meant by prediction and the purpose it serves is quite varied between models incorporating it (<xref ref-type="bibr" rid="bib20">Chalk et al., 2018</xref>; <xref ref-type="bibr" rid="bib77">Salisbury and Palmer, 2016</xref>). With regards to perception and prediction, two contrasting but interrelated frameworks have been distinguished (<xref ref-type="bibr" rid="bib20">Chalk et al., 2018</xref>; <xref ref-type="bibr" rid="bib77">Salisbury and Palmer, 2016</xref>). In the ‘predictive coding’ framework (<xref ref-type="bibr" rid="bib35">Huang and Rao, 2011</xref>; <xref ref-type="bibr" rid="bib69">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib30">Friston, 2003</xref>), prediction is used to remove statistical redundancy in order to provide an efficient representation of the entire stimulus. Some models of this type use prediction as a term for estimation of the current or a static input (such as images) from latent variables (<xref ref-type="bibr" rid="bib69">Rao and Ballard, 1999</xref>), whereas other have also considered the temporal dimension of the input (<xref ref-type="bibr" rid="bib68">Rao and Ballard, 1997</xref>; <xref ref-type="bibr" rid="bib70">Rao, 1999</xref>; <xref ref-type="bibr" rid="bib82">Srinivasan et al., 1982</xref>). Sparse coding models (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>) can be related to this framework (<xref ref-type="bibr" rid="bib35">Huang and Rao, 2011</xref>). In contrast, the ‘predictive information’ framework (<xref ref-type="bibr" rid="bib12">Bialek et al., 2001</xref>; <xref ref-type="bibr" rid="bib77">Salisbury and Palmer, 2016</xref>; <xref ref-type="bibr" rid="bib65">Palmer et al., 2015</xref>; <xref ref-type="bibr" rid="bib33">Heeger, 2017</xref>), which our approach relates to more closely, involves selective encoding of those features of the stimulus that predict future input. A related idea to predictive information is the encoding of slowly varying features (<xref ref-type="bibr" rid="bib11">Berkes and Wiskott, 2005</xref>; <xref ref-type="bibr" rid="bib22">Creutzig and Sprekeler, 2008</xref>; <xref ref-type="bibr" rid="bib41">Kayser et al., 2001</xref>; <xref ref-type="bibr" rid="bib38">Hyvärinen et al., 2003</xref>), which are one kind of predictive feature. Hence, the predictive coding approach seeks to find a compressed representation of the entire input, whereas the predictive information approach selectivity encodes only predictive features (<xref ref-type="bibr" rid="bib20">Chalk et al., 2018</xref>; <xref ref-type="bibr" rid="bib77">Salisbury and Palmer, 2016</xref>). Our model relates to the predictive information approach in that it is optimized to predict the future from the past, but it has a combination of characteristics, such a non-linear encoder and sparse weight regularization, which have not previously been explored for such an approach.</p><p>To evaluate the representations produced by these normative theoretical models, they can be optimized for natural stimuli, and the tuning properties of their units compared to the receptive fields of real neurons. A useful and commonly used definition of a neuron’s receptive field (RF) is the stimulus that maximally linearly drives the neuron (<xref ref-type="bibr" rid="bib1">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="bib3">Aertsen et al., 1981</xref>; <xref ref-type="bibr" rid="bib2">Aertsen and Johannesma, 1981</xref>; <xref ref-type="bibr" rid="bib72">Reid et al., 1987</xref>; <xref ref-type="bibr" rid="bib27">deCharms et al., 1998</xref>; <xref ref-type="bibr" rid="bib31">Harper et al., 2016</xref>). In mammalian primary visual cortex (V1), neurons typically respond strongly to oriented edge-like structures moving over a particular retinal location (<xref ref-type="bibr" rid="bib36">Hubel and Wiesel, 1959</xref>; <xref ref-type="bibr" rid="bib39">Jones and Palmer, 1987</xref>; <xref ref-type="bibr" rid="bib26">DeAngelis et al., 1993</xref>; <xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>). In mammalian primary auditory cortex (A1), most neurons respond strongly to changes in the amplitude of sounds within a certain frequency range (<xref ref-type="bibr" rid="bib27">deCharms et al., 1998</xref>).</p><p>The temporal prediction model provides a principled approach to understanding the temporal aspects of RFs. Previous models, based on sparsity or slowness related principles, were successful in accounting for many spatial aspects of V1 RF structure (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib86">van Hateren and Ruderman, 1998a</xref>; <xref ref-type="bibr" rid="bib11">Berkes and Wiskott, 2005</xref>; <xref ref-type="bibr" rid="bib10">Berkes et al., 2009</xref>; <xref ref-type="bibr" rid="bib87">van Hateren and van der Schaaf, 1998b</xref>), and had some success in accounting for spectral aspects of A1 RF structure (<xref ref-type="bibr" rid="bib44">Klein et al., 2003</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib91">Zhao and Zhaoping, 2011</xref>; <xref ref-type="bibr" rid="bib23">Cusack and Carlyon, 2004</xref>). However, these models do not account well for the temporal structure of V1 or A1 RFs. Notably, for both vision (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>) and audition (<xref ref-type="bibr" rid="bib27">deCharms et al., 1998</xref>), the envelopes of real neuronal RFs tend to be asymmetric in time, with greater sensitivity to very recent inputs compared to inputs further in the past. In contrast, the RFs predicted by previous models (<xref ref-type="bibr" rid="bib86">van Hateren and Ruderman, 1998a</xref>; <xref ref-type="bibr" rid="bib44">Klein et al., 2003</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib45">Kozlov and Gentner, 2016</xref>; <xref ref-type="bibr" rid="bib23">Cusack and Carlyon, 2004</xref>) typically show symmetrical temporal envelopes, with either approximately flat envelopes over time or a balanced falloff of the envelope over time either side of a peak. They also lack the greater sensitivity to very recent inputs.</p><p>Here we show using qualitative and quantitative comparisons that, for both V1 and A1 RFs, these shortcomings are largely overcome by the temporal prediction approach. This suggests that neural sensitivity at early levels of the cortical hierarchy may be organized to facilitate a rapid and efficient prediction of what the environment will look like in the next fraction of a second.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>The temporal prediction model</title><p>To determine what type of sensory RF structures would facilitate predictions of the imminent future, we built a feedforward network model with a single layer of nonlinear hidden units, mapping the inputs to the outputs through weighted connections (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Each hidden unit’s output results from a linear mapping (by input weights) from the past input, followed by a monotonic nonlinearity, much like the classic linear-nonlinear model of sensory neurons (<xref ref-type="bibr" rid="bib44">Klein et al., 2003</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib91">Zhao and Zhaoping, 2011</xref>). The model then generates a prediction of the future from a linear mapping (by output weights) from the hidden units’ outputs. This is consistent with the observation that decoding from the neural response is often well approximated by a linear transformation (<xref ref-type="bibr" rid="bib28">Eliasmith and Anderson, 2003</xref>).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.31557.003</object-id><label>Figure 1.</label><caption><title>Temporal prediction model implemented using a feedforward artificial neural network, with the same architecture in both visual and auditory domains.</title><p>(<bold>a</bold>), Network trained on cochleagram clips (spectral content over time) of natural sounds, aims to predict immediate future time steps of each clip from recent past time steps. (<bold>b</bold>), Network trained on movie clips of natural scenes, aims to predict immediate future frame of each clip from recent past frames. <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, input – the past; <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, input weights; <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, hidden unit output; <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, output weights; <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, output – the predicted future; <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, target output – the true future. Hidden unit’s RF is the <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> between the input and that unit <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig1-v3"/></fig><p>We trained the temporal prediction model on extensive corpora, either of soundscapes or silent movies, modelling A1 (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) or V1 (<xref ref-type="fig" rid="fig1">Figure 1b</xref>) neurons, respectively. In each case, the networks were trained by optimizing their synaptic weights to most accurately predict the immediate future of the stimulus from its very recent past. For vision, the inputs were patches of videos of animals moving in natural settings, and we trained the network to predict the pixel values for one movie frame (40 ms) into the future, based on the seven most recent frames (280 ms). For audition, we trained the network to predict the next three time steps (15 ms) of cochleagrams of natural sounds based on the 40 most recent time steps (200 ms). Cochleagrams resemble spectrograms but are adjusted to approximate the auditory nerve representation of sounds (see Materials and methods).</p><p>During training we used sparse, <italic>L</italic><sub>1</sub> weight regularization (see <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> in Materials and methods) to constrain the network to predict future stimuli in a parsimonious fashion, forcing the network to use as few weights as possible while maintaining an accurate prediction. This constraint can be viewed as an assumption about the sparse nature of causal dependencies underlying the sensory input, or alternatively as analogous to the energy and space restrictions of neural connectivity. It also prevents our network model from overfitting to its inputs. Note that this sparsity constraint differs from that used in sparse coding models, in that it is applied to the weights rather than the activity of the units, being more like a constraint on the wiring between neurons than a constraint on their firing rates.</p></sec><sec id="s2-2"><title>Qualitative assessment of auditory receptive fields</title><p>To compare with the model, we recorded responses of 114 auditory neurons (including 76 single units) in A1 and the anterior auditory field (AAF) of 5 anesthetized ferrets (<xref ref-type="bibr" rid="bib88">Willmore et al., 2016</xref>) and measured their spectrotemporal RFs (see Materials and methods). Ferrets are commonly used for auditory research, because they are readily trained in a range of sound detection, discrimination or localization tasks (<xref ref-type="bibr" rid="bib57">Nodal and King, 2014</xref>), the frequency range of their hearing (approximately 40 Hz–40 kHz [<xref ref-type="bibr" rid="bib40">Kavanagh and Kelly, 1988</xref>]) overlaps well with (and extends beyond) the human range, and most of their auditory cortex is not buried in a sulcus and hence easily accessible for electrophysiological or optical measurements.</p><p>The A1 RFs we recorded are diverse (<xref ref-type="fig" rid="fig2">Figure 2a</xref>); their frequency tuning can be narrowband or broadband, and sometimes showing flanking inhibition. Some may also be more complex in frequency tuning, lack clear order, or be selective for the direction of frequency modulation (<xref ref-type="bibr" rid="bib18">Carlin and Elhilali, 2013</xref>).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.31557.004</object-id><label>Figure 2.</label><caption><title>Auditory spectrotemporal and visual spatiotemporal RFs of real neurons and temporal prediction model units.</title><p>(<bold>a</bold>), Example spectrotemporal RFs of real A1 neurons (<xref ref-type="bibr" rid="bib88">Willmore et al., 2016</xref>). Red – excitation, blue – inhibition. Most recent two time steps (10 ms) were removed to account for conduction delay. (<bold>b</bold>), Example spectrotemporal RFs of model units when model is trained to predict the future of natural sound inputs. Note that the overall sign of a receptive field learned by the model is arbitrary. Hence, in all figures and analyses we multiplied each model receptive field by −1 where appropriate to obtain receptive fields which all have positive leading excitation (see Materials and methods). (<bold>c</bold>), Example spatiotemporal (I, space-time separable, and II, space-time inseparable) RFs of real V1 neurons (<xref ref-type="bibr" rid="bib59">Ohzawa et al., 1996</xref>). Left, grayscale: 3D (space-space-time) spatiotemporal RFs showing the spatial RF at each of the most recent six time steps. Most recent time step (40 ms) was removed to account for conduction delay. White – excitation, black – inhibition. Right: corresponding 2D (space-time) spatiotemporal RFs obtained by summing along the unit’s axis of orientation for each time step. Red – excitation, blue – inhibition. (<bold>d</bold>), Example 3D and corresponding 2D spatiotemporal (I-III, space-time separable, and IV-VI, space-time inseparable) RFs of model units when model is trained to predict the future of natural visual inputs.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig2-v3"/></fig><p>In their temporal tuning, A1 RFs tend to weight recent inputs more heavily, with a temporally asymmetric power profile, involving excitation near the present followed by lagging inhibition of a longer duration (<xref ref-type="bibr" rid="bib27">deCharms et al., 1998</xref>). The temporal prediction model RFs (<xref ref-type="fig" rid="fig2">Figure 2b</xref>) are similarly diverse, showing all of the RF types seen in vivo (including examples of localized, narrowband, broadband, complex, disordered and directional RFs) and are well matched in scale and form to those measured in A1. This includes having greater power (mean square) near the present, with brief excitation followed by longer lagging inhibition, producing an asymmetric power profile. This stands in contrast to previous attempts to model RFs based on efficient coding,sparsecoding and slow feature hypotheses, which either did not capture the diversity of RFs (<xref ref-type="bibr" rid="bib91">Zhao and Zhaoping, 2011</xref>), or lacked temporal asymmetry, punctate structure, or appropriate time scale (<xref ref-type="bibr" rid="bib44">Klein et al., 2003</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib45">Kozlov and Gentner, 2016</xref>; <xref ref-type="bibr" rid="bib23">Cusack and Carlyon, 2004</xref>; <xref ref-type="bibr" rid="bib18">Carlin and Elhilali, 2013</xref>; <xref ref-type="bibr" rid="bib17">Brito and Gerstner, 2016</xref>).</p></sec><sec id="s2-3"><title>Qualitative assessment of visual receptive fields</title><p>By eye, substantial similarities were also apparent when we compared the temporal prediction model’s RFs trained using visual inputs (<xref ref-type="fig" rid="fig1">Figure 1b</xref>) with the 3D (space-space-time) and 2D (space-time) spatiotemporal RFs of real V1 simple cells, which were obtained from Ohzawa et al (<xref ref-type="bibr" rid="bib59">Ohzawa et al., 1996</xref>). Simple cells (<xref ref-type="bibr" rid="bib36">Hubel and Wiesel, 1959</xref>) have stereotyped RFs containing parallel, spatially localized excitatory and inhibitory regions, with each cell having a particular preferred orientation and spatial frequency (<xref ref-type="bibr" rid="bib39">Jones and Palmer, 1987</xref>; <xref ref-type="bibr" rid="bib26">DeAngelis et al., 1993</xref>; <xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>) (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). These features are also clearly apparent in the model RFs (<xref ref-type="fig" rid="fig2">Figure 2d</xref>).</p><p>Unlike previous models (<xref ref-type="bibr" rid="bib86">van Hateren and Ruderman, 1998a</xref>; <xref ref-type="bibr" rid="bib38">Hyvärinen et al., 2003</xref>; <xref ref-type="bibr" rid="bib62">Olshausen, 2003</xref>), the temporal prediction model captures the temporal asymmetry of real RFs. The RF power is highest near the present and decays into the past (<xref ref-type="fig" rid="fig2">Figure 2d</xref>), as observed in real neurons (<xref ref-type="bibr" rid="bib59">Ohzawa et al., 1996</xref>) (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). Furthermore, simple cell RFs have two types of spatiotemporal structure: space-time separable RFs (<xref ref-type="fig" rid="fig2">Figure 2cI</xref>), whose optimal stimulus resembles a flashing or slowly ramping grating, and space-time inseparable RFs, whose optimal stimulus is a drifting grating (<xref ref-type="bibr" rid="bib26">DeAngelis et al., 1993</xref>) (<xref ref-type="fig" rid="fig2">Figure 2cII</xref>). Our model captures this diversity (<xref ref-type="fig" rid="fig2">Figure 2dI–III</xref> separable, <xref ref-type="fig" rid="fig2">Figure 2dIV–VI</xref> inseparable).</p><p>We also examined linear aspects of the tuning of the output units for the visual temporal prediction model using a response-weighted average to white noise input, and found punctate non-oriented RFs that decay into the past.</p></sec><sec id="s2-4"><title>Qualitative comparison to other models</title><p>For comparison, we trained a sparse coding model (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>) (<ext-link ext-link-type="uri" xlink:href="https://github.com/zayd/sparsenet">https://github.com/zayd/sparsenet</ext-link>) using our dataset. We would expect such a model to perform less well in the temporal domain, because unlike the temporal prediction model, the direction of time is not explicitly accounted for. The sparse coding model was chosen because it has set the standard for normative models of visual RFs (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib62">Olshausen, 2003</xref>; <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>), and the same model has also been applied for auditory RFs (<xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib17">Brito and Gerstner, 2016</xref>; <xref ref-type="bibr" rid="bib52">Młynarski and McDermott, 2017</xref>; <xref ref-type="bibr" rid="bib16">Blättler et al., 2011</xref>). Past studies (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>) have largely analysed the basis functions produced by the sparse coding model and compared their properties to neuronal RFs. To be consistent with these studies we have done the same, and to have a common term, refer to the basis functions as RFs (although strictly, they are projective fields). We can visually compare the large set of RFs recorded from A1 neurons (<xref ref-type="fig" rid="fig3">Figure 3</xref>) to the full set of RFs obtained from the temporal prediction model when trained on auditory inputs (<xref ref-type="fig" rid="fig4">Figure 4</xref>) and those of the sparse coding model (<xref ref-type="fig" rid="fig5">Figure 5</xref>) when trained on the same auditory inputs.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.31557.005</object-id><label>Figure 3.</label><caption><title>Full dataset of real auditory RFs.</title><p>114 neuronal RFs recorded from A1 and AAF of 5 ferrets. Red – excitation, blue - inhibition. Inset shows axes.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig3-v3"/></fig><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.31557.006</object-id><label>Figure 4.</label><caption><title>Full set of auditory RFs of the temporal prediction model units.</title><p>Units were obtained by training the model with 1600 hidden units on auditory inputs. The hidden unit number and <italic>L</italic><sub>1</sub> weight regularization strength (10<sup>−3.5</sup>) was chosen because it results in the lowest MSE on the prediction task, as measured using a cross validation set. Many hidden units’ weight matrices decayed to near zero during training (due to the <italic>L</italic><sub>1</sub> regularization), leaving 167 active units. Inactive units were excluded from analysis and are not shown. Example units in <xref ref-type="fig" rid="fig2">Figure 2</xref> come from this set. Red – excitation, blue - inhibition. Inset shows axes. <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> shows the same RFs on a finer timescale. The full sets of visual spatial and corresponding spatiotemporal RFs for the temporal prediction model when it is trained on visual inputs are shown in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplements 2</xref>–<xref ref-type="fig" rid="fig4s3">3</xref>. <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref> shows the auditory RFs of the temporal prediction model when a linear activation function instead of a sigmoid nonlinearity was used. <xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>–<xref ref-type="fig" rid="fig4s7">7</xref> show the auditory spectrotemporal and visual spatial and 2D spatiotemporal RFs of the temporal prediction model when it was trained on inputs without added noise.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig4-v3"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.007</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Full set of auditory RFs of the temporal prediction model units shown on a finer timescale.</title><p>All details are as in <xref ref-type="fig" rid="fig3">Figure 3</xref>, but the only the most recent 100 ms of the response profile is shown in order to illustrate details of the RFs. Inset shows axes.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig4-figsupp1-v3"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.008</object-id><label>Figure 4—figure supplement 2.</label><caption><title>Full set of visual spatial RFs of the temporal prediction model units.</title><p>Units in <xref ref-type="fig" rid="fig2">Figures 2</xref>,<xref ref-type="fig" rid="fig7">7</xref> come from this set. Each square represents the spatial RF of a single unit, shown at its best time step. The best time step was determined by selecting the time step for which the power (sum of squares) of the RF was greatest. White – excitation, black - inhibition.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig4-figsupp2-v3"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.009</object-id><label>Figure 4—figure supplement 3.</label><caption><title>Visual 2D (space-time) spatiotemporal RFs of temporal prediction model units.</title><p>Units in <xref ref-type="fig" rid="fig2">Figures 2</xref>,<xref ref-type="fig" rid="fig7">7</xref> come from this set. M . Each square represents the 2D spatiotemporal RF of a single unit corresponding to the unit in the same position in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>, obtained by summing across space along the axis of the orientation for that unit. Red – excitation, blue - inhibition. Inset shows axes.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig4-figsupp3-v3"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.010</object-id><label>Figure 4—figure supplement 4.</label><caption><title>Full set of auditory RFs of the temporal prediction model units using a linear activation function.</title><p>Units were obtained by training the model with 1600 hidden units on auditory inputs. The hidden unit number and <italic>L</italic><sub>1</sub> weight regularization strength (10<sup>−3.25</sup>) was chosen because they result in the lowest MSE on the prediction task, as measured using a cross validation set. Almost all hidden units’ weight matrices decayed to near zero during training (due to the <italic>L</italic><sub>1</sub> regularization), leaving 35 active units. Inactive units were excluded from analysis and are not shown. Red – excitation, blue - inhibition. Inset shows axes.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig4-figsupp4-v3"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.011</object-id><label>Figure 4—figure supplement 5.</label><caption><title>Full set of auditory RFs of the temporal prediction model units trained on auditory inputs without added noise.</title><p>Units were obtained by training the model with 1600 hidden units on auditory inputs. The hidden unit number and <italic>L</italic><sub>1</sub> weight regularization strength (10<sup>−4</sup>) was chosen because it results in the lowest MSE on the prediction task, as measured using a cross validation set. Many hidden units’ weight matrices decayed to near zero during training (due to the <italic>L</italic><sub>1</sub> regularization), leaving 465 active units. Inactive units were excluded from analysis and are not shown. Red – excitation, blue - inhibition. Inset shows axes.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig4-figsupp5-v3"/></fig><fig id="fig4s6" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.012</object-id><label>Figure 4—figure supplement 6.</label><caption><title>Full set of visual spatial RFs of temporal prediction model units trained on visual inputs without added noise.</title><p>Model units were obtained by training the model with 1600 hidden units on visual inputs. The hidden unit number and <italic>L</italic><sub>1</sub> weight regularization strength (10<sup>−6.25</sup>) was chosen because it results in the lowest MSE on the prediction task, as measured using a cross validation set. Some hidden units’ weight matrices decayed to near zero during training (due to the <italic>L</italic><sub>1</sub> regularization), leaving 1585 active units, which were included in analysis. Inactive units were excluded from analysis. Each square represents the spatial RF of a single unit, shown at its best time step. The best time step was determined by selecting the time step for which the power (sum of squares) of the RF was greatest. White – excitation, black - inhibition.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig4-figsupp6-v3"/></fig><fig id="fig4s7" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.013</object-id><label>Figure 4—figure supplement 7.</label><caption><title>2D (space-time) visual spatiotemporal RFs of temporal prediction model units trained on visual inputs without added noise.</title><p>Obtained from the same units shown in <xref ref-type="fig" rid="fig4s6">Figure 4—figure supplement 6</xref> using methods outlined in <xref ref-type="fig" rid="fig2">Figure 2c</xref>. Red – excitation, blue - inhibition. Inset shows axes.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig4-figsupp7-v3"/></fig></fig-group><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.31557.014</object-id><label>Figure 5.</label><caption><title>Full set of auditory ‘RFs’ (basis functions) of sparse coding model used as a control.</title><p>Units were obtained by training the sparse coding model with 1600 units on the identical auditory inputs used to train the network shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. <italic>L</italic><sub>1</sub> regularization of strength 10<sup>0.5</sup> was applied to the units’ activities. This network configuration was selected as it produced unit RFs that most closely resembled those recorded in A1, as determined using the KS measure of similarity <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> . Although the basis functions of the sparse coding model are not receptive fields, but projective fields, they tend to be similar in structure (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>). In this manuscript, to have a common term between models and the data, we refer to sparse coding basis functions as RFs. Red – excitation, blue - inhibition. Inset shows axes. The full sets of visual spatial and corresponding spatiotemporal RFs for the sparse coding model when it is trained on visual inputs are shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s2">2</xref>. <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplements 3</xref>–<xref ref-type="fig" rid="fig5s5">5</xref> show the auditory spectrotemporal and visual spatial and 2D spatiotemporal RFs of the sparse coding model when it was trained on inputs without added noise.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig5-v3"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.015</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Full set of visual spatial RFs of sparse coding model units.</title><p>Model units were obtained by training the sparse coding model with 3200 units on identical visual inputs used to train the temporal prediction model <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>. The model configuration (3200 units, <italic>L</italic><sub>1</sub> sparsity strength of 10<sup>0.5</sup> on the unit activities) was chosen because it resulted in the RFs that look most like the RFs of V1 simple cells as determined by visual inspection. Each square represents the spatial RF of a single unit, shown at its best time step. The best time step was determined by selecting the time step for which the power (sum of squares) of the RF was greatest. White – excitation, black - inhibition.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig5-figsupp1-v3"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.016</object-id><label>Figure 5—figure supplement 2.</label><caption><title>2D (space-time) visual spatiotemporal RFs of sparse coding model units.</title><p>Obtained from the same units shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> using methods outlined in <xref ref-type="fig" rid="fig2">Figure 2c</xref>. Red – excitation, blue - inhibition. Inset shows axes.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig5-figsupp2-v3"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.017</object-id><label>Figure 5—figure supplement 3.</label><caption><title>Full set of auditory RFs of sparse coding model trained on auditory inputs without added noise.</title><p>Units were obtained by training the sparse coding model with 1600 units on the identical auditory inputs used to train the network shown in <xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>. <italic>L</italic><sub>1</sub> regularization of strength 10<sup>0.5</sup> was applied to the units’ activities. This network configuration was selected as it produced unit RFs that most closely resembled those recorded in A1, as determined by visual inspection. Red – excitation, blue - inhibition. Inset shows axes.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig5-figsupp3-v3"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.018</object-id><label>Figure 5—figure supplement 4.</label><caption><title>Full set of visual spatial RFs of sparse coding model units trained on visual inputs without added noise.</title><p>Model units were obtained by training the sparse coding model with 3200 units on identical visual inputs used to train the temporal prediction model <xref ref-type="fig" rid="fig4s6">Figure 4—figure supplement 6</xref>. The model configuration (3200 units, <italic>L</italic><sub>1</sub> sparsity strength of 10<sup>0.5</sup> on the unit activities) was chosen because it resulted in the RFs that look most like the RFs of V1 simple cells as determined by visual inspection. Each square represents the spatial RF of a single unit, shown at its best time step. The best time step was determined by selecting the time step for which the power (sum of squares) of the RF was greatest. White – excitation, black - inhibition.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig5-figsupp4-v3"/></fig><fig id="fig5s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.019</object-id><label>Figure 5—figure supplement 5.</label><caption><title>2D (space-time) visual spatiotemporal RFs of sparse coding model units trained on visual inputs without added noise.</title><p>Obtained from the same units shown in <xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref> using methods outlined in <xref ref-type="fig" rid="fig2">Figure 2c</xref>. Red – excitation, blue - inhibition. Inset shows axes.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig5-figsupp5-v3"/></fig></fig-group><p>A range of RFs were produced by the sparse coding model, some of which show characteristics reminiscent of A1 RFs, particularly in the frequency domain. However, the temporal properties of A1 neurons are not well captured by these RFs. While some RFs display excitation followed by lagging inhibition, very few, if any, show distinct brief excitation followed by extended inhibition. Instead, RFs that show both excitation and inhibition tend to have a symmetric envelope and these features are randomly localized in time, and many RFs display temporally elongated structures that are not found in A1 neurons.</p><p>We also trained the sparse coding model on the dataset of visual inputs to serve as a control for the temporal prediction model trained on these same inputs. We compared the full population of spatial and 2D spatiotemporal visual RFs of the temporal prediction model (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplements 2–3</xref>) and the sparse coding model (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s2">2</xref>). As shown in previous studies (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib86">van Hateren and Ruderman, 1998a</xref>; <xref ref-type="bibr" rid="bib87">van Hateren and van der Schaaf, 1998b</xref>), the sparse coding model produces RFs whose spatial structure resembles that of V1 simple cells (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s2">2</xref>), but does not capture the asymmetric nature of the temporal tuning of V1 neurons. Furthermore, while it does produce examples of both separable and inseparable spatiotemporal RFs, those that are separable tend to be completely stationary over time, resembling immobile rather than flashing gratings (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>).</p></sec><sec id="s2-5"><title>Quantitative analysis of auditory results</title><p>We compared the RFs generated by both models to the RFs of the population of real A1 neurons we recorded. We first compared the RFs in a non-parametric manner by measuring the Euclidean distances between the coefficient values of the RFs, and then used multi-dimensional scaling to embed these distances in a two-dimensional space (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). The RFs of the sparse coding model span a much larger region than the real A1 and temporal prediction model RFs. Furthermore, the A1 and temporal prediction model RFs occupy a similar region of the space, indicating their greater similarity to each other relative to those of the sparse coding model. We then examined specific attributes of the RFs to determine points of similarity and difference between each of the models and the recorded data. We first considered the temporal properties of the RFs and found that for the data and the temporal prediction model, most of the power is contained in the most recent time-steps (<xref ref-type="fig" rid="fig2">Figures 2a–b</xref>, <xref ref-type="fig" rid="fig3">3</xref>–<xref ref-type="fig" rid="fig4">4</xref> and <xref ref-type="fig" rid="fig6">6b</xref>, and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Given that the direction of time is not explicitly accounted for in the sparse coding model, as expected, it does not show this feature (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6b</xref>). Next, we examined the tuning widths of the RFs in each population for both time and frequency, looking at excitation and inhibition separately. In the time domain, the real data tend to show leading excitation followed by lagging inhibition of longer duration (<xref ref-type="fig" rid="fig2">Figures 2a</xref>, <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig6">6c–e</xref>). The temporal prediction model also shows many RFs with this temporal structure, with lagging inhibition of longer duration than the leading excitation (<xref ref-type="fig" rid="fig2">Figures 2b</xref>, <xref ref-type="fig" rid="fig4">4</xref> and <xref ref-type="fig" rid="fig6">6c–e</xref>, and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). This is not the case with the sparse coding model, where units tend to show either excitation and inhibition having the same duration or an elongated temporal structure that does not show such stereotyped polarity changes (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6c–e</xref>). It is also the case that the absolute timescales of excitation and inhibition match the data more closely in the case of the temporal prediction model (<xref ref-type="fig" rid="fig6">Figure 6c–e</xref>), although a few units display inhibition of a longer duration than is seen in the data (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). The sparse coding model shows a wide range of temporal spans of excitation and inhibition, in keeping with previous studies (<xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib18">Carlin and Elhilali, 2013</xref>).</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.31557.020</object-id><label>Figure 6.</label><caption><title>Population measures for real A1, temporal prediction model and sparse coding model auditory spectrotemporal RFs.</title><p>The population measures are taken from the RFs shown in <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig5">5</xref>. (<bold>a</bold>), Each point represents a single RF (with 32 frequency and 38 time steps) which has been embedded in a 2-dimensional space using Multi-Dimensional Scaling (MDS). Red circles - real A1 neurons, black circles – temporal prediction model units, blue triangles – sparse coding model units. Colour scheme applies to all subsequent panels. (<bold>b</bold>), Proportion of power contained in each time step of the RF, taken as an average across the population of units. (<bold>c</bold>), Temporal span of excitatory subfields versus that of inhibitory subfields, for real neurons and temporal prediction and sparse coding model units. The area of each circle is proportional to the number of occurrences at that point. The inset plots, which zoom in on the distribution use a smaller constant of proportionality for the circles to make the distributions clearer. (<bold>d</bold>), Distribution of temporal spans of excitatory subfields, taken by summing along the x-axis in (<bold>c</bold>). (<bold>e</bold>), Distribution of temporal spans of inhibitory subfields, taken by summing along the y-axis in (<bold>c</bold>). (<bold>f</bold>), Frequency span of excitatory subfields versus that of inhibitory subfields, for real neurons and temporal prediction and sparse coding model units. (<bold>g</bold>), Distribution of frequency spans of excitatory subfields, taken by summing along the x-axis in (<bold>f</bold>). (<bold>h</bold>), Distribution of frequency spans of inhibitory subfields, taken by summing along the y-axis in (<bold>f</bold>). <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> shows the same analysis for the temporal prediction model and sparse coding model trained on auditory inputs without added noise.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig6-v3"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.021</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Population measures for real A1, temporal prediction model and sparse coding model auditory spectrotemporal RFs when models are trained on auditory inputs without added noise.</title><p>Real units are the same as those shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Temporal prediction model units are the same as those shown in <xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>. Sparse coding model units are the same as those shown in <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>. (<bold>a</bold>), Each point represents a single RF (with 32 frequency and 38 time steps) which has been embedded in a two dimensional space using Multi-Dimensional Scaling (MDS). Red circles - real A1 neurons, black circles – temporal prediction model units, blue triangles – sparse coding model units. Colour scheme applies to all subsequent panels in Figure. (<bold>b</bold>), Proportion of power contained in each time step of the RF, taken as an average across the population of units. (<bold>c</bold>), Temporal span of excitatory subfields versus that of inhibitory subfields, for real neurons and temporal prediction and sparse coding model units. The area of each circle is proportional to the number of occurrences at that point. The inset plots, which zoom in on the distribution use a smaller constant of proportionality for the circles to make the distributions clearer. (<bold>d</bold>), Distribution of temporal spans of excitatory subfields, taken by summing along the x-axis in (<bold>c</bold>). (<bold>e</bold>), Distribution of temporal spans of inhibitory subfields, taken by summing along the y-axis in (<bold>c</bold>). (<bold>f</bold>), Frequency span of excitatory subfields versus that of inhibitory subfields, for real neurons and temporal prediction and sparse coding model units. (<bold>g</bold>) Distribution of frequency spans of excitatory subfields, taken by summing along the x-axis in (<bold>f</bold>). (<bold>h</bold>), Distribution of frequency spans of inhibitory subfields, taken by summing along the y-axis in (<bold>f</bold>). The addition of noise leads to subtle changes in the RFs. Without noise, the inhibition in the temporal prediction model tends to be slightly less extended and the RFs a little less smooth (see <xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref> for qualitative comparison).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig6-figsupp1-v3"/></fig></fig-group><p>Regarding the spectral properties of real neuronal RFs, the spans of inhibition and excitation over sound frequency tend to be similar (<xref ref-type="fig" rid="fig6">Figure 6f–h</xref>). This is also seen in the temporal prediction model, albeit with slightly more variation (<xref ref-type="fig" rid="fig6">Figure 6f–h</xref>). The sparse coding model shows more extensive variation in frequency spans than either the data or our model (<xref ref-type="fig" rid="fig6">Figure 6f–h</xref>).</p></sec><sec id="s2-6"><title>Quantitative analysis of visual results</title><p>We also compared the spatiotemporal RFs derived from the temporal prediction and sparse coding models with restricted published datasets summarizing RF characteristics of V1 neurons (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>) and a small number of full spatiotemporal visual RFs acquired from Ohzawa et al (<xref ref-type="bibr" rid="bib59">Ohzawa et al., 1996</xref>). We assessed the orientation and spatial frequency tuning properties of the models’ RFs by fitting Gabor functions to them (see Materials and methods).</p><p>We compared temporal properties of the RFs from the neural data and the temporal prediction model. In both cases, most power (mean over space and neurons of squared values) is in the most recent time steps (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). Previous normative models of spatiotemporal RFs (<xref ref-type="bibr" rid="bib86">van Hateren and Ruderman, 1998a</xref>; <xref ref-type="bibr" rid="bib38">Hyvärinen et al., 2003</xref>; <xref ref-type="bibr" rid="bib62">Olshausen, 2003</xref>) (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1c–d</xref>) do not show this property, being either invariant over time or localized, but with a symmetric profile that is not restricted to the recent past. We also measured the space-time separability of the RFs of the temporal prediction model (see Materials and methods); substantial numbers of both space-time separable and inseparable units were apparent (631 separable, 969 inseparable; <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). In addition to this, we measured the tilt direction index (TDI) of the model units from their 2D spatiotemporal RFs. This index indicates spatiotemporal asymmetry in space-time RFs and correlates with direction selectivity (<xref ref-type="bibr" rid="bib26">DeAngelis et al., 1993</xref>; <xref ref-type="bibr" rid="bib63">Pack et al., 2006</xref>; <xref ref-type="bibr" rid="bib4">Anzai et al., 2001</xref>; <xref ref-type="bibr" rid="bib7">Baker, 2001</xref>; <xref ref-type="bibr" rid="bib48">Livingstone and Conway, 2007</xref>). The mean TDI for the population was 0.34 (0.29 SD), comparable with the ranges in the neural data (mean 0.16; 0.12 SD in cat area 17/18 (<xref ref-type="bibr" rid="bib7">Baker, 2001</xref>), mean 0.51; 0.30 SD in macaque V1 [<xref ref-type="bibr" rid="bib48">Livingstone and Conway, 2007</xref>]). Finally, we observed an inverse correlation (r<sup>2</sup> = −0.33, p&lt;10<sup>−9</sup>, n = 1205) between temporal and spatial frequency tuning (See Materials and methods), which is also a property of real V1 RFs (<xref ref-type="bibr" rid="bib26">DeAngelis et al., 1993</xref>) and is seen in a sparse-coding-related model (<xref ref-type="bibr" rid="bib86">van Hateren and Ruderman, 1998a</xref>).</p><fig-group><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.31557.022</object-id><label>Figure 7.</label><caption><title>Population measures for real V1 and temporal prediction model visual spatial and spatiotemporal RFs.</title><p>Model units were obtained by training the model with 1600 hidden units on visual inputs. The hidden unit number and <italic>L</italic><sub>1</sub> weight regularization strength (10<sup>−6.25</sup>) was chosen because it results in the lowest MSE on the prediction task, as measured using a cross validation set. Example units in <xref ref-type="fig" rid="fig2">Figure 2</xref> come from this set. (<bold>a</bold>), Proportion of power (sum of squared weights over space and averaged across units) in each time step, for real (<xref ref-type="bibr" rid="bib59">Ohzawa et al., 1996</xref>) and model populations. (<bold>b</bold>), Joint distribution of spatial frequency and orientation tuning for population of model unit RFs at their time step with greatest power. (<bold>c</bold>), Distribution of orientation tuning for population of model unit RFs at their time step with greatest power. (<bold>d</bold>), Distribution of RF shapes for real neurons (cat, <xref ref-type="bibr" rid="bib39">Jones and Palmer, 1987</xref>, mouse, <xref ref-type="bibr" rid="bib55">Niell and Stryker, 2008</xref> and monkey, <xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>) and model units. <italic>n<sub>x</sub></italic> and <italic>n<sub>y</sub></italic> measure RF span parallel and orthogonal to orientation tuning, as a proportion of spatial oscillation period (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>). For (<bold>b–d</bold>), only units that could be well approximated by Gabor functions (n = 1205 units; see Materials and methods) were included in the analysis. Of these, only model units that were space-time separable (n = 473) are shown in (<bold>d</bold>) to be comparable with the neuronal data (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>). A further 4 units with 1.5 &lt; n<sub>y</sub> &lt; 3.1 are not shown in (<bold>d</bold>). <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplements 1</xref>–<xref ref-type="fig" rid="fig7s3">3</xref> show example visual RFs and the same population measures for the sparse coding model trained on visual inputs with added noise and for the temporal prediction and sparse coding models trained on visual inputs without added noise.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig7-v3"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.023</object-id><label>Figure 7—figure supplement 1.</label><caption><title>Visual RFs and population measures for real V1 neurons and sparse coding model units.</title><p>Model units are the same as those used in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>; <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>. (<bold>a</bold>), Example spatial RFs of randomly selected units at their best time step. (<bold>b–c</bold>), Example 3D and corresponding 2D spatiotemporal RFs at most recent six time steps of (<bold>b</bold>) (I, space-time separable, and II, space-time inseparable) real (<xref ref-type="bibr" rid="bib69">Rao and Ballard, 1999</xref>) V1 neurons and (<bold>c</bold>) (I-III, space-time separable, and IV-VI, space-time inseparable) sparse coding model units. (<bold>d</bold>), Proportion of power (sum of squared weights over space and averaged across units) in each time step, for real and model populations. (<bold>e</bold>), Joint distribution of spatial frequency and orientation tuning for population of model units. (<bold>f</bold>), Distribution of orientation tuning for population of model units. (<bold>f</bold>), Distribution of RF shapes for real neurons (cat, <xref ref-type="bibr" rid="bib39">Jones and Palmer, 1987</xref>, mouse, <xref ref-type="bibr" rid="bib55">Niell and Stryker, 2008</xref>, and monkey, <xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>) and model units. For (<bold>e–g</bold>), only units that could be well approximated by Gabor functions (n = 2402 units; see Materials and methods) were included in the analysis. Of these, only model units that were space-time separable (n = 881) are shown in (<bold>f</bold>) to be comparable with the neuronal data (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig7-figsupp1-v3"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.024</object-id><label>Figure 7—figure supplement 2.</label><caption><title>Visual RFs and population measures for real V1 neurons and temporal prediction model units trained on visual inputs without added noise.</title><p>Model units are drawn from <xref ref-type="fig" rid="fig4s6">Figure 4—figure supplement 6</xref>; <xref ref-type="fig" rid="fig4s7">Figure 4—figure supplement 7</xref> (<bold>a</bold>), Example spatial RFs of randomly selected units at their best time step. (<bold>b–c</bold>), Example 3D and corresponding 2D spatiotemporal RFs at most recent six time steps of (I, space-time separable, and II, space-time inseparable) real (<xref ref-type="bibr" rid="bib69">Rao and Ballard, 1999</xref>) V1 neurons and (<bold>c</bold>) (I-III, space-time separable, and IV-VI, space-time inseparable) sparse coding model units. (<bold>d</bold>), Proportion of power (sum of squared weights over space and averaged across units) in each time step, for real and model populations. (<bold>e</bold>), Joint distribution of spatial frequency and orientation tuning for population of model units. (<bold>f</bold>), Distribution of orientation tuning for population of model units. (<bold>g</bold>), Distribution of RF shapes for real neurons (cat, <xref ref-type="bibr" rid="bib39">Jones and Palmer, 1987</xref>, mouse, <xref ref-type="bibr" rid="bib55">Niell and Stryker, 2008</xref> and monkey, <xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>) and model units. For (<bold>e–g</bold>), only units that could be well approximated by Gabor functions (n = 1246 units; see Materials and methods) were included in the analysis. Of these, only model units that were space-time separable (n = 569) are shown in (g) to be comparable with the neuronal data (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>). The addition of noise only leads to subtle changes in the RFs; most apparently, there are more units with RFs comprising multiple short subfields (forming an increased number of points towards the lower right quadrant of (<bold>g</bold>) than is seen in the case when noise is used.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig7-figsupp2-v3"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.025</object-id><label>Figure 7—figure supplement 3.</label><caption><title>Visual RFs and population measures for real V1 neurons and sparse coding model units trained on visual inputs without added noise.</title><p>Model units are drawn from <xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>; <xref ref-type="fig" rid="fig5s5">Figure 5—figure supplement 5</xref>. (<bold>a</bold>), Example spatial RFs of randomly selected units at their best time step. (<bold>b–c</bold>), Example 3D and corresponding 2D spatiotemporal RFs at most recent six time steps of (<bold>b</bold>) (I, space-time separable, and II, space-time inseparable) real (<xref ref-type="bibr" rid="bib69">Rao and Ballard, 1999</xref>) V1 neurons and (<bold>c</bold>) (I-III, space-time separable, and IV-VI, space-time inseparable) sparse coding model units. (<bold>d</bold>), Proportion of power (sum of squared weights over space and averaged across units) in each time step, for real and model populations. (<bold>e</bold>), Joint distribution of spatial frequency and orientation tuning for population of model units. (<bold>f</bold>), Distribution of orientation tuning for population of model units. (<bold>g</bold>), Distribution of RF shapes for real neurons (cat,<xref ref-type="bibr" rid="bib39">Jones and Palmer, 1987</xref>, mouse, <xref ref-type="bibr" rid="bib55">Niell and Stryker, 2008</xref>, and monkey, <xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>) and model units. For (<bold>e–g</bold>), only units that could be well approximated by Gabor functions (n = 2482 units; see Materials and methods) were included in the analysis. Of these, only model units that were space-time separable (n = 860) are shown in (<bold>f</bold>) to be comparable with the neuronal data (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig7-figsupp3-v3"/></fig></fig-group><p>The spatial tuning characteristics of the temporal prediction model’s RFs displayed a wide range of orientation and spatial frequency preferences, consistent with the neural data (<xref ref-type="bibr" rid="bib26">DeAngelis et al., 1993</xref>; <xref ref-type="bibr" rid="bib46">Kreile et al., 2011</xref>) (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Both model and real RFs (<xref ref-type="bibr" rid="bib46">Kreile et al., 2011</xref>) show a preference for spatial orientations along the horizontal and vertical axes, although this orientation bias is seen to a greater extent in the temporal prediction model than in the data. The orientation and frequency tuning characteristics are also well captured by sparse coding related models of spatiotemporal RFs (<xref ref-type="bibr" rid="bib86">van Hateren and Ruderman, 1998a</xref>; <xref ref-type="bibr" rid="bib62">Olshausen, 2003</xref>) (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1e-f</xref>). Furthermore, the widths and lengths of the RFs of the temporal prediction model, relative to the period of their oscillation, also match the neural data well (<xref ref-type="fig" rid="fig7">Figure 7d</xref>). The distribution of units extends along a curve from blob-like RFs, which lie close to the origin in this plot, to stretched RFs with several subfields, which lie further from the origin. Although this property is again fairly well captured by previous models (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib10">Berkes et al., 2009</xref>; <xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>; <xref ref-type="bibr" rid="bib87">van Hateren and van der Schaaf, 1998b</xref>) (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1g</xref>), only the temporal prediction model seems to be able to capture the blob-like RFs that form a sizeable proportion of the neural data (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>) (<xref ref-type="fig" rid="fig7">Figure 7d</xref> where <italic>n</italic><sub>x</sub> and <italic>n</italic><sub>y</sub> &lt; ~0.25, <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). A small proportion of the population have RFs with several short subfields, forming a wing from the main curve in <xref ref-type="fig" rid="fig7">Figure 7d</xref>.</p></sec><sec id="s2-7"><title>Optimizing predictive capacity</title><p>Under our hypothesis of temporal prediction, we would expect that the better the temporal prediction model network is at predicting the future, the more the RFs of the network should resemble those of real neurons. To examine this hypothesis, we plotted the prediction error of the network as a function of two hyperparameters; the regularization strength and the number of hidden units (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). Then, we plotted the similarity between the auditory RFs of real A1 neurons and those of the temporal prediction model (<xref ref-type="fig" rid="fig8">Figure 8b</xref>), as measured by the mean KS distances of the temporal and frequency span distributions (<xref ref-type="fig" rid="fig6">Figure 6d–e,g–h</xref>, Materials and methods). The set of hyperparameter settings that give good predictions are also those where the temporal prediction model produces RFs that are most similar to those recorded in A1 (r<sup>2</sup> = 0.8, p&lt;10<sup>−9</sup>, n = 55). This result argues that cortical neurons are indeed optimized for temporal prediction.</p><fig-group><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.31557.026</object-id><label>Figure 8.</label><caption><title>Correspondence between the temporal prediction model’s ability to predict future auditory input and the similarity of its units’ responses to those of real A1 neurons.</title><p>Performance of model as a function of number of hidden units and <italic>L</italic><sub>1</sub> regularization strength on the weights as measured by (<bold>a</bold>), prediction error (mean squared error) on the validation set at the end of training and (<bold>b</bold>), similarity between model units and real A1 neurons. The similarity between the real and model units is measured by averaging the Kolmogorov-Smirnov distance between each of the real and model distributions for the span of temporal and frequency tuning of the excitatory and inhibitory RF subfields (e.g. the distributions in <xref ref-type="fig" rid="fig6">Figure 6d–e</xref> and <xref ref-type="fig" rid="fig6">Figure 6g–h</xref>). <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> shows the same analysis, performed for the sparse coding model, which does not produce a similar correspondence.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig8-v3"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.027</object-id><label>Figure 8—figure supplement 1.</label><caption><title>Correspondence between sparse coding model’s ability to reproduce its input and the similarity of its units’ responses to those of real A1 neurons.</title><p>Performance of model as a function of number of units and the <italic>L</italic><sub>1</sub> regularization strength on the activities as measured by (<bold>a</bold>), reconstruction error (mean squared error) on the validation set at the end of training and (<bold>b</bold>), similarity between model units and real A1 neurons. The similarity between the real and model units is measured by averaging the Kolmogorov-Smirnov distance between each of the real and model distributions for the span of temporal and frequency tuning of the excitatory and inhibitory RF subfields (e.g. the distributions in <xref ref-type="fig" rid="fig3">Figure 3d–e</xref> and <xref ref-type="fig" rid="fig3">Figure 3g–h</xref>).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig8-figsupp1-v3"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.028</object-id><label>Figure 8—figure supplement 2.</label><caption><title>Interactive figure exploring the relationship between the strength of <italic>L</italic><sub>1</sub> regularization on the network weights and the structure of the RFs the network produces when the network is trained on auditory inputs.</title></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig8-figsupp2-v3"/></fig><fig id="fig8s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31557.029</object-id><label>Figure 8—figure supplement 3.</label><caption><title>Interactive figure exploring the relationship between the strength of <italic>L</italic><sub>1</sub> regularization on the network weights and the structure of the RFs the network produces when the network is trained on visual inputs.</title></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31557-fig8-figsupp3-v3"/></fig></fig-group><p>When the similarity measure was examined as a function of the same hyperparameters for the sparse coding model (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>), and this was compared to that model’s stimulus reconstruction capacity as a function of the same hyperparameters, a monotonic relationship between stimulus reconstruction capacity and similarity of real RFs was not found (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>; r<sup>2</sup> = −0.05, p=0.69, n = 50). In previous studies in which comparisons have been made between normative models and real data, the model hyperparameters have been selected to maximize the similarity between the real and model RFs. In contrast, the temporal prediction model provides an independent criterion, the prediction error, to perform hyperparameter selection. To our knowledge, no such effective, measurable, independent criterion for hyperparameter selection has been proposed for other normative models of RFs.</p></sec><sec id="s2-8"><title>Variants of the temporal prediction model</title><p>The change in the qualitative structure of the RFs as a function of the number of hidden units and <italic>L</italic><sub>1</sub> regularization strength, for both the visual and auditory models, can be seen in the interactive supplementary figures (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplements 2</xref>–<xref ref-type="fig" rid="fig8s3">3</xref>; <ext-link ext-link-type="uri" xlink:href="https://yossing.github.io/temporal_prediction_model/figures/interactive_supplementary_figures.html">https://yossing.github.io/temporal_prediction_model/figures/interactive_supplementary_figures.html</ext-link>) The main effect of the regularization is to restrict the RFs in space for the visual case and in frequency and time for the auditory case. When the regularization is non-existent or substantially weaker than the optimum for prediction, the visual RFs become less localized in space with more elongated bars. The auditory RFs become more disordered, losing clear structure in most cases. When the regularization is made stronger than the optimum, the RFs become more punctate, for both the visual and auditory models. When the regularization strength is at the optimum for prediction, the auditory and visual model RFs qualitatively most closely resemble those of A1 neurons and V1 simple cells, respectively. This is consistent with what we found quantitatively in the previous section for the auditory model.</p><p>The temporal prediction model and the sparse coding model both produce oriented Gabor-like RFs when trained on visual inputs. This raises the possibility that optimization for prediction implicitly optimizes for a sparse response distribution, and hence leads to oriented RFs. To test for this, we measured the sparsity of the visual temporal prediction model’s hidden unit activities (by the Vinje-Gallant measure [<xref ref-type="bibr" rid="bib7">Baker, 2001</xref>]) in response to the natural image validation set. Examining the relationship between predictive capacity and sparsity, over the range of <italic>L</italic><sub>1</sub> weight regularization strength and hidden units explored, we did not find a clear monotonic relationship. Indeed, in both the auditory and visual cases, the hidden unit and <italic>L</italic><sub>1</sub> regularization combination with the best prediction had intermediate sparsity. For the visual case, the best-predicting model had sparsity 0.25, and other models within the grid search had sparsity ranging from 0.16 to 0.57. For the auditory case, the best-predicting model had sparsity 0.58, and other models had sparsity ranging from 0.42 to 0.69.</p><p>We also varied other characteristics of the temporal prediction model to understand their influence. For both the auditory and visual models, when a different hidden unit nonlinearity (tanh or rectified linear) was used, the networks had similar predictive capacity and produced comparable RFs. However, when the temporal prediction model had linear hidden units, it no longer predicted as well and produced RFs that were less like real neurons in their structure. For the auditory model, the linear model RFs generally became more narrowband in frequency with temporally extended excitation, instead of extended lagging inhibition (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>). For the visual model, the linear model RFs also showed substantially less similarity to the V1 data. At low regularization (the best predicting case), the RFs formed full-field grid-like structures. At higher regularization, they were more punctate, with some units having oriented RFs with short subfields. The RFs also did not change form or polarity over time, but simply decayed into the past.</p><p>The temporal prediction model and sparse coding model results shown in the main figures of this paper were trained on inputs with added Gaussian noise (6 dB SNR), mimicking inherent noise in the nervous system. To determine the effect of adding this noise, all models were also trained without noise, producing similar results (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplements 5</xref>–<xref ref-type="fig" rid="fig4s7">7</xref>; <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplements 3</xref>–<xref ref-type="fig" rid="fig5s5">5</xref>; <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>; <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplements 2</xref>–<xref ref-type="fig" rid="fig7s3">3</xref>). The results were also robust to changes in the duration of the temporal window being predicted. We trained the auditory model to predict a span of either 1, 3, 6, or 9 time steps into the future and the visual model to predict 1, 3 or 6 time steps into the future. For the auditory case, we found that increasing the number of time steps being predicted had little effect on the RF structure, both qualitatively and by the KS measure of similarity to the real data. In the visual case, Gabor-like units were present in all cases. Increasing the number of time steps made the RFs more restricted in space and increased the proportion of blob-like RFs.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We hypothesized that finding features that can efficiently predict future input from its past is a principle that influences the structure of sensory RFs. We implemented an artificial neural network model that instantiates a restricted version of this hypothesis. When this model was trained using natural sounds, it produced RFs that are both qualitatively and quantitatively similar to those of A1 neurons. Similarly, when we trained the model using natural movies it produced RFs with many of the properties of V1 simple cells. This similarity is particularly notable in the temporal domain; the model RFs have asymmetric envelopes, with a preference for the very recent past, as is seen in A1 and V1. Finally, the more accurate a temporal prediction model is at prediction, the more its RFs tend to be like real neuronal RFs by the measures we use for comparison.</p><sec id="s3-1"><title>Relationship to other models</title><p>A number of principles, often acting together, have been proposed to explain the form and diversity of sensory RFs. These include efficient coding (<xref ref-type="bibr" rid="bib8">Barlow, 1959</xref>; <xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib91">Zhao and Zhaoping, 2011</xref>; <xref ref-type="bibr" rid="bib82">Srinivasan et al., 1982</xref>; <xref ref-type="bibr" rid="bib17">Brito and Gerstner, 2016</xref>; <xref ref-type="bibr" rid="bib62">Olshausen, 2003</xref>; <xref ref-type="bibr" rid="bib6">Attneave, 1954</xref>), sparseness (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib45">Kozlov and Gentner, 2016</xref>; <xref ref-type="bibr" rid="bib17">Brito and Gerstner, 2016</xref>; <xref ref-type="bibr" rid="bib62">Olshausen, 2003</xref>), and slowness (<xref ref-type="bibr" rid="bib38">Hyvärinen et al., 2003</xref>; <xref ref-type="bibr" rid="bib18">Carlin and Elhilali, 2013</xref>). Efficient coding indicates that neurons should encode maximal information about sensory input given certain constraints, such as spike count or energy costs. Sparseness posits that only a small proportion of neurons in the population should be active for a given input. Finally, slowness means that neurons should be sensitive to features that change slowly over time. The temporal prediction principle we describe here provides another unsupervised objective of sensory coding. It has been described in a very general manner by the information bottleneck concept (<xref ref-type="bibr" rid="bib12">Bialek et al., 2001</xref>; <xref ref-type="bibr" rid="bib77">Salisbury and Palmer, 2016</xref>; <xref ref-type="bibr" rid="bib65">Palmer et al., 2015</xref>). We have instantiated a specific version of this idea, with linear-nonlinear encoding of the input, followed by a linear transform from the encoding units’ output to the prediction.</p><p>In the following discussion, we describe previous normative models that infer RFs with temporal structure from auditory or movie input and relate them to spectrotemporal RFs in A1 or simple cell spatiotemporal RFs in V1, respectively. For focus, other normative models of less directly relevant areas, such as spatial receptive fields without a temporal component (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>), complex cells (<xref ref-type="bibr" rid="bib11">Berkes and Wiskott, 2005</xref>), retinal receptive fields (<xref ref-type="bibr" rid="bib35">Huang and Rao, 2011</xref>; <xref ref-type="bibr" rid="bib82">Srinivasan et al., 1982</xref>), or auditory nerve impulse responses (<xref ref-type="bibr" rid="bib80">Smith and Lewicki, 2006</xref>), will not be examined.</p></sec><sec id="s3-2"><title>Auditory normative models</title><p>A number of coding objectives have been explored in normative models of A1 spectrotemporal RFs. One approach (<xref ref-type="bibr" rid="bib91">Zhao and Zhaoping, 2011</xref>) found analytically that the optimal typical spectrotemporal RF for efficient coding was spectrally localized with lagging and flanking inhibition, and showed an asymmetric temporal envelope. However, the resulting RF also showed substantially more flanking inhibition, more ringing over time and frequency, and operated over a much shorter timescale (~10 ms) than seen in A1 RFs (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Moreover, this approach produced a single generic RF, rather than capturing the diversity of the population.</p><p>Other models have produced a diverse range of spectrotemporal RFs. In the sparse coding approach (<xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib17">Brito and Gerstner, 2016</xref>; <xref ref-type="bibr" rid="bib52">Młynarski and McDermott, 2017</xref>; <xref ref-type="bibr" rid="bib16">Blättler et al., 2011</xref>), a spectrogram snippet is reconstructed from a sum of basis functions (a linear generative model), each weighted by its unit’s activity, with a constraint to have few active units. This approach is the same as the sparse coding model we used as a control (<xref ref-type="fig" rid="fig5">Figure 5</xref>). A challenge with many sparse generative models is that the activity of the units is found by a recurrent iterative process that needs to find a steady state; this is fine for static stimuli such as images, but for dynamic stimuli like sounds it is questionable whether the nervous system would have sufficient time to settle on appropriate activities before the stimulus had changed. Related work also used a sparsity objective, but rather than minimizing stimulus reconstruction error, forced high dispersal (<xref ref-type="bibr" rid="bib45">Kozlov and Gentner, 2016</xref>) or decorrelation (<xref ref-type="bibr" rid="bib44">Klein et al., 2003</xref>; <xref ref-type="bibr" rid="bib18">Carlin and Elhilali, 2013</xref>) of neural responses. Although lacking some of the useful probabilistic interpretations of sparse generative models, this approach does not require a settling process for inference. An alternative to sparseness is temporal slowness, which can be measured by temporal coherence (<xref ref-type="bibr" rid="bib18">Carlin and Elhilali, 2013</xref>). Here the linear transform from sequential spectrogram snippets to unit activity is optimized to maximize the correlation of each unit’s response over a certain time window, while maintaining decorrelation between the units’ activities.</p><p>Although the frequency tuning derived with these models can resemble that found in the midbrain or cortex (<xref ref-type="bibr" rid="bib44">Klein et al., 2003</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib45">Kozlov and Gentner, 2016</xref>; <xref ref-type="bibr" rid="bib18">Carlin and Elhilali, 2013</xref>; <xref ref-type="bibr" rid="bib17">Brito and Gerstner, 2016</xref>; <xref ref-type="bibr" rid="bib52">Młynarski and McDermott, 2017</xref>; <xref ref-type="bibr" rid="bib16">Blättler et al., 2011</xref>) (<xref ref-type="fig" rid="fig5">Figure 5</xref>), the resulting RFs lack the distinct asymmetric temporal profile and lagging inhibition seen in real midbrain or A1 RFs. Furthermore, they often have envelopes that are too elongated over time, often spanning the full temporal width of the spectrotemporal RF. This is related to the fact that the time window to be encoded by the model is set arbitrarily, and every time point within that window is given equal importance, that is, the direction of time is not accounted for. This is in contrast to the temporal prediction model, which naturally gives greater weighting to time-points near the present than to those in the past due to their greater predictive capacity.</p></sec><sec id="s3-3"><title>Visual normative models</title><p>The earliest normative model of spatiotemporal RFs of simple cells used independent component analysis (ICA) (<xref ref-type="bibr" rid="bib86">van Hateren and Ruderman, 1998a</xref>), which is practically equivalent for visual or auditory data to the critically complete case of the sparse coding model (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>) we used as a control (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s2">2</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). The RFs produced by this model and the control model reproduced fairly well the spatial aspects of simple cell RFs. However, in contrast to the temporal prediction model <xref ref-type="fig" rid="fig7">(Figure 7d</xref>), the subset of more ‘blob-like’ RFs seen in the data are not well captured by our control sparse coding model (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1g</xref>). In the temporal domain, again unlike the temporal prediction model and real V1 simple cells, the RFs of the ICA and sparse coding models are not pressed up against the present with an asymmetrical temporal envelope, but instead show a symmetrical envelope or span the entire range of times examined. A related model (<xref ref-type="bibr" rid="bib62">Olshausen, 2003</xref>) assumes that a longer sequence of frames is generated by convolving each basis function with a time-varying sparse coefficient and summing the result, so that each basis function is applied at each point in time. The resulting spatiotemporal RFs are similar to those produced by ICA (<xref ref-type="bibr" rid="bib86">van Hateren and Ruderman, 1998a</xref>), or our control model (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1c</xref>). Although they tend not to span the entire range of times examined, they do show a symmetrical envelope, and require an iterative inference procedure, as described above for audition.</p><p>Temporal slowness constraints have also been used to model the spatiotemporal RFs of simple cells. The bubbles (<xref ref-type="bibr" rid="bib38">Hyvärinen et al., 2003</xref>) approach combines sparse and temporal coherence constraints with reconstruction. The resulting RFs show similar spatial and temporal properties to those found using ICA. A related framework is slow feature analysis (SFA) (<xref ref-type="bibr" rid="bib11">Berkes and Wiskott, 2005</xref>; <xref ref-type="bibr" rid="bib89">Wiskott and Sejnowski, 2002</xref>), which enforces temporal smoothness by minimizing the derivative of unit responses over time, while maximizing decorrelation between units. SFA has been used to model complex cell spatiotemporal RFs (over only two time steps, <xref ref-type="bibr" rid="bib11">Berkes and Wiskott, 2005</xref>), and a modified version has been used to model spatial (not spatiotemporal) RFs of simple cells (<xref ref-type="bibr" rid="bib10">Berkes et al., 2009</xref>). These results are not directly comparable with our results or the spatiotemporal RFs of simple cells.</p><p>In the slowness framework, the features found are those that persist over time; the presence of such a feature in the recent past predicts that the same feature will be present in the near future. This is also the case for our predictive approach, which, additionally, can capture features in the past that predict features in the future that are subtly or radically different from themselves. The temporal prediction principle will also give different weighting to features, as it values predictive capacity rather than temporal slowness (<xref ref-type="bibr" rid="bib22">Creutzig and Sprekeler, 2008</xref>). In addition, although slowness models can be extended to model RFs over more than one time step (<xref ref-type="bibr" rid="bib11">Berkes and Wiskott, 2005</xref>; <xref ref-type="bibr" rid="bib38">Hyvärinen et al., 2003</xref>; <xref ref-type="bibr" rid="bib18">Carlin and Elhilali, 2013</xref>), capturing temporal structure, they do not inherently give more weighting to information in the most recent past and therefore do not give rise to asymmetric temporal profiles in RFs.</p><p>There is one study that has directly examined temporal prediction as an objective for visual RFs in a manner similar to ours (<xref ref-type="bibr" rid="bib64">Palm, 2012</xref>). Here, as in our model, a single hidden layer feedforward neural network was used to predict the immediate future frame of a movie patch from its past frames. However, only two frames of the past were used in this study, so a detailed exploration of the temporal profile of the spatiotemporal RFs was not possible. Nevertheless, some similarities and differences in the spatial RFs between the two frames were noted, and some units had oriented RFs. In contrast to our model, however, many RFs were noisy and did not resemble those of simple cells. Potential reasons for this difference include the use of <italic>L</italic><sub>2</sub> rather than <italic>L</italic><sub>1</sub> regularization on the weights, an output nonlinearity not present in our model, the optimization algorithm used, network size, or the dataset. Another very recent related study (<xref ref-type="bibr" rid="bib20">Chalk et al., 2018</xref>) also implemented a somewhat different form of temporal prediction, with a linear (rather than linear-nonlinear) encoder, and linear decoder. When applied to visual scenes, oriented receptive fields were produced, but they were spatio-temporally separable and hence not direction selective.</p></sec><sec id="s3-4"><title>Strengths and limitations of the temporal prediction model</title><p>Temporal prediction has several strengths as an objective function for sensory processing. First, it can capture underlying features in the world (<xref ref-type="bibr" rid="bib12">Bialek et al., 2001</xref>); this is also the case with sparseness (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>) and slowness (<xref ref-type="bibr" rid="bib89">Wiskott and Sejnowski, 2002</xref>), but temporal prediction will prioritize different features. Second, it can predict future inputs, which is very important for guiding action, especially given internal processing delays. Third, objectives such as efficient or sparse reconstruction retain everything about the stimulus, whereas an important part of neural information processing is the selective elimination of irrelevant information (<xref ref-type="bibr" rid="bib51">Marzen and DeDeo, 2017</xref>). Prediction provides a good initial criterion for eliminating potentially unwanted information. Fourth, prediction provides a natural method to determine the hyperparameters of the model (such as regularization strength, number of hidden units, activation function and temporal window size). Other models select their hyperparameters depending on what best reproduces the neural data, whereas we have an independent criterion – the capacity of the network to predict the future. One notable hyperparameter is how many time-steps of past input to encode. As described above, this is naturally decided by our model because only time-steps that help predict the future have significant weighting. Fifth, the temporal prediction model computes neuronal activity without needing to settle to a steady state, unlike some other models (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>; <xref ref-type="bibr" rid="bib17">Brito and Gerstner, 2016</xref>; <xref ref-type="bibr" rid="bib52">Młynarski and McDermott, 2017</xref>). For dynamic stimuli, a model that requires settling may not reach equilibrium in time to be useful. Sixth, and most importantly, temporal prediction successfully models many aspects of the RFs of primary cortical neurons. In addition to accounting for spatial and spectral tuning in V1 and A1, respectively, at least as well as other normative models, it reproduces the temporal properties of RFs, particularly the asymmetry of the envelopes of RFs, something few previous models have attempted to explain.</p><p>Although the temporal prediction model’s ability to describe neuronal RFs is high, the match with real neurons is not perfect. For example, the span of frequency tuning of our modelled auditory RFs is narrower than in A1 (<xref ref-type="fig" rid="fig6">Figure 6g–h</xref>). We also found an overrepresentation of vertical and horizontal orientations compared to real V1 data (<xref ref-type="fig" rid="fig7">Figure 7b–c</xref>). Some of these differences could be a consequence of the data used to train the model. Although the statistics of natural stimuli are broadly conserved (<xref ref-type="bibr" rid="bib29">Field, 1987</xref>), there is still variation (<xref ref-type="bibr" rid="bib85">Torralba and Oliva, 2003</xref>), and the dataset used to train the network may not match the sensory world of the animal experienced during development and over the course of evolution. In future work, it would be valuable to explore the influence of natural datasets with different statistics, and also to match those datasets more precisely to the evolutionary context and individual experience of the animals examined. Furthermore, a comparison of the model with neural data from different species, at different ages, and reared in different environments would be useful.</p><p>Another cause of differences between the model and neural RFs may be the recording location of the RFs and how they are characterized. We used the primary sensory cortices as regions for comparison, because we performed transformations on the input data that are similar to the preprocessing that takes place in afferent subcortical structures. We spatially filtered the visual data in a similar way to the retina (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>), and spectrally decomposed the auditory data as in the inner ear, and then used time bins (5 ms) which are coarser than, but close to, the maximum amplitude modulation period that can be tracked by auditory midbrain neurons (<xref ref-type="bibr" rid="bib71">Rees and Møller, 1983</xref>). However, primary cortex is not a homogenous structure, with neurons in different layers displaying certain differences in their response properties (<xref ref-type="bibr" rid="bib32">Harris and Mrsic-Flogel, 2013</xref>). Furthermore, the methods by which neurons are sampled from the cortex may not provide a representative sample. For example, multi-electrode arrays tend to favour larger and more active neurons. In addition, the method and stimuli used to construct RFs from the data can bias their structure somewhat (<xref ref-type="bibr" rid="bib88">Willmore et al., 2016</xref>).</p><p>The model presented here is based on a simple feedforward network with one layer of hidden units. This limits its ability to predict features of the future input, and to account for RFs with nonlinear tuning. More complex networks, with additional layers or recurrency may allow the model to account for more complex tuning properties, including those found beyond the primary sensory cortices. Careful, principled adjustment of the preprocessing, or different regularization methods (such as sparseness or slowness applied to the units’ activities), may also help. There is an open question as to whether the current model may eliminate some information that is useful for reconstruction of the past input or for prediction of higher order statistical properties of the future input, which might bring it into conflict with the principle of least commitment (<xref ref-type="bibr" rid="bib50">Marr, 1976</xref>). It is an empirical question how much organisms preserve information that is not predictive of the future, although there are theoretical arguments against such preservation (<xref ref-type="bibr" rid="bib12">Bialek et al., 2001</xref>). Such conflict might be remedied, and the model improved, by adding feedback from higher areas or by adding an objective to reconstruct the past or present (<xref ref-type="bibr" rid="bib8">Barlow, 1959</xref>; <xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib6">Attneave, 1954</xref>) in addition to predicting the future.</p><p>To determine whether the model could help explain neuronal responses in higher areas, it would be useful to develop a hierarchical version of the temporal prediction model, applying the same model again to the activity of the hidden units rather than to the input. Another useful extension would be to see if the features learnt by the temporal prediction model could be used to accelerate learning of useful tasks such as speech or object recognition, by providing input or initialization for a supervised or reinforcement learning network. Indeed, temporal predictive principles have been shown to be useful for unsupervised training of networks used in visual object recognition (<xref ref-type="bibr" rid="bib83">Srivastava et al., 2015</xref>; <xref ref-type="bibr" rid="bib67">Ranzato, 2016</xref>; <xref ref-type="bibr" rid="bib49">Lotter et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Oh et al., 2015</xref>).</p><p>Finally, it is interesting to consider possible more explicit biological bases for our model. We envisage the input units of the model as thalamic input, and the hidden units as primary cortical neurons. Although the function of the output units could be seen as just a method to optimize the hidden units to find the most predictive code given sensory input statistics, they may also have a physiological analogue. Current evidence (<xref ref-type="bibr" rid="bib25">Dahmen and King, 2007</xref>; <xref ref-type="bibr" rid="bib37">Huberman et al., 2008</xref>; <xref ref-type="bibr" rid="bib43">Kiorpes, 2015</xref>) suggests that while primary cortical RFs are to an extent hard-wired in form by natural selection, their tuning is also refined by individual sensory experience. This refinement process may require a predictive learning mechanism in the animal’s brain, at least at some stage of development and perhaps also into adulthood. Hence, one might expect to find a subpopulation of neurons that represent the prediction (analogous to the output units of the model) or the prediction error (analogous to the difference between the output unit activity and the target). Indeed, signals relating to sensory prediction error have been found in A1 (<xref ref-type="bibr" rid="bib74">Rubin et al., 2016</xref>), though they may also be located in other regions of the brain. Finally, it is important to note that, although the biological plausibility of backpropagation has long been questioned, recent progress has been made in developing trainable networks that perform similarly to artificial neural networks trained with backpropagation, but with more biologically plausible characteristics (<xref ref-type="bibr" rid="bib9">Bengio et al., 2015</xref>), for example, by having spikes or avoiding the weight transport problem (<xref ref-type="bibr" rid="bib47">Lillicrap et al., 2016</xref>).</p></sec><sec id="s3-5"><title>Conclusion</title><p>We have shown that a simple principle - predicting the imminent future of a sensory scene from its recent past - explains many features of the RFs of neurons in both primary visual and auditory cortex. This principle may also account for neural tuning in other sensory systems, and may prove useful for the study of higher sensory processing and aspects of neural development and learning. While the importance of temporal prediction is increasingly widely recognized, it is perhaps surprising nonetheless that many basic tuning properties of sensory neurons, which we have known about for decades, appear, in fact, to be a direct consequence of the brain’s need to efficiently predict what will happen next.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Data used for model training and testing</title><sec id="s4-1-1"><title>Visual inputs</title><p>Videos (without sound, sampled at 25 fps) of wildlife in natural settings were used to create visual stimuli for training the artificial neural network. The videos were obtained from <ext-link ext-link-type="uri" xlink:href="http://www.arkive.org/species">http://www.arkive.org/species</ext-link>, contributed by: BBC Natural History Unit, <ext-link ext-link-type="uri" xlink:href="http://www.gettyimages.co.uk/footage/bbcmotiongallery">http://www.gettyimages.co.uk/footage/bbcmotiongallery</ext-link>; BBC Natural History Unit and Discovery Communications Inc., <ext-link ext-link-type="uri" xlink:href="http://www.bbcmotiongallery.com">http://www.bbcmotiongallery.com</ext-link>; Granada Wild, <ext-link ext-link-type="uri" xlink:href="http://www.itnsource.com">http://www.itnsource.com</ext-link>; Mark Deeble and Victoria Stone, Flat Dog Productions Ltd., <ext-link ext-link-type="uri" xlink:href="http://www.deeblestone.com">http://www.deeblestone.com</ext-link>; Getty Images, <ext-link ext-link-type="uri" xlink:href="http://www.gettyimages.com">http://www.gettyimages.com</ext-link>; National Geographic Digital Motion, <ext-link ext-link-type="uri" xlink:href="http://www.ngdigitalmotion.com">http://www.ngdigitalmotion.com</ext-link>. The longest dimension of each video frame was clipped to form a square image. Each frame was then band-pass filtered (<xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>) and downsampled (using bilinear interpolation) over space, to provide 180 × 180 pixel frames. Non-overlapping patches of 20 × 20 pixels were selected from a fixed region in the centre of the frames, where there tended to be visual motion. The video patches were cut into sequential overlapping clips each of 8 frames duration. Thus, each training example (clip) was made up of a 20 × 20 pixel section of the video with a duration of 8 frames (320 ms), providing a training set of <italic>N</italic> =~500,000 clips from around 5.5 hr of video, and a validation set of <italic>N</italic> =~100,000 clips. Finally, the training and validation sets were normalized by subtracting the mean and dividing by the standard deviation (over all pixels, frames and clips in the training set). The goal of the neural network was to predict the final frame (the ‘future’) of each clip from the first seven frames (the ‘past’).</p></sec><sec id="s4-1-2"><title>Auditory inputs</title><p>Auditory stimuli were compiled from databases of human speech (~60%), animal vocalizations (~20%) and sounds from inanimate objects found in natural settings (e.g. running water, rustling leaves; ~20%). Stimuli were recorded using a Zoom H4 or collected from online sources. Natural sounds were obtained from <ext-link ext-link-type="uri" xlink:href="http://www.freesound.org">www.freesound.org</ext-link>, contributed by users sedi, higginsdj, jult, kvgarlic, xenognosis, zabuhailo, funnyman374, videog, j-zazvurek, samueljustice00, gfrog, ikbenraar, felix-blume, orbitalchiller, saint-sinner, carlvus, vflefevre, hitrison, willstepp, timbahrij, xdimebagx, r-nd0mm3m, the-yura, rsilveira-88, stomachache, foongaz, edufigg, yurkobb, sandermotions, darius-kedros, freesoundjon-01, dwightsabeast, borralbi, acclivity, J.Zazvurek, Zabuhailo, soundmary, Darius Kedros, Kyster, urupin, RSilveira and freelibras. Human speech sounds were obtained from http://databases.forensic-voice-comparison.net/ (<xref ref-type="bibr" rid="bib54">Morrison et al., 2015</xref>, <xref ref-type="bibr" rid="bib53">Morrison et al., 2012</xref>).</p><p>Each sound was sampled at (or resampled to) 44.1 kHz and converted into a simple ‘cochleagram’, to make it more analogous to the activity pattern that would be passed to the auditory pathway after processing by the cochlea. To calculate the cochleagram, a power spectrogram was computed using 10 ms Hamming windows, overlapping by 5 ms (giving time steps of 5 ms). The power across neighbouring Fourier frequency components was then aggregated into 32 frequency channels using triangular windows with a base width of 1/3 octave whose centre frequencies ranged from 500 to 17,827 Hz (1/6<sup>th</sup> octave spacing, using code adapted from melbank.m, <ext-link ext-link-type="uri" xlink:href="http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html">http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html</ext-link>). The cochleagrams were then decomposed into sequential overlapping clips, each of 43 time steps (415 ms) in duration, providing a training set of ~1,000,000 clips (~1.3 hr of audio) and a validation set of ~200,000 clips. To approximately model the intensity compression seen in the auditory nerve (<xref ref-type="bibr" rid="bib75">Sachs and Abbas, 1974</xref>), each frequency band in the stimulus set was divided by the median value in that frequency band over the training set, and passed through a hill function, defined as <italic><inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> with <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.02. Finally, the training and cross-validation sets were normalized by subtracting the mean and dividing by the standard deviation over all time steps, frequency bands and clips in the training set. The first 40 time steps (200 ms) of each clip (the ‘past’) were used as inputs to the neural network, whose aim was to predict the content (the ‘future’) of the remaining three time steps (15 ms).</p></sec><sec id="s4-1-3"><title>Addition of Gaussian noise</title><p>To replicate the effect of noise found in the nervous system, Gaussian noise was added to both the auditory and visual inputs with a signal-to-noise ratio (SNR) of 6 dB. While the addition of noise did not make substantial differences to the RFs of units trained on visual inputs, this improved the similarity to the data when the model was trained on auditory inputs. The results from training the network on inputs without added noise are shown for auditory inputs in <xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> and for visual inputs in <xref ref-type="fig" rid="fig4s6">Figure 4—figure supplements 6</xref>–<xref ref-type="fig" rid="fig4s7">7</xref> and <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>. The results from the sparse coding model were similar in both cases for inputs with and without noise (<xref ref-type="fig" rid="fig5">Figures 5</xref>–<xref ref-type="fig" rid="fig6">6</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s5">5</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplements 1</xref> and <xref ref-type="fig" rid="fig7s3">3</xref>).</p></sec></sec><sec id="s4-2"><title>Temporal prediction model</title><sec id="s4-2-1"><title>The model and cost function</title><p>The temporal prediction model was implemented using a standard fully connected feed-forward neural network with one hidden layer. Each hidden unit in the network computed the linear weighted sum of inputs, and its output was determined by passing this sum through a monotonic nonlinearity. This nonlinearity <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> was either a logistic function <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> or a similar nonlinear function (such as <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>tanh</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). For results reported here, we used the logistic function, though obtained similar results when we trained the model using <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. For comparison, we also trained the model replacing the nonlinearity with a linear function, where <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. In this case, we found that the RFs tended to be punctate in space or frequency and did not typically show the alternating excitation and inhibition over time that is characteristic real neurons in A1 and V1.</p><p>Formally, for a network with <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> input variables, <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> output units and a single layer of <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> hidden units, the output <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of hidden unit <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for clip <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The value <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of input variable <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for clip <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is simply the value for a particular pixel and time step (frame) of the ‘past’ in preprocessed visual clip <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 20 pixels × 20 pixels × 7 time steps = 2800), or the value for a particular frequency band and time step of the ‘past’ of cochleagram clip <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 32 frequencies × 40 time steps = 1280). Hence, the index <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> spans over several frequencies or pixels and also over time steps into the past. The subscript <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> has been dropped for clarity in the figures (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The parameters in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> are the connective input weights <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (between each input variable <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and hidden unit <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), and the bias <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (of hidden unit <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>The activity <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of each output unit <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is the estimate of the true future <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> given the past <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, is given by:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The parameters in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> are the connective output weights <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (between each hidden unit <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and output unit <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and the bias <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (of output unit <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). The activity <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of output unit <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for clip <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the estimate for a particular pixel of the ‘future’ in the visual case (<inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 20 pixels × 20 pixels × 1 time step = 400), or the value for a particular frequency band and time step of the ‘future’ in the auditory case (<inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 32 frequencies × 3 time steps = 96).</p><p>The parameters <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> were optimized for the training set by minimizing the cost function given by:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus, <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean squared error (the prediction error) between the prediction <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the target <inline-formula><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> over all <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> training examples and <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> target variables, plus an <italic>L</italic><sub>1</sub> regularization term, which is proportional to the sum of absolute values of all weights in the network and its strength is determined by the hyper-parameter <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This regularization tends to drive redundant weights to near zero and provides a parsimonious network.</p></sec><sec id="s4-2-2"><title>Implementation details</title><p>The networks were implemented in Python (<ext-link ext-link-type="uri" xlink:href="https://lasagne.readthedocs.io/en/latest/">https://lasagne.readthedocs.io/en/latest/</ext-link>; <ext-link ext-link-type="uri" xlink:href="http://deeplearning.net/software/theano/">http://deeplearning.net/software/theano/</ext-link>). The objective function was minimized using backpropagation as performed by the Adam optimization method (<xref ref-type="bibr" rid="bib42">Kingma and Adam, 2014</xref>). An alternative implementation of the model was also made in MATLAB using the Sum-of-Functions Optimizer (<xref ref-type="bibr" rid="bib81">Sohl-Dickstein et al., 2014</xref>) (<ext-link ext-link-type="uri" xlink:href="https://github.com/Sohl-Dickstein/Sum-of-Functions-Optimizer">https://github.com/Sohl-Dickstein/Sum-of-Functions-Optimizer</ext-link>) to train the network using backpropagation. Training examples were split into minibatches of approximately 200 training examples each.</p><p>During model network training, several hyperparameters were varied, including the regularization strength (<inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), the number of units in the hidden layer and the nonlinearity used by each hidden unit. For each hyperparameter setting, the training algorithm was run for 1000 iterations. Running the network for longer (10000 iterations) showed negligible improvement to the prediction error (as measured on the validation set) or change in RF structure.</p><p>The effect of varying the number of hidden units and <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on the prediction error for the validation set is shown in <xref ref-type="fig" rid="fig8">Figure 8</xref>. In both the visual and auditory case, the results presented (<xref ref-type="fig" rid="fig2">Figure 2</xref>,<xref ref-type="fig" rid="fig4">4</xref>,<xref ref-type="fig" rid="fig6">6</xref>,<xref ref-type="fig" rid="fig7">7</xref> and supplements) are the networks that predicted best on the validation set after 1000 iterations through the training data. For the auditory case, the settings that resulted in the best prediction were 1600 hidden units and <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 10<sup>−3.5</sup>, while in the visual case, the optimal settings were 1600 hidden units and <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 10<sup>−6.25</sup>.</p></sec><sec id="s4-2-3"><title>Model receptive fields</title><p>In the model, the combination of linear weights and nonlinear activation function are similar to the basic linear non-linear (LN) model (<xref ref-type="bibr" rid="bib78">Simoncelli et al., 2004</xref>; <xref ref-type="bibr" rid="bib24">Dahmen et al., 2008</xref>; <xref ref-type="bibr" rid="bib5">Atencio et al., 2008</xref>; <xref ref-type="bibr" rid="bib21">Chichilnisky, 2001</xref>; <xref ref-type="bibr" rid="bib66">Rabinowitz et al., 2011</xref>) commonly used to describe neural RFs. Hence, the input weights between the input layer and a hidden unit of the model network are taken directly to represent the unit’s RF, indicating the features of the input that are important to that unit.</p><p>Because of the symmetric nature of the sigmoid function, <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, after appropriate modification of the biases a hidden unit has the same influence on the prediction if its input and output matrices are both multiplied by −1. That is, for unit <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, if we convert <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, this will have no effect on the prediction or the cost function. This can be done independently for each hidden unit. Hence, the sign of each unit’s RF could equally be positive or negative and have the same result on the predictions given by the network. However, we know that auditory units always have leading excitation (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Hence, for both the predictive model and for the sparse coding model, we assume leading excitation for each unit. This was done for all auditory analyses.</p><p>As more units are added to the model network, the number of inactive units increases. To account for this, we measured the relative strength of all input connections to each hidden unit by summing the square of all input weights for that unit. Units for which the sum of square input weights was &lt;1% of the maximum strength for the population were deemed to be inactive and excluded from all subsequent analyses. The difference in connection strength between active and inactive units was very distinct; a threshold &lt;0.0001% only marginally increases the number of active units.</p></sec></sec><sec id="s4-3"><title>Sparse coding model</title><p>The sparse coding model was used as a control for both visual and auditory cases. The Python implementation of this model (<ext-link ext-link-type="uri" xlink:href="https://github.com/zayd/sparsenet">https://github.com/zayd/sparsenet</ext-link>) was trained using the same visual and auditory inputs used to train the predictive model. The training data were divided into mini-batches which were shuffled and the model optimized for one full pass through the data. Inference was performed using the Fast Iterative Shrinkage and Thresholding (FISTA) algorithm. A sparse <italic>L</italic><sub>1</sub> prior with strength <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> was applied to the unit activities, providing activity regularization. A range of <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-values and unit numbers were tried (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). The learning rate and batch size were also varied until reasonable values were found. As there was no independent criterion by which to determine the ‘best’ settings, we chose the network that produced basis functions whose receptive fields were most similar to those of real neurons. In the auditory case, this was determined using the mean KS measure of similarity (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). In the visual case, as a similarity measure was not performed, this was done by inspection. In both cases, the model configurations chosen were restricted to those trained in an overcomplete condition (having more units than the number of input variables) in order to remain consistent with previous instantiations of this model (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib19">Carlson et al., 2012</xref>). In this manner, we selected a sparse coding network with 1600 units, <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 10<sup>0.5</sup>, learning rate = 0.01 and 100 mini-batches in the auditory case (<xref ref-type="fig" rid="fig5">Figures 5</xref>–<xref ref-type="fig" rid="fig6">6</xref>). In the visual case, the network selected was trained with 3200 units, <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 10<sup>0.5</sup>, learning rate = 0.05 and 100 mini-batches (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s2">2</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). Although the sparse coding basis functions are projective fields, they tend to be similar in structure to receptive fields (<xref ref-type="bibr" rid="bib60">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib61">Olshausen and Field, 1997</xref>), and, for simplicity, are referred to as RFs.</p></sec><sec id="s4-4"><title>Auditory receptive field analysis</title><sec id="s4-4-1"><title>In vivo A1 RF data</title><p>Auditory RFs of neurons were recorded in the primary auditory cortex (A1) and anterior auditory field (AAF) of 5 pigmented ferrets of both sexes (all &gt;6 months of age) and used as a basis for comparison with the RFs of model units trained on auditory stimuli. Systematic differences in response properties of A1 and AAF neurons are minor and not relevant for this study, and for simplicity here, we refer to neurons from either primary field indiscriminately as ‘A1 neurons’. These recordings were performed under license from the UK Home Office and were approved by the University of Oxford Committee on Animal Care and Ethical Review. Full details of the recording methods are described in earlier studies (<xref ref-type="bibr" rid="bib88">Willmore et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Bizley et al., 2009</xref>). Briefly, we induced general anaesthesia with a single intramuscular dose of medetomidine (0.022 mg · kg<sup>−1</sup> · h<sup>−1</sup>) and ketamine (5 mg · kg<sup>−1</sup> · h<sup>−1</sup>), which was then maintained with a continuous intravenous infusion of medetomidine and ketamine in saline. Oxygen was supplemented with a ventilator, and we monitored vital signs (body temperature, end-tidal CO<sub>2</sub>, and the electrocardiogram) throughout the experiment. The temporal muscles were retracted, a head holder was secured to the skull surface, and a craniotomy and a durotomy were made over the auditory cortex. Extracellular recordings were made using silicon probe electrodes (Neuronexus Technologies) and acoustic stimuli were presented via Panasonic RPHV27 earphones, which were coupled to otoscope specula that were inserted into each ear canal, and driven by Tucker-Davis Technologies System III hardware (48 kHz sample rate).</p><p>The neuronal recordings used the ‘BigNat’ stimulus set (<xref ref-type="bibr" rid="bib88">Willmore et al., 2016</xref>), which consists of natural sounds including animal vocalizations (e.g., ferrets and birds), environmental sounds (e.g., water and wind), and speech. To identify those neural units that were driven by the stimuli, we calculated a ‘noise ratio’ statistic (<xref ref-type="bibr" rid="bib66">Rabinowitz et al., 2011</xref>; <xref ref-type="bibr" rid="bib76">Sahani and Linden, 2003</xref>) for each unit and excluded from further analysis any units with a noise ratio &gt;40. In total, driven spiking responses of 114 units (75 single unit, 39 multi-unit) were recorded to this stimulus set. Then, the auditory (spectrotemporal) RF of each unit was constructed using a previously described method (<xref ref-type="bibr" rid="bib88">Willmore et al., 2016</xref>). Briefly, linear regression was performed in order to minimize the squared error between each neuron’s spiking response over time and the cochleagram of the stimuli that gave rise to that response. The method used was exactly the same as in our earlier study (<xref ref-type="bibr" rid="bib88">Willmore et al., 2016</xref>), except that <italic>L</italic><sub>1</sub> rather than <italic>L</italic><sub>2</sub> regularization was used to constrain the regression. The spectrotemporal RFs of these neurons took the same form as the inputs to the model neural network (i.e., 32 frequencies and 40 time-steps over the same range of values) and were therefore comparable to the model units’ RFs. In order to account for the latency of auditory cortical responses, the most recent two time-steps (10 ms) of the neuronal RFs were removed, leaving 38 time-steps.</p></sec><sec id="s4-4-2"><title>Multi-dimensional scaling (MDS)</title><p>To get a non-parametric indication of how similar the model units’ RFs were to those of real A1 neurons, each RF was embedded into a 2-dimensional space using MDS (<xref ref-type="fig" rid="fig6">Figure 6a</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1a</xref>). First, 100 units each from the temporal prediction and sparse coding models and from the real population were chosen at random. To ensure that the model RFs were of the same dimensionality as the real RFs prior to embedding, the least recent two time steps of each model RF were removed.</p></sec><sec id="s4-4-3"><title>Measuring temporal and frequency spans of RFs</title><p>We quantified the span, over time and frequency, of the excitatory and inhibitory subfields of each RF. To do this, each RF was first separated into excitatory and inhibitory subfields, where the excitatory subfield was the RF with negative values set to 0, and the inhibitory subfield the RF with positive values set to 0. In some cases, model units did not exhibit notable inhibitory subfields. To account for this, the power contained in each subfield was calculated (sum of the squares of the subfield). Inhibitory subfields with &lt;5% of the power of that unit’s excitatory subfield were excluded from further analysis. According to this criterion, 44 of 167 active units in the temporal prediction model and 193 of 1600 units in the sparse model did not display inhibition.</p><p>Singular value decomposition (SVD) was performed on each subfield separately, and the first pair of singular vectors was taken, one of which is over time, the other over frequency. For the excitatory subfield, the temporal span was measured as the proportion of values in the temporal singular vector that exceeded 50% of the maximum value in the vector. The same analysis provided the temporal span for the inhibitory subfield. Similarly, we measured the frequency spans of the RFs by applying this measure to the frequency singular vectors of the excitatory and the inhibitory subfields.</p><p>We also examined, for both real and model RFs, the mean power for each of the 38 time steps in the RFs (<xref ref-type="fig" rid="fig6">Figure 6b</xref>), which was calculated as the mean of the squared RF values, over all frequencies and RFs, at each time step.</p></sec><sec id="s4-4-4"><title>Mean KS measure</title><p>To compare each network’s units with those recorded in A1 (<xref ref-type="fig" rid="fig3">Figure 3</xref>), the two-sample Kolmogorov-Smirnov (KS) distance between the real and model distribution was measured for both the temporal and spectral span of the excitatory and inhibitory subfields (e.g. the distributions in <xref ref-type="fig" rid="fig6">Figure 6d–e</xref> and <xref ref-type="fig" rid="fig6">Figure 6g–h</xref>). These four KS measures were then averaged to give a single mean KS measure for each network, indicating how closely the temporal and frequency characteristics of real and model units matched on average for that network. The KS measure is low for similar distributions and high for distributions that diverge greatly. Thus networks whose units display temporal and frequency tuning characteristics that match those of real neurons more closely give rise to a lower mean KS measure.</p></sec></sec><sec id="s4-5"><title>Visual receptive field analysis</title><sec id="s4-5-1"><title>In vivo V1 RF data</title><p>Visual RFs measured using recordings from V1 simple cells were compared against the model (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, and <xref ref-type="fig" rid="fig7">Figure 7a</xref>, cat, <xref ref-type="bibr" rid="bib59">Ohzawa et al., 1996</xref>). The model was also compared to measures of simple cell RFs (<xref ref-type="fig" rid="fig7">Figure 7d</xref> and corresponding supplements, cat, <xref ref-type="bibr" rid="bib39">Jones and Palmer, 1987</xref>, mouse, <xref ref-type="bibr" rid="bib55">Niell and Stryker, 2008</xref> and monkey, <xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>). The data were taken from the authors’ website (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>) or extracted from relevant papers (<xref ref-type="bibr" rid="bib39">Jones and Palmer, 1987</xref>) or provided by the authors (<xref ref-type="bibr" rid="bib59">Ohzawa et al., 1996</xref>; <xref ref-type="bibr" rid="bib55">Niell and Stryker, 2008</xref>).</p></sec><sec id="s4-5-2"><title>Fitting Gabors</title><p>In order to quantify tuning properties of the model’s visual RFs, 2D Gabors were fitted to the optimal time-step of each unit’s response (<xref ref-type="bibr" rid="bib39">Jones and Palmer, 1987</xref>; <xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>). This allowed comparison to previous experimental studies which parameterized real RFs by the same method (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>). The optimal time-step was defined (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>) as the time-step of the unit’s response which contained the most power (mean square). The Gabor function has been shown to provide a good approximation for most spatial aspects of simple visual RFs (<xref ref-type="bibr" rid="bib39">Jones and Palmer, 1987</xref>; <xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>). The 2D Gabor is given as:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where, the spatial coordinates (<inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) are acquired by translating the centre of the RF (<inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) to the origin and rotating the RF by its spatial orientation <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>: <disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p><italic><inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> and <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> provide the width of the Gaussian envelope in the <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <italic><inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> directions, while <italic><inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> and <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameterize the spatial frequency and phase of the sinusoid along the <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> axis. <italic>A</italic> parameterizes the height of the Gaussian envelope.</p><p>For each RF, the parameters (<inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow/></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow/></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) of the Gabor were fitted by minimizing the mean squared error between the Gabor model and the RF using the minFunc minimization package (<ext-link ext-link-type="uri" xlink:href="http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html">http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html</ext-link>). In order to avoid local minima, the fitting was performed in two steps. First, the spatial RF was converted to the spectral domain using a 2D Fourier transform. Since the Fourier transform of a 2D Gabor is a 2D Gaussian (<xref ref-type="bibr" rid="bib39">Jones and Palmer, 1987</xref>), which is easier to fit, an estimate of many of the parameters was obtained by first fitting a 2D Gaussian in the spectral magnitude domain. Using the parameters obtained from the spectral fitting as initial estimates, a 2D Gabor was then fitted to the original RF in the spatial domain. The fitted parameters provided a good estimate of the units’ responses, with residual errors between the spatial responses and the corresponding Gabor fits being small and lacking spatial structure, and the median pixel-wise correlation coefficient of the Gabor fits for the temporal prediction model units was 0.88. Units whose fitted Gabors had a poor fit (those with a correlation coefficient &lt;0.7; 214 units) were excluded from further analysis. We also excluded units with a high correlation coefficient (&gt;0.7) if the centre position of the Gabor was estimated to be outside the RF, and hence only the Gabor’s tail was being fitted to the response (39 units), and those for which the estimated standard deviation of the Gaussian envelope in either x or y was &lt;0.5 pixels, which meant very few non-negligible pixel values were used to constrain the parameters (146 units). Together, these exclusion criteria (which sometimes overlapped), led to 395 of the 1600 responsive units being excluded for the temporal prediction model.</p></sec><sec id="s4-5-3"><title>2D spatiotemporal receptive fields</title><p>In order to better view their temporal characteristics we collapsed the 3D spatiotemporal real and model RFs (space-space-time) along a single spatial direction to create 2D spatiotemporal (space-time) representations (<xref ref-type="bibr" rid="bib26">DeAngelis et al., 1993</xref>). First, we determined the 3D RFs’ optimal time step (the time step with the largest sum of squared values). We then acquired the rotation and translation that centres the RF on zero and places the oriented bars parallel to the y-axis at the optimal time step from the Gabor parameterization of each unit at its optimal time step. We applied this fixed transformation to each time step and collapsed the RF by summing the activity along the newly defined y-axis. The resulting 2D (space-time) RFs provide intuitive visualization of the RF across time, while losing minimal information. For the RFs of real neurons (<xref ref-type="bibr" rid="bib59">Ohzawa et al., 1996</xref>), the most recent time step (40 ms) of the 3D and 2D spatiotemporal RFs were removed to account for the latency of V1 neurons (<xref ref-type="fig" rid="fig2">Figures 2c and 7a</xref>).</p></sec><sec id="s4-5-4"><title>Estimating space-time separability</title><p>The population of model units contained both space-time (ST) separable and inseparable units. First the two spatial dimensions of the 20 × 20 × 7 3D RF were collapsed to a single vector to yield a single 400 × 7 matrix. The SVD of this matrix was then taken and the singular values examined. If the ratio between the second and first singular value was ≥0.5, the unit was deemed to be inseparable. Otherwise, the unit was deemed to be separable. Examining the 20 × 7 2D spatiotemporal RFs (obtained as outlined in the preceding section; <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>) showed this to be an accurate way of separating space-time separable and inseparable units.</p></sec><sec id="s4-5-5"><title>Spatial RF structure</title><p>For comparison with the real V1 RF and previous theoretical studies, the width and length of our model’s RFs were measured relative to their spatial frequency (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>). Here, <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> gives a measure of the length of the bars in the RF, while <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> gives a measure of the number of oscillations of its sinusoidal component. Thus, in the <inline-formula><mml:math id="inf102"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf103"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> plane, blob-like RFs with few cycles lie close to the origin, while stretched RFs with many subfields lie away from the origin. RFs with values high along the <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> axis, have many bars, while those far along the <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> axis have long bars. As in Ringach (<xref ref-type="bibr" rid="bib73">Ringach, 2002</xref>) only space-time separable units were included in this analysis.</p></sec><sec id="s4-5-6"><title>Temporal weighting profile of the population</title><p>The mean power for each of the seven time steps of the RFs was examined for both real and model populations (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). The temporal weighting profile was calculated as the mean, over space and the population, of the squared values of the 2D spatiotemporal RFs at each time step.</p></sec><sec id="s4-5-7"><title>Tilt direction index</title><p>The tilt direction index (TDI) (<xref ref-type="bibr" rid="bib26">DeAngelis et al., 1993</xref>; <xref ref-type="bibr" rid="bib63">Pack et al., 2006</xref>; <xref ref-type="bibr" rid="bib4">Anzai et al., 2001</xref>; <xref ref-type="bibr" rid="bib7">Baker, 2001</xref>; <xref ref-type="bibr" rid="bib48">Livingstone and Conway, 2007</xref>) of an RF is given by <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the amplitude at the peak of the 2D Fourier transform of the 2D spatiotemporal RF, found at spatial frequency <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and temporal frequency <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the amplitude at (<inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) in the 2D Fourier transform. The mean and standard deviations of TDI for experimental data for the cat (<xref ref-type="bibr" rid="bib7">Baker, 2001</xref>) and macaque (<xref ref-type="bibr" rid="bib48">Livingstone and Conway, 2007</xref>) were measured from data extracted from figures in the relevant references (Figure 11A and the low-contrast axis of Figure 3A in these papers respectively).</p></sec><sec id="s4-5-8"><title>Peak temporal frequency</title><p>The 2D spatiotemporal RFs were also useful for calculating further temporal response properties of the model. The temporal frequency was calculated as the peak temporal frequency of each spatiotemporal RF as measured from its 2D Fourier transform.</p></sec></sec><sec id="s4-6"><title>Code and data availability</title><p>All custom code used in this study was implemented in MATLAB and Python. We have uploaded the code to a public Github repository (<xref ref-type="bibr" rid="bib79">Singer, 2018</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/temporal_prediction_model">https://github.com/elifesciences-publications/temporal_prediction_model</ext-link>). The raw auditory experimental data is available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/ayw2p/">https://osf.io/ayw2p/</ext-link>. The movies and sounds used for training the models are all publicly available at the websites detailed in the Materials and methods.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>Nicol Harper was supported by a Sir Henry Wellcome Postdoctoral Fellowship (WT082692) and other Wellcome Trust funding (WT076508AIA, WT108369/Z/2015/Z), by the Department of Physiology, Anatomy and Genetics at the University of Oxford, by Action on Hearing Loss (PA07), and by the Biotechnology and Biological Sciences Research Council (BB/H008608/1). Yosef Singer and Yayoi Teramoto were supported by the Clarendon Fund. Yayoi Teramoto was supported by the Wellcome Trust (10525/Z/14/Z). Andrew King and Ben Willmore were supported by the Wellcome Trust (WT076508AIA, WT108369/Z/2015/Z). We thank Bruno Olshausen for discussions on his model.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Senior Editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Investigation, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Software, Formal analysis, Supervision, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Resources, Supervision, Funding acquisition, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Supervision, Funding acquisition, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Methodology, Writing—original draft, Writing—review and editing, Funding acquisition</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Animal experimentation: Auditory RFs of neurons were recorded in the primary auditory cortex (A1) and anterior auditory field (AAF) of 5 pigmented ferrets of both sexes (all &gt; 6 months of age) and used as a basis for comparison with the RFs of model units trained on auditory stimuli. These recordings were performed under license from the UK Home Office and were approved by the University of Oxford Committee on Animal Care and Ethical Review. Full details of the recording methods are described in earlier studies (Willmore et al., 2016; Bizley et al., 2009). Briefly, we induced general anaesthesia with a single intramuscular dose of medetomidine (0.022 mg · kg<sup>−1</sup> · h<sup>−1</sup>) and ketamine (5 mg · kg<sup>−1</sup> · h<sup>−1</sup>), which was then maintained with a continuous intravenous infusion of medetomidine and ketamine in saline. Oxygen was supplemented with a ventilator, and we monitored vital signs (body temperature, end-tidal CO<sub>2</sub>, and the electrocardiogram) throughout the experiment. The temporal muscles were retracted, a head holder was secured to the skull surface, and a craniotomy and a durotomy were made over the auditory cortex. Extracellular recordings were made using silicon probe electrodes (Neuronexus Technologies) and acoustic stimuli were presented via Panasonic RPHV27 earphones, which were coupled to otoscope specula that were inserted into each ear canal, and driven by Tucker-Davis Technologies System III hardware (48 kHz sample rate).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.31557.030</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-31557-transrepform-v3.docx"/></supplementary-material><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All custom code used in this study was implemented in MATLAB and Python. We have uploaded the code to a public Github repository (Singer Y., 2018). The raw auditory experimental data are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/ayw2p/">https://osf.io/ayw2p/</ext-link>. The movies and sounds used for training the models are all publicly available at the websites detailed in the Materials and methods.</p><p>The following dataset was generated:</p><p><related-object content-type="generated-dataset" id="dataset1" source-id="https://osf.io/ayw2p/" source-id-type="uri"><collab collab-type="author">Jan Schnupp</collab><year>2016</year><source>NetworkReceptiveFields</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/ayw2p/">https://osf.io/ayw2p/</ext-link><comment>Available at the Open Science Framework</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname> <given-names>EH</given-names></name><name><surname>Bergen</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatiotemporal energy models for the perception of motion</article-title><source>Journal of the Optical Society of America A</source><volume>2</volume><fpage>284</fpage><pub-id pub-id-type="doi">10.1364/JOSAA.2.000284</pub-id><pub-id pub-id-type="pmid">3973762</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aertsen</surname> <given-names>AM</given-names></name><name><surname>Johannesma</surname> <given-names>PI</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>A comparison of the spectro-temporal sensitivity of auditory neurons to tonal and natural stimuli</article-title><source>Biological Cybernetics</source><volume>42</volume><fpage>145</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1007/BF00336732</pub-id><pub-id pub-id-type="pmid">6976799</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aertsen</surname> <given-names>AM</given-names></name><name><surname>Olders</surname> <given-names>JH</given-names></name><name><surname>Johannesma</surname> <given-names>PI</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Spectro-temporal receptive fields of auditory neurons in the grassfrog. III. Analysis of the stimulus-event relation for natural stimuli</article-title><source>Biological Cybernetics</source><volume>39</volume><fpage>195</fpage><lpage>209</lpage><pub-id pub-id-type="pmid">6972785</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anzai</surname> <given-names>A</given-names></name><name><surname>Ohzawa</surname> <given-names>I</given-names></name><name><surname>Freeman</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Joint-encoding of motion and depth by visual cortical neurons: neural basis of the Pulfrich effect</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>513</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1038/87462</pub-id><pub-id pub-id-type="pmid">11319560</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atencio</surname> <given-names>CA</given-names></name><name><surname>Sharpee</surname> <given-names>TO</given-names></name><name><surname>Schreiner</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cooperative nonlinearities in auditory cortical neurons</article-title><source>Neuron</source><volume>58</volume><fpage>956</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.04.026</pub-id><pub-id pub-id-type="pmid">18579084</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attneave</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Some informational aspects of visual perception</article-title><source>Psychological Review</source><volume>61</volume><fpage>183</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1037/h0054663</pub-id><pub-id pub-id-type="pmid">13167245</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname> <given-names>CL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Linear filtering and nonlinear interactions in direction-selective visual cortex neurons: a noise correlation analysis</article-title><source>Visual Neuroscience</source><volume>18</volume><fpage>465</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1017/S0952523801183136</pub-id><pub-id pub-id-type="pmid">11497423</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname> <given-names>HB</given-names></name></person-group><year iso-8601-date="1959">1959</year><chapter-title>Sensory mechanisms, the reduction of redundancy, and intelligence</chapter-title><person-group person-group-type="editor"><name><surname>Blake</surname> <given-names>D. V</given-names></name><name><surname>Uttley</surname> <given-names>A. M</given-names></name></person-group><source>The Mechanisation of Thought Processes</source><publisher-loc>London</publisher-loc><publisher-name>Her Majesty's Stationery Office</publisher-name><fpage>535</fpage><lpage>539</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Lee</surname> <given-names>D-H</given-names></name><name><surname>Bornschein</surname> <given-names>J</given-names></name><name><surname>Lin</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Towards biologically plausible deep learning</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1502.04156">https://arxiv.org/abs/1502.04156</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berkes</surname> <given-names>P</given-names></name><name><surname>Turner</surname> <given-names>RE</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A structured model of video reproduces primary visual cortical organisation</article-title><source>PLoS Computational Biology</source><volume>5</volume><elocation-id>e1000495</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000495</pub-id><pub-id pub-id-type="pmid">19730679</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berkes</surname> <given-names>P</given-names></name><name><surname>Wiskott</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Slow feature analysis yields a rich repertoire of complex cell properties</article-title><source>Journal of Vision</source><volume>5</volume><fpage>9</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1167/5.6.9</pub-id><pub-id pub-id-type="pmid">16097870</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>Nemenman</surname> <given-names>I</given-names></name><name><surname>Tishby</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Predictability, complexity, and learning</article-title><source>Neural Computation</source><volume>13</volume><fpage>2409</fpage><lpage>2463</lpage><pub-id pub-id-type="doi">10.1162/089976601753195969</pub-id><pub-id pub-id-type="pmid">11674845</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bixler</surname> <given-names>EO</given-names></name><name><surname>Bartlett</surname> <given-names>NR</given-names></name><name><surname>Lansing</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>Latency of the blink reflex and stimulus intensity J</article-title><source>Perception &amp; Psychophysics</source><volume>2</volume><fpage>559</fpage><lpage>560</lpage><pub-id pub-id-type="doi">10.3758/BF03210267</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname> <given-names>JK</given-names></name><name><surname>Nodal</surname> <given-names>FR</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Functional organization of ferret auditory cortex</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>1637</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhi042</pub-id><pub-id pub-id-type="pmid">15703254</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname> <given-names>JK</given-names></name><name><surname>Walker</surname> <given-names>KM</given-names></name><name><surname>Silverman</surname> <given-names>BW</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name><name><surname>Schnupp</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Interdependent encoding of pitch, timbre, and spatial location in auditory cortex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>2064</fpage><lpage>2075</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4755-08.2009</pub-id><pub-id pub-id-type="pmid">19228960</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blättler</surname> <given-names>F</given-names></name><name><surname>Hahnloser</surname> <given-names>RH</given-names></name><name><surname>Doupe</surname> <given-names>A</given-names></name><name><surname>Hahnloser</surname> <given-names>R</given-names></name><name><surname>Wilson</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>An efficient coding hypothesis links sparsity and selectivity of neural responses</article-title><source>PLoS One</source><volume>6</volume><elocation-id>e25506</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0025506</pub-id><pub-id pub-id-type="pmid">22022405</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brito</surname> <given-names>CS</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Nonlinear hebbian learning as a unifying principle in receptive field formation</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005070</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005070</pub-id><pub-id pub-id-type="pmid">27690349</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlin</surname> <given-names>MA</given-names></name><name><surname>Elhilali</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Sustained firing of model central auditory neurons yields a discriminative spectro-temporal representation for natural sounds</article-title><source>PLoS Computational Biology</source><volume>9</volume><elocation-id>e1002982</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002982</pub-id><pub-id pub-id-type="pmid">23555217</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname> <given-names>NL</given-names></name><name><surname>Ming</surname> <given-names>VL</given-names></name><name><surname>Deweese</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sparse codes for speech predict spectrotemporal receptive fields in the inferior colliculus</article-title><source>PLoS Computational Biology</source><volume>8</volume><elocation-id>e1002594</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002594</pub-id><pub-id pub-id-type="pmid">22807665</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalk</surname> <given-names>M</given-names></name><name><surname>Marre</surname> <given-names>O</given-names></name><name><surname>Tkačik</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Toward a unified theory of efficient, predictive, and sparse coding</article-title><source>PNAS</source><volume>115</volume><fpage>186</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1073/pnas.1711114115</pub-id><pub-id pub-id-type="pmid">29259111</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A simple white noise analysis of neuronal light responses</article-title><source>Network: Computation in Neural Systems</source><volume>12</volume><fpage>199</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1080/713663221</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Creutzig</surname> <given-names>F</given-names></name><name><surname>Sprekeler</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Predictive coding and the slowness principle: an information-theoretic approach</article-title><source>Neural Computation</source><volume>20</volume><fpage>1026</fpage><lpage>1041</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.01-07-455</pub-id><pub-id pub-id-type="pmid">18085988</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cusack</surname> <given-names>R</given-names></name><name><surname>Carlyon</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><chapter-title>Auditory perceptual organization inside and outside the laboratory</chapter-title><person-group person-group-type="editor"><name><surname>Neuhoff</surname> <given-names>J. G</given-names></name></person-group><source>Echological Pyschoacoustics</source><publisher-name>Elsevier</publisher-name><fpage>15</fpage><lpage>48</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahmen</surname> <given-names>JC</given-names></name><name><surname>Hartley</surname> <given-names>DE</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Stimulus-timing-dependent plasticity of cortical frequency representation</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>13629</fpage><lpage>13639</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4429-08.2008</pub-id><pub-id pub-id-type="pmid">19074036</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahmen</surname> <given-names>JC</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning to hear: plasticity of auditory cortical processing</article-title><source>Current Opinion in Neurobiology</source><volume>17</volume><fpage>456</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.07.004</pub-id><pub-id pub-id-type="pmid">17714932</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Ohzawa</surname> <given-names>I</given-names></name><name><surname>Freeman</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Spatiotemporal organization of simple-cell receptive fields in the cat's striate cortex. I. General characteristics and postnatal development</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>1091</fpage><lpage>1117</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.4.1091</pub-id><pub-id pub-id-type="pmid">8492151</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>deCharms</surname> <given-names>RC</given-names></name><name><surname>Blake</surname> <given-names>DT</given-names></name><name><surname>Merzenich</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Optimizing sound features for cortical neurons</article-title><source>Science</source><volume>280</volume><fpage>1439</fpage><lpage>1444</lpage><pub-id pub-id-type="doi">10.1126/science.280.5368.1439</pub-id><pub-id pub-id-type="pmid">9603734</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eliasmith</surname> <given-names>C</given-names></name><name><surname>Anderson</surname> <given-names>CH</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Neural Engineering : Computation, Representation, and Dynamics in Neurobiological Systems</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Relations between the statistics of natural images and the response properties of cortical cells</article-title><source>Journal of the Optical Society of America A</source><volume>4</volume><fpage>2379</fpage><pub-id pub-id-type="doi">10.1364/JOSAA.4.002379</pub-id><pub-id pub-id-type="pmid">3430225</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Learning and inference in the brain</article-title><source>Neural Networks</source><volume>16</volume><fpage>1325</fpage><lpage>1352</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2003.06.005</pub-id><pub-id pub-id-type="pmid">14622888</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harper</surname> <given-names>NS</given-names></name><name><surname>Schoppe</surname> <given-names>O</given-names></name><name><surname>Willmore</surname> <given-names>BD</given-names></name><name><surname>Cui</surname> <given-names>Z</given-names></name><name><surname>Schnupp</surname> <given-names>JW</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Network receptive field modeling reveals extensive integration and Multi-feature selectivity in auditory cortical neurons</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005113</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005113</pub-id><pub-id pub-id-type="pmid">27835647</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical connectivity and sensory coding</article-title><source>Nature</source><volume>503</volume><fpage>51</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1038/nature12654</pub-id><pub-id pub-id-type="pmid">24201278</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Theory of cortical function</article-title><source>PNAS</source><volume>114</volume><fpage>1773</fpage><lpage>1782</lpage><pub-id pub-id-type="doi">10.1073/pnas.1619788114</pub-id><pub-id pub-id-type="pmid">28167793</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Helmholtz</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1962">1962</year><chapter-title>Concerning the perceptions in general</chapter-title><source> Treatise on Physiological Optics</source><edition>3rd ed</edition><publisher-loc>New York</publisher-loc><publisher-name>Dover Publications</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>Y</given-names></name><name><surname>Rao</surname> <given-names>RPN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Predictive coding</article-title><source>Wiley Interdisciplinary Reviews: Cognitive Science</source><volume>2</volume><fpage>580</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1002/wcs.142</pub-id><pub-id pub-id-type="pmid">26302308</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Receptive fields of single neurones in the cat's striate cortex</article-title><source>The Journal of Physiology</source><volume>148</volume><fpage>574</fpage><lpage>591</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1959.sp006308</pub-id><pub-id pub-id-type="pmid">14403679</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huberman</surname> <given-names>AD</given-names></name><name><surname>Feller</surname> <given-names>MB</given-names></name><name><surname>Chapman</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mechanisms underlying development of visual maps and receptive fields</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>479</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.31.060407.125533</pub-id><pub-id pub-id-type="pmid">18558864</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvärinen</surname> <given-names>A</given-names></name><name><surname>Hurri</surname> <given-names>J</given-names></name><name><surname>Väyrynen</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Bubbles: a unifying framework for low-level statistical properties of natural image sequences</article-title><source>Journal of the Optical Society of America A</source><volume>20</volume><fpage>1237</fpage><lpage>1252</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.20.001237</pub-id><pub-id pub-id-type="pmid">12868630</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>JP</given-names></name><name><surname>Palmer</surname> <given-names>LA</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>An evaluation of the two-dimensional gabor filter model of simple receptive fields in cat striate cortex</article-title><source>Journal of Neurophysiology</source><volume>58</volume><fpage>1233</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.1152/jn.1987.58.6.1233</pub-id><pub-id pub-id-type="pmid">3437332</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kavanagh</surname> <given-names>GL</given-names></name><name><surname>Kelly</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Hearing in the ferret (Mustela putorius): effects of primary auditory cortical lesions on thresholds for pure tone detection</article-title><source>Journal of Neurophysiology</source><volume>60</volume><fpage>879</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1152/jn.1988.60.3.879</pub-id><pub-id pub-id-type="pmid">3171665</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Einhäuser</surname> <given-names>W</given-names></name><name><surname>Dümmer</surname> <given-names>O</given-names></name><name><surname>König</surname> <given-names>P</given-names></name><name><surname>Körding</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2001">2001</year><chapter-title>Extracting Slow Subspaces from Natural Videos Leads to Complex Cells</chapter-title><person-group person-group-type="editor"><name><surname>Dorffner</surname> <given-names>G</given-names></name><name><surname>Bischof</surname> <given-names>H</given-names></name><name><surname>Hornik</surname> <given-names>K</given-names></name></person-group><source>Artificial Neural Networks — ICANN 2001</source><volume>2130</volume><publisher-name>Springer</publisher-name><fpage>1075</fpage><lpage>1080</lpage><pub-id pub-id-type="doi">10.1007/3-540-44668-0_149</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Adam</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title> A Method for Stochastic Optimization</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiorpes</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visual development in primates: Neural mechanisms and critical periods</article-title><source>Developmental Neurobiology</source><volume>75</volume><fpage>1080</fpage><lpage>1090</lpage><pub-id pub-id-type="doi">10.1002/dneu.22276</pub-id><pub-id pub-id-type="pmid">25649764</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname> <given-names>DJ</given-names></name><name><surname>König</surname> <given-names>P</given-names></name><name><surname>Körding</surname> <given-names>KP</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Sparse spectrotemporal coding of sounds</article-title><source>EURASIP Journal on Advances in Signal Processing</source><volume>2003</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/S1110865703303051</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kozlov</surname> <given-names>AS</given-names></name><name><surname>Gentner</surname> <given-names>TQ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Central auditory neurons have composite receptive fields</article-title><source>PNAS</source><volume>113</volume><fpage>1441</fpage><lpage>1446</lpage><pub-id pub-id-type="doi">10.1073/pnas.1506903113</pub-id><pub-id pub-id-type="pmid">26787894</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreile</surname> <given-names>AK</given-names></name><name><surname>Bonhoeffer</surname> <given-names>T</given-names></name><name><surname>Hübener</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Altered visual experience induces instructive changes of orientation preference in mouse visual cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>13911</fpage><lpage>13920</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2143-11.2011</pub-id><pub-id pub-id-type="pmid">21957253</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname> <given-names>TP</given-names></name><name><surname>Cownden</surname> <given-names>D</given-names></name><name><surname>Tweed</surname> <given-names>DB</given-names></name><name><surname>Akerman</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13276</pub-id><pub-id pub-id-type="pmid">27824044</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname> <given-names>MS</given-names></name><name><surname>Conway</surname> <given-names>BR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Contrast affects speed tuning, space-time slant, and receptive-field organization of simple cells in macaque V1</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>849</fpage><lpage>857</lpage><pub-id pub-id-type="doi">10.1152/jn.00762.2006</pub-id><pub-id pub-id-type="pmid">17108092</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lotter</surname> <given-names>W</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name><name><surname>Cox</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep predictive coding networks for video prediction and unsupervised learning</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1605.08104">https://arxiv.org/abs/1605.08104</ext-link></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marr</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Early processing of visual information</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>275</volume><fpage>483</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1098/rstb.1976.0090</pub-id><pub-id pub-id-type="pmid">12519</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marzen</surname> <given-names>SE</given-names></name><name><surname>DeDeo</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The evolution of lossy compression</article-title><source>Journal of the Royal Society Interface</source><volume>14</volume><elocation-id>20170166</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2017.0166</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Młynarski</surname> <given-names>W</given-names></name><name><surname>McDermott</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning Mid-Level auditory codes from natural sound statistics</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1701.07138">https://arxiv.org/abs/1701.07138</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrison</surname> <given-names>GS</given-names></name><name><surname>Rose</surname> <given-names>P</given-names></name><name><surname>Zhang</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Protocol for the collection of databases of recordings for forensic-voice-comparison research and practice</article-title><source>Australian Journal of Forensic Sciences</source><volume>44</volume><fpage>155</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1080/00450618.2011.630412</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Morrison</surname> <given-names>GS</given-names></name><name><surname>Zhang</surname> <given-names>C</given-names></name><name><surname>Enzinger</surname> <given-names>E</given-names></name><name><surname>Ochoa</surname> <given-names>F</given-names></name><name><surname>Bleach</surname> <given-names>D</given-names></name><name><surname>Johnson</surname> <given-names>M</given-names></name><name><surname>Folkes</surname> <given-names>BK</given-names></name><name><surname>DeSouza</surname> <given-names>S</given-names></name><name><surname>Cummins</surname> <given-names>N</given-names></name><name><surname>Chow</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Forensic database of voice recordings of 500+ Australian English speakers</article-title><ext-link ext-link-type="uri" xlink:href="http://databases.forensic-voice-comparison.net/">http://databases.forensic-voice-comparison.net/</ext-link><date-in-citation iso-8601-date="2016-08-16">August 16, 2016</date-in-citation></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname> <given-names>CM</given-names></name><name><surname>Stryker</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Highly selective receptive fields in mouse visual cortex</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>7520</fpage><lpage>7536</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0623-08.2008</pub-id><pub-id pub-id-type="pmid">18650330</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nijhawan</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Motion extrapolation in catching</article-title><source>Nature</source><volume>370</volume><fpage>256</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.1038/370256b0</pub-id><pub-id pub-id-type="pmid">8035873</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nodal</surname> <given-names>FR</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Hearing and Auditory Function in Ferrets</chapter-title><source>Biology and Diseases of the Ferret</source><publisher-name>John Wiley &amp; Sons, Inc</publisher-name><fpage>685</fpage><lpage>710</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Oh</surname> <given-names>J</given-names></name><name><surname>Guo</surname> <given-names>X</given-names></name><name><surname>Lee</surname> <given-names>H</given-names></name><name><surname>Lewis</surname> <given-names>R</given-names></name><name><surname>Singh</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Action-Conditional video prediction using deep networks in atari games</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1507.08750">https://arxiv.org/abs/1507.08750</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohzawa</surname> <given-names>I</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Freeman</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Encoding of binocular disparity by simple cells in the cat's visual cortex</article-title><source>Journal of Neurophysiology</source><volume>75</volume><fpage>1779</fpage><lpage>1805</lpage><pub-id pub-id-type="doi">10.1152/jn.1996.75.5.1779</pub-id><pub-id pub-id-type="pmid">8734580</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Sparse coding with an overcomplete basis set: a strategy employed by V1?</article-title><source>Vision Research</source><volume>37</volume><fpage>3311</fpage><lpage>3325</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(97)00169-7</pub-id><pub-id pub-id-type="pmid">9425546</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Learning sparse, overcomplete representations of time-varying natural images </article-title><conf-name>IEEE International Conference on Image Processing</conf-name><conf-loc>Barcelona, Spain</conf-loc><pub-id pub-id-type="doi">10.1109/ICIP.2003.1246893</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pack</surname> <given-names>CC</given-names></name><name><surname>Conway</surname> <given-names>BR</given-names></name><name><surname>Born</surname> <given-names>RT</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spatiotemporal structure of nonlinear subunits in macaque visual cortex</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>893</fpage><lpage>907</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3226-05.2006</pub-id><pub-id pub-id-type="pmid">16421309</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Palm</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Prediction as a candidate for learning deep hierarchical models of data</article-title><publisher-name>Technical University of Denmark, (DTU) Informatics</publisher-name></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname> <given-names>SE</given-names></name><name><surname>Marre</surname> <given-names>O</given-names></name><name><surname>Berry</surname> <given-names>MJ</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predictive information in a sensory population</article-title><source>PNAS</source><volume>112</volume><fpage>6908</fpage><lpage>6913</lpage><pub-id pub-id-type="doi">10.1073/pnas.1506855112</pub-id><pub-id pub-id-type="pmid">26038544</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinowitz</surname> <given-names>NC</given-names></name><name><surname>Willmore</surname> <given-names>BD</given-names></name><name><surname>Schnupp</surname> <given-names>JW</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contrast gain control in auditory cortex</article-title><source>Neuron</source><volume>70</volume><fpage>1178</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.04.030</pub-id><pub-id pub-id-type="pmid">21689603</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ranzato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Video (language) modeling: a baseline for generative models of natural videos</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6604">https://arxiv.org/abs/1412.6604</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname> <given-names>RP</given-names></name><name><surname>Ballard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Dynamic model of visual recognition predicts neural response properties in the visual cortex</article-title><source>Neural Computation</source><volume>9</volume><fpage>721</fpage><lpage>763</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.4.721</pub-id><pub-id pub-id-type="pmid">9161021</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname> <given-names>RP</given-names></name><name><surname>Ballard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>An optimal estimation approach to visual perception and learning</article-title><source>Vision Research</source><volume>39</volume><fpage>1963</fpage><lpage>1989</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(98)00279-X</pub-id><pub-id pub-id-type="pmid">10343783</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rees</surname> <given-names>A</given-names></name><name><surname>Møller</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Responses of neurons in the inferior colliculus of the rat to AM and FM tones</article-title><source>Hearing Research</source><volume>10</volume><fpage>301</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1016/0378-5955(83)90095-3</pub-id><pub-id pub-id-type="pmid">6874603</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reid</surname> <given-names>RC</given-names></name><name><surname>Soodak</surname> <given-names>RE</given-names></name><name><surname>Shapley</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Linear mechanisms of directional selectivity in simple cells of cat striate cortex</article-title><source>PNAS</source><volume>84</volume><fpage>8740</fpage><lpage>8744</lpage><pub-id pub-id-type="doi">10.1073/pnas.84.23.8740</pub-id><pub-id pub-id-type="pmid">3479811</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ringach</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Spatial structure and symmetry of simple-cell receptive fields in macaque primary visual cortex</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>455</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1152/jn.2002.88.1.455</pub-id><pub-id pub-id-type="pmid">12091567</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname> <given-names>J</given-names></name><name><surname>Ulanovsky</surname> <given-names>N</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name><name><surname>Tishby</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The representation of prediction error in auditory cortex</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005058</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005058</pub-id><pub-id pub-id-type="pmid">27490251</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sachs</surname> <given-names>MB</given-names></name><name><surname>Abbas</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Rate versus level functions for auditory-nerve fibers in cats: tone-burst stimuli</article-title><source>The Journal of the Acoustical Society of America</source><volume>56</volume><fpage>1835</fpage><lpage>1847</lpage><pub-id pub-id-type="doi">10.1121/1.1903521</pub-id><pub-id pub-id-type="pmid">4443483</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Linden</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>How linear are auditory cortical responses?</article-title><source>Advances in Neural Information Processing Systems</source><volume>15</volume><fpage>109</fpage><lpage>116</lpage></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salisbury</surname> <given-names>JM</given-names></name><name><surname>Palmer</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Optimal prediction in the retina and natural motion statistics</article-title><source>Journal of Statistical Physics</source><volume>162</volume><fpage>1309</fpage><lpage>1323</lpage><pub-id pub-id-type="doi">10.1007/s10955-015-1439-y</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simoncelli</surname> <given-names>E</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Schwartz</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2004">2004</year><chapter-title>Characterization of neural responses with stochastic stimuli</chapter-title><person-group person-group-type="editor"><name><surname>Gazzaniga</surname> <given-names>M</given-names></name></person-group><source>The Cognitive Neurosciences, III</source><publisher-name>MIT Press</publisher-name><fpage>327</fpage><lpage>338</lpage></element-citation></ref><ref id="bib79"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Singer</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>temporal_prediction_model</data-title><source>Github</source><version designator="ba8ed26">ba8ed26</version><ext-link ext-link-type="uri" xlink:href="https://github.com/yossing/temporal_prediction_model">https://github.com/yossing/temporal_prediction_model</ext-link></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>EC</given-names></name><name><surname>Lewicki</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Efficient auditory coding</article-title><source>Nature</source><volume>439</volume><fpage>978</fpage><lpage>982</lpage><pub-id pub-id-type="doi">10.1038/nature04485</pub-id><pub-id pub-id-type="pmid">16495999</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sohl-Dickstein</surname> <given-names>J</given-names></name><name><surname>Poole</surname> <given-names>B</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1311.2115">https://arxiv.org/abs/1311.2115</ext-link></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname> <given-names>MV</given-names></name><name><surname>Laughlin</surname> <given-names>SB</given-names></name><name><surname>Dubs</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Predictive coding: a fresh view of inhibition in the retina</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>216</volume><fpage>427</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1098/rspb.1982.0085</pub-id><pub-id pub-id-type="pmid">6129637</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Srivastava</surname> <given-names>N</given-names></name><name><surname>Mansimov</surname> <given-names>E</given-names></name><name><surname>Salakhutdinov</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Unsupervised learning of video representations using LSTMs</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1502.04681">https://arxiv.org/abs/1502.04681</ext-link></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barton</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>An adaptive network that constructs and uses an internal model of its world</article-title><source>Cognition and Brain Theory</source><volume>4</volume><fpage>217</fpage><lpage>246</lpage></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Statistics of natural image categories</article-title><source>Network: Computation in Neural Systems</source><volume>14</volume><fpage>391</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_14_3_302</pub-id><pub-id pub-id-type="pmid">12938764</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname> <given-names>JH</given-names></name><name><surname>Ruderman</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="1998">1998a</year><article-title>Independent component analysis of natural image sequences yields spatio-temporal filters similar to simple cells in primary visual cortex</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>265</volume><fpage>2315</fpage><lpage>2320</lpage><pub-id pub-id-type="doi">10.1098/rspb.1998.0577</pub-id><pub-id pub-id-type="pmid">9881476</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname> <given-names>JH</given-names></name><name><surname>van der Schaaf</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998b</year><article-title>Independent component filters of natural images compared with simple cells in primary visual cortex</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>265</volume><fpage>359</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1098/rspb.1998.0303</pub-id><pub-id pub-id-type="pmid">9523437</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname> <given-names>BD</given-names></name><name><surname>Schoppe</surname> <given-names>O</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name><name><surname>Schnupp</surname> <given-names>JW</given-names></name><name><surname>Harper</surname> <given-names>NS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Incorporating midbrain adaptation to mean sound level improves models of auditory cortical processing</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>280</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2441-15.2016</pub-id><pub-id pub-id-type="pmid">26758822</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiskott</surname> <given-names>L</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Slow feature analysis: unsupervised learning of invariances</article-title><source>Neural Computation</source><volume>14</volume><fpage>715</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1162/089976602317318938</pub-id><pub-id pub-id-type="pmid">11936959</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeomans</surname> <given-names>JS</given-names></name><name><surname>Frankland</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The acoustic startle reflex: neurons and connections</article-title><source>Brain Research Reviews</source><volume>21</volume><fpage>301</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1016/0165-0173(96)00004-5</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname> <given-names>L</given-names></name><name><surname>Zhaoping</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Understanding auditory spectro-temporal receptive fields and their changes with input statistics by efficient coding principles</article-title><source>PLoS Computational Biology</source><volume>7</volume><elocation-id>e1002123</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002123</pub-id><pub-id pub-id-type="pmid">21887121</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.31557.034</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Gallant</surname><given-names>Jack L</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Sensory cortex is optimized for prediction of future input&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by Reviewing Editor Jack Gallant and Senior Editor Sabine Kastner. The following individuals involved in review of your submission have agreed to reveal their identity: Rhodri Cusack (Reviewer #1); Laurenz Wiskott (Reviewer #2); Christoph Zetzsche (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another, and they and the Reviewing Editor agree that your paper is potentially suitable for publication in <italic>eLife</italic> after appropriate revision. The Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>The Reviewing Editor hopes that you will address all of the concerns of the authors, most of which are straightforward. But to help you in revision the major issues are listed here:</p><p>1) If you look at the reviews you will see that the list of suggestions is very long, but the vast majority of the comments only ask for clarification, they do not require additional data analysis or substantial rewriting. It would be good if you could address all questions in your reply to reviewers and revise the text appropriately where necessary to provide necessary information to the reader.</p><p>2) The proposed model is interesting, but it has many components that may differentially contribute to the result, such as the nonlinearity or <italic>L</italic><sub>1</sub> regularization. The reviewers felt that the paper would be stronger if the specific effects of these various model component choices were analyzed in a bit more detail, in order to try to pin down more precisely why these components are important. (See for example suggestions of reviewer 2.)</p><p>3) In some places the choices made during modelling seemed arbitrary (e.g., the choice of temporal windows and the regularization parameters). Either grid search over a training set should be used to choose these parameters, or hyperparameter modelling should be performed to show that the specific values chosen are not critical, or the choices should be justified. The first of these is obviously most desirable.</p><p>4) Some of the figures are difficult to interpret (c.f. Figure 7B and Figure 7—figure supplement 2B). Please try to improve the figures where necessary.</p><p>5) Please address the concerns of reviewer 3 regarding the introductory material on temporal prediction and the neurobiological plausibility of the approach.</p><p>Reviewer #1:</p><p>This manuscript compares two stimulus coding principles that could explain the form of receptive fields in sensory cortex: efficient sparse coding and temporal prediction. A simple temporal prediction model was found create receptive fields that were similar in many ways to those seen in sensory cortex. It performed much better than a sparse coding model.</p><p>This manuscript makes an interesting and important contribution. The &quot;sparse coding&quot; hypothesis is popular, both in neuroscience and in artificial intelligence, where autoencoding in deep-neural networks implements efficient coding. The authors argue that for dynamic stimuli, the need to predict what will happen next may be a more important than merely encoding efficiently what has happened recently. Their model is simple and elegant, and the results are convincing.</p><p>I felt there were some places where choices were made during modelling that seemed arbitrary – such as the choice of temporal windows and the regularization parameters. The manuscript would be stronger if these choices were either justified, or hyperparameter modelling done to show that the specific values chosen are not critical, to allay concerns readers may have of &quot;p-hacking&quot;.</p><p>I found the manuscript to be well-structured, thorough and well written, clearly conveying a convincing message.</p><p>Reviewer #2:</p><p>The paper presents a two-layer network optimized for predicting immediate future sensory input (auditory or visual) from recent past sensory input. The resulting spatio- or spectrotemporal receptive fields, i.e. weight vectors of the first layer, are analyzed and compared with physiological receptive fields. For model comparison, results from a sparse coding network are used.</p><p>The results show that the predicting neural network captures receptive field properties fairly well, in particular temporal structure is reproduced much better than by the sparse coding network.</p><p>The topic is interesting, and the results are highly relevant to the field. I must add, however, that I have not followed the field recently, so I cannot really tell, whether some similar work has been published recently. But the authors seem to have done a careful literature research and discuss alternative approaches fairly.</p><p>The paper is well structured and has obviously been written very carefully. I have rarely reviewed a manuscript that feels so ready for publication. So, I am tempted to recommend the paper for publication as it is.</p><p>There is just one issue I would invite the authors to consider a bit further: The claim of the paper is that the objective of temporal prediction results in the receptive field properties found. But there are additional factors, such as the nonlinearity and the <italic>L</italic><sub>1</sub> regularization, that contribute to it. The authors have investigated this to some extent. For example, they find that receptive fields are seriously degraded if the nonlinearity is replaced by a linear activation function. My suggestion is to try to pin down, what objective is implicitly added by the nonlinearity and the <italic>L</italic><sub>1</sub> regularization. I suggest to perform a similar experiment as in Figure 8, but with a sparseness or independence measure rather than final validation loss. This could also be done on the hidden units.</p><p>I suggest this, because I believe that temporal prediction alone does not do the trick. I feel it must be combined with some sparseness or independence objective to yield the receptive fields. And I feel that this missing objective is implicitly added by the nonlinearity and the <italic>L</italic><sub>1</sub> regularization. Making this more transparent would be great and the suggested experiment should be very easy to do.</p><p>Reviewer #3:</p><p>The authors propose a new principle for the development of cortical receptive fields which combines the concepts of predictive coding and sparse coding. They train a three-layer network with one hidden layer in order to predict the future visual spatial input or the future auditory auditory spectro-temporal input from the recent spatio-/spectro-temporal input, subject to a sparsity constraint.</p><p>They perform this training for two examples: for an auditory network, based on training data which contain human speech, animal vocalization and inanimate natural sounds, and for a visual network based on training data with movies of wildlife in natural settings.</p><p>They compare the resulting networks to real cortical neurons from A1 and V1 and to an alternative sparse coding approach intended to provide a sparse representation of the complete spatio/spectro-temporal input.</p><p>For their comparison they consider the spectrotemporal and spatiotemporal receptive fields and various population measures, e.g. the temporal decay of power in the receptive fields, the temporal span of excitation an inhibition, orientation and frequency tuning properties, and receptive field dimensions.</p><p>Except for orientation, for which the majority of visual units is restricted in their orientation preference to horizontal and vertical orientations, the proposed model can capture neural tuning properties as well as the established models. And in case of the asymmetric emphasis of the most recent past it can even provide a better description.</p><p>In my opinion this is a quite interesting paper. First, it presents a novel approach which unifies the principles of sparse coding and of temporal prediction. This combination enables the explanation of a large set of spatio-/spectro-temporal tuning properties within one single integrated framework. Second, the authors have an important point in stressing the asymmetry of the temporal response with its emphasis of the most recent past, as observed in typical cortical neurons. This is indeed a property that other learning schemes, like sparse coding, by the very nature of their objective functions, cannot produce.</p><p>There are some points that, in my view, need to be clarified or described in more detail. In the following I describe the modifications and additions which I assume to be helpful in a revision of the paper. Due to my background I will put more emphasis on the visual aspects.</p><p>1) The description of the history of the concept of temporal prediction is not clear enough, both in the introduction and in the discussion. I am aware of the pressure for novelty in current science but in my view, there is sufficient novelty in the suggested model to allow the authors to avoid such ambiguities. Currently, the paper might be misinterpreted by a swift non-specialist reader as if the concept of the &quot;prediction of the immediate future&quot; is a novel principle being introduced here (Introduction; Discussion section: &quot;We hypothesized&quot;). Only a few selected papers are cited directly in this context (only Bialek in the Introduction), and in the Discussion section they are characterized as unspecific: &quot;The temporal prediction principle we describe.… has been described in a very general manner&quot;(reference only to Bialek, Palmer). Other references exist but are spread out through the further text. But of course, the principle as such has a long history, there are numerous papers which describe the prediction of the future sensory input as an important goal of neural information processing. I am no specialist, and this is not comprehensive but early examples are corollary discharge theories, and already Sutton and Barto, (1981) and Srinivasan et al., (1982), for example, considered the temporal dimension of prediction. Motion extrapolation has also been interpreted as prediction computed in visual cortex (Nijhawan, 1994). A further, canonical example of a method for the optimal prediction of the future sensory input is the Kalman filter, as considered by, e.g., Rao, (1999). I suggest that the authors devote one paragraph to the history of the concept, with all the appropriate references included there, and then make precisely clear in which aspects their novel contribution extends beyond these earlier approaches.</p><p>2) Neurobiological plausibility of the approach: I do not think that the authors have to be as clear about the neural implementation of the suggested architecture as the other predictive coding approaches, but at least some rough or speculative ideas should be presented: What is the status of the second-order units? Where in the cortex are they (V2?) and what do they encode? Really the future INPUT itself? That is, they have no selectivity, no tuning properties? Have such units been observed? Where and how is the prediction error computed? Does this model not require a bypass line which brings the retinal spatial input directly to V1 or V2 to enable the comparison?.…</p><p>3) The sparse coding data appear quite unusual. Why are the units not more &quot;localized&quot; in the temporal dimension, in particular for the visual model? Furthermore, it seems as if the visual sparse model is not used in the usual overcomplete regime. I also would have expected a more concentrated distribution of the temporal span of excitation and inhibition for a typical sparse coding model. Please discuss this in the paper.</p><p>4) Subsection “Model receptive fields” by inspection: is there really no other possibility to determine the optimal hyperparameters of the sparse coding model? A fit to the neural data? You have Figure 8—figure supplement 1B anyway. Why have you not made use of it? One could use only a training subset, if this seems critical issue. And one can include KS measures of other tuning properties.</p><p>5) Subsection “Addition of Gaussian noise”: Noise. For me the use of noise in this investigation is somewhat unclear. First, it seems to favor the prediction model over the sparse model, which is more susceptible to noise. Second, the noise level used appears unusually strong (is this dB?). This issue should be clearly motivated and discussed in the main text of the paper.</p><p>6) Subsection “Model receptive fields”: I am a bit skeptic with respect to the sign-flipping of excitation and inhibition. The argument that the signs could as well be flipped if this is done for the first-order and the second-order units alike appears only valid because any specification and relation to real neurons is omitted for the second-order units. In fact, this sign-flipping will inevitably imply a prediction of how excitation and inhibition operate in the second-order units.</p><p>Furthermore, if this argumentation would be accepted then one could arbitrarily flip signs to the desired result in any learning model, because one can always argue that appropriate sign flips at some subsequent processing stages could compensate for this. Used in this way, excitation and inhibition would lose any meaning.</p><p>It is perfectly ok for me if a model is agnostic with respect to the correct prediction of excitation and inhibition, a model does not have to be perfect in all aspects. But if this is the case this should be clearly visible for the reader in the presented receptive field plots. (This does not exclude to use of an appropriate sign-flip in population measures.)</p><p>7) Figure 4—figure supplement 2 and Figure 4—figure supplement 3: Two separate populations? Visual inspection of these figures suggests the possible existence of two distinct populations. Is this related to separability, or blob-like units, or both? (Is ordering according to separability?) I am not sure about the current state in the field, but I remember a discussion about the existence of two distinct populations as opposed to a continuous distribution for separability. This issue should be described and discussed.</p><p>8) ibid. The percentage of blob-like units appears quite high. Is this percentage comparable to the neural data? Or only if the mouse data, which are special in this respect, are being included?</p><p>9) Figure 4—figure supplement 6 The percentage of blob-like units seems to be substantially reduced in comparison to the noisy case (Figure 4—figure supplement 2). Is there a systematical relation between high noise levels and the emergence of blob-like units? Please discuss this issue in the paper.</p><p>10) ibid. It is difficult to understand the relation between Figure 4—figure supplement 2 and Figure 7C as opposed to Figure 4—figure supplement 6 and Figure 7—figure supplement 2F. Can you describe how properties of the receptive field plots relate to properties of the population distribution in these two cases?</p><p>11) Figure 7 and others Spatiotemporal population properties: Although the article is about spatiotemporal processing it provides only two population measures of purely static spatial properties and one of a purely temporal property for the vision case. Spatiotemporal measures of particular interest would be: DSI (directional selectivity index)/TDI (tilt direction index) (direction selectivity is considered to be a major spatiotemporal property of visual cortex); if possible: a scatter plot of temporal frequency vs. spatial frequency; population distribution of motion tuning. The necessary data for these plots should be already available.</p><p>12) Figure 7B and Figure 7—figure supplement 2B: The orientation scatter plot is visually difficult to interpret. The quantitative degree of concentration of the preferred orientations on the vertical and horizontal orientations as opposed to the oblique orientations remains unclear. Please provide either an orientation histogram or the percentage of units which fall into the 30-60, 120-150 deg range. In both cases the lowest spatial frequencies should be omitted for the analysis.</p><p>13) Figure 7—figure supplement 1: I am a bit surprised that only 289/400 sparse-coding units can be fitted by a Gabor function. Why is this? Usually most sparse coding units have a good Gabor fit. And with such a high percentage excluded I see the risk of a systematic bias regarding certain tuning parameters.</p><p>14) Figure 7C and Figure 7—figure supplement 2F seem to indicate that the model produces two distinct sub-populations with respect to receptive field parameters n_x and n_y. It should be discussed whether this is the case, and if yes, whether it is systematically related to other parameters (selectivities) of the units. Could this be related related to the two apparent sub-populations regarding separability, blob-like shapes, cf. Figure 4—figure supplement 2? And has such a tendency has also been observed in neural data? In contrast, Ringach, (2004) claimed clustering around a one-dimensional curve. Please describe and discuss in text.</p><p>15) Figure 8 and subsection “Implementation details: Are we expected to see here that 1600 hidden units are a distinguished optimum? Does this figure not tell us that the prediction error does not substantially depend on the number? And when a biological system could achieve basically the same prediction quality with 100 neurons why should it then invest 1500 additional units for such a small advantage?</p><p>16) Figure 8 and Figure 8—figure supplement 1. The comparison between the models prediction capability and the similarity of the model units to real units should not only be presented for the auditory neurons but also for the visual neurons (according to the text the data seem to be already available).</p><p>17) Subsection “Optimising predictive capacity” It is not clear whether the similarity measure (only the span of temporal and frequency tuning is considered) is fair or biased for the comparison with the sparse coding model. What will happen, for example, if the distribution of orientation preference would be used instead (for the visual model)? Would then the sparse coding model appear more similar to the real neurons than the prediction model?</p><p>Please motivate and discuss.</p><p>18) Discussion section: I cannot follow the argumentation that the class of prediction models (or this specific model) should somehow be unique with respect to the ability to provide an independent criterion for the selection of hyperparameters. Should this somehow be a principle, or a logical conclusion? Or an empirical observation only for this special case? Is it logically impossible that we find a measure, for example something entropy-related or whatever, for sparse coding that could have the same status? Please clarify.</p><p>19) Subsection “Visual normative models”:does not refer to prediction of the future: Why not? Is not the temporal prediction made in these models that the future spatial pattern is the same as the previous spatial pattern?</p><p>20) Subsection “Visual normative models”:The model is selective, throws away information, as opposed to the information preserving properties of other models. But is this really a good strategy for *early* stages of a multi-stage information processing system? Does the principle of least commitment not suggest just the opposite strategy?</p><p>21) For the Discussion section it would be of interest to consider the contributions from the two components of the prediction model, i.e., which properties of the units are genuinely caused by prediction and which are more due to the sparse coding part?</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Sensory cortex is optimized for prediction of future input&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Sabine Kastner (Senior Editor/Reviewing Editor), and two reviewers.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>The authors have addressed my comments in a careful manner. In particular, I appreciate that they made considerable and appropriate changes to text and figures instead of just providing arguments in the response letter. From my view, the manuscript is basically ready for publication. I have a few final minor suggestions.</p><p>1).… We examined linear aspects of the tuning of the output units for the visual temporal prediction model using a response-weighted average to white noise input and found punctate un-oriented RFs that decay into the past..…</p><p>This is interesting. Can you mention this somewhere in the text?</p><p>2) I understand that the model, by its very nature would not care about the sign. But the fact remains that you have an output of a model and you post hoc manipulate this output to obtain a &quot;better suited&quot; presentation (e.g., to ease comparison). My only point is that it should be totally clear to even a superficial reader that such a post hoc change has been applied. So please just include an appropriate sentence that makes this clear, e.g.:</p><p>Note that the model does not care about the sign (excitation/inhibition) and thus provides no systematic prediction of it. We hence switched the signs of the respective receptive fields of the model output appropriately to obtain receptive fields which all have positive leading excitation.</p><p>(3) Can you mention this alternative goal of least commitment somewhere in the discussion? And the empirical question.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.31557.035</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>The reviewers have discussed the reviews with one another, and they and the Reviewing Editor agree that your paper is potentially suitable for publication in eLife after appropriate revision. The Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>The Reviewing Editor hopes that you will address all of the concerns of the authors, most of which are straightforward. But to help you in revision the major issues are listed here:</p><p>1) If you look at the reviews you will see that the list of suggestions is very long, but the vast majority of the comments only ask for clarification, they do not require additional data analysis or substantial rewriting. It would be good if you could address all questions in your reply to reviewers and revise the text appropriately where necessary to provide necessary information to the reader.</p></disp-quote><p>We hope that our explanations in this document and clarifications throughout the main text of the paper address these points. We have addressed all of the comments in this document and made corresponding changes in the text.</p><disp-quote content-type="editor-comment"><p>2) The proposed model is interesting, but it has many components that may differentially contribute to the result, such as the nonlinearity or L<sub>1</sub> regularization. The reviewers felt that the paper would be stronger if the specific effects of these various model component choices were analyzed in a bit more detail, in order to try to pin down more precisely why these components are important. (See for example suggestions of reviewer 2.)</p></disp-quote><p>In preparing the original manuscript, we performed a grid search over the <italic>L</italic><sub>1</sub> regularization parameter and number of hidden units. The effect of changing these parameters on the predictive capacity of the model are shown for the auditory model in Figure 8. We did not previously show the effects of these parameters on the receptive field structure. We have now produced interactive figures which illustrate this for visual and auditory RFs (Figure 8—figure supplement 2 and Figure 8—figure supplement 3; <ext-link ext-link-type="uri" xlink:href="https://yossing.github.io/temporal_prediction_model/figures/interactive_supplementary_figures.html">https://yossing.github.io/temporal_prediction_model/figures/interactive_supplementary_figures.html</ext-link>). The effects of these parameters are now described in the main text (see paragraph below).</p><p>We have also explored the effects of the activation function (nonlinearity). We implemented versions of the network with tanh and rectified linear activation functions and found that the choice of nonlinearity does not have a decisive effect on RF structure However, if linear activation is used its RFs do not look like those seen in V1 or A1, as discussed in the main text (see paragraph below).</p><p>We have also explored other components of the model (see the response to point 3). However, two particular components of the model appear to be particularly important for prediction and having RFs that match the biology; having a non-linearity and having the correct amount of <italic>L</italic><sub>1</sub> weight regularization. We suspect that there are two reasons why having appropriate <italic>L</italic><sub>1</sub> regularization is likely to be important; first to avoid overfitting and hence find the most predictive code, and second to mimic the efficiency constraints on connectivity of the nervous system due to space and energy limitations. We suspect that a reason why having a nonlinearity is likely to be important, is that the future input depends non-linearly on the past input.</p><p>Reviewer #2 makes the interesting point that although the sparseness or independence of the hidden unit activities is not an explicit goal of the model, this may emerge implicitly in cases where the model’s RFs are most similar to the neural data (and where prediction error is lowest). To test for this, we measured the sparsity of the trained model’s hidden unit activities (by the measure of Vinje and Gallant, (2000)) in response to the natural input validation set. Examining the relationship between predictive capacity and sparsity, over a range of <italic>L</italic><sub>1</sub> weight regularization strength and hidden units, we do not find a clear monotonic relationship. Indeed, the hidden unit and <italic>L</italic><sub>1</sub> regularization combination with the best prediction was not the sparsest model, but of intermediate sparsity over the span we explored.</p><p>Addressing these points and others, we have now added a new section to the Results section titled “Variants of the temporal prediction model”. The relevant part at the start of this new section reads:</p><p>“The change in the qualitative structure of the RFs as a function of the number of hidden units and <italic>L</italic><sub>1</sub> regularization strength, for both the visual and auditory models, can be seen in the interactive supplementary figures (Figure 8—figure supplements 2-3; <ext-link ext-link-type="uri" xlink:href="https://yossing.github.io/temporal_prediction_model/figures/interactive_supplementary_figures.html">https://yossing.github.io/temporal_prediction_model/figures/interactive_supplementary_figures.html</ext-link>) [...] The RFs also did not change form or polarity over time, but simply decayed into the past.”.</p><disp-quote content-type="editor-comment"><p>3) In some places the choices made during modelling seemed arbitrary (e.g., the choice of temporal windows and the regularization parameters). Either grid search over a training set should be used to choose these parameters, or hyperparameter modelling should be performed to show that the specific values chosen are not critical, or the choices should be justified. The first of these is obviously most desirable.</p></disp-quote><p>We have explored the effects of all of the crucial parameters, and we explain these results in the new section “Variants on the temporal prediction model”.</p><p>Regularization parameter and hidden unit number: In all the model results shown in the original manuscript, we did not choose the values of these parameters, but used the parameters which provided the optimal prediction within a grid search (see response to point 2, above).</p><p>Temporal window into the past: We have explored the effect of different temporal windows. As can be seen in Figure 6B and Figure 7A, most of the energy of the real and model RFs is within 100ms (20 time steps) in the auditory case and within 160ms (4 time steps) in the visual case. Hence, the temporal window into the past was chosen to be slightly larger than these values, and so long as the window into the past is sufficiently long, it’s length is not a critical.</p><p>Temporal window into the future: We have explored the effect of different temporal windows into the future. We found that increasing the number of time steps being predicted had little effect on the RFs of the auditory model either qualitatively or by the KS similarity measure. In the visual case, it caused the RFs to be more restricted in space and increased the proportion of blob-like units.</p><p>Noise: In the Supplementary Material of the original manuscript we included figures showing that the effects of the input noise were subtle and not-critical to the form of RF that we see. We now discuss this in the main Results section as requested by reviewer #3.</p><p>Nonlinearity: Although a non-linearity is critical to achieve our results, with the RFs appearing quite different and less like those seen in V1 or A1 when no non-linearity is used, the exact choice of non-linearity (sigmoid, tanh, or rectified linear) was not critical, and the RFs seen were similar (see our response to point 2 above).</p><p>In addition to the parts of the new section given in response to the Editor’s previous point, the relevant part of this new section subsection “Visual normative models”) reads:</p><p>“The temporal prediction model and sparse coding model results shown in the main figures of this paper were trained on inputs with added Gaussian noise (6dB SNR), mimicking inherent noise in the nervous system. To determine the effect of adding this noise, all models were also trained without noise, producing similar results (Figure 4—figure supplements 5–7; Figure 5—figure supplements 3–5; Figure 6—figure supplement 1; Figure 7—figure supplements 2-3). The results were also robust to changes in the duration of the temporal window being predicted. We trained the auditory model to predict a span of either 1, 3, 6, or 9 time steps into the future and the visual model to predict 1, 3 or 6 time steps into the future. For the auditory case, we found that increasing the number of time steps being predicted had little effect on the RF structure, both qualitatively and by the KS measure of similarity to the real data. In the visual case, Gabor-like units were present in all cases. Increasing the number of time steps made the RFs more restricted in space and increased the proportion of blob-like RFs.”</p><disp-quote content-type="editor-comment"><p>4) Some of the figures are difficult to interpret (c.f. Figure 7B and Figure 7—figure supplement 2B). Please try to improve the figures where necessary.</p></disp-quote><p>We have changed these figures, as suggested by the reviewers, in order to make them more interpretable. We have added an additional panel to Figure 7 and corresponding supplementary figures showing a histogram of orientation tuning preferences for the model units. We have also added insets to Figure 6 and corresponding supplementary figures.</p><disp-quote content-type="editor-comment"><p>5) Please address the concerns of reviewer 3 regarding the introductory material on temporal prediction and the neurobiological plausibility of the approach.</p></disp-quote><p>We have now added the following paragraph to the Introduction on predictive normative models of sensory processing.</p><p>“The idea that prediction is an important component of perception dates at least as far back as Helmholtz<sup>18,19</sup>, although what is meant by prediction and the purpose it serves is quite varied between models incorporating it<sup>20,21</sup>. […] Our model relates to the predictive information approach in that it is optimized to predict the future from the past, but it has a combination of characteristics, such a non-linear encoder and sparse weight regularization, which have not previously been explored for such an approach.”</p><p>We have also extended the paragraph exploring the neurobiological plausibility of the model (subsection “Strengths and limitations of the temporal prediction model”). It now reads:</p><p>“Finally, it is interesting to consider possible more explicit biological bases for our model. We envisage the input units of the model as thalamic input, and the hidden units as primary cortical neurons. […] Finally, it is important to note that, although the biological plausibility of backpropagation has long been questioned, recent progress has been made in developing trainable networks that perform similarly to artificial neural networks trained with backpropagation, but with more biologically plausible characteristics<sup>77</sup>, for example, by having spikes or avoiding the weight transport problem<sup>78</sup>”</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>This manuscript compares two stimulus coding principles that could explain the form of receptive fields in sensory cortex: efficient sparse coding and temporal prediction. A simple temporal prediction model was found create receptive fields that were similar in many ways to those seen in sensory cortex. It performed much better than a sparse coding model.</p><p>This manuscript makes an interesting and important contribution. The &quot;sparse coding&quot; hypothesis is popular, both in neuroscience and in artificial intelligence, where autoencoding in deep-neural networks implements efficient coding. The authors argue that for dynamic stimuli, the need to predict what will happen next may be a more important than merely encoding efficiently what has happened recently. Their model is simple and elegant, and the results are convincing.</p></disp-quote><p>Thank you.</p><disp-quote content-type="editor-comment"><p>I felt there were some places where choices were made during modelling that seemed arbitrary – such as the choice of temporal windows and the regularization parameters. The manuscript would be stronger if these choices were either justified, or hyperparameter modelling done to show that the specific values chosen are not critical, to allay concerns readers may have of &quot;p-hacking&quot;.</p></disp-quote><p>In order to address this point, we have performed a grid search over a large space of hyperparameter values. The results of this search are outlined above in response to the editor’s third point. From these plots, and the associated text, one can see that there is substantial robustness to the exact modelling choices.</p><p>The duration of the temporal window into the past is relatively unimportant because most of the energy is in the most recent few steps (Figure 6B and Figure 7A), and so long as it is long enough to capture this span it suffices. We have now explored the effect of the duration of the temporal window into the future, and as we mention in our reply to point 3 of the editor, this has little effect on the RFs. The regularization parameter was chosen by selecting the value that best predicts the future of a held-out validation set (see Figure 8 and the corresponding subsection “Optimizing Predictive Capacity”.</p><disp-quote content-type="editor-comment"><p>I found the manuscript to be well-structured, thorough and well written, clearly conveying a convincing message.</p></disp-quote><p>Thank you.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The paper presents a two-layer network optimized for predicting immediate future sensory input (auditory or visual) from recent past sensory input. The resulting spatio- or spectrotemporal receptive fields, i.e. weight vectors of the first layer, are analyzed and compared with physiological receptive fields. For model comparison, results from a sparse coding network are used.</p><p>The results show that the predicting neural network captures receptive field properties fairly well, in particular temporal structure is reproduced much better than by the sparse coding network.</p><p>The topic is interesting, and the results are highly relevant to the field. I must add, however, that I have not followed the field recently, so I cannot really tell, whether some similar work has been published recently. But the authors seem to have done a careful literature research and discuss alternative approaches fairly.</p><p>The paper is well structured and has obviously been written very carefully. I have rarely reviewed a manuscript that feels so ready for publication. So, I am tempted to recommend the paper for publication as it is.</p></disp-quote><p>We appreciate the reviewer’s very positive comments.</p><disp-quote content-type="editor-comment"><p>There is just one issue I would invite the authors to consider a bit further: The claim of the paper is that the objective of temporal prediction results in the receptive field properties found. But there are additional factors, such as the nonlinearity and the L<sub>1</sub> regularization, that contribute to it. The authors have investigated this to some extent. For example, they find that receptive fields are seriously degraded if the nonlinearity is replaced by a linear activation function. My suggestion is to try to pin down, what objective is implicitly added by the nonlinearity and the L<sub>1</sub> regularization. I suggest to perform a similar experiment as in Figure 8, but with a sparseness or independence measure rather than final validation loss. This could also be done on the hidden units.</p><p>I suggest this, because I believe that temporal prediction alone does not do the trick. I feel it must be combined with some sparseness or independence objective to yield the receptive fields. And I feel that this missing objective is implicitly added by the nonlinearity and the L<sub>1</sub> regularization. Making this more transparent would be great and the suggested experiment should be very easy to do.</p></disp-quote><p>We have now examined the effects of these choices in some detail -- see our response to the second point of the Editor.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The authors propose a new principle for the development of cortical receptive fields which combines the concepts of predictive coding and sparse coding. They train a three-layer network with one hidden layer in order to predict the future visual spatial input or the future auditory auditory spectro-temporal input from the recent spatio-/spectro-temporal input, subject to a sparsity constraint.</p><p>They perform this training for two examples: for an auditory network, based on training data which contain human speech, animal vocalization and inanimate natural sounds, and for a visual network based on training data with movies of wildlife in natural settings.</p><p>They compare the resulting networks to real cortical neurons from A1 and V1 and to an alternative sparse coding approach intended to provide a sparse representation of the complete spatio/spectro-temporal input.</p><p>For their comparison they consider the spectrotemporal and spatiotemporal receptive fields and various population measures, e.g. the temporal decay of power in the receptive fields, the temporal span of excitation an inhibition, orientation and frequency tuning properties, and receptive field dimensions.</p><p>Except for orientation, for which the majority of visual units is restricted in their orientation preference to horizontal and vertical orientations, the proposed model can capture neural tuning properties as well as the established models. And in case of the asymmetric emphasis of the most recent past it can even provide a better description.</p><p>In my opinion this is a quite interesting paper. First, it presents a novel approach which unifies the principles of sparse coding and of temporal prediction. This combination enables the explanation of a large set of spatio-/spectro-temporal tuning properties within one single integrated framework. Second, the authors have an important point in stressing the asymmetry of the temporal response with its emphasis of the most recent past, as observed in typical cortical neurons. This is indeed a property that other learning schemes, like sparse coding, by the very nature of their objective functions, cannot produce.</p></disp-quote><p>Thank you.</p><disp-quote content-type="editor-comment"><p>There are some points that, in my view, need to be clarified or described in more detail. In the following I describe the modifications and additions which I assume to be helpful in a revision of the paper. Due to my background I will put more emphasis on the visual aspects.</p><p>1) The description of the history of the concept of temporal prediction is not clear enough, both in the introduction and in the discussion. I am aware of the pressure for novelty in current science but in my view, there is sufficient novelty in the suggested model to allow the authors to avoid such ambiguities. Currently, the paper might be misinterpreted by a swift non-specialist reader as if the concept of the &quot;prediction of the immediate future&quot; is a novel principle being introduced here (Introduction; Discussion section: &quot;We hypothesized&quot;). Only a few selected papers are cited directly in this context (only Bialek in the Introduction), and in the Discussion section they are characterized as unspecific: &quot;The temporal prediction principle we describe.… has been described in a very general manner&quot;(reference only to Bialek, Palmer). Other references exist but are spread out through the further text. But of course, the principle as such has a long history, there are numerous papers which describe the prediction of the future sensory input as an important goal of neural information processing. I am no specialist, and this is not comprehensive but early examples are corollary discharge theories, and already Sutton and Barto, (1981) and Srinivasan et al., (1982), for example, considered the temporal dimension of prediction. Motion extrapolation has also been interpreted as prediction computed in visual cortex (Nijhawan, 1994). A further, canonical example of a method for the optimal prediction of the future sensory input is the Kalman filter, as considered by, e.g., Rao, (1999). I suggest that the authors devote one paragraph to the history of the concept, with all the appropriate references included there, and then make precisely clear in which aspects their novel contribution extends beyond these earlier approaches.</p></disp-quote><p>We have added a new paragraph to the Introduction to address this. This paragraph points out the novel aspects of our model’s structure. See our response to the fifth point of the Editor. We have also expanded the final paragraph of the Introduction to set out a central novel contribution of our model, which is that it successfully explains temporal properties of both V1 and A1 neurons, something previous models have not been able to do. The expanded part of the final paragraph (Introduction) reads: “Here we show using qualitative and quantitative comparisons that, for both V1 and A1 RFs, these shortcomings are largely overcome by the temporal prediction approach. This stands in contrast to previous models”</p><disp-quote content-type="editor-comment"><p>2) Neurobiological plausibility of the approach: I do not think that the authors have to be as clear about the neural implementation of the suggested architecture as the other predictive coding approaches, but at least some rough or speculative ideas should be presented: What is the status of the second-order units? Where in the cortex are they (V2?) and what do they encode? Really the future INPUT itself? That is, they have no selectivity, no tuning properties? Have such units been observed? Where and how is the prediction error computed? Does this model not require a bypass line which brings the retinal spatial input directly to V1 or V2 to enable the comparison?.…</p></disp-quote><p>We have now addressed this more fully in the paper -- see our response to the fifth point of the Editor. We now describe how the output units of our model would represent the prediction of the input or the prediction error and note that signals relating to prediction error have been found in A1 (Rubin et al., 2016). These properties of the output units are non-linear and wouldn’t be fully captured by linear RF methods. We examined linear aspects of the tuning of the output units for the visual temporal prediction model using a response-weighted average to white noise input and found punctate un-oriented RFs that decay into the past. We suspect that no bypass line would be required, just appropriate temporal asymmetries in input and/or synaptic plasticity mechanisms.</p><disp-quote content-type="editor-comment"><p>3) The sparse coding data appear quite unusual. Why are the units not more &quot;localized&quot; in the temporal dimension, in particular for the visual model?</p></disp-quote><p>For the auditory data, our results are not unusual, see Carlson et al., (2012) and Carlin and Elhilali, (2013), where RFs that fully span the temporal dimension are common.</p><p>For the visual data, our results are also not unusual. If you examine the figure of van Hateran and Ruderman, (1998), they only show four units. It is not clear whether the units they show are reflective of the tuning properties of the entire population or have been selected because they are temporally localised. In the sparse coding results we presented, although we also see some units which are temporally localised, the majority were not. We have now run the sparse coding model in the overcomplete regime as is commonly done in the literature, and while the majority of spatiotemporal RFs are still not temporally localized, there are more examples that are. Notably, in a later paper looking at ICA of spatiotemporal visual inputs by Hyvärinen et al. (2003), which does show the full set of RFs, there are very few examples of units whose RFs are temporally localized, while the vast majority are not.</p><disp-quote content-type="editor-comment"><p>Furthermore, it seems as if the visual sparse model is not used in the usual overcomplete regime.</p></disp-quote><p>We have now run the sparse coding model with more hidden units and restricted the results presented in both the visual and auditory cases to configurations where the sparse coding models were run in the overcomplete regime. We have added a sentence to clarify this in the Materials and methods section. It reads:</p><p>“In both cases, the model configurations chosen were restricted to those trained in an overcomplete condition (having more units than the number of input variables) in order to remain consistent with previous instantiations of this model<sup>4,5,11</sup>…we selected a sparse coding network with 1600 units…in the auditory case (Figure 5 and Figure 6).In the visual case, the network selected was trained with 3200 units, λ=10<sup>0.5</sup>, learning rate = 0.05 and 100 mini-batches.”</p><disp-quote content-type="editor-comment"><p>I also would have expected a more concentrated distribution of the temporal span of excitation and inhibition for a typical sparse coding model. Please discuss this in the paper.</p></disp-quote><p>Our results are in keeping with previous studies in this regard (see Carlson et al., (2012) and Carlin and Elhilali, (2013)). We have added a sentence to the Results section highlighting this point. It reads: “The sparse coding model shows a wide range of temporal spans of excitation and inhibition, in keeping with previous studies<sup>11,48</sup>.”</p><disp-quote content-type="editor-comment"><p>4) Subsection “Model receptive fields” by inspection: is there really no other possibility to determine the optimal hyperparameters of the sparse coding model? A fit to the neural data? You have Figure 8—figure supplement 1B anyway. Why have you not made use of it? One could use only a training subset, if this seems critical issue. And one can include KS measures of other tuning properties.</p></disp-quote><p>This is a good point. We simply chose the ones that, by eye, presented the sparse coding model in the best light. However, if we choose the model in which RFs lie at the minimum by the KS measure in the auditory case, they do not look much different from the ones we chose. We have now changed the set of sparse coding model units in the auditory case to those that are most similar to the real neurons according to the KS measure, while still being overcomplete. In the visual case, as we do not perform a KS measure due to the limited amount of data, particularly pertaining to temporal response properties (see response to point sixteen below), this model was chosen by inspection. We have changed the text in subsection “Sparse coding model” to reflect this. It now reads: “…we chose the network that produced basis functions whose receptive fields were most similar to those of real neurons. In the auditory case, this was determined using the mean KS measure of similarity (Figure 8—figure supplement 1). In the visual case, as a similarity measure was not performed, this was done by inspection.”</p><disp-quote content-type="editor-comment"><p>5) Subsection “Addition of Gaussian noise”: Noise. For me the use of noise in this investigation is somewhat unclear. First, it seems to favor the prediction model over the sparse model, which is more susceptible to noise. Second, the noise level used appears unusually strong (is this dB?). This issue should be clearly motivated and discussed in the main text of the paper.</p></disp-quote><p>The temporal prediction and sparse coding models were also run without noise, and the results in all cases were similar. We have now added a paragraph in the new Results section which motivates and discusses the influence of the noise in the model as suggested by the reviewer. The paragraph in subsection “Variants of the temporal prediction model” reads: “The temporal prediction model and sparse coding model results shown in the main figures of this paper were trained on inputs with added Gaussian noise (6dB SNR), mimicking inherent noise in the nervous system. To determine the effect of adding this noise, all models were also trained without noise, producing similar results (Figure 4—figure supplements 5-7; Figure 5—figure supplements 3–5; Figure 6—figure supplement 1; Figure 7—figure supplements 2–3).”</p><p>We have also added clarifying text to the caption of Figure 7—figure supplement 2 highlighting the main effect of the addition of noise to the quantitative results in the visual case. It reads: “The addition of noise only leads to subtle changes in the RFs; most apparently, there are more units with RFs comprising multiple short subfields (forming an increased number of points towards the lower right quadrant of g) than is seen in the case when noise is used.”</p><p>We have also added text to the caption of Figure 6—figure supplement 1 highlighting the main effects of the addition of noise to the model in the auditory case. It reads: “The addition of noise leads to subtle changes in the RFs. Without noise, the inhibition in the temporal prediction model tends to be slightly less extended and the RFs a little less smooth (see Figure 4, Figure 4—figure supplement 5 for qualitative comparison).”</p><p>The addition of noise does not seem to negatively impact the sparse coding model’s results. For the sparse coding model in the visual case, the results are much the same with noise as without (Figure 5—figure supplement 1, Figure 5—figure supplement 2, Figure 5—figure supplement 4Figure 5—figure supplement 5 and Figure 7—figure supplement 2 and Figure 7—figure supplement 3). In the auditory case without added noise, the RFs tended to have somewhat smoother backgrounds, but were otherwise much the same in form as when noise was added (Figure 5, Figure 5—figure supplement 3). Quantitative comparison of the models trained on auditory inputs without added noise shows only subtle differences from the case with noise (Figure 6, Figure 6—figure supplement 1).</p><p>The signal-to-noise ratio (SNR) given was the variance of the signal divided by the variance of the noise, not in decibels. Apologies this was not more clear. We have now give the SNR in decibels, a more conventional measure. In decibels, the SNR is 6dB. Hence the noise is weaker than may have been implied.</p><disp-quote content-type="editor-comment"><p>6) Subsection “Model receptive fields”: I am a bit skeptic with respect to the sign-flipping of excitation and inhibition. The argument that the signs could as well be flipped if this is done for the first-order and the second-order units alike appears only valid because any specification and relation to real neurons is omitted for the second-order units. In fact, this sign-flipping will inevitably imply a prediction of how excitation and inhibition operate in the second-order units.</p><p>Furthermore, if this argumentation would be accepted then one could arbitrarily flip signs to the desired result in any learning model, because one can always argue that appropriate sign flips at some subsequent processing stages could compensate for this. Used in this way, excitation and inhibition would lose any meaning.</p><p>It is perfectly ok for me if a model is agnostic with respect to the correct prediction of excitation and inhibition, a model does not have to be perfect in all aspects. But if this is the case this should be clearly visible for the reader in the presented receptive field plots. (This does not exclude to use of an appropriate sign-flip in population measures.)</p></disp-quote><p>We are agnostic as to what the biological analogue of the output units is (see reply to the Editor’s fifth point). Therefore, the only units for which we make a direct comparison with data are the hidden units.</p><p>The sign flipping makes no difference to the function of the network, as we describe in subsection “Model receptive fields”. This degeneracy arises because, in the network, the units’ activation functions are symmetric, whereas for real neurons, high firing rates are meaningfully different from low ones. We therefore need to choose between two equivalent descriptions of the receptive fields. We agree that flipping the signs of each pixel in each RF or of each RF independently to maximize similarity to the data would be arbitrary and unjustified. However, this is not what we do. What we do instead is make all the units have positive leading excitation, which reflects the structure of the majority of cortical units and allows the reader to make a visual comparison to the data.</p><disp-quote content-type="editor-comment"><p>7) Figure 4—figure supplement 2 and Figure 4—figure supplement 3: Two separate populations? Visual inspection of these figures suggests the possible existence of two distinct populations. Is this related to separability, or blob-like units, or both? (Is ordering according to separability?) I am not sure about the current state in the field, but I remember a discussion about the existence of two distinct populations as opposed to a continuous distribution for separability. This issue should be described and discussed.</p></disp-quote><p>We discovered that some of the movie snippets we were using to train the model contained writing at a very high contrast. These examples have since been removed and the results updated with RFs obtained when the model was trained on inputs without any of these writing snippets included. In the updated results, the subpopulation of small blob-like units specified by the reviewer is no longer present (Figure 4—Figure supplement 2 and Figure 4—figure supplement 3).</p><disp-quote content-type="editor-comment"><p>8) ibid. The percentage of blob-like units appears quite high. Is this percentage comparable to the neural data? Or only if the mouse data, which are special in this respect, are being included?</p></disp-quote><p>See above – this subpopulation of small, blob-like units is no longer present in the results presented (Figure 4—Figure supplement 2 and Figure 4—figure supplement 3).</p><disp-quote content-type="editor-comment"><p>9) Figure 4—figure supplement 6 The percentage of blob-like units seems to be substantially reduced in comparison to the noisy case (Figure 4—figure supplement 2). Is there a systematical relation between high noise levels and the emergence of blob-like units? Please discuss this issue in the paper.</p></disp-quote><p>As mentioned in response to the previous two points, we discovered and removed visual training examples that contained writing. In the updated results, the subpopulation of small blob-like units that differentiated the results in the noise and non-noise cases is no longer present and the results are now more similar between the two cases.</p><disp-quote content-type="editor-comment"><p>10) ibid. It is difficult to understand the relation between Figure 4 supplement 2 and Figure 7c as opposed to figure 4—figure supplement 6 and Figure 7—figure supplement 2F. Can you describe how properties of the receptive field plots relate to properties of the population distribution in these two cases?</p></disp-quote><p>We now discuss these distributions in more detail. In subsection “Quantitative analysis of visual results”, we write: “The distribution of units extends along a curve from blob-like RFs, which lie close to the origin in this plot, to stretched RFs with several subfields, which lie further from the origin.” In the caption of Figure 7—figure supplement 2, we write “The addition of noise in both cases only leads to subtle changes in the RFs; most apparently, there are more units with RFs comprising multiple short subfields (forming an increased number of points towards the lower right quadrant of g) than is seen in the case when noise is used.”</p><disp-quote content-type="editor-comment"><p>11) Figure 7 and others Spatiotemporal population properties: Although the article is about spatiotemporal processing it provides only two population measures of purely static spatial properties and one of a purely temporal property for the vision case. Spatiotemporal measures of particular interest would be: DSI (directional selectivity index)/TDI (tilt direction index) (direction selectivity is considered to be a major spatiotemporal property of visual cortex); if possible: a scatter plot of temporal frequency vs. spatial frequency; population distribution of motion tuning. The necessary data for these plots should be already available.</p></disp-quote><p>We have now measured the TDI of the model population and added a section specifying the result in the main text (subsection “Quantitative analysis of visual results”). It reads: “In addition to this, we measured the tilt direction index (TDI) of the model units from their 2D spatiotemporal RFs. This index indicates spatiotemporal asymmetry in space-times RFs and correlates with direction selectivity.<sup>41,54–57</sup> The mean TDI for the population was 0.33 (0.29 SD), comparable with the ranges in the neural data (mean 0.16; 0.12 SD in cat area 17/18<sup>57</sup>, mean 0.51; 0.30 SD in macaque V1<sup>56</sup>).”</p><p>An explanatory paragraph has also been added to Materials and methods section. It reads: “The tilt direction index (TDI)<sup>41,54–57</sup> of an RF is given by (<italic>R<sub>p</sub></italic> – <italic>R<sub>q</sub>)/(R<sub>p</sub></italic> + R<sub>q</sub>), where <italic>R<sub>p</sub></italic> is the amplitude at the peak of the 2D Fourier transform of the 2D spatiotemporal RF, found at spatial frequency <italic>F</italic><sub>space</sub> and temporal frequency <italic>F</italic><sub>time</sub>. <italic>R<sub>q</sub></italic> is the amplitude at (<italic>F</italic><sub>space</sub>, -<italic>F</italic><sub>time</sub>) in the 2D Fourier transform. The mean and standard deviations of TDI for experimental data for the cat<sup>57</sup> and macaque<sup>56</sup> were measured from data extracted from figures in the relevant references (Figure 11A and the low-contrast axis of Figure 3A respectively).”</p><p>We also describe the relationship between the spatial and temporal frequency of the model units in the Results section.</p><disp-quote content-type="editor-comment"><p>12) Figure 7B and Figure 7—figure supplement 2B: The orientation scatter plot is visually difficult to interpret. The quantitative degree of concentration of the preferred orientations on the vertical and horizontal orientations as opposed to the oblique orientations remains unclear. Please provide either an orientation histogram or the percentage of units which fall into the 30-60, 120-150 deg range. In both cases the lowest spatial frequencies should be omitted for the analysis.</p></disp-quote><p>A histogram showing the distribution of orientation tuning preferences of the model units has now been added to the Figure 7 and corresponding supplementary figures.</p><disp-quote content-type="editor-comment"><p>13) Figure 7—figure supplement 1: I am a bit surprised that only 289/400 sparse-coding units can be fitted by a Gabor function. Why is this? Usually most sparse coding units have a good Gabor fit. And with such a high percentage excluded I see the risk of a systematic bias regarding certain tuning parameters.</p></disp-quote><p>Sparse coding of images produces mostly Gabor-like filters; however, even in this case, some filters are at the edge of the pixel grid and cannot be reliably fitted. When training on movies, Hyvarinen et al., (2003) also appears to show a similar proportion of units that are not Gabor-like when the full dataset is inspected. In van Hateren and Ruderman, (1998) they only show four example units, so it is difficult to assess how many of the units in the full population are Gabor-like.</p><disp-quote content-type="editor-comment"><p>14) Figure 7C and Figure 7—figure supplement 2F seem to indicate that the model produces two distinct sub-populations with respect to receptive field parameters n_x and n_y. It should be discussed whether this is the case, and if yes, whether it is systematically related to other parameters (selectivities) of the units. Could this be related related to the two apparent sub-populations regarding separability, blob-like shapes, cf. Figure 4—figure supplement 2? And has such a tendency has also been observed in neural data? In contrast, Ringach, (2004) claimed clustering around a one-dimensional curve. Please describe and discuss in text.</p></disp-quote><p>Examination of density plots of these figures suggest that the vast majority of units are spread around a one-dimensional curve, as is the case in Ringach, (2002). However, there is a slight wing formed by a small number of units that extends rightwards from the main curve, but this does not form a distinct separate cluster. We have added a sentence describing this subpopulation to the main text (subsection “Quantitative analysis of visual results”). It reads: “The distribution of units extends along a curve from blob-like RFs, which lie close to the origin in this plot, to stretched RFs with several subfields, which lie further from the origin. A small proportion of the population have RFs with several short subfields, forming a wing from the main curve in Figure 7D.”</p><p>As mentioned in response to points 7-9 of the reviewer above, the apparent subpopulation of small blob-like units is no longer present in the new results. Hence, the wing in Figure 7D is not related to this.</p><disp-quote content-type="editor-comment"><p>15) Figure 8 and subsection “Implementation details: Are we expected to see here that 1600 hidden units are a distinguished optimum? Does this figure not tell us that the prediction error does not substantially depend on the number? And when a biological system could achieve basically the same prediction quality with 100 neurons why should it then invest 1500 additional units for such a small advantage?</p></disp-quote><p>As the reviewer points out, in the auditory case, there is not a big change in the performance of the model or in the similarity of its RFs for an increase in the number of units, as can be seen from Figure 8. Nevertheless, a shallow minimum does exist when this parameter is varied and we took the network that gave an absolute minimum as measured by the mean squared error prediction on a held-out validation set so as to be unbiased in our selection process. Furthermore, in the visual case, the number of hidden units seems to play a bigger role both in the network’s performance on the prediction task and on the shapes of the RFs produced. This should now be clear from the additional interactive supplementary figure (Figure 8—Figure Supplement 3; <ext-link ext-link-type="uri" xlink:href="https://yossing.github.io/temporal_prediction_model/figures/interactive_supplementary_figures.html">https://yossing.github.io/temporal_prediction_model/figures/interactive_supplementary_figures.html</ext-link>).</p><disp-quote content-type="editor-comment"><p>16) Figure 8 and Figure 8—figure supplement 1. The comparison between the models prediction capability and the similarity of the model units to real units should not only be presented for the auditory neurons but also for the visual neurons (according to the text the data seem to be already available).</p></disp-quote><p>Our lab is primarily focused on auditory processing, hence, we were able to easily obtain recordings from a large population of neurons in A1. However, it was harder to find a large population of spatiotemporal RFs of V1 neurons to compare to, despite requests to multiple labs who focus on visual cortex. We were kindly provided with the spatiotemporal RF data from 8 neurons recorded in V1 of cats by Ohzawa et al., but did not feel that a meaningful quantitative comparison of the kind shown in Figure 8 could be performed with so few neurons.</p><disp-quote content-type="editor-comment"><p>17) Subsection “Optimising predictive capacity” It is not clear whether the similarity measure (only the span of temporal and frequency tuning is considered) is fair or biased for the comparison with the sparse coding model. What will happen, for example, if the distribution of orientation preference would be used instead (for the visual model)? Would then the sparse coding model appear more similar to the real neurons than the prediction model?</p><p>Please motivate and discuss.</p></disp-quote><p>In comparison to many other papers examining normative models of auditory RFs, we do far more in the way of quantitative comparisons both to other models and to the real data. We took the span of temporal and frequency tuning as reasonable measures of similarity although we concede that other measures could, of course, be chosen. It should be noted that in an attempt to be as fair as possible to the sparse coding model, we did not include measures of similarity (for instance of the proportion of power contained in each time step, as seen in Figure 6B) that seemed to obviously favour our model. Evidence for our hypothesis is also bolstered by the analysis of the effects of multidimensional scaling (Figure 6A), which is a nonparametric measure of similarity.</p><p>We have modified the Discussion section to highlight the point raised by the reviewer. The text now reads: “Finally, the more accurate the temporal prediction model is at prediction, the more its RFs tend to be like real neuronal RFs by the measures we use for comparison.”</p><p>For the visual data, we did not have sufficient data to compare with the models, particularly in the temporal domain, as we describe above. Quantitative comparison with measurements of data from different datasets and species, recorded using different methods, is no substitute for a single consolidated dataset, which we could not obtain from other labs. Hence, we decided it better to only do this for the auditory dataset and take a more descriptive approach for the visual model.</p><disp-quote content-type="editor-comment"><p>18) Discussion section: I cannot follow the argumentation that the class of prediction models (or this specific model) should somehow be unique with respect to the ability to provide an independent criterion for the selection of hyperparameters. Should this somehow be a principle, or a logical conclusion? Or an empirical observation only for this special case? Is it logically impossible that we find a measure, for example something entropy-related or whatever, for sparse coding that could have the same status? Please clarify.</p></disp-quote><p>It is conceivable that some independent reason for picking the hyperparameters for the sparse coding model could be found that also provides the most neural-like RFs. However, in the previous literature, no such measure is used or proposed. Instead, the hyperparameters, which can greatly affect the nature of the RF structures, are typically chosen in order to produce RFs that look most like the real neural data. Similarly, no obvious measure is apparent to us. To clarify our argument, the following sentence has been added to the end of the paragraph the reviewer refers to (subsection “Optimising predictive capacity”). It reads: “To our knowledge, no such effective, measurable, independent criterion for hyperparameter selection has been proposed for other normative models of RFs.”</p><disp-quote content-type="editor-comment"><p>19) Subsection “Visual normative models”:does not refer to prediction of the future: Why not? Is not the temporal prediction made in these models that the future spatial pattern is the same as the previous spatial pattern?</p></disp-quote><p>Because of the new paragraph in the Introduction that sets out what is meant by predictive coding and temporal prediction, we feel that this paragraph is now largely redundant. The paragraph and sentence to which the reviewer refers has therefore been removed.</p><disp-quote content-type="editor-comment"><p>20) Subsection “Visual normative models”: The model is selective, throws away information, as opposed to the information preserving properties of other models. But is this really a good strategy for *early* stages of a multi-stage information processing system? Does the principle of least commitment not suggest just the opposite strategy?</p></disp-quote><p>It could be true that the structure of sensory RFs is governed by the principle of least commitment, preserving all information. Conversely, it could be the case that temporal prediction is a more important principle governing the structure of sensory RFs. This is an empirical question. We hope that our results provide some good evidence in favour of the latter hypothesis.</p><disp-quote content-type="editor-comment"><p>21) For the Discussion section it would be of interest to consider the contributions from the two components of the prediction model, i.e., which properties of the units are genuinely caused by prediction and which are more due to the sparse coding part?</p></disp-quote><p>We have added a sentence to the Results section clarifying the nature of the sparsity in our model. It reads: “Note that this sparsity constraint differs from that used in sparse coding models, in that it is applied to the weights rather than the activity of the units, being more like a constraint on the wiring between neurons than a constraint on their firing rates.”</p><p>We have also examined the specific role of the regularization on the model, as shown in (Figure 8—figure supplement 2 and Figure 8—figure supplement 3;https://yossing.github.io/temporal_prediction_model/figures/interactive_supplementary_figures.html) and discussed this in detail in the responses to the Editor’s second and third points. We find that all three components of the model (prediction, <italic>L</italic><sub>1</sub> regularization of the weights and nonlinearity) are essential for providing V1 or A1 like RFs.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>The authors have addressed my comments in a careful manner. In particular, I appreciate that they made considerable and appropriate changes to text and figures instead of just providing arguments in the response letter. From my view, the manuscript is basically ready for publication. I have a few final suggestions.</p><p>1).… We examined linear aspects of the tuning of the output units for the visual temporal prediction model using a response-weighted average to white noise input, and found punctate un-oriented RFs that decay into the past..…</p><p>This is interesting. Can you mention this somewhere in the text?</p></disp-quote><p>We have now added the following sentence to the Results section.</p><p>“We examined linear aspects of the tuning of the output units for the visual temporal prediction model using a response-weighted average to white noise input, and found punctate non-oriented RFs that decay into the past.”</p><disp-quote content-type="editor-comment"><p>2) I understand that the model, by its very nature would not care about the sign. But the fact remains that you have an output of a model and you post hoc manipulate this output to obtain a &quot;better suited&quot; presentation (e.g., to ease comparison). My only point is that it should be totally clear to even a superficial reader that such a post hoc change has been applied. So please just include an appropriate sentence that makes this clear, e.g.:</p><p>Note that the model does not care about the sign (excitation/inhibition) and thus provides no systematic prediction of it. We hence switched the signs of the respective receptive fields of the model output appropriately to obtain receptive fields which all have positive leading excitation.</p></disp-quote><p>We have added the following text to the legend of Figure 2D:</p><p>“Note that the overall sign of a receptive field learned by the model is arbitrary. Hence, in all figures and analyses we multiplied each model receptive field by -1 where appropriate to obtain receptive fields which all have positive leading excitation (see Materials and methods section).”</p><disp-quote content-type="editor-comment"><p>(3) Can you mention this alternative goal of least commitment somewhere in the discussion? And the empirical question.</p></disp-quote><p>After further consideration our thoughts on this point have become more nuanced. The principle of least commitment requires not doing something that may later have to be undone. Whether the temporal prediction hypothesis is in conflict with least commitment is unclear, and a detailed exploration of this beyond the scope of this paper. According to the temporal prediction hypothesis, aspects of the past which never influence the future will never be of use to an animal, and thus it could be argued that not encoding those aspects will never need to be undone, and hence there is no conflict. However, specific models instantiating the temporal prediction may have limited capacity to identify predictive information, and thus may discard some information that may be useful in the future, and hence may run into conflict with the principle of least commitment. It is also the case that given limited brain capacities, at some point in the brain commitment is required, and the temporal prediction principle may provide a good mechanism to decide what to commit to representing and what to discard. Hence, it is a complicated empirical and theoretical question as to whether and when the principles are in conflict or congruence, and if in conflict under what conditions one is dominant. To reflect this more nuanced view we have how added the following text to the Discussion section:</p><p>“There is an open question as to whether the current model may eliminate some information that is useful for reconstruction of the past input or for prediction of higher order statistical properties of the future input, which might bring it into conflict with the principle of least commitment<sup>69</sup>. It is an empirical question how much organisms preserve information that is not predictive of the future, although there are theoretical arguments against such preservation<sup>2</sup>. Such conflict might be remedied, and the model improved, by adding feedback from higher areas or by adding an objective<sup>4–6,60</sup> to reconstruct the past or present in addition to predicting the future.”</p></body></sub-article></article>