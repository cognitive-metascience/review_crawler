<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">41690</article-id><article-id pub-id-type="doi">10.7554/eLife.41690</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Discovering and deciphering relationships across disparate data modalities</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-27358"><name><surname>Vogelstein</surname><given-names>Joshua T</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2487-6237</contrib-id><email>jovo@jhu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-121783"><name><surname>Bridgeford</surname><given-names>Eric W</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-121787"><name><surname>Wang</surname><given-names>Qing</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-121788"><name><surname>Priebe</surname><given-names>Carey E</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-121789"><name><surname>Maggioni</surname><given-names>Mauro</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-121790"><name><surname>Shen</surname><given-names>Cencheng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1030-1432</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Johns Hopkins University</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Child Mind Institute</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>University of Delaware</institution><addr-line><named-content content-type="city">Delaware</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Taylor</surname><given-names>Dane</given-names></name><role>Reviewing Editor</role><aff><institution>University of Buffalo</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>15</day><month>01</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e41690</elocation-id><history><date date-type="received" iso-8601-date="2018-09-03"><day>03</day><month>09</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2019-01-14"><day>14</day><month>01</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Vogelstein et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Vogelstein et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-41690-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.41690.001</object-id><p>Understanding the relationships between different properties of data, such as whether a genome or connectome has information about disease status, is increasingly important. While existing approaches can test whether two properties are related, they may require unfeasibly large sample sizes and often are not interpretable. Our approach, ‘Multiscale Graph Correlation’ (MGC), is a dependence test that juxtaposes disparate data science techniques, including k-nearest neighbors, kernel methods, and multiscale analysis. Other methods may require double or triple the number of samples to achieve the same statistical power as MGC in a benchmark suite including high-dimensional and nonlinear relationships, with dimensionality ranging from 1 to 1000. Moreover, MGC uniquely characterizes the latent geometry underlying the relationship, while maintaining computational efficiency. In real data, including brain imaging and cancer genetics, MGC detects the presence of a dependency and provides guidance for the next experiments to conduct.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.41690.002</object-id><title>eLife digest</title><p>If you want to estimate whether height is related to weight in humans, what would you do? You could measure the height and weight of a large number of people, and then run a statistical test. Such ‘independence tests’ can be thought of as a screening procedure: if the two properties (height and weight) are not related, then there is no point in proceeding with further analyses.</p><p>In the last 100 years different independence tests have been developed. However, classical approaches often fail to accurately discern relationships in the large, complex datasets typical of modern biomedical research. For example, connectomics datasets include tens or hundreds of thousands of connections between neurons that collectively underlie how the brain performs certain tasks. Discovering and deciphering relationships from these data is currently the largest barrier to progress in these fields. Another drawback to currently used methods of independence testing is that they act as a ‘black box’, giving an answer without making it clear how it was calculated. This can make it difficult for researchers to reproduce their findings – a key part of confirming a scientific discovery. Vogelstein et al. therefore sought to develop a method of performing independence tests on large datasets that can easily be both applied and interpreted by practicing scientists.</p><p>The method developed by Vogelstein et al., called Multiscale Graph Correlation (MGC, pronounced ‘magic’), combines recent developments in hypothesis testing, machine learning, and data science. The result is that MGC typically requires between one half to one third as big a sample size as previously proposed methods for analyzing large, complex datasets. Moreover, MGC also indicates the nature of the relationship between different properties; for example, whether it is a linear relationship or not.</p><p>Testing MGC on real biological data, including a cancer dataset and a human brain imaging dataset, revealed that it is more effective at finding possible relationships than other commonly used independence methods. MGC was also the only method that explained how it found those relationships.</p><p>MGC will enable relationships to be found in data across many fields of inquiry – and not only in biology. Scientists, policy analysts, data journalists, and corporate data scientists could all use MGC to learn about the relationships present in their data. To that extent, Vogelstein et al. have made the code open source in MATLAB, R, and Python.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>statistics</kwd><kwd>machine learning</kwd><kwd>data science</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Child Mind Institute</institution></institution-wrap></funding-source><award-id>Endeavor Scientist Program</award-id><principal-award-recipient><name><surname>Vogelstein</surname><given-names>Joshua T</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Vogelstein</surname><given-names>Joshua T</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000185</institution-id><institution>Defense Advanced Research Projects Agency</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Vogelstein</surname><given-names>Joshua T</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Vogelstein</surname><given-names>Joshua T</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000181</institution-id><institution>Air Force Office of Scientific Research</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Vogelstein</surname><given-names>Joshua T</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Multiscale Graph Correlation, an interpretable hypothesis test with strong theoretical guarantees for discerning relationships in complex data, requires about half the sample size as other methods, whilst maintaining computational tractability.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Identifying the existence of a relationship between a pair of properties or modalities is the critical initial step in data science investigations. Only if there is a statistically significant relationship does it make sense to try to decipher the nature of the relationship. Discovering and deciphering relationships is fundamental, for example, in high-throughput screening (<xref ref-type="bibr" rid="bib94">Zhang et al., 1999</xref>), precision medicine (<xref ref-type="bibr" rid="bib56">Prescott, 2013</xref>), machine learning (<xref ref-type="bibr" rid="bib32">Hastie et al., 2001</xref>), and causal analyses (<xref ref-type="bibr" rid="bib54">Pearl, 2000</xref>). One of the first approaches for determining whether two properties are related to—or statistically dependent on—each other is Pearson’s Product-Moment Correlation (published in 1895; <xref ref-type="bibr" rid="bib55">Pearson, 1895</xref>). This seminal paper prompted the development of entirely new ways of thinking about and quantifying relationships (see <xref ref-type="bibr" rid="bib57">Reimherr and Nicolae, 2013</xref> and <xref ref-type="bibr" rid="bib40">Josse and Holmes, 2013</xref> for recent reviews and discussion). Modern datasets, however, present challenges for dependence-testing that were not addressed in Pearson’s era. First, we now desire methods that can correctly detect any kind of dependence between all kinds of data, including high-dimensional data (such as ’omics), structured data (such as images or networks), with nonlinear relationships (such as oscillators), even with very small sample sizes as is common in modern biomedical science. Second, we desire methods that are interpretable by providing insight into how or why they discovered the presence of a statistically significant relationship. Such insight can be a crucial component of designing the next computational or physical experiment.</p><p>While many statistical and machine learning approaches have been developed over the last 120 years to combat aspects of the first issue—detecting dependencies—no approach satisfactorily addressed the challenges across all data types, relationships, and dimensionalities. Hoeffding and Renyi proposed non-parametric tests to address nonlinear but univariate relationships (<xref ref-type="bibr" rid="bib36">Hoeffding, 1948</xref>; <xref ref-type="bibr" rid="bib58">Rényi, 1959</xref>). In the 1970s and 1980s, nearest neighbor style approaches were popularized (<xref ref-type="bibr" rid="bib24">Friedman and Rafsky, 1983</xref>; <xref ref-type="bibr" rid="bib65">Schilling, 1986</xref>), but they were sensitive to algorithm parameters resulting in poor empirical performance. ‘Energy statistics’, and in particular the distance correlation test (D<sc>corr</sc>), was recently shown to be able to detect any dependency with sufficient observations, at arbitrary dimensions, and structured data under a proper distance metric (<xref ref-type="bibr" rid="bib76">Székely et al., 2007</xref>; <xref ref-type="bibr" rid="bib79">Székely and Rizzo, 2009</xref>; <xref ref-type="bibr" rid="bib80">Szekely and Rizzo, 2013</xref>; <xref ref-type="bibr" rid="bib51">Lyons, 2013</xref>). Another set of methods, referred to a ‘kernel mean embedding’ approaches, including the Hilbert Schmidt Independence Criterion (H<sc>sic</sc>) (<xref ref-type="bibr" rid="bib30">Gretton and Gyorfi, 2010</xref>; <xref ref-type="bibr" rid="bib53">Muandet et al., 2017</xref>), have the same theoretical guarantees, which is shown to be a kernel version of the energy statistics (<xref ref-type="bibr" rid="bib66">Sejdinovic et al., 2013</xref>; <xref ref-type="bibr" rid="bib70">Shen and Vogelstein, 2018</xref>). The energy statistics can perform very well with a relatively small sample size on high-dimensional linear data, whereas the kernel methods and another test (Heller, Heller, and Gorfine’s test, H<sc>hg</sc>) (<xref ref-type="bibr" rid="bib34">Heller et al., 2013</xref>) perform well on low-dimensional nonlinear data. But no test performs particularly well on high-dimensional nonlinear data with typical sample sizes, which characterizes a large fraction of real data challenges in the current big data era.</p><p>Moreover, to our knowledge, existing dependency tests do not attempt to further characterize the dependency structure. On the other hand, much effort has been devoted to characterizing ‘point cloud data’, that is, summarizing certain global properties in unsupervised settings (for example, having genomics data, but no disease data). Classic examples of such approaches include Fourier (<xref ref-type="bibr" rid="bib8">Bracewell and Bracewell, 1986</xref>) and wavelet analysis (<xref ref-type="bibr" rid="bib18">Daubechies, 1992</xref>). More recently, topological and geometric data analysis compute properties of graphs, or even higher order simplices (<xref ref-type="bibr" rid="bib20">Edelsbrunner and Harer, 2009</xref>). Such methods build multiscale characterization of the samples, much like recent developments in harmonic analysis (<xref ref-type="bibr" rid="bib11">Coifman and Maggioni, 2006</xref>; <xref ref-type="bibr" rid="bib2">Allard et al., 2012</xref>). However, these tools typically lack statistical guarantees under noisy observations and are often computationally burdensome.</p><p>We surmised that both (i) empirical performance in all dependency structures, in particular high-dimensional, nonlinear, low-sample size settings, and (ii) providing insight into the discovery process, can be addressed via extending existing dependence tests to be <italic>adaptive</italic> to the data (<xref ref-type="bibr" rid="bib95">Zhang et al., 2012</xref>). Existing tests rely on a fixed <italic>a priori</italic> selection of an algorithmic parameter, such as the kernel bandwidth (<xref ref-type="bibr" rid="bib29">Gretton et al., 2006</xref>), intrinsic dimension (<xref ref-type="bibr" rid="bib2">Allard et al., 2012</xref>), and/or local scale (<xref ref-type="bibr" rid="bib24">Friedman and Rafsky, 1983</xref>; <xref ref-type="bibr" rid="bib65">Schilling, 1986</xref>). Indeed, the Achilles Heel of manifold learning has been the requirement to manually choose these parameters (<xref ref-type="bibr" rid="bib49">Levina and Bickel, 2004</xref>). Post-hoc cross-validation is often used to make these methods effectively adaptive, but doing so adds an undesirable computational burden and may weaken or destroy any statistical guarantees. There is therefore a need for statistically valid and computationally efficient adaptive methods.</p><p>To illustrate the importance of adapting to different kinds of relationships, consider a simple illustrative example: investigate the relationship between cloud density and grass wetness. If this relationship were approximately linear, the data might look like those in <xref ref-type="fig" rid="fig1">Figure 1A</xref> (top). On the other hand, if the relationship were nonlinear—such as a spiral—it might look like those in <xref ref-type="fig" rid="fig1">Figure 1A</xref> (bottom). Although the relationship between clouds and grass is unlikely to be spiral, spiral relationships are prevalent in nature and mathematics (for example, shells, hurricanes, and galaxies), and are canonical in evaluations of manifold learning techniques (<xref ref-type="bibr" rid="bib48">Lee and Verleysen, 2007</xref>), thereby motivating its use here.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.41690.003</object-id><label>Figure 1.</label><caption><title>Illustration of Multiscale Graph Correlation (M<sc>gc</sc>) on simulated cloud density (<inline-formula><mml:math id="inf1"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>) and grass wetness (<inline-formula><mml:math id="inf2"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>).</title><p>We present two different relationships: linear (top) and nonlinear spiral (bottom; see Materials and methods for simulation details). (<bold>A</bold>) Scatterplots of the raw data using <inline-formula><mml:math id="inf3"><mml:mn>50</mml:mn></mml:math></inline-formula> pairs of samples for each scenario. Samples <inline-formula><mml:math id="inf4"><mml:mn>1</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:mn>2</mml:mn></mml:math></inline-formula>, and <inline-formula><mml:math id="inf6"><mml:mn>3</mml:mn></mml:math></inline-formula> (black) are highlighted; arrows show <inline-formula><mml:math id="inf7"><mml:mi>x</mml:mi></mml:math></inline-formula> distances between these pairs of points while their <inline-formula><mml:math id="inf8"><mml:mi>y</mml:mi></mml:math></inline-formula> distances are almost 0. (<bold>B</bold>) Scatterplots of all pairs of distances comparing <inline-formula><mml:math id="inf9"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"><mml:mi>y</mml:mi></mml:math></inline-formula> distances. Distances are linearly correlated in the linear relationship, whereas they are not in the spiral relationship. D<sc>corr</sc> uses all distances (gray dots) to compute its test statistic and p-value, whereas M<sc>gc</sc> chooses the local scale and then uses only the local distances (green dots). (<bold>C</bold>) Heatmaps characterizing the strength of the generalized correlation at all possible scales (ranging from <inline-formula><mml:math id="inf11"><mml:mn>2</mml:mn></mml:math></inline-formula> to <inline-formula><mml:math id="inf12"><mml:mi>n</mml:mi></mml:math></inline-formula> for both <inline-formula><mml:math id="inf13"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). For the linear relationship, the global scale is optimal, which is the scale that M<sc>gc</sc> selects and results in a p-value identical to D<sc>corr</sc>. For the nonlinear relationship, the optimal scale is local in both <inline-formula><mml:math id="inf15"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:mi>y</mml:mi></mml:math></inline-formula>, so M<sc>gc</sc> achieves a far larger test statistic, and a correspondingly smaller and significant p-value. Thus, M<sc>gc</sc> uniquely detects dependence and characterizes the geometry in both relationships.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41690-fig1-v2.tif"/></fig><p>Under the linear relationship (top panels), when a pair of observations are close to each other in cloud density, they also tend to be close to each other in grass wetness (for example, observations 1 and 2 highlighted in black in <xref ref-type="fig" rid="fig1">Figure 1A</xref>, and distances between them in <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Similarly, when a pair of observations are far from each other in cloud density, they also tend to be far from each other in grass wetness (see for example, distances between observations 2 and 3). On the other hand, consider the nonlinear (spiral) relationship (bottom panels). Here, when a pair of observations are close to each other in cloud density, they also tend to be close to each other in grass wetness (see points 1 and 2 again). However, the same is not true for large distances (see points 2 and 3). Thus, in the linear relationship, the distance between every pair of points is informative with respect to the relationship, while under the nonlinear relationship, only a subset of the distances are.</p><p>For this reason, we juxtapose nearest neighbor mechanism with distance methods. Specifically, for each point, we find its <inline-formula><mml:math id="inf17"><mml:mi>k</mml:mi></mml:math></inline-formula>-nearest neighbors for one property (e.g. cloud density), and its <inline-formula><mml:math id="inf18"><mml:mi>l</mml:mi></mml:math></inline-formula>-nearest neighbors for the other property (e.g. grass wetness); we call the pair <inline-formula><mml:math id="inf19"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> the ‘scale’. <italic>A priori</italic>, however, we do not know which scales will be most informative. We compute all distance pairs, then efficiently compute the distance correlations for all scales. The local correlations (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, described in detail below) illustrate which scales are relatively informative about the relationship. The key, therefore, to successfully discover and decipher relationships between disparate data modalities is to adaptively determine which scales are the most informative, and the geometric implication for the most informative scales. Doing so not only provides an estimate of whether the modalities are related, but also provides insight into how the determination was made. This is especially important in high-dimensional data, where simple visualizations do not reveal relationships to the unaided human eye.</p><p>Our method, ‘Multiscale Graph Correlation’ (M<sc>gc</sc>, pronounced ‘magic’), generalized and extends previously proposed pairwise comparison-based approaches by adaptively estimating the informative scales for any relationship — linear or nonlinear, low-dimensional or high-dimensional, unstructured or structured—in a computationally efficient and statistically valid and consistent fashion. This adaptive nature of M<sc>gc</sc> effectively guarantees an improved statistical performance. Moreover, the dependency strength across all scales is informative about the structure of a statistical relationship, therefore providing further guidance for subsequent experimental or analytical steps. M<sc>gc</sc> is thus a hypothesis-testing and insight-providing approach that builds on recent developments in manifold and kernel learning, with complementary developments in nearest-neighbor search, and multiscale analyses.</p><sec id="s1-1"><title>The multiscale graph correlation procedure</title><p>M<sc>gc</sc> is a multi-step procedure to discover and decipher dependencies across disparate data modalities or properties. Given <inline-formula><mml:math id="inf20"><mml:mi>n</mml:mi></mml:math></inline-formula> samples of two different properties, proceed as follows (see Materials and methods and (<xref ref-type="bibr" rid="bib69">Shen et al., 2018</xref>) for details):</p><list list-type="order"><list-item><p>Compute two distance matrices, one consisting of distances between all pairs of one property (e.g. cloud densities, entire genomes or connectomes) and the other consisting of distances between all pairs of the other property (e.g. grass wetnesses or disease status). Then center each distance matrix (by subtracting its overall mean, the column-wise mean from each column, and the row-wise mean from each row), and denote the resulting n-by-n matrices <inline-formula><mml:math id="inf21"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf22"><mml:mi>B</mml:mi></mml:math></inline-formula>.</p></list-item><list-item><p>For all possible values of <inline-formula><mml:math id="inf23"><mml:mi>k</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:mi>l</mml:mi></mml:math></inline-formula> from <inline-formula><mml:math id="inf25"><mml:mn>1</mml:mn></mml:math></inline-formula> to <inline-formula><mml:math id="inf26"><mml:mi>n</mml:mi></mml:math></inline-formula>:</p><list list-type="alpha-lower"><list-item><p>Compute the <inline-formula><mml:math id="inf27"><mml:mi>k</mml:mi></mml:math></inline-formula>-nearest neighbor graphs for one property, and the <inline-formula><mml:math id="inf28"><mml:mi>l</mml:mi></mml:math></inline-formula>-nearest neighbor graphs for the other property. Let <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>G</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula> be the adjacency matrices for the nearest neighbor graphs, so that <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates that <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is within the <inline-formula><mml:math id="inf33"><mml:mi>k</mml:mi></mml:math></inline-formula> smallest values of the <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> row of <inline-formula><mml:math id="inf35"><mml:mi>A</mml:mi></mml:math></inline-formula>, and similarly for <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula>.</p></list-item><list-item><p>Estimate the local correlations—the correlation between distances restricted to only the <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> neighbors—by summing the products of the above matrices, <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>Normalize <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> such that the result is always between <inline-formula><mml:math id="inf40"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf41"><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> by dividing by<inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></p></list-item></list></list-item><list-item><p>Estimate the optimal local correlation <inline-formula><mml:math id="inf43"><mml:msup><mml:mi>c</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> by finding the smoothed maximum of all local correlations <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Smoothing mitigates biases and provides M<sc>gc</sc> with theoretical guarantee and better finite-sample performance.</p></list-item><list-item><p>Determine whether the relationship is significantly dependent—that is, whether <inline-formula><mml:math id="inf45"><mml:msup><mml:mi>c</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> is more extreme than expected under the null—via a permutation test. The permutation procedure repeats steps 1–4 on each permutation, thereby eliminating the multiple hypothesis testing problem by only computing one overall p-value, rather than one p-value per scale, ensuring that it is a valid test (meaning that the false positive rate is properly controlled at the specified type I error rate).</p></list-item></list><p>Computing all local correlations, the test statistic, and the p-value requires <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> time, which is about the same running time complexity as other methods (<xref ref-type="bibr" rid="bib69">Shen et al., 2018</xref>).</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>M<sc>gc</sc> typically requires substantially fewer samples to achieve the same power across all dependencies and dimensions</title><p>When, and to what extent, does M<sc>gc</sc> outperform other approaches, and when does it not? To address this question, we formally pose the following hypothesis test (see Materials and methods for details):<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mspace width="thickmathspace"/><mml:mi>X</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>Y</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mspace width="thickmathspace"/><mml:mi>X</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>Y</mml:mi><mml:mspace width="thinmathspace"/><mml:mtext>are not independent</mml:mtext><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The standard criterion for evaluating statistical tests is the testing power, which equals the probability that a test correctly rejects the null hypothesis at a given type one error level, that is power = Prob(<inline-formula><mml:math id="inf47"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is rejected <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is false). The higher the testing power, the better the test procedure. A consistent test has power converging to <inline-formula><mml:math id="inf49"><mml:mn>1</mml:mn></mml:math></inline-formula> under dependence, and a valid test controls the type one error level under independence. In a complementary manuscript (<xref ref-type="bibr" rid="bib69">Shen et al., 2018</xref>), we established the theoretical properties of M<sc>gc</sc>, proving its validity and universal consistency for dependence testing against all distributions of finite second moments.</p><p>Here, we address the empirical performance of M<sc>gc</sc> as compared with multiple popular tests: (i) D<sc>corr</sc>, a popular approach from the statistics community (<xref ref-type="bibr" rid="bib76">Székely et al., 2007</xref>; <xref ref-type="bibr" rid="bib79">Székely and Rizzo, 2009</xref>), (ii) M<sc>corr</sc>, a modified version of D<sc>corr</sc> designed to be unbiased for sample data (<xref ref-type="bibr" rid="bib80">Szekely and Rizzo, 2013</xref>), (iii) H<sc>hg</sc>, a distance-based test that is very powerful for detecting low-dimensional nonlinear relationships (<xref ref-type="bibr" rid="bib34">Heller et al., 2013</xref>). (iv) H<sc>sic</sc>, a kernel dependency measure (<xref ref-type="bibr" rid="bib30">Gretton and Gyorfi, 2010</xref>) formulated in the same way as D<sc>corr</sc> except operating on kernels, (v) M<sc>antel</sc>, which is historically widely used in biology and ecology (<xref ref-type="bibr" rid="bib52">Mantel, 1967</xref>). (vi) RV coefficient (<xref ref-type="bibr" rid="bib55">Pearson, 1895</xref>; <xref ref-type="bibr" rid="bib40">Josse and Holmes, 2013</xref>), which is a multivariate generalization of P<sc>earson’</sc>s product moment correlation whose test statistic is the sum of the trace-norm of the cross-covariance matrix, and (vii) the C<sc>ca</sc> method, which is the largest (in magnitude) singular value of the cross-covariance matrix, and can be viewed as a different generalization of P<sc>earson</sc> in high-dimensions that is more appropriate for sparse settings (<xref ref-type="bibr" rid="bib37">Hotelling, 1936</xref>; <xref ref-type="bibr" rid="bib89">Witten et al., 2009</xref>; <xref ref-type="bibr" rid="bib90">Witten and Tibshirani, 2011</xref>). Note that while we focus on high-dimensional settings, Appendix 1 shows further results in one-dimensional settings, also comparing to a number of tests that are limited to one dimension, including: (viii) P<sc>earson</sc>’s product moment correlation, (ix) S<sc>pearman</sc>’s rank correlation (<xref ref-type="bibr" rid="bib72">Spearman, 1904</xref>), (x) K<sc>endall</sc>’s tau correlation (<xref ref-type="bibr" rid="bib43">Kendall, 1970</xref>), and (xi) M<sc>ic</sc> (<xref ref-type="bibr" rid="bib59">Reshef et al., 2011</xref>). Under the regularity condition that the data distribution has finite second moment, the first four tests are universally consistent, whereas the other tests are not.</p><p>We generate an extensive benchmark suite of 20 relationships, including different polynomial (linear, quadratic, cubic), trigonometric (sinusoidal, circular, ellipsoidal, spiral), geometric (square, diamond, W-shape), and other functions. This suite includes and extends the simulated settings from previous dependence testing work (<xref ref-type="bibr" rid="bib76">Székely et al., 2007</xref>; <xref ref-type="bibr" rid="bib71">Simon and Tibshirani, 2012</xref>; <xref ref-type="bibr" rid="bib28">Gorfine et al., 2012</xref>; <xref ref-type="bibr" rid="bib34">Heller et al., 2013</xref>; <xref ref-type="bibr" rid="bib80">Szekely and Rizzo, 2013</xref>). For many of them, we introduce high-dimensional variants, to more extensively evaluate the methods; function details are in Materials and methods. The visualization of one-dimensional noise-free (black) and noisy (gray) samples is shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. For each relationship, we compute the power of each method relative to M<sc>gc</sc> for ~20 different dimensionalities, ranging from 1 up to 10, 20, 40, 100, or 1000. The high-dimensional relationships are more challenging because (1) they cannot be easily visualized and (2) each dimension is designed to have less and less signal, so there are many noisy dimensions. <xref ref-type="fig" rid="fig2">Figure 2</xref> shows that M<sc>gc</sc> achieves the highest (or close to the highest) power given 100 samples for each relationship and dimensionality. <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> shows the same advantage in one-dimension with increasing sample size.</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.41690.004</object-id><label>Figure 2.</label><caption><title>An extensive benchmark suite of 20 different relationships spanning polynomial, trigonometric, geometric, and other relationships demonstrates that M<sc>gc</sc> empirically nearly dominates eight other methods across dependencies and dimensionalities ranging from 1 to 1000 (see Materials and methods and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for details).</title><p>Each panel shows the testing power of other methods relative to the power of M<sc>gc</sc> (e.g. power of M<sc>corr</sc> minus the power of M<sc>gc</sc>) at significance level <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> versus dimensionality for <inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>. Any line below zero at any point indicates that that method’s power is less than M<sc>gc</sc>’s power for the specified setting and dimensionality. M<sc>gc</sc> achieves empirically better (or similar) power than all other methods in almost all relationships and all dimensions. For the independent relationship (#20), all methods yield power <inline-formula><mml:math id="inf52"><mml:mn>0.05</mml:mn></mml:math></inline-formula> as they should. Note that M<sc>gc</sc> is always plotted ‘on top’ of the other methods, therefore, some lines are obscured.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41690-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.41690.005</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Visualization of the <inline-formula><mml:math id="inf53"><mml:mn>20</mml:mn></mml:math></inline-formula> dependencies at <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</title><p>For each, <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> points are sampled with noise (<inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) to show the actual sample data used for one-dimensional relationships (gray dots). For comparison purposes, <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula> points are sampled without noise (<inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) to highlight each underlying dependency (black dots). Note that only black points are plotted for type 19 and 20, as they do not have the noise parameter <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41690-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.41690.006</object-id><label>Figure 2—figure supplement 2.</label><caption><title>The same power plots as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, except the 20 dependencies are one-dimensional with noise, and the x-axis shows sample size increasing from 5 to 100.</title><p>M<sc>gc</sc> empirically achieves similar or better power than the previous state-of-the-art approaches on most problems. Note that M<sc>ic</sc> is included in 1D case; RV and C<sc>ca</sc> both equal P<sc>earson</sc> in 1D; K<sc>endall</sc> and S<sc>pearman</sc> are too similar to P<sc>earson</sc> in power and thus omitted in plotting.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41690-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.41690.007</object-id><label>Figure 2—figure supplement 3.</label><caption><title>The same set-ups as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, comparing different M<sc>gc</sc> implementations versus its global counterparts.</title><p>The default M<sc>gc</sc> builds upon M<sc>corr</sc> throughout the paper, and we further consider M<sc>gc</sc> on M<sc>antel</sc> to illustrate the generalization. The magenta line shows the power difference between M<sc>corr</sc> and M<sc>gc</sc> , and the cyan line shows the power difference between M<sc>antel</sc> and the M<sc>gc</sc> version of M<sc>antel</sc>. Indeed, M<sc>gc</sc> is able to improve the global counterpart in testing power under nonlinear dependencies, and maintains similar power under linear and independent dependencies.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41690-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.41690.008</object-id><label>Figure 2—figure supplement 4.</label><caption><title>The same power plots as in <xref ref-type="fig" rid="fig3">Figure 3</xref>, except the 20 dependencies are one-dimensional with noise, and the x-axis shows sample size increasing from 5 to 100.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41690-fig2-figsupp4-v2.tif"/></fig></fig-group><p>Moreover, for each relationship and each method we compute the required sample size to achieve power 85% at error level 0.05, and summarize the median size for monotone relationships (type 1–5) and non-monotone relationships (type 6–19) in <xref ref-type="table" rid="table1">Table 1</xref>. Other methods typically require double or triple the number of samples as M<sc>gc</sc> to achieve the same power. More specifically, traditional correlation methods (P<sc>earson</sc>, RV, C<sc>ca</sc>, S<sc>pearman</sc>, K<sc>endall</sc>) always perform the best in monotonic simulations, distance-based methods including M<sc>corr</sc>, D<sc>corr</sc>, M<sc>gc</sc>, H<sc>hg</sc> and H<sc>sic</sc> are slightly worse, while M<sc>ic</sc> and M<sc>antel</sc> are the worst. M<sc>gc</sc>’s performance is equal to linear methods on monotonic relationships. For non-monotonic relationships, traditional correlations fail to detect the existence of dependencies, D<sc>corr</sc>, M<sc>corr</sc>, and M<sc>ic</sc>, do reasonably well, but H<sc>hg</sc> and M<sc>gc</sc> require the fewest samples. In the high-dimensional non-monotonic relationships that motivated this work, and are common in biomedicine, M<sc>gc</sc> significantly outperforms other methods. The second best test that is universally consistent (H<sc>hg</sc>) requires nearly double as many samples as M<sc>gc</sc>, demonstrating that M<sc>gc</sc> could half the time and cost of experiments designed to discover relationships at a given effect size.</p><p>M<sc>gc</sc> extends previously proposed global methods, such as M<sc>antel</sc> and D<sc>corr</sc> . The above experiments extended M<sc>corr</sc> , because M<sc>corr</sc> is universally consistent and an unbiased version of D<sc>corr</sc> (<xref ref-type="bibr" rid="bib80">Szekely and Rizzo, 2013</xref>). <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> directly compares multiscale generalizations of M<sc>antel</sc> and M<sc>corr</sc> as dimension increases, demonstrating that empirically, M<sc>gc</sc> nearly dominates its global variant for essen- tially all dimensions and simulation settings considered here. <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref> shows a similar result for one-dimensional settings while varying sample size. Thus, not only does M<sc>gc</sc> empirically nearly dominate existing tests, it is a framework that one can apply to future tests to further improve their performance.</p><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.41690.009</object-id><label>Table 1.</label><caption><title>The median sample size for each method to achieve power 85% at type one error level 0.05, grouped into monotone (type 1–5) and non-monotone relationships (type 6–19) for both one- and ten-dimensional settings, normalized by the number of samples required by M<sc>gc</sc>.</title><p>In other words, a 2.0 indicates that the method requires double the sample size to achieve 85% power relative to M<sc>gc</sc>. P<sc>earson</sc>, R<sc>v</sc>, and C<sc>ca</sc> all achieve the same performance, as do S<sc>pearman</sc> and K<sc>endall</sc>. M<sc>gc</sc> requires the fewest number of samples in all settings, and for high-dimensional non-monotonic relationships, all other methods require about double or triple the number of samples M<sc>gc</sc> requires.</p><p><supplementary-material id="table1sdata1"><object-id pub-id-type="doi">10.7554/eLife.41690.010</object-id><label>Table 1—source data 1.</label><caption><title>Testing power sample size data in one dimension.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-41690-table1-data1-v2.mat"/></supplementary-material></p><p><supplementary-material id="table1sdata2"><object-id pub-id-type="doi">10.7554/eLife.41690.011</object-id><label>Table 1—source data 2.</label><caption><title>Testing power sample size data in high-dimensions.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-41690-table1-data2-v2.mat"/></supplementary-material></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dimensionality</th><th align="center" colspan="3">One-Dimensional</th><th align="center" colspan="3">Ten-Dimensional</th></tr><tr><th>Dependency type</th><th>Monotone</th><th>Non-Mono</th><th>Average</th><th>Monotone</th><th>Non-Mono</th><th>Average</th></tr></thead><tbody><tr><th>M<sc>gc</sc></th><td><bold>1</bold></td><td><bold>1</bold></td><td><bold>1</bold></td><td><bold>1</bold></td><td><bold>1</bold></td><td><bold>1</bold></td></tr><tr><th>D<sc>corr</sc></th><td><bold>1</bold></td><td>2.6</td><td>2.2</td><td><bold>1</bold></td><td>3.2</td><td>2.6</td></tr><tr><th>M<sc>corr</sc></th><td><bold>1</bold></td><td>2.8</td><td>2.4</td><td><bold>1</bold></td><td>3.1</td><td>2.6</td></tr><tr><th>H<sc>hg</sc></th><td>1.4</td><td><bold>1</bold></td><td>1.1</td><td>1.7</td><td>1.9</td><td>1.8</td></tr><tr><th>H<sc>sic</sc></th><td>1.4</td><td>1.1</td><td>1.2</td><td>1.7</td><td>2.4</td><td>2.2</td></tr><tr><th>M<sc>antel</sc></th><td>1.4</td><td>1.8</td><td>1.7</td><td>3</td><td>1.6</td><td>1.9</td></tr><tr><th>P<sc>earson</sc> / R<sc>v</sc> / C<sc>ca</sc></th><td><bold>1</bold></td><td>&gt;10</td><td>&gt;10</td><td><bold>0.8</bold></td><td>&gt;10</td><td>&gt;10</td></tr><tr><th>S<sc>pearman</sc> / K<sc>endall</sc></th><td><bold>1</bold></td><td>&gt;10</td><td>&gt;10</td><td>n/a</td><td>n/a</td><td>n/a</td></tr><tr><th>M<sc>ic</sc></th><td>2.4</td><td>2</td><td>2.1</td><td>n/a</td><td>n/a</td><td>n/a</td></tr></tbody></table></table-wrap></sec><sec id="s2-2"><title>M<sc>gc</sc> deciphers latent dependence structure</title><p>Beyond simply testing the existence of a relationship, the next goal is often to decipher the nature or structure of the relationship, thereby providing insight and guiding future experiments. A single scalar quantity (such as effect size) is inadequate given the vastness and complexities of possible relationships. Existing methods would require a secondary procedure to characterize the relationship, which introduces complicated ‘post selection’ statistical quandaries that remain mostly unresolved (<xref ref-type="bibr" rid="bib4">Berk et al., 2013</xref>). Instead, M<sc>gc</sc> provides a simple, intuitive, and nonparametric (and therefore infinitely flexible) 'map’ of how it discovered the relationship. As described below, this map not only provides interpretability for how M<sc>gc</sc> detected a dependence, it also partially characterize the geometry of the investigated relationship.</p><p>The M<sc>gc</sc><italic>-Map</italic> shows local correlation as a function of the scales of the two properties. More concretely, it is the matrix of <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>’s, as defined above. Thus, the M<sc>gc</sc>-Map is an n-by-n matrix which encodes the strength of dependence for each possible scale. <xref ref-type="fig" rid="fig3">Figure 3</xref> provides the M<sc>gc</sc>-Map for all 20 different one-dimensional relationships; the optimal scale to achieve <inline-formula><mml:math id="inf61"><mml:msub><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msub></mml:math></inline-formula> is marked with a green dot. For the monotonic dependencies (1-5), the optimal scale is always the largest scale, that is the global one. For all non-monotonic dependencies (6-19), M<sc>gc</sc> chooses smaller scales. Thus, a global optimal scale implies a close-to-linear dependency, otherwise the dependency is strongly nonlinear. In fact, this empirical observation led to the following theorem (which is proved in Materials and methods):</p><p><italic><bold>Theorem 1</bold>. When <inline-formula><mml:math id="inf62"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are linearly related (meaning that <inline-formula><mml:math id="inf63"><mml:mi>Y</mml:mi></mml:math></inline-formula> can be constructed from <inline-formula><mml:math id="inf64"><mml:mi>X</mml:mi></mml:math></inline-formula> by rotation, scaling, translation, and/or reflection), the optimal scale of M<sc>gc</sc> equals the global scale. Conversely, a local optimal scale implies a nonlinear relationship.</italic></p><p>Thus, the M<sc>gc</sc>-Map explains how M<sc>gc</sc> discovers relationships, specifically, which scale has the most informative pairwise comparisons, and how that relates to the geometry of the relationship. Note that M<sc>gc</sc> provides the geometric characterization ‘for free’, meaning that no separate procedure is required; therefore, M<sc>gc</sc> provides both a valid test and information about the geometric relationship.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.41690.012</object-id><label>Figure 3.</label><caption><title>The M<sc>gc</sc>-Map characterizes the geometry of the dependence function.</title><p>For each of the 20 panels, the abscissa and ordinate denote the number of neighbors for <inline-formula><mml:math id="inf65"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf66"><mml:mi>Y</mml:mi></mml:math></inline-formula>, respectively, and the color denotes the magnitude of each local correlation. For each simulation, the sample size is 60, and both <inline-formula><mml:math id="inf67"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf68"><mml:mi>Y</mml:mi></mml:math></inline-formula> are one-dimensional. Each dependency has a different M<sc>gc</sc>-Map characterizing the geometry of dependence, and the optimal scale is shown in green. In linear or close-to-linear relationships (first row), the optimal scale is global, that is the green dot is in the top right corner. Otherwise the optimal scale is non-global, which holds for the remaining dependencies. Moreover, similar dependencies often share similar M<sc>gc</sc>-Maps and similar optimal scales, such as (10) logarithmic and (11) fourth root, the trigonometric functions in (12) and (13 , 16) circle and (17) ellipse, and (14) square and (18) diamond. The M<sc>gc</sc>-Maps for high-dimensional simulations are provided in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41690-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.41690.013</object-id><label>Figure 3—figure supplement 1.</label><caption><title>The M<sc>gc</sc>-Map for the 20 panels for high-dimensional dependencies.</title><p>For each simulation, the sample size is 100, and the dimension is selected as the dimension such that M<sc>gc</sc> has a testing power above 0.5. It has similar behavior and interpretation as the one-dimensional power maps in <xref ref-type="fig" rid="fig3">Figure 3</xref>, that is the linear relationships optimal scales are global, and similar dependencies share similar M<sc>gc</sc>-Maps.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41690-fig3-figsupp1-v2.tif"/></fig></fig-group><p>Moreover, similar dependencies have similar M<sc>gc</sc>-Maps and often similar optimal scales. For example, logarithmic (10) and fourth root (11), although very different functions analytically, are geometrically similar, and yield very similar M<sc>gc</sc>-Maps. Similarly, (12) and (13) are trigonometric functions, and they share a narrow range of significant local scales. Both circle (16) and ellipse (17), as well as square (14) and diamond (18), are closely related geometrically and also have similar M<sc>gc</sc>-Maps. This indicates that the M<sc>gc</sc>-Map partially characterizes the geometry of these relationships, differentiating different dependence structures and assisting subsequent analysis steps. Moreover, in <xref ref-type="bibr" rid="bib70">Shen and Vogelstein, 2018</xref>, we proved that the sample M<sc>gc</sc>-Map (which M<sc>gc</sc> estimates) converges to the true M<sc>gc</sc>-Map provided by the underlying joint distribution of the data. In other words, each relationship has a specific map that characterizes it based on its joint distribution, and M<sc>gc</sc> is able to accurately estimate it via sample observations. The existence of a population level characterization of the joint distribution strongly differentiates M<sc>gc</sc> from previously proposed multi-scale geometric or topological characterizations of data, such as persistence diagrams (<xref ref-type="bibr" rid="bib20">Edelsbrunner and Harer, 2009</xref>).</p><sec id="s2-2-1"><title>M<sc>gc</sc> is computationally efficient</title><p>M<sc>gc</sc> does not incur large computational costs and has a similar complexity as existing methods. Though a naïve implementation of M<sc>gc</sc> requires <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> operations, we devised a nested implementation that requires only <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> operations. Moreover, obtaining the M<sc>gc</sc>-Map costs no additional computation, whereas other methods would require running a secondary computational step to decipher geometric properties of the relationship. M<sc>gc</sc> can also trivially be parallelized, reducing computation to <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf72"><mml:mi>T</mml:mi></mml:math></inline-formula> is the number of cores (see Algorithm C1 for details). Since <inline-formula><mml:math id="inf73"><mml:mi>T</mml:mi></mml:math></inline-formula> is often larger than <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, in practice, M<sc>gc</sc> can be <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, meaning only a constant factor slower than D<sc>corr</sc> and H<sc>sic</sc>, which is illustrated in Figure 6 of <xref ref-type="bibr" rid="bib70">Shen and Vogelstein, 2018</xref>. For example, at sample size <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5000</mml:mn></mml:mrow></mml:math></inline-formula> and dimension <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, on a typical laptop computer, D<sc>corr</sc> requires around 0.5 s to compute the test statistic, whereas M<sc>gc</sc> requires no more than <inline-formula><mml:math id="inf78"><mml:mn>5</mml:mn></mml:math></inline-formula> s. But the cost and time to obtain 2.5× more data (so D<sc>corr</sc> has same average power as M<sc>gc</sc>) typically far exceeds a few seconds. In comparison, the cost to compute a persistence diagram is typically <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is orders of magnitude slower when <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The running time of each method on the real data experiments are reported in Materials and methods.</p></sec></sec><sec id="s2-3"><title>Mgc uniquely reveals relationships in real data</title><p>Geometric intuition, numerical simulations, and theory all provide evidence that M<sc>gc</sc> will be useful for real data discoveries. Nonetheless, real data applications provide another necessary ingredient to justify its use in practice. Below, we describe several real data applications where we have used M<sc>gc</sc> to understand relationships in data that other methods were unable to provide.</p><sec id="s2-3-1"><title>M<sc>gc</sc> discovers the relationships between brain and mental properties</title><p>The human psyche is of course dependent on brain activity and structure. Previous work has studied two particular aspects of our psyche: personality and creativity, developing quantitative metrics for evaluating them using structured interviews (<xref ref-type="bibr" rid="bib12">Costa and McCrae, 1992</xref>; <xref ref-type="bibr" rid="bib41">Jung et al., 2009</xref>). However, the relationship between brain activity and structure, and these aspects of our psyche, remains unclear (<xref ref-type="bibr" rid="bib19">DeYoung et al., 2010</xref>; <xref ref-type="bibr" rid="bib92">Xu and Potenza, 2012</xref>; <xref ref-type="bibr" rid="bib7">Bjørnebekk et al., 2013</xref>; <xref ref-type="bibr" rid="bib64">Sampaio et al., 2014</xref>). For example, prior work did not evaluate the relationship between entire brain connectivity and all five factors of the standard personality model (<xref ref-type="bibr" rid="bib12">Costa and McCrae, 1992</xref>). We therefore utilized M<sc>gc</sc> to investigate published open access data (see Materials and methods for details).</p><p>First, we analyzed the relationship between resting-state functional magnetic resonance (rs-fMRI) activity and personality (<xref ref-type="bibr" rid="bib1">Adelstein et al., 2011</xref>). The first row of <xref ref-type="table" rid="table2">Table 2</xref> compares the p-value of different methods, and <xref ref-type="fig" rid="fig4">Figure 4A</xref> shows the M<sc>gc</sc>-Map for the sample data. M<sc>gc</sc> is able to yield a significant p-value (&lt; 0.05), whereas all previously proposed global dependence tests under consideration (M<sc>antel</sc>, D<sc>corr</sc>, M<sc>corr</sc>, or H<sc>hg</sc>) fail to detect dependence at a significance level of 0.05. Moreover, the M<sc>gc</sc>-Map provides a characterization of the dependence, for which the optimal scale indicates that the dependency is strongly nonlinear. Interestingly, the M<sc>gc</sc>-Map does not look like any of the 20 images from the simulated data, suggesting that the nonlinearity characterizing this dependency is more complex or otherwise different from those we have considered so far.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.41690.014</object-id><label>Figure 4.</label><caption><title>Demonstration that M<sc>gc</sc> successfully detects dependency, distinguishes linearity from nonlinearity, and identifies the most informative feature in a variety of real data experiments.</title><p>(<bold>A</bold>) The M<sc>gc</sc>-Map for brain activity versus personality. M<sc>gc</sc> has a large test statistic and a significant p-value at the optimal scale (13, 4), while the global counterpart is non-significant. That the optimal scale is non-global implies a strongly nonlinear relationship. (<bold>B</bold>) The M<sc>gc</sc>-Map for brain connectivity versus creativity. The image is similar to that of a linear relationship, and the optimal scale equals the global scale, thus both M<sc>gc</sc> and M<sc>corr</sc> are significant in this case. (<bold>C</bold>) For each peptide, the x-axis shows the p-value for testing dependence between pancreatic and healthy subjects by M<sc>gc</sc>, and the y-axis shows the p-value for testing dependence between pancreatic and all other subjects by M<sc>gc</sc>. At critical level <inline-formula><mml:math id="inf81"><mml:mn>0.05</mml:mn></mml:math></inline-formula>, M<sc>gc</sc> identifies a unique protein after multiple testing adjustment. (<bold>D</bold>) The true and false positive counts using a k-nearest neighbor (choosing the best <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>) leave-one-out classification using only the significant peptides identified by each testing method. The peptide identified by M<sc>gc</sc> achieves the best true and false positive rates, as compared to the peptides identified by H<sc>sic</sc> or H<sc>hg</sc>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41690-fig4-v2.tif"/></fig><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.41690.015</object-id><label>Table 2.</label><caption><title>The p-values for brain imaging vs mental properties.</title><p>M<sc>gc</sc> <italic>always</italic> uncovers the existence of significant relationships and discovers the underlying optimal scales. Bold indicates significant p-value per dataset.</p><p><supplementary-material id="table2sdata1"><object-id pub-id-type="doi">10.7554/eLife.41690.016</object-id><label>Table 2—source data 1.</label><caption><title>p-value data for activity vs personality.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-41690-table2-data1-v2.mat"/></supplementary-material></p><p><supplementary-material id="table2sdata2"><object-id pub-id-type="doi">10.7554/eLife.41690.017</object-id><label>Table 2—source data 2.</label><caption><title>p-value data for connetivity vs creativity.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-41690-table2-data2-v2.mat"/></supplementary-material></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Testing Pairs/Methods</th><th>M<sc>gc</sc></th><th>D<sc>corr</sc></th><th>M<sc>corr</sc></th><th>H<sc>hg</sc></th><th>H<sc>sic</sc></th></tr></thead><tbody><tr><td>Activity vs Personality</td><td><bold>0.043</bold></td><td>0.667</td><td>0.441</td><td>0.059</td><td>0.124</td></tr><tr><td>Connectivity vs Creativity</td><td><bold>0.011</bold></td><td><bold>0.010</bold></td><td><bold>0.011</bold></td><td><bold>0.031</bold></td><td>0.092</td></tr></tbody></table></table-wrap><p>Second, we investigated the relationship between diffusion MRI derived connectivity and creativity (<xref ref-type="bibr" rid="bib41">Jung et al., 2009</xref>). The second row of <xref ref-type="table" rid="table2">Table 2</xref> shows that M<sc>gc</sc> is able to ascertain a dependency between the whole brain network and the subject’s creativity. The M<sc>gc</sc>-Map in <xref ref-type="fig" rid="fig4">Figure 4B</xref> closely resembles a linear relationship where the optimal scale is global. The close-to-linear relationship is also supported from the p-value table as all methods except H<sc>sic</sc> are able to detect significant dependency, which suggests that there is relatively little to gain by pursuing nonlinear regression techniques, potentially saving valuable research time by avoiding tackling an unnecessary problem. The test statistic for both M<sc>gc</sc> and M<sc>corr</sc> equal 0.04, which is quite close to zero despite a significant p-value, implying a relatively weak and noisy relationship. A prediction of creativity via linear regression turns out to be non-significant, which implies that the sample size is too low to obtain useful predictive accuracy (not shown), indicating that more data are required for single subject predictions. If one had first directly estimated the regression function, obtaining a null result, it would remain unclear whether a relationship existed. This experiment demonstrates that for high-dimensional and potentially structured data, M<sc>gc</sc> is able to reveal dependency with relatively small sample size while parametric techniques and directly estimating regression functions can often be ineffective.</p><p>The performance in the real data closely matches the simulations results: the first dataset exhibits a strongly nonlinear relationship, for which M<sc>gc</sc> has the lowest p-value, followed by H<sc>hg</sc> and H<sc>sic</sc> and then all other methods; the second dataset exhibits a close-to-linear relationship, for which global methods perform the best while H<sc>hg</sc> and H<sc>sic</sc> are trailing. Moreover, M<sc>gc</sc> detected a complex nonlinear relationship for brain activity versus personality, and a nearly linear but noisy relationship for brain network versus creativity, the only method able to make either of those claims. In a separate experiment, we assessed the frequency with which M<sc>gc</sc> obtained false positive results using brain activity data, based on experiments from <xref ref-type="bibr" rid="bib21">Eklund et al. (2012)</xref>; <xref ref-type="bibr" rid="bib22">Eklund et al. (2016)</xref>. <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> shows that M<sc>gc</sc> achieves a false positive rate of 5% when using a significance level of 0.05, implying that it correctly controls for false positives, unlike typical parametric methods on these data.</p></sec><sec id="s2-3-2"><title>M<sc>gc</sc> identifies potential cancer proteomics biomarkers</title><p>M<sc>gc</sc> can also be useful for a completely complementary set of scientific questions: screening proteomics data for biomarkers, often involving the analysis of tens of thousands of proteins, peptides, or transcripts in multiple samples representing a variety of disease types. Determining whether there is a relationship between one or more of these markers and a particular disease state can be challenging but is a necessary first step (<xref ref-type="bibr" rid="bib23">Frantzi et al., 2014</xref>). We sought to discover new useful protein biomarkers from a quantitative proteomics technique that measures protein and peptide abundance called Selected Reaction Monitoring (SRM) (<xref ref-type="bibr" rid="bib85">Wang et al., 2011</xref>). Specifically, we were interested in finding biomarkers that were unique to pancreatic cancer, because it is lethal and no clinically useful biomarkers are currently available (<xref ref-type="bibr" rid="bib5">Bhat et al., 2012</xref>).</p><p>The data consist of proteolytic peptides derived from the blood samples of 95 individuals harboring pancreatic (<inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>), ovarian (<inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>24</mml:mn></mml:mrow></mml:math></inline-formula>), colorectal cancer (<inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>28</mml:mn></mml:mrow></mml:math></inline-formula>), and healthy controls (<inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>33</mml:mn></mml:mrow></mml:math></inline-formula>). The processed data included 318 peptides derived from 121 proteins. Previously, we used these data and other techniques to find ovarian cancer biomarkers (a much easier task because the dataset has twice as many ovarian patients) and validated them with subsequent experiments (<xref ref-type="bibr" rid="bib87">Wang et al., 2017</xref>). Therefore, our first step was to check whether M<sc>gc</sc> could correctly identify ovarian biomarkers. Indeed, the peptides that have been validated previously are also identified by M<sc>gc</sc>. Emboldened, using the same dataset, we applied M<sc>gc</sc> to screen for biomarkers unique to pancreatic cancer. To do so, we first screened for a difference between pancreatic cancer and healthy controls, identifying several potential biomarkers. Then, we screened for a difference between pancreatic cancer and all other conditions, to find peptides that differentiate pancreatic cancer from other cancers. <xref ref-type="fig" rid="fig4">Figure 4C</xref> shows the p-value of each peptide assigned by M<sc>gc</sc>, which reveals one particular protein, neurogranin, that exhibits a strong dependency specifically with pancreatic cancer. Subsequent literature searches reveal that neurogranin is a potentially valuable biomarker for pancreatic cancer because it is exclusively expressed in brain tissue among normal tissues and has not been linked with any other cancer type (<xref ref-type="bibr" rid="bib93">Yang et al., 2015</xref>; <xref ref-type="bibr" rid="bib88">Willemse et al., 2018</xref>). In comparison, H<sc>sic</sc> identified neurogranin as well, but it also identified another peptide; H<sc>hg</sc> identified the same two by H<sc>sic</sc>, and a third peptide. A literature evaluation of these additional peptides shows that they are upregulated in other cancers as well and are unlikely to be useful as a pancreatic biomarker (<xref ref-type="bibr" rid="bib33">Helfman et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Lam et al., 2012</xref>). The rest of the global methods did not identify any markers at significance level <inline-formula><mml:math id="inf87"><mml:mn>0.05</mml:mn></mml:math></inline-formula>, see Materials and methods for more details and <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref> for identified peptide information using each method.</p><p>Since there is no ground truth yet in this experiment, we further carried out a classification task using the biomarkers identified by the various algorithms, using a k-nearest-neighbor classifier to predict pancreatic cancer, and a leave-one-subject-out validation. <xref ref-type="fig" rid="fig4">Figure 4D</xref> shows that the peptide selected by M<sc>gc</sc> (neurogranin) works better than any other subset of the peptides selected by H<sc>sic</sc> or H<sc>hg</sc>, in terms of both fewer false positives and negatives. This analysis suggests M<sc>gc</sc> can effectively be used for screening and subsequent classification.</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>There are a number of connections between M<sc>gc</sc> and other prominent statistical procedures that may be worth further exploration. First, M<sc>gc</sc> can be thought of as a regularized or sparsified variant of distance or kernel methods. Regularization is central to high-dimensional and ill-posed problems, where dimensionality is larger than sample size. Second, M<sc>gc</sc> can also be thought of as learning a metric because it chooses the optimal scale amongst a set of <inline-formula><mml:math id="inf88"><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> truncated distances, motivating studying the relationship between M<sc>gc</sc> and recent advances in metric learning (<xref ref-type="bibr" rid="bib91">Xing et al., 2003</xref>). In particular, deep learning can be thought of as metric learning (<xref ref-type="bibr" rid="bib25">Giryes et al., 2015</xref>), and generative adversarial networks (<xref ref-type="bibr" rid="bib27">Goodfellow et al., 2014</xref>) are implicitly testing for equality, which is closely related to dependence (<xref ref-type="bibr" rid="bib75">Sutherland et al., 2016</xref>). While M<sc>gc</sc> searches over a two-dimensional parameter space to optimize the metric, deep learning searches over a much larger parameter space, sometimes including millions of dimensions. Probably neither is optimal, and somewhere between the two would be useful in many tasks. Third, energy statistics provide state of the art approaches to other problems, including goodness-of-fit (<xref ref-type="bibr" rid="bib78">Székely and Rizzo, 2005</xref>), analysis of variance (<xref ref-type="bibr" rid="bib60">Rizzo and Székely, 2010</xref>), conditional dependence (<xref ref-type="bibr" rid="bib81">Székely and Rizzo, 2014</xref>; <xref ref-type="bibr" rid="bib86">Wang et al., 2015</xref>), and feature selection (<xref ref-type="bibr" rid="bib50">Li et al., 2012</xref>; <xref ref-type="bibr" rid="bib97">Zhong and Zhu, 2015</xref>), so M<sc>gc</sc> can be adapted for them as well. Indeed, M<sc>gc</sc> can also implement a two-sample (or generally the K-sample) test (<xref ref-type="bibr" rid="bib77">Szekely and Rizzo, 2004</xref>; <xref ref-type="bibr" rid="bib35">Heller et al., 2016</xref>; <xref ref-type="bibr" rid="bib70">Shen and Vogelstein, 2018</xref>). Specifically, for more than two modalities, one may use summation of pairwise M<sc>gc</sc> test statistics, similar to how energy statistic is generalized to K-sample testing from two-sample testing (<xref ref-type="bibr" rid="bib60">Rizzo and Székely, 2010</xref>; <xref ref-type="bibr" rid="bib61">Rizzo and Székely, 2016</xref>; <xref ref-type="bibr" rid="bib70">Shen and Vogelstein, 2018</xref>), or how canonical correlation analysis is generalized into more than two modalities (<xref ref-type="bibr" rid="bib44">Kettenring, 1971</xref>; <xref ref-type="bibr" rid="bib84">Tenenhaus and Tenenhaus, 2011</xref>; <xref ref-type="bibr" rid="bib67">Shen et al., 2014</xref>). Finally, although energy statistics have not yet been explicitly used for classification, regression, or dimensionality reduction, M<sc>gc</sc> opens the door to these applications by providing guidance as to how to proceed. Specifically, it is well documented in machine learning literature that the choice of kernel, metric, or scale often has a strong effect on the performance of different machine learning algorithms (<xref ref-type="bibr" rid="bib49">Levina and Bickel, 2004</xref>). M<sc>gc</sc> provides a mechanism to estimate scale that is both theoretically justified and computationally efficient, by optimizing a metric for a task wherein the previous methods lacked a notion of optimization. Nonlinear dimensionality reduction procedures, such as Isomap (<xref ref-type="bibr" rid="bib83">Tenenbaum et al., 2000</xref>) and local linear embedding (<xref ref-type="bibr" rid="bib63">Roweis and Saul, 2000</xref>) for example, must also choose a scale, but have no principled criteria for doing so. M<sc>gc</sc> could be used to provide insight into multimodal dimensionality reduction as well.</p><p>The default metric choice of M<sc>gc</sc> in this paper is always the Euclidean distance, but other metric choices may be more appropriate in different fields, and using the strong negative type metric as specified in <xref ref-type="bibr" rid="bib51">Lyons (2013)</xref> guarantees consistency. However, if multiple metric choices are experimented to yield multiple M<sc>gc</sc> p-values, then the optimal p-value should be properly corrected for multiple testing. Alternatively, one may use the maximum M<sc>gc</sc> statistic among multiple metric choices, apply the same procedure in each permutation (i.e. in each permutation, use the same number of metric choices and take the maximum M<sc>gc</sc> as the permuted statistic), then derive a single p-value. Such a testing procedure properly controls the type one error level without the need for additional correction.</p><p>M<sc>gc</sc> also addresses a particularly vexing statistical problem that arises from the fact that methods methods for discovering dependencies are typically dissociated from methods for deciphering them. This dissociation creates a problem because the statistical assumptions underlying the ‘deciphering’ methods become compromised in the process of ‘discoverying’; this is called the ‘post-selection inference’ problem (<xref ref-type="bibr" rid="bib4">Berk et al., 2013</xref>). The most straightforward way to address this issue is to collect new data, which is costly and time-consuming. Therefore, researchers often ignore this fact and make statistically invalid claims. M<sc>gc</sc> circumvents this dilemma by carefully constructing its permutation test to estimate the scale in the process of estimating a p-value, rather than after. To our knowledge, M<sc>gc</sc> is the first dependence test to take a step towards valid post-selection inference.</p><p>As a separate next theoretical extension, we could reduce the computational space and time required by M<sc>gc</sc>. M<sc>gc</sc> currently requires space and time that are quadratic with respect to the number of samples, which can be costly for very large data. Recent advances in related work demonstrated that one could reduce computational time of distance-based tests to close to linear via faster implementation, subsampling, random projection, and null distribution approximation (<xref ref-type="bibr" rid="bib39">Huo and Székely, 2016</xref>; <xref ref-type="bibr" rid="bib38">Huang and Huo, 2017</xref>; <xref ref-type="bibr" rid="bib96">Zhang et al., 2018</xref>; <xref ref-type="bibr" rid="bib10">Chaudhuri and Hu, 2018</xref>), making it feasible for large amount of data. Alternately, semi-external memory implementations would allow running M<sc>gc</sc> even as the interpoint comparison matrix exceeds the size of main memory (<xref ref-type="bibr" rid="bib14">Da Zheng et al., 2015</xref>; <xref ref-type="bibr" rid="bib15">Da Zheng et al., 2016a</xref>; <xref ref-type="bibr" rid="bib16">Da Zheng et al., 2016b</xref>; <xref ref-type="bibr" rid="bib17">Da Zheng et al., 2016c</xref>).</p><p>Finally, M<sc>gc</sc> is easy to use. Source code is available in MATLAB, R, and Python from <ext-link ext-link-type="uri" xlink:href="https://mgc.neurodata.io/">https://mgc.neurodata.io/</ext-link> (<xref ref-type="bibr" rid="bib9">Bridgeford et al., 2018</xref>; experiments archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/MGC-paper">https://github.com/elifesciences-publications/MGC-paper</ext-link>). Code for reproducing all the figures in this manuscript is also available from the above websites. We showed M<sc>gc</sc>’s value in diverse applications spanning neuroscience (which motivated this work) and an ’omics example. Applications in other domains facing similar questions of dependence, such as finance, pharmaceuticals, commerce, and security, could likewise benefit from M<sc>gc</sc>.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Mathematical details</title><p>This section contains essential mathematical details on independence testing, the notion of the generalized correlation coefficient and the distance-based correlation measure, how to compute the local correlations, and the smoothing technique. A statistical treatment on MGC is in <xref ref-type="bibr" rid="bib70">Shen and Vogelstein, 2018</xref>, which introduces the population version of M<sc>gc</sc> and various theoretical properties.</p><sec id="s4-1-1"><title>Testing independence</title><p>Given pairs of observations <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, assume they are independently identically distributed as <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mover><mml:mo>∼</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mover><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. If the two random variables <inline-formula><mml:math id="inf92"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf93"><mml:mi>Y</mml:mi></mml:math></inline-formula> are independent, the joint distribution equals the product of the marginals, that is <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The statistical hypotheses for testing independence is as follows:<disp-formula id="equ2"><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ3"><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Given a test statistic, the testing power equals the probability of rejecting the independence hypothesis (i.e. the null hypothesis) when it is false. A test statistic is consistent if and only if the testing power increases to <inline-formula><mml:math id="inf95"><mml:mn>1</mml:mn></mml:math></inline-formula> as sample size increases to infinity. We would like a test to be universally consistent, that is consistent against all joint distributions. D<sc>corr</sc>, M<sc>corr</sc>, H<sc>sic</sc>, and H<sc>hg</sc> are all consistent against any joint distribution of finite second moments and finite dimension.</p><p>Note that <inline-formula><mml:math id="inf96"><mml:mi>p</mml:mi></mml:math></inline-formula> is the dimension for <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>’s, <inline-formula><mml:math id="inf98"><mml:mi>q</mml:mi></mml:math></inline-formula> is the dimension for <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>’s. For M<sc>gc</sc> and all benchmark methods, there is no restriction on the dimensions, that is the dimensions can be arbitrarily large, and <inline-formula><mml:math id="inf100"><mml:mi>p</mml:mi></mml:math></inline-formula> is not required to equal <inline-formula><mml:math id="inf101"><mml:mi>q</mml:mi></mml:math></inline-formula>. The ability to handle data of arbitrary dimension is crucial for modern big data. There also exist some special methods that only operate on one-dimensional data, such as (<xref ref-type="bibr" rid="bib59">Reshef et al., 2011</xref>; <xref ref-type="bibr" rid="bib35">Heller et al., 2016</xref>; <xref ref-type="bibr" rid="bib39">Huo and Székely, 2016</xref>), which are not directly applicable to multidimensional data.</p></sec><sec id="s4-1-2"><title>Correlation measures</title><p>To achieve consistent testing, most state-of-the-art dependence measures operate on pairwise comparisons, either similarities (such as kernels) or dissimilarities (such as distances).</p><p>Let <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒳</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒴</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the matrices of sample observations, and <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></inline-formula> be the distance function for <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>’s and <inline-formula><mml:math id="inf106"><mml:msub><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula> for <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>’s. One can then compute two <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> distance matrices <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>B</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>b</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>b</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. A common example of the distance function is the Euclidean metric (<inline-formula><mml:math id="inf113"><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> norm), which serves as the starting point for all methods in this manuscript.</p><p>Let <inline-formula><mml:math id="inf114"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf115"><mml:mi>B</mml:mi></mml:math></inline-formula> be the transformed (e.g., centered) versions of the distance matrices <inline-formula><mml:math id="inf116"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf117"><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula>, respectively. Any ‘generalized correlation coefficient’ (<xref ref-type="bibr" rid="bib72">Spearman, 1904</xref>; <xref ref-type="bibr" rid="bib43">Kendall, 1970</xref>) can be written as:<disp-formula id="equ4"><label>(1)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒳</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒴</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>z</mml:mi></mml:mfrac><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf118"><mml:mi>z</mml:mi></mml:math></inline-formula> is proportional to the standard deviations of <inline-formula><mml:math id="inf119"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf120"><mml:mi>B</mml:mi></mml:math></inline-formula>, that is <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. In words, <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the global sample correlation across <italic>pairwise comparison matrices </italic><inline-formula><mml:math id="inf123"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf124"><mml:mi>B</mml:mi></mml:math></inline-formula>, and is normalized into the range <inline-formula><mml:math id="inf125"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, which usually has expectation 0 under independence and implies a stronger dependency when the correlation is further away from 0.</p><p>Traditional correlations such as the Pearson’s correlation and the rank correlation can be written via the above correlation formulation, by using <inline-formula><mml:math id="inf126"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:mi>B</mml:mi></mml:math></inline-formula> directly from sample observations rather than distances. Distance-based methods like D<sc>corr</sc> and M<sc>antel</sc> operate on the Euclidean distance by default, or other metric choices on the basis of domain knowledge; then transform the resulting distance matrices <inline-formula><mml:math id="inf128"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf129"><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> by certain centering schemes into <inline-formula><mml:math id="inf130"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf131"><mml:mi>B</mml:mi></mml:math></inline-formula>. H<sc>sic</sc> chooses the Gaussian kernel and computes two kernel matrices, then transform the kernel matrices <inline-formula><mml:math id="inf132"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf133"><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> by the same centering scheme as D<sc>corr</sc>. For M<sc>gc</sc>, <inline-formula><mml:math id="inf134"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf135"><mml:mi>B</mml:mi></mml:math></inline-formula> are always distance matrices (or can be transformed to distances from kernels by <xref ref-type="bibr" rid="bib66">Sejdinovic et al. (2013)</xref>), and we shall apply a slightly different centering scheme that turns out to equal D<sc>corr</sc>.</p><p>To carry out the hypothesis testing on sample data via a nonparametric test statistic, for example a generalized correlation, the permutation test is often an effective choice (<xref ref-type="bibr" rid="bib26">Good, 2005</xref>), because a p-value can be computed by comparing the correlation of the sample data to the correlation of the permuted sample data. The independence hypothesis is rejected if the p-value is lower than a pre-determined type <inline-formula><mml:math id="inf136"><mml:mn>1</mml:mn></mml:math></inline-formula> error level, say 0.05. Then the power of the test statistic equals the probability of a correct rejection at a specific type <inline-formula><mml:math id="inf137"><mml:mn>1</mml:mn></mml:math></inline-formula> error level. Note that H<sc>hg</sc> is the only exception that cannot be cast as a generalized correlation coefficient, but the permutation testing is similarly effective for the H<sc>hg</sc> test statistic; also note that the <italic>iid</italic> assumption is critical for permutation test to be valid, which may not be applicable in special cases like auto-correlated time series (<xref ref-type="bibr" rid="bib31">Guillot and Rousset, 2013</xref>).</p></sec><sec id="s4-1-3"><title>Distance correlation (D<sc>corr</sc>) and the Unbiased Version (M<sc>corr</sc>)</title><p>Define the row and column means of <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> by <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. D<sc>corr</sc> defines<disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext> if </mml:mtext></mml:mstyle><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext> if </mml:mtext></mml:mstyle><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>and similarly for <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. For distance correlation, the numerator of <xref ref-type="disp-formula" rid="equ4">Equation 1</xref> is named the distance covariance (Dcov), while <inline-formula><mml:math id="inf142"><mml:msub><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf143"><mml:msub><mml:mi>s</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math></inline-formula> in the denominator are the square root of each distance variance. The centering scheme is important to guarantee the universal consistency of D<sc>corr</sc>, whereas Mantel uses a simple centering scheme and thus not universally consistent.</p><p>Let <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> be the population distance correlation, that is, the distance correlation between the underlying random variables <inline-formula><mml:math id="inf145"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf146"><mml:mi>Y</mml:mi></mml:math></inline-formula>. <xref ref-type="bibr" rid="bib76">Székely et al. (2007)</xref> define the population distance correlation via the characteristic functions of <inline-formula><mml:math id="inf147"><mml:msub><mml:mi>F</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf148"><mml:msub><mml:mi>F</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:math></inline-formula>, and show that the population distance correlation equals zero if and only if <inline-formula><mml:math id="inf149"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf150"><mml:mi>Y</mml:mi></mml:math></inline-formula> are independent, for any joint distribution <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of finite second moments and finite dimensionality. They also show that as <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the sample distance correlation converges to the population distance correlation, that is, <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒳</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒴</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus the sample distance correlation is consistent against any dependency of finite second moments and dimensionality. Of note, the distance covariance, distance variance, and distance correlation are always non-negative. Moreover, the consistency result holds for a much larger family of metrics, those of strong negative type (<xref ref-type="bibr" rid="bib51">Lyons, 2013</xref>).</p><p>It turns out that the sample distance correlation has a finite-sample bias, especially as the dimension <inline-formula><mml:math id="inf154"><mml:mi>p</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf155"><mml:mi>q</mml:mi></mml:math></inline-formula> increases (<xref ref-type="bibr" rid="bib80">Szekely and Rizzo, 2013</xref>). For example, for independent Gaussian distributions, the sample distance correlation converges to <inline-formula><mml:math id="inf156"><mml:mn>1</mml:mn></mml:math></inline-formula> as <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. By excluding the diagonal entries and slightly modifies the off-diagonal entries of <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒜</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf159"><mml:mi class="ltx_font_mathcaligraphic">B</mml:mi></mml:math></inline-formula>, Szekely and Rizzo (<xref ref-type="bibr" rid="bib80">Szekely and Rizzo, 2013</xref>; <xref ref-type="bibr" rid="bib81">Székely and Rizzo, 2014</xref>) show that M<sc>corr</sc> is an unbiased estimator of the population distance correlation <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf161"><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>, which is approximately normal even if <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus it enjoys the same theoretical consistency as D<sc>corr</sc> and always has zero mean under independence.</p></sec><sec id="s4-1-4"><title>Local correlations</title><p>Given any matrices <inline-formula><mml:math id="inf163"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf164"><mml:mi>B</mml:mi></mml:math></inline-formula>, we can define a set of local correlations as follows. Let <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> be the ‘rank’ of <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> relative to <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, that is, <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> closest point (or ‘neighbor’) to <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, as determined by ranking the <inline-formula><mml:math id="inf172"><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> distances to <inline-formula><mml:math id="inf173"><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>. Define <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> equivalently for the <inline-formula><mml:math id="inf175"><mml:mi>Y</mml:mi></mml:math></inline-formula>’s, but ranking relative to the rows rather than the columns (see below for explanation). For any neighborhood size <inline-formula><mml:math id="inf176"><mml:mi>k</mml:mi></mml:math></inline-formula> around each <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and any neighborhood size <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> around each <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, we define the local pairwise comparisons:<disp-formula id="equ6"><label>(2)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>;</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msubsup><mml:mrow><mml:mover><mml:mi>b</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>if </mml:mtext></mml:mstyle><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>;</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>and then let <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf181"><mml:msup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula> is the mean of <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and similarly for <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The <italic>local</italic> correlation coefficient at a given scale is defined to effectively exclude large distances:<disp-formula id="equ7"><label>(3)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒳</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒴</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf185"><mml:msubsup><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf186"><mml:msubsup><mml:mi>s</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:math></inline-formula> is the standard deviations for the truncated pairwise comparisons. The M<sc>gc</sc>-Map can be constructed by computing all local correlations, which allows the discovery of the optimal correlation. For any aforementioned correlation (D<sc>corr</sc>, M<sc>corr</sc>, H<sc>sic</sc>, M<sc>antel</sc>, P<sc>earson</sc>), one can define its local correlations by using <xref ref-type="disp-formula" rid="equ7">Equation 3</xref> and plugging in the respective <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from <xref ref-type="disp-formula" rid="equ4">Equation 1</xref>.</p><p>As most nonlinear relationships intrinsically exhibit a local linear structure, considering the nearest-neighbors is able to amplify the dependency signal over the global correlation. There could be two other scenarios: when the small distances in one modality mostly correspond to large distances in another modality, or when the large distances in one modality correspond to large distance in another modality. For the first scenario, the small distances become negative terms after centering while the large distances become positive terms after centering, so adding their product to <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> will cause the test statistic to be smaller — in fact, as distance correlation is shown to be &gt; 0 under dependence (<xref ref-type="bibr" rid="bib76">Székely et al., 2007</xref>), the first scenario cannot happen for all distances pairs. For the second scenario, one can experiment using the large distances (or the furthest neighbors) only by reversing the ranking scheme in local correlation to descending order. However, whenever the large distances are highly correlated, the small distances must also be highly correlated after centering by the mean distances, so global correlation coefficient like D<sc>corr</sc> already handles this scenario. Therefore considering the nearest-neighbor may significantly improve the performance over global correlation, while considering the other scenarios does not.</p></sec><sec id="s4-1-5"><title>M<sc>gc</sc> as the optimal local correlation</title><p>We define the multiscale graph correlation statistic as the optimal local correlation, for which the family of local correlation is computed based on Euclidean distance and M<sc>corr</sc> transformation.</p><p>Instead of taking a direct maximum, M<sc>gc</sc> takes a smoothed maximum, that is the maximum local correlation of the largest connected component <inline-formula><mml:math id="inf190"><mml:mi>R</mml:mi></mml:math></inline-formula> such that all local correlations within <inline-formula><mml:math id="inf191"><mml:mi>R</mml:mi></mml:math></inline-formula> are significant. If no such region exists, M<sc>gc</sc> defaults the test statistic to the global correlation (details in Algorithm C2). Thus, we can write:<disp-formula id="equ8"><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒳</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒴</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒳</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒴</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ9"><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>Largest Connected Component of </mml:mtext></mml:mstyle><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext> such that</mml:mtext></mml:mstyle><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Then the optimal scale equals all scales within <inline-formula><mml:math id="inf192"><mml:mi>R</mml:mi></mml:math></inline-formula> whose local correlations are as large as <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The choice of <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is made explicit in the pseudo-code, with further discussion and justification offered in <xref ref-type="bibr" rid="bib70">Shen and Vogelstein, 2018</xref>.</p></sec><sec id="s4-1-6"><title>Proof for theorem 1</title><p><bold>Theorem 1</bold>. When <inline-formula><mml:math id="inf195"><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo mathvariant="normal">,</mml:mo><mml:mi>Y</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are linearly related (rotation, scaling, translation, reflection), the optimal scale of M<sc>gc</sc> equals the global scale. Conversely, that. the optimal scale is local implies a nonlinear relationship.</p><p><italic>Proof.</italic> It suffices to prove the first statement, then the second statement follows by contrapositive. When <inline-formula><mml:math id="inf196"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are linearly related, <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for a unitary matrix <inline-formula><mml:math id="inf198"><mml:mi>W</mml:mi></mml:math></inline-formula> and a constant <inline-formula><mml:math id="inf199"><mml:mi>b</mml:mi></mml:math></inline-formula> up-to possible scaling, in which case the distances are preserved, that is <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>W</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>W</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. It follows that <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mrow><mml:mtext mathvariant="monospace">M</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mrow><mml:mtext mathvariant="monospace">corr</mml:mtext></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒳</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒴</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, so the global scale achieves the maximum possible correlation, and the largest connected region <inline-formula><mml:math id="inf202"><mml:mi>R</mml:mi></mml:math></inline-formula> is empty. Thus the optimal scale is global and <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mrow><mml:mtext mathvariant="monospace">M</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mrow><mml:mtext mathvariant="monospace">gc</mml:mtext></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒳</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒴</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mrow><mml:mtext mathvariant="monospace">M</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mrow><mml:mtext mathvariant="monospace">corr</mml:mtext></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒳</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒴</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-1-7"><title>Computational complexity of each step</title><p>The distance computation takes <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the ranking process takes <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Once the distance and ranking are completed, computing one local generalized correlation requires <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (see Algorithm C4). Thus, a naive approach to compute all local generalized correlations requires at least <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> by going through all possible scales, meaning possibly <inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> which would be computationally prohibitive. However, given the distance and ranking information, we devised an algorithm that iteratively computes all local correlations in <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> by re-using adjacent smaller local generalized correlations (see Algorithm C5). Therefore, when including the distance computation and ranking overheads, the MGC statistic is computed in <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>), which has the same running time as the H<sc>hg</sc> statistic, and the same running time up to a factor of <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as global correlations like D<sc>corr</sc> and M<sc>corr</sc>, which require <inline-formula><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> time. By utilizing a multi-core architecture, M<sc>gc</sc> can be computed in <inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> instead. As <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is often a small number, for example <inline-formula><mml:math id="inf215"><mml:mi>T</mml:mi></mml:math></inline-formula> is no more than <inline-formula><mml:math id="inf216"><mml:mn>30</mml:mn></mml:math></inline-formula> at <inline-formula><mml:math id="inf217"><mml:mn>1</mml:mn></mml:math></inline-formula> billion samples, thus M<sc>gc</sc> can be effectively computed in the same complexity as D<sc>corr</sc>. Note that the permutation test adds another <inline-formula><mml:math id="inf218"><mml:mi>r</mml:mi></mml:math></inline-formula> random permutations to the <inline-formula><mml:math id="inf219"><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> term, so computing the p-value requires <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec></sec><sec id="s4-2"><title>M<sc>gc</sc> algorithms and testing procedures</title><p>Six algorithms are presented in order:</p><list list-type="bullet"><list-item><p>Algorithm C1 describes M<sc>gc</sc> in its entirety (which calls most of the other algorithms as functions).</p></list-item><list-item><p>Algorithm C2 computes the M<sc>gc</sc> test statistic.</p></list-item><list-item><p>Algorithm C3 computes the p-value of M<sc>gc</sc> by the permutation test.</p></list-item><list-item><p>Algorithm C4 computes the local generalized correlation coefficient at a given scale <inline-formula><mml:math id="inf221"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, for a given choice of the global correlation coefficient.</p></list-item><list-item><p>Algorithm C5 efficiently computes all local generalized correlations, in nearly the same running time complexity as computing one local generalized correlation.</p></list-item><list-item><p>Algorithm C6 evaluates the testing power of M<sc>gc</sc> by a given distribution.</p></list-item></list><p>For ease of presentation, we assume there are no repeating observations of <inline-formula><mml:math id="inf222"><mml:mi>X</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf223"><mml:mi>Y</mml:mi></mml:math></inline-formula>, and note that M<sc>corr</sc> is the global correlation choice that M<sc>gc</sc> builds on.</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><tbody><tr valign="top"><td colspan="3"><bold>Pseudocode C1</bold> Multiscale Graph Correlation (M<sc>gc</sc>); requires <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> time, where <inline-formula><mml:math id="inf225"><mml:mi>r</mml:mi></mml:math></inline-formula> is the number of permutations and <inline-formula><mml:math id="inf226"><mml:mi>T</mml:mi></mml:math></inline-formula> is the number of cores available for parallelization.</td></tr><tr valign="top"><td colspan="3"><bold>Input:</bold> <inline-formula><mml:math id="inf227"><mml:mi>n</mml:mi></mml:math></inline-formula> samples of <inline-formula><mml:math id="inf228"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> pairs, an integer <inline-formula><mml:math id="inf229"><mml:mi>r</mml:mi></mml:math></inline-formula> for the number of random permutations.</td></tr><tr valign="top"><td colspan="3"><bold>Output:</bold> (i) MGC statistic <inline-formula><mml:math id="inf230"><mml:msup><mml:mi>c</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, (ii) the optimal scale <inline-formula><mml:math id="inf231"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, (iii) the p-value <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>,</td></tr><tr valign="top"><td colspan="2">     <bold>function</bold> MG<inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, for <inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td colspan="2">     <bold>(1)</bold> Calculate all pairwise distances:</td><td align="right"/></tr><tr valign="top"><td colspan="2">            <bold>for</bold> <inline-formula><mml:math id="inf235"><mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>:=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> <bold>do</bold></td><td align="right"/></tr><tr valign="top"><td colspan="2">                    <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"><inline-formula><mml:math id="inf237"><mml:msub><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></inline-formula> is the distance between pairs of <inline-formula><mml:math id="inf238"><mml:mi>x</mml:mi></mml:math></inline-formula> samples</td></tr><tr valign="top"><td colspan="2">                    <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"><inline-formula><mml:math id="inf240"><mml:msub><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula> is the distance between pairs of <inline-formula><mml:math id="inf241"><mml:mi>y</mml:mi></mml:math></inline-formula> samples</td></tr><tr valign="top"><td colspan="2">            <bold>end for</bold></td><td align="right"/></tr><tr valign="top"><td colspan="2">            Let <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td><td align="right"/></tr><tr valign="top"><td colspan="2">     <bold>(2)</bold> Calculate Multiscale Correlation Map <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒞</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> &amp; M<sc>gc</sc> Test Statistic:</td><td align="right"/></tr><tr valign="top"><td colspan="2">            <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒞</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">Algorithm C2</td></tr><tr valign="top"><td colspan="2">     <bold>(3)</bold> Calculate the p-value</td><td align="right"/></tr><tr valign="top"><td colspan="2">            <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">U</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">Algorithm C3</td></tr><tr valign="top"><td colspan="2">     <bold>end Function</bold></td><td align="right"/></tr></tbody></table></table-wrap><table-wrap id="inlinetable2" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td colspan="3"><bold>Pseudocode C2</bold> M<sc>gc</sc> test statistic. This algorithm computes all local correlations, take the smoothed maximum, and reports the <inline-formula><mml:math id="inf247"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> pair that achieves it. For the smoothing step, it: (i) finds the largest connected region in the correlation map, such that each correlation is significant, that is larger than a certain threshold to avoid correlation inflation by sample noise, (ii) take the largest correlation in the region, (iii) if the region area is too small, or the smoothed maximum is no larger than the global correlation, the global correlation is used instead. The running time is <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td></tr><tr valign="top"><td colspan="3"><bold>Input:</bold> A pair of distance matrices <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td></tr><tr valign="top"><td colspan="3"><bold>Output:</bold> The M<sc>gc</sc> statistic <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, all local statistics <inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒞</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the corresponding local scale <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td></tr><tr valign="top"><td>1:</td><td><bold>function</bold> MGCS<sc>ample</sc>S<sc>tat</sc><inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>2:</td><td>       <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒞</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">All local correlations</td></tr><tr valign="top"><td>3:</td><td>       <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒞</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">find a threshold to determine large local correlations</td></tr><tr valign="top"><td>4:</td><td>       <bold>for</bold> <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>:=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><bold>do</bold> <inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><bold>end for</bold> </td><td align="right">identify all scales with large correlation</td></tr><tr valign="top"><td>5:</td><td>       <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">binary map encoding scales with large correlation</td></tr><tr valign="top"><td>6:</td><td>       <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">largest connected component of the binary matrix</td></tr><tr valign="top"><td>7:</td><td>       <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒞</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">use the global correlation by default</td></tr><tr valign="top"><td>8:</td><td>       <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>9:</td><td>       <bold>if</bold> <inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold></td><td align="right">proceed when the significant region is sufficiently large</td></tr><tr valign="top"><td>10:</td><td>              <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒞</mml:mi></mml:mrow><mml:mo>∘</mml:mo><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">find the smoothed maximum and the respective scale</td></tr><tr valign="top"><td>11:</td><td>       <bold>end if</bold></td><td align="right"/></tr><tr valign="top"><td>12:</td><td><bold>end Function</bold></td><td align="right"/></tr><tr valign="top"><td colspan="3"><bold>Input:</bold> <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒞</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td></tr><tr valign="top"><td colspan="3"><bold>Output:</bold> A threshold <inline-formula><mml:math id="inf265"><mml:mi>t</mml:mi></mml:math></inline-formula> to identify large correlations.</td></tr><tr valign="top"><td>13:</td><td><bold>function</bold> T<sc>hresholding</sc> <inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒞</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>14:</td><td>       <inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">variance of all negative local generalized correlations</td></tr><tr valign="top"><td>15:</td><td>       <inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:msqrt><mml:mi>τ</mml:mi></mml:msqrt><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>×</mml:mo><mml:mn>3.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">threshold based on negative correlations</td></tr><tr valign="top"><td>16:</td><td>       <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>17:</td><td><bold>end Function</bold></td><td align="right"/></tr></tbody></table></table-wrap><table-wrap id="inlinetable3" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td colspan="3"><bold>Pseudocode C3</bold> Permutation Test. This algorithm uses the random permutation test with <inline-formula><mml:math id="inf270"><mml:mi>r</mml:mi></mml:math></inline-formula> random permutations for the p-value, requiring <inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for M<sc>gc</sc>. In the real-data experiment, we always set <inline-formula><mml:math id="inf272"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>,<inline-formula><mml:math id="inf273"><mml:mn>000</mml:mn></mml:math></inline-formula>. Note that the p-value computation for any other global generalized correlation coefficient follows from the same algorithm by replacing M<sc>gc</sc> with the respective test statistic.</td></tr><tr valign="top"><td colspan="3"><bold>Input:</bold> A pair of distance matrices <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, the number of permutations <inline-formula><mml:math id="inf275"><mml:mi>r</mml:mi></mml:math></inline-formula>, and M<sc>gc</sc> statistic <inline-formula><mml:math id="inf276"><mml:msup><mml:mi>c</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> for the observed data.</td></tr><tr valign="top"><td colspan="3"><bold>Output:</bold> The p-value <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td></tr><tr valign="top"><td>1:</td><td><bold>function</bold> P<sc>ermutation</sc>T<sc>est(</sc><inline-formula><mml:math id="inf278"><mml:mi>A</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf279"><mml:mi>B</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf280"><mml:mi>r</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf281"><mml:msup><mml:mi>c</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>)</td><td align="right"/></tr><tr valign="top"><td>2:</td><td>       <bold>for</bold> <inline-formula><mml:math id="inf282"><mml:mrow><mml:mi>t</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> <bold>do</bold></td><td align="right"/></tr><tr valign="top"><td>3:</td><td>              <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">generate a random permutation of size <inline-formula><mml:math id="inf284"><mml:mi>n</mml:mi></mml:math></inline-formula></td></tr><tr valign="top"><td>4:</td><td>              <inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace linebreak="newline"/></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">calculate the permuted M<sc>gc</sc> statistic</td></tr><tr valign="top"><td>5:</td><td>       <bold>end for</bold></td><td align="right"/></tr><tr valign="top"><td>6:</td><td>       <inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:munderover><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">compute p-value of M<sc>gc</sc></td></tr><tr valign="top"><td>7:</td><td><bold>end function</bold></td><td align="right"/></tr></tbody></table></table-wrap><table-wrap id="inlinetable4" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td colspan="3"><bold>Pseudocode C4</bold> Compute local test statistic at a given scale. This algorithm runs in <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> once the rank information is provided, which is suitable for M<sc>gc</sc> computation if an optimal scale is already estimated. But it would take <inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> if used to compute all local generalized correlations. Note that for the default M<sc>gc</sc> implementation uses single centering, the centering function centers <inline-formula><mml:math id="inf289"><mml:mi>A</mml:mi></mml:math></inline-formula> by column and <inline-formula><mml:math id="inf290"><mml:mi>B</mml:mi></mml:math></inline-formula> by row, and the sorting function sorts <inline-formula><mml:math id="inf291"><mml:mi>A</mml:mi></mml:math></inline-formula> within column and <inline-formula><mml:math id="inf292"><mml:mi>B</mml:mi></mml:math></inline-formula> within row. By utilizing <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> cores, the sorting function can be easily parallelized to take <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td></tr><tr valign="top"><td colspan="3"><bold>Input:</bold> A pair of distance matrices <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and a local scale <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td></tr><tr valign="top"><td colspan="3"><bold>Output:</bold> The local generalized correlation coefficient <inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td></tr><tr valign="top"><td>1:</td><td><bold>function</bold> L<sc>ocal</sc>G<sc>en</sc>C<sc>orr(</sc><inline-formula><mml:math id="inf298"><mml:mi>A</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf299"><mml:mi>B</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf300"><mml:mi>k</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf301"><mml:mi>l</mml:mi></mml:math></inline-formula>)</td><td align="right"/></tr><tr valign="top"><td>2:</td><td>      <bold> for</bold> <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>:=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>do</bold> <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>end for</bold> </td><td align="right">parallelized sorting</td></tr><tr valign="top"><td>3:</td><td>       <bold>for</bold> <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>:=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>do</bold> <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>end for</bold></td><td align="right">center distance matrices</td></tr><tr valign="top"><td>4:</td><td>       <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>∘</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo>∘</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">un-normalized local distance covariance</td></tr><tr valign="top"><td>5:</td><td>       <inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>∘</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>∘</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">local distance variances</td></tr><tr valign="top"><td>6:</td><td>       <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo>∘</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo>∘</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>7:</td><td>       <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>∘</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">sample means</td></tr><tr valign="top"><td>8:</td><td>       <inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo>∘</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>9:</td><td>       <inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">center and normalize</td></tr><tr valign="top"><td>10:</td><td><bold>end function</bold></td><td align="right"/></tr></tbody></table></table-wrap><table-wrap id="inlinetable5" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td colspan="3"><bold>Pseudocode C5</bold> Compute the multiscale correlation map (i.e., all local generalized correlations) in <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Once the distances are sorted, the remaining algorithm runs in <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. An important observation is that each product <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is included in <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> if and only if <inline-formula><mml:math id="inf316"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> satisfies <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mo>≤</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, so it suffices to iterate through <inline-formula><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf320"><mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>:=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and add the product simultaneously to all <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> whose scales are no more than <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. To achieve the above, we iterate through each product, add it to <inline-formula><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> at <inline-formula><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> only (so only one local scale is accessed for each operation); then add up adjacent <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf326"><mml:mrow><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The same applies to all local covariances, variances, and expectations.</td></tr><tr valign="top"><td colspan="3"><bold>Input:</bold> A pair of distance matrices <inline-formula><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td></tr><tr valign="top"><td colspan="3"><bold>Output:</bold> The multiscale correlation map <inline-formula><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒞</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula><bold>for</bold> <inline-formula><mml:math id="inf329"><mml:mrow><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</td></tr><tr valign="top"><td>1:</td><td><bold>function</bold> MGCA<sc>ll</sc>L<sc>ocal(</sc><inline-formula><mml:math id="inf330"><mml:mi>A</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf331"><mml:mi>B</mml:mi></mml:math></inline-formula>)</td><td align="right"/></tr><tr valign="top"><td>2:</td><td>      <bold> for</bold> <inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>:=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>do</bold> <inline-formula><mml:math id="inf333"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>end for</bold> </td><td align="right"/></tr><tr valign="top"><td>3:</td><td>       <bold>for</bold> <inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>:=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>do</bold> <inline-formula><mml:math id="inf335"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><bold>end for</bold></td><td align="right"/></tr><tr valign="top"><td>4:</td><td>       for <inline-formula><mml:math id="inf336"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>:=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>do</bold></td><td align="right">iterate through all local scales to calculate each term</td></tr><tr valign="top"><td>5:</td><td>              <inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>6:</td><td>              <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>7:</td><td>              <inline-formula><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>8:</td><td>              <inline-formula><mml:math id="inf340"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>9:</td><td>              <inline-formula><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>10:</td><td>              <inline-formula><mml:math id="inf342"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>11:</td><td>              <inline-formula><mml:math id="inf343"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>12:</td><td>       <bold>end for</bold></td><td align="right"/></tr><tr valign="top"><td>13:</td><td>       <bold>for</bold> <inline-formula><mml:math id="inf344"><mml:mrow><mml:mi>k</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> do</td><td align="right">iterate through each scale again and add up adjacent terms</td></tr><tr valign="top"><td>14:</td><td>              <inline-formula><mml:math id="inf345"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>15:</td><td>              <inline-formula><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>16:</td><td>              <bold> for</bold> <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>:=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>do</bold> <inline-formula><mml:math id="inf348"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>end for</bold> </td><td align="right"/></tr><tr valign="top"><td>17:</td><td>              <bold> for</bold> <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>:=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>do</bold> <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">←</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>end for</bold> </td><td align="right"/></tr><tr valign="top"><td>18:</td><td>       <bold>end for</bold></td><td align="right"/></tr><tr valign="top"><td>19:</td><td>      <bold> for</bold> <inline-formula><mml:math id="inf351"><mml:mrow><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>:=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> <bold>do</bold></td><td align="right"/></tr><tr valign="top"><td>20:</td><td>              <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>21:</td><td>       <bold>end for</bold></td><td align="right"/></tr><tr valign="top"><td>22:</td><td>       <bold>for</bold> <inline-formula><mml:math id="inf353"><mml:mrow><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>:=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> <bold>do</bold></td><td align="right"/></tr><tr valign="top"><td>23:</td><td>              <inline-formula><mml:math id="inf354"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right"/></tr><tr valign="top"><td>24:</td><td>       <bold>end for</bold></td><td align="right"/></tr><tr valign="top"><td>25:</td><td><bold>end function</bold></td><td align="right"/></tr></tbody></table></table-wrap><table-wrap id="inlinetable6" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td colspan="3"><bold>Pseudocode C6</bold> Power computation of M<sc>gc</sc> against a given distribution. By repeatedly sampling from the joint distribution <inline-formula><mml:math id="inf355"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, sample data of size <inline-formula><mml:math id="inf356"><mml:mi>n</mml:mi></mml:math></inline-formula> under the null and the alternative are generated for <inline-formula><mml:math id="inf357"><mml:mi>r</mml:mi></mml:math></inline-formula> Monte-Carlo replicates. The power of M<sc>gc</sc> follows by computing the test statistic under the null and the alternative using Algorithm C2. In the simulations we use <inline-formula><mml:math id="inf358"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>,<inline-formula><mml:math id="inf359"><mml:mn>000</mml:mn></mml:math></inline-formula> MC replicates. Note that power computation for other benchmarks follows from the same algorithm by plugging in the respective test statistic.</td></tr><tr valign="top"><td colspan="3"><bold>Input:</bold> A joint distribution <inline-formula><mml:math id="inf360"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the sample size <inline-formula><mml:math id="inf361"><mml:mi>n</mml:mi></mml:math></inline-formula>, the number of MC replicates <inline-formula><mml:math id="inf362"><mml:mi>r</mml:mi></mml:math></inline-formula>, and the type <inline-formula><mml:math id="inf363"><mml:mn>1</mml:mn></mml:math></inline-formula> error level <inline-formula><mml:math id="inf364"><mml:mi>a</mml:mi></mml:math></inline-formula>.</td></tr><tr valign="top"><td colspan="3"><bold>Output:</bold> The power <inline-formula><mml:math id="inf365"><mml:mi>ß</mml:mi></mml:math></inline-formula> of M<sc>gc</sc>.</td></tr><tr valign="top"><td>1:</td><td><bold>function</bold> MGCP<sc>ower(</sc><inline-formula><mml:math id="inf366"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf367"><mml:mi>n</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf368"><mml:mi>r</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf369"><mml:mi>a</mml:mi></mml:math></inline-formula>)</td><td align="right"/></tr><tr valign="top"><td>2:</td><td>      <bold> for</bold> <inline-formula><mml:math id="inf370"><mml:mrow><mml:mi>t</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> <bold>do</bold></td><td align="right"/></tr><tr valign="top"><td>3:</td><td>             <bold>for</bold> <inline-formula><mml:math id="inf371"><mml:mrow><mml:mi>i</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> <bold>do</bold></td><td align="right"/></tr><tr valign="top"><td>4:</td><td>                      <inline-formula><mml:math id="inf372"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mo>∼</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mover></mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mo>∼</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mover></mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">sample from null</td></tr><tr valign="top"><td>5:</td><td>                      <inline-formula><mml:math id="inf373"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mo>∼</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mover></mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">sample from alternative</td></tr><tr valign="top"><td>6:</td><td>             <bold>end for</bold></td><td align="right"/></tr><tr valign="top"><td>7:</td><td>             <bold>for</bold> <inline-formula><mml:math id="inf374"><mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>:=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> <bold>do</bold></td><td align="right"/></tr><tr valign="top"><td>8:</td><td>                      <inline-formula><mml:math id="inf375"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf376"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">pairwise distances under the null</td></tr><tr valign="top"><td>9:</td><td>                      <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf378"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">pairwise distances under the alternative</td></tr><tr valign="top"><td>10:</td><td>             <bold>end for</bold></td><td align="right"/></tr><tr valign="top"><td>11:</td><td>             <inline-formula><mml:math id="inf379"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">M<sc>gc</sc> statistic under the null</td></tr><tr valign="top"><td>12:</td><td>             <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">M<sc>gc</sc> statistic under the alternative</td></tr><tr valign="top"><td>13:</td><td>       <bold>end for</bold></td><td align="right"/></tr><tr valign="top"><td>14:</td><td>       <inline-formula><mml:math id="inf381"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mstyle mathsize="0.75em"><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">F</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">the critical value of M<sc>gc</sc> under the null</td></tr><tr valign="top"><td>15:</td><td>       <inline-formula><mml:math id="inf382"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right">compute power by the alternative distribution</td></tr><tr valign="top"><td>16:</td><td><bold>end function</bold></td><td align="right"/></tr></tbody></table></table-wrap></sec><sec id="s4-3"><title>Simulation dependence functions</title><p>This section provides the 20 different dependency functions used in the simulations. We used essentially the exact same relationships as previous publications to ensure a fair comparison (<xref ref-type="bibr" rid="bib76">Székely et al., 2007</xref>; <xref ref-type="bibr" rid="bib71">Simon and Tibshirani, 2012</xref>; <xref ref-type="bibr" rid="bib28">Gorfine et al., 2012</xref>). We only made changes to add white noise and a weight vector for higher dimensions, thereby making them more difficult, to better compare all methods throughout different dimensions and sample sizes. A few additional relationships are also included.</p><p>For each sample <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, we denote <inline-formula><mml:math id="inf384"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as the <inline-formula><mml:math id="inf385"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> dimension of the vector <inline-formula><mml:math id="inf386"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the purpose of high-dimensional simulations, <inline-formula><mml:math id="inf387"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is a decaying vector with <inline-formula><mml:math id="inf388"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for each <inline-formula><mml:math id="inf389"><mml:mi>d</mml:mi></mml:math></inline-formula>, such that <inline-formula><mml:math id="inf390"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a weighted summation of all dimensions of <inline-formula><mml:math id="inf391"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Furthermore, <inline-formula><mml:math id="inf392"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the uniform distribution on the interval <inline-formula><mml:math id="inf393"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf394"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℬ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the Bernoulli distribution with probability <inline-formula><mml:math id="inf395"><mml:mi>p</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf396"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the normal distribution with mean <inline-formula><mml:math id="inf397"><mml:mi>µ</mml:mi></mml:math></inline-formula> and covariance <inline-formula><mml:math id="inf398"><mml:mi mathvariant="normal">S</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf399"><mml:mi>U</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf400"><mml:mi>V</mml:mi></mml:math></inline-formula> represent some auxiliary random variables, <inline-formula><mml:math id="inf401"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a scalar constant to control the noise level (which equals <inline-formula><mml:math id="inf402"><mml:mn>1</mml:mn></mml:math></inline-formula> for one-dimensional simulations and 0 otherwise), and <inline-formula><mml:math id="inf403"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a white noise from independent standard normal distribution unless mentioned otherwise.</p><p>For all the below equations, <inline-formula><mml:math id="inf404"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mover><mml:mo>∼</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mover><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. For each relationship, we provide the space of <inline-formula><mml:math id="inf405"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and define <inline-formula><mml:math id="inf406"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf407"><mml:msub><mml:mi>F</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:math></inline-formula>, as well as any additional auxiliary distributions.</p><p>1. Linear <inline-formula><mml:math id="inf408"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ10"><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>κ</mml:mi><mml:mi>ϵ</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>2. Exponential <inline-formula><mml:math id="inf409"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ11"><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>10</mml:mn><mml:mi>κ</mml:mi><mml:mi>ϵ</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>3. Cubic <inline-formula><mml:math id="inf410"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ12"><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>Y</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mn>128</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>48</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>12</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>80</mml:mn><mml:mi>κ</mml:mi><mml:mi>ϵ</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>4. Joint normal <inline-formula><mml:math id="inf411"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>: Let <inline-formula><mml:math id="inf412"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf413"><mml:msub><mml:mi>I</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> be the identity matrix of size <inline-formula><mml:math id="inf414"><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf415"><mml:msub><mml:mi>J</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> be the matrix of ones of size <inline-formula><mml:math id="inf416"><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf417"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi>ρ</mml:mi><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ρ</mml:mi><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then<disp-formula id="equ13"><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>5. Step Function <inline-formula><mml:math id="inf418"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ14"><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf419"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the indicator function, that is <inline-formula><mml:math id="inf420"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is unity whenever <inline-formula><mml:math id="inf421"><mml:mi>z</mml:mi></mml:math></inline-formula> true, and zero otherwise.</p><p>6. Quadratic <inline-formula><mml:math id="inf422"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ15"><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mi>κ</mml:mi><mml:mi>ϵ</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>7. W Shape <inline-formula><mml:math id="inf423"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mo>:</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>U</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ16"><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>500</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mi>κ</mml:mi><mml:mi>ϵ</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>8. Spiral <inline-formula><mml:math id="inf424"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mo>:</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>U</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf425"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ17"><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>cos</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:msup><mml:mi>cos</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.4</mml:mn><mml:mi>p</mml:mi><mml:mi>ϵ</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>9. Uncorrelated Bernoulli <inline-formula><mml:math id="inf426"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mo>:</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>U</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">ℬ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ18"><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">ℬ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>U</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>10. Logarithmic <inline-formula><mml:math id="inf427"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msup><mml:mo>:</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ19"><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:mi>κ</mml:mi><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>11. Fourth Root <inline-formula><mml:math id="inf428"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msup><mml:mo>:</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ20"><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mi>κ</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mi>ϵ</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>12. Sine Period <inline-formula><mml:math id="inf429"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>4</mml:mn><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:mi>U</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ21"><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mo>+</mml:mo><mml:mn>0.02</mml:mn><mml:mi>p</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>κ</mml:mi><mml:mi>ϵ</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>13. Sine Period <inline-formula><mml:math id="inf430"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>16</mml:mn><mml:mi>π</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>: Same as above except <inline-formula><mml:math id="inf431"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the noise on <inline-formula><mml:math id="inf432"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is changed to <inline-formula><mml:math id="inf433"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.5</mml:mn><mml:mi>κ</mml:mi><mml:mi>ϵ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>14. Square <inline-formula><mml:math id="inf434"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>: Let <inline-formula><mml:math id="inf435"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>U</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>V</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>8</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then<disp-formula id="equ22"><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mn>0.05</mml:mn><mml:mi>p</mml:mi><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>U</mml:mi><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><bold>for</bold> <inline-formula><mml:math id="inf436"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>15. Two Parabolas <inline-formula><mml:math id="inf437"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>: <inline-formula><mml:math id="inf438"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>U</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">ℬ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ23"><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>κ</mml:mi><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>U</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>16. Circle <inline-formula><mml:math id="inf439"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mo>:</mml:mo><mml:mi>U</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ24"><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.4</mml:mn><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.4</mml:mn><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>17. Ellipse <inline-formula><mml:math id="inf440"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>: Same as above except <inline-formula><mml:math id="inf441"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>18. Diamond <inline-formula><mml:math id="inf442"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>: Same as 'Square' except <inline-formula><mml:math id="inf443"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>19. Multiplicative Noise <inline-formula><mml:math id="inf444"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:mi>u</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ25"><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>20. Multimodal Independence <inline-formula><mml:math id="inf445"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>U</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><inline-formula><mml:math id="inf446"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">ℬ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">ℬ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then<disp-formula id="equ26"><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>U</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mn>1.</mml:mn></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>For each distribution, <inline-formula><mml:math id="inf447"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf448"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are dependent except (20); for some relationships (8,14,16-18) they are independent upon conditioning on the respective auxiliary variables, while for others they are 'directly' dependent. A visualization of each dependency with <inline-formula><mml:math id="inf449"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p><p>For the increasing dimension simulation in the main paper, we always set <inline-formula><mml:math id="inf450"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf451"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf452"><mml:mi>p</mml:mi></mml:math></inline-formula> increasing. Note that <inline-formula><mml:math id="inf453"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> for types 4, 10, 12, 13, 14, 18, 19, 20,, otherwise <inline-formula><mml:math id="inf454"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The decaying vector <inline-formula><mml:math id="inf455"><mml:mi>w</mml:mi></mml:math></inline-formula> is utilized for <inline-formula><mml:math id="inf456"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to make the high-dimensional relationships more difficult (otherwise, additional dimensions only add more signal). For the one-dimensional simulations, we always set <inline-formula><mml:math id="inf457"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf458"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf459"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was partially supported by the Child Mind Institute Endeavor Scientist Program, the National Science Foundation award DMS-1712947, the National Security Science and Engineering Faculty Fellowship (NSSEFF), the Johns Hopkins University Human Language Technology Center of Excellence (JHU HLT COE), the Defense Advanced Research Projects Agency’s (DARPA) SIMPLEX program through SPAWAR contract N66001-15-C-4041, the XDATA program of DARPA administered through Air Force Research Laboratory contract FA8750-12-2-0303, DARPA Lifelong Learning Machines program through contract FA8650-18-2-7834, the Office of Naval Research contract N00014-12-1-0601, the Air Force Office of Scientific Research contract FA9550-14-1-0033. The authors thank Dr. Brett Mensh of Optimize Science for acting as our intellectual consigliere, Julia Kuhl for help with figures, and Dr. Ruth Heller, Dr. Bert Vogelstein, Dr. Don Geman, and Dr. Yakir Reshef for insightful suggestions. And we’d like to thank Satish Palaniappan, Sambit Panda, and Junhao (Bear) Xiong for porting the code to Python.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Software</p></fn><fn fn-type="con" id="con3"><p>Data curation</p></fn><fn fn-type="con" id="con4"><p>Supervision</p></fn><fn fn-type="con" id="con5"><p>Conceptualization</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Formal analysis, Investigation, Visualization, Methodology, Writing—review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.41690.018</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-41690-transrepform-v2.pdf"/></supplementary-material><sec id="s10" sec-type="data-availability"><title>Data availability</title><p>To facilitate reproducibility, we make all datasets available from: <ext-link ext-link-type="uri" xlink:href="https://github.com/neurodata/MGC-paper/tree/master/Data/Preprocessed">https://github.com/neurodata/MGC-paper/tree/master/Data/Preprocessed</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/MGC-paper">https://github.com/elifesciences-publications/MGC-paper</ext-link>).</p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelstein</surname> <given-names>JS</given-names></name><name><surname>Shehzad</surname> <given-names>Z</given-names></name><name><surname>Mennes</surname> <given-names>M</given-names></name><name><surname>Deyoung</surname> <given-names>CG</given-names></name><name><surname>Zuo</surname> <given-names>XN</given-names></name><name><surname>Kelly</surname> <given-names>C</given-names></name><name><surname>Margulies</surname> <given-names>DS</given-names></name><name><surname>Bloomfield</surname> <given-names>A</given-names></name><name><surname>Gray</surname> <given-names>JR</given-names></name><name><surname>Castellanos</surname> <given-names>FX</given-names></name><name><surname>Milham</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Personality is reflected in the brain's intrinsic functional architecture</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e27633</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0027633</pub-id><pub-id pub-id-type="pmid">22140453</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allard</surname> <given-names>WK</given-names></name><name><surname>Chen</surname> <given-names>G</given-names></name><name><surname>Maggioni</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Multi-scale geometric methods for data sets II: geometric Multi-Resolution analysis</article-title><source>Applied and Computational Harmonic Analysis</source><volume>32</volume><fpage>435</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1016/j.acha.2011.08.001</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname> <given-names>Y</given-names></name><name><surname>Hochberg</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berk</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>L</given-names></name><name><surname>Buja</surname> <given-names>A</given-names></name><name><surname>Zhang</surname> <given-names>K</given-names></name><name><surname>Zhao</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Valid post-selection inference</article-title><source>The Annals of Statistics</source><volume>41</volume><fpage>802</fpage><lpage>837</lpage><pub-id pub-id-type="doi">10.1214/12-AOS1077</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhat</surname> <given-names>K</given-names></name><name><surname>Wang</surname> <given-names>F</given-names></name><name><surname>Ma</surname> <given-names>Q</given-names></name><name><surname>Li</surname> <given-names>Q</given-names></name><name><surname>Mallik</surname> <given-names>S</given-names></name><name><surname>Hsieh</surname> <given-names>TC</given-names></name><name><surname>Wu</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Advances in biomarker research for pancreatic cancer</article-title><source>Current Pharmaceutical Design</source><volume>18</volume><fpage>2439</fpage><lpage>2451</lpage><pub-id pub-id-type="doi">10.2174/13816128112092439</pub-id><pub-id pub-id-type="pmid">22372502</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biswal</surname> <given-names>BB</given-names></name><name><surname>Mennes</surname> <given-names>M</given-names></name><name><surname>Zuo</surname> <given-names>XN</given-names></name><name><surname>Gohel</surname> <given-names>S</given-names></name><name><surname>Kelly</surname> <given-names>C</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Beckmann</surname> <given-names>CF</given-names></name><name><surname>Adelstein</surname> <given-names>JS</given-names></name><name><surname>Buckner</surname> <given-names>RL</given-names></name><name><surname>Colcombe</surname> <given-names>S</given-names></name><name><surname>Dogonowski</surname> <given-names>AM</given-names></name><name><surname>Ernst</surname> <given-names>M</given-names></name><name><surname>Fair</surname> <given-names>D</given-names></name><name><surname>Hampson</surname> <given-names>M</given-names></name><name><surname>Hoptman</surname> <given-names>MJ</given-names></name><name><surname>Hyde</surname> <given-names>JS</given-names></name><name><surname>Kiviniemi</surname> <given-names>VJ</given-names></name><name><surname>Kötter</surname> <given-names>R</given-names></name><name><surname>Li</surname> <given-names>SJ</given-names></name><name><surname>Lin</surname> <given-names>CP</given-names></name><name><surname>Lowe</surname> <given-names>MJ</given-names></name><name><surname>Mackay</surname> <given-names>C</given-names></name><name><surname>Madden</surname> <given-names>DJ</given-names></name><name><surname>Madsen</surname> <given-names>KH</given-names></name><name><surname>Margulies</surname> <given-names>DS</given-names></name><name><surname>Mayberg</surname> <given-names>HS</given-names></name><name><surname>McMahon</surname> <given-names>K</given-names></name><name><surname>Monk</surname> <given-names>CS</given-names></name><name><surname>Mostofsky</surname> <given-names>SH</given-names></name><name><surname>Nagel</surname> <given-names>BJ</given-names></name><name><surname>Pekar</surname> <given-names>JJ</given-names></name><name><surname>Peltier</surname> <given-names>SJ</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name><name><surname>Riedl</surname> <given-names>V</given-names></name><name><surname>Rombouts</surname> <given-names>SA</given-names></name><name><surname>Rypma</surname> <given-names>B</given-names></name><name><surname>Schlaggar</surname> <given-names>BL</given-names></name><name><surname>Schmidt</surname> <given-names>S</given-names></name><name><surname>Seidler</surname> <given-names>RD</given-names></name><name><surname>Siegle</surname> <given-names>GJ</given-names></name><name><surname>Sorg</surname> <given-names>C</given-names></name><name><surname>Teng</surname> <given-names>GJ</given-names></name><name><surname>Veijola</surname> <given-names>J</given-names></name><name><surname>Villringer</surname> <given-names>A</given-names></name><name><surname>Walter</surname> <given-names>M</given-names></name><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Weng</surname> <given-names>XC</given-names></name><name><surname>Whitfield-Gabrieli</surname> <given-names>S</given-names></name><name><surname>Williamson</surname> <given-names>P</given-names></name><name><surname>Windischberger</surname> <given-names>C</given-names></name><name><surname>Zang</surname> <given-names>YF</given-names></name><name><surname>Zhang</surname> <given-names>HY</given-names></name><name><surname>Castellanos</surname> <given-names>FX</given-names></name><name><surname>Milham</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Toward discovery science of human brain function</article-title><source>PNAS</source><volume>107</volume><fpage>4734</fpage><lpage>4739</lpage><pub-id pub-id-type="doi">10.1073/pnas.0911855107</pub-id><pub-id pub-id-type="pmid">20176931</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bjørnebekk</surname> <given-names>A</given-names></name><name><surname>Fjell</surname> <given-names>AM</given-names></name><name><surname>Walhovd</surname> <given-names>KB</given-names></name><name><surname>Grydeland</surname> <given-names>H</given-names></name><name><surname>Torgersen</surname> <given-names>S</given-names></name><name><surname>Westlye</surname> <given-names>LT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neuronal correlates of the five factor model (FFM) of human personality: multimodal imaging in a large healthy sample</article-title><source>NeuroImage</source><volume>65</volume><fpage>194</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.10.009</pub-id><pub-id pub-id-type="pmid">23063449</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bracewell</surname> <given-names>RN</given-names></name><name><surname>Bracewell</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="1986">1986</year><source>The Fourier Transform and Its Applications</source><publisher-loc>New York</publisher-loc><publisher-name>McGraw-Hill</publisher-name></element-citation></ref><ref id="bib9"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bridgeford</surname> <given-names>E</given-names></name><name><surname>Shen</surname> <given-names>C</given-names></name><name><surname>Vogelstein</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>R package for MGC code</data-title><version designator="034795d">034795d</version><publisher-name>GitHub</publisher-name><ext-link ext-link-type="uri" xlink:href="https://github.com/neurodata/mgc">https://github.com/neurodata/mgc</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chaudhuri</surname> <given-names>A</given-names></name><name><surname>Hu</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A fast algorithm for computing distance correlation</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.11332">https://arxiv.org/abs/1810.11332</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coifman</surname> <given-names>RR</given-names></name><name><surname>Maggioni</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Diffusion wavelets</article-title><source>Applied and Computational Harmonic Analysis</source><volume>21</volume><fpage>53</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/j.acha.2006.04.004</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Costa</surname> <given-names>PT</given-names></name><name><surname>McCrae</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>Neo PI-R Professional Manual</source><volume>396</volume><publisher-name> PAR Psychological Assessment R</publisher-name></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craddock</surname> <given-names>C</given-names></name><name><surname>Sikka</surname> <given-names>S</given-names></name><name><surname>Cheung</surname> <given-names>B</given-names></name><name><surname>Khanuja</surname> <given-names>R</given-names></name><name><surname>Ghosh</surname> <given-names>SS</given-names></name><name><surname>Yan</surname> <given-names>C</given-names></name><name><surname>Li</surname> <given-names>Q</given-names></name><name><surname>Lurie</surname> <given-names>D</given-names></name><name><surname>Vogelstein</surname> <given-names>J</given-names></name><name><surname>Burns</surname> <given-names>R</given-names></name><name><surname>Colcombe</surname> <given-names>S</given-names></name><name><surname>Mennes</surname> <given-names>M</given-names></name><name><surname>Kelly</surname> <given-names>C</given-names></name><name><surname>Martino</surname> <given-names>AD</given-names></name><name><surname>Castellanos</surname> <given-names>FX</given-names></name><name><surname>Milham</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Towards automated analysis of connectomes: the configurable pipeline for the analysis of connectomes (C-PAC)</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><pub-id pub-id-type="doi">10.3389/conf.fninf.2014.08.00117</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Da Zheng</surname> <given-names>DM</given-names></name><name><surname>Burns</surname> <given-names>R</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name><name><surname>Priebe</surname> <given-names>CE</given-names></name><name><surname>Szalay</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>FlashGraph: processing Billion-Node graphs on an array of commodity SSDs</article-title><conf-name>USENIX Conference on File and Storage Technologies</conf-name></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Da Zheng</surname> <given-names>DM</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name><name><surname>Priebe</surname> <given-names>CE</given-names></name><name><surname>Burns</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>FlashMatrix: parallel, scalable data analysis with generalized matrix operations using commodity SSDs</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1604.06414v1">http://arxiv.org/abs/1604.06414v1</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Da Zheng</surname> <given-names>RB</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name><name><surname>Priebe</surname> <given-names>CE</given-names></name><name><surname>Szalay</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>An SSD-based eigensolver for spectral analysis on billion-node graphs</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1602.01421">https://arxiv.org/abs/1602.01421</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Da Zheng</surname> <given-names>DM</given-names></name><name><surname>Lyzinski</surname> <given-names>V</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name><name><surname>Priebe</surname> <given-names>CE</given-names></name><name><surname>Burns</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016c</year><article-title>Semi-External Memory Sparse Matrix Multiplication on Billion-node Graphs in a Multicore Architecture</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1602.02864">http://arxiv.org/abs/1602.02864</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Daubechies</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>Ten Lectures on Wavelets</source><publisher-name>SIAM</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeYoung</surname> <given-names>CG</given-names></name><name><surname>Hirsh</surname> <given-names>JB</given-names></name><name><surname>Shane</surname> <given-names>MS</given-names></name><name><surname>Papademetris</surname> <given-names>X</given-names></name><name><surname>Rajeevan</surname> <given-names>N</given-names></name><name><surname>Gray</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Testing predictions from personality neuroscience. brain structure and the big five</article-title><source>Psychological Science</source><volume>21</volume><fpage>820</fpage><lpage>828</lpage><pub-id pub-id-type="doi">10.1177/0956797610370159</pub-id><pub-id pub-id-type="pmid">20435951</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Edelsbrunner</surname> <given-names>H</given-names></name><name><surname>Harer</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Computational Topology: An Introduction</source><publisher-name>American Mathematical Society</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eklund</surname> <given-names>A</given-names></name><name><surname>Andersson</surname> <given-names>M</given-names></name><name><surname>Josephson</surname> <given-names>C</given-names></name><name><surname>Johannesson</surname> <given-names>M</given-names></name><name><surname>Knutsson</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Does Parametric fMRI analysis with SPM yield valid results? an empirical study of 1484 rest datasets</article-title><source>NeuroImage</source><volume>61</volume><fpage>565</fpage><lpage>578</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.03.093</pub-id><pub-id pub-id-type="pmid">22507229</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eklund</surname> <given-names>A</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name><name><surname>Knutsson</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cluster failure: why fMRI inferences for spatial extent have inflated false-positive rates</article-title><source>PNAS</source><volume>113</volume><fpage>7900</fpage><lpage>7905</lpage><pub-id pub-id-type="doi">10.1073/pnas.1602413113</pub-id><pub-id pub-id-type="pmid">27357684</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frantzi</surname> <given-names>M</given-names></name><name><surname>Bhat</surname> <given-names>A</given-names></name><name><surname>Latosinska</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Clinical proteomic biomarkers: relevant issues on study design &amp; technical considerations in biomarker development</article-title><source>Clinical and Translational Medicine</source><volume>3</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.1186/2001-1326-3-7</pub-id><pub-id pub-id-type="pmid">24679154</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname> <given-names>JH</given-names></name><name><surname>Rafsky</surname> <given-names>LC</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Graph-Theoretic measures of multivariate association and prediction</article-title><source>The Annals of Statistics</source><volume>11</volume><fpage>377</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1214/aos/1176346148</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Giryes</surname> <given-names>R</given-names></name><name><surname>Sapiro</surname> <given-names>G</given-names></name><name><surname>Bronstein</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks with random gaussian weights: a universal classification strategy</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1504.08291">https://arxiv.org/abs/1504.08291</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Good</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Permutation, Parametric, and Bootstrap Tests of Hypotheses</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname> <given-names>I</given-names></name><name><surname>Pouget-Abadie</surname> <given-names>J</given-names></name><name><surname>Mirza</surname> <given-names>M</given-names></name><name><surname>Xu</surname> <given-names>B</given-names></name><name><surname>Warde-Farley</surname> <given-names>D</given-names></name><name><surname>Ozair</surname> <given-names>S</given-names></name><name><surname>Courville</surname> <given-names>A</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Generative adversarial nets</chapter-title><source>Advances in Neural Information Processing System</source><publisher-name>MIT Press</publisher-name><fpage>2672</fpage><lpage>2680</lpage></element-citation></ref><ref id="bib28"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Gorfine</surname> <given-names>M</given-names></name><name><surname>Heller</surname> <given-names>R</given-names></name><name><surname>Heller</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Comment on Detecting Novel Associations in Large Data Sets</source><publisher-name>Israel Institute of Technology</publisher-name></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gretton</surname> <given-names>A</given-names></name><name><surname>Borgwardt</surname> <given-names>KM</given-names></name><name><surname>Rasch</surname> <given-names>M</given-names></name><name><surname>Schölkopf</surname> <given-names>B</given-names></name><name><surname>Smola</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><chapter-title>A kernel method for the two-sample-problem</chapter-title><source>Advances in Neural Information Processing Systems</source><publisher-name>MIT Press</publisher-name><fpage>513</fpage><lpage>520</lpage></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gretton</surname> <given-names>A</given-names></name><name><surname>Gyorfi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Consistent nonparametric tests of independence</article-title><source>Journal of Machine Learning Research</source><volume>11</volume><fpage>1391</fpage><lpage>1423</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guillot</surname> <given-names>G</given-names></name><name><surname>Rousset</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dismantling the mantel tests</article-title><source>Methods in Ecology and Evolution</source><volume>4</volume><fpage>336</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1111/2041-210x.12018</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname> <given-names>T</given-names></name><name><surname>Tibshirani</surname> <given-names>R</given-names></name><name><surname>Friedman</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Elements of Statistical Learning</source><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helfman</surname> <given-names>D</given-names></name><name><surname>Flynn</surname> <given-names>P</given-names></name><name><surname>Khan</surname> <given-names>P</given-names></name><name><surname>Saeed</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Tropomyosin as a regulator of cancer cell transformation</article-title><source>Advances in Experimental Medicine and Biology</source><volume>644</volume><fpage>124</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1007/978-0-387-85766-4_10</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heller</surname> <given-names>R</given-names></name><name><surname>Heller</surname> <given-names>Y</given-names></name><name><surname>Gorfine</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A consistent multivariate test of association based on ranks of distances</article-title><source>Biometrika</source><volume>100</volume><fpage>503</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1093/biomet/ass070</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heller</surname> <given-names>R</given-names></name><name><surname>Heller</surname> <given-names>Y</given-names></name><name><surname>Kaufman</surname> <given-names>S</given-names></name><name><surname>Brill</surname> <given-names>B</given-names></name><name><surname>Gorfine</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Consistent distribution-free -sample and independence tests for univariate random variables</article-title><source>Journal of Machine Learning Research</source><volume>17</volume><fpage>1</fpage><lpage>54</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoeffding</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>A Non-Parametric test of independence</article-title><source>The Annals of Mathematical Statistics</source><volume>19</volume><fpage>546</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177730150</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hotelling</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1936">1936</year><article-title>Relations between two sets of variates</article-title><source>Biometrika</source><volume>28</volume><fpage>321</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1093/biomet/28.3-4.321</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>C</given-names></name><name><surname>Huo</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A statistically and numerically efficient independence test based on random projections and distance covariance</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1701.06054">https://arxiv.org/abs/1701.06054</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huo</surname> <given-names>X</given-names></name><name><surname>Székely</surname> <given-names>GJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast computing for distance covariance</article-title><source>Technometrics</source><volume>58</volume><fpage>435</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1080/00401706.2015.1054435</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Josse</surname> <given-names>J</given-names></name><name><surname>Holmes</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Measures of dependence between random vectors and tests of independence</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1307.7383">http://arxiv.org/abs/1307.7383</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname> <given-names>RE</given-names></name><name><surname>Segall</surname> <given-names>JM</given-names></name><name><surname>Jeremy Bockholt</surname> <given-names>H</given-names></name><name><surname>Flores</surname> <given-names>RA</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Chavez</surname> <given-names>RS</given-names></name><name><surname>Haier</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neuroanatomy of creativity</article-title><source>Human Brain Mapping</source><volume>43</volume><fpage>NA</fpage><pub-id pub-id-type="doi">10.1002/hbm.20874</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karsani</surname> <given-names>SA</given-names></name><name><surname>Saihen</surname> <given-names>NA</given-names></name><name><surname>Zain</surname> <given-names>RB</given-names></name><name><surname>Cheong</surname> <given-names>SC</given-names></name><name><surname>Abdul Rahman</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Comparative proteomics analysis of oral cancer cell lines: identification of cancer associated proteins</article-title><source>Proteome Science</source><volume>12</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1186/1477-5956-12-3</pub-id><pub-id pub-id-type="pmid">24422745</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kendall</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="1970">1970</year><source>Rank Correlation Methods</source><publisher-loc>London</publisher-loc><publisher-name>Griffin</publisher-name></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kettenring</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Canonical analysis of several sets of variables</article-title><source>Biometrika</source><volume>58</volume><fpage>433</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1093/biomet/58.3.433</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koutra</surname> <given-names>D</given-names></name><name><surname>Shah</surname> <given-names>N</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name><name><surname>Gallagher</surname> <given-names>BJ</given-names></name><name><surname>Faloutsos</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>DeltaCon: a principled massive-graph similarity function</article-title><source>ACM Transactions on Knowledge Discovery From Data</source><volume>10</volume><pub-id pub-id-type="doi">10.1145/2824443</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lam</surname> <given-names>CY</given-names></name><name><surname>Yip</surname> <given-names>CW</given-names></name><name><surname>Poon</surname> <given-names>TC</given-names></name><name><surname>Cheng</surname> <given-names>CK</given-names></name><name><surname>Ng</surname> <given-names>EW</given-names></name><name><surname>Wong</surname> <given-names>NC</given-names></name><name><surname>Cheung</surname> <given-names>PF</given-names></name><name><surname>Lai</surname> <given-names>PB</given-names></name><name><surname>Ng</surname> <given-names>IO</given-names></name><name><surname>Fan</surname> <given-names>ST</given-names></name><name><surname>Cheung</surname> <given-names>ST</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Identification and characterization of tropomyosin 3 associated with granulin-epithelin precursor in human hepatocellular carcinoma</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e40324</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0040324</pub-id><pub-id pub-id-type="pmid">22792281</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>HH</given-names></name><name><surname>Lim</surname> <given-names>CA</given-names></name><name><surname>Cheong</surname> <given-names>YT</given-names></name><name><surname>Singh</surname> <given-names>M</given-names></name><name><surname>Gam</surname> <given-names>LH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Comparison of protein expression profiles of different stages of lymph nodes metastasis in breast cancer</article-title><source>International Journal of Biological Sciences</source><volume>8</volume><fpage>353</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.7150/ijbs.3157</pub-id><pub-id pub-id-type="pmid">22393307</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>JA</given-names></name><name><surname>Verleysen</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Nonlinear Dimensionality Reduction</source><publisher-name>Springer Science &amp; Business Media</publisher-name></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Levina</surname> <given-names>E</given-names></name><name><surname>Bickel</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><chapter-title>Maximum likelihood estimation of intrinsic dimension</chapter-title><source>Advances in Neural Information Processing Systems</source><publisher-name> MIT Press</publisher-name></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>R</given-names></name><name><surname>Zhong</surname> <given-names>W</given-names></name><name><surname>Zhu</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Feature screening via distance correlation learning</article-title><source>Journal of the American Statistical Association</source><volume>107</volume><fpage>1129</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1080/01621459.2012.695654</pub-id><pub-id pub-id-type="pmid">25249709</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyons</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Distance covariance in metric spaces</article-title><source>The Annals of Probability</source><volume>41</volume><fpage>3284</fpage><lpage>3305</lpage><pub-id pub-id-type="doi">10.1214/12-AOP803</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mantel</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>The detection of disease clustering and a generalized regression approach</article-title><source>Cancer Research</source><volume>27</volume><fpage>209</fpage><lpage>220</lpage><pub-id pub-id-type="pmid">6018555</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muandet</surname> <given-names>K</given-names></name><name><surname>Fukumizu</surname> <given-names>K</given-names></name><name><surname>Sriperumbudur</surname> <given-names>B</given-names></name><name><surname>Schölkopf</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Kernel mean embedding of distributions: A review and beyond</article-title><source>Foundations and Trends in Machine Learning</source><volume>10</volume><fpage>1</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1561/2200000060</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pearl</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Causality: Models, Reasoning, and Inference</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1895">1895</year><article-title>Notes on regression and inheritance in the case of two parents</article-title><source>Proceedings of the Royal Society of London</source><volume>58</volume><fpage>240</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1098/rspl.1895.0041</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prescott</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Quantitative imaging biomarkers: the application of advanced image processing and analysis to clinical and preclinical decision making</article-title><source>Journal of Digital Imaging</source><volume>26</volume><fpage>97</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1007/s10278-012-9465-7</pub-id><pub-id pub-id-type="pmid">22415112</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reimherr</surname> <given-names>M</given-names></name><name><surname>Nicolae</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>On quantifying dependence: a framework for developing interpretable measures</article-title><source>Statistical Science</source><volume>28</volume><fpage>116</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1214/12-STS405</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rényi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>On measures of dependence</article-title><source>Acta Mathematica Academiae Scientiarum Hungaricae</source><volume>10</volume><fpage>441</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1007/BF02024507</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reshef</surname> <given-names>DN</given-names></name><name><surname>Reshef</surname> <given-names>YA</given-names></name><name><surname>Finucane</surname> <given-names>HK</given-names></name><name><surname>Grossman</surname> <given-names>SR</given-names></name><name><surname>McVean</surname> <given-names>G</given-names></name><name><surname>Turnbaugh</surname> <given-names>PJ</given-names></name><name><surname>Lander</surname> <given-names>ES</given-names></name><name><surname>Mitzenmacher</surname> <given-names>M</given-names></name><name><surname>Sabeti</surname> <given-names>PC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Detecting novel associations in large data sets</article-title><source>Science</source><volume>334</volume><fpage>1518</fpage><lpage>1524</lpage><pub-id pub-id-type="doi">10.1126/science.1205438</pub-id><pub-id pub-id-type="pmid">22174245</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzo</surname> <given-names>ML</given-names></name><name><surname>Székely</surname> <given-names>GJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>DISCO analysis: a nonparametric extension of analysis of variance</article-title><source>The Annals of Applied Statistics</source><volume>4</volume><fpage>1034</fpage><lpage>1055</lpage><pub-id pub-id-type="doi">10.1214/09-AOAS245</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzo</surname> <given-names>ML</given-names></name><name><surname>Székely</surname> <given-names>GJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Energy distance</article-title><source>Wiley Interdisciplinary Reviews: Computational Statistics</source><volume>8</volume><fpage>27</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1002/wics.1375</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Roncal</surname> <given-names>WG</given-names></name><name><surname>Koterba</surname> <given-names>ZH</given-names></name><name><surname>Mhembere</surname> <given-names>D</given-names></name><name><surname>Kleissas</surname> <given-names>DM</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name><name><surname>Burns</surname> <given-names>R</given-names></name><name><surname>Bowles</surname> <given-names>AR</given-names></name><name><surname>Donavos</surname> <given-names>DK</given-names></name><name><surname>Ryman</surname> <given-names>S</given-names></name><name><surname>Jung</surname> <given-names>RE</given-names></name><name><surname>Wu</surname> <given-names>L</given-names></name><name><surname>Calhoun</surname> <given-names>VD</given-names></name><name><surname>Jacob Vogelstein</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>MIGRAINE: mri graph reliability analysis and inference for connectomics</article-title><conf-name>Global Conference on Signal and Information Processing</conf-name></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roweis</surname> <given-names>ST</given-names></name><name><surname>Saul</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Nonlinear dimensionality reduction by locally linear embedding</article-title><source>Science</source><volume>290</volume><fpage>2323</fpage><lpage>2326</lpage><pub-id pub-id-type="doi">10.1126/science.290.5500.2323</pub-id><pub-id pub-id-type="pmid">11125150</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sampaio</surname> <given-names>A</given-names></name><name><surname>Soares</surname> <given-names>JM</given-names></name><name><surname>Coutinho</surname> <given-names>J</given-names></name><name><surname>Sousa</surname> <given-names>N</given-names></name><name><surname>Gonçalves</surname> <given-names>ÓF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The big five default brain: functional evidence</article-title><source>Brain Structure and Function</source><volume>219</volume><fpage>1913</fpage><lpage>1922</lpage><pub-id pub-id-type="doi">10.1007/s00429-013-0610-y</pub-id><pub-id pub-id-type="pmid">23881294</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schilling</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Multivariate Two-Sample tests based on nearest neighbors</article-title><source>Journal of the American Statistical Association</source><volume>81</volume><fpage>799</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1080/01621459.1986.10478337</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sejdinovic</surname> <given-names>D</given-names></name><name><surname>Sriperumbudur</surname> <given-names>B</given-names></name><name><surname>Gretton</surname> <given-names>A</given-names></name><name><surname>Fukumizu</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Equivalence of distance-based and RKHS-based statistics in hypothesis testing</article-title><source>The Annals of Statistics</source><volume>41</volume><fpage>2263</fpage><lpage>2291</lpage><pub-id pub-id-type="doi">10.1214/13-AOS1140</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname> <given-names>C</given-names></name><name><surname>Sun</surname> <given-names>M</given-names></name><name><surname>Tang</surname> <given-names>M</given-names></name><name><surname>Priebe</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Generalized canonical correlation analysis for classification</article-title><source>Journal of Multivariate Analysis</source><volume>130</volume><fpage>310</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1016/j.jmva.2014.05.011</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname> <given-names>C</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name><name><surname>Priebe</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Manifold matching using shortest-path distance and joint neighborhood selection</article-title><source>Pattern Recognition Letters</source><volume>92</volume><fpage>41</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2017.04.005</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname> <given-names>C</given-names></name><name><surname>Priebe</surname> <given-names>CE</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>From distance correlation to multiscale graph correlation</article-title><source>Journal of the American Statistical Association</source><fpage>1</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1080/01621459.2018.1543125</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shen</surname> <given-names>C</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The exact equivalence of distance and kernel methods for hypothesis testing</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1806.05514">https://arxiv.org/abs/1806.05514</ext-link></element-citation></ref><ref id="bib71"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simon</surname> <given-names>N</given-names></name><name><surname>Tibshirani</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Comment on &quot;Detecting Novel Associations In Large Data Sets&quot; by Reshef Et Al, Science Dec 16, 2011</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1401.7645">http://arxiv.org/abs/1401.7645</ext-link></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spearman</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1904">1904</year><article-title>The proof and measurement of association between two things</article-title><source>The American Journal of Psychology</source><volume>15</volume><elocation-id>72</elocation-id><pub-id pub-id-type="doi">10.2307/1412159</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname> <given-names>Y</given-names></name><name><surname>Xia</surname> <given-names>Z</given-names></name><name><surname>Shang</surname> <given-names>Z</given-names></name><name><surname>Sun</surname> <given-names>K</given-names></name><name><surname>Niu</surname> <given-names>X</given-names></name><name><surname>Qian</surname> <given-names>L</given-names></name><name><surname>Fan</surname> <given-names>L-Y</given-names></name><name><surname>Cao</surname> <given-names>C-X</given-names></name><name><surname>Xiao</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Facile preparation of salivary extracellular vesicles for cancer proteomics</article-title><source>Scientific Reports</source><volume>6</volume><pub-id pub-id-type="doi">10.1038/srep24669</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussman</surname> <given-names>DL</given-names></name><name><surname>Tang</surname> <given-names>M</given-names></name><name><surname>Fishkind</surname> <given-names>DE</given-names></name><name><surname>Priebe</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A consistent adjacency spectral embedding for stochastic blockmodel graphs</article-title><source>Journal of the American Statistical Association</source><volume>107</volume><fpage>1119</fpage><lpage>1128</lpage><pub-id pub-id-type="doi">10.1080/01621459.2012.699795</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sutherland</surname> <given-names>DJ</given-names></name><name><surname>Tung</surname> <given-names>H-Y</given-names></name><name><surname>Strathmann</surname> <given-names>H</given-names></name><name><surname>De</surname> <given-names>S</given-names></name><name><surname>Ramdas</surname> <given-names>A</given-names></name><name><surname>Smola</surname> <given-names>A</given-names></name><name><surname>Gretton</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Generative models and model criticism via optimized maximum mean discrepancy</article-title><conf-name>International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Székely</surname> <given-names>GJ</given-names></name><name><surname>Rizzo</surname> <given-names>ML</given-names></name><name><surname>Bakirov</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Measuring and testing dependence by correlation of distances</article-title><source>The Annals of Statistics</source><volume>35</volume><fpage>2769</fpage><lpage>2794</lpage><pub-id pub-id-type="doi">10.1214/009053607000000505</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szekely</surname> <given-names>GJ</given-names></name><name><surname>Rizzo</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Testing for equal distributions in high dimension</article-title><source>InterStat</source><volume>10</volume></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Székely</surname> <given-names>GJ</given-names></name><name><surname>Rizzo</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A new test for multivariate normality</article-title><source>Journal of Multivariate Analysis</source><volume>93</volume><fpage>58</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.jmva.2003.12.002</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Székely</surname> <given-names>GJ</given-names></name><name><surname>Rizzo</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Brownian distance covariance</article-title><source>The Annals of Applied Statistics</source><volume>3</volume><fpage>1236</fpage><lpage>1265</lpage><pub-id pub-id-type="doi">10.1214/09-AOAS312</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szekely</surname> <given-names>GJ</given-names></name><name><surname>Rizzo</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The distance correlation t-test of independence in high dimension</article-title><source>Journal of Multivariate Analysis</source><volume>117</volume><fpage>193</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1016/j.jmva.2013.02.012</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Székely</surname> <given-names>GJ</given-names></name><name><surname>Rizzo</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Partial distance correlation with methods for dissimilarities</article-title><source>The Annals of Statistics</source><volume>42</volume><fpage>2382</fpage><lpage>2412</lpage><pub-id pub-id-type="doi">10.1214/14-AOS1255</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname> <given-names>M</given-names></name><name><surname>Athreya</surname> <given-names>A</given-names></name><name><surname>Sussman</surname> <given-names>DL</given-names></name><name><surname>Lyzinski</surname> <given-names>V</given-names></name><name><surname>Park</surname> <given-names>Y</given-names></name><name><surname>Priebe</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A semiparametric Two-Sample hypothesis testing problem for random graphs</article-title><source>Journal of Computational and Graphical Statistics</source><volume>26</volume><fpage>344</fpage><lpage>354</lpage><pub-id pub-id-type="doi">10.1080/10618600.2016.1193505</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name><name><surname>de Silva</surname> <given-names>V</given-names></name><name><surname>Langford</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A global geometric framework for nonlinear dimensionality reduction</article-title><source>Science</source><volume>290</volume><fpage>2319</fpage><lpage>2323</lpage><pub-id pub-id-type="doi">10.1126/science.290.5500.2319</pub-id><pub-id pub-id-type="pmid">11125149</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenhaus</surname> <given-names>A</given-names></name><name><surname>Tenenhaus</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Regularized generalized canonical correlation analysis</article-title><source>Psychometrika</source><volume>76</volume><fpage>257</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1007/s11336-011-9206-8</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>Q</given-names></name><name><surname>Chaerkady</surname> <given-names>R</given-names></name><name><surname>Wu</surname> <given-names>J</given-names></name><name><surname>Hwang</surname> <given-names>HJ</given-names></name><name><surname>Papadopoulos</surname> <given-names>N</given-names></name><name><surname>Kopelovich</surname> <given-names>L</given-names></name><name><surname>Maitra</surname> <given-names>A</given-names></name><name><surname>Matthaei</surname> <given-names>H</given-names></name><name><surname>Eshleman</surname> <given-names>JR</given-names></name><name><surname>Hruban</surname> <given-names>RH</given-names></name><name><surname>Kinzler</surname> <given-names>KW</given-names></name><name><surname>Pandey</surname> <given-names>A</given-names></name><name><surname>Vogelstein</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mutant proteins as cancer-specific biomarkers</article-title><source>PNAS</source><volume>108</volume><fpage>2444</fpage><lpage>2449</lpage><pub-id pub-id-type="doi">10.1073/pnas.1019203108</pub-id><pub-id pub-id-type="pmid">21248225</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Pan</surname> <given-names>W</given-names></name><name><surname>Hu</surname> <given-names>W</given-names></name><name><surname>Tian</surname> <given-names>Y</given-names></name><name><surname>Zhang</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Conditional distance correlation</article-title><source>Journal of the American Statistical Association</source><volume>110</volume><fpage>1726</fpage><lpage>1734</lpage><pub-id pub-id-type="doi">10.1080/01621459.2014.993081</pub-id><pub-id pub-id-type="pmid">26877569</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>Q</given-names></name><name><surname>Zhang</surname> <given-names>M</given-names></name><name><surname>Tomita</surname> <given-names>T</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name><name><surname>Zhou</surname> <given-names>S</given-names></name><name><surname>Papadopoulos</surname> <given-names>N</given-names></name><name><surname>Kinzler</surname> <given-names>KW</given-names></name><name><surname>Vogelstein</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Selected reaction monitoring approach for validating peptide biomarkers</article-title><source>PNAS</source><volume>114</volume><fpage>13519</fpage><lpage>13524</lpage><pub-id pub-id-type="doi">10.1073/pnas.1712731114</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willemse</surname> <given-names>EAJ</given-names></name><name><surname>De Vos</surname> <given-names>A</given-names></name><name><surname>Herries</surname> <given-names>EM</given-names></name><name><surname>Andreasson</surname> <given-names>U</given-names></name><name><surname>Engelborghs</surname> <given-names>S</given-names></name><name><surname>van der Flier</surname> <given-names>WM</given-names></name><name><surname>Scheltens</surname> <given-names>P</given-names></name><name><surname>Crimmins</surname> <given-names>D</given-names></name><name><surname>Ladenson</surname> <given-names>JH</given-names></name><name><surname>Vanmechelen</surname> <given-names>E</given-names></name><name><surname>Zetterberg</surname> <given-names>H</given-names></name><name><surname>Fagan</surname> <given-names>AM</given-names></name><name><surname>Blennow</surname> <given-names>K</given-names></name><name><surname>Bjerke</surname> <given-names>M</given-names></name><name><surname>Teunissen</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neurogranin as cerebrospinal fluid biomarker for alzheimer disease: an assay comparison study</article-title><source>Clinical Chemistry</source><volume>64</volume><fpage>927</fpage><lpage>937</lpage><pub-id pub-id-type="doi">10.1373/clinchem.2017.283028</pub-id><pub-id pub-id-type="pmid">29523639</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witten</surname> <given-names>DM</given-names></name><name><surname>Tibshirani</surname> <given-names>R</given-names></name><name><surname>Hastie</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis</article-title><source>Biostatistics</source><volume>10</volume><fpage>515</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1093/biostatistics/kxp008</pub-id><pub-id pub-id-type="pmid">19377034</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witten</surname> <given-names>DM</given-names></name><name><surname>Tibshirani</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Penalized classification using Fisher's linear discriminant</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>73</volume><fpage>753</fpage><lpage>772</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9868.2011.00783.x</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xing</surname> <given-names>EP</given-names></name><name><surname>Ng</surname> <given-names>AY</given-names></name><name><surname>Jordan</surname> <given-names>MI</given-names></name><name><surname>Russell</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Distance metric learning with application to clustering with side-information</article-title><source>Advances in Neural Information Processing Systems</source><volume>15</volume><fpage>505</fpage><lpage>512</lpage></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname> <given-names>J</given-names></name><name><surname>Potenza</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>White matter integrity and five-factor personality measures in healthy adults</article-title><source>NeuroImage</source><volume>59</volume><fpage>800</fpage><lpage>807</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.07.040</pub-id><pub-id pub-id-type="pmid">21840401</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>J</given-names></name><name><surname>Korley</surname> <given-names>FK</given-names></name><name><surname>Dai</surname> <given-names>M</given-names></name><name><surname>Everett</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Serum neurogranin measurement as a biomarker of acute traumatic brain injury</article-title><source>Clinical Biochemistry</source><volume>48</volume><fpage>843</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1016/j.clinbiochem.2015.05.015</pub-id><pub-id pub-id-type="pmid">26025774</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>JH</given-names></name><name><surname>Chung</surname> <given-names>TD</given-names></name><name><surname>Oldenburg</surname> <given-names>KR</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A simple statistical parameter for use in evaluation and validation of high throughput screening assays</article-title><source>Journal of Biomolecular Screening</source><volume>4</volume><fpage>67</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1177/108705719900400206</pub-id><pub-id pub-id-type="pmid">10838414</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>Z</given-names></name><name><surname>Wang</surname> <given-names>J</given-names></name><name><surname>Zha</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Adaptive manifold learning</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>34</volume><fpage>253</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2011.115</pub-id><pub-id pub-id-type="pmid">21670485</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>Q</given-names></name><name><surname>Filippi</surname> <given-names>S</given-names></name><name><surname>Gretton</surname> <given-names>A</given-names></name><name><surname>Sejdinovic</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Large-scale kernel methods for independence testing</article-title><source>Statistics and Computing</source><volume>28</volume><fpage>113</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1007/s11222-016-9721-7</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname> <given-names>W</given-names></name><name><surname>Zhu</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>An iterative approach to distance correlation-based sure independence screening</article-title><source>Journal of Statistical Computation and Simulation</source><volume>85</volume><fpage>2331</fpage><lpage>2345</lpage><pub-id pub-id-type="doi">10.1080/00949655.2014.928820</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.41690.019</object-id><sec id="s7" sec-type="appendix"><title>Real data processing</title><sec id="s7-1"><title>Brain activity vs personality</title><p>This experiment investigates whether there is any dependency between resting brain activity and personality. Human personality has been intensively studied for many decades; the most widely used and studied approach is the NEO Personality Inventory-Revised the characterized personality along five dimensions (<xref ref-type="bibr" rid="bib12">Costa and McCrae, 1992</xref>).</p><p>This dataset consists of 42 subjects, each with 197 time-steps of resting-state functional magnetic resonance activity (rs-fMRI) activity, as well as the subject’s five-dimensional 'personality'. Adelstein et al. (<xref ref-type="bibr" rid="bib1">Adelstein et al., 2011</xref>) were able to detect dependence between the activity of certain brain regions and dimensions of personality, but lacked the tools to test for dependence of whole brain activity against all five dimensions of personality.</p><p>For the five-factor personality modality, we used the Euclidean distance. For the brain activity modality, we derived the following comparison function. For each scan, (i) run Configurable Pipeline for the Analysis of Connectomes pipeline (<xref ref-type="bibr" rid="bib13">Craddock et al., 2013</xref>) to process the raw brain images yielding a parcellation into 197 regions of interest, (ii) run a spectral analysis on each region and keep the power of band, (iii) bandpass and normalize it to sum to one, (iv) calculate the Kullback-Leibler divergence across regions to obtain a similarity matrix across comparing all regions. Then, use the normalized Hellinger distance to compute distances between each subject.</p></sec><sec id="s7-2"><title>Brain connectivity vs creativity</title><p>This experiment investigates whether there is any dependency between brain structural networks and creativity. Creativity has been extensively studied in psychology; the 'creativity composite index' (CCI) is an index similar to an 'intelligence quotient' but for creativity rather than intelligence (<xref ref-type="bibr" rid="bib41">Jung et al., 2009</xref>).</p><p>This dataset consists of 109 subjects, each with diffusion weighted MRI data as well as the subject’s CCI. Neural correlates of CCI have previously been investigated, though largely using structural MRI and cortical thickness (<xref ref-type="bibr" rid="bib41">Jung et al., 2009</xref>). Previously published results explored the relationship between graphs and CCI (<xref ref-type="bibr" rid="bib45">Koutra et al., 2015</xref>), but did not provide a valid test.</p><p>We used Euclidean distance to compare CCI values. For the raw brain imaging data, we derived the following comparison function. For each scan we estimated brain networks from diffusion and structural MRI data via Migraine, a pipeline for estimating brain networks from diffusion data (<xref ref-type="bibr" rid="bib62">Roncal et al., 2013</xref>). We compute the distance between the graphs using the semi-parametric graph test statistic (<xref ref-type="bibr" rid="bib74">Sussman et al., 2012</xref>; <xref ref-type="bibr" rid="bib68">Shen et al., 2017</xref>; <xref ref-type="bibr" rid="bib82">Tang et al., 2017</xref>), embedding each graph into two dimensions and aligning the embeddings via a Procrustes analysis.</p></sec><sec id="s7-3"><title>Proteins vs cancer</title><p>This experiment investigated whether there is any dependency between abundance levels of peptides in human plasma and the presence of cancers. Selected Reaction Monitoring (SRM) is a targeted quantitative proteomics technique for measuring protein and peptide abundance in complicated biological samples (<xref ref-type="bibr" rid="bib85">Wang et al., 2011</xref>). In a previous study, we used SRM to identify 318 peptides from 33 normal, 10 pancreatic cancer, 28 colorectal cancer, and 24 ovarian cancer samples (<xref ref-type="bibr" rid="bib87">Wang et al., 2017</xref>). Then, using other methods, we identifed three peptides that were implicated in ovarian cancer, and validated them as legitimate biomarkers with a follow-up experiment.</p><p>In this study, we performed the following five sets of tests on those data:</p><list list-type="order"><list-item><p>Ovarian vs. normal for all proteins,</p></list-item><list-item><p>Ovarian vs. normal for each individual protein,</p></list-item><list-item><p>Pancreas vs. normal for all proteins,</p></list-item><list-item><p>Pancreas vs. all others for each individual protein,</p></list-item><list-item><p>Pancreas vs. normal for each individual protein.</p></list-item></list><p>These tests are designed to first validate the M<sc>gc</sc> method from ovarian cancer, then identify biomarkers unique to pancreatic cancer, that is, find a protein that is able to tell the difference between pancreas and normals, as well as pancreas vs all other cancers. For each of the five tests, we create a binary label vector, with <inline-formula><mml:math id="inf460"><mml:mn>1</mml:mn></mml:math></inline-formula> indicating the cancer type of interest for the corresponding subject, and 0 otherwise. Then each algorithm is applied to each task. For all tests we used Euclidean distances and the type 1 error level is set to <inline-formula><mml:math id="inf461"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>. The three test sets assessing individual proteins provide 318 p-values; we used the Benjamini-Hochberg procedure (<xref ref-type="bibr" rid="bib3">Benjamini and Hochberg, 1995</xref>) to control the false discovery rate. A summary of the results are reported in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>.</p><table-wrap id="app1table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.41690.020</object-id><label>Appendix 1—table 1.</label><caption><title>Results for cancer peptide screening.</title><p>The first two rows report the p-values for the tests of interest based on all peptides. The next four rows report the number of significant proteins from individual peptide tests; the Benjamini-Hochberg procedure is used to locate the significant peptides by controlling the false discovery rate at 0.05.</p><p><supplementary-material id="app1table1sdata1"><object-id pub-id-type="doi">10.7554/eLife.41690.021</object-id><label>Appendix 1—table 1—source data 1.</label><caption><title>Ovarian testing results.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-41690-app1-table1-data1-v2.mat"/></supplementary-material></p><p><supplementary-material id="app1table1sdata2"><object-id pub-id-type="doi">10.7554/eLife.41690.022</object-id><label>Appendix 1—table 1—source data 2</label><caption><title>Pancreatic testing results.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-41690-app1-table1-data2-v2.mat"/></supplementary-material></p><p><supplementary-material id="app1table1sdata3"><object-id pub-id-type="doi">10.7554/eLife.41690.023</object-id><label>Appendix 1—table 1—source data 3.</label><caption><title>Peptide screening results for pancreatic.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-41690-app1-table1-data3-v2.mat"/></supplementary-material></p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Testing pairs / Methods</th><th>M<sc>gc</sc></th><th>M<sc>antel</sc></th><th>D<sc>corr</sc></th><th>M<sc>corr</sc></th><th>H<sc>hg</sc></th></tr></thead><tbody><tr><td>1</td><td>Ovar vs. Norm: p-value</td><td><bold>0.0001</bold></td><td><bold>0.0001</bold></td><td><bold>0.0001</bold></td><td><bold>0.0001</bold></td><td><bold>0.0001</bold></td></tr><tr><td>2</td><td>Ovar vs. Norm: # peptides</td><td>218</td><td>190</td><td>186</td><td>178</td><td>225</td></tr><tr><td>3</td><td>Pancr vs. Norm: p-value</td><td><bold>0.0082</bold></td><td>0.0685</td><td>0.0669</td><td>0.0192</td><td>0.0328</td></tr><tr><td>4</td><td>Panc vs. Norm: # peptides</td><td>9</td><td>7</td><td>6</td><td>7</td><td>11</td></tr><tr><td>5</td><td>Panc vs. All: # peptides</td><td>1</td><td>0</td><td>0</td><td>0</td><td>3</td></tr><tr><td>6</td><td># peptides unique to Panc</td><td>1</td><td>0</td><td>0</td><td>0</td><td>2</td></tr><tr><td>7</td><td># false positives for Panc</td><td><bold>0</bold></td><td>n/a</td><td>n/a</td><td>n/a</td><td>1</td></tr></tbody></table></table-wrap><p>All methods are able to successfully detect a dependence between peptide abundances in ovarian cancer samples versus normal samples (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>, line 1). This is likely because there are so many individual peptides that have different abundance distributions between ovarian and normal samples (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>, line 2). Nonetheless, M<sc>gc</sc> identified more putative biomarkers than any of the other methods. While we have not checked all of them with subsequent experiments to identify potential false positives, we do know from previous experiments that three peptides in particular are effective biomarkers.</p><p>All three peptides have p-value ≈ 0 for all methods including M<sc>gc</sc>, that is, they are all correctly identified as significant. However, by ranking the peptides based on the actual test statistic of each peptide, M<sc>gc</sc> is the method that ranks the three known biomarkers the lowest, suggesting that it is the least likely to falsely identify peptides.</p><p>We then investigated the pancreatic samples in an effort to identify biomarkers that are unique to pancreas. We first checked whether the methods could identify a difference using all the peptides. Indeed, three methods found a dependence at the 0.05 level, with M<sc>gc</sc> obtaining the lowest p-value (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>, line 3). We then investigated how many individual peptides the methods identified; all of them found 6 to 11 peptides with a significant difference between pancreatic and normal samples (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>, line 4). Because we were interested in identifying peptides that were uniquely useful for pancreatic cancer, we then compared pancreatic samples to all others. At significance level <inline-formula><mml:math id="inf462"><mml:mn>0.05</mml:mn></mml:math></inline-formula>, only M<sc>gc</sc>, H<sc>sic</sc>, and H<sc>hg</sc> identified peptides that expressed different abundances in this more challenging case, and we list the top four peptides in <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref> along with the corrected p-value for each peptide.</p><table-wrap id="app1table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.41690.024</object-id><label>Appendix 1—table 2.</label><caption><title>For each of M<sc>gc</sc>, D<sc>corr</sc>, M<sc>corr</sc>, H<sc>hg</sc>, H<sc>sic</sc>, M<sc>antel</sc>, P<sc>earson</sc>, and M<sc>ic</sc>, list the top four peptides identified for Panc vs All and the respective corrected p-value using Benjamini-Hochberg.</title><p>Bold indicates a significant peptide at type 1 error level 0.05. The top candidates are very much alike except M<sc>ic</sc>. In particular, neurogranin is consistently among the top candidates for all methods, but is only significant while using M<sc>gc</sc>, H<sc>sic</sc>, and H<sc>hg</sc>; there are two other significant proteins from H<sc>sic</sc> and H<sc>hg</sc>, but they do not further improve the classification performance comparing to just using neurogranin. Note that the p-values from M<sc>antel</sc> and P<sc>earson</sc> are always 1 after Benjamini-Hochberg correction, so their respective top peptides are identified using raw p-values without correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>method</th><th align="center" colspan="4">Top four identified peptides</th></tr></thead><tbody><tr><th>M<sc>gc</sc></th><td><bold>neurogranin</bold></td><td>fibrinogen protein 1</td><td>tropomyosin alpha-3</td><td>ras suppressor protein 1</td></tr><tr><th>p-value</th><td><bold>0.03</bold></td><td>0.33</td><td>0.49</td><td>0.52</td></tr><tr><th>D<sc>corr</sc></th><td>neurogranin</td><td>fibrinogen protein 1</td><td>kinase 6</td><td>twinfilin-2</td></tr><tr><th>p-value</th><td>0.41</td><td>0.60</td><td>0.60</td><td>0.93</td></tr><tr><th>M<sc>corr</sc></th><td>neurogranin</td><td>fibrinogen protein 1</td><td>kinase 6</td><td>tropomyosin alpha-3</td></tr><tr><th>p-value</th><td>0.45</td><td>0.80</td><td>0.80</td><td>0.83</td></tr><tr><th>H<sc>sic</sc></th><td><bold>neurogranin</bold></td><td><bold>tropomyosin alpha-3</bold></td><td>kinase 6</td><td>tripeptidyl-peptidase 2</td></tr><tr><th>p-value</th><td><bold>0.01</bold></td><td><bold>0.01</bold></td><td>0.09</td><td>0.09</td></tr><tr><th>H<sc>hg</sc></th><td><bold>neurogranin</bold></td><td><bold>fibrinogen protein 1</bold></td><td><bold>tropomyosin alpha-3</bold></td><td>platelet basic protein</td></tr><tr><th>p-value</th><td><bold>0.03</bold></td><td><bold>0.03</bold></td><td><bold>0.03</bold></td><td>0.11</td></tr><tr><th>M<sc>antel</sc></th><td>neurogranin</td><td>adenylyl cyclase</td><td>tropomyosin alpha-3</td><td>alpha-actinin-1</td></tr><tr><th>p-value</th><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><th>P<sc>earson</sc></th><td>neurogranin</td><td>adenylyl cyclase</td><td>tropomyosin alpha-3</td><td>alpha-actinin-1</td></tr><tr><th>p-value</th><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><th>M<sc>ic</sc></th><td>kinase B</td><td>S100-A9</td><td>ERF3A</td><td>thymidine</td></tr><tr><th>p-value</th><td>0.15</td><td>0.15</td><td>0.15</td><td>0.15</td></tr></tbody></table></table-wrap><p>All three methods reveal the same unique protein for pancreas: neurogranin. H<sc>sic</sc> identifies another peptide (tropomyosin alpha-3 chain isoform 4), and H<sc>hg</sc> identifies a third peptide (fibrinogen-like protein 1 precursor). However, fibrinogen-like protein 1 precursor is not significant for p-value testing between pancreatic and normal subjects. On the other hand, tropomyosin is a ubiquitously expressed protein, since normal tissues and other cancers will also express tropomyosin and leak it into blood, whereas neurogranin is exclusively expressed only in brain tissues. Moreover, there exists strong evidence of tropomyosin 3 upregulated in other cancers (<xref ref-type="bibr" rid="bib42">Karsani et al., 2014</xref>; <xref ref-type="bibr" rid="bib73">Sun et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib46">Lam et al., 2012</xref>). Therefore, it suggests that the other two peptides identified by H<sc>hg</sc> and H<sc>sic</sc> are likely false positives.</p><p>In fact, neurogranin is always one of the top 4 candidates in all methods except M<sc>ic</sc>; the only difference is that the corrected p-values are not significant enough for other methods. Along with the classification result in <xref ref-type="fig" rid="fig4">Figure 4D</xref> showing that neurogranin alone has the best classification error, M<sc>gc</sc> discovers an ideal candidate for potential biomarker. Moreover, the fact that M<sc>gc</sc>, H<sc>hg</sc> and H<sc>sic</sc> discover the dependency while others cannot implies a nonlinear relationship.</p></sec></sec><sec id="s8" sec-type="appendix"><title>M<sc>gc</sc> does not inflate false positive rates in screening</title><p>In this final experiment, we empirically determine that M<sc>gc</sc> does not inflate false positive rates via a neuroimaging screening. To do so, we extend the work of Eklund et al. (<xref ref-type="bibr" rid="bib21">Eklund et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Eklund et al., 2016</xref>), where a number of parametric methods are shown to largely inflate the false positives. Specifically, we applied M<sc>gc</sc> to test whether there is any dependency between brain voxel activities and random numbers. For each brain region, M<sc>gc</sc> attempts to test the following hypothesis: Is activity of a brain region independent of the time-varying stimuli? Any region that is selected as significant is a false positive by construction. By testing each brain region separately, M<sc>gc</sc> provides a distribution of false positive rates. If M<sc>gc</sc> is valid, the resulting distribution should be centered around the significance level, which is set at 0.05 for these experiments.</p><p>We considered 25 resting state fMRI experiments from the 1000 Functional Connectomes Project consisting of a total of 1583 subjects (<xref ref-type="bibr" rid="bib6">Biswal et al., 2010</xref>). <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> shows the false positive rates of M<sc>gc</sc> for each dataset, which are centered around the critical level 0.05, as it should be. In contrast, many standard parametric methods for fMRI analysis, such as generalized linear models, can significantly increase the false positive rates, depending on the data and pre-processing details (<xref ref-type="bibr" rid="bib21">Eklund et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Eklund et al., 2016</xref>). Moreover, even the proposed solutions to those issues make linearity assumptions, thereby limiting detection to only a small subset of possible dependence functions.</p><fig id="app1fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.41690.025</object-id><label>Appendix 1—figure 1.</label><caption><title>We demonstrate that M<sc>gc</sc> is a valid test that does not inflate the false positives in screening and variable selection.</title><p>This figure shows the density estimate for the false positive rates of applying M<sc>gc</sc> to select the 'falsely significant' brain regions versus independent noise experiments; dots indicate the false positive rate of each experiment. The mean ± standard deviation is 0.0538 ± 0.0394.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41690-app1-fig1-v2.tif"/></fig></sec><sec id="s9" sec-type="appendix"><title>Running time report in experiments</title><p><xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref> lists the actual running time of M<sc>gc</sc> versus other methods for testing on the real data, based on a modern desktop with a six core I7-6850K CPU and 32 GB memory on MATLAB 2017a on Windows 10. The first two experiments are timed based on 1000 permutations, while the screening experiment is timed without permutation, that is compute the test statistic only. Pearson runs the fastest, trailed by M<sc>ic</sc> and then D<sc>corr</sc>. P<sc>earson</sc> and M<sc>ic</sc> are only possible to run in the screening experiment, as the other two experiments are multivariate. The running time of M<sc>gc</sc> is a constant times (about 10) higher than that of D<sc>corr</sc>, and H<sc>hg</sc> is implemented in a running time of <inline-formula><mml:math id="inf463"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and thus significantly slower.</p><table-wrap id="app1table3" position="float"><object-id pub-id-type="doi">10.7554/eLife.41690.026</object-id><label>Appendix 1—table 3.</label><caption><title>The actual testing time (in seconds) on real data.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Data</th><th>Personality</th><th>Creativity</th><th>Screening</th></tr></thead><tbody><tr><th>M<sc>gc</sc></th><td>2.5</td><td>7.5</td><td>1.9</td></tr><tr><th>D<sc>corr</sc></th><td>0.2</td><td>0.4</td><td>0.18</td></tr><tr><th>H<sc>sic</sc></th><td>0.5</td><td>1.7</td><td>0.23</td></tr><tr><th>H<sc>hg</sc></th><td>6.3</td><td>53.4</td><td>12.3</td></tr><tr><th>P<sc>earson</sc></th><td>NA</td><td>NA</td><td>0.03</td></tr><tr><th>M<sc>ic</sc></th><td>NA</td><td>NA</td><td>0.1</td></tr><tr><th>M<sc>rule</sc></th><td/><td/><td/></tr></tbody></table></table-wrap></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.41690.029</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Taylor</surname><given-names>Dane</given-names></name><role>Reviewing Editor</role><aff><institution>University of Buffalo</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Discovering and Deciphering Relationships Across Disparate Data Modalities&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor, Dane Taylor, and a Senior Editor, Joshua Gold. The following individual involved in the review of your submission has agreed to reveal their identity: James D Wilson (Reviewer #2).</p><p>The Reviewers and Reviewing Editor have discussed the reviews with one another and generally found the paper to be well written, clearly organized, and an important advancement to the data-analytics community. While we do not accept the article for publication in its current form, we invite you to resubmit the manuscript after taking the following issues into consideration.</p><p>Summary:</p><p>In this work, the authors Vogelstein et al. introduce a new technique called Multiscale Graph Correlation (MGC) to discover (i.e., statistically infer using locally biased, distance-based hypothesis testing) and decipher (e.g., study the geometry of) pairwise relationships between different modalities of a dataset. Their approach is straightforward, principled, and computationally reasonable. It improves upon widely used relationship-inference techniques including test statistics for detecting linear relationships (e.g., Pearson's correlation coefficient) and nonlinear relationships (e.g., distance-correlation-based analyses including kNN-based methods, kernel-based methods and Mantel's test). Specifically, it provides an improved null-hypothesis relationship-test statistic that requires fewer samples and can be implemented at a marginal increase in computational cost [the algorithm scales as O(n<sup>2</sup>log <italic>n</italic>) for n data points. The authors study the MGC approach using a synthetic dataset comprised on 20 models, finding MGC to typically outperform competing approaches. In addition to identifying pairwise relationships between datasets, the MGC approach is a multiscale analysis and identifies the spatial scale at which the relationship's inference is most powerful. In doing so, the approach yields an MGC-map that provides rich insight into the nature of the relationship (particularly its geometry). To conclude, the authors apply the MGC test statistic to explore relationships for three biological datasets: brain activity and personality, brain connectivity and creativity, and proteomic biomarkers and cancers.</p><p>Essential revisions:</p><p>1) The experimental section is lacking sufficient discussion and citation to the literature on related scientific studies. For example, there are several statements in paragraph 2 of subsection “MGC Identifies Potential Cancer Proteomics Biomarkers” that need citation. (Discussion and citations for the brain study also appear to be missing.)</p><p>2) The authors should provide more detail about how this method scales up to larger datasets. It should be noted that the provided examples are restricted to small datasets, n\le1000. What are the practical limitations on how you might adapt your algorithm to larger data? It would be helpful to provide further results on computational time, similar to Table 4 in the Appendix. In particular, can the authors provide numerical support for their O(<italic>n</italic><sup>2</sup>log <italic>n</italic>) scaling result.</p><p>3) The proteomics study and discussion is lacking exploration. For example: Were any biomarkers besides neurogranin identified as significant and biologically relevant? What are some of the top hits in Panc vs All and Panc VS norm individually? Also, what are some of the top hits for both of these comparisons? Do these top hits also make sense biologically? Also, you mention that &quot;the rest of the global methods did not identify any markers.&quot; Even if no markers were identified to be statistically significant with other methods, could you still consider their relative ranking? In particular, does neurogranin appear in other methods as one of the more important biomarkers? What are the other additional top hits identified by other methods? Where do those hits appear on the scatter plot in Figure 4C?</p><p>4) Step 3 of the MGC algorithm (see subsection “The Multiscale Graph Correlation Procedure”) could use further discussion/motivation. First, the local generalized correlation is normalized in the Appendix but not in the subsection “The Multiscale Graph Correlation Procedure”, which is confusing. Second, the local generalized distance correlations are computed using the intersection of the two graphs (the k-nn and the l-nn graphs). That is, the product of terms <italic>A(i,j) G<sub>k</sub>(i,j) B(i,j) H<sub>l</sub>(i,j)</italic> is nonzero if <italic>(i,j)</italic> are in the &quot;closest&quot; neighborhood for both the graph associated with adjacency matrix G<sub>k</sub> as well as that associated with <italic>H<sub>l</sub></italic>. Could this be too stringent? That is, couldn't there be a significant correlation between two data sets where only one of the data modality coordinates is in the nearest neighbors? In particular, an entry</p><p><italic>A(i,j) G<sub>k</sub>(i,j) B(i,j) H<sub>l</sub>(i,j)</italic></p><p>is treated as the same in the following 3 situations:</p><p>a) G<sub>k</sub>(i,j) = H<sub>l</sub>(i,j) = 0</p><p>b) G<sub>k</sub>(i,j) = 0, H<sub>l</sub>(i,j) = 1</p><p>c) G<sub>k</sub>(i,j) = 1, H<sub>l</sub>(i,j) = 0</p><p>Shouldn't case (a) be treated differently than cases (b) and (c)? It may be useful for the authors to discuss this and explain their choice.</p><p>5) For the original choice of k-nearest neighbors, in practice, how does one choose the distance metric (of course Euclidean is often selected by default)? I know it is context dependent, but is there any general data driven advice for this? The reason why I state this is that one could, if they wanted, basically p-value hunt by choosing the right metric to give a small p-value.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.41690.030</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The experimental section is lacking sufficient discussion and citation to the literature on related scientific studies. For example, there are several statements in paragraph 2 of subsection “MGC Identifies Potential Cancer Proteomics Biomarkers” that need citation. (Discussion and citations for the brain study also appear to be missing.)</p></disp-quote><p>For the proteomics study, subsection “MGC Identifies Potential Cancer Proteomics Biomarkers”, we rephrased and added new citations: Bhat et al., 2012, Frantzi et al., 2014, Wang et al., 2011, Willemse et al., 2018, Yang et al., 2015. The complete text is below:</p><p>“MGC can also be useful for a completely complementary set of scientific questions: screening proteomics data for biomarkers, often involving the analysis of tens of thousands of proteins, peptides, or transcripts in multiple samples representing a variety of disease types. […] The rest of the global methods did not identify any markers, see Materials and method for more details and Table 4 for identified peptide information using each method.”</p><p>We also added a few sentences and citations to the brain imaging study in subsection “MGC Discovers the Relationships between Brain and Mental Properties”:</p><p>“However, the relationship between brain activity and structure, and these aspects of our psyche, remains unclear (Deyoung et al., 2010; Xu and Potenza, 2012; Bjørnebekk et al., 2013; Sampaio et al., 2014). For example, prior work did not evaluate the relationship between entire brain connectivity to all five factors of the standard personality model (Costa and McCraw, 1992).”</p><disp-quote content-type="editor-comment"><p>2) The authors should provide more detail about how this method scales up to larger datasets. It should be noted that the provided examples are restricted to small datasets, n\le1000. What are the practical limitations on how you might adapt your algorithm to larger data? It would be helpful to provide further results on computational time, similar to Table 4 in the Appendix. In particular, can the authors provide numerical support for their O(n<sup>2</sup>log n) scaling result.</p></disp-quote><p>Thank you for the valuable points here. Yes, applying the method directly to millions of data can be slow due to the <italic>n</italic><sup>2</sup> operation and the permutation test. There are a number of ways to reduce the running time for DCORR and HSIC, which is equally applicable to MGC and we are currently working on incorporating all such fast implementations to MGC. We have now modified the Discussion fourth paragraph:</p><p>“Recent advances in related work demonstrated that one could reduce computational time of distance-based tests to close to linear via faster implementation, subsampling, random projection, and null distribution approximation (Huo and Szekely, 2016; Huang and Huo, 2017; Zhang et al., 2017; Chaudhuri and Wenhao, 2018), making it feasible for large amount of data.”</p><p>The simulation time comparison can be found in Figure 6 in Shen and Vogelstein, 2018, which validates that MGC has almost the same complexity as DCORR and HSIC and they differ by a constant. For convenience, it is attached here in <xref ref-type="fig" rid="respfig1">Author response image 1</xref>, and pointed to in subsection “MGC is Computationally Efficient”:</p><p>“…in practice, MGC can be <italic>O(n</italic><sup>2</sup>), meaning only a constant factor slower than DCORR and HSIC, which is illustrated in Figure 6 of Shen et al., 2018.”</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.41690.033</object-id><label>Author response image 1.</label><caption><title>Compute the test statistics of MGC, DCORR, and HSIC for 100 replicates, and then plot the average running time in log scale (clocked using Matlab 2017a on a Windows 10 machine with I7 six-core CPU).</title><p>The sample data are repeatedly generated using the quadratic relationship in Appendix, the sample size increases from 25 to 500, and the dimensionality is fixed at <italic>p</italic> = 1 on the left and <italic>p</italic> = 1000 on the right. In either panel, the three lines differ by some constants in the log scale, suggesting the same running time complexity but different constants. MGC has a higher intercept than the other two, which translates to about a constant of 6 times of DCORR and 3 times of HSIC at <italic>n</italic> = 500 and <italic>p</italic> = 1, and about 3 at <italic>p</italic> = 1000.</p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-41690-resp-fig1-v2"/></fig><disp-quote content-type="editor-comment"><p>3) The proteomics study and discussion is lacking exploration. For example: Were any biomarkers besides neurogranin identified as significant and biologically relevant? What are some of the top hits in Panc vs All and Panc VS norm individually? Also, what are some of the top hits for both of these comparisons? Do these top hits also make sense biologically? Also, you mention that &quot;the rest of the global methods did not identify any markers.&quot; Even if no markers were identified to be statistically significant with other methods, could you still consider their relative ranking? In particular, does neurogranin appear in other methods as one of the more important biomarkers? What are the other additional top hits identified by other methods? Where do those hits appear on the scatter plot in Figure 4C?</p></disp-quote><p>Yes, MGC, HSIC, and HHG all identify neurogranin, while HSIC and HHG identify another two peptides. We do not know the ground truth here, but the other two peptides are related to other cancer types; and a leave-one-out classification also supports using neurogranin alone. The rest of the global methods do not identify any markers after multiple testing corrections, but actually mostly coincide with the MGC discovery in terms of relative ranking. We provide a table (Table 4) for Panc vs All containing the top 4 ranked peptides for each method, (in scatter plot they are the 4 dots with least p-value along the y-axis for MGC method). We first state in paragraph two of subsection “MGC Identifies Potential Cancer Proteomics Biomarkers”:</p><p>“In comparison, HSIC identified neurogranin as well, but it also identified another peptide; HHG identified the same two by HSIC, and a third peptide. […] The rest of the global methods did not identify any markers, see Materials and methods for more details and Table 4 for identified peptide information using each method.”</p><p>In subsection “Proteins vs Cancer”, we add the table and point out that:</p><p>“…Because we were interested in identifying peptides that were uniquely useful for pancreatic cancer, we then compared pancreatic samples to all others. […] Moreover, the fact that MGC, HHG and HSIC discover the dependency while others cannot implies a nonlinear relationship.”</p><disp-quote content-type="editor-comment"><p>4) Step 3 of the MGC algorithm (see subsection “The Multiscale Graph Correlation Procedure”) could use further discussion/motivation. First, the local generalized correlation is normalized in the Appendix but not in the subsection “The Multiscale Graph Correlation Procedure”, which is confusing.</p></disp-quote><p>Thank you for pointing out this discrepancy! We corrected the main text where we define the local correlation measure to the normalized version:</p><p>“For all possible values of <italic>k</italic> and <italic>l</italic> from 1 to <italic>n</italic>:</p><p>a) Compute the <italic>k</italic>-nearest neighbor graphs for one property, and the <italic>l</italic>-nearest neighbor graphs for the other property. Let <italic>Gk</italic> and <italic>Hl</italic> be the adjacency matrices for the nearest neighbor graphs, so that <italic>Gk (i, j</italic>) = 1 indicates that <italic>A (i, j</italic>) is within the <italic>k</italic> smallest values of the <italic>i<sup>th</sup> </italic>row of <italic>A</italic>, and similarly for <italic>Hl</italic>.</p><p>b) Estimate the local correlations—the correlation between distances restricted to only the (<italic>k, l</italic>) neighbors—by summing the products of the above matrices,</p><p>ckl=∑ijAi,jGki,jBi,jHl(i,j)</p><p>c) Normalize <italic>c<sup>kl</sup> </italic>such that the result is always between <italic>−</italic>1 and +1 by dividing by</p><p>∑ijA2(ij)Gk(i,j)x∑ijB2(i,j)Hl(i,j)”</p><disp-quote content-type="editor-comment"><p>Second, the local generalized distance correlations are computed using the intersection of the two graphs (the k-nn and the l-nn graphs). That is, the product of terms A(i,j) G<sub>k</sub>(i,j) B(i,j) H<sub>l</sub>(i,j) is nonzero if (i,j) are in the &quot;closest&quot; neighborhood for both the graph associated with adjacency matrix G<sub>k</sub> as well as that associated with H<sub>l</sub>. Could this be too stringent? That is, couldn't there be a significant correlation between two data sets where only one of the data modality coordinates is in the nearest neighbors? In particular, an entry</p><p>A(i,j) G<sub>k</sub>(i,j) B(i,j) H<sub>l</sub>(i,j)</p><p>is treated as the same in the following 3 situations:</p><p>a) G<sub>k</sub>(i,j) = H<sub>l</sub>(i,j) = 0</p><p>b) G<sub>k</sub>(i,j) = 0, H<sub>l</sub>(i,j) = 1</p><p>c) G<sub>k</sub>(i,j) = 1, H<sub>l</sub>(i,j) = 0</p><p>Shouldn't case (a) be treated differently than cases (b) and (c)? It may be useful for the authors to discuss this and explain their choice.</p></disp-quote><p>The reason we consider the k-nearest-neighbor and l-nearest-neighbor is that most nonlinear relationships are intrinsically local linear relationship, where only small distances exhibit strong relationship. Geometrically speaking, if there exists a dependency structure where the large distance pairs are highly linearly correlated, then the nearest- neighbors must also be highly linear correlated after centering by the average distance, so the global correlation suffices in this case. It is actually easy to consider only the furthest neighbors in MGC by simply reverting the ranking scheme (so it is sorted in descending order and <italic>k</italic> = 1 includes the largest distance pair), but it does not work better than the global correlation in any simulation, implying the global correlation is able to capture case (a). On the other hand, if small distances in one modality correspond to large distances in another modality, their product after centering is a negative term, causing the test statistic to be smaller. Moreover, since distance correlation is proved larger than 0 if and only if dependency, it cannot happen that small distances always correspond to large distances. We modified the discussion of MGC on in subsection “Local Correlations” to discuss this point.</p><p>“As most nonlinear relationships intrinsically exhibit a local linear structure, considering the nearest-neighbors is able to amplify the dependency signal over the global correlation. […] Therefore considering the nearest-neighbor may significantly improve the performance over global correlation, while considering the other scenarios does not.”</p><disp-quote content-type="editor-comment"><p>5) For the original choice of k-nearest neighbors, in practice, how does one choose the distance metric (of course Euclidean is often selected by default)? I know it is context dependent, but is there any general data driven advice for this? The reason why I state this is that one could, if they wanted, basically p-value hunt by choosing the right metric to give a small p-value.</p></disp-quote><p>Thank you for pointing this out. Yes, additional care is needed if one opts to experiment on multiple metric choices. Either one has to correct the smallest p-value for multiple testing, or design a proper procedure to produce a single p-value that correctly controls the type 1 error. We added a paragraph to the Discussion section (second paragraph) to address this issue.</p><p>“The default metric choice of MGC in this paper is always the Euclidean distance, other metric choices may be more appropriate in different fields, and using the strong negative type metric as specified in (Lyons, 2013) can guarantee the consistency property. […] Such a testing procedure properly controls the type 1 error level without the need for additional correction.”</p></body></sub-article></article>