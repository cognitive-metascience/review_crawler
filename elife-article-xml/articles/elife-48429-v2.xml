<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">48429</article-id><article-id pub-id-type="doi">10.7554/eLife.48429</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Rats exhibit similar biases in foraging and intertemporal choice tasks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-141550"><name><surname>Kane</surname><given-names>Gary A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7703-5055</contrib-id><email>gkane@rowland.harvard.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-141551"><name><surname>Bornstein</surname><given-names>Aaron M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6251-6000</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-141552"><name><surname>Shenhav</surname><given-names>Amitai</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-141553"><name><surname>Wilson</surname><given-names>Robert C</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2963-2971</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-41719"><name><surname>Daw</surname><given-names>Nathaniel D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5029-1430</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-23689"><name><surname>Cohen</surname><given-names>Jonathan D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Psychology, Princeton Neuroscience Institute</institution><institution>Princeton University</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Rowland Institute at Harvard</institution><institution>Harvard University</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Cognitive Sciences, Center for the Neurobiology of Learning and Memory</institution><institution>University of California, Irvine</institution><addr-line><named-content content-type="city">Irvine</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Cognitive, Linguistic and Psychological Sciences, Carney Institute for Brain Science</institution><institution>Brown University</institution><addr-line><named-content content-type="city">Providence</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Department of Psychology, Cognitive Science Program</institution><institution>University of Arizona</institution><addr-line><named-content content-type="city">Tucson</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution>University of Texas at Austin</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>Schoenbaum</surname><given-names>Geoffrey</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute on Drug Abuse, National Institutes of Health</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>18</day><month>09</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e48429</elocation-id><history><date date-type="received" iso-8601-date="2019-05-14"><day>14</day><month>05</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-09-17"><day>17</day><month>09</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Kane et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Kane et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-48429-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.48429.001</object-id><p>Animals, including humans, consistently exhibit myopia in two different contexts: foraging, in which they harvest locally beyond what is predicted by optimal foraging theory, and intertemporal choice, in which they exhibit a preference for immediate vs. delayed rewards beyond what is predicted by rational (exponential) discounting. Despite the similarity in behavior between these two contexts, previous efforts to reconcile these observations in terms of a consistent pattern of time preferences have failed. Here, via extensive behavioral testing and quantitative modeling, we show that rats exhibit similar time preferences in both contexts: they prefer immediate vs. delayed rewards and they are sensitive to opportunity costs of delays to future decisions. Further, a quasi-hyperbolic discounting model, a form of hyperbolic discounting with separate components for short- and long-term rewards, explains individual rats’ time preferences across both contexts, providing evidence for a common mechanism for myopic behavior in foraging and intertemporal choice.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.48429.002</object-id><title>eLife digest</title><p>Often decisions have to be made on whether to stick with a resource or leave it behind to search for a better alternative. Should you book that hotel room or continue looking at others? Is it time to start searching for a new job, or even for a new partner? Animals face similar 'stick or twist' decisions when foraging for food. Knowing how to maximize the amount of food you obtain is key to survival. Studies have shown that most animals tend to stick with a food source for a little too long, a phenomenon known as 'overharvesting'.</p><p>To find out why, Kane et al. designed carefully controlled experiments to compare foraging behavior in rats to another form of decision-making, known as intertemporal choice. The latter involves choosing between a small reward now versus a larger reward later. Given this choice, most rats opt to receive a smaller reward now rather than wait for the larger reward. This suggests that rats value rewards available in the future less than rewards they can get immediately.</p><p>Kane et al. showed that this preference for short-term rewards can also explain why rats overharvest in foraging scenarios. By leaving one food source to go in search of another, rats must put up with a delay before they can access the new food supply. This delay, due to the time required to travel and search, reduces the value of the future reward. As a result, rats are more likely to stick with their current food source, even though leaving it would yield a greater reward in the long run.</p><p>These findings in rats raise important questions about the mechanisms that lead to biases in thinking, and how factors like changes in the environment or specific disease states can influence these biases.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>foraging</kwd><kwd>intertemporal choice</kwd><kwd>temporal discounting</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>F31MH109286</award-id><principal-award-recipient><name><surname>Kane</surname><given-names>Gary A</given-names></name><name><surname>Cohen</surname><given-names>Jonathan D</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>In both foraging and intertemporal choice tasks, rats prefer immediate rewards to delayed rewards, and this preference can be explained by a form of hyperbolic discounting.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Serial stay-or-search problems are ubiquitous across many domains, including employment, internet search, mate search, and animal foraging. For instance, in patch foraging problems, animals must choose between an immediately available opportunity for reward or the pursuit of potentially better but more distal opportunities. It is typically assumed that animals seek to maximize the long-term average reward (net of cost) rate, as a proxy for reproductive fitness. The optimal behavior for maximizing this currency in foraging tasks, described by the Marginal Value Theorem (MVT; <xref ref-type="bibr" rid="bib10">Charnov, 1976</xref>), is to choose the immediately available opportunity if it provides a reward rate greater than the average reward rate across all alternative options, which includes the costs of accessing those options. Animals tend to follow the basic predictions of long-term reward maximization: they are generally more likely to pursue opportunities for larger vs. smaller rewards and, if the cost of searching for alternatives is greater, they are more likely to pursue opportunities for smaller rewards (<xref ref-type="bibr" rid="bib42">Stephens and Krebs, 1986</xref>; <xref ref-type="bibr" rid="bib11">Constantino and Daw, 2015</xref>; <xref ref-type="bibr" rid="bib17">Hayden et al., 2011</xref>; <xref ref-type="bibr" rid="bib22">Kane et al., 2017</xref>).</p><p>Although animal behavior follows the basic predictions of optimal foraging behavior described by MVT, in the majority of studies across a variety of species, including humans, non-human primates, and rodents, animals exhibit a consistent bias towards pursuing immediately available rewards relative to predictions of MVT, often referred to as 'overharvesting' (<xref ref-type="bibr" rid="bib11">Constantino and Daw, 2015</xref>; <xref ref-type="bibr" rid="bib17">Hayden et al., 2011</xref>; <xref ref-type="bibr" rid="bib22">Kane et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Nonacs, 2001</xref>; <xref ref-type="bibr" rid="bib25">Kolling et al., 2012</xref>; <xref ref-type="bibr" rid="bib37">Shenhav et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Wikenheiser et al., 2013</xref>; <xref ref-type="bibr" rid="bib9">Carter and Redish, 2016</xref>). Prior studies have proposed two explanations for overharvesting: subjective costs, such as an aversion to rejecting an immediately available reward (<xref ref-type="bibr" rid="bib45">Wikenheiser et al., 2013</xref>; <xref ref-type="bibr" rid="bib9">Carter and Redish, 2016</xref>); and nonlinear reward utility or diminishing returns, by which larger rewards are not perceived as proportionally larger than smaller rewards (<xref ref-type="bibr" rid="bib11">Constantino and Daw, 2015</xref>). But these hypotheses have never been systematically compared in a set of experiments designed to directly test their predictions. Furthermore, according to these rate-maximizing hypotheses, the perceived value of rewards does not differ between situations in which the delays occur before or after reward is received. In this respect, the predictions made by these hypotheses (which are still grounded in a core assumption that animals attempt to maximize the long-term reward rate) are not compatible with an otherwise seemingly similar bias that is widely observed in standard intertemporal choice tasks (also referred to as delay discounting or self-control tasks): a preference for smaller, more immediate rewards over larger, delayed rewards (<xref ref-type="bibr" rid="bib1">Ainslie, 1992</xref>; <xref ref-type="bibr" rid="bib24">Kirby, 1997</xref>).</p><p>The preference for more immediate rewards in intertemporal choice tasks is commonly explained in one of two ways, both assuming that animals choose as though they were optimizing a different currency than long-term reward rate: temporal discounting or short-term rate maximization. According to temporal discounting, the perceived value of a future reward is discounted by the time until its receipt. Temporal discounting can arise even when maximizing the long-term reward rate, for certain environments. In particular, discounting can be adaptive in unstable environments — if the environment is likely to change before future rewards can be acquired, it is appropriate to place greater value on more predictable rewards available in the near future. Under this hypothesis, and the further assumption that expected rewards disappear at a constant rate, a long-term reward rate maximizer would discount rewards exponentially in their delay (<xref ref-type="bibr" rid="bib16">Gallistel and Gibbon, 2000</xref>; <xref ref-type="bibr" rid="bib21">Kacelnik and Todd, 1992</xref>). However, animal preferences typically follow a hyperbolic-like form: the rate of discounting is steeper initially and decreases over time (<xref ref-type="bibr" rid="bib16">Gallistel and Gibbon, 2000</xref>; <xref ref-type="bibr" rid="bib21">Kacelnik and Todd, 1992</xref>; <xref ref-type="bibr" rid="bib44">Thaler, 1981</xref>). This yields inconsistent time preferences or preference reversals: an animal may prefer to wait longer for a larger reward if both options are distant, but will change their mind and prefer the smaller reward as the time to both options draws near (<xref ref-type="bibr" rid="bib1">Ainslie, 1992</xref>; <xref ref-type="bibr" rid="bib24">Kirby, 1997</xref>; <xref ref-type="bibr" rid="bib16">Gallistel and Gibbon, 2000</xref>; <xref ref-type="bibr" rid="bib21">Kacelnik and Todd, 1992</xref>). Recent theoretical work has shown that hyperbolic time preferences may arise from imperfect foresight — if the variance in predicting the timing of future outcomes increases with the delay to the outcome, a long-term reward rate maximizer would exhibit hyperbolic time preferences (<xref ref-type="bibr" rid="bib15">Gabaix and Laibson, 2017</xref>). Similarly, short-term maximization rules predict that animals seek to maximize reward over shorter time horizons; this may also be motivated as an approximation to long-term reward maximization as it may be difficult to accurately predict all future rewards (<xref ref-type="bibr" rid="bib39">Stephens, 2002</xref>; <xref ref-type="bibr" rid="bib40">Stephens et al., 2004</xref>). Along similar lines, <xref ref-type="bibr" rid="bib33">Namboodiri et al. (2014)</xref> argues that, rather than maximizing long-term reward rate into the future, animals may select options that maximize reward rate up to the current point in time or due to environmental factors (e.g. non-stationarity) or biological constraints (e.g. computational constraints), over a finite interval of time. Just as hyperbolic discounting may arise from imperfect foresight (<xref ref-type="bibr" rid="bib15">Gabaix and Laibson, 2017</xref>), maximizing reward rate over shorter time horizons predicts hyperbolic time preferences (<xref ref-type="bibr" rid="bib33">Namboodiri et al., 2014</xref>).</p><p>An alternative explanation for the preference for immediate rewards in intertemporal choice tasks is that animals simply underestimate the duration of post-reward delays; that is, delays between receiving reward and making the next decision (<xref ref-type="bibr" rid="bib35">Pearson et al., 2010</xref>; <xref ref-type="bibr" rid="bib4">Blanchard et al., 2013</xref>). Typically, in intertemporal choice tasks, a variable post-reward delay is added to ensure that the overall amount of time for each trial is equal, regardless of the reward size or the duration of the pre-reward delay. It has been argued that it may be difficult for animals to learn these variable delays, and thus, animals may fail to consider the full duration of the delay in their decision process. Consequently, animals will perceive that it takes less time to acquire the smaller, more immediate reward and overestimate the reward rate for choosing this option. Consistent with this hypothesis, providing an explicit cue for the duration of the post-reward delay or increasing its salience by providing a small reward at the end of the post-reward delay reduces temporal discounting (<xref ref-type="bibr" rid="bib35">Pearson et al., 2010</xref>; <xref ref-type="bibr" rid="bib4">Blanchard et al., 2013</xref>).</p><p>Despite the similarities between overharvesting and the preference for more immediate rewards in intertemporal choice tasks, prior attempts to use temporal discounting and/or short-term rate maximization functions fit to intertemporal choice data to predict foraging behavior have failed (<xref ref-type="bibr" rid="bib8">Carter et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Carter and Redish, 2016</xref>; <xref ref-type="bibr" rid="bib5">Blanchard and Hayden, 2015</xref>). In these studies, animals are typically closer to long-term rate maximization in foraging tasks than in intertemporal choice tasks. This has been taken as further evidence that, while animals have a good understanding of the structure of foraging tasks, they struggle to understand the structure of intertemporal choice tasks (i.e. they fail to incorporate post-reward delays into their decision process; <xref ref-type="bibr" rid="bib5">Blanchard and Hayden, 2015</xref>). However, there are two additional possibilities for why intertemporal choice models have failed to predict foraging behavior. First, models of intertemporal choice tasks usually consider rewards for the current trial and not rewards on future trials since, in these tasks, reward opportunities on future trials are often independent of the current decision. This is not true of foraging tasks, in which future opportunities for rewards depend on the current decision. Thus, this difference in decision horizon may make it difficult to explain foraging data using discounting models fit to intertemporal choice data. Second, these studies have only examined standard, single-parameter exponential and hyperbolic discounting functions. More flexible forms of temporal discounting that produce different patterns of hyperbolic time preferences have never been tested in these contexts. More flexible discounting functions include constant sensitivity discounting, by which rewards in the distant future are discounted less than rewards in the near future due to a bias in time estimation — agents become less sensitive to longer time delays (<xref ref-type="bibr" rid="bib14">Ebert and Prelec, 2007</xref>; <xref ref-type="bibr" rid="bib46">Zauberman et al., 2009</xref>); additive-utility discounting, by which the utility of a reward, not its value, is discounted (<xref ref-type="bibr" rid="bib23">Killeen, 2009</xref>); or quasi-hyperbolic discounting, which has separate terms, or different discount rates, for short- or long-term rewards rewards that correlate with activity in limbic and fronto-parietal networks respectively (<xref ref-type="bibr" rid="bib28">Laibson, 1997</xref>; <xref ref-type="bibr" rid="bib31">McClure et al., 2004</xref>; <xref ref-type="bibr" rid="bib32">McClure et al., 2007</xref>).</p><p>In the present study, we found that rats exhibit similar time preferences in foraging and intertemporal choice tasks and that time preferences in both tasks can be explained by a quasi-hyperbolic discounting model that, in both contexts, considers future rewards. Rats were tested in a series of patch foraging tasks and an intertemporal choice task. In foraging tasks, they followed the basic predictions of long-term rate maximization: they stayed longer in patches that yielded greater rewards and when the cost of searching was greater. But under certain conditions, they violated these predictions in a manner consistent with time preferences: they stayed longer in patches when given larger rewards with proportionally longer delays, and they exhibited greater sensitivity to pre- vs. post-reward delays. Similarly, in an intertemporal choice task, rats exhibited greater sensitivity to pre- vs. post-reward delays. Using these data, we tested several models to determine if temporal discounting or biases in time perception, such as insensitivity to post-reward delays, could explain rats’ behavior across tasks. One model, a quasi-hyperbolic discounting model (<xref ref-type="bibr" rid="bib28">Laibson, 1997</xref>; <xref ref-type="bibr" rid="bib32">McClure et al., 2007</xref>), provided the best fit to rat behavior across all experiments. Furthermore, the quasi-hyperbolic discounting model proved to be externally valid: discounting functions fit to foraging data provided as good a fit to intertemporal choice data as discounting functions fit directly to intertemporal choice data for some rats. These findings suggest that rats exhibit similar biases in the two tasks, and quasi-hyperbolic discounting may be a common mechanism for suboptimal decision-making across tasks.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Rats consider long-term rewards, but exhibit a bias in processing pre- vs. post-reward delays</title><p>Long Evans rats (n = 8) were tested in a series of patch foraging tasks in operant conditioning chambers (<xref ref-type="bibr" rid="bib22">Kane et al., 2017</xref>). To harvest reward (10% sucrose water) from a patch, rats pressed a lever on one side of the front of the chamber (left or right) and reward was delivered in an adjacent port. After a post-reward delay (inter-trial interval or ITI), rats again chose to harvest a smaller reward or to leave the patch by nose poking in the back of the chamber. A nose poke to leave the patch caused the harvest lever to retract and initiated a delay to control the time to travel to the next patch. After the delay, the opposite lever extended (e.g. if the left lever was extended previously, the right lever would be extended now), and rats could then harvest from (or leave) this replenished patch (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>In four separate experiments, we manipulated different variables of the foraging environment: (i) in the ‘Travel Time Experiment,' a 10 s vs. 30 s delay was imposed between patches, (ii) in the ‘Depletion Rate Experiment,' reward depleted at a rate of 8 vs. 16 μL per harvest, (iii) in the ‘Scale Experiment,' the overall magnitude of rewards and delays was varied, such that in one condition, the size of rewards and length of delays was twice that of the other. (iv) Finally, in the ‘Pre-vs-Post Experiment,' the placement of delays was varied, such that the total time to harvest reward remained constant, but in one condition there was no pre-reward delay and ∼13 s post reward delay, and in the other there was a 3 s pre-reward delay and ∼10 s post-reward delay. Parameters for each experiment are shown in <xref ref-type="table" rid="table1">Table 1</xref>. For each condition within each experiment, rats were trained for 5 days and tested for an additional 5 days; all behavioral data presented is from the 5 test days. The order of conditions within each experiment was counterbalanced across rats. Every patch visit was included for analysis; mixed effects models were used to examine the effect of task condition on the number of trials spent in each patch. Random intercepts and random slopes for the effect of task condition were used to group observations within each rat. To compare rat behavior to the optimal behavior in each condition, a mixed effects model was used to test the effect of task condition on the difference between the number of trials spent in each patch and the optimal number of trials for that patch, with random intercepts and slopes for each rat. For this mixed effects model, an intercept of zero indicates optimal performance, and the slope indicates the change in behavior relative to the optimal behavior between conditions (see Materials and ethods for additional detail).</p><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.48429.006</object-id><label>Table 1.</label><caption><title>Parameters for each of the first four foraging experiments.</title><p>Harvest time = time to make a decision to harvest + pre-reward delay + post-reward delay (inter-trial interval). To control reward rate in the patch, the post-reward delay was adjusted relative to the decision time to hold the harvest time constant.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Experiment</th><th>Condition</th><th>Start Reward</th><th>Depletion Rate</th><th>Pre-Reward Delay</th><th>Harvest Time</th><th>Travel Time</th></tr></thead><tbody><tr><td rowspan="2">travel time</td><td>10 s</td><td rowspan="2">60, 90, or 120 μL<sup>†</sup></td><td rowspan="2">−8 μL</td><td rowspan="2">0 s</td><td rowspan="2">10 s</td><td>10 s</td></tr><tr><td>30 s</td><td>30 s</td></tr><tr><td rowspan="2">depletion rate</td><td>−8 μL</td><td rowspan="2">90 μL</td><td>−8 μL</td><td rowspan="2">0 s</td><td rowspan="2">12 s</td><td rowspan="2">12 s</td></tr><tr><td>−16 μL</td><td>−16 μL</td></tr><tr><td rowspan="2">scale</td><td>90 μL/10 s</td><td>90 μL</td><td>−8 μL</td><td rowspan="2">0 s</td><td>10 s</td><td>10 s</td></tr><tr><td>180 μL/20 s</td><td>180 μL</td><td>−16 μL</td><td>20 s</td><td>20 s</td></tr><tr><td rowspan="2">handling time</td><td>0 s</td><td rowspan="2">90 μL</td><td rowspan="2">−8 μL</td><td>0 s</td><td rowspan="2">15 s</td><td rowspan="2">15 s</td></tr><tr><td>3 s</td><td>3 s</td></tr><tr><td rowspan="2">post-reward delay*</td><td>3 s</td><td rowspan="2">90 μL</td><td rowspan="2">−8 μL</td><td rowspan="2">0 s</td><td>5–8 s**</td><td rowspan="2">10 s</td></tr><tr><td>12 s</td><td>13–16 s**</td></tr></tbody></table><table-wrap-foot><fn><p><sup>†</sup>Rats encountered all three patch types in both conditions.</p><p><sup>*</sup>One group of rats (n = 8) was tested on the first four experiments, but a separate group (n = 8) was tested on this final foraging experiment.</p></fn><fn><p><sup>**</sup>In this experiment, the harvest time was not held constant — the post-reward delay was always 3 s or 12 s regardless of the time to make a decision.</p></fn></table-wrap-foot></table-wrap><p>The Travel Time Experiment was designed to test the two main predictions of MVT: (i) that animals should stay longer in patches that yield greater rewards and (ii) animals should stay longer in all patches when the cost of traveling to a new patch is greater. In this experiment, rats encountered three different patch types within sessions, which started with varying amount of reward (60, 90, or 120 μL) and depleted at the same rate (8 μL/harvest). The delay between patches was either 10 s or 30 s; each travel time delay was tested in its own block of sessions and the order was counterbalanced across rats, with a range of 87–236 patches visited per condition per rat. As predicted by MVT, rats stayed for more trials in patch types that started with larger reward volume (β = 118.091 trials/mL, SE = 1.862, t(2490.265) = 63.423, p &lt; .001), indicating that rats considered reward across future patches. Rats also stayed longer in all patch types when time between patches was longer (β = 1.893 trials, SE = 0.313, t(118.839) = 6.040, p &lt; .001; <xref ref-type="fig" rid="fig1">Figure 1A</xref>), indicating sensitivity to opportunity costs. However, rats uniformly overharvested relative to predictions of MVT (β<sub>rat-MVT</sub> = 3.396 trials, SE = 0.176, t(6.960) = 19.269, p &lt; .001). The degree to which rats overharvested was not significantly different between the 10 s and 30 s travel conditions (β<sub>10 s-30 s</sub> = 0.304 trials, SE = 0.155, t(7.3857) = 1.964, p = 0.088).</p><p>The Depletion Rate Experiment tested another critical variable in foraging environments: the rate of reward depletion within a patch. Quicker reward depletion causes the local reward rate to deplete to the long-run average reward rate quicker, thus MVT predicts earlier patch leaving. Within sessions, rats encountered a single patch type (starting volume of 90 μL) that depleted at a rate of either 8 or 16 μL/trial, tested in separate sessions and counterbalanced, with a range of 152–283 patches visited per condition per rat. As predicted by MVT, rats left patches earlier when patches depleted more quickly (β = 2.589 trials, SE = 0.155, t(7.000) = 16.75, p &lt; .001; <xref ref-type="fig" rid="fig1">Figure 1B</xref>). But, again, rats stayed in patches longer than is predicted by MVT (β<sub>rat-MVT</sub> = 2.005 trials, SE = 0.134, t(7.004) = 14.97, p &lt; .001). Rats overharvested to a greater degree in the 8 μL depletion condition than the 16 μL depletion condition (β<sub>8μL-16μL</sub> = 1.589 trials, SE = 0.155, t(7.000) = 10.28, p &lt; .001).</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.48429.003</object-id><label>Figure 1.</label><caption><title>Rat foraging behavior.</title><p>Rat foraging behavior in the (<bold>A</bold>) Travel Time, (<bold>B</bold>) Depletion Rate, (<bold>C</bold>) Scale, and (<bold>D</bold>) Pre-vs-Post Experiments. In (<bold>A</bold>), points and error bars represent mean ± standard error. In (<bold>B-D</bold>), points and connecting lines represent behavior of each individual rat. Red lines indicate optimal behavior (per MVT).</p><p><supplementary-material id="fig1sdata1"><object-id pub-id-type="doi">10.7554/eLife.48429.005</object-id><label>Figure 1—source data 1.</label><caption><title>Trial-by-trial foraging behavior.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-48429-fig1-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48429.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Diagram of the foraging task.</title><p>Rats press a lever to harvest reward from the patch then receive reward in an adjacent port following a pre-reward delay (handling time or HT). After receiving reward, there is a post-reward delay (inter-trial interval or) before rats can make their next decision. Rats can leave the patch by nose poking in the back of the chamber (trial n+2), which initiates a delay simulating time to travel to the next patch, after which, rats can harvest from a new replenished patch.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig1-figsupp1-v2.tif"/></fig></fig-group><p>These first two experiments confirm that rats qualitatively follow the predictions of MVT, but consistently overharvest. There are many possible explanations for this pattern of overharvesting, including an aversion to leaving the offer of reward within a patch and nonlinear reward utility (<xref ref-type="bibr" rid="bib45">Wikenheiser et al., 2013</xref>; <xref ref-type="bibr" rid="bib9">Carter and Redish, 2016</xref>; <xref ref-type="bibr" rid="bib11">Constantino and Daw, 2015</xref>). The Scale Experiment was conducted in an effort to distinguish between these hypotheses by manipulating the scale of time delays and rewards. Long-term rate maximization predicts that an increase in reward size in proportion to reward delay should have no effect on the number of harvests per patch, as the reward rate across trials would be equal. But if animals’ perception of reward or time is nonlinear, a manipulation of scale will affect their subjective point of equality and predict a change in behavior across the two environments. The scale of rewards and delays was manipulated in the following manner: patches started with (A) 90 or (B) 180 μL of reward, depleted at a rate of (A) 8 or (B) 16 μL/trial, and the duration of harvest trials and travel time between patches was (A) 10 or (B) 20 s. Rats visited a range of 60–212 patches per condition. They overharvested in both A and B conditions (β<sub>rat-MVT</sub> = 4.374 trials, SE = 0.153, t(6.900) = 28.597, p &lt; .001) and, contrary to predictions of MVT, they stayed in patches significantly longer and overharvested to a greater degree in the B condition that provided larger rewards but at proportionately longer delays (β = 1.937 trials, SE = 0.193, t(6.972) = 9.996, p &lt; .001; <xref ref-type="fig" rid="fig1">Figure 1C</xref>). This finding suggests that a nonlinearity in the perception of reward value and/or time contributes to overharvesting.</p><p>To distinguish between biases in perception of reward, such as nonlinear reward utility, and time, such as temporal discounting or insensitivity to post-reward delays, the Pre-vs-Post Experiment directly tested rats sensitivity to time delays before vs. after reward. In this experiment, in one condition, rats received reward immediately after lever press followed by a post-reward delay of ∼13 s before the start of the next trial. In the other condition, there was a 3 s pre-reward delay between lever press and receiving reward followed by a shorter post-reward delay of ∼10 s. The total time of each trial was held constant between conditions (15 s total), so there was no difference in reward rates. Both MVT and nonlinear reward utility predict that the placement of delays is inconsequential and that rats will behave similarly in both conditions. Both temporal discounting and insensitivity to post-reward delays predict that rats will value the immediate reward more than the delayed reward and thus, would leave patches earlier in the condition with the pre-reward delay. Consistent with predictions of temporal discounting and insensitivity to post-reward delays, and contrary to predictions of MVT and nonlinear reward utility, rats left patches earlier in the environment with the pre-reward delay (β = 2.345 trials, SE = 0.313, t(7.017) = 7.503, p &lt; .001; <xref ref-type="fig" rid="fig1">Figure 1D</xref>). This result suggests that a bias in rats’ perception of time or the way in which they perceive delayed reward values contributes to overharvesting.</p><p>To determine whether the preference for immediate rewards can be explained by insensitivity to post-reward delays, a fifth foraging experiment, the ‘Post-Reward Delay Experiment,' directly tested rats’ sensitivity to post-reward delays. A separate cohort of rats (n = 8) was used for this experiment. Rats were tested in two conditions in this experiment: a short (3 s) or long (12 s) post-reward delay. The total time of harvest trials was not held constant; the longer post-reward delay increased the time to harvest from the patch. Since the longer post-reward delay increases the cost of harvesting from the patch relative to the cost of traveling to a new patch, MVT predicts that rats should leave patches earlier. Prior studies of intertemporal choice behavior have shown that animals are insensitive to post-reward delays, suggesting that they are only concerned with maximizing short-term reward rate (<xref ref-type="bibr" rid="bib38">Stephens, 2001</xref>; <xref ref-type="bibr" rid="bib3">Bateson and Kacelnik, 1996</xref>) or that they may not have learned the duration of post-reward delays, and underestimate this duration in their decision process (<xref ref-type="bibr" rid="bib35">Pearson et al., 2010</xref>; <xref ref-type="bibr" rid="bib4">Blanchard et al., 2013</xref>). Consistent with MVT, rats were sensitive to the post-reward delay, leaving patches earlier in the 12 s delay condition (β = 1.411 trials, SE = 0.254, t(6.966) = 5.546, p &lt; .001; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). If rats were sensitive to the delay, but underestimated its duration, one would still expect rats to overharvest to a greater degree due to the longer delay. There was no difference in the degree to which rats overharvested between the 3 s and 12 s delay conditions (β<sub>rat-MVT;3 s-12 s</sub> = 0.340 trials, SE = 0.286, t(6.963) = 1.188, p = 0.274). This finding suggests that overharvesting in this experiment is not due to insensitivity to post-reward delays. However, it is possible that this finding could be explained by other forms of altered time perception that remain to be described.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.48429.007</object-id><label>Figure 2.</label><caption><title>Post-reward delay foraging and intertemporal choice behavior.</title><p>(<bold>A</bold>) Rat behavior in the Post-Reward Delay Experiment. Points and lines represent behavior of individual rats. Red line indicates optimal behavior (per MVT). (<bold>B</bold>) Rat behavior in the intertemporal choice task. Points and error bars represent mean ± standard error for each condition.</p><p><supplementary-material id="fig2sdata1"><object-id pub-id-type="doi">10.7554/eLife.48429.008</object-id><label>Figure 2—source data 1.</label><caption><title>Trial-by-trial intertemporal choice behavior.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-48429-fig2-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig2-v2.tif"/></fig><p>The data from the foraging experiments described above suggest that rats exhibit time preferences in the foraging task. In a final ‘Intertemporal Choice Experiment,' we tested whether the same rats that participated in the Post-Reward Delay Experiment would exhibit similar time preferences in a standard intertemporal choice (i.e. a delay-discounting) task. This task consisted of a series of 20-trial episodes. On each trial, rats pressed either the left or right lever to receive a smaller-sooner (SS) reward of 40 μL after a 1 s delay or a larger-later (LL) reward of 40, 80, or 120 μL after a 1, 2, 4, or 6 s delay. For the first 10 trials of each episode, rats were forced to press either the left or right lever to learn the value and delay associated with that lever (only one lever extended on each of these trials). For the last 10 trials of an episode, both levers extended and rats were free to choose. The LL reward value and delay, and the LL lever (left or right) were randomly selected at the start of each episode. Rats were tested in two different versions of this task: one in which the post-reward delay was held constant, such that the longer pre-reward delays reduced reward rate (constant delay); and another in which the time of the trial was held constant, such that longer pre-reward delays resulted in shorter post-reward delays to keep reward rate constant (constant rate). MVT, which maximizes long-term reward rate, predicts that rats would be sensitive to the pre-reward delay in the constant delay condition but not the constant trial condition (in which the pre-reward delay does not affect reward rate).</p><p>Rats were given three training sessions to learn the structure of the intertemporal choice task after previously being tested in the foraging task, then they were tested for an additional 13 sessions in each condition, participating in a range of 590–2810 free choice trials per condition (constant delay vs. constant rate). Each free choice trial within each episode was counted as a separate observation. Choice data were analyzed using a generalized linear mixed-effects model (i.e. a mixed-effects logistic regression) to examine the effect of the size of the LL reward, the length of the LL delay, task condition (constant delay vs. constant rate), and their interactions on decisions to choose the LL vs. SS option, with random intercepts and random slopes for the effects of LL reward, LL delay, and task condition for each rat. Three post-hoc comparisons were used to test the effects of (i) LL reward and (ii) LL delay within each condition, and (iii) LL delay between the constant delay and constant rate conditions (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). (i) In both conditions, rats were more likely to choose larger LL rewards (constant delay: β = 0.477, SE = 0.090, <inline-formula><mml:math id="inf1"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>(1)=28.320, p &lt; .001; constant rate: β = 0.450, SE = 0.089, <inline-formula><mml:math id="inf2"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>(1) = 25.378, p &lt; .001), showing that they were sensitive to reward magnitude. (ii) They were also sensitive to the pre-reward delay in both conditions (constant delay: β = −0.240, SE = 0.023, <inline-formula><mml:math id="inf3"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>(1) = 104.882, p &lt; .001; constant rate: β = −0.152, SE = 0.022, <inline-formula><mml:math id="inf4"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>(1) = 46.919, p &lt; .001). On average, rats were equally likely to select the LL option across conditions — the main effect of task condition was not significant (β=0.010, SE = 0.105, z = 0.092, p = 0.927). (iii) However, rats were less sensitive to increasing pre-reward delays when pre-reward delays did not affect reward rate (in the constant rate condition), indicated by a change in LL delay slope between conditions (β = 0.088, SE = 0.026, <inline-formula><mml:math id="inf5"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>(1) = 11.376, p &lt; .001). Overall, rats exhibited similar time preferences in the foraging and intertemporal choice tasks: they valued rewards less with longer delays until receipt but they were sensitive to opportunity costs (e.g. time delays between receiving reward and future decisions).</p></sec><sec id="s2-2"><title>Quasi-hyperbolic discounting best explains behavior across all tasks</title><p>To test whether a common set of cognitive biases could explain time preferences in both the foraging and intertemporal choice tasks, both tasks were modeled as continuous time semi-markov processes. These models consisted of a set of states that represented the time between each event in each of the tasks (e.g. cues turning on/off, lever press, reward delivery; for state space diagrams of both tasks, see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). These models assumed that animals have learned the appropriate structure of the task (i.e. the time spent and reward obtained in each state) unless otherwise noted. The value of a given state was the discounted value of all future rewards available from that state, and the agent chose the option that yielded the greatest discounted future reward via a stochastic process. As the discount factor approached 1 (i.e. no temporal discounting), this model converged to long-term reward maximization, equivalent to MVT. Additional parameters were added to the model to test four specific hypotheses for suboptimal foraging behavior: (i) subjective costs associated with leaving a patch, in which the value of leaving was reduced by a ‘cost' term; (ii) nonlinear reward utility, in which the subjective utility of a reward increased sublinearly with respect to the reward magnitude; (iii) biased time perception, which assumed that animals underestimate post-reward delays, possibly due to insufficient learning of task structure (<xref ref-type="bibr" rid="bib4">Blanchard et al., 2013</xref>; <xref ref-type="bibr" rid="bib35">Pearson et al., 2010</xref>), or overestimate pre-reward delays; and (iv) temporal discounting. A brief description of each hypothesis and its general predictions can be found in <xref ref-type="table" rid="table2">Table 2</xref>. For each model, group level parameters and parameters for each individual rat were fit simultaneously using an expectation-maximization algorithm (<xref ref-type="bibr" rid="bib19">Huys et al., 2011</xref>). Parameters were fit to each experiment separately (one set of parameters for both conditions in each experiment). Model predictions were calculated separately for each rat, using the rat’s individual parameters. Full details for all models, fitting procedures, and model comparison can be found in the Materials and methods.</p><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.48429.009</object-id><label>Table 2.</label><caption><title>Description of the hypotheses for overharvesting with general, qualitative predictions for the degree of overharvesting in each experiment.</title><p>Quantitative predictions depend on the exact formalization of each model and its specific parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Hypothesis</th><th>Experimental predictions</th></tr></thead><tbody><tr><td>Subjective Costs</td><td>A cost term <inline-formula><mml:math id="inf6"><mml:mi>c</mml:mi></mml:math></inline-formula> reduces the value of leaving a patch. Predicts greater overharvesting with higher <inline-formula><mml:math id="inf7"><mml:mi>c</mml:mi></mml:math></inline-formula>. Not affected by specific manipulations to reward or time.</td><td>Rats will follow qualitative predictions of the Marginal Value Theorem, but exhibit an equal degree of overharvesting across conditions in each experiment.</td></tr><tr><td>Nonlinear Reward Utility</td><td>Subjective value increases sublinear to reward magnitude. Predicts greater overharvesting with steeper utility functions with larger rewards.</td><td>Rats will exhibit an equal degree of ovarharvesting in all experiments except for the Scale experiment. In the Scale experiment, rats will overharvest more in the conditions with larger rewards.</td></tr><tr><td>Biased Time Perception</td><td>i) Post-reward delays perceived as shorter, ii) pre-reward delays perceived as longer, or iii) longer delays (irrespective of their placement) perceived as shorter. All three hypotheses predict greater overharvesting with longer delays.</td><td>Rats will exhibit a greater degree of overharvesting in the condition with longer delays in the Scale environment, in the condition with the longer post-reward delay in the Pre-vs-Post experiment, and in the condition with longer post-reward delay in the Post-Reward Delay experiment</td></tr><tr><td>Temporal Discounting</td><td>Value of future rewards discounted due to delay to receive them. Predicts greater overharvesting with greater levels of discounting and with longer delays</td><td>Rats will overharvest to a greater degree in the conditions with longer delays in the Scale and Post-Reward Delay experiments and they will leave patches earlier due to the longer pre-reward delay in the Pre-vs-Post experiment.</td></tr></tbody></table></table-wrap><p>Subjective costs to leave a patch and nonlinear reward utility have explained suboptimal foraging behavior in prior studies that have manipulated opportunity costs (e.g. travel time or pre-reward delays) and depletion rate (<xref ref-type="bibr" rid="bib11">Constantino and Daw, 2015</xref>; <xref ref-type="bibr" rid="bib45">Wikenheiser et al., 2013</xref>; <xref ref-type="bibr" rid="bib9">Carter and Redish, 2016</xref>). However, these factors are insensitive to the placement of time delays (pre- vs. post-reward) and thus, cannot explain the preference for more immediate rewards. Consistent with these prior studies, the subjective costs and nonlinear reward utility models explained overharvesting in the Travel Time, Depletion Rate, and Post-Reward Delay Experiments, but they failed to explain time preferences in the Pre-vs-Post foraging experiment (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p><p>We next examined whether biased time perception and temporal discounting could explain suboptimal foraging behavior across all tasks. Three implementations for biased time perception were tested: linear underestimation of post-reward delays (<inline-formula><mml:math id="inf8"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>*</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), non-linear underestimation of post-reward delays (<inline-formula><mml:math id="inf9"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>), and overestimation of pre-reward delays (<inline-formula><mml:math id="inf10"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>*</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). For temporal discounting, we tested the two common single-parameter discounting functions, exponential (<inline-formula><mml:math id="inf11"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>*</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>*</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) and standard hyperbolic (<inline-formula><mml:math id="inf12"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>*</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>), and two more flexible discounting models: constant sensitivity discounting (<xref ref-type="bibr" rid="bib14">Ebert and Prelec, 2007</xref>; <xref ref-type="bibr" rid="bib46">Zauberman et al., 2009</xref>), which predicts hyperbolic time preferences due to insensitivity to longer delays (<inline-formula><mml:math id="inf13"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>*</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:msup><mml:mo>*</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>); and quasi-hyperbolic discounting, formalized as two competing exponential discounting systems (<inline-formula><mml:math id="inf14"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>ω</mml:mi><mml:mo>*</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>*</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mo>*</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; (<xref ref-type="bibr" rid="bib28">Laibson, 1997</xref>; <xref ref-type="bibr" rid="bib32">McClure et al., 2007</xref>). All of these models qualitatively predicted rat behavior across foraging experiments (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>, <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>).</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.48429.010</object-id><label>Figure 3.</label><caption><title>Model predictions for foraging tasks.</title><p>(<bold>A-E</bold>) Predictions of the best fit quasi-hyperbolic discounting model to all foraging tasks. Points and error bars represent mean ± standard deviation of the means for each individual rat; lines and ribbon represent the mean ± standard deviation of the means of the model-predicted behavior for each individual rat. (<bold>F</bold>) The sum of iBIC scores across all foraging tasks for each model. Cost = subjective cost model, util-pwr and util-crra = nonlinear reward utility with power and CRRA function respectively, pre-del = linear overestimation of pre-reward delays, post-del = linear underestimation of post-reward delays, post-del-pwr=underestimation of post-reward delays according to a power function, disc-exp = exponential discounting, disc-hyp = hyperbolic discounting, disc-cs = constant sensitivity discounting, disc-quasi = quasi hyperbolic discounting.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48429.011</object-id><label>Figure 3—figure supplement 1.</label><caption><title>State space diagram of the foraging task.</title><p>State space diagram for the semi-markov model of the foraging task. Decisions to stay vs. leave are made in Decision states. A decision to stay causes a transition to the handling time, then reward, ITI, and to the Decision state on the next trial. Reward is delivered uniformly throughout time spent in the each reward state. Reward depletion is achieved via shorter time spent in the reward state (resulting in longer stay in the ITI state). A decision to leave causes a transition to the Travel state, then to the first trial of the patch.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48429.012</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Predictions of the best fit subjective cost and nonlinear reward utility models.</title><p>Predictions of the best fit subjective cost and nonlinear reward utility models (power law = util pwr; constant relative risk aversion = util CRRA). Black points and error bars represent mean ± standard error of observed behavior. Colored lines represent the mean model predicted behavior across rats.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48429.013</object-id><label>Figure 3—figure supplement 3.</label><caption><title>Predictions of the best fit biased time perception models.</title><p>Predictions of the best fit models of overestimation of pre-reward delays (pre-delay), linear underestimation of post-reward delays (post-delay), and nonlinear underestimation of post-reward delays (post-delay-nonlinear). Points and errorbars are the mean ± standard deviation of rat behavior, colored lines represent the mean model predicted number of harvests across all rats.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig3-figsupp3-v2.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48429.014</object-id><label>Figure 3—figure supplement 4.</label><caption><title>Predictions of the best fit discounting models.</title><p>Predictions of the best fit exponential discounting model (disc-exp), hyperbolic discounting model (disc-hyp), and constant sensitivity discounting model (disc-cs). Points and error bars are the mean ± standard deviation of rat behavior; colored lines represent the mean predicted number of harvests across all rats.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig3-figsupp4-v2.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48429.015</object-id><label>Figure 3—figure supplement 5.</label><caption><title>iBIC for each model for each foraging experiment.</title><p>Cost = subjective cost model, util-pwr and util-crra = nonlinear reward utility with power and CRRA function respectively, pre-del = linear overestimation of pre-reward delays, post-del = linear underestimation of post-reward delays, post-del-pwr=underestimation of post-reward delays according to a power function, disc-exp = exponential discounting, disc-hyp = hyperbolic discounting, disc-cs = constant sensitivity discounting, disc-quasi = quasi hyperbolic discounting.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig3-figsupp5-v2.tif"/></fig></fig-group><p>To determine which model provided the best quantitative fit, we compared the group-level Bayesian Information Criterion (integrated BIC or iBIC; <xref ref-type="bibr" rid="bib19">Huys et al., 2011</xref>; <xref ref-type="bibr" rid="bib20">Huys et al., 2012</xref>) of all models in each of the foraging tasks. To compare across tasks, we took the sum of iBIC for each model. The quasi-hyperbolic discounting model had the lowest sum of iBIC, the constant sensitivity discounting model the second lowest, and the hyperbolic discounting model third (<xref ref-type="fig" rid="fig3">Figure 3</xref>). These three models were also among the lowest iBIC values for each individual experiment <xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5</xref>). All three of these models predict that animals will exhibit hyperbolic time preferences, suggesting that suboptimal foraging behavior observed in these experiments is due to time preferences.</p><p>Next, we tested whether the quasi-hyperbolic discounting model that provided the best fit to foraging behavior could also explain behavior in the intertemporal choice task. As in the model of the foraging task, the model of the intertemporal choice task took into account all future rewards, including rewards from future episodes (see abbreviated state space diagram in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). We tested the nonlinear reward utility, biased time perception and temporal discounting models in this task (the subjective cost model does not apply to this task). Again, the quasi-hyperbolic discounting model had the lowest iBIC and hyperbolic discounting model the second lowest, but the constant sensitivity model had a higher iBIC than the biased time perception models (<xref ref-type="fig" rid="fig4">Figure 4</xref>). As the constant sensitivity model produces hyperbolic time preferences via insensitivity to longer delays, these results suggest that hyperbolic time preferences without insensitivity to delays is the best explanation for rat intertemporal choice behavior.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.48429.016</object-id><label>Figure 4.</label><caption><title>Model predictions for the intertemporal choice task.</title><p>(<bold>A</bold>) Quasi-hyperbolic model predictions for the intertemporal choice task. Points and error bars represent the mean ± standard error of individual rat behavior; lines and ribbon represent mean ± standard error of model predicted behavior for each individual rat. (<bold>B</bold>) The iBIC score for each model for the delay discounting experiment. Util-pwr and util-crra = nonlinear reward utility with power and CRRA function respectively, pre-del = linear overestimation of pre-reward delays, post-del = linear underestimation of post-reward delays, post-del-pwr=underestimation of post-reward delays according to a power function, disc-exp = exponential discounting, disc-hyp = hyperbolic discounting, disc-cs = constant sensitivity discounting, disc-quasi = quasi hyperbolic discounting.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48429.017</object-id><label>Figure 4—figure supplement 1.</label><caption><title>State space diagram of the intertemporal choice task.</title><p>Decisions made in Decision states cause transition to the Delay, Reward, and ITI states for the option chosen (either SS or LL), then back to the next Decision state. The model consisted of 10 consecutive trials — the number of free choice trials — plus the value of rewards in future games.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48429.018</object-id><label>Figure 4—figure supplement 2.</label><caption><title>Comparison of all-future horizon and one-trial horizon discounting models.</title><p>(<bold>A</bold>) iBIC for the full horizon and one-trial horizon discounting models (<bold>B</bold>) Measured log-transformed discount factors for the full horizon and one-trial horizon discounting models. Bars and errorbars represent mean ± standard error.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig4-figsupp2-v2.tif"/></fig></fig-group><p>If hyperbolic time preferences reflect a common explanation for suboptimal decision-making, then it might be expected that a model of behavior fit to one task could predict a rat’s behavior in the other task. To test the external validity of this hypothesis, data from each task were separated into three subsets. The best fitting model from both tasks, the quasi-hyperbolic discounting model, was fit to two subsets of data from one task, then the negative log likelihood (-LL) of the data was assessed on the left out sample from both tasks. This process was repeated such that each subset served as the left out sample. To determine which discount function provided the better fit to data from each task, we calculated the difference in -LL of the left out sample between the model fit to intertemporal choice data and the model fit to foraging data (-LL difference = -LL<sub>itc</sub> - LL<sub>forage</sub>). Since smaller -LL indicates a better fit, a positive -LL difference indicates that the discount function fit to foraging data provided a better fit (i.e. the foraging -LL was lower than the intertemporal choice -LL). For the foraging task, discounting functions fit to foraging data provided a better fit than discounting functions fit to intertemporal choice data for all eight rats. Interestingly, for the intertemporal choice task, discounting functions fit to foraging data provided a better fit than discounting functions fit to intertemporal choice data for 3 of 8 rats (<xref ref-type="fig" rid="fig5">Figure 5</xref>). The quasi-hyperbolic model fit to the foraging task generalized well to the intertemporal choice task, providing support for the idea that foraging and intertemporal choice can be described by a common discount function.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.48429.019</object-id><label>Figure 5.</label><caption><title>Cross-task model predictions.</title><p>(<bold>A</bold>) Predicted foraging behavior for quasi-hyperbolic model parameters fit to either the foraging task (red line) or delay discounting task (DD; blue line). Black points and error bars represent mean ± standard error of rat data. (<bold>B</bold>) Predicted intertemporal choice behavior for quasi-hyperbolic model parameters fit to data from either the foraging or delay discounting task, plotted against rat behavior. (<bold>C</bold>) The difference in negative log likelihood of the left out sample of foraging data (left) or intertemporal choice data (right) between parameters fit to the intertemporal choice task and parameters fit to the foraging task. A negative -LL difference indicates the negative log likelihood of the data for parameters fit to the intertemporal choice task was lower than for parameters fit to the foraging task. Each point and line represents data from individual rats.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig5-v2.tif"/></fig><p>With temporal discounting models that consider all future rewards, the more flexible quasi-hyperbolic discounting function provided the best fit to behavior across tasks. We next directly tested whether considering future rewards affects the fit of discounting models to intertemporal choice data and the estimates of discount factors compared to temporal discounting models that only consider the next reward (one-trial horizon models). We fit one-trial horizon models for all of the previously tested discounting functions — exponential, hyperbolic, constant sensitivity, and quasi-hyperbolic discounting — and compared them to the discounting models that considered all future rewards (all-future horizon models). For all discounting functions, the all-future horizon models had lower iBIC than one-trial horizon models (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). To compare the discount factors of each model (for the quasi-hyperbolic function that has two discount factors, we used the slow discounting β), we performed paired t-tests between log transformed discount factors measured by the all-future horizon models vs. discount factors measured by the one-trial horizon models. Measured discount factors were lower for the all-future models for all discounting functions (exponential: t(7) = 22.439, p &lt; .001; hyperbolic: t(7) = 7.000, p &lt; .001; constant sensitivity: t(7) = 15.497, p &lt; .001; quasi-hyperbolic: t(7) = 25.322, p &lt; .001; p-values adjusted using Bonferroni correction). Lastly, we tested whether the all-future horizon quasi-hyperbolic discounting model fit to the intertemporal choice data would predict foraging behavior better than the one-trial horizon quasi-hyperbolic discounting model fit to intertemporal choice data. For 6 of 8 rats, parameters fit to the one-trial horizon model produced a better fit to foraging behavior than parameters fit to the all-future horizon model. Overall, using full horizon temporal discounting models explained more of the intertemporal choice data, produced smaller estimates of discounting factors, but in the present study, it did not improve the ability of a model fit to intertemporal choice data to predict foraging behavior.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In foraging studies, animals exhibit behavior that conforms qualitatively to predictions made by optimal foraging theory (i.e., the MVT), choosing to leave a patch when its value falls below that of the average expected value of other(s) available in the environment. However, an almost ubiquitous finding is that they overharvest, leaving a patch when its value falls to a value lower than the one predicted by MVT. Given that the rewards available within the current patch are generally available sooner than those at other patches due to travel time, one interpretation of overharvesting is that this reflects a similarly prevalent bias observed in intertemporal choice tasks, in which animals consistently show a greater preference for smaller more immediate rewards over later delayed rewards than would be predicted by optimal (i.e,. exponential) discounting of future values. However, in prior studies, models of intertemporal choice behavior have been poor predictors of foraging behavior (<xref ref-type="bibr" rid="bib5">Blanchard and Hayden, 2015</xref>; <xref ref-type="bibr" rid="bib9">Carter and Redish, 2016</xref>). Here, we show that in a carefully designed series of experiments, rats exhibit similar time preferences in foraging and intertemporal choice tasks, and that a quasi-hyperbolic discounting model can explain the rich pattern of behaviors observed in both tasks.</p><p>The foraging behavior we observed was consistent with previous studies of foraging behavior in rats, monkeys, and humans, while also revealing novel aspects of overharvesting behavior. Consistent with prior studies, rats stayed longer in patches that yielded greater rewards, stayed longer in all patch types when the cost of traveling to a new patch was greater, left patches earlier when rewards depleted more quickly, and consistently overharvested (<xref ref-type="bibr" rid="bib11">Constantino and Daw, 2015</xref>; <xref ref-type="bibr" rid="bib17">Hayden et al., 2011</xref>; <xref ref-type="bibr" rid="bib22">Kane et al., 2017</xref>). Our experiments also demonstrated that in certain environments rats violate qualitative predictions of MVT. Rats overharvested more when reward amount and delay were increased, even though reward rate was held constant, and they were differentially sensitive to whether the delay was before the receipt of the proximal reward or following its delivery. These findings supported the conjecture that overharvesting is related to time preferences.</p><p>A number of studies have found that the preference for smaller, more immediate rewards can be explained by insensitivity to post-reward delays (<xref ref-type="bibr" rid="bib3">Bateson and Kacelnik, 1996</xref>; <xref ref-type="bibr" rid="bib4">Blanchard et al., 2013</xref>; <xref ref-type="bibr" rid="bib35">Pearson et al., 2010</xref>; <xref ref-type="bibr" rid="bib38">Stephens, 2001</xref>; <xref ref-type="bibr" rid="bib30">Mazur, 1991</xref>). One hypothesis for why animals fail to incorporate post-reward delays into decisions is that they haven’t learned the structure of the task well, and thus cannot accurately predict future post-reward delays. Accordingly, providing explicit cues for the post-reward delays or increasing the salience of post-reward delays helps animals incorporate these delays into their decisions, reducing the bias towards selecting smaller, more immediate rewards over larger, delayed ones (<xref ref-type="bibr" rid="bib35">Pearson et al., 2010</xref>; <xref ref-type="bibr" rid="bib4">Blanchard et al., 2013</xref>). However, in the present study, rats were sensitive to post-reward delays in both the foraging and intertemporal choice task, providing further evidence that the preference for smaller, more immediate rewards in both tasks is due to time preferences and not a poor understanding of the task structure. Furthermore, quantitative modeling supported the hypothesis that suboptimal behavior was driven by time preferences rather than insensitivity to delays.</p><p>The idea that animals exhibit similar decision biases in foraging and intertemporal choice paradigms, and that these biases can be explained by a common model of discounting, is in conflict with prior studies that found that animals are better at maximizing long-term reward rate in foraging than in intertemporal choice tasks, and that delay discounting models of intertemporal choice tasks are poor predictors of foraging behavior (<xref ref-type="bibr" rid="bib41">Stephens, 2008</xref>; <xref ref-type="bibr" rid="bib5">Blanchard and Hayden, 2015</xref>; <xref ref-type="bibr" rid="bib8">Carter et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Carter and Redish, 2016</xref>). It has been argued that animals may perform better in foraging tasks because decision-making systems have evolved to solve foraging problems rather than two-alternative intertemporal choice problems (<xref ref-type="bibr" rid="bib5">Blanchard and Hayden, 2015</xref>; <xref ref-type="bibr" rid="bib40">Stephens et al., 2004</xref>; <xref ref-type="bibr" rid="bib41">Stephens, 2008</xref>). This idea has been challenged by a recent study of human decision-making in foraging and intertemporal choice tasks, finding that a long-term rate maximization model explained both foraging and intertemporal choice behavior better than a standard hyperbolic discounting model (<xref ref-type="bibr" rid="bib36">Seinstra et al., 2018</xref>). Results from the present study support the interpretation that foraging and intertemporal choice behavior can be explained via a common model, but suggest that this model is quasi-hyperbolic discounting. We found that a quasi-hyperbolic discounting model provided the best explanation to rat behavior across multiple foraging tasks and an intertemporal choice task, and that a quasi-hyperbolic discounting model fit to individual rat foraging behavior can predict their intertemporal choice behavior.</p><p>Two potential explanations for why temporal discounting models have failed to predict foraging behavior in prior studies are that (i) prior studies have only tested single-parameter exponential and hyperbolic discounting functions, whereas the present study also tested the more flexible quasi-hyperbolic discounting function; and (ii) in most of these studies, models of intertemporal choice tasks have only considered the most proximal reward (the reward received as a consequence of the decision at hand). This assumption seems appropriate as, in most intertemporal choice tasks, opportunities for future rewards do not depend on the current decision, so the value of rewards received for future decisions are equal for both the SS and LL rewards. But in foraging tasks, future opportunities for reward depend on current decisions, so it is critical for foraging models to include all future rewards into estimates of reward value. For this reason, comparing discount functions fit to intertemporal choice models that consider all future reward may provide better estimates of foraging behavior than discount functions fit to intertemporal choice models that only consider rewards from the most proximal decision. Consistent with this hypothesis, we found that adding the value of future rewards to intertemporal choice models reduces estimates of discount factors. However, with our data, the quasi-hyperbolic discounting model fit to the intertemporal choice task that included all future rewards did not predict foraging behavior better than an equivalent model that only considered the most proximal reward. One reason why including all future rewards may not have improved cross-task predictions is that, in the present study, the quasi-hyperbolic discounting model fit to the intertemporal choice task predicted less overharvesting than was exhibited by rats in the foraging task. Reducing estimates of the discount factor with a model that considers all future rewards predicts even less overharvesting (i.e. behavior that is closer to long-term reward maximization). But in other studies, temporal discounting models typically predict greater overharvesting than is exhibited by animals (<xref ref-type="bibr" rid="bib5">Blanchard and Hayden, 2015</xref>; <xref ref-type="bibr" rid="bib8">Carter et al., 2015</xref>). In these cases, obtaining smaller, potentially more accurate estimates of discount factors by including all future rewards into intertemporal choice models may improve cross-task predictions.</p><p>Although quasi-hyperbolic discounting provided the best singular explanation for rat behavior across our tasks, many of the other models tested were capable of explaining some of the biases exhibited by rats. Thus, we cannot exclude the possibility that subjective costs, diminishing marginal utility, and/or biased estimation of time intervals may independently contribute to suboptimal decision-making. Furthermore, additional hypotheses or additional variants of the above-mentioned hypotheses that have not been tested in the present study may provide alternative explanations for suboptimal decision making in foraging and intertemporal choice tasks. Importantly, our data indicate that quasi-hyperbolic discounting may provide a link between foraging and intertemporal choice tasks, and it highlights the importance of future work considering the source of time preferences. These observations are buttressed by recent theoretical work demonstrating that the appearance of time preferences in intertemporal choice tasks can emerge rationally from a value construction process by which estimates increase in variability with the delay until reward receipt — an account that shares features with the short-term rate maximization hypotheses (<xref ref-type="bibr" rid="bib40">Stephens et al., 2004</xref>). Under this account, ‘as-if' discounting is hyperbolic when variability increases linearly with delay (<xref ref-type="bibr" rid="bib15">Gabaix and Laibson, 2017</xref>). Further, a sequential sampling model of two-alternative forced choice (<xref ref-type="bibr" rid="bib6">Bogacz et al., 2006</xref>), parameterized such that outcome delay scales variability in this way, has recently been shown to capture key dynamical features of both patch foraging (<xref ref-type="bibr" rid="bib12">Davidson and El Hady, 2019</xref>) and hyperbolic discounting in intertemporal choice (<xref ref-type="bibr" rid="bib18">Hunter et al., 2018</xref>). Future work should build on these findings to explore directly whether the common biases identified here reflect a core computation underlying sampling and decision-making under uncertainty and across time.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>Adult Long-Evans rats were used (Charles River, Kingston, NY). One group of eight rats participated in the scale, travel time, depletion rate, and handling time experiments (in that order), a different set of eight rats were tested on the post-reward delay foraging experiment then the delay discounting task. Rats were housed on a reverse 12 hr/12 hr light/dark cycle. All behavioral testing was conducted during the dark period. Rats were food restricted to maintain a weight of 85–90% ad-lib feeding weight, and were given ad-lib access to water. All procedures were approved by the Princeton University and Rutgers University Institutional Animal Care and Use Committee.</p></sec><sec id="s4-2"><title>Foraging task</title><p>Animals were trained and tested as in <xref ref-type="bibr" rid="bib22">Kane et al. (2017)</xref>. Rats were first trained to lever press for 10% sucrose water on an FR1 reinforcement schedule. Once exhibiting 100+ lever presses in a one hour session, rats were trained on a sudden patch depletion paradigm — the lever stopped yielding reward after 4–12 lever presses — and rats learned to nose poke to reset the lever. Next rats were tested on the full foraging task.</p><p>A diagram of the foraging task is in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. On a series of trials, rats had to repeatedly decide to lever press to harvest reward from the patch or to nose poke to travel to a new, full patch, incurring the cost of a time delay. At the start of each trial, a cue light above the lever and inside the nose poke turned on, indicating rats could now make a decision. The time from cues turning on until rats pressed a lever or nose poked was recorded as the decision time (DT). A decision to harvest from the patch (lever press) yielded reward after a short pre-reward delay (referred to as the handling time delay, simulating the time to ‘handle' prey after deciding to harvest). Reward (sucrose water) was delivered when the rat entered the reward magazine. The next trial began after an inter-trial interval (ITI). To control the reward rate within the patch, the length of the ITI was adjusted based on the DT of the current trial, such that the length of all harvest trials was equivalent. With each consecutive harvest, the rat received a smaller volume of reward to simulate depletion from the patch. A nose poke to leave the patch caused the lever to retract for a delay period simulating the time to travel to a new patch. After the delay, the opposite lever extended, and rats could harvest from a new, replenished patch.</p><p>Details of the foraging environment for each experiment can be found in <xref ref-type="table" rid="table1">Table 1</xref>. For each experiment, rats were trained on a specific condition for 5 days, then tested for 5 days. Conditions within experiments were counterbalanced.</p></sec><sec id="s4-3"><title>Foraging data analysis</title><p>Rat foraging behavior was assessed using linear mixed effects models. Models were fit using the lme4 package in R (<xref ref-type="bibr" rid="bib2">Bates et al., 2015</xref>). The lme4 package provides only t-statistics for fixed effects; p-values were calculated using the lmerTest package (<xref ref-type="bibr" rid="bib27">Kuznetsova et al., 2017</xref>), which uses Scatterwaithe’s method to approximate the degrees of freedom for the t-test. In the Travel Time Experiment, we assessed the effect of starting volume of the patch and the travel time on number of harvests per patch, with random intercepts and random slopes for both variables across subjects (lme4 formula: <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>P</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>∼</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>V</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>*</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>V</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mpadded width="+1.7pt"><mml:mi>e</mml:mi></mml:mpadded><mml:mo stretchy="false">|</mml:mo><mml:mo rspace="4.2pt" stretchy="false">|</mml:mo><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). In all other foraging experiments, we assessed the effect of experimental condition on harvests per patch, with random intercepts and random effect of experimental condition across subjects (lme4 formula: <inline-formula><mml:math id="inf16"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>n</mml:mi></mml:mpadded></mml:mrow><mml:mo lspace="2.5pt" rspace="4.2pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>We also tested whether rats overharvested relative to MVT predictions in each experiment, and whether the degree of overharvesting was different between conditions within each experiment. To do so, we subtracted the MVT predicted number of harvests in each patch from the observed number of harvests (see ‘Foraging Models' section for details on the calculation of the optimal number of harvests). Mixed effects models were used to fit an intercept and effect of experimental condition on the difference from optimal number of harvests (lme4 formula: <inline-formula><mml:math id="inf17"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="-1.7pt"><mml:mi>i</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="-1.7pt"><mml:mi>f</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="-1.7pt"><mml:mi>f</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>n</mml:mi></mml:mpadded></mml:mrow><mml:mo lspace="2.5pt" rspace="4.2pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>). In this model, an intercept greater than zeros would indicate that rats harvested more trials than was predicted by MVT, and a difference in the effect of task condition would indicate that the degree to which rats differed from optimal was affected by the task condition.</p></sec><sec id="s4-4"><title>Intertemporal choice task</title><p>Rats were immediately transferred from the foraging task to the intertemporal choice task with no special training; rats were given three 2 hr sessions to learn the structure of the new task. This task consisted of a series of episodes that lasted 20 trials. At the beginning of each episode one lever was randomly selected as the shorter-sooner lever, yielding 40 μL of reward following a 1 s delay. The other lever (larger-later lever) was initialized to yield a reward of 40, 80, or 120 μL after a 1, 2, 4 or 6 s delay. For the first 10 trials of each episode, only one lever extended, and rats were forced to press that lever to learn its associated reward value and delay. The last four forced trials (trials 7–10) were counterbalanced to reduce the possibility of rats developing a perseveration bias. For the remaining 10 trials of each episode, both levers extended, and rats were free to choose the option they prefer. At the beginning of each trial, cue lights turned on above the lever indicating rats could now make a decision. Once the rat pressed the lever, the cue light turned off, and the delay period was initiated. A cue light turned on in the reward magazine at the end of the delay period, and rats received reward as soon as they entered the reward magazine. Reward magnitude was cued by light and tone. Following reward delivery, there was an ITI before the start of the next trial. At the completion of the episode, the levers retracted, and rats had to nose poke to begin the next episode, which reset the larger-later reward and delay.</p></sec><sec id="s4-5"><title>Intertemporal choice data analysis</title><p>Intertemporal choice data was analyzed using a mixed effects logistic regression, examining the the effect of larger-later reward value, larger-later delay, and task condition on rats choices, with random intercepts and random effects for all three variables. This model was fit as a generalized linear mixed effects model using the lme4 package in R (lme4 formula: <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo>∼</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo>*</mml:mo><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo>*</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mpadded width="+1.7pt"><mml:mi>n</mml:mi></mml:mpadded><mml:mo stretchy="false">|</mml:mo><mml:mo rspace="4.2pt" stretchy="false">|</mml:mo><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; <xref ref-type="bibr" rid="bib2">Bates et al., 2015</xref>). Post-hoc comparisons of interest were tested using the phia package in R (<xref ref-type="bibr" rid="bib13">De Rosario-Martinez, 2015</xref>), using Holm’s method to correct for multiple comparisons.</p></sec><sec id="s4-6"><title>Foraging models</title><p>All models were constructed as continuous time semi-markov processes. This provided a convenient way to capture the dynamics of timing in both tasks, such as slow delivery and consumption of reward (up to 6 s for the largest rewards). To model the foraging task, each event within the task (e.g. cues turning on/off, lever press, reward delivery, etc.) marked a state transition (abbreviated state space diagram in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. All state transitions were deterministic, except for decisions to stay in vs. leave the patch, which occurred in ‘decision‘ states (the time between cues turning on at the start of the trial and rats performing a lever press or nosepoke). In decision states, a decision to stay in the patch transitioned to the handling time state, then reward state, ITI state, and to the decision state on the next trial. A decision to leave transitioned to the travel time state, then to the first decision state in the patch. Using the notation of <xref ref-type="bibr" rid="bib7">Bradtke and Duff, 1995</xref>, the value of staying in state <inline-formula><mml:math id="inf19"><mml:mi>s</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is the reward provided for staying in state <inline-formula><mml:math id="inf21"><mml:mi>s</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, plus the discounted value of the next state:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the discount applied to the value of the next state for staying in state <inline-formula><mml:math id="inf24"><mml:mi>s</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the value of the next state in the patch. For all non-decision states, rats did not have the option to leave the patch, so for these states, <inline-formula><mml:math id="inf26"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. For decision states, the value of the state was the greater of <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>For simplicity, we assume the time spent in a given state is constant, calculated as the average amount of time a given rat spent in the state. Under this assumption, the reward in a given state, <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is equal to the reward rate provided over the course of the state, <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, multiplied by the time spent in that state <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, discounted according to discount factor <inline-formula><mml:math id="inf32"><mml:mi>β</mml:mi></mml:math></inline-formula>:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>*</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mi>β</mml:mi></mml:mfrac><mml:mo>*</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mtext>and</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>*</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The value of leaving a patch, <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, was equal to the discounted value of the first state in the next patch, <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the discount factor applied to the next state in the first patch. Assuming no variance in the travel time <inline-formula><mml:math id="inf36"><mml:mi>τ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf37"><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>*</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Per MVT, we assumed rats left patches at the first state in the patch in which <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. To model variability in the trial at which rats left patches, we added gaussian noise to <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. As decisions within each patch are not independent, the patch leaving threshold did not vary trial-by-trial, but rather patch by patch, such that the cumulative probability that a rat has left the patch by state <inline-formula><mml:math id="inf40"><mml:mi>s</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, was the probability that <inline-formula><mml:math id="inf42"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≤</mml:mo><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo rspace="4.2pt">,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, with free parameter <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> scaled with <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to enable comparisons across conditions within experiments.</p><p>The optimal policy for a given set of parameters was found using value iteration (<xref ref-type="bibr" rid="bib43">Sutton and Barto, 1998</xref>). MVT predictions (maximization of undiscounted long-term reward rate) were determined by fixing the discount factor <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>.001</mml:mn></mml:mrow></mml:math></inline-formula> and assuming no decision noise (<inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). MVT predictions were determined for each rat; the time spent in each state was taken from a given rat’s data. For each model, we fit both group level parameters and individual parameters for each rat using an expectation-maximization algorithm (<xref ref-type="bibr" rid="bib19">Huys et al., 2011</xref>).</p><p>To model subjective costs, a free parameter <inline-formula><mml:math id="inf49"><mml:mi>c</mml:mi></mml:math></inline-formula> representing an aversion to leaving the patch was subtracted from the leaving threshold (<xref ref-type="bibr" rid="bib45">Wikenheiser et al., 2013</xref>; <xref ref-type="bibr" rid="bib9">Carter and Redish, 2016</xref>):<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mpadded lspace="-1.7pt" width="-3.4pt"><mml:mi>f</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To investigate whether nonlinear reward utility could explain rats' overharvesting behavior, we tested models in which the utility of a reward received in the task increased in a sublinear fashion with respect to the magnitude of the reward. Two different utility functions were tested: a power law function and a steeper constant relative risk aversion (CRRA) utility function that became increasingly risk averse with larger rewards, both with free parameter <inline-formula><mml:math id="inf50"><mml:mi>η</mml:mi></mml:math></inline-formula>:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>η</mml:mi></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mtext>or</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To examine linear and nonlinear underestimation of post-reward delays, respectively, the time spent in post-reward delay (ITI) states was transformed, with free parameter <inline-formula><mml:math id="inf51"><mml:mi>α</mml:mi></mml:math></inline-formula>:<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow> <mml:mtext>where </mml:mtext><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>α</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mtext>or</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Similarly, for overestimation of pre-reward delays, the handling time and travel time were transformed:<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo> <mml:mtext>and</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow> <mml:mtext>where </mml:mtext><mml:mo>⁢</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the exponential discounting model, β was fit as a free parameter.</p><p>As standard hyperbolic discounting cannot conveniently be expressed recursively, this model was implemented using the μAgents model described by <xref ref-type="bibr" rid="bib26">Kurth-Nelson and Redish (2009)</xref>. The value functions of the overall model, <inline-formula><mml:math id="inf52"><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf53"><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, were the average of the μAgents, each with their own exponential discount factor <inline-formula><mml:math id="inf54"><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, and thus individual reward functions <inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, discount functions <inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf57"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and value functions <inline-formula><mml:math id="inf58"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf59"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mpadded lspace="-1.7pt" width="-3.4pt"><mml:mi>f</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>If the μAgent discount factors, <inline-formula><mml:math id="inf61"><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, are drawn from an exponential distribution with rate parameter <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the discounting function of the overall model approximated the standard hyperbolic discount function, <inline-formula><mml:math id="inf63"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>*</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with discount rate <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This model was implemented using 10 μAgents with <inline-formula><mml:math id="inf65"><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> equal to the 5%, 15%, …, 95% quantile of the exponential distribution. The relationship of this implementation of the μAgent model to the standard hyperbolic discount function is presented in <xref ref-type="fig" rid="fig6">Figure 6</xref>. <inline-formula><mml:math id="inf66"><mml:mi>k</mml:mi></mml:math></inline-formula> was fit as a free parameter.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.48429.020</object-id><label>Figure 6.</label><caption><title>Discount function of the μAgent hyperbolic discounting model vs. standard hyperbolic discounting.</title><p>Points represent the standard hyperbolic discounting function, <inline-formula><mml:math id="inf67"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>*</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Lines represent the μAgent discount function in which the discount factor for each of the 10 μAgents was equal to the 5–95% quantile of an exponential distribution with rate parameter <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-fig6-v2.tif"/></fig><p>The constant sensitivity discounting model was based on <xref ref-type="bibr" rid="bib14">Ebert and Prelec (2007)</xref>. In this model, hyperbolic time preferences are produced via exponential discounting with insensitivity to longer delays. To implement this model, insensitivity to all time delays — the decision time, pre-reward delay, reward time, and post-reward delay, and travel time — was achieved using a power function, just as in the nonlinear post-reward delay model. This model was then equivalent to the exponential discounting model, replacing the time in each state <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with a power function of the time in each state <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>Quasi-hyperbolic discounting was originally formulated for discrete time applications (<xref ref-type="bibr" rid="bib28">Laibson, 1997</xref>). We used the continuous time formulation from <xref ref-type="bibr" rid="bib32">McClure et al. (2007)</xref>, in which the value functions of the overall model were the weighted sum of two exponential discount systems, a steep discounting β system that prefers immediate rewards and a slower discounting <inline-formula><mml:math id="inf71"><mml:mi>δ</mml:mi></mml:math></inline-formula> system, each with their own reward functions, <inline-formula><mml:math id="inf72"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>δ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and discount functions <inline-formula><mml:math id="inf74"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf76"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>δ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>δ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>δ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>δ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>δ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>δ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>δ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The value functions of the overall quasi-hyperbolic discounting model were:<disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>ω</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ22"><mml:math id="m22"><mml:mrow><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>ω</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf78"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>ω</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> was the weight of the <inline-formula><mml:math id="inf79"><mml:mi>β</mml:mi></mml:math></inline-formula> system relative to the <inline-formula><mml:math id="inf80"><mml:mi>δ</mml:mi></mml:math></inline-formula> system. <inline-formula><mml:math id="inf81"><mml:mi>β</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf82"><mml:mi>δ</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf83"><mml:mi>ω</mml:mi></mml:math></inline-formula> were all free parameters.</p></sec><sec id="s4-7"><title>Intertemporal choice task models</title><p>Similar to the foraging task, events within the intertemporal choice task marked state transitions, and all state transitions were deterministic except for decisions to choose the smaller-sooner option (SS) or larger-later option (LL), which occurred only in decision states (abbreviated state space diagram in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). From decision states, animals transitioned to delay, reward, and post-reward delay (ITI) states for the chosen option — the delay, reward and ITI for the SS and LL options were represented by separate states. The value of choosing SS or LL in decision state <inline-formula><mml:math id="inf84"><mml:mi>s</mml:mi></mml:math></inline-formula> is the discounted value of the next state, the following delay state:<disp-formula id="equ23"><mml:math id="m23"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo rspace="4.2pt">,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>S</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo rspace="4.2pt">,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>L</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The value of delay states were the discounted value of the reward state for that action, the value of reward states were the reward for that action plus the discounted value of the ITI state for that action, and the value of ITI states were the discounted value of the next decision state:<disp-formula id="equ25"><mml:math id="m25"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>S</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>S</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>S</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ26"><mml:math id="m26"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>S</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>S</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>S</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>S</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>S</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>S</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>t</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where the value of the next decision state, <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>t</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the greater of <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>t</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>t</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Decisions were made assuming the value of <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo rspace="4.2pt">,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo rspace="4.2pt">,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were represented as Gaussian distributions with noise that scaled with their magnitude. The probability of choosing the LL option was the probability that a random sample from the LL distribution was greater than a random sample from the SS distribution for that state:<disp-formula id="equ28"><mml:math id="m28"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>e</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mo rspace="4.2pt">,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo rspace="4.2pt">,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>*</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mo rspace="4.2pt">,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo rspace="4.2pt">,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msqrt></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the normal cumulative distribution function.</p><p>The nonlinear reward utility, biased time perception, and temporal discounting models were implemented as they were in the foraging task.</p><p>For the one-trial horizon discounting models, the value of choosing a given option was the discounted value of the reward on the current trial only, with delay <inline-formula><mml:math id="inf91"><mml:mi>d</mml:mi></mml:math></inline-formula> and reward <inline-formula><mml:math id="inf92"><mml:mi>r</mml:mi></mml:math></inline-formula>:<disp-formula id="equ29"><mml:math id="m29"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>*</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>*</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ30"><mml:math id="m30"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>*</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ31"><mml:math id="m31"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>*</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:msup><mml:mo>*</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ32"><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>ω</mml:mi><mml:mo>∗</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mo>∗</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>∗</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>∗</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To calculate the probability of choosing the LL option, the same decision rule was used as in the all-future horizon model.</p></sec><sec id="s4-8"><title>Model comparison</title><p>All models had two parameters except for the constant sensitivity discounting model with three and the quasi-hyperbolic discounting model with four. To determine the model that provided the best fit to the data, while accounting for the increased flexibility of these models, we calculated the Bayesian Information Criterion over the group level parameters (iBIC) (<xref ref-type="bibr" rid="bib29">MacKay, 2003</xref>; <xref ref-type="bibr" rid="bib19">Huys et al., 2011</xref>). iBIC penalizes the log marginal likelihood, <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which is the integral of the log likelihood of the data <inline-formula><mml:math id="inf94"><mml:mi>D</mml:mi></mml:math></inline-formula> over the distribution of group level parameters <inline-formula><mml:math id="inf95"><mml:mi>θ</mml:mi></mml:math></inline-formula>, for model complexity. Complexity is determined by the number of parameters <inline-formula><mml:math id="inf96"><mml:mi>k</mml:mi></mml:math></inline-formula>, and the size of the penalty depends on the total number of observations, <inline-formula><mml:math id="inf97"><mml:mi>n</mml:mi></mml:math></inline-formula>:<disp-formula id="equ33"><mml:math id="m33"><mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As in <xref ref-type="bibr" rid="bib19">Huys et al. (2011)</xref>, we use a Laplace approximation to the log marginal likelihood:<disp-formula id="equ34"><mml:math id="m34"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf98"><mml:mi>s</mml:mi></mml:math></inline-formula> is the number of subjects, and <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the hessian matrix of the likelihood for subject <inline-formula><mml:math id="inf100"><mml:mi>i</mml:mi></mml:math></inline-formula> at the individual parameters <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>To compare the fit of the quasi-hyperbolic discounting model across the foraging and intertemporal choice tasks, a cross-validation method was used. Data from each task was separated into thirds. The quasi-hyperbolic discounting model was fit to 2 of the samples from each task using maximum likelihood estimation (fitting only individual parameters for each rat). The log likelihood of the data from the left out sample was evaluated. This process was repeated three times, leaving out each of the samples once, and we took the sum of the likelihood of the three left out samples. As the structure of variability was different between the foraging model (variability in the patch leaving threshold) and intertemporal choice models (noise in the estimates of SS and LL values), to compare the discount function fit to the foraging task on intertemporal choice data, a new noise parameter was fit to the intertemporal choice data (and vice-versa). We report the difference in the log likelihood of the data using parameters fit to the intertemporal choice task and of the log likelihood using parameters fit to the foraging task (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Dr. Gary Aston-Jones for helpful discussions. This work was supported by NIH grant F31MH109286 (GAK) and the Princeton Program in Cognitive Science.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Supervision, Writing—original draft, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All procedures were approved by the Princeton University (Protocol 1969) and Rutgers University (Protocol 14-075) Institutional Animal Care and Use Committees.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><object-id pub-id-type="doi">10.7554/eLife.48429.021</object-id><label>Source code 1.</label><caption><title>Foraging mixed effects models.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-48429-code1-v2.r"/></supplementary-material><supplementary-material id="scode2"><object-id pub-id-type="doi">10.7554/eLife.48429.022</object-id><label>Source code 2.</label><caption><title>Intertemporal choice mixed effects model.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-48429-code2-v2.r"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.48429.023</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-48429-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ainslie</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1992">1992</year><chapter-title>The Strategic Interaction of Successive Motivational States Within the Person</chapter-title><source>Picoeconomics</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname> <given-names>D</given-names></name><name><surname>Mächler</surname> <given-names>M</given-names></name><name><surname>Bolker</surname> <given-names>BM</given-names></name><name><surname>Walker</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fitting linear mixed-effects models using lme4</article-title><source>Journal of Statistical Software</source><volume>67</volume><elocation-id>v067i01</elocation-id><pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bateson</surname> <given-names>M</given-names></name><name><surname>Kacelnik</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Rate currencies and the foraging starling: the fallacy of the averages revisited</article-title><source>Behavioral Ecology</source><volume>7</volume><fpage>341</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1093/beheco/7.3.341</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanchard</surname> <given-names>TC</given-names></name><name><surname>Pearson</surname> <given-names>JM</given-names></name><name><surname>Hayden</surname> <given-names>BY</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Postreward delays and systematic biases in measures of animal temporal discounting</article-title><source>PNAS</source><volume>110</volume><fpage>15491</fpage><lpage>15496</lpage><pub-id pub-id-type="doi">10.1073/pnas.1310446110</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanchard</surname> <given-names>TC</given-names></name><name><surname>Hayden</surname> <given-names>BY</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Monkeys are more patient in a foraging task than in a standard intertemporal choice task</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0117057</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0117057</pub-id><pub-id pub-id-type="pmid">25671436</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>E</given-names></name><name><surname>Moehlis</surname> <given-names>J</given-names></name><name><surname>Holmes</surname> <given-names>P</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks</article-title><source>Psychological Review</source><volume>113</volume><fpage>700</fpage><lpage>765</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id><pub-id pub-id-type="pmid">17014301</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bradtke</surname> <given-names>SJ</given-names></name><name><surname>Duff</surname> <given-names>MO</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Reinforcement learning methods for continuous-time markov decision problems</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>393</fpage><lpage>400</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/889-reinforcement-learning-methods-for-continuous-time-markov-decision-problems">https://papers.nips.cc/paper/889-reinforcement-learning-methods-for-continuous-time-markov-decision-problems</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname> <given-names>EC</given-names></name><name><surname>Pedersen</surname> <given-names>EJ</given-names></name><name><surname>McCullough</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reassessing intertemporal choice: human decision-making is more optimal in a foraging task than in a self-control task</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>95</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00095</pub-id><pub-id pub-id-type="pmid">25774140</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname> <given-names>EC</given-names></name><name><surname>Redish</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rats value time differently on equivalent foraging and delay-discounting tasks</article-title><source>Journal of Experimental Psychology: General</source><volume>145</volume><fpage>1093</fpage><lpage>1101</lpage><pub-id pub-id-type="doi">10.1037/xge0000196</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charnov</surname> <given-names>EL</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Optimal foraging, the marginal value theorem</article-title><source>Theoretical Population Biology</source><volume>9</volume><fpage>129</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/0040-5809(76)90040-X</pub-id><pub-id pub-id-type="pmid">1273796</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantino</surname> <given-names>SM</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning the opportunity cost of time in a patch-foraging task</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>15</volume><fpage>837</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.3758/s13415-015-0350-y</pub-id><pub-id pub-id-type="pmid">25917000</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname> <given-names>JD</given-names></name><name><surname>El Hady</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Foraging as an evidence accumulation process</article-title><source>PLOS Computational Biology</source><volume>15</volume><fpage>e1007060</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007060</pub-id><pub-id pub-id-type="pmid">31339878</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>De Rosario-Martinez</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>.phia: Post-Hoc Interaction Analysis</data-title><ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=phia">https://CRAN.R-project.org/package=phia</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebert</surname> <given-names>JEJ</given-names></name><name><surname>Prelec</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The fragility of Time: time-insensitivity and valuation of the near and far future</article-title><source>Management Science</source><volume>53</volume><fpage>1423</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1287/mnsc.1060.0671</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Gabaix</surname> <given-names>X</given-names></name><name><surname>Laibson</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Myopia and Discounting</source><publisher-name>National Bureau of Economic Research</publisher-name><ext-link ext-link-type="uri" xlink:href="https://www.nber.org/papers/w23254">https://www.nber.org/papers/w23254</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallistel</surname> <given-names>CR</given-names></name><name><surname>Gibbon</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Time, rate, and conditioning</article-title><source>Psychological Review</source><volume>107</volume><fpage>289</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.107.2.289</pub-id><pub-id pub-id-type="pmid">10789198</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayden</surname> <given-names>BY</given-names></name><name><surname>Pearson</surname> <given-names>JM</given-names></name><name><surname>Platt</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neuronal basis of sequential foraging decisions in a patchy environment</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>933</fpage><lpage>939</lpage><pub-id pub-id-type="doi">10.1038/nn.2856</pub-id><pub-id pub-id-type="pmid">21642973</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hunter</surname> <given-names>LE</given-names></name><name><surname>Bornstein</surname> <given-names>AM</given-names></name><name><surname>Hartley</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A common deliberative process underlies model-based planning and patient intertemporal choice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/499707</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname> <given-names>QJ</given-names></name><name><surname>Cools</surname> <given-names>R</given-names></name><name><surname>Gölzer</surname> <given-names>M</given-names></name><name><surname>Friedel</surname> <given-names>E</given-names></name><name><surname>Heinz</surname> <given-names>A</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Disentangling the roles of approach, activation and Valence in instrumental and pavlovian responding</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002028</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002028</pub-id><pub-id pub-id-type="pmid">21556131</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname> <given-names>QJ</given-names></name><name><surname>Eshel</surname> <given-names>N</given-names></name><name><surname>O'Nions</surname> <given-names>E</given-names></name><name><surname>Sheridan</surname> <given-names>L</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Roiser</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Bonsai trees in your head: how the pavlovian system sculpts goal-directed choices by pruning decision trees</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002410</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002410</pub-id><pub-id pub-id-type="pmid">22412360</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kacelnik</surname> <given-names>A</given-names></name><name><surname>Todd</surname> <given-names>IA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Psychological mechanisms and the marginal value theorem: effect of variability in travel time on patch exploitation</article-title><source>Animal Behaviour</source><volume>43</volume><fpage>313</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1016/S0003-3472(05)80226-X</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kane</surname> <given-names>GA</given-names></name><name><surname>Vazey</surname> <given-names>EM</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Shenhav</surname> <given-names>A</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Aston-Jones</surname> <given-names>G</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Increased locus coeruleus tonic activity causes disengagement from a patch-foraging task</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>17</volume><fpage>1073</fpage><lpage>1083</lpage><pub-id pub-id-type="doi">10.3758/s13415-017-0531-y</pub-id><pub-id pub-id-type="pmid">28900892</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Killeen</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>An additive-utility model of delay discounting</article-title><source>Psychological Review</source><volume>116</volume><fpage>602</fpage><lpage>619</lpage><pub-id pub-id-type="doi">10.1037/a0016414</pub-id><pub-id pub-id-type="pmid">19618989</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirby</surname> <given-names>KN</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Bidding on the future: evidence against normative discounting of delayed rewards</article-title><source>Journal of Experimental Psychology: General</source><volume>126</volume><fpage>54</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.126.1.54</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolling</surname> <given-names>N</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Mars</surname> <given-names>RB</given-names></name><name><surname>Rushworth</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural mechanisms of foraging</article-title><source>Science</source><volume>336</volume><fpage>95</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1126/science.1216930</pub-id><pub-id pub-id-type="pmid">22491854</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurth-Nelson</surname> <given-names>Z</given-names></name><name><surname>Redish</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Temporal-difference reinforcement learning with distributed representations</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e7362</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0007362</pub-id><pub-id pub-id-type="pmid">19841749</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuznetsova</surname> <given-names>A</given-names></name><name><surname>Brockhoff</surname> <given-names>PB</given-names></name><name><surname>Christensen</surname> <given-names>RHB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>lmerTest Package: Tests in Linear Mixed Effects Models</article-title><source>Journal of Statistical Software</source><volume>82</volume><elocation-id>v082i13</elocation-id><pub-id pub-id-type="doi">10.18637/jss.v082.i13</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laibson</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Golden eggs and hyperbolic discounting</article-title><source>The Quarterly Journal of Economics</source><volume>112</volume><fpage>443</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1162/003355397555253</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>MacKay</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Information Theory, Inference and Learning Algorithms</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazur</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Choice with probabilistic reinforcement: effects of delay and conditioned reinforcers</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>55</volume><fpage>63</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1901/jeab.1991.55-63</pub-id><pub-id pub-id-type="pmid">2002302</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClure</surname> <given-names>SM</given-names></name><name><surname>Laibson</surname> <given-names>DI</given-names></name><name><surname>Loewenstein</surname> <given-names>G</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Separate neural systems value immediate and delayed monetary rewards</article-title><source>Science</source><volume>306</volume><fpage>503</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1126/science.1100907</pub-id><pub-id pub-id-type="pmid">15486304</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClure</surname> <given-names>SM</given-names></name><name><surname>Ericson</surname> <given-names>KM</given-names></name><name><surname>Laibson</surname> <given-names>DI</given-names></name><name><surname>Loewenstein</surname> <given-names>G</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Time discounting for primary rewards</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>5796</fpage><lpage>5804</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4246-06.2007</pub-id><pub-id pub-id-type="pmid">17522323</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Namboodiri</surname> <given-names>VM</given-names></name><name><surname>Mihalas</surname> <given-names>S</given-names></name><name><surname>Marton</surname> <given-names>TM</given-names></name><name><surname>Hussain Shuler</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A general theory of intertemporal decision-making and the perception of time</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>8</volume><elocation-id>61</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2014.00061</pub-id><pub-id pub-id-type="pmid">24616677</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nonacs</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>State dependent behavior and the marginal value theorem</article-title><source>Behavioral Ecology</source><volume>12</volume><fpage>71</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1093/oxfordjournals.beheco.a000381</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname> <given-names>JM</given-names></name><name><surname>Hayden</surname> <given-names>BY</given-names></name><name><surname>Platt</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Explicit information reduces discounting behavior in monkeys</article-title><source>Frontiers in Psychology</source><volume>1</volume><elocation-id>237</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2010.00237</pub-id><pub-id pub-id-type="pmid">21833291</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seinstra</surname> <given-names>MS</given-names></name><name><surname>Sellitto</surname> <given-names>M</given-names></name><name><surname>Kalenscher</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rate maximization and hyperbolic discounting in human experiential intertemporal decision making</article-title><source>Behavioral Ecology</source><volume>29</volume><fpage>193</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1093/beheco/arx145</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenhav</surname> <given-names>A</given-names></name><name><surname>Straccia</surname> <given-names>MA</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Anterior cingulate engagement in a foraging context reflects choice difficulty, not foraging value</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1249</fpage><lpage>1254</lpage><pub-id pub-id-type="doi">10.1038/nn.3771</pub-id><pub-id pub-id-type="pmid">25064851</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephens</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The adaptive value of preference for immediacy: when shortsighted rules have farsighted consequences</article-title><source>Behavioral Ecology</source><volume>12</volume><fpage>330</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1093/beheco/12.3.330</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephens</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Discrimination, discounting and impulsivity: a role for an informational constraint</article-title><source>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</source><volume>357</volume><fpage>1527</fpage><lpage>1537</lpage><pub-id pub-id-type="doi">10.1098/rstb.2002.1062</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephens</surname> <given-names>DW</given-names></name><name><surname>Kerr</surname> <given-names>B</given-names></name><name><surname>Fernández-Juricic</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Impulsiveness without discounting: the ecological rationality hypothesis</article-title><source>Proceedings of the Royal Society of London. Series B: Biological Sciences</source><volume>271</volume><fpage>2459</fpage><lpage>2465</lpage><pub-id pub-id-type="doi">10.1098/rspb.2004.2871</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephens</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision ecology: foraging and the ecology of animal decision making</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>8</volume><fpage>475</fpage><lpage>484</lpage><pub-id pub-id-type="doi">10.3758/CABN.8.4.475</pub-id><pub-id pub-id-type="pmid">19033242</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stephens</surname> <given-names>DW</given-names></name><name><surname>Krebs</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="1986">1986</year><source>Foraging Theory, Ser. Monographs in Behavior and Ecology</source><publisher-loc>Princeton, NJ</publisher-loc><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><chapter-title>An Introduction</chapter-title><source>Reinforcement Learning </source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thaler</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Some empirical evidence on dynamic inconsistency</article-title><source>Economics Letters</source><volume>8</volume><fpage>201</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1016/0165-1765(81)90067-7</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wikenheiser</surname> <given-names>AM</given-names></name><name><surname>Stephens</surname> <given-names>DW</given-names></name><name><surname>Redish</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Subjective costs drive overly patient foraging strategies in rats on an intertemporal foraging task</article-title><source>PNAS</source><volume>110</volume><fpage>8308</fpage><lpage>8313</lpage><pub-id pub-id-type="doi">10.1073/pnas.1220738110</pub-id><pub-id pub-id-type="pmid">23630289</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zauberman</surname> <given-names>G</given-names></name><name><surname>Kim</surname> <given-names>BK</given-names></name><name><surname>Malkoc</surname> <given-names>SA</given-names></name><name><surname>Bettman</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Discounting time and time discounting: subjective time perception and intertemporal preferences</article-title><source>Journal of Marketing Research</source><volume>46</volume><fpage>543</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1509/jmkr.46.4.543</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48429.025</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Schoenbaum</surname><given-names>Geoffrey</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute on Drug Abuse, National Institutes of Health</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Namboordiri</surname><given-names>Vijay</given-names> </name><role>Reviewer</role></contrib><contrib contrib-type="reviewer"><name><surname>van Wingerden</surname><given-names>Marijn</given-names> </name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Rats exhibit similar biases in foraging and intertemporal choice tasks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by Geoffrey Schoenbaum as the Reviewing Editor and Laura Colgin as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Vijay Namboordiri (Reviewer #2); Marijn van Wingerden (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This study aims to reconcile behavior exhibited in patch foraging tasks with that shown during intertemporal choice. Across several experiments, the authors replicate the &quot;overharvesting&quot; biases observed previously, examining several competing proposals to explain them, concluding that models incorporating time preferences are required for any adequate explanation. The quasi hyperbolic models derived from patch foraging work were best to describe behavior in both tasks, suggesting these models may represent a general underlying function in both behaviors.</p><p>Essential revisions:</p><p>The reviewers were in agreement that the work was well done and addressed an important question. Each had concerns and questions; however, it was agreed that as long as the authors make a good faith attempt to address these, none of them required a very particular answer for the paper to be acceptable for publication. It is important however that they are carefully considered and adequately addressed. These include, in particular, discussion of how the current proposal relates to prior ideas as well as other issues raised (please refer to individual reviews below). Despite these concerns, the reviewers were overall impressed with the work and its contribution to the literature.</p><p><italic>Reviewer #1:</italic></p><p>Summary:</p><p>The authors test rats' temporal decision-making in six tasks: five foraging tasks and one intertemporal choice task. The foraging tasks test increasingly complex predictions of marginal value theory (MVT). In the first, second, and fifth tasks, they systematically validate the qualitative predictions that rats will stay longer in patches that i) deliver larger reward, ii) are separated by longer travel times, iii) deplete at slower rates, iv) deliver reward at shorter intervals. However, they observed that rats 'overharvested': stayed longer in patches than is optimal and predicted by MVT. These findings add to a growing literature refuting MVT by reporting overharvesting. In the 'scale' task, the authors observe that when all magnitudes and rewards are scaled 2x, the rats overharvested longer than the 1x condition. In the 'pre-vs-post' task, the authors replicate divergent effects of time requirements placed before or after rewards (a common feature of experiments showing suboptimal intertemporal choice) layered on top of foraging decisions. The fifth task refutes any hypothesis that attributes suboptimal behavior to failure to consider post-reward delays. The authors further test rats in more traditional intertemporal choice experiments so that they can compare behavior across all tasks and find the best-fitting quantitative model. Interestingly, they find a two-parameter model they call 'quasi-hyperbolic' to fit better than any one-parameter model attempted.</p><p>Strengths:</p><p>This is a relatively thorough investigation of the sensitivities of rats behavior to many of the variables that could possibly impact optimal foraging behavior. For every variable investigated, the authors observe the expected qualitative effect predicted by MVT, and replicate qualitative suboptimalities observed in simpler choice contexts within the foraging context. The authors also have the opportunity to investigate the relationship across behaviors for each individual.</p><p>Weaknesses in analysis:</p><p>In the Scale Experiment, did the rats overharvest 2x as long in the 2x condition? Perhaps this shouldn't be presented as unexpected.</p><p>The best-fit evaluation (quasi-hyperbolic) is the only 2-parameter version. The authors should invent additional 2-parameter models for further comparison.</p><p>The variability across rats in identified parameters should be better discussed and illustrated. The analysis is not a convincing illustration that rats' performance in one task is predictive of performance in another. Perhaps a correlation in parameters across rats would be more convincing.</p><p>Only a few specific forms of misrepresenting time spent before and after reward are tested, but this is presented as a thorough refutation of theories of altered time perception.</p><p>Weaknesses in presentation:</p><p>The numerous motivating hypotheses describing decision making are poorly presented. I recommend a table describing them and indicating what they predict for each of the experiments. A glossary clearly explaining each theory would also be helpful. Why isn't the fifth foraging experiment included in Table 1? Nonlinear reward magnitude utility is described as &quot;decreasing marginal value,&quot; which is confusing.</p><p><italic>Reviewer #2:</italic></p><p>This manuscript by Kane et al. tests behavior of rats in a foraging-like task and a delay-discounting task to advance the claim that behavior in both of these tasks can be explained by a single set of rules. This is an important claim since prior papers have argued that both humans and animals might show different behaviors depending on the nature of the tasks, even though value reduction of a delayed reward should play a role across both conditions. My only major concern relates to the treatment of prior work.</p><p>1) The authors should cite Seinstra, Sellitto and Kalenscher, 2017. This paper argues that human behavior across foraging and delay-discounting settings can be explained by a single framework. The framework is rate maximization and hence, is different from the one advanced by the authors. But nevertheless, it would be good to discuss this paper since it is highly relevant.</p><p>2) The argument presented in Blanchard et al., 2013 is that post reward delays are not accounted for, primarily due to the deficit in learning these delays. This is because most post reward delays are not signaled explicitly in the environment. Therefore, they show that adding additional small rewards after the post-reward delay is sufficient to cause learning of the delay and for inclusion of the delay in the decision process. This is different from the interpretation presented here. Related to this, I am not convinced that the rats learn the post-reward delays during the foraging task. The ITI is adjusted depending on the DT, but if the animals are not keeping track of time since trial start, they would not learn this structure of the environment. Of course, in the absence of such learning, it is not surprising that post-reward delays are not weighted equally as the pre-reward delays. So, one possible modification could be that only a weighted value of the post-reward delay is included in the decision.</p><p>3) Another set of papers actually also provide a theoretical framework to explain intertemporal decision-making during foraging and delay-discounting. I will confess that I am an author of these papers. While I typically am loathe to suggesting my own papers during peer reviews, I think it is highly relevant in this case. The papers introducing this model are Namboodiri et al., 2014 A general theory of intertemporal decision-making and the perception of time, Frontiers in Beh. Neurosci., (primary research article) and Namboodiri et al., 2014 Rationalizing decision-making: understanding the cost and perception of time, Timing and Time Perception Reviews, (review). There are a few other papers investigating errors in decision-making using a trial-by-trial approach, but the mean field model presented in these papers is sufficient to highlight the relevance. In these papers, the main model that is presented for illustration purposes is almost a toy model, and was falsified by Carter and Redish, 2016. However, there are simple and more realistic extensions that were not considered by Carter and Redish and are in the Appendix of the original paper. I will present a (hopefully short) summary of its application to the foraging data presented here to show that at least all of the qualitative findings hold within a rate maximization perspective over a limited temporal horizon.</p><p>Decision-making in this TIMERR model is based on comparing the expected reward rate over a temporal horizon (T<sub>ime</sub>) and the expected delays to the next reward. Of course, even though the post-reward delay is after the delay to the next reward, it should be incorporated in the decision, if learned. If one assumes a potential learning deficit in these tasks due to the delay not being explicitly signaled, a simple model for their inclusion could be a weight (w) multiplied by the perceived delay (assumed to be the objective delay, t<sub>post</sub>, for simplicity). So effectively, the time horizon of consideration is T<sub>ime</sub>+t<sub>pre</sub>+w*t<sub>post</sub>. If the animal estimates an average reward rate (a<sub>est</sub>) to be obtained during the temporal horizon Time, the net reward expected over Time is a<sub>est</sub>*T<sub>ime</sub>. The toy model in the above papers assumes that a<sub>est</sub> is calculated over a time window exactly equal to T<sub>ime</sub>, but this need not be the case. This is the key modification that Carter and Redish, 2016, did not consider. Thus, the expected reward rate for the nth visit to a patch equals</p><p>RR(n) = (r-d*n+a<sub>est</sub>*T<sub>ime</sub>)/(t<sub>decision</sub>+t<sub>pre</sub>+w*t<sub>post</sub>+T<sub>ime</sub>)</p><p>Where r is the starting reward, d is the rate of reduction of the reward and t<sub>decision</sub> is the time until the decision is made after the cue. Since t<sub>harvest</sub>=t<sub>decision</sub>+t<sub>pre</sub>+t<sub>post</sub>, the above can be written as</p><p>RR(n) = (r-d*n+a<sub>est</sub>*T<sub>ime</sub>)/(t<sub>harvest</sub>-(1-w)*t<sub>post</sub>+T<sub>ime</sub>)</p><p>Similarly, the expected reward rate for the limited temporal horizon until the next reward after traveling to a new patch is</p><p>RR(travel) = (r+a<sub>est</sub>*T<sub>ime</sub>)/(t<sub>travel</sub>+t<sub>harvest</sub>-(1-w)*t<sub>post</sub>+T<sub>ime</sub>)</p><p>Thus, the number of visits one would stay in a patch would be the number at which the above two expected reward rates equal. Doing some algebra and noting that a<sub>est</sub> is actually dependent on the task itself (average estimated reward rate over some time horizon prior to the decision), hopefully, it will be clear that the findings of this manuscript at least qualitatively are consistent with this model. There are three free parameters here, viz., T<sub>ime</sub>, w, and the time horizon over which a<sub>est</sub> is calculated. In the interest of brevity of this review, I will not prolong this, but the main point is that this simple model of constrained rate maximization is at least qualitatively consistent with the key findings of this manuscript. Indeed, a more accurate model would require considering potential biases due to errors in time perception as treated in Namboodiri et al. 2014, Analytical calculation of errors in time and value perception due to a subjective time accumulator: A mechanistic model and the generation of Weber's law, Neural Computation, 2016.</p><p>I want to highlight that the point of my raising this is not to force a detailed consideration of my own work. I raise it only because the authors genuinely seem to care about these problems in decision-making and may perhaps find a different perspective on their data valuable.</p><p>4) Related to points 2 and 3 above, the assumption is that animals have learned the structure of the task fully and can include all appropriate values objectively in their decision-making. This is an assumption that may be worth explicitly pointing out.</p><p>5) Another descriptive discounting function that could potentially also fit the data is Killeen, 2009. This form is more of a descriptive model than a normative one, but has some math that is superficially similar to the TIMERR model, and would also likely be consistent with these data.</p><p><italic>Reviewer #3:</italic></p><p>The manuscript by Kane et al. is an exciting addition to the literature than has been trying to reconcile animal decision making in patch foraging vs. intertemporal choice problems. Using a surprisingly low number of animals, the authors robustly replicate a number of decision biases that have been shown in previous research, collectively described as &quot;overharvesting&quot;, related to handling time, pre/post-delay sensitivity and other choice attributes. Next, they proceed to examine several (competing) proposals for cognitive choice preferences that could explain some of these biases. The authors convincingly demonstrate that any model not assuming time preferences fails to adequate explain all choice biases displayed by the animals, and that of the models incorporating time preferences, β-δ quasi-hyperbolic discounting provides the best fit across the paradigms, and also in almost all of the single experiment cases. Importantly, the quasi hyperbolic models derived from patch foraging work well to describe behavior in the intertemporal choice setting and vice versa, suggesting that they might touch upon a basic choice heuristic that plays a significant role animal decision making.</p><p>The manuscript is well written and concise. I have no objections to the line of argument in general. One concern that I hope the authors can address concerns the following: in the Markov analyses, future rewards are taken into account and are assumed to include all rewards up to the last one in the current block, to induce the incorporation of the long-term reward rate into the decisions. It is however unclear why the authors chose to expand the horizon to the last choice. It seems to me that the &quot;reward horizon&quot; of animals might actually be limited or even fluctuate, for example based on satiety. It seems that with the current data at hand, it would be possible to model the reward horizon as a free parameter to approach this choice empirically. What horizon appears from this analysis? And can the authors replicate the &quot;failure&quot; for cross-explanation between foraging and ITC when the horizon is set to only the current trial?</p><p>Additionally, while the quasi-hyperbolic discount model provides an overall better fit than for example the diminishing marginal returns model, it seems that animals exhibit both time preferences AND experience sensitivity to diminishing marginal returns, based on reward size differences and (the very real consequences of) already accumulated rewards.</p><p>If accumulated marginal returns are not important, one would assume that a median split (early vs. late) on the trials in the foraging dataset, or the first 5 vs. last 5 trials in the ITC dataset would produce similar model estimates. However, if the model fits diverge substantially this would suggest that an extension of the model with quasi-hyperbolic time-preferences to include marginal reward size/accumulation effects might provide an even better fit. I think this question could also be answered with the current dataset that is unique equipped to compare models under different experimental circumstances.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48429.026</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The reviewers were in agreement that the work was well done and addressed an important question. Each had concerns and questions; however, it was agreed that as long as the authors make a good faith attempt to address these, none of them required a very particular answer for the paper to be acceptable for publication. It is important however that they are carefully considered and adequately addressed. These include, in particular, discussion of how the current proposal relates to prior ideas as well as other issues raised (please refer to individual reviews below). Despite these concerns, the reviewers were overall impressed with the work and its contribution to the literature.</p><p>Reviewer #1:</p><p>Strengths:</p><p>This is a relatively thorough investigation of the sensitivities of rats behavior to many of the variables that could possibly impact optimal foraging behavior. For every variable investigated, the authors observe the expected qualitative effect predicted by MVT, and replicate qualitative suboptimalities observed in simpler choice contexts within the foraging context. The authors also have the opportunity to investigate the relationship across behaviors for each individual.</p><p>Weaknesses in analysis:</p><p>In the Scale Experiment, did the rats overharvest 2x as long in the 2x condition? Perhaps this shouldn't be presented as unexpected.</p></disp-quote><p>We apologize for not being clear with regard to the interpretation of statistical analysis. Rats did not overharvest by a factor of two, rather they overharvested for an additional 2 trials. This point has been clarified in the text by adding units to regression coefficients where appropriate (e.g. “β = 1.937 trials, SE = .193…”). Further, we now clarify that this is not being presented as surprising or unexpected, but only as contrary to predictions of the Marginal Value Theorem:</p><p>“[Rats] overharvested in both A and B conditions… and, contrary to predictions of MVT, they stayed in patches significantly longer….”</p><disp-quote content-type="editor-comment"><p>The best-fit evaluation (quasi-hyperbolic) is the only 2-parameter version. The authors should invent additional 2-parameter models for further comparison.</p></disp-quote><p>As suggested by reviewers 2 and 3 (comments 6 and 3 respectively), we tested additional models with greater complexity (&gt; 2 parameters), including a constant time sensitivity model that combined exponential discounting with biases in time perception, and a model reminiscent of additive-utility discounting, combining nonlinear reward utility, biased time perception and exponential discounting (see below). Overall, neither of these models performed better than the quasi-hyperbolic discounting model. We would gladly test any additional models the reviewer may suggest.</p><p>In addition, to account for different numbers of parameters across models, we used the Bayesian Information Criterion which penalizes the fit of a model for its complexity (i.e. the number of parameters), and thus, does not provide a particular advantage to a model with more parameters. In fact, because BIC uses a fixed penalty per additional parameter, it may “overpenalize” more complex models in situations where an additional parameter does not provide a multiplicative increase in explanatory power (Bernardo and Smith, 1994, Bayesian Theory).</p><disp-quote content-type="editor-comment"><p>The variability across rats in identified parameters should be better discussed and illustrated. The analysis is not a convincing illustration that rats' performance in one task is predictive of performance in another. Perhaps a correlation in parameters across rats would be more convincing.</p></disp-quote><p>We agree that investigating correlations between parameters could provide an additional line of evidence that rats behave in similar ways across the two tasks, but we were wary of this analysis for two reasons: i) correlations between model parameters do not imply that behavior in one task can predict behavior in the other task and ii) with only 8 rats in each experiment, correlations are unreliable (e.g. see Schonbrodt and Perugini, 2013, J Res Personality) and greatly underpowered. A power analysis revealed that a correlation coefficient r = 0.833 would be needed to achieve power = 0.8 with significance level of p=0.05.</p><p>Despite these issues, we conducted a few exploratory correlation analyses. First, we examined the correlation between the behavioral effect of the post-reward delay across tasks (i.e. the effect of the delay in the Post-Reward Delay foraging experiment, and the interaction between the effect of the LL delay and task condition – the difference in the effect of the LL delay when it affected reward rate vs. when it did not affect reward rate). These effects were estimated for each rat using mixed effects models. There was a strong, but not significant, correlation between the effect of the delay (r = .606, p = .111).</p><p>We also tested correlations between each of the four quasi-hyperbolic model parameters across the Post-Reward Delay foraging task and intertemporal choice task. Again, correlations between some parameters were quite strong, but not significant following Bonferroni correction for multiple comparisons (see <xref ref-type="fig" rid="respfig1">Author response image 1</xref>). Although these observations must be treated with caution, they suggest there may be stability of behavior and model parameters across tasks.</p><fig id="respfig1"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48429-resp-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Only a few specific forms of misrepresenting time spent before and after reward are tested, but this is presented as a thorough refutation of theories of altered time perception.</p></disp-quote><p>We regret that we gave this impression, as we certainly do not mean to suggest that this work is a thorough refutation that altered time perception contributes to overharvesting or suboptimal behavior in intertemporal choice tasks. Rather, our goal is to communicate that a particular set of models that represent the hypothesis that animals are differently sensitive to pre- vs. post-reward delays do not explain rat foraging or intertemporal choice data as well as the quasi-hyperbolic discounting model. We have revised the manuscript to provide more context for these hypotheses by providing more discussion of Blanchard et al., 2013 (as suggested by reviewer 2). We emphasize the behavioral sensitivity to post-reward delays in the Post-Reward Delay experiment:</p><p>“Prior studies of intertemporal choice behavior suggest that animals are insensitive to post-reward delays, suggesting that they are only concerned with maximizing short-term reward rate (Stephens and Anderson, 2001, Bateson and Kacelnik 1996) or they may not have learned the duration of post-reward delays, and underestimate this duration in their decision process (Pearson et al., 2010, Blanchard and Hayden, 2013). […] However, it is possible that this finding could be explained by other forms of altered time perception that remain to be described.”</p><p>And in the discussion, we clarify that our findings do not implicate quasi-hyperbolic discounting as the one-and-only explanation for suboptimal behavior in foraging and intertemporal choice tasks:</p><p>“Although quasi-hyperbolic discounting provided the best singular explanation for rat behavior across our tasks, many of the other models tested were capable of explaining some of the biases exhibited by rats. […] Importantly, our data indicate that quasi-hyperbolic discounting may provide a link between foraging and intertemporal choice tasks, and it highlights the importance of future work considering the source of time preferences.”</p><disp-quote content-type="editor-comment"><p>Weaknesses in presentation:</p><p>The numerous motivating hypotheses describing decision making are poorly presented. I recommend a table describing them and indicating what they predict for each of the experiments. A glossary clearly explaining each theory would also be helpful. Why isn't the fifth foraging experiment included in Table 1? Nonlinear reward magnitude utility is described as &quot;decreasing marginal value,&quot; which is confusing.</p></disp-quote><p>Thank you for the valuable suggestion to include a table of each hypothesis along with predictions for each experiment. The table is now included and cited in the first paragraph of the Results section: “Quasi-hyperbolic discounting best explains behavior across all tasks.”</p><p>We expanded this table to include the fifth experiment. The fifth foraging experiment (post-reward delay experiment) was not initially included in the table because a separate cohort of rats was tested and we did not adjust the post-reward delay to control trial time as was done for the first four experiments. These differences are now noted in the table and its caption.</p><p>To avoid confusion, we changed “diminishing marginal value” and “diminishing marginal returns” to “nonlinear reward utility” throughout the text.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>This manuscript by Kane et al. tests behavior of rats in a foraging-like task and a delay-discounting task to advance the claim that behavior in both of these tasks can be explained by a single set of rules. This is an important claim since prior papers have argued that both humans and animals might show different behaviors depending on the nature of the tasks, even though value reduction of a delayed reward should play a role across both conditions. My only major concern relates to the treatment of prior work.</p><p>1) The authors should cite Seinstra, Sellitto and Kalenscher, 2017. This paper argues that human behavior across foraging and delay-discounting settings can be explained by a single framework. The framework is rate maximization and hence, is different from the one advanced by the authors. But nevertheless, it would be good to discuss this paper since it is highly relevant.</p></disp-quote><p>We agree that this paper is highly relevant to the current work and have included a brief discussion of these findings:</p><p>“[The idea that decision-making systems evolved to solve foraging problems rather two-alternative choice problems] has been challenged by a recent study of human decision-making in foraging and intertemporal choice tasks, finding that a long-term rate maximization model explained behavior in both tasks better than a standard hyperbolic discounting model (Seinstra et al., 2017). Results from the present study support the interpretation that foraging and intertemporal choice behavior can be explained by a common model, but suggest that this model is a form of discounted rate maximization.”</p><disp-quote content-type="editor-comment"><p>2) The argument presented in Blanchard et al., 2013 is that post reward delays are not accounted for, primarily due to the deficit in learning these delays. This is because most post reward delays are not signaled explicitly in the environment. Therefore, they show that adding additional small rewards after the post-reward delay is sufficient to cause learning of the delay and for inclusion of the delay in the decision process. This is different from the interpretation presented here.</p></disp-quote><p>We agree with the reviewer’s interpretation of Blanchard et al., 2013 and apologize for inadvertently misrepresenting their argument. We have revised the way in which we discuss these findings throughout the paper (below), but this revised discussion does not change our interpretation of the results.</p><p>In the Introduction: “An alternative explanation for the preference for immediate rewards in intertemporal choice tasks is that animals simply underestimate the duration of post-reward delays; that is, delays between receiving reward and making the next decision (Blanchard et al., 2013, Pearson et al., 2010). […] Consistent with this hypothesis, providing an explicit cue for the duration of the post-reward delay or increasing its salience by providing a small reward at the end of the post-reward delay reduces temporal discounting (Blanchard et al., 2013, Pearson et al., 2010).”</p><p>Also, in the Results (see reviewer 1, comment 4), and in the Discussion:</p><p>“A number of studies have found that the preference for smaller, more immediate rewards can be explained by insensitivity to post-reward delays (Bateson et al., 1996, Blanchard et al., 2013, Pearson et al., 2010, Stephens and Anderson 2001, Mazur, 1991). One hypothesis for why animals fail to incorporate post-reward delays into decisions is that they haven't learned the structure of the task well, and thus cannot accurately predict future post-reward delays. It has been shown that providing explicit cues for the post-reward delays or increasing the salience of post-reward delays helps animals incorporate these delays into their decisions, reducing the bias towards selecting smaller, more immediate rewards over larger, delayed ones (Blanchard et al., 2013, Pearson et al., 2010).”</p><disp-quote content-type="editor-comment"><p>Related to this, I am not convinced that the rats learn the post-reward delays during the foraging task. The ITI is adjusted depending on the DT, but if the animals are not keeping track of time since trial start, they would not learn this structure of the environment. Of course, in the absence of such learning, it is not surprising that post-reward delays are not weighted equally as the pre-reward delays. So, one possible modification could be that only a weighted value of the post-reward delay is included in the decision.</p></disp-quote><p>Results from the Post-Reward Delay Experiment, which directly tested rats’ ability to incorporate the post-reward delay into decisions, showed that rats were sensitive to post-reward delays. However, in this experiment, the post-reward delay was fixed, whereas in the other foraging experiments, the post-reward delay was variable. It’s possible that rats struggled to incorporate the variable post-reward delay into decision processes in other experiments. To test this hypothesis, we tested the suggested model, which included a weighted value of the post-reward delay (referred to as the linear underestimation of post-reward delays model; formal definition).</p><p>Figure 3—figure supplement 3 shows that this model, when fit to rat behavior, predicts that rats will leave patches earlier with a longer pre-reward delay and shorter post-reward delay. However, according to Bayes Information Criterion (Figure 3—figure supplement 5), it does not fit as well as the quasi-hyperbolic discounting model in this experiment.</p><disp-quote content-type="editor-comment"><p>3) Another set of papers actually also provide a theoretical framework to explain intertemporal decision-making during foraging and delay-discounting. I will confess that I am an author of these papers. While I typically am loathe to suggesting my own papers during peer reviews, I think it is highly relevant in this case. The papers introducing this model are Namboodiri et al., 2014 A general theory of intertemporal decision-making and the perception of time, Frontiers in Beh. Neurosci., (primary research article) and Namboodiri et al., 2014 Rationalizing decision-making: understanding the cost and perception of time, Timing and Time Perception Reviews, (review). There are a few other papers investigating errors in decision-making using a trial-by-trial approach, but the mean field model presented in these papers is sufficient to highlight the relevance. In these papers, the main model that is presented for illustration purposes is almost a toy model, and was falsified by Carter and Redish, 2016. However, there are simple and more realistic extensions that were not considered by Carter and Redish and are in the Appendix of the original paper. I will present a (hopefully short) summary of its application to the foraging data presented here to show that at least all of the qualitative findings hold within a rate maximization perspective over a limited temporal horizon.</p><p>Decision-making in this TIMERR model is based on comparing the expected reward rate over a temporal horizon (T<sub>ime</sub>) and the expected delays to the next reward. Of course, even though the post-reward delay is after the delay to the next reward, it should be incorporated in the decision, if learned. If one assumes a potential learning deficit in these tasks due to the delay not being explicitly signaled, a simple model for their inclusion could be a weight (w) multiplied by the perceived delay (assumed to be the objective delay, t<sub>post</sub>, for simplicity). So effectively, the time horizon of consideration is T<sub>ime</sub>+t<sub>pre</sub>+w*t<sub>post</sub>. If the animal estimates an average reward rate (a<sub>est</sub>) to be obtained during the temporal horizon Time, the net reward expected over Time is a<sub>est</sub>*T<sub>ime</sub>. The toy model in the above papers assumes that a<sub>est</sub> is calculated over a time window exactly equal to T<sub>ime</sub>, but this need not be the case. This is the key modification that Carter and Redish, 2016, did not consider. Thus, the expected reward rate for the nth visit to a patch equals</p><p>RR(n) = (r-d*n+a<sub>est</sub>*T<sub>ime</sub>)/(t<sub>decision</sub>+t<sub>pre</sub>+w*t<sub>post</sub>+T<sub>ime</sub>)</p><p>Where r is the starting reward, d is the rate of reduction of the reward and t<sub>decision</sub> is the time until the decision is made after the cue. Since t<sub>harvest</sub>=t<sub>decision</sub>+t<sub>pre</sub>+t<sub>post</sub>, the above can be written as</p><p>RR(n) = (r-d*n+a<sub>est</sub>*T<sub>ime</sub>)/(t<sub>harvest</sub>-(1-w)*t<sub>post</sub>+T<sub>ime</sub>)</p><p>Similarly, the expected reward rate for the limited temporal horizon until the next reward after traveling to a new patch is</p><p>RR(travel) = (r+a<sub>est</sub>*T<sub>ime</sub>)/(t<sub>travel</sub>+t<sub>harvest</sub>-(1-w)*t<sub>post</sub>+T<sub>ime</sub>)</p><p>Thus, the number of visits one would stay in a patch would be the number at which the above two expected reward rates equal. Doing some algebra and noting that a<sub>est</sub> is actually dependent on the task itself (average estimated reward rate over some time horizon prior to the decision), hopefully, it will be clear that the findings of this manuscript at least qualitatively are consistent with this model. There are three free parameters here, viz., T<sub>ime</sub>, w, and the time horizon over which a<sub>est</sub> is calculated. In the interest of brevity of this review, I will not prolong this, but the main point is that this simple model of constrained rate maximization is at least qualitatively consistent with the key findings of this manuscript. Indeed, a more accurate model would require considering potential biases due to errors in time perception as treated in Namboodiri et al., Analytical calculation of errors in time and value perception due to a subjective time accumulator: A mechanistic model and the generation of Weber's law, Neural Computation, 2016.</p><p>I want to highlight that the point of my raising this is not to force a detailed consideration of my own work. I raise it only because the authors genuinely seem to care about these problems in decision-making and may perhaps find a different perspective on their data valuable.</p></disp-quote><p>We greatly appreciate the reviewer's thoughtful consideration of these models. We agree that the TIMERR model is very relevant to this manuscript and we have now included a brief discussion of the TIMERR model:</p><p>“Similarly, short-term maximization rules predict that animals seek to maximize reward over shorter time horizons; this may also be motivated as an approximation to long-term reward maximization as it may be difficult to accurately predict all future rewards (Stephens 2002, 2004). Along similar lines, Namboodiri et al., 2014, argue that, rather than maximizing long-term reward rate into the future, animals may select options that maximize reward rate up to the current point in time, or due to environmental factors (e.g. non-stationarity) or biological constraints (e.g. computational constraints), over a finite interval of time. Just as hyperbolic discounting may arise from imperfect foresight (Gabaix and Laibson, 2017), maximizing reward rate over shorter time horizons predicts hyperbolic time preferences (Namboodiri et al., 2014).”</p><p>Additionally, the suggested extension to the TIMERR model is very interesting, but we did not test this model directly for a few reasons. First, this model is difficult to translate into the semi-markov decision process used for all other models. Importantly, the semi-markov process enabled us to accurately model all timing features of the task, including slow delivery of larger rewards that occurred over up to 2.5 s and were consumed over the course of ~5 s. Second, as the TIMERR model predicts hyperbolic time preferences (Namboodiri et al., 2014), we view the additional flexibility of the extended TIMERR model similar to the additional flexibility offered by the constant sensitivity discounting function (a new model included in the manuscript, see comment 6 below) or the quasi-hyperbolic discounting function.</p><disp-quote content-type="editor-comment"><p>4) Related to points 2 and 3 above, the assumption is that animals have learned the structure of the task fully and can include all appropriate values objectively in their decision-making. This is an assumption that may be worth explicitly pointing out.</p></disp-quote><p>We agree that this assumption should be made explicit, and have now done so:</p><p>“These models consisted of a set of states that represented the time between each event in each of the tasks (e.g. cues turning on/off, lever press, reward delivery; for state space diagrams of both tasks, see Figure 4—figure supplement 1 and Figure 4—figure supplement 1). These models assumed that animals have learned the appropriate structure of the task (i.e. the time spent and reward obtained in each state) unless otherwise noted.”</p><disp-quote content-type="editor-comment"><p>5) Another descriptive discounting function that could potentially also fit the data is Killeen, An additive-utility model of delay discounting, Psychol Rev, 2009. This form is more of a descriptive model than a normative one, but has some math that is superficially similar to the TIMERR model, and would also likely be consistent with these data.</p></disp-quote><p>We agree that this is an interesting model given our data and thank the reviewer for bringing this model to our attention. This additive-utility discounting model cannot be expressed recursively within the framework of our semi-markov models, but we have tested two new models consisting of the main components of the additive-utility discounting model. The first is the constant sensitivity discounting model from Eberle and Prelec, 2007, which is discussed in Killeen, 2009. This model predicts hyperbolic time preferences via exponential discounting with a bias in time sensitivity (agents are less sensitive to longer delays). The second model combined the main components of the additive utility discounting function – discounted utility instead of value, and decreasing sensitivity to delays – by adding a nonlinear reward utility parameter to the constant sensitivity discounting model.</p><p>The constant sensitivity discounting performed well on the foraging task, but did not fit intertemporal choice task data well. The utility + constant sensitivity model performed similarly, but had a higher iBIC than the constant sensitivity discounting model alone (foraging: sum iBIC<sub>without-util</sub> = 25479, sum iBIC<sub>with-util</sub> = 25621; intertemporal choice: iBIC<sub>without-util</sub> = 18021, iBIC<sub>with-util</sub> = 18024). Because adding the utility parameter did not improve the fit of this model, we have opted to not include the utility version in the manuscript. The constant sensitivity model has been included (the “disc-cs” model). Please see the following figures (sum of iBIC in foraging tasks in Figure 3F, iBIC in intertemporal choice task from Figure 4B).</p><p>In addition, the non-linear post-reward delay model has been changed from a exponential decay function to a power function (the same power function used to model biased time perception in the constant sensitivity discounting model). This change was made for two reasons: for consistency with the constant-sensitivity discounting model and the power delay function had a lower iBIC (iBIC<sub>exp</sub> = 26595.25; iBIC<sub>pwr</sub> = 26229.35).</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The manuscript by Kane et al. is an exciting addition to the literature than has been trying to reconcile animal decision making in patch foraging vs. intertemporal choice problems. Using a surprisingly low number of animals, the authors robustly replicate a number of decision biases that have been shown in previous research, collectively described as &quot;overharvesting&quot;, related to handling time, pre/postdelay sensitivity and other choice attributes. Next, they proceed to examine several (competing) proposals for cognitive choice preferences that could explain some of these biases. The authors convincingly demonstrate that any model not assuming time preferences fails to adequate explain all choice biases displayed by the animals, and that of the models incorporating time preferences, β-δ quasi-hyperbolic discounting provides the best fit across the paradigms, and also in almost all of the single experiment cases. Importantly, the quasi hyperbolic models derived from patch foraging work well to describe behavior in the intertemporal choice setting and vice versa, suggesting that they might touch upon a basic choice heuristic that plays a significant role animal decision making.</p><p>The manuscript is well written and concise. I have no objections to the line of argument in general. One concern that I hope the authors can address concerns the following: in the Markov analyses, future rewards are taken into account and are assumed to include all rewards up to the last one in the current block, to induce the incorporation of the long-term reward rate into the decisions. It is however unclear why the authors chose to expand the horizon to the last choice. It seems to me that the &quot;reward horizon&quot; of animals might actually be limited or even fluctuate, for example based on satiety. It seems that with the current data at hand, it would be possible to model the reward horizon as a free parameter to approach this choice empirically. What horizon appears from this analysis?</p></disp-quote><p>To improve consistency between the horizon of the foraging and intertemporal choice models, the intertemporal choice model has been modified to include all future rewards, including those on future episodes (in the initial version of the manuscript, this model only considered rewards on the current episode). However, for temporal discounting models, the effective horizon is determined by the rate of discounting – at some point, the weight on future rewards approaches zero, and animals will not consider rewards beyond that time point. Whether reward horizon fluctuates due to satiety or other factors is an interesting question but we unfortunately could not test that directly in the current study.</p><disp-quote content-type="editor-comment"><p>And can the authors replicate the &quot;failure&quot; for cross-explanation between foraging and ITC when the horizon is set to only the current trial?</p></disp-quote><p>We have included a new section in the results comparing one-trial horizon discounting models with the future-rewards discounting models. We performed two comparisons: for all discounting functions, we found that future-rewards models provided a better fit (lower iBIC) and smaller estimates of discount factors relative to one-trial horizon models (see Figure 4—figure supplement 2). However, as asked by the reviewer, we could not replicate the “failure” for cross-explanation between the foraging and ITC tasks due to a shorter horizon. For the quasi-hyperbolic discounting function, we found that the future-rewards model parameters fit to intertemporal choice task did not predict foraging behavior better than one-trial horizon parameters fit to the intertemporal choice task:</p><p>“With temporal discounting models that consider all future rewards, the more flexible quasi-hyperbolic discounting function provided the best fit to behavior across tasks. […] Overall, using full horizon temporal discounting models explained more of the intertemporal choice data, produced smaller estimates of discounting factors, but in the present study, it did not improve the ability of a model fit to intertemporal choice data to predict foraging behavior.”</p><p>Also, in the Discussion:</p><p>“… we found that adding the value of future rewards to intertemporal choice models reduces estimates of discount factors. […] In these cases, obtaining smaller, potentially more accurate estimates of discount factors by including all future rewards into intertemporal choice models may improve cross-task predictions.”</p><disp-quote content-type="editor-comment"><p>Additionally, while the quasi-hyperbolic discount model provides an overall better fit than for example the diminishing marginal returns model, it seems that animals exhibit both time preferences AND experience sensitivity to diminishing marginal returns, based on reward size differences and (the very real consequences of) already accumulated rewards. If accumulated marginal returns are not important, one would assume that a median split (early vs. late) on the trials in the foraging dataset, or the first 5 vs. last 5 trials in the ITC dataset would produce similar model estimates. However, if the model fits diverge substantially this would suggest that an extension of the model with quasi-hyperbolic time-preferences to include marginal reward size/accumulation effects might provide an even better fit. I think this question could also be answered with the current dataset that is unique equipped to compare models under different experimental circumstances.</p></disp-quote><p>In both models, values are calculated on future projections alone; prior accumulated rewards will not have an effect on model predictions. To account for the magnitude of future expected rewards, we have modified both the foraging and intertemporal choice models such that noise in decisions scales with the value estimates. In their current form, the greater the projected future rewards, the more noise in decisions. Consequently, these models can predict that rats will be more likely to select the LL option in the intertemporal choice task as they approach the end of an episode (assuming rewards from future games are highly discounted).</p><p>To directly test whether behavior changed between early/late periods in the foraging task or between the first 5/last 5 choices of each episode in the intertemporal choice task, we added a parameter to each of the mixed effects models. Neither the early/late parameter in the foraging mixed effects model nor the first 5/ last 5 parameter in the intertemporal choice mixed effects model had a significant impact (foraging early/late: β = .013, SE = .192, t(7.054) = .069, p = .947; intertemporal choice: β = .018, SE = .042, z = .439, p = .661). We also tested a nonlinear reward utility + quasi-hyperbolic discounting model. Adding the utility parameter did not improve the fit of the quasi-hyperbolic model to either foraging or intertemporal choice data (foraging: sum of iBIC<sub>quasi</sub> = 25383.31, sum of iBIC<sub>util+quasi</sub> = 25408.36; intertemporal choice: iBIC<sub>quasi</sub> = 17981.98, iBIC<sub>util+quasi</sub> = 17983.86). These analyses suggest that there were no effects of reward accumulation over the course of a foraging session or intertemporal choice episode.</p></body></sub-article></article>