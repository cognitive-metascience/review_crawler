<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">27430</article-id><article-id pub-id-type="doi">10.7554/eLife.27430</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A causal role for right frontopolar cortex in directed, but not random, exploration</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-62331"><name><surname>Zajkowski</surname><given-names>Wojciech K</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-62782"><name><surname>Kossut</surname><given-names>Malgorzata</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-62025"><name><surname>Wilson</surname><given-names>Robert C</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2963-2971</contrib-id><email>bob@arizona.edu</email><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>University of Social Sciences and Humanities</institution><addr-line><named-content content-type="city">Warsaw</named-content></addr-line><country>Poland</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Psychology</institution><institution>University of Social Sciences and Humanities</institution><addr-line><named-content content-type="city">Warsaw</named-content></addr-line><country>Poland</country></aff><aff id="aff3"><label>3</label><institution>Nencki Institute of Experimental Biology</institution><addr-line><named-content content-type="city">Warsaw</named-content></addr-line><country>Poland</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Psychology</institution><institution>University of Arizona</institution><addr-line><named-content content-type="city">Tucson</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Cognitive Science Program</institution><institution>University of Arizona</institution><addr-line><named-content content-type="city">Tucson</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-24218"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Reviewing Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>15</day><month>09</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e27430</elocation-id><history><date date-type="received" iso-8601-date="2017-04-04"><day>04</day><month>04</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2017-09-14"><day>14</day><month>09</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Zajkowski et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Zajkowski et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-27430-v2.pdf"/><related-object ext-link-type="url" xlink:href="https://elifesciences.org/articles/e27430v1"><date date-type="v1" iso-8601-date="2017-09-15"><day>15</day><month>09</month><year>2017</year></date></related-object><abstract><object-id pub-id-type="doi">10.7554/eLife.27430.001</object-id><p>The explore-exploit dilemma occurs anytime we must choose between exploring unknown options for information and exploiting known resources for reward. Previous work suggests that people use two different strategies to solve the explore-exploit dilemma: directed exploration, driven by information seeking, and random exploration, driven by decision noise. Here, we show that these two strategies rely on different neural systems. Using transcranial magnetic stimulation to inhibit the right frontopolar cortex, we were able to selectively inhibit directed exploration while leaving random exploration intact. This suggests a causal role for right frontopolar cortex in directed, but not random, exploration and that directed and random exploration rely on (at least partially) dissociable neural systems.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>explore-exploit</kwd><kwd>frontal pole</kwd><kwd>decision making</kwd><kwd>reinforcement learning</kwd><kwd>directed exploration</kwd><kwd>random exploration</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group> <funding-statement>No external funding was received for this work.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Disruption of right frontopolar cortex with transcranial magnetic stimulation causes selective deficits in exploratory behavior suggesting that different strategies of exploration are implemented by different neural circuits.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In an uncertain world adaptive behavior requires us to carefully balance the exploration of new opportunities with the exploitation of known resources. Finding the optimal balance between exploration and exploitation is a hard computational problem and there is considerable interest in understanding how humans and animals strike this balance in practice (<xref ref-type="bibr" rid="bib3">Badre et al., 2012</xref>; <xref ref-type="bibr" rid="bib6">Cavanagh et al., 2011</xref>; <xref ref-type="bibr" rid="bib7">Cohen et al., 2007</xref>; <xref ref-type="bibr" rid="bib10">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="bib13">Frank et al., 2009</xref>; <xref ref-type="bibr" rid="bib18">Hills et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Mehlhorn et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Wilson et al., 2014</xref>). Recent work has suggested that humans use two distinct strategies to solve the explore-exploit dilemma: directed exploration, based on information seeking, and random exploration, based on decision noise (<xref ref-type="bibr" rid="bib38">Wilson et al., 2014</xref>). Even though both of these strategies serve the same purpose, that is, balancing exploration and exploitation, it is likely they rely on different cognitive mechanisms. Directed exploration is driven by information and is thought to be computationally complex (<xref ref-type="bibr" rid="bib14">Gittins and Jones, 1979</xref>; <xref ref-type="bibr" rid="bib2">Auer et al., 2002</xref>; <xref ref-type="bibr" rid="bib15">Gittins, 1974</xref>). On the other hand, random exploration can be implemented in a simpler fashion by using neural or environmental noise to randomize choice (<xref ref-type="bibr" rid="bib36">Thompson, 1933</xref>).</p><p>A key question is whether these dissociable behavioral strategies rely on dissociable neural systems. Of particular interest is the frontopolar cortex (FPC) – an area that has been associated with a number of functions, such as tracking pending and/or alternate options (<xref ref-type="bibr" rid="bib22">Koechlin and Hyafil, 2007</xref>; <xref ref-type="bibr" rid="bib5">Boorman et al., 2009</xref>), strategies (<xref ref-type="bibr" rid="bib11">Domenech and Koechlin, 2015</xref>) and goals (<xref ref-type="bibr" rid="bib30">Pollmann, 2016</xref>) and that has been implicated in exploration itself (<xref ref-type="bibr" rid="bib3">Badre et al., 2012</xref>; <xref ref-type="bibr" rid="bib6">Cavanagh et al., 2011</xref>; <xref ref-type="bibr" rid="bib10">Daw et al., 2006</xref>). Importantly, however, the exact role that FPC plays in exploration is unknown as how exploration is defined varies from paper to paper. In one line of work, exploration is defined as information seeking. Understood this way, exploration correlates with RFPC activity measured via fMRI (<xref ref-type="bibr" rid="bib3">Badre et al., 2012</xref>) and a frontal theta component in EEG (<xref ref-type="bibr" rid="bib6">Cavanagh et al., 2011</xref>), suggesting a role for RFPC in directed exploration. However, in another line of work, exploration is operationalized differently, as choosing the low value option, not the most informative. Such a measure of exploration is more consistent with random exploration where decision noise drives the sampling of low value options by chance. Defined in this way, exploratory choice correlates with lateral FPC activation (<xref ref-type="bibr" rid="bib10">Daw et al., 2006</xref>) and stimulation and inhibition of RFPC with direct current (tDCS) can increase and decrease the frequency with which such exploratory choices occur (<xref ref-type="bibr" rid="bib31">Raja Beharelle et al., 2015</xref>).</p><p>Taken together, these two sets of findings suggest that RFPC plays a crucial role in both directed and random exploration. However, we believe that such a conclusion is premature because of a subtle confound that arises between reward and information in most explore-exploit tasks. This confound arises because participants only gain information from the options they choose, yet are incentivized to choose more rewarding options. Thus, over many trials, participants gain more information about more rewarding options and the two ways of defining exploration, that is, choosing high information or low reward options, become confounded (<xref ref-type="bibr" rid="bib38">Wilson et al., 2014</xref>). This makes it impossible to tell whether the link between RFPC and exploration is specific to either directed or random exploration, or whether it is general to both.</p><p>To distinguish these interpretations and investigate the causal role of FPC in directed and random exploration, we used continuous theta-burst TMS (<xref ref-type="bibr" rid="bib19">Huang et al., 2005</xref>) to selectively inhibit right frontopolar cortex (RFPC) in participants performing the ‘Horizon Task’, an explore-exploit task specifically designed to separate directed and random exploration (<xref ref-type="bibr" rid="bib38">Wilson et al., 2014</xref>). Using this task we find evidence that inhibition of RFPC selectively inhibits directed exploration while leaving random exploration intact.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We used our previously published ‘Horizon Task’ (<xref ref-type="fig" rid="fig1">Figure 1</xref>) to measure the effects of TMS stimulation of RFPC on directed and random exploration. In this task, participants play a set of games in which they make choices between two slot machines (one-armed bandits) that pay out rewards from different Gaussian distributions. To maximize their rewards in each game, participants need to exploit the slot machine with the highest mean, but they cannot identify this best option without exploring both options first.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.27430.002</object-id><label>Figure 1.</label><caption><title>The horizon task.</title><p>Participants make a series of decisions between two one-armed bandits that pay out probabilistic rewards with unknown means. At the start of each game, ‘forced-choice’ trials give participants partial information about the mean of each option. We use the forced-choice trials to set up one of two information conditions: (<bold>A</bold>) an unequal (or [1 3]) condition in which participants see 1 play from one option and 3 plays from the other and (<bold>B</bold>) an equal (or [2 2]) condition in which participants see 2 plays from both options. A model-free measure of directed exploration is then defined as the change in information seeking with horizon in the unequal condition (<bold>A</bold>). Likewise a model-free measure of random exploration is defined as the change choosing the low mean option in the equal condition (<bold>B</bold>).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27430-fig1-v2"/></fig><p>The Horizon Task has two key manipulations that allow us to measure directed and random exploration. The first manipulation is the horizon itself, i.e. the number of decisions remaining in each game. The idea behind this manipulation is that when the horizon is long (6 trials), participants should explore more frequently, because any information they acquire from exploring can be used to make better choices later on. In contrast, when the horizon is short (1 trial), participants should exploit the option they believe to be best. Thus, this task allows us to quantify directed and random exploration as changes in information seeking and behavioral variability that occur with horizon.</p><p>The second manipulation is the amount of information participants have about each option <italic>before</italic> making their first choice. This information manipulation is achieved by using four forced-choice trials, in which participants are told which option to pick, at the start of each game. We use these forced-choice trials to setup one of two information conditions: an unequal, or (<xref ref-type="bibr" rid="bib1">Aston-Jones and Cohen, 2005</xref>; <xref ref-type="bibr" rid="bib3">Badre et al., 2012</xref>), condition, in which participants see 1 play from one option and 3 plays from the other option, and an unequal, or (<xref ref-type="bibr" rid="bib2">Auer et al., 2002</xref>; <xref ref-type="bibr" rid="bib2">Auer et al., 2002</xref>), condition, in which participants see two outcomes from both options. By varying the amount of information participants have about each option <italic>independent</italic> of the mean payout of that option, this information manipulation allows us to remove the reward-information confound, at least on the first free-choice trial (<xref ref-type="fig" rid="fig2">Figure 2</xref>). After the first free-choice trial, however, participants tend to choose more rewarding options more frequently and reward and information are rapidly confounded. For this reason the bulk of our analyses are focussed on the first free-choice trial where the confound has been removed.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.27430.003</object-id><label>Figure 2.</label><caption><title>The reward-information confound.</title><p>The y-axis corresponds to the correlation between the sign of the difference in mean (<inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>) between options and the sign of difference in the number of times each option has been played (<inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>). The forced trials are chosen such that the the correlation is approximately zero on the first free-choice trial. After the first trial, however, a positive correlation quickly emerges as participants choose the more rewarding options more frequently. This strong confound between reward and information makes it difficult to dissociate directed and random exploration on later trials.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27430-fig2-v2"/></fig><sec id="s2-1"><title>RFPC stimulation selectively inhibits directed exploration on the first free-choice</title><p>In this section we analyze behavior on the first free-choice trial in each game. This way we are able to remove any effect of the reward-information confound and fairly compare behavior between horizon conditions. We analyze the data with both a model-free approach, using simple statistics of the data to quantify directed and random exploration, as well as a model-based approach, using a cognitive model of the behavior to draw more precise conclusions. Both analyses point to the same conclusion that RFPC stimulation selectively inhibits directed, but not random, exploration.</p><sec id="s2-1-1"><title>Model-free analysis</title><p>The two information conditions in the Horizon Task allow us to quantify directed and random exploration in a model-free way. In particular, directed exploration, which involves information seeking, can be quantified as the probability of choosing the high information option, <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> in the [1 3] condition, while random exploration, which involves decision noise, can be quantified as the probability of making a mistake, or choosing the low mean reward option, <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, in the [2 2] condition.</p><p>Using these measures of exploration, we found that inhibiting the RFPC had a significant effect on directed exploration but not random exploration (<xref ref-type="fig" rid="fig3">Figure 3A,B</xref>). In particular, for directed exploration, a repeated measures ANOVA with horizon, TMS condition and order as factors revealed a significant interaction between stimulation condition and horizon (<inline-formula><mml:math id="inf5"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>24</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>4.96</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.036</mml:mn></mml:mrow></mml:math></inline-formula>). Conversely, a similar analysis for random exploration revealed no effects of stimulation condition (main effect of stimulation condition, <inline-formula><mml:math id="inf7"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>24</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.88</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.36</mml:mn></mml:mrow></mml:math></inline-formula>; interaction of stimulation condition with horizon, <inline-formula><mml:math id="inf9"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>24</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.24</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.28</mml:mn></mml:mrow></mml:math></inline-formula>). Post hoc analyses revealed that the change in directed exploration was driven by changes in information seeking in horizon 6 (one-sided t-test, <inline-formula><mml:math id="inf11"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>24</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>2.62</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.008</mml:mn></mml:mrow></mml:math></inline-formula>) and not in horizon 1 (two-sided t-test, <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>24</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.30</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.27430.004</object-id><label>Figure 3.</label><caption><title>Model-free analysis of the first free-choice trial shows that RPFC stimulation affects directed, but not random, exploration.</title><p>(<bold>A</bold>) In the control (vertex) condition, information seeking increases with horizon, consistent with directed exploration. When RFPC is stimulated, directed exploration is reduced, an effect that is entirely driven by changes in horizon 6 (* denotes <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi mathsize="111%">p</mml:mi><mml:mo mathsize="111%" stretchy="false">&lt;</mml:mo><mml:mn mathsize="111%">0.02</mml:mn></mml:mrow></mml:math></inline-formula> and ** denotes <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi mathsize="111%">p</mml:mi><mml:mo mathsize="111%" stretchy="false">&lt;</mml:mo><mml:mn mathsize="111%">0.005</mml:mn></mml:mrow></mml:math></inline-formula>; error bars are <inline-formula><mml:math id="inf16"><mml:mo mathsize="111%" stretchy="false">±</mml:mo></mml:math></inline-formula> s.e.m.). (<bold>B</bold>) Random exploration increases with horizon but is not affected by RFPC stimulation.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27430-fig3-v2"/></fig></sec><sec id="s2-1-2"><title>Model-based analysis</title><p>While the model-free analyses are intuitive, the model-free statistics, <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, are not pure reflections of information seeking and behavioral variability and could be influenced by other factors such as spatial bias and learning. To account for these possibilities we performed a model-based analysis using a model that extends our earlier work (<xref ref-type="bibr" rid="bib38">Wilson et al., 2014</xref>; <xref ref-type="bibr" rid="bib35">Somerville et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Krueger et al., 2017</xref>) see Materials and methods for a complete description. In this model, the level of directed and random exploration is captured by two parameters: an information bonus for directed exploration, and decision noise for random exploration. In addition the model includes terms for the spatial bias and to describe learning.</p><sec id="s2-1-2-1"><title>Overview of model</title><p>Before presenting the results of the model-based analysis we begin with a brief overview of the most salient points of the model. A full description of the model can be found in the Methods and code to implement the model can be found in the Supplementary Material.</p><p>Conceptually, the model breaks the explore-exploit choice down into two components: a learning component, in which participants estimate the mean payoff of each option from the rewards they see, and a decision component, in which participants use this estimated payoff to guide their choice. The learning component assumes that participants compute an estimate of the average payoff for each slot machine, <inline-formula><mml:math id="inf19"><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula>, using a simple delta rule update equation (based on a Kalman filter (<xref ref-type="bibr" rid="bib20">Kalman, 1960</xref>), see Materials and methods):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is the reward on trial <inline-formula><mml:math id="inf21"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf22"><mml:msubsup><mml:mi>α</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula> is the time-varying learning rate that determines the extent to which the prediction error, <inline-formula><mml:math id="inf23"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, updates the estimate of the mean of bandit <inline-formula><mml:math id="inf24"><mml:mi>i</mml:mi></mml:math></inline-formula>. The learning process is described by three free parameters: the initial value of the estimated payoff, <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, and two learning rates, the initial learning rate, <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, and the asymptotic learning rate, <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>α</mml:mi><mml:mo>inf</mml:mo></mml:msub></mml:math></inline-formula>, which together describe the evolution of the actual learning rate, <inline-formula><mml:math id="inf28"><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, over time. For simplicity, we assume that these parameters are independent of horizon and uncertainty condition (<xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.27430.005</object-id><label>Table 1.</label><caption><title>Model parameters.</title><p>Subject’s behavior on the first free choice of each session is described by 13 free parameters. Three of these parameters (<inline-formula><mml:math id="inf29"><mml:msub><mml:mi mathsize="111%">R</mml:mi><mml:mn mathsize="111%">0</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf30"><mml:msub><mml:mi mathsize="111%">α</mml:mi><mml:mn mathsize="111%">1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf31"><mml:msub><mml:mi mathsize="111%">α</mml:mi><mml:mi mathsize="111%" mathvariant="normal">∞</mml:mi></mml:msub></mml:math></inline-formula>) describe the learning process and do not vary with horizon or uncertainty condition. Ten of these parameters (<inline-formula><mml:math id="inf32"><mml:mi mathsize="111%">A</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf33"><mml:mi mathsize="111%">B</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf34"><mml:mi mathsize="111%">σ</mml:mi></mml:math></inline-formula> in the different horizon and information conditions) describe the decision process. All parameters are estimated for each subject in each stimulation condition and the key analysis asks whether parameters change between vertex and RFPC stimulation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Parameter</th><th>Horizon dependent?</th><th>Uncertainty dependent?</th><th>TMS dependent?</th></tr></thead><tbody><tr><td>prior mean, <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula></td><td>no</td><td>no</td><td>yes</td></tr><tr><td>initial learning rate, <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula></td><td>no</td><td>no</td><td>yes</td></tr><tr><td>asymptotic learning rate, <inline-formula><mml:math id="inf37"><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub></mml:math></inline-formula></td><td>no</td><td>no</td><td>yes</td></tr><tr><td>information bonus, <inline-formula><mml:math id="inf38"><mml:mi>A</mml:mi></mml:math></inline-formula></td><td>yes</td><td>n/a</td><td>yes</td></tr><tr><td>spatial bias, <inline-formula><mml:math id="inf39"><mml:mi>B</mml:mi></mml:math></inline-formula></td><td>yes</td><td>yes</td><td>yes</td></tr><tr><td>decision noise, <inline-formula><mml:math id="inf40"><mml:mi>σ</mml:mi></mml:math></inline-formula></td><td>yes</td><td>yes</td><td>yes</td></tr></tbody></table></table-wrap><p>The decision component of the model assumes that participants choose between the two options (left and right) probabilistically according to.<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> ( <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> ) is the difference in expected reward between left and right options and <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> is the difference in information between left and right options (which we define as +1 when left is more informative, −1 when right is more informative, and 0 when both options convey equal information in the [2 2] condition). The decision process is described by three free parameters: the information bonus <inline-formula><mml:math id="inf44"><mml:mi>A</mml:mi></mml:math></inline-formula>, the spatial bias <inline-formula><mml:math id="inf45"><mml:mi>B</mml:mi></mml:math></inline-formula>, and the decision noise <inline-formula><mml:math id="inf46"><mml:mi>σ</mml:mi></mml:math></inline-formula>. We estimate separate values of the decision parameters for each horizon and (since the information bonus is only used in the [1 3] condition) separate values of only the bias and decision noise for each uncertainty condition.</p><p>Overall, subject’s behavior in each session (vertex vs RFPC stimulation) is described by 13 free parameters (<xref ref-type="table" rid="table1">Table 1</xref>): three describing learning (<inline-formula><mml:math id="inf47"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf48"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub></mml:math></inline-formula>) and 10 describing the decision process (<inline-formula><mml:math id="inf50"><mml:mi>A</mml:mi></mml:math></inline-formula> in the two horizon conditions, <inline-formula><mml:math id="inf51"><mml:mi>B</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf52"><mml:mi>σ</mml:mi></mml:math></inline-formula> in the four horizon-x-uncertainty conditions). These 13 parameters were fit to each subject in each stimulation condition using a hierarchical Bayesian approach (<xref ref-type="bibr" rid="bib24">Lee and Wagenmakers, 2014</xref>) (see Materials and methods).</p></sec><sec id="s2-1-2-2"><title>Model fitting results</title><p>Posterior distributions over the group-level means are shown in the left column of <xref ref-type="fig" rid="fig4">Figure 4</xref>, while posteriors over the TMS-related change in parameters are shown in the right column. Both columns suggest a selective effect of RFPC stimulation on the information bonus in horizon 6.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.27430.006</object-id><label>Figure 4.</label><caption><title>Model-based analysis of the first free-choice trial showing the effect of RFPC stimulation on each of the 13 parameters.</title><p>Left column: Posterior distributions over each parameter value for RFPC and vertex stimulation condition. Right column: posterior distributions over the change in each parameter between stimulation conditions. Note that, because information bonus, decision noise and spatial bias are all in units of points, we plot them on the same scale to facilitate comparison of effect size.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27430-fig4-v2"/></fig><p>Focussing on the left column first, overall the parameter values seem reasonable. The prior mean is close to the generative mean of 50 used in the actual experiment, and the decision parameters are comparable to those found in our previous work (<xref ref-type="bibr" rid="bib38">Wilson et al., 2014</xref>). The learning rate parameters, <inline-formula><mml:math id="inf53"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf54"><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub></mml:math></inline-formula>, were not included in our previous models and are worth discussing in more detail. As expected for Bayesian learning (<xref ref-type="bibr" rid="bib20">Kalman, 1960</xref>; <xref ref-type="bibr" rid="bib27">Nassar et al., 2010</xref>), the initial learning rate is higher than the asymptotic learning rate (95% of samples in the vertex condition, 94% in the RFPC condition). However, the actual values of the learning rates are quite far from their ‘optimal’ settings of <inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> that would correspond to perfectly computing the mean reward. This suggests a greater than optimal reliance on the prior (<inline-formula><mml:math id="inf57"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) and a pronounced recency bias (<inline-formula><mml:math id="inf58"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) such that the most recent rewards are weighted more heavily in the computation of expected reward, <inline-formula><mml:math id="inf59"><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula>. Both of these findings are likely due to the fact that the version of the task we employed did not keep the outcomes of the forced trials on screen and instead relied on people’s memories to compute the expected value.</p><p>Turning to the right hand column of <xref ref-type="fig" rid="fig4">Figure 4</xref>, we can see that the model-based analysis yields similar result to the model-free analysis. In particular we see a reduction (of about 4.8 points) in the information bonus in horizon 6 (with 99% of samples showing a reduced information bonus in the RFPC stimulation condition) and no effect on decision noise in either horizon in either the [2 2] or [1 3] uncertainty conditions (with between 40% and 63% of samples below zero).</p><p>In addition to the effect on the information bonus in horizon 6, there is also a hint of an effect on the information bonus in horizon 1 (85% samples less than zero) and on the prior mean <inline-formula><mml:math id="inf60"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> (88% samples above zero). While these results may suggest that RFPC stimulation affects more than just information bonus in horizon 6, they more likely reflect an inherent tradeoff between prior mean and information bonus that is peculiar to this task. In particular, because the prior mean has a stronger effect on the more uncertain option, an increase in <inline-formula><mml:math id="inf61"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> increases the value of the more informative option in much the same way as an information bonus. Thus, when applied to this task, the model has a built in tradeoff between prior mean and information bonus that can muddy the interpretation of both. Note that this tradeoff is not a general feature of the model and could be removed with a different task design that employed more forced choice trials and hence more time for the effects of the prior to be removed.</p><p><xref ref-type="fig" rid="fig5">Figure 5</xref> exposes the tradeoff between <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:mi>A</mml:mi></mml:math></inline-formula> in more detail. Panels A and B plot samples from the posterior over the TMS-related change in information bonus, <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, against the TMS-related change in prior mean, <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. For both horizon conditions we see a strong negative correlation such that increasing <inline-formula><mml:math id="inf66"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> decreases <inline-formula><mml:math id="inf67"><mml:mi>A</mml:mi></mml:math></inline-formula>. This negative correlation is especially problematic for the interpretation of the horizon 1 change in information bonus where a sizable fraction of the posterior centers on no change in either variable. In contrast the negative correlation between <inline-formula><mml:math id="inf68"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> does not affect our interpretation of the horizon 6 result where the TMS-related change in <inline-formula><mml:math id="inf70"><mml:mi>A</mml:mi></mml:math></inline-formula> is negative regardless of of the change in <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.27430.007</object-id><label>Figure 5.</label><caption><title>Correlation between TMS-induced changes in information bonus, <inline-formula><mml:math id="inf72"><mml:mi mathsize="111%">A</mml:mi></mml:math></inline-formula>, and TMS-induced changes in the prior mean, <inline-formula><mml:math id="inf73"><mml:msub><mml:mi mathsize="111%">R</mml:mi><mml:mn mathsize="111%">0</mml:mn></mml:msub></mml:math></inline-formula>.</title><p>(<bold>A, B</bold>) Samples from the posterior distributions over the TMS-related changes in prior mean, <inline-formula><mml:math id="inf74"><mml:msub><mml:mi mathsize="111%">R</mml:mi><mml:mn mathsize="111%">0</mml:mn></mml:msub></mml:math></inline-formula>, and TMS-related change in information bonus in horizon 1 (<bold>A</bold>) and horizon 6 (<bold>B</bold>). In both cases we see a negative correlation between the change in <inline-formula><mml:math id="inf75"><mml:msub><mml:mi mathsize="111%">R</mml:mi><mml:mn mathsize="111%">0</mml:mn></mml:msub></mml:math></inline-formula> and the change in <inline-formula><mml:math id="inf76"><mml:mi mathsize="111%">A</mml:mi></mml:math></inline-formula> consistent with a tradeoff between these variables in the model. (<bold>C</bold>) Samples from the posterior over the effect of TMS stimulation on the horizon-related change in information bonus, <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi mathsize="111%" mathvariant="normal">Δ</mml:mi><mml:mi mathsize="111%">A</mml:mi><mml:mo mathsize="111%" stretchy="false">=</mml:mo><mml:mi mathsize="111%">A</mml:mi><mml:mrow><mml:mo maxsize="111%" minsize="111%">(</mml:mo><mml:mi mathsize="111%">h</mml:mi><mml:mo mathsize="111%" stretchy="false">=</mml:mo><mml:mn mathsize="111%">6</mml:mn><mml:mo maxsize="111%" minsize="111%">)</mml:mo></mml:mrow><mml:mo mathsize="111%" stretchy="false">-</mml:mo><mml:mi mathsize="111%">A</mml:mi><mml:mrow><mml:mo maxsize="111%" minsize="111%">(</mml:mo><mml:mi mathsize="111%">h</mml:mi><mml:mo mathsize="111%" stretchy="false">=</mml:mo><mml:mn mathsize="111%">1</mml:mn><mml:mo maxsize="111%" minsize="111%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> plotted against samples from the TMS-related change in prior mean. Here we see no correlation between variables and the majority of <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> samples below zero consistent with an effect of RFPC stimulation on directed exploration.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27430-fig5-v2"/></fig><p>Finally we asked whether the horizon-dependent change in information seeking, i.e. <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, was different in each TMS condition. As shown in <xref ref-type="fig" rid="fig5">Figure 5C</xref>, the TMS-related change in <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> is about −3.1 points (94% samples below 0) and is uncorrelated with the TMS-related change in <inline-formula><mml:math id="inf81"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. Taken together, this suggests that we can be fairly confident in our claim that RFPC stimulation has a selective effect on directed exploration.</p></sec></sec></sec><sec id="s2-2"><title>The effect of RFPC stimulation on later trials</title><p>Our analyses so far have focussed on just the first free choice and have ignored the remaining five choices in the horizon six games. The reason for this is the reward-information confound, illustrated in <xref ref-type="fig" rid="fig2">Figure 2</xref>, which makes interpretation of the later trials more difficult. Despite this difficulty, we note that in <xref ref-type="fig" rid="fig2">Figure 2</xref> the size of the confound is almost identical in the two stimulation conditions and so we proceed, with caution, to present a model-free analysis of the later trials below.</p><p>In <xref ref-type="fig" rid="fig6">Figure 6</xref> we plot the model-free measures, <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, as a function of trial number. Both measures show a decrease over the course of the horizon six games although, because of the confound, it is difficult to say whether these changes reflect a reduction in directed exploration, random exploration, or both. Interestingly, the differences in <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> between vertex and RFPC conditions on the first free-choice trial appear to persist into the second, a result that becomes more apparent when we plot the TMS-related change, that is, <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6C,D</xref>). More formally a repeated measures ANOVA with trial number, TMS condition as factors reveals a significant main effect of trial number (<inline-formula><mml:math id="inf86"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>120</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>126</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>), no main effect of TMS condition (<inline-formula><mml:math id="inf88"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>120</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.17</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.29</mml:mn></mml:mrow></mml:math></inline-formula>) and a near significant interaction between trial number and TMS condition (<inline-formula><mml:math id="inf90"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>120</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>2.26</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.053</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). A post hoc, one-sided t-test on the second trial reveals a marginally significant reduction in <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> on the second trial (<inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>24</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1.61</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). In contrast, a similar analysis for random exploration shows no evidence for any effect of TMS condition on <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (main effect of TMS, <inline-formula><mml:math id="inf94"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>120</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.16</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.69</mml:mn></mml:mrow></mml:math></inline-formula>; TMS x trial number, <inline-formula><mml:math id="inf96"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>120</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.69</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.63</mml:mn></mml:mrow></mml:math></inline-formula>) although the main effect of trial number persists (<inline-formula><mml:math id="inf98"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>120</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>13.7</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>). Thus, the analysis of later trials provides additional, albeit modest, support for the idea that RFPC stimulation selectively disrupts directed but not random exploration at long horizons.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.27430.008</object-id><label>Figure 6.</label><caption><title>Model-free analysis of all trials.</title><p>(<bold>A, B</bold>) Model-free measures of directed (<bold>A</bold>) and random (<bold>B</bold>) exploration as a function of trial number suggests a reduction in both directed and random exploration over the course of the game. (<bold>C, D</bold>) TMS-induced change in measures of directed and random exploration as a function of trial number. This suggests that the reduction in directed exploration on the first free-choice trial, persists into the second trial of the game.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27430-fig6-v2"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work we used continuous theta-burst transcranial magnetic stimulation (cTBS) to investigate whether right frontopolar cortex (RFPC) is causally involved in directed and random exploration. Using a task that is able to behaviorally dissociate these two types of exploration, we found that inhibition of RFPC caused a selective reduction in directed, but not random exploration. To the best of our knowledge, this finding represents the first causal evidence that directed and random exploration rely on dissociable neural systems and is consistent with our recent findings showing that directed and random exploration have different developmental profiles (<xref ref-type="bibr" rid="bib35">Somerville et al., 2017</xref>). This suggests that, contrary to the assumption underlying many contemporary studies (<xref ref-type="bibr" rid="bib10">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="bib3">Badre et al., 2012</xref>), exploration is not a unitary process, but a dual process in which the distinct strategies of information seeking and choice randomization are implemented via distinct neural systems.</p><p>Such a dual-process view of exploration is consistent with the classical idea that there are multiple types of exploration (<xref ref-type="bibr" rid="bib4">Berlyne, 1966</xref>). In particular Berlyne’s constructs of ‘specific exploration’, involving a drive for information and ‘diversive exploration’, involving a drive for variety, bear a striking resemblance to our definitions of directed and random exploration. Despite the importance of Berlyne’s work, more modern views of exploration tend not to make the distinction between different types of exploration, considering instead a single exploratory state or exploratory drive that controls information seeking across a wide range of tasks (<xref ref-type="bibr" rid="bib4">Berlyne, 1966</xref>; <xref ref-type="bibr" rid="bib1">Aston-Jones and Cohen, 2005</xref>; <xref ref-type="bibr" rid="bib18">Hills et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Kidd and Hayden, 2015</xref>). At face value, such unitary accounts seem at odds with a dual-process view of exploration. However, these two viewpoints can be reconciled if we allow for the possibility that, while directed and random exploration are implemented by different systems, their levels are set by a common exploratory drive.</p><p>Intriguingly, individual differences in behavior on the Horizon Task provide some support for the idea that directed and random exploration are driven by a common source. In particular, in a large behavioral data set of 277 people performing the Horizon Task, we find a positive correlation between the levels of directed and random exploration such that people with high levels of directed exploration also tend to have high levels random exploration (<inline-formula><mml:math id="inf100"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>275</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.29</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf101"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>), <xref ref-type="fig" rid="fig7">Figure 7</xref>. This is consistent with the idea that the levels of directed and random exploration are set by the strength of an exploratory drive that varies as an individual difference between people.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.27430.009</object-id><label>Figure 7.</label><caption><title>Correlation between individual differences in the levels of directed and random exploration in a sample of 277 people performing the Horizon Task.</title></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27430-fig7-v2"/></fig><p>While the present study does allow us to conclude that directed and random exploration rely on different neural systems, the limited spatial specificity of TMS limits our ability to say exactly what those systems are. In particular, because the spatial extent of TMS is quite large, stimulation aimed at frontal pole may directly affect activity in nearby areas such as ventromedial prefrontal cortex (vmPFC) and orbitofrontal cortex (OFC), both areas that have been implicated in exploratory decision making and that may be contributing to our effect (<xref ref-type="bibr" rid="bib10">Daw et al., 2006</xref>). In addition to such direct effects of TMS on nearby regions, indirect changes in areas that are connected to the frontal pole could also be driving our effect. For example, cTBS of left frontal pole has been associated with changes in blood perfusion in areas such as amygdala, fusiform gyrus and posterior parietal cortex (<xref ref-type="bibr" rid="bib37">Volman et al., 2011</xref>) and by changes in the fMRI BOLD signal in OFC, insula and striatum (<xref ref-type="bibr" rid="bib17">Hanlon et al., 2017</xref>). In addition (<xref ref-type="bibr" rid="bib37">Volman et al., 2011</xref>) showed that unilateral cTBS of left frontal pole is associated with changes in blood perfusion to the right frontal pole. Indeed, such a bilateral effect of cTBS may explain why our intervention was effective at all given that a number of neuroimaging studies have shown bilateral activation of the frontal pole associated with exploration (<xref ref-type="bibr" rid="bib10">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="bib3">Badre et al., 2012</xref>). Future work combining cTBS with neuroimaging will be necessary to shed light on these issues.</p><p>With the above caveats that our results may not be entirely due to disruption of frontal pole, the interpretation that RFPC plays a role in directed, but not random, exploration is consistent with a number of previous findings. For example, frontal pole has been associated with tracking the value of the best unchosen option (<xref ref-type="bibr" rid="bib5">Boorman et al., 2009</xref>), inferring the reliability of alternate strategies (<xref ref-type="bibr" rid="bib5">Boorman et al., 2009</xref>; <xref ref-type="bibr" rid="bib11">Domenech and Koechlin, 2015</xref>), arbitrating between old and new strategies (<xref ref-type="bibr" rid="bib12">Donoso et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">Mansouri et al., 2015</xref>), and reallocating cognitive resources among potential goals in underspecified situations (<xref ref-type="bibr" rid="bib30">Pollmann, 2016</xref>). Taken together, these findings suggest a role for frontal pole in model-based decisions (<xref ref-type="bibr" rid="bib10">Daw et al., 2006</xref>) that involve long-term planning and the consideration of alternative actions. From this perspective, it is perhaps not surprising that directed exploration relies on RFPC, since computing an information bonus relies heavily on an internal model of the world. It is also perhaps not surprising that random exploration is independent of RFPC, as this simpler strategy could be implemented without reference to an internal model. Indeed, the ability to explore effectively in a model-free manner, may be an important function of random exploration as it allows us to explore even when our model of the world is wrong.</p><p>More generally, it is unlikely that frontal pole is the only area involved in directed exploration, and more work will be needed to map out the areas involved in directed and random exploration and expose their causal relationship to explore-exploit behavior.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>31 healthy right-handed, adult volunteers (19 female, 12 male; ages 19–32). An initial sample size of 16 was chosen based on two studies using a very similar cTBS design that stimulated lateral FPC (<xref ref-type="bibr" rid="bib9">Costa et al., 2011</xref>; <xref ref-type="bibr" rid="bib8">Costa et al., 2013</xref>) and this was augmented to 31 on the basis of feedback from reviewers. Five participants (5 female, 0 male) were excluded from the analysis due to chance-level performance in both experimental sessions. One (female) participant failed to return for the second (vertex stimulation condition) session and is excluded from the model-free analyses but not the model-based analyses as that can handle missing data more gracefully. Thus our final data set consisted of 25 participants (13 female, 12 male, ages 19–32) with complete data and one participant (female, aged 20) with data from the RFPC session only.</p><p>All participants were informed about potential risks connected to TMS and signed a written consent. The study was approved by University of Social Sciences and Humanities ethics committee.</p></sec><sec id="s4-2"><title>Procedure</title><p>There were two experimental TMS sessions and a preceding MRI session. On the first session T1 structural images were acquired using a 3T Siemens TRIO scanner. The scanning session lasted up to 10 min. Before the first two sessions, participants filled in standard safety questionnaires regarding MRI scanning and TMS. During the experimental sessions, prior to the stimulation participants went through 16 training games to get accustomed to the task. Afterwards, resting motor thresholds were obtained and the stimulation took place. Participants began the main task immediately after stimulation. The two experimental sessions were performed with an intersession interval of at least 5 days. The order of stimulation conditions was counterbalanced across subjects. All sessions took place at Nencki Institute of Experimental Biology in Warsaw.</p></sec><sec id="s4-3"><title>Stimulation site</title><p>The RFPC peak was defined as [x,y,z]= [35,50,15] in MNI (Montreal Neurological Institute) space. The coordinates were based on a number of fMRI findings that indicated RFPC involvement in exploration (<xref ref-type="bibr" rid="bib3">Badre et al., 2012</xref>; <xref ref-type="bibr" rid="bib5">Boorman et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Daw et al., 2006</xref>) and constrained by the plausibility of stimulation (e.g. defining ‘z’ coordinate lower would result in the coil being placed uncomfortably close to the eyes). Vertex corresponded to the Cz position of the 10–20 EEG system. In order to locate the stimulation sites we used a frameless neuronavigation system (Brainsight software, Rogue Research, Montreal, Canada) with a Polaris Vicra infrared camera (Northern Digital, Waterloo, Ontario, Canada).</p></sec><sec id="s4-4"><title>TMS protocol</title><p>We used continuous theta burst stimulation (cTBS) (<xref ref-type="bibr" rid="bib19">Huang et al., 2005</xref>). cTBS requires 50 Hz stimulation at 80% resting motor threshold. 40 s stimulation is equivalent to 600 pulses and can decrease cortical excitability for up to 50 min (<xref ref-type="bibr" rid="bib39">Wischnewski and Schutter, 2015</xref>).</p><p>Individual resting motor thresholds were assessed by stimulating the right motor knob and inspecting if the stimulation caused an involuntary hand twitch in 50% of the cases. We used a MagPro X100 stimulator (MagVenture, Hueckelhoven, Germany) with a 70 mm figure-eight coil. The TMS was delivered in line with established safety guidelines (<xref ref-type="bibr" rid="bib33">Rossi et al., 2009</xref>).</p></sec><sec id="s4-5"><title>Limitations</title><p>Defining stimulation target by peak coordinates based on findings from previous studies did not allow to account for individual differences in either brain anatomy or the impact of TMS on brain networks (<xref ref-type="bibr" rid="bib16">Gratton et al., 2013</xref>). However, a study by Volman and colleagues (<xref ref-type="bibr" rid="bib37">Volman et al., 2011</xref>) that used the same theta-burst protocol on the left frontopolar cortex has shown bilateral inhibitory effects on blood perfusion in the frontal pole. This suggests that both right and left parts of the frontopolar cortex might have been inhibited in our experiment, which is consistent with imaging results indicating bilateral involvement of the frontal pole in exploratory decisions.</p></sec><sec id="s4-6"><title>Task</title><p>The task was a modified version of the Horizon Task (<xref ref-type="bibr" rid="bib38">Wilson et al., 2014</xref>). As in the original paper, the distributions of payoffs tied to bandits were independent between games and drawn from a Gaussian distribution with variable means and fixed standard deviation of 8 points. Participants were informed that in every game one of the bandits was objectively ‘better’ (has a higher payoff mean). Differences between the mean payouts of the two slot machines were set to either 4, 8, 12 or 20. One of the means was always equal to either 40 or 60 and the second was set accordingly. The order of games was randomized. Mean sizes and order of presentation were counterbalanced. Participants played 160 games and the whole task lasted between 39 and 50 min (mean 43.4 min).</p><p>Each game consisted of 5 or 10 choices. Every game started with a screen saying ‘New game’ and information about whether it was a long or short horizon, followed by sequentially presented choices. Every choice was presented on a separate screen, so that participants had to keep previous the scores in memory. There was no time limit for decisions. During forced choices participants had to press the prompted key to move to the next choice. During free choices they could press either ‘z’ or ‘m’ to indicate their choice of left or right bandit. The decision could not be made in a time shorter than 200 ms, preventing participants from accidentally responding too soon. The score feedback was presented for 500 ms. A counter at the bottom of the screen indicated the number of choices left in a given game. The task was programmed using PsychoPy software v1.86 (<xref ref-type="bibr" rid="bib28">Peirce, 2007</xref>).</p><p>Participants were rewarded based on points scored in two sessions. The payoff bounds were set between 50 and 80 zl (equivalent to approximately 12 and 19 euro). Participants were informed about their score and monetary reward after the second session.</p><p>Finally, the random seeds were not perfectly controlled between subjects. The first 16 subjects ran the task with identical random seeds and thus all 16 saw the same sequence of forced-choice trials in both vertex and RFPC sessions. For the remaining subjects the random seed was unique for each subject and each session, thus these subjects had unique a series of forced-choice trials for each session. Despite this limitation we saw no evidence of different behavior across the two groups.</p></sec><sec id="s4-7"><title>Data and code</title><p>Behavioral data as well as Matlab code to recreate the main figures from this paper can be found on the Dataverse website at <ext-link ext-link-type="uri" xlink:href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CZT6EE">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CZT6EE</ext-link>.</p></sec><sec id="s4-8"><title>Model-based analysis</title><p>We modeled behavior on the first free choice of the Horizon Task using a version of the logistic choice model in <xref ref-type="bibr" rid="bib38">Wilson et al. (2014)</xref> that was modified to include a learning component. In particular, we assume that participants use the outcomes of the forced-choice trials to learn an estimate of the mean reward of each option, before inputting that mean reward into a decision function that includes terms for directed and random exploration. This model naturally decomposes into a learning component and a decision component and we consider each of these components in turn.</p><sec id="s4-8-1"><title>Learning component</title><p>The learning component of the model assumes that participants use a Kalman filter to learn a value for the mean reward of each option. The Kalman filter (<xref ref-type="bibr" rid="bib20">Kalman, 1960</xref>) has been used to model learning in other explore-exploit tasks (<xref ref-type="bibr" rid="bib10">Daw et al., 2006</xref>) and is a popular model of Bayesian learning as it is both analytically tractable and easily relatable to the delta-rule update equations of reinforcement learning.</p><p>More specifically, the Kalman filter assumes a generative model in which the rewards from each bandit, <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, are generated from Gaussian distribution with a fixed standard deviation, <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>σ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula>, and a mean, <inline-formula><mml:math id="inf104"><mml:msubsup><mml:mi>m</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula>, that is different for each bandit and can vary over time. The time dependence of the mean is determined by a Gaussian random walk with mean 0 and standard deviation <inline-formula><mml:math id="inf105"><mml:msub><mml:mi>σ</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula>. Note that this generative model, assumed by the Kalman filter, is slightly different to the true generative model used in the Horizon Task, which assumes that the mean of each bandit is constant over time, that is, <inline-formula><mml:math id="inf106"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:math></inline-formula> This mismatch between the assumed and actual generative models, is quite deliberate and allows us to account for the suboptimal learning of the subjects. In particular, this mismatch, introduces the possibility of a recency bias (when <inline-formula><mml:math id="inf107"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) whereby more recent rewards are over-weighted in the computation of <inline-formula><mml:math id="inf108"><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula>.</p><p>The actual equations of the Kalman filter model are straightforward. The model keeps track of an estimate of both the mean reward, <inline-formula><mml:math id="inf109"><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula>, of each option, <inline-formula><mml:math id="inf110"><mml:mi>i</mml:mi></mml:math></inline-formula>, and the uncertainty in that estimate, <inline-formula><mml:math id="inf111"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula>. When option <inline-formula><mml:math id="inf112"><mml:mi>i</mml:mi></mml:math></inline-formula> is played on trial <inline-formula><mml:math id="inf113"><mml:mi>t</mml:mi></mml:math></inline-formula>, these two parameters update according to<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>When option i is not played on trial t we assume that the estimate of the mean stays the same, but that the uncertainty in this estimate grows as the generative model assumes the mean drifts over time. Thus for unchosen option <inline-formula><mml:math id="inf114"><mml:mi>j</mml:mi></mml:math></inline-formula> we have<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>When the option is played, the update <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> for <inline-formula><mml:math id="inf115"><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula> is essentially just a ‘delta rule’ (<xref ref-type="bibr" rid="bib32">Rescorla and Wagner, 1972</xref>; <xref ref-type="bibr" rid="bib34">Schultz et al., 1997</xref>), with the estimate of the mean being updated in proportion to the prediction error, <inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. This relationship to the reinforcement learning literature is made more clear by rewriting the learning equations in terms of the time varying learning rate,<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Written in terms of this learning rate, <xref ref-type="disp-formula" rid="equ3 equ4">Equations 3 and 4</xref> become<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>α</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>The learning model has four free parameters, the noise variance, <inline-formula><mml:math id="inf117"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, the drift variance, <inline-formula><mml:math id="inf118"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, and the initial values of the estimated reward, <inline-formula><mml:math id="inf119"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, and uncertainty in that variance estimate, <inline-formula><mml:math id="inf120"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. In practice, only three of these parameters are identifiable from behavioral data, and we will find it useful to reparameterize the learning model in terms of <inline-formula><mml:math id="inf121"><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and an initial, <inline-formula><mml:math id="inf122"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, and asymptotic, <inline-formula><mml:math id="inf123"><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub></mml:math></inline-formula>, learning rate. In particular, the initial value of the learning rate relates to <inline-formula><mml:math id="inf124"><mml:msub><mml:mi>σ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf125"><mml:msub><mml:mi>σ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf126"><mml:msub><mml:mi>σ</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> as<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>While the asymptotic value of the learning rate, which corresponds to the steady state value of <inline-formula><mml:math id="inf127"><mml:msubsup><mml:mi>α</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula> if option <inline-formula><mml:math id="inf128"><mml:mi>i</mml:mi></mml:math></inline-formula> is played forever, relates to <inline-formula><mml:math id="inf129"><mml:msub><mml:mi>α</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> (and hence <inline-formula><mml:math id="inf130"><mml:msub><mml:mi>σ</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf131"><mml:msub><mml:mi>σ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula>) as<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>While this choice to parameterize the learning equations in terms of <inline-formula><mml:math id="inf132"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf133"><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub></mml:math></inline-formula> is somewhat arbitrary, we feel that the learning rate parameterization has the advantage of being slightly more intuitive and leads to parameter values between 0 and 1 which are (at least for us) easier to interpret.</p></sec><sec id="s4-8-2"><title>Decision component</title><p>Once the payoffs of each option, <inline-formula><mml:math id="inf134"><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula>, have been estimated from the outcomes of the forced-choice trials, the model makes a decision using a simple logistic choice rule:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf135"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> ( <inline-formula><mml:math id="inf136"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> ) is the difference in expected reward between left and right options and <inline-formula><mml:math id="inf137"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> is the difference in information between left and right options (which we define as +1 when left is more informative, −1 when right is more informative, and 0 when both options convey equal information in the (<xref ref-type="bibr" rid="bib2">Auer et al., 2002</xref>; <xref ref-type="bibr" rid="bib2">Auer et al., 2002</xref>) condition). The three free parameters of the decision process are: the information bonus, <inline-formula><mml:math id="inf138"><mml:mi>A</mml:mi></mml:math></inline-formula>, the spatial bias, <inline-formula><mml:math id="inf139"><mml:mi>B</mml:mi></mml:math></inline-formula>, and the decision noise <inline-formula><mml:math id="inf140"><mml:mi>σ</mml:mi></mml:math></inline-formula>. We assume that these three decision parameters can take on different values in the different horizon and uncertainty conditions (with the proviso that <inline-formula><mml:math id="inf141"><mml:mi>A</mml:mi></mml:math></inline-formula> is undefined in the (<xref ref-type="bibr" rid="bib2">Auer et al., 2002</xref>; <xref ref-type="bibr" rid="bib2">Auer et al., 2002</xref>) information condition since <inline-formula><mml:math id="inf142"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). Thus the decision component of the model has 10 free parameters (<inline-formula><mml:math id="inf143"><mml:mi>A</mml:mi></mml:math></inline-formula> in the two horizon conditions, and <inline-formula><mml:math id="inf144"><mml:mi>B</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf145"><mml:mi>σ</mml:mi></mml:math></inline-formula> in the 4 horizon x uncertainty conditions). Directed exploration is then quantified as the change in information bonus with horizon, while random exploration is quantified as the change in decision noise with horizon.</p></sec></sec><sec id="s4-9"><title>Model fitting</title><sec id="s4-9-1"><title>Hierarchical bayesian model</title><p>Between the learning and decision components of the model, each subject’s behavior is described by 13 free parameters, all of which are allowed to vary between TMS conditions. These parameters are: the initial mean, R<inline-formula><mml:math id="inf146"><mml:msub><mml:mi/><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, the initial learning rate, <inline-formula><mml:math id="inf147"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, the asymptotic learning rate, <inline-formula><mml:math id="inf148"><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub></mml:math></inline-formula>, the information bonus, <inline-formula><mml:math id="inf149"><mml:mi>A</mml:mi></mml:math></inline-formula>, in both horizon conditions, the spatial bias, <inline-formula><mml:math id="inf150"><mml:mi>B</mml:mi></mml:math></inline-formula>, in the four horizon x uncertainty conditions, and the decision noise, <inline-formula><mml:math id="inf151"><mml:mi>σ</mml:mi></mml:math></inline-formula>, in the four horizon x uncertainty conditions (<xref ref-type="table" rid="table2">Table 2</xref>, <xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.27430.010</object-id><label>Figure 8.</label><caption><title>Graphical representation of the model.</title><p>Each variable is represented by a node, with edges denoting the dependence between variables. Shaded nodes correspond to observed variables, that is, the free choices <inline-formula><mml:math id="inf152"><mml:msup><mml:mi mathsize="111%">c</mml:mi><mml:mrow><mml:mi mathsize="111%">τ</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">s</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">h</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">u</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">g</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, forced-trial rewards, <inline-formula><mml:math id="inf153"><mml:msup><mml:mi mathsize="111%" mathvariant="bold">𝐫</mml:mi><mml:mrow><mml:mi mathsize="111%">τ</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">s</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">h</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">u</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">g</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and forced-trial choices <inline-formula><mml:math id="inf154"><mml:msup><mml:mi mathsize="111%" mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mi mathsize="111%">τ</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">s</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">h</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">u</mml:mi><mml:mo mathsize="90%" stretchy="false">⁢</mml:mo><mml:mi mathsize="111%">g</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. Unshaded nodes correspond to unobserved variables whose values are inferred by the model.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27430-fig8-v2"/></fig><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.27430.011</object-id><label>Table 2.</label><caption><title>Model parameters, priors, hyperparameters and hyperpriors.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Parameter</th><th>Prior</th><th>Hyperparameters</th><th>Hyperpriors</th></tr></thead><tbody><tr><td>prior mean, <inline-formula><mml:math id="inf155"><mml:msubsup><mml:mi>R</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf156"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Gaussian(<inline-formula><mml:math id="inf157"><mml:msubsup><mml:mi>μ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf158"><mml:msubsup><mml:mi>σ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup></mml:math></inline-formula>)</td><td><inline-formula><mml:math id="inf159"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf160"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Gaussian( 50, 14 ) <break/><inline-formula><mml:math id="inf161"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Gamma( 1, 0.001 )</td></tr><tr><td>initial learning rate, <inline-formula><mml:math id="inf162"><mml:msubsup><mml:mi>α</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf163"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Beta(<inline-formula><mml:math id="inf164"><mml:msubsup><mml:mi>a</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf165"><mml:msubsup><mml:mi>b</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup></mml:math></inline-formula>)</td><td><inline-formula><mml:math id="inf166"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf167"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Uniform( 0.1, 10 ) <break/><inline-formula><mml:math id="inf168"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Uniform( 0.5, 10 )</td></tr><tr><td>asymptotic learning rate, <inline-formula><mml:math id="inf169"><mml:msubsup><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf170"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Beta(<inline-formula><mml:math id="inf171"><mml:msubsup><mml:mi>a</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mi>τ</mml:mi></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf172"><mml:msubsup><mml:mi>b</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mi>τ</mml:mi></mml:msubsup></mml:math></inline-formula>)</td><td><inline-formula><mml:math id="inf173"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf174"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Uniform( 0.1, 10 ) <break/><inline-formula><mml:math id="inf175"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mi>τ</mml:mi></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Uniform( 0.1, 10 )</td></tr><tr><td>information bonus, <inline-formula><mml:math id="inf176"><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf177"><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Gaussian(<inline-formula><mml:math id="inf178"><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf179"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>)</td><td><inline-formula><mml:math id="inf180"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf181"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Gaussian( 0, 100 ) <break/><inline-formula><mml:math id="inf182"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Gamma( 1, 0.001 )</td></tr><tr><td>spatial bias, <inline-formula><mml:math id="inf183"><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf184"><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Gaussian(<inline-formula><mml:math id="inf185"><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>B</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf186"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>B</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>)</td><td><inline-formula><mml:math id="inf187"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>B</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>B</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>B</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf188"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>B</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Gaussian( 0, 100 ) <break/><inline-formula><mml:math id="inf189"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>B</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Gamma( 1, 0.001 )</td></tr><tr><td>decision noise, <inline-formula><mml:math id="inf190"><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf191"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Gamma(<inline-formula><mml:math id="inf192"><mml:msubsup><mml:mi>k</mml:mi><mml:mi>σ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf193"><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>σ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>)</td><td><inline-formula><mml:math id="inf194"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>σ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mi>σ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>σ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf195"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mi>σ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Exp( 0.1 ) <break/><inline-formula><mml:math id="inf196"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>σ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> Exp( 10 )</td></tr></tbody></table></table-wrap><p>Each of the free parameters is fit to the behavior of each subject using a hierarchical Bayesian approach (<xref ref-type="bibr" rid="bib24">Lee and Wagenmakers, 2014</xref>). In this approach to model fitting, each parameter for each subject is assumed to be sampled from a group-level prior distribution whose parameters, the so-called ‘hyperparameters’, are estimated using a Markov Chain Monte Carlo (MCMC) sampling procedure. The hyper-parameters themselves are assumed to be sampled from ‘hyperprior’ distributions whose parameters are defined such that these hyperpriors are broad. For notational convenience, we refer to the hyperparameters that define the prior for variable X as <inline-formula><mml:math id="inf197"><mml:msup><mml:mi>θ</mml:mi><mml:mi>X</mml:mi></mml:msup></mml:math></inline-formula>. In addition we use subscripts to refer to the dependence of both parameters and hyperparameters on TMS stimulation condition, <inline-formula><mml:math id="inf198"><mml:mi>τ</mml:mi></mml:math></inline-formula>, horizon condition, <inline-formula><mml:math id="inf199"><mml:mi>h</mml:mi></mml:math></inline-formula>, uncertainty condition, <inline-formula><mml:math id="inf200"><mml:mi>u</mml:mi></mml:math></inline-formula>, subject, <inline-formula><mml:math id="inf201"><mml:mi>s</mml:mi></mml:math></inline-formula>, and game, <inline-formula><mml:math id="inf202"><mml:mi>g</mml:mi></mml:math></inline-formula>.</p><p>The particular priors and hyperpriors for each parameter are shown in <xref ref-type="table" rid="table2">Table 2</xref>. For example, we assume that the prior mean, <inline-formula><mml:math id="inf203"><mml:msubsup><mml:mi>R</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, for each stimulation condition <inline-formula><mml:math id="inf204"><mml:mi>τ</mml:mi></mml:math></inline-formula> and horizon condition <inline-formula><mml:math id="inf205"><mml:mi>h</mml:mi></mml:math></inline-formula>, is sampled from a Gaussian prior with mean <inline-formula><mml:math id="inf206"><mml:msubsup><mml:mi>μ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="inf207"><mml:msubsup><mml:mi>σ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup></mml:math></inline-formula>. These prior parameters are sampled in turn from their respective hyperpriors: <inline-formula><mml:math id="inf208"><mml:msubsup><mml:mi>μ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup></mml:math></inline-formula>, from a Gaussian distribution with mean 50 and standard deviation 14, <inline-formula><mml:math id="inf209"><mml:msubsup><mml:mi>σ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>τ</mml:mi></mml:msubsup></mml:math></inline-formula> from a Gamma distribution with shape parameter 1 and rate parameter 0.001.</p></sec><sec id="s4-9-2"><title>Model fitting using MCMC</title><p>The model was fit to the data using Markov Chain Monte Carlo approach implemented in the JAGS package (<xref ref-type="bibr" rid="bib29">Plummer, 2003</xref>) via the MATJAGS interface (psiexp.ss.uci.edu/research/programs_data/jags/). This package approximates the posterior distribution over model parameters by generating samples from this posterior distribution given the observed behavioral data.</p><p>In particular we used 4 independent Markov chains to generate 4000 samples from the posterior distribution over parameters (1000 samples per chain). Each chain had a burn in period of 500 samples, which were discarded to reduce the effects of initial conditions, and posterior samples were acquired at a thin rate of 1. Convergence of the Markov chains was confirmed post hoc by eye. Code and data to replicate our analysis and reproduce our Figures is provided as part of the Supplementary Materials.</p></sec></sec></sec></body><back><sec id="s6" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Supervision, Project administration</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Formal analysis, Supervision, Visualization, Writing—original draft, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants were informed about potential risks connected to TMS and signed a written consent. The study was approved by University of Social Sciences and Humanities ethics committee.</p></fn></fn-group></sec><sec id="s5" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.27430.012</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-27430-transrepform-v2.pdf"/></supplementary-material><sec id="s7" sec-type="datasets"><title>Major datasets</title><p>The following dataset was generated:</p><p><related-object content-type="generated-dataset" id="dataset1" source-id="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CZT6EE" source-id-type="uri"><collab collab-type="author">Zajkowski</collab><collab collab-type="author">W</collab><collab collab-type="author">Kossut</collab><collab collab-type="author">M</collab><collab collab-type="author">Wilson</collab><collab collab-type="author">RC</collab><year>2017</year><source>A causal role for right frontopolar cortex in directed, but not random, exploration</source><ext-link ext-link-type="uri" xlink:href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CZT6EE">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CZT6EE</ext-link><comment>Publicly accessible via the Harvard Dataverse website (https://dx.doi.org/10.7910/DVN/CZT6EE)</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aston-Jones</surname> <given-names>G</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Adaptive gain and the role of the locus coeruleus-norepinephrine system in optimal performance</article-title><source>The Journal of Comparative Neurology</source><volume>493</volume><fpage>99</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1002/cne.20723</pub-id><pub-id pub-id-type="pmid">16254995</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auer</surname> <given-names>P</given-names></name><name><surname>Cesa-Bianchi</surname> <given-names>N</given-names></name><name><surname>Fischer</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Finite-time analysis of the multiarmed bandit problem</article-title><source>Machine Learning</source><volume>47</volume><fpage>235</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1023/A:1013689704352</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname> <given-names>D</given-names></name><name><surname>Doll</surname> <given-names>BB</given-names></name><name><surname>Long</surname> <given-names>NM</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rostrolateral prefrontal cortex and individual differences in uncertainty-driven exploration</article-title><source>Neuron</source><volume>73</volume><fpage>595</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.025</pub-id><pub-id pub-id-type="pmid">22325209</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berlyne</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Curiosity and exploration</article-title><source>Science</source><volume>153</volume><fpage>25</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1126/science.153.3731.25</pub-id><pub-id pub-id-type="pmid">5328120</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boorman</surname> <given-names>ED</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Rushworth</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>How green is the grass on the other side? Frontopolar cortex and the evidence in favor of alternative courses of action</article-title><source>Neuron</source><volume>62</volume><fpage>733</fpage><lpage>743</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.05.014</pub-id><pub-id pub-id-type="pmid">19524531</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname> <given-names>JF</given-names></name><name><surname>Bismark</surname> <given-names>AJ</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Allen</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Larger Error Signals in Major Depression are Associated with Better Avoidance Learning</article-title><source>Frontiers in Psychology</source><volume>2</volume><elocation-id>331</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00331</pub-id><pub-id pub-id-type="pmid">22084638</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>McClure</surname> <given-names>SM</given-names></name><name><surname>Yu</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Should I stay or should I go? How the human brain manages the trade-off between exploitation and exploration</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>362</volume><fpage>933</fpage><lpage>942</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2098</pub-id><pub-id pub-id-type="pmid">17395573</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname> <given-names>A</given-names></name><name><surname>Oliveri</surname> <given-names>M</given-names></name><name><surname>Barban</surname> <given-names>F</given-names></name><name><surname>Bonnì</surname> <given-names>S</given-names></name><name><surname>Koch</surname> <given-names>G</given-names></name><name><surname>Caltagirone</surname> <given-names>C</given-names></name><name><surname>Carlesimo</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The right frontopolar cortex is involved in visual-spatial prospective memory</article-title><source>PLoS ONE</source><volume>8</volume><elocation-id>e56039</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0056039</pub-id><pub-id pub-id-type="pmid">23418505</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname> <given-names>A</given-names></name><name><surname>Oliveri</surname> <given-names>M</given-names></name><name><surname>Barban</surname> <given-names>F</given-names></name><name><surname>Torriero</surname> <given-names>S</given-names></name><name><surname>Salerno</surname> <given-names>S</given-names></name><name><surname>Lo Gerfo</surname> <given-names>E</given-names></name><name><surname>Koch</surname> <given-names>G</given-names></name><name><surname>Caltagirone</surname> <given-names>C</given-names></name><name><surname>Carlesimo</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Keeping memory for intentions: a cTBS investigation of the frontopolar cortex</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>2696</fpage><lpage>2703</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr052</pub-id><pub-id pub-id-type="pmid">21515712</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>O'Doherty</surname> <given-names>JP</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Seymour</surname> <given-names>B</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical substrates for exploratory decisions in humans</article-title><source>Nature</source><volume>441</volume><fpage>876</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1038/nature04766</pub-id><pub-id pub-id-type="pmid">16778890</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Domenech</surname> <given-names>P</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Executive control and decision-making in the prefrontal cortex</article-title><source>Current Opinion in Behavioral Sciences</source><volume>1</volume><fpage>101</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2014.10.007</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoso</surname> <given-names>M</given-names></name><name><surname>Collins</surname> <given-names>AG</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Human cognition. Foundations of human reasoning in the prefrontal cortex</article-title><source>Science</source><volume>344</volume><fpage>1481</fpage><lpage>1486</lpage><pub-id pub-id-type="doi">10.1126/science.1252254</pub-id><pub-id pub-id-type="pmid">24876345</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Doll</surname> <given-names>BB</given-names></name><name><surname>Oas-Terpstra</surname> <given-names>J</given-names></name><name><surname>Moreno</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Prefrontal and striatal dopaminergic genes predict individual differences in exploration and exploitation</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1062</fpage><lpage>1068</lpage><pub-id pub-id-type="doi">10.1038/nn.2342</pub-id><pub-id pub-id-type="pmid">19620978</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gittins</surname> <given-names>JC</given-names></name><name><surname>Jones</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>A dynamic allocation index for the discounted multiarmed bandit problem</article-title><source>Biometrika</source><volume>66</volume><elocation-id>561</elocation-id><lpage>565</lpage><pub-id pub-id-type="doi">10.1093/biomet/66.3.561</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gittins</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Resource allocation in speculative chemical research</article-title><source>Journal of Applied Probability</source><volume>11</volume><elocation-id>255</elocation-id><lpage>265</lpage><pub-id pub-id-type="doi">10.1017/S0021900200036718</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gratton</surname> <given-names>C</given-names></name><name><surname>Lee</surname> <given-names>TG</given-names></name><name><surname>Nomura</surname> <given-names>EM</given-names></name><name><surname>D'Esposito</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The effect of theta-burst TMS on cognitive control networks measured with resting state fMRI</article-title><source>Frontiers in Systems Neuroscience</source><volume>7</volume><elocation-id>124</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2013.00124</pub-id><pub-id pub-id-type="pmid">24416003</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanlon</surname> <given-names>CA</given-names></name><name><surname>Dowdle</surname> <given-names>LT</given-names></name><name><surname>Correia</surname> <given-names>B</given-names></name><name><surname>Mithoefer</surname> <given-names>O</given-names></name><name><surname>Kearney-Ramos</surname> <given-names>T</given-names></name><name><surname>Lench</surname> <given-names>D</given-names></name><name><surname>Griffin</surname> <given-names>M</given-names></name><name><surname>Anton</surname> <given-names>RF</given-names></name><name><surname>George</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Left frontal pole theta burst stimulation decreases orbitofrontal and insula activity in cocaine users and alcohol users</article-title><source>Drug and Alcohol Dependence</source><volume>178</volume><fpage>310</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.drugalcdep.2017.03.039</pub-id><pub-id pub-id-type="pmid">28686990</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hills</surname> <given-names>TT</given-names></name><name><surname>Todd</surname> <given-names>PM</given-names></name><name><surname>Lazer</surname> <given-names>D</given-names></name><name><surname>Redish</surname> <given-names>AD</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name><collab>Cognitive Search Research Group</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Exploration versus exploitation in space, mind, and society</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>46</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.10.004</pub-id><pub-id pub-id-type="pmid">25487706</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>YZ</given-names></name><name><surname>Edwards</surname> <given-names>MJ</given-names></name><name><surname>Rounis</surname> <given-names>E</given-names></name><name><surname>Bhatia</surname> <given-names>KP</given-names></name><name><surname>Rothwell</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Theta burst stimulation of the human motor cortex</article-title><source>Neuron</source><volume>45</volume><fpage>201</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.12.033</pub-id><pub-id pub-id-type="pmid">15664172</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>A New Approach to Linear Filtering and Prediction Problems</article-title><source>Journal of Basic Engineering</source><volume>82</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1115/1.3662552</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kidd</surname> <given-names>C</given-names></name><name><surname>Hayden</surname> <given-names>BY</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The psychology and neuroscience of curiosity</article-title><source>Neuron</source><volume>88</volume><fpage>449</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.010</pub-id><pub-id pub-id-type="pmid">26539887</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koechlin</surname> <given-names>E</given-names></name><name><surname>Hyafil</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Anterior prefrontal function and the limits of human decision-making</article-title><source>Science</source><volume>318</volume><fpage>594</fpage><lpage>598</lpage><pub-id pub-id-type="doi">10.1126/science.1142995</pub-id><pub-id pub-id-type="pmid">17962551</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krueger</surname> <given-names>PM</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Strategies for exploration in the domain of losses</article-title><source>Judgment and Decision Making</source><volume>12</volume><fpage>104</fpage><lpage>117</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>MD</given-names></name><name><surname>Wagenmakers</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Bayesian Cognitive Modeling: A Practical Course</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mansouri</surname> <given-names>FA</given-names></name><name><surname>Buckley</surname> <given-names>MJ</given-names></name><name><surname>Mahboubi</surname> <given-names>M</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Behavioral consequences of selective damage to frontal pole and posterior cingulate cortices</article-title><source>PNAS</source><volume>112</volume><fpage>E3940</fpage><lpage>E3949</lpage><pub-id pub-id-type="doi">10.1073/pnas.1422629112</pub-id><pub-id pub-id-type="pmid">26150522</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehlhorn</surname> <given-names>K</given-names></name><name><surname>Newell</surname> <given-names>BR</given-names></name><name><surname>Todd</surname> <given-names>PM</given-names></name><name><surname>Lee</surname> <given-names>MD</given-names></name><name><surname>Morgan</surname> <given-names>K</given-names></name><name><surname>Braithwaite</surname> <given-names>VA</given-names></name><name><surname>Hausmann</surname> <given-names>D</given-names></name><name><surname>Fiedler</surname> <given-names>K</given-names></name><name><surname>Gonzalez</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Unpacking the exploration–exploitation tradeoff: A synthesis of human and animal literatures</article-title><source>Decision</source><volume>2</volume><fpage>191</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1037/dec0000033</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname> <given-names>MR</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Heasly</surname> <given-names>B</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An approximately Bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>12366</fpage><lpage>12378</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0822-10.2010</pub-id><pub-id pub-id-type="pmid">20844132</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>PsychoPy--Psychophysics software in Python</article-title><source>Journal of Neuroscience Methods</source><volume>162</volume><fpage>8</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.11.017</pub-id><pub-id pub-id-type="pmid">17254636</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plummer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>JAGS: a program for analysis of bayesian graphical models using gibbs sampling</article-title><source>Proceedings of the 3rd International Workshop on Distributed Statistical Computing</source><volume>124</volume><fpage>125</fpage></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollmann</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Frontopolar resource allocation in human and nonhuman primates</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>84</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.11.006</pub-id><pub-id pub-id-type="pmid">26699223</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raja Beharelle</surname> <given-names>A</given-names></name><name><surname>Polanía</surname> <given-names>R</given-names></name><name><surname>Hare</surname> <given-names>TA</given-names></name><name><surname>Ruff</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Transcranial stimulation over frontopolar cortex elucidates the choice attributes and neural mechanisms used to resolve exploration-exploitation trade-offs</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>14544</fpage><lpage>14556</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2322-15.2015</pub-id><pub-id pub-id-type="pmid">26511245</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rescorla</surname> <given-names>RA</given-names></name><name><surname>Wagner</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>A theory of pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement</article-title><source>Classical Conditioning II: Current Research and Theory</source><volume>2</volume><fpage>64</fpage><lpage>99</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossi</surname> <given-names>S</given-names></name><name><surname>Hallett</surname> <given-names>M</given-names></name><name><surname>Rossini</surname> <given-names>PM</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name><collab>Safety of TMS Consensus Group</collab></person-group><year iso-8601-date="2009">2009</year><article-title>Safety, ethical considerations, and application guidelines for the use of transcranial magnetic stimulation in clinical practice and research</article-title><source>Clinical Neurophysiology</source><volume>120</volume><fpage>2008</fpage><lpage>2039</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2009.08.016</pub-id><pub-id pub-id-type="pmid">19833552</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname> <given-names>W</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Montague</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Somerville</surname> <given-names>LH</given-names></name><name><surname>Sasse</surname> <given-names>SF</given-names></name><name><surname>Garrad</surname> <given-names>MC</given-names></name><name><surname>Drysdale</surname> <given-names>AT</given-names></name><name><surname>Abi Akar</surname> <given-names>N</given-names></name><name><surname>Insel</surname> <given-names>C</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Charting the expansion of strategic exploratory behavior during adolescence</article-title><source>Journal of Experimental Psychology: General</source><volume>146</volume><fpage>155</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1037/xge0000250</pub-id><pub-id pub-id-type="pmid">27977227</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname> <given-names>WR</given-names></name></person-group><year iso-8601-date="1933">1933</year><article-title>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</article-title><source>Biometrika</source><volume>25</volume><fpage>285</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1093/biomet/25.3-4.285</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Volman</surname> <given-names>I</given-names></name><name><surname>Roelofs</surname> <given-names>K</given-names></name><name><surname>Koch</surname> <given-names>S</given-names></name><name><surname>Verhagen</surname> <given-names>L</given-names></name><name><surname>Toni</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Anterior prefrontal cortex inhibition impairs control over social emotional actions</article-title><source>Current Biology</source><volume>21</volume><fpage>1766</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.08.050</pub-id><pub-id pub-id-type="pmid">22000109</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Geana</surname> <given-names>A</given-names></name><name><surname>White</surname> <given-names>JM</given-names></name><name><surname>Ludvig</surname> <given-names>EA</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Humans use directed and random exploration to solve the explore-exploit dilemma</article-title><source>Journal of Experimental Psychology: General</source><volume>143</volume><fpage>2074</fpage><lpage>2081</lpage><pub-id pub-id-type="doi">10.1037/a0038199</pub-id><pub-id pub-id-type="pmid">25347535</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wischnewski</surname> <given-names>M</given-names></name><name><surname>Schutter</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Efficacy and time course of theta burst stimulation in healthy humans</article-title><source>Brain Stimulation</source><volume>8</volume><fpage>685</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1016/j.brs.2015.03.004</pub-id><pub-id pub-id-type="pmid">26014214</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.27430.015</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Reviewing Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>[Editors’ note: a previous version of this study was rejected after peer review, but the authors submitted for reconsideration. The first decision letter after peer review is shown below.]</p><p>Thank you for submitting your work entitled &quot;A causal role for right frontopolar cortex in directed, but not random, exploration&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor. The reviewers have opted to remain anonymous.</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your work in the current state will not be considered further for publication in <italic>eLife</italic>.</p><p>All involved found the work to have great merit and contributes to the literature on RLPFC and exploration. In our view this is perhaps the clearest demonstration to date that the RLPFC is involved in directed, uncertainty-guided exploration, in that it is the first to imply causality. However, given the state of the literature with other studies (cited in your manuscript) that show RLPFC activation during exploration, that it codes for uncertainty and/or the value of alternative actions, together with an existing TDCS study manipulating it and affecting exploration (albeit not in a way that clearly implicates uncertainty), we felt that the bar for establishing causality in your study needs to be quite high. The reviewers agreed that given the small sample and somewhat marginal statistics, it would be more reassuring if the results held up in a larger N study (or a separate independent replication). Moreover, while the findings here are compelling (e.g. the selectivity to horizon 6 directed exploration), they would be more so especially if you had a control site of stimulation (e.g. DLPFC or IFG) to establish specificity of the RLPFC site. (One of the reviewers noted in the consultation session that RLFPC stimulation may cause discomfort relative to vertex stimulation, which could differentially impact conditions that may require differences in effort).</p><p>Reviewer 1 also had concerns regarding potential power differences to detect effects in directed vs. random exploration.</p><p>If you feel strongly that you can address these concerns we could consider a resubmission. But because the nature of the concerns requires new data collection, and it is unclear whether the results of new studies will provide more clarity, we are rejecting the paper as it stands. We would understand if you chose to submit this study as it is elsewhere.</p><p><italic>Reviewer #1:</italic> </p><p>This manuscript reports the results of a TMS study in which participants are stimulated with theta-burst TMS while participating in a one armed bandit gambling task aimed at distinguishing directed from random exploration. The authors hypothesize that frontopolar cortex is involved in directed but not random exploration. Using both model-based and model-free analyses the authors report that frontopolar cortex inhibition impacts on directed but not random exploration, allowing the authors to conclude that this structure plays a specific role in directed exploration.</p><p>Overall, the study is an interesting one that identifies a potentially important finding. The notion that frontopolar cortex is especially involved in directed exploration is highly plausible, and the results do indeed provide some indication of this possibility. However, I do have several major concerns which I detail below.</p><p>1) One major concern is the possibility that there is a substantial difference in power to detect the effects of TMS on the two forms of exploration due to perhaps a big difference in the number of behavioral choice that index these two forms of exploration on the first trial of each block. While directed exploration in the vertex treatment in the [<xref ref-type="bibr" rid="bib15">1 3</xref>] condition perhaps occurs frequently (the number of trials in which this behavior is found are not reported, but I am inferring this from the high probability of directed exploration reported in the horizon 6 condition), it seems natural to expect that there would be much fewer instances of &quot;random&quot; exploration as defined by choice of the lower valued option in the [<xref ref-type="bibr" rid="bib24">2 2</xref>] condition – this appears to be reflected in the much lower reported probabilities of random exploration in that condition. If there are many fewer trials of random exploration in the first place this ought to make an effect of random exploration following TMS stimulation much harder to detect. Therefore, one trivial account for the authors' double dissociation is that it occurs as a result of a difference in the experimental power to detect these two effects in the paradigm. The claim the authors have about an effect of TMS on direct exploration per se seems well supported in my opinion, but the claim for the specificity of the effect to random exploration seems a lot weaker.</p><p>2) Another concern is that for a behavioral and TMS study the use of such a small sample size of only 15 participants seems hard to justify, especially given that the authors are reporting effects that are just barely reaching significance at p&lt;0.05. Given the concerns raised above about power to detect effects on random exploration, and given that there are a very small number of trials per subject enabling the authors to test their claimed effects (as they are throwing away most of the trials per block and focusing only on the first), suggest that it would not be unreasonable to expect the authors to obtain a larger sample size.</p><p>3) A more generic concern with TMS over frontopolar cortex is that it is unclear with this stimulation protocol how diffuse the effect of the TMS stimulation has been, and to what extent the stimulation protocol has also impacted adjacent regions of frontal cortex. This is an inherent limitation of this technique of course, but there are ways to ameliorate concerns in this regard such as by measuring effects of the stimulation protocol with fMRI. The authors could discuss this limitation and ideally bolster their claims about the degree to which these effects can be specifically attributed to effects of stimulation on frontopolar cortex per se.</p><p>4) Could the apparent effect on directed exploration be driven by other more prosaic possibilities such as an impairment in the ability to flexibly change task set (e.g. from a short to long horizon) across blocks or alterations in the capacity to attend to the task cues indicating the horizon length or even the capacity to incorporate knowledge of task instructions could be impacted instead of directed exploration per se.</p><p>5) Can the authors discriminate between different ways in which directed exploration could be implemented computationally on this task? For instance one could imagine a Bayesian implementation in which a representation of uncertainty over the options is computed and used to direct exploration toward the more uncertain options, or else one could simply use a heuristic strategy of just counting the number of samples of each option to try to ensure each option has been sampled an equivalent number of times.</p><p>6) Although the authors cite Wilson et al. (2014) to describe their modeling strategies, it would be important to reproduce details of exactly how they implemented the model fitting etc. in the current paradigm, as these analyses are central to the current paper and the reader shouldn't be required to go searching for another paper to understand precisely what was done.</p><p>7) I wonder whether more use can be made of the subsequent trials in each block. It seems a shame to throw these trials away, even if the utility of the trials for distinguishing these constructs drops off over repeated trials within a block it seems plausible to me that the 2nd and 3rd trials at the very least would contain useful information.</p><p><italic>Reviewer #2:</italic> </p><p>Zajkowski and colleagues present a study showing that continuous theta burst stimulation to right frontopolar cortex, but not the vertex, selectively reduces directed exploration, but not random exploration. I commend the authors for their experimental approach, combining a carefully designed experimental paradigm and computational modeling of behavior with a transient causal manipulation, such as cTBS. While the results look straightforward, and I do believe they represent an advance on current knowledge in the field, I do not think they represent such a significant advance to merit publication in <italic>eLife</italic> (or a similar high impact journal of broad interest), but would be appropriate for a more specialized journal in the field. Rather than advancing thinking on this topic in some new way, developing a new methodology, or resolving a debate, I believe the results essentially confirm what could be inferred to be likely from the existing fMRI (Daw et al., 2006; Badre et al., 2012) and stimulation (TDCS) literature (Beharelle et al., 2015) on the RFPC and exploration/exploitation. Furthermore, the experimental paradigm and modeling results have been published (Wilson, et al., 2014). I do not mean to discourage the authors, who I think have conducted a genuinely interesting study by combining approaches in an unusual way, and confirming their main hypothesis. I simply do not believe the paper is best suited for a journal of the caliber of <italic>eLife</italic>, but I of course leave this up to the editor's discretion. I have added a few comments below that I hope will be helpful to the authors.</p><p>In the Introduction random exploration is framed as simply increasing decision noise, and directed exploration as information seeking. But is that really the critical distinction? In the real world random exploration is likely to occur when the environmental statistics have changed very rapidly and/or the animal has inferred (for whatever reason) their prior causal model (or even set of models) is (are) no longer tenable. In these circumstances their exploratory behavior is likely still characterized as information seeking, even if it manifests formally as an increase in decision noise. It seems to me, therefore, that the key distinction between directed and undirected exploration is that animals no longer know which options to explore. Can the authors clarify their view, and perhaps modify the Introduction and/or Discussion as needed?</p><p>What were the instructions to participants? Do they necessarily understand that the bandit means are constant and independent? Is there any evidence they weight more recent past samples more strongly than more distant samples? Would this change the estimates of the means in any meaningful way?</p><p>Given the demonstrated effects of cTBS on the hemodynamic signal measured in control networks (Gratton et al., 2013), how specific is the effect of stimulation to RFPC? To address this question, I would have liked to see the investigators target another frontal comparison brain region, in addition or instead of the vertex.</p><p>[Editors’ note: what now follows is the decision letter after the authors submitted for further consideration.]</p><p>Thank you for resubmitting your work entitled &quot;A causal role for right frontopolar cortex in directed, but not random, exploration&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Sabine Kastner as Senior Editor, Michael Frank as Reviewing editor and two reviewers.</p><p>The manuscript has been improved, especially given the doubled sample size, and the model-based and model-free analyses are sophisticated, comprehensive, and generally compelling. However, there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>1) Why do the authors binarize relative information such that it is coded as +1 when the left gamble is more informative and -1 when the right gamble is? Based on Badre, Doll, Frank, et al. I would have thought that the estimated relative uncertainty between options would be more appropriate to quantitatively test the impact of stimulation on directed exploration. Or is variance in this quantity negligible across the critical choices in this task? Related to this question, is this quantity matched across conditions and do all subjects see identical or different schedules?</p><p>2) Although I am not requesting the authors conduct another experiment, a second stimulation site within prefrontal cortex would make for an important comparison for future studies. My suggestion is in part due to the quite severe discomfort frequently caused by TMS stimulation to FPC and neighboring regions due to the underlying facial musculature, as compared to say the vertex. Any differences between stimulation sites could in theory be due to differences in discomfort or subsequent distraction produced by the stimulation sites. Here, this difference could conceivably interact with the comparison between horizon 6 and horizon 1 in the unequal condition if this horizon 6 condition is in fact more cognitively demanding. Note this is not a concern in the cited tDCS study by Raja Beharelle et al. because tDCS does not stimulate the facial muscles and because the excitation and inhibition respectively following anodal and cathodal tDCS provides for an internal control. Can the authors provide some evidence that horizon 6 in the unequal condition is not the most cognitively demanding for their subjects, for instance by analysing RTs? Are there existing data that address this concern by comparing stimulation of FPC and other PFC regions using cTBS?</p><p>3) The trend of an effect of RFPC stimulation on the information bonus for horizon 1, although smaller than that of horizon 6, seems problematic for an interpretation purely based on directed exploration, since there is no opportunity to exploit the newly acquired information for horizon 1. The authors suggest subjects may become less information-seeking in both conditions (consistent with risk or ambiguity aversion in horizon 1 and reduced directed exploration in horizon 6), but this begs the question of what process or mechanism underlies this decrease in both horizons. Given the broader literature on the role of FPC, one interpretation would be that stimulation has disrupted the FPC's ability to faithfully encode the parameters of a &quot;pending&quot; option that they may choose in the future (e.g. Koechlin and Hyafil, Science, 2007) – in this task this could be seen as the option that has not been selected as frequently or attended to recently during forced choices. However I am sure there are other plausible interpretations. How do the authors interpret this effect across horizons in the unequal condition with respect to the broader literature on FPC?</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.27430.016</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the author responses to the first round of peer review follow.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>[…] 1) One major concern is the possibility that there is a substantial difference in power to detect the effects of TMS on the two forms of exploration due to perhaps a big difference in the number of behavioral choice that index these two forms of exploration on the first trial of each block. While directed exploration in the vertex treatment in the [1 3] condition perhaps occurs frequently (the number of trials in which this behavior is found are not reported, but I am inferring this from the high probability of directed exploration reported in the horizon 6 condition), it seems natural to expect that there would be much fewer instances of &quot;random&quot; exploration as defined by choice of the lower valued option in the [2 2] condition – this appears to be reflected in the much lower reported probabilities of random exploration in that condition. If there are many fewer trials of random exploration in the first place this ought to make an effect of random exploration following TMS stimulation much harder to detect. Therefore, one trivial account for the authors' double dissociation is that it occurs as a result of a difference in the experimental power to detect these two effects in the paradigm. The claim the authors have about an effect of TMS on direct exploration per se seems well supported in my opinion, but the claim for the specificity of the effect to random exploration seems a lot weaker.</p></disp-quote><p>This is an important point. Put simply, do we find no effect on random exploration because our experiment is underpowered to detect effects on random exploration? We believe that we do have sufficient power to detect an effect on random exploration (if it were there) and we try to show this using both a model-free and model-based approach.</p><p>For the model-free approach we consider the size of the horizon effect for directed and random exploration in the control condition. This horizon effect is essentially the effect we are trying to remove with TMS and the idea is that, if the horizon effect size is smaller for random than directed exploration, there would be a difference in power to detect changes to the horizon effect. Fortunately the horizon effects are of equal size in this study (in the vertex condition Cohen’s d for directed = 0.71; for random = 0.68). These numbers are largely in line with pure behavioral subjects (the 60 undergraduates from Somerville et al. 2016) where we find d = 0.75 for directed and, a slightly larger, d = 1.18 for random. Thus, if TMS were to reduce the horizon effect by 50% we would have essentially equal power to detect both effects (note we have the same number of trials in the [2 2] condition, for measuring p(low mean) and random exploration, and [1 3] condition, for measuring <italic>p</italic>(high info) and directed exploration).</p><p>For the model-based approach, we can fit the decision noise in the [1 3] uncertainty condition in addition to the [2 2] condition. This gives us an independent estimate of decision noise and gives us another chance to see an effect of TMS on random exploration. In addition, in our new model-based analysis, we use hierarchical Bayesian model fitting to compute posterior distributions over all model parameters given the data (see reviewer #1 response #6 for more details on this model). As shown by the posterior distributions (<xref ref-type="fig" rid="fig4">Figure 4</xref>, main text) we see no effect of TMS on decision noise in <italic>any</italic> of the four uncertainty x horizon conditions, but we do see an effect on information bonus in horizon 6.</p><disp-quote content-type="editor-comment"><p>2) Another concern is that for a behavioral and TMS study the use of such a small sample size of only 15 participants seems hard to justify, especially given that the authors are reporting effects that are just barely reaching significance at p&lt;0.05. Given the concerns raised above about power to detect effects on random exploration, and given that there are a very small number of trials per subject enabling the authors to test their claimed effects (as they are throwing away most of the trials per block and focusing only on the first), suggest that it would not be unreasonable to expect the authors to obtain a larger sample size.</p></disp-quote><p>We agree that N = 15 was not ideal. We have now run an additional 16 subjects and our results hold (see <xref ref-type="fig" rid="respfig1">Author response image 1</xref>).</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>No difference in effects between original and replication experiments.</title><p>In each panel we plot the model-free measures of directed and random exploration and how they change between stimulation conditions. For example, in Panel A, we plot <italic>p</italic>(high info) in horizon 1 for vertex stimulation (x-axis) and RFPC stimulation (y-axis). Each point in this plot is a single subject and the diagonal line represents equality. Participants below the diagonal line have a smaller value of <italic>p</italic>(high info) in the RFPC stimulation condition. From this we can clearly see that there is no effect of RFPC stimulation on directed exploration in horizon 1 (panel A), or random exploration in either horizon (B, D). However, there is a strong effect of RFPC stimulation on directed exploration in horizon 6 with the majority of points lying below the diagonal (C). Moreover, both the original and replication datasets point to the same conclusions in all four panels.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-27430-resp-fig1-v2"/></fig><p>In addition, we have included two new analyses: a model-based Bayesian analysis (results of which are shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>), as well as a model-free analysis of later trials. Both of these analyses point to the same conclusion – inhibition of RFPC leads to selective inhibition of directed exploration in horizon 6.</p><p>The model-free analysis of later trials is presented in the main paper in <xref ref-type="fig" rid="fig6">Figure 6</xref> in its own section. In this analysis we compute <italic>p</italic>(high info) and <italic>p</italic>(low mean) for all trials in the horizon 6 game to see whether behavior on the later trials is affected by stimulation of frontal pole. For directed exploration we find some evidence that the reduction in <italic>p</italic>(high info) on the first trial continues into the second (post hoc, one-sided t-test on the second trial, t(24) = 1.61; p = 0.06), <xref ref-type="fig" rid="fig6">Figure 6</xref> panels A and C. While this is a marginal result, it is consistent with our hypothesis and provides more support for frontal pole playing a role in directed exploration. For random exploration we find no effect of RFPC stimulation on any trial. This is consistent with the idea that frontal pole is not involved in random exploration.</p><p>For completeness we reproduce the particular section of text here:</p><p>“The effect of RFPC stimulation on later trials</p><p>Our analyses so far have focused on just the first free choice and have ignored the remaining five choices in the horizon 6 games. […] Thus, the analysis of later trials provides additional, albeit modest, support for the idea that RFPC stimulation selectively disrupts directed but not random exploration at long horizons.”</p><disp-quote content-type="editor-comment"><p>3) A more generic concern with TMS over frontopolar cortex is that it is unclear with this stimulation protocol how diffuse the effect of the TMS stimulation has been, and to what extent the stimulation protocol has also impacted adjacent regions of frontal cortex. This is an inherent limitation of this technique of course, but there are ways to ameliorate concerns in this regard such as by measuring effects of the stimulation protocol with fMRI. The authors could discuss this limitation and ideally bolster their claims about the degree to which these effects can be specifically attributed to effects of stimulation on frontopolar cortex per se.</p></disp-quote><p>We agree that this is an important point and would be an important follow-up study. We have added the following to the Discussion to address this point:</p><p>“While the present study does allow us to conclude that directed and random exploration rely on different neural systems, the limited spatial specificity of TMS limits our ability to say exactly what those systems are. […] Future work combining cTBS with neuroimaging will be necessary to shed light on these issues.”</p><disp-quote content-type="editor-comment"><p>4) Could the apparent effect on directed exploration be driven by other more prosaic possibilities such as an impairment in the ability to flexibly change task set (e.g. from a short to long horizon) across blocks or alterations in the capacity to attend to the task cues indicating the horizon length or even the capacity to incorporate knowledge of task instructions could be impacted instead of directed exploration per se.</p></disp-quote><p>This is an interesting idea that we believe we can rule out. To paraphrase, the idea is that RFPC stimulation inhibits the ability to adapt to horizon in general (e.g. by causing subjects to ignore relevant task cues) rather than causing a specific deficit in directed exploration. Such a general deficit would predict that the horizon effect on random exploration would also be abolished with RFPC stimulation and this is something we do not see at all in three separate analyses.</p><p>First, in the model-free analysis (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) we see that <italic>p</italic>(low mean) increases with horizon even in the RFPC condition and that RFPC stimulation has no effect on this measure of random exploration.</p><p>Second, this model-free result also holds for the later trials in which we see no stimulation based change in <italic>p</italic>(low mean) over the course of horizon 6 games (<xref ref-type="fig" rid="fig6">Figure 6B, D</xref>). Of course, these later trial results are subject to the reward information confound and so should not be overinterpreted, but they do at least point to the same conclusion that RFPC stimulation does not change the horizon dependence of random exploration.</p><p>Third, our model-based analysis points to the same conclusion that there is no change in decision noise with stimulation condition (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><disp-quote content-type="editor-comment"><p>5) Can the authors discriminate between different ways in which directed exploration could be implemented computationally on this task? For instance one could imagine a Bayesian implementation in which a representation of uncertainty over the options is computed and used to direct exploration toward the more uncertain options, or else one could simply use a heuristic strategy of just counting the number of samples of each option to try to ensure each option has been sampled an equivalent number of times.</p></disp-quote><p>Unfortunately the vanilla Horizon Task used here is not well suited to addressing this question. The reason is that uncertainty on the first free choice is not parametrically modulated – there either is a difference in uncertainty (in the [1 3] condition) or else there is no difference in uncertainty (in the [2 2] condition). While one could try to look at this with a model-based analysis of the later trials, such an analysis is deeply affected by the reward-information confound which makes interpreting results of such an analysis difficult.</p><p>In an on-going set of experiments, we have performed a (purely behavioral) version of the task with parametric modulation of uncertainty. This reveals that the information bonus does appear to scale with uncertainty in a more Bayesian manner, more analysis needs to be done to be sure and the result requires internal replication (much easier with pure behavior than TMS!) before we publish.</p><disp-quote content-type="editor-comment"><p>6) Although the authors cite Wilson et al. (2014) to describe their modeling strategies, it would be important to reproduce details of exactly how they implemented the model fitting etc. in the current paradigm, as these analyses are central to the current paper and the reader shouldn't be required to go searching for another paper to understand precisely what was done.</p></disp-quote><p>This is a fair point and we have now included much more detail on the model. In addition we have expanded the model to include a learning component and fit the model in a different (and more rigorous) hierarchical Bayesian manner. We describe the model at two different points in the text and provide code to implement the model in the Supplementary Material. In the Results section, we highlight the salient points to try to convey the main intuition in the subsection “RFPC stimulation selectively inhibits directed exploration on the first free-choice”. In the Materials and methods section, we go into all the gory details. As this text is extensive, we do not quote it here.</p><disp-quote content-type="editor-comment"><p>7) I wonder whether more use can be made of the subsequent trials in each block. It seems a shame to throw these trials away, even if the utility of the trials for distinguishing these constructs drops off over repeated trials within a block it seems plausible to me that the 2nd and 3rd trials at the very least would contain useful information.</p></disp-quote><p>I wonder this too and have been for quite a while! In trying to model the later trials, it quickly becomes apparent that the reward-information confound is very real and introduces very strong correlations between the fitted parameter values that makes interpretation of the results essentially impossible.</p><p>Despite this difficulty in interpreting the model-based parameters, the model-free statistics (while still being confounded) are at least more straightforward. As mentioned above (response #2), we include this model-free analysis of later trials in a separate section of the Results, along with appropriate health warnings about the reward information confound.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>Zajkowski and colleagues present a study showing that continuous theta burst stimulation to right frontopolar cortex, but not the vertex, selectively reduces directed exploration, but not random exploration. I commend the authors for their experimental approach, combining a carefully designed experimental paradigm and computational modeling of behavior with a transient causal manipulation, such as cTBS.</p></disp-quote><p>We thank the reviewer for the positive comments and helpful feedback. We hope this revision will change your mind about the “importance” of the findings, but regardless of whether the paper is accepted to <italic>eLife</italic>, your comments have greatly improved the paper!</p><disp-quote content-type="editor-comment"><p>While the results look straightforward, and I do believe they represent an advance on current knowledge in the field, I do not think they represent such a significant advance to merit publication in eLife (or a similar high impact journal of broad interest), but would be appropriate for a more specialized journal in the field. Rather than advancing thinking on this topic in some new way, developing a new methodology, or resolving a debate, I believe the results essentially confirm what could be inferred to be likely from the existing fMRI (Daw et al., 2006; Badre et al., 2012) and stimulation (TDCS) literature (Beharelle et al., 2015) on the RFPC and exploration/exploitation. Furthermore, the experimental paradigm and modeling results have been published (Wilson, et al., 2014). I do not mean to discourage the authors, who I think have conducted a genuinely interesting study by combining approaches in an unusual way, and confirming their main hypothesis. I simply do not believe the paper is best suited for a journal of the caliber of eLife, but I of course leave this up to the editor's discretion. I have added a few comments below that I hope will be helpful to the authors.</p></disp-quote><p>While we acknowledge that such judgments of “importance” are often a matter of taste and perspective (all things look big when viewed up close!), we respectfully disagree with this point and believe our study represents a major update to current thinking. In particular, by showing that RFPC stimulation selectively inhibits directed exploration we show that “exploration” is not a <italic>unitary</italic> process, it is a <italic>dual</italic> process in which directed and random exploration rely on (at least partially) dissociable neural systems.</p><p>That exploration is a dual process is absolutely not something one would have concluded from previous work. For example, Daw and Badre see similar activations despite defining exploration in very different ways (choosing low value option for Daw and (loosely) choosing high information options for Badre). The reason the activations are similar is that both tasks have a reward-information confound and after making just a few free choices, the high information options <italic>are</italic> the low value options. This means that every single exploration-related activation in those studies now has a big question mark on it – is it an activation related to directed exploration, random exploration or both? The same can be said of the Beharelle finding, which is beautiful in how it shows opposite effects for anodal and cathodal stimulation, but which cannot dissociate directed and random exploration because of the nature of the behavioral task. To be clear, we do not mean to attack previous work here – these are all incredibly important studies. However, our findings do open them up to reinterpretation.</p><p>We have tried to emphasize this dual-process interpretation in the Discussion:</p><p>“In this work we used continuous theta-burst transcranial magnetic stimulation (cTBS) to investigate whether right frontopolar cortex (RFPC) is causally involved in directed and random exploration. […] This is consistent with the idea that the levels of directed and random exploration are set by the strength of an exploratory drive that varies as an individual difference between people.”</p><disp-quote content-type="editor-comment"><p>In the Introduction random exploration is framed as simply increasing decision noise, and directed exploration as information seeking. But is that really the critical distinction? In the real world random exploration is likely to occur when the environmental statistics have changed very rapidly and/or the animal has inferred (for whatever reason) their prior causal model (or even set of models) is (are) no longer tenable. In these circumstances their exploratory behavior is likely still characterized as information seeking, even if it manifests formally as an increase in decision noise. It seems to me, therefore, that the key distinction between directed and undirected exploration is that animals no longer know which options to explore. Can the authors clarify their view, and perhaps modify the Introduction and/or Discussion as needed?</p></disp-quote><p>This is a really interesting idea and one that would be worth investigating in its own right. We have added a few sentences to the Discussion suggesting that random exploration may be a “model-free” method of exploration that works especially well when the model is unknown.</p><p>“With the above caveats that our results may not be entirely due to disruption of frontal pole, the interpretation that RFPC plays a role in directed, but not random, exploration is consistent with a number of previous findings. […] Indeed, the ability to explore effectively in a model-free manner, may be an important function of random exploration as it allows us to explore even when our model of the world is wrong.”</p><disp-quote content-type="editor-comment"><p>What were the instructions to participants? Do they necessarily understand that the bandit means are constant and independent?</p></disp-quote><p>The instructions were a direct Polish translation of the original instructions used by Wilson et al. (2014). These instructions clearly state that the average reward from each bandit is constant in each game and that the variability is constant over the entire game. For reference see the supplementary material of the original paper. If you feel it would be important for this paper, we would be happy to include them as Supplementary Material.</p><disp-quote content-type="editor-comment"><p>Is there any evidence they weight more recent past samples more strongly than more distant samples? Would this change the estimates of the means in any meaningful way?</p></disp-quote><p>This is a great question and one that has pushed us to update the model. In particular, we have now modeled the learning process (i.e. the process by which participants infer the mean of each option from the forced trials) using a Kalman filter. This model assumes that participants learn the mean reward for each option using a delta-rule update equation</p><p><italic>R<sup>i</sup> <sub>t+1</sub> = R<sup>i</sup><sub>t</sub> + α<sup>i</sup><sub>t</sub> (r<sub>t</sub> – R<sup>i</sup><sub>t</sub>)</italic> (*)</p><p>Where <italic>α<sup>i</sup><sub>t</sub></italic> is the time varying learning rate. The time dependence of the learning rate is determined by the Kalman filter equations (see Materials and methods for full description of the model) and can be parameterized by two parameters: the initial learning rate <italic>α<sub>0</sub></italic> and the asymptotic learning rate <italic>α<sub>inf</sub></italic>. Crucially, equation (*) allows for potentially uneven weighting of the reward depending on the values of <italic>α<sub>0</sub></italic> and <italic>α<sub>inf</sub></italic>. Our previous model, with equal weighting given to all points, corresponds to the case of <italic>α<sub>0</sub> </italic>= 1, <italic>α<sub>inf</sub></italic> = 0. Models with <italic>α<sub>0</sub> </italic>&lt; 1 and <italic>α<sub>inf</sub></italic> &gt; 0 have a recency bias, weighting more recent rewards more strongly.</p><p>The posterior distributions over the group average values of <italic>α<sub>0</sub> </italic>and <italic>α<sub>inf</sub></italic> are shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> in the main paper. In particular <italic>α<sub>0</sub> </italic>~ 0.6 and <italic>α<sub>inf</sub></italic> ~ 0.45, suggesting quite a pronounced recency effect. Importantly, however, neither of these parameters changes between stimulation conditions, and including this learning term in the model does not change the effect of TMS on directed exploration (information bonus in horizon 6).</p><disp-quote content-type="editor-comment"><p>Given the demonstrated effects of cTBS on the hemodynamic signal measured in control networks (Gratton et al., 2013), how specific is the effect of stimulation to RFPC? To address this question, I would have liked to see the investigators target another frontal comparison brain region, in addition or instead of the vertex.</p></disp-quote><p>We agree that our inability to nail down the specificity of the effect is an important limitation of this work. Unfortunately we currently lack the resources to run a study looking at stimulation of other areas and have instead focused our efforts on increasing the sample size of the current study. Likewise, combining TMS with fMRI will be important in future work to more precisely characterize the effects of the perturbation. We have acknowledged both of these limitations in the Discussion as follows:</p><p>“While the present study does allow us to conclude that directed and random exploration rely on different neural systems, the limited spatial specificity of TMS limits our ability to say exactly what those systems are. […] Future work combining cTBS with neuroimaging will be necessary to shed light on these issues.”</p><p>[Editors' note: the author responses to the re-review follow.]</p><disp-quote content-type="editor-comment"><p>1) Why do the authors binarize relative information such that it is coded as +1 when the left gamble is more informative and -1 when the right gamble is? Based on Badre, Doll, Frank, et al. I would have thought that the estimated relative uncertainty between options would be more appropriate to quantitatively test the impact of stimulation on directed exploration. Or is variance in this quantity negligible across the critical choices in this task? Related to this question, is this quantity matched across conditions and do all subjects see identical or different schedules?</p></disp-quote><p>There are a few thoughts behind binarizing information. First, binary information matches the task design in which there is only one unequal information condition and no gradations in uncertainty from a normative perspective. Related to this, and as the reviewer rightly intuits, the single unequal uncertainty condition means that the variance in relative uncertainty between options is relatively small meaning that there is very little difference between the binarized vs continuous definition of information. Because of this we have decided to stick with the binarized version in the paper so as to avoid over interpreting the data.</p><p>More generally, the parametric effect of uncertainty in this task is a key question and is something we are looking at behaviorally in ongoing experiments with different numbers of forced trials. Such explicit manipulation of information leads to much more variance in the uncertainties allowing us to compute parametric effects of uncertainty with more confidence. In brief, these results do suggest a linear effect of uncertainty as seen in previous work.</p><p>Of course, it is possible to fit the continuous model to the data in this paper and when we do so we come to the same conclusions as the binarized model – a selective effect of RFPC stimulation on directed exploration (see <xref ref-type="fig" rid="respfig2">Author response image 2</xref> and <xref ref-type="fig" rid="respfig3">3</xref>).</p><fig id="respfig2"><label>Author response image 2.</label><caption><title>Effect of TMS on information bonus in model with bonus proportional to uncertainty.</title></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-27430-resp-fig2-v2"/></fig><fig id="respfig3"><label>Author response image 3.</label><caption><title>Effect of TMS on decision noise in model in which bonus is a linear function of uncertainty.</title></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-27430-resp-fig3-v2"/></fig><p>Finally, as to the question of whether participants receive exactly the same schedule of trials or not, unfortunately this was not perfectly controlled in either direction. The first 16 subjects (the initial group) were run with the same random seed while the remaining subjects (the replication group) were run with unique random seeds. Given the results replicate between groups we do not think this is a major issue although we now include the following text in the Materials and methods section:</p><p>“Finally, the random seeds were not perfectly controlled between subjects. […] Despite this limitation we saw no evidence of different behavior across the two groups.”</p><disp-quote content-type="editor-comment"><p>2) Although I am not requesting the authors conduct another experiment, a second stimulation site within prefrontal cortex would make for an important comparison for future studies. My suggestion is in part due to the quite severe discomfort frequently caused by TMS stimulation to FPC and neighboring regions due to the underlying facial musculature, as compared to say the vertex. Any differences between stimulation sites could in theory be due to differences in discomfort or subsequent distraction produced by the stimulation sites. Here, this difference could conceivably interact with the comparison between horizon 6 and horizon 1 in the unequal condition if this horizon 6 condition is in fact more cognitively demanding. Note this is not a concern in the cited tDCS study by Raja Beharelle et al. because tDCS does not stimulate the facial muscles and because the excitation and inhibition respectively following anodal and cathodal tDCS provides for an internal control. Can the authors provide some evidence that horizon 6 in the unequal condition is not the most cognitively demanding for their subjects, for instance by analysing RTs?</p></disp-quote><p>We agree that other types and locations of stimulation will be an important avenue for future work and is something that I (RCW) am planning once TMS becomes available at UA.</p><p>The point about pain is also important. As we understand it, the idea is that RFPC stimulation can be painful. Pain is distracting which leads to worse performance, especially when a task is cognitively demanding. Thus if directed exploration in horizon 6 is the most cognitively demanding component of the task, then distraction from pain could cause the effect.</p><p>While we cannot rule this interpretation out entirely, two results suggest that simple distraction is likely not to blame.</p><p>First, one prediction of the distraction hypothesis is that people should perform worse overall when distracted by pain. In the model-free analysis this should show up as increased <italic>p</italic>(low mean) with stimulation of frontal pole. In the model-based analysis, distraction should manifest as increased decision noise in both [1 3] and [2 2] conditions. In both analyses we see no effect of RFPC stimulation (<xref ref-type="fig" rid="fig3">Figures 3B</xref> and <xref ref-type="fig" rid="fig4">4</xref>). This effectively puts an <italic>upper bound</italic> on how distracting the pain could be – the distraction effect must be small enough to cause no change in the ability to pick out the high reward option.</p><p>Of course, the above analysis says nothing about the <italic>lower bound</italic> and it could still be the case that, while the pain is not distracting enough to affect computing the mean reward, it <italic>is</italic> distracting enough to affect the computations of the information bonus. This could be the case if computing the information bonus were harder than computing the mean. Evidence for this increased computational load could come from reaction times. Specifically if computing the bonus is difficult, then RTs should be longer in the [1 3] condition in horizon 6 than in horizon 1. As shown in <xref ref-type="fig" rid="respfig4">Author Response Image 4</xref> this is not the case and there is no effect of horizon on RT for the first free choice (F = 1.32, p = 0.26). Thus computing the information bonus is not a time consuming process, suggesting it is not any more taxing than computing the difference in means between options.</p><fig id="respfig4"><label>Author response image 4.</label><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-27430-resp-fig4-v2"/></fig><p>Together with the null effect on <italic>p</italic>(low mean) we believe that these results provide good evidence that our effects are driven by neural changes (presumably in RFPC – although this is impossible to verify without neuroimaging) not as a response to pain.</p><disp-quote content-type="editor-comment"><p>Are there existing data that address this concern by comparing stimulation of FPC and other PFC regions using cTBS?</p></disp-quote><p>A Google Scholar search for “cTBS frontal pole” found only one paper that reported pain measures. None that we could find directly compared pain from stimulation to FPC and other areas of PFC.</p><p>Hanlon, C. A., Dowdle, L. T., Correia, B., Mithoefer, O., Kearney-Ramos, T., Lench, D.,[…] and George, M. S. (2017). Left frontal pole theta burst stimulation decreases orbitofrontal and insula activity in cocaine users and alcohol users. Drug and Alcohol Dependence.</p><p>This study compared cTBS to frontal pole to a sham stimulation of muscles with electrodes. The study found that participants could not distinguish TMS from sham stimulation. More importantly for our purposes they also found that pain subsided quickly “Subjective reports indicated that the painfulness of the protocol subsided after the first 15-30 s”.</p><p>The following other studies uncovered by the same search did not report measures of pain / discomfort.</p><p>Costa, A., Oliveri, M., Barban, F., Torriero, S., Salerno, S., Lo Gerfo, E.,.[…] and Carlesimo, G. A. (2011). Keeping memory for intentions: a cTBS investigation of the frontopolar cortex. <italic>Cerebral cortex, 21</italic>(12), 2696-2703.</p><p>Costa, A., Oliveri, M., Barban, F., Bonnì, S., Koch, G., Caltagirone, C., and Carlesimo, G. A. (2013). The right frontopolar cortex is involved in visual-spatial prospective memory. PLoS One, 8(2), e56039.</p><p>Rahnev, D., Nee, D. E., Riddle, J., Larson, A. S., and D’Esposito, M. (2016). Causal evidence for frontal cortex organization for perceptual decision making. Proceedings of the National Academy of Sciences, 113(21), 6059-6064.</p><disp-quote content-type="editor-comment"><p>3) The trend of an effect of RFPC stimulation on the information bonus for horizon 1, although smaller than that of horizon 6, seems problematic for an interpretation purely based on directed exploration, since there is no opportunity to exploit the newly acquired information for horizon 1. The authors suggest subjects may become less information-seeking in both conditions (consistent with risk or ambiguity aversion in horizon 1 and reduced directed exploration in horizon 6), but this begs the question of what process or mechanism underlies this decrease in both horizons. Given the broader literature on the role of FPC, one interpretation would be that stimulation has disrupted the FPC's ability to faithfully encode the parameters of a &quot;pending&quot; option that they may choose in the future (e.g. Koechlin and Hyafil, Science, 2007) – in this task this could be seen as the option that has not been selected as frequently or attended to recently during forced choices. However I am sure there are other plausible interpretations. How do the authors interpret this effect across horizons in the unequal condition with respect to the broader literature on FPC?</p></disp-quote><p>We have dug into this point more and can now include more detail. What we believe is going on here is a tradeoff between the mean of the prior, <italic>R<sub>0</sub></italic>, and the information bonus <italic>A</italic>. While this tradeoff does not affect our conclusions that RFPC stimulation selectively affects directed exploration, we believe that the tradeoff does suggest caution when interpreting the horizon 1 result.</p><p>In particular, note that in <xref ref-type="fig" rid="fig4">Figure 4</xref>, in addition to the information bonus going down in both horizons, the prior mean goes up suggesting a possible tradeoff between the information bonus parameter and the mean of the prior. Such a tradeoff is to be expected in this task because the prior has a larger effect on the more uncertain option – i.e. the option chosen once in the [1 3] condition. This larger effect of the prior means that increasing <italic>R<sub>0</sub></italic> can have a similar effect to an information bonus in the task by increasing the relative value of the uncertain option (in RL terms, this would be exploration by optimistic initialization).Thus, in the context of this task, the model contains an inherent tradeoff between the information bonus and mean of the prior.</p><p>In practice, the tradeoff between <italic>R<sub>0</sub> </italic>and <italic>A</italic> shows up as correlations in the posteriors. This is shown in the updated <xref ref-type="fig" rid="fig5">Figure 5</xref> in the manuscript where we plot samples from the posterior over the change in <italic>R<sub>0</sub></italic> between stimulation conditions (<italic>R<sub>0</sub></italic> (vertex) – <italic>R<sub>0</sub></italic> (RFPC)) against the change in information bonus (<italic>A</italic>(vertex) – <italic>A</italic>(RFPC)). In <italic>both</italic> horizon 1 (panel A) and horizon 6 (panel B) there is a tradeoff between the two parameters. However, while the tradeoff affects the interpretation of the horizon 1 and horizon 6 result <italic>alone</italic>, it does not affect the interpretation of the horizon-based <italic>change</italic> in information bonus (panel C).</p><p>In addition to including this new figure, we have addressed this point in the manuscript with the following text:</p><p>“In addition to the effect on the information bonus in horizon 6, there is also a hint of an effect on the information bonus in horizon 1 (85% samples less than zero) and on the prior mean R0 (88% samples above zero). […] Taken together, this suggests that we can be fairly confident in our claim that RFPC stimulation has a selective effect on directed exploration.”</p></body></sub-article></article>