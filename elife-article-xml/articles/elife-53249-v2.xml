<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">53249</article-id><article-id pub-id-type="doi">10.7554/eLife.53249</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Feature Article</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Meta-Research</subject></subj-group></article-categories><title-group><article-title>Large-scale language analysis of peer review reports</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-163284"><name><surname>Buljan</surname><given-names>Ivan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8719-7277</contrib-id><email>ibuljan@mefst.hr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Ivan Buljan</bold> is in the Department of Research in Biomedicine and Health, University of Split School of Medicine, Split, Croatia</p></bio></contrib><contrib contrib-type="author" id="author-165452"><name><surname>Garcia-Costa</surname><given-names>Daniel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8939-8451</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Daniel Garcia-Costa</bold> is in the Department d'Informàtica, University of Valencia, Burjassot-València, Spain</p></bio></contrib><contrib contrib-type="author" id="author-103017"><name><surname>Grimaldo</surname><given-names>Francisco</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1357-7170</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Francisco Grimaldo</bold> is in the Department d'Informàtica, University of Valencia, Burjassot-València, Spain</p></bio></contrib><contrib contrib-type="author" id="author-165453"><name><surname>Squazzoni</surname><given-names>Flaminio</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6503-6077</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Flaminio Squazzoni</bold> is in the Department of Social and Political Sciences, University of Milan, Milan, Italy</p></bio></contrib><contrib contrib-type="author" id="author-165454"><name><surname>Marušić</surname><given-names>Ana</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6272-0917</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Ana Marušić</bold> is in the Department of Research in Biomedicine and Health, University of Split School of Medicine, Split, Croatia</p></bio></contrib><aff id="aff1"><label>1</label><institution>Department of Research in Biomedicine and Health, University of Split School of Medicine</institution><addr-line><named-content content-type="city">Split</named-content></addr-line><country>Croatia</country></aff><aff id="aff2"><label>2</label><institution>Department d'Informàtica, University of Valencia</institution><addr-line><named-content content-type="city">Burjassot-València</named-content></addr-line><country>Spain</country></aff><aff id="aff3"><label>3</label><institution>Department of Social and Political Sciences, University of Milan</institution><addr-line><named-content content-type="city">Milan</named-content></addr-line><country>Italy</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Senior Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>07</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e53249</elocation-id><history><date date-type="received" iso-8601-date="2019-11-01"><day>01</day><month>11</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-07-16"><day>16</day><month>07</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Buljan et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Buljan et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-53249-v2.pdf"/><abstract><p>Peer review is often criticized for being flawed, subjective and biased, but research into peer review has been hindered by a lack of access to peer review reports. Here we report the results of a study in which text-analysis software was used to determine the linguistic characteristics of 472,449 peer review reports. A range of characteristics (including analytical tone, authenticity, clout, three measures of sentiment, and morality) were studied as a function of reviewer recommendation, area of research, type of peer review and reviewer gender. We found that reviewer recommendation had the biggest impact on the linguistic characteristics of reports, and that area of research, type of peer review and reviewer gender had little or no impact. The lack of influence of research area, type of review or reviewer gender on the linguistic characteristics is a sign of the robustness of peer review.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>peer review</kwd><kwd>sentiment analysis</kwd><kwd>linguistic analysis</kwd><kwd>meta-research</kwd><kwd>scientific publishing</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004837</institution-id><institution>Ministerio de Ciencia e Innovación</institution></institution-wrap></funding-source><award-id>RTI2018-095820-B-I00</award-id><principal-award-recipient><name><surname>Garcia-Costa</surname><given-names>Daniel</given-names></name><name><surname>Grimaldo</surname><given-names>Francisco</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100011033</institution-id><institution>Spanish Agencia Estatal de Investigación</institution></institution-wrap></funding-source><award-id>RTI2018-095820-B-I00</award-id><principal-award-recipient><name><surname>Garcia-Costa</surname><given-names>Daniel</given-names></name><name><surname>Grimaldo</surname><given-names>Francisco</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100008530</institution-id><institution>European Regional Development Fund</institution></institution-wrap></funding-source><award-id>RTI2018-095820-B-I00</award-id><principal-award-recipient><name><surname>Garcia-Costa</surname><given-names>Daniel</given-names></name><name><surname>Grimaldo</surname><given-names>Francisco</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004488</institution-id><institution>Croatian Science Foundation</institution></institution-wrap></funding-source><award-id>IP-2019-04-4882</award-id><principal-award-recipient><name><surname>Marušić</surname><given-names>Ana</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The linguistic characteristics of peer review reports are not influenced by research area, type of review or reviewer gender, which is evidence for the robustness of peer review.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Template</meta-name><meta-value>5</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Most journals rely on peer review to ensure that the papers they publish are of a certain quality, but there are concerns that peer review suffers from a number of shortcomings (<xref ref-type="bibr" rid="bib9">Grimaldo et al., 2018</xref>; <xref ref-type="bibr" rid="bib5">Fyfe et al., 2020</xref>). These include gender bias, and other less obvious forms of bias, such as more favourable reviews for articles with positive findings, articles by authors from prestigious institutions, or articles by authors from the same country as the reviewer (<xref ref-type="bibr" rid="bib10">Haffar et al., 2019</xref>; <xref ref-type="bibr" rid="bib15">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Resnik and Elmore, 2016</xref>).</p><p>Analysing the linguistic characteristics of written texts, speeches, and audio-visual materials is well established in the humanities and psychology (<xref ref-type="bibr" rid="bib21">Pennebaker, 2017</xref>). A recent example of this is the use of machine learning by Garg et al. to track gender and ethnic stereotypes in the United States over the past 100 years (<xref ref-type="bibr" rid="bib6">Garg et al., 2018</xref>). Similar techniques have been used to analyse scientific articles, with an early study showing that scientific writing is a complex process that is sensitive to formal and informal standards, context-specific canons and subjective factors (<xref ref-type="bibr" rid="bib11">Hartley et al., 2003</xref>). Later studies found that fraudulent scientific papers seem to be less readable than non-fraudulent papers (<xref ref-type="bibr" rid="bib17">Markowitz and Hancock, 2016</xref>), and that papers in economics written by women are better written than equivalent papers by men (and that this gap increases during the peer review process; <xref ref-type="bibr" rid="bib12">Hengel, 2018</xref>). There is clearly scope for these techniques to be used to study other aspects of the research and publishing process.</p><p>To date most research on the linguistic characteristics of peer review has focused on comparisons between different types of peer review, and it has been shown that open peer review (in which peer review reports and/or the names of reviewers are made public) leads to longer reports and a more positive emotional tone compared to confidential peer review (<xref ref-type="bibr" rid="bib2">Bravo et al., 2019</xref>; <xref ref-type="bibr" rid="bib1">Bornmann et al., 2012</xref>). Similar techniques have been used to explore possible gender bias in the peer review of grant applications, but a consensus has not been reached yet (<xref ref-type="bibr" rid="bib18">Marsh et al., 2011</xref>; <xref ref-type="bibr" rid="bib16">Magua et al., 2017</xref>). To date, however, these techniques have not been applied to the peer review process at a large scale, largely because most journals strictly limit access to peer review reports.</p><p>Here we report the results of a linguistic analysis of 472,449 peer review reports from the PEERE database (<xref ref-type="bibr" rid="bib28">Squazzoni et al., 2017</xref>). The reports came from 61 journals published by Elsevier in four broad areas of research: health and medical sciences (22 journals); life sciences (5); physical sciences (30); social sciences and economics (4). For each review we had data on the following: i) the recommendation made by the reviewer (accept [n = 26,387, 5.6%]; minor revisions required [134,858, 28.5%]; major revisions required [161,696, 34.2%]; reject [n = 149,508, 31.7%]); ii) the broad area of research; iii) the type of peer review used by the journal (single-blind [n = 411,727, 87.1%] or double-blind [n = 60,722, 12.9%]); and the gender of the reviewer (75.9% were male; 24.1% were female).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We used various linguistic tools to examine the peer review reports in our sample (see Methods for more details). Linguistic Inquiry and Word Count (LIWC) text-analysis software was used to perform word counts and to return scores of between 0% and 100% for ‘analytical tone’, ‘clout’ and ‘authenticity’ (<xref ref-type="bibr" rid="bib20">Pennebaker et al., 2015</xref>). Three different approaches were used to perform sentiment analysis: i) LIWC returns a score between 0% and 100% for ‘emotional tone’ (with more positive emotions leading to higher scores); ii) the SentimentR package returns a majority of scores between –1 (negative sentiment) and +1 (positive sentiment), with an extremely low number of results outside that range (0.03% in our sample); iii) the Stanford CoreNLP returns a score between 0 (negative sentiment) to +4 (positive sentiment). We also used LIWC to analyse the reports in terms of five foundations of morality (<xref ref-type="bibr" rid="bib8">Graham et al., 2009</xref>).</p><sec id="s2-1"><title>Length of report</title><p>For all combinations of area of research, type of peer review and reviewer gender, reports recommending accept were shortest, followed by reports recommending minor revisions, reject, and major revisions (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Reports written by reviewers for social sciences and economics journals were significantly longer than those written by reviewers for medical journals; men also tended to write longer reports than women; however, the type of peer review (i.e., single- vs. double-blind) did not have any influence on the length of reports (see Table 2 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Words counts in peer review reports.</title><p>Word count (mean and 95% confidence interval; LIWC analysis) of peer review reports in four broad areas of research for double-blind review (top) and single-blind review (bottom), and for female reviewers (left) and male reviewers (right). Reports recommending accept (red) were consistently the shortest, and reports recommending major revisions (green) were consistently the longest. See <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for summary data and mixed model linear regression coefficients and residuals. HMS: health and medical sciences; LS: life sciences; PS: physical sciences; SS&amp;E: social sciences and economics.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53249-fig1-v2.tif"/></fig></sec><sec id="s2-2"><title>Analytical tone, clout and authenticity</title><p>LIWC returned high scores (typically between 85.0 and 91.0) for analytical tone, and low scores (typically between 18.0 and 25.0) for authenticity, for the peer review reports in our sample (<xref ref-type="fig" rid="fig2">Figure 2A,C</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A,C</xref>). High authenticity of a text is defined as the use of more personal words (I-words), present tense words, and relativity words, and fewer non-personal words and modal words (<xref ref-type="bibr" rid="bib20">Pennebaker et al., 2015</xref>). Low authenticity and high analytical tone are characteristic of texts describing medical research (<xref ref-type="bibr" rid="bib14">Karačić et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Glonti et al., 2017</xref>). There was some variation with reviewer recommendation in the scores returned for clout, with accept having the highest scores for clout, followed by minor revisions, major revisions and reject (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Analytical tone, clout and authenticity and in peer review reports for single-blind review.</title><p>Scores returned by LIWC (mean percentages and 95% confidence interval) for analytical tone (<bold>A</bold>), clout (<bold>B</bold>) and authenticity (<bold>C</bold>) for peer review reports in four broad areas of research for female reviewers (left) and male reviewers (right) using single-blind review. Reports recommending accept (red) consistently had the most clout, and reports recommending reject (purple) consistently had the least clout. See <xref ref-type="supplementary-material" rid="supp2">Supplementary files 2</xref>–<xref ref-type="supplementary-material" rid="supp4">4</xref> for summary data, mixed model linear regression coefficients and residuals, and examples of reports with high and low scores for analytical tone, clout and authenticity. HMS: health and medical sciences; LS: life sciences; PS: physical sciences; SS&amp;E: social sciences and economics.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53249-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Analytical tone, clout and authenticity in peer review reports for double-blind review.</title><p>Scores returned by LIWC (mean percentages and 95% confidence interval) for analytical tone (<bold>A</bold>), clout (<bold>B</bold>) and authenticity (<bold>C</bold>) for peer review reports in four broad areas of research for female reviewers (left) and male reviewers (right) using double-blind review.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53249-fig2-figsupp1-v2.tif"/></fig></fig-group><p>When reviewers recommended major revisions, the text of the report was more analytical. The analytical tone was higher when reviewers were women and for single-blind peer review, but we did not find any effect of the area of research (see Table 4 in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p><p>Clout levels varied with area of research, with the highest levels in social sciences and economics journals (see Table 7 in <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). When reviewers recommended rejection, the text showed low levels of clout, as it did when reviewers were men and when the journal useded single-blind peer review (see Table 7 in <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>).</p><p>The text of reports in social sciences and economics journals had the highest levels of authenticity. Authenticity was prevalent also when reviewers recommended rejection. There was no significant variation in terms of authenticity per reviewer gender or type of peer review (see Table 10 in <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>).</p></sec><sec id="s2-3"><title>Sentiment analysis</title><p>The three approaches were used to perform sentiment analysis on our sample – LIWC, SentimentR and the Stanford CoreNLP – produced similar results. Reports recommending accept had the highest scores, indicating higher sentiment, followed by reports recommending minor revisions, major revisions and reject (<xref ref-type="fig" rid="fig3">Figure 3</xref>; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Furthermore, reports for social sciences and economics journals had the highest levels of sentiment, as did reviews written by women. We did not find any association between sentiment and the type of peer review (see Table 13 in <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>, Table 16 in <xref ref-type="supplementary-material" rid="supp6">Supplementary file 6</xref> and Table 19 in <xref ref-type="supplementary-material" rid="supp7">Supplementary file 7</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Sentiment analysis of peer review reports for single-blind review.</title><p>Scores for sentiment analysis returned by LIWC (<bold>A</bold>; mean percentage and 95% confidence interval, CI), SentimentR (<bold>B</bold>; mean score and 95% CI), and Stanford CoreNLP (<bold>C</bold>; mean score and 95% CI) for peer review reports in four broad areas of research for female reviewers (left) and male reviewers (right) using single-blind review. See <xref ref-type="supplementary-material" rid="supp5">Supplementary files 5</xref>–<xref ref-type="supplementary-material" rid="supp7">7</xref> for summary data, mixed model linear regression coefficients and residuals, and examples of reports with high and low scores for sentiment according to LIWC, SentimentR and Stanford CoreNLP analysis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53249-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Sentiment analysis of peer review reports for double-blind review.</title><p>Scores for sentiment analysis returned by LIWC (<bold>A</bold>; mean percentage and 95% confidence interval, CI), SentimentR (<bold>B</bold>; mean score and 95% CI), and Stanford CoreNLP (<bold>C</bold>; mean score and 95% CI) for peer review reports in four broad areas of research for female reviewers (left) and male reviewers (right) using double-blind review.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53249-fig3-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-4"><title>Moral foundations</title><p>LIWC was also used to explore the morality of the reports in our sample (<xref ref-type="bibr" rid="bib8">Graham et al., 2009</xref>). The differences between peer review recommendations were statistically significant. Reports recommending acceptance had the highest scores for general morality, followed by reports recommending minor revisions, major revisions and reject (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Regarding the research area, we found a lowest proportion of words related to morality in the social sciences and economics, when reviewers were men, and when single-blind peer review was used (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Moral foundations in peer review reports.</title><p>Scores returned by LIWC (mean percentage on a log scale) for general morality in peer review reports in four broad areas of research for double-blind review (top) and single-blind review (bottom), and for female reviewers (left) and male reviewers (right). Reports recommending accept (red) consistently had the highest scores. See <xref ref-type="supplementary-material" rid="supp8">Supplementary file 8</xref> for lists of the ten most frequent words found in peer review reports for general morality and the five moral foundation variables. HMS: health and medical sciences; LS: life sciences; PS: physical sciences; SS&amp;E: social sciences and economics.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53249-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Scores returned by LIWC (mean percentage on a log scale and 95% CI) for care/harm, one of the five foundations of Moral Foundations Theory.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53249-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Scores returned by LIWC (mean percentage on a log scale and 95% CI) for fairness/cheating, one of the five foundations of Moral Foundations Theory.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53249-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Scores returned by LIWC (mean percentage on a log scale and 95% CI) for loyalty/betrayal, one of the five foundations of Moral Foundations Theory.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53249-fig4-figsupp3-v2.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Scores returned by LIWC (mean percentage on a log scale and 95% CI) for authority/subversion, one of the five foundations of Moral Foundations Theory.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53249-fig4-figsupp4-v2.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 5.</label><caption><title>Scores returned by LIWC (mean percentage on a log scale and 95% CI) for sanctity/degradation, one of the five foundations of Moral Foundations Theory.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53249-fig4-figsupp5-v2.tif"/></fig></fig-group><p>We also explored five foundations of morality – care/harm, fairness/cheating, loyalty/betrayal, authority/subversion, and sanctity/degradation – but no clear patterns emerged (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s5">5</xref>). See the Methods section for more details, and <xref ref-type="supplementary-material" rid="supp8">Supplementary file 8</xref> for lists of the ten most common phrases from the LIWC Moral Foundation dictionary. In general, the prevalence of these words was minimal, with average scores lower than 1%. Moreover, these words tended to be part of common phrases and thus did not speak to the moral content of the reviews. This suggests that a combination of qualitative and quantitative methods, including machine learning tools, will be required to explore the moral aspects of peer review.</p></sec></sec><sec id="s3"><title>Conclusion</title><p>Our study suggests that the reviewer recommendation has the biggest influence on the linguistic characteristics (and length) of peer review reports, which is consistent with previous, case-based research (<xref ref-type="bibr" rid="bib3">Casnici et al., 2017</xref>). It is probable that whenever reviewers recommend revision, they write a longer report in order to justify their requests and/or to suggest changes to improve the manuscript (which they do not have to do when they recommend to accept or reject). In our study, in the case of the two more negative recommendations (reject and major revisions), the reports were shorter, and language was less emotional and more analytical. We found that the type of peer review – single-blind or double-blind – had no significant influence on the reports, contrary to previous reports on smaller samples (<xref ref-type="bibr" rid="bib2">Bravo et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">van Rooyen et al., 1999</xref>). Likewise, area of research had no significant influence on the reports in the sample, and neither did reviewer gender, which is consistent with a previous smaller study (<xref ref-type="bibr" rid="bib2">Bravo et al., 2019</xref>). The lack of influence exerted by the area of research, the type of peer review or the reviewer gender on the linguistic characteristics of the reports is a sign of the robustness of peer review.</p><p>The results of our study should be considered in the light of certain limitations. Most of the journals were in the health and medical sciences and the physical sciences, and most used single-blind peer review. However, the size, depth and uniqueness of our dataset helped us provide a more comprehensive analysis of peer review reports than previous studies, which were often limited to small samples and incomplete data (<xref ref-type="bibr" rid="bib30">van den Besselaar et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Sizo et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Falk Delgado et al., 2019</xref>). Future research would also benefit from baseline data against which results could be compared, although our results match the preliminary results from a study at a single biomedical journal (<xref ref-type="bibr" rid="bib7">Glonti et al., 2017</xref>), and from knowing more about the referees (such as their status or expertise). Finally, we did not examine the actual content of the manuscripts under review, so we could not determine how reliable reviewers were in their assessments. Combining language analyses of peer review reports with estimates of peer review reliability for the same manuscripts (via inter-reviewer ratings) could provide new insights into the peer review process.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>The PEERE dataset</title><p>PEERE is a collaboration between publishers and researchers (<xref ref-type="bibr" rid="bib29">Squazzoni et al., 2020</xref>), and the PEERE dataset contains 583,365 peer review reports from 61 journal published by Elsevier, with data on reviewer recommendation, area of research (health and medical sciences; life sciences; physical sciences; social sciences and economics), type of peer review (single blind or double blind), and reviewer gender for each report. Most of the reports (N = 481,961) are for original research papers, with the rest (N = 101,404) being for opinion pieces, editorials and letters to the editor. The database was first filtered to exclude reviews that included reference to manuscript revisions, resulting in 583,365 reports. We eliminated 110,636 due to the impossibility to determine reviewer gender, and 260 because we did not have data on the recommendation. Our analysis was performed on a total number of 472,449 peer review reports.</p></sec><sec id="s4-2"><title>Gender determination</title><p>To determine reviewer gender, we followed a standard disambiguation algorithm that has already been validated on a dataset of scientists extracted from the Web of Science database covering a similar publication time window (<xref ref-type="bibr" rid="bib25">Santamaría and Mihaljević, 2018</xref>). Gender was assigned following a multi-stage gender inference procedure consisting of three steps. First, we performed a preliminary gender determination using, when available, gender salutation (i.e., Mr, Mrs, Ms...). Secondly, we queried the Python package gender-guesser about the extracted first names and country of origin, if any. Gender-guesser has demonstrated to achieve the lowest misclassification rate and introduce the smallest gender bias (<xref ref-type="bibr" rid="bib19">Paltridge, 2017</xref>). Lastly, we queried the best performer gender inference service, Gender API (<ext-link ext-link-type="uri" xlink:href="https://gender-api.com/">https://gender-api.com/</ext-link>), and used the returned gender whenever we found a minimum of 62 samples with, at least, 57% accuracy, which follows the optimal values found in benchmark 2 of the previous research (<xref ref-type="bibr" rid="bib25">Santamaría and Mihaljević, 2018</xref>). This threshold for the obtained confidence parameters was suitable to ensure that the rate of misclassified names did not exceed 5% (<xref ref-type="bibr" rid="bib25">Santamaría and Mihaljević, 2018</xref>). This allowed us to determine the gender of 81.1% of reviewers, among which 75.9% were male and 24.1% female. With regards to the three possible gender sources, 6.3% of genders came from scientist salutation, 77.2% from gender-guesser, and 16.5% from the Gender API. The remaining 18.9% of reviewers were assigned an unknown gender. This level of gender determination is consistent with the non-classification rate for names of scientists in previous research (<xref ref-type="bibr" rid="bib25">Santamaría and Mihaljević, 2018</xref>).</p></sec><sec id="s4-3"><title>Analytical tone, authenticity and clout</title><p>We used a version of the Linguistic Inquiry and Word Count (LIWC) text-analysis software with standardized scores (<ext-link ext-link-type="uri" xlink:href="http://liwc.wpengine.com/">http://liwc.wpengine.com/</ext-link>) to analyze the peer review reports in our sample. LIWC measures the percentage of words related to three psychological features (so scores range from 0 to 100): ‘analytical tone’; ‘clout’; and &quot;authenticity. A high score for analytical tone indicates a report with a logical and hierarchical style of writing. Clout reveals personal sensitivity towards social status, confidence or leadership: a low score for clout is associated with insecurities and a less confident and more tentative tone (<xref ref-type="bibr" rid="bib13">Kacewicz et al., 2014</xref>). A high score for authenticity indicates a report written in a style that is honest and humble, whereas a low score indicates a style that is deceptive and superficial (<xref ref-type="bibr" rid="bib20">Pennebaker et al., 2015</xref>). The words people use also reflect how authentic or personal they sound. People who are authentic tend to use more I-words (e.g. I, me, mine), present-tense verbs, and relativity words (e.g. near, new) and fewer she-he words (e.g. his, her) and discrepancies (e.g. should, could) (<xref ref-type="bibr" rid="bib20">Pennebaker et al., 2015</xref>).</p></sec><sec id="s4-4"><title>Sentiment analysis</title><p>We used three different methodological approaches to assess sentiment. (i) LIWC measures ‘emotional tone’, which indicates writing dominated by either positive or negative emotions by counting number of words from a pre-specified dictionary. (ii) The SentimentR package (<xref ref-type="bibr" rid="bib24">Rinker, 2019</xref>) classifies the proportion of words related to sentiment in the text, similarly to the ‘emotional tone’ scores in LIWC but using a different vocabulary. The SentimentR score is the valence of words related with the specific sentiment, majority of scores (99.97%) ranging from −1 (negative sentiment) +1 (positive sentiment). (iii) Stanford CoreNLP is a deep language analysis program that uses machine learning to determine the emotional valence of the text (<xref ref-type="bibr" rid="bib27">Socher et al., 2013</xref>), and score ranges from 0 (negative sentiment) to +4 (positive sentiment). Examples of characteristic text variables from the peer review reports analysed with these approaches are given in <xref ref-type="supplementary-material" rid="supp5">Supplementary files 5</xref>–<xref ref-type="supplementary-material" rid="supp7">7</xref>.</p></sec><sec id="s4-5"><title>Moral foundations</title><p>We used LIWC and Moral Foundations Theory (<ext-link ext-link-type="uri" xlink:href="https://moralfoundations.org/other-materials/">https://moralfoundations.org/other-materials/</ext-link>) to analyse the reports in our sample according to five moral foundations: care/harm (also known as care-virtue/care-vice); fairness/cheating (or fairness-virtue/fairness-vice); loyalty/betrayal (or loyalty-virtue/loyalty-vice); authority/subversion (authority virtue/authority-vice); and sanctity/degradation (or sanctity-virtue/sanctity-vice).</p></sec><sec id="s4-6"><title>Statistical methods</title><p>Data were analysed using the R programming language, version 3.6.3. (<xref ref-type="bibr" rid="bib22">R Development Core Team, 2017</xref>). To test the interaction effects and compare different peer review characteristics, we conducted a mixed model linear analysis on each variable (analytical tone, authenticity, clout; the measures of sentiment; and the measures of morality) with reviewer recommendation, area of research, type of peer review (single- or double-blind) and reviewer gender as fixed factors (predictors) and the journal, word count and article type as the random factor. This was to control across-journal interactions, number of words and article type.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Dr Bahar Mehmani from Elsevier for helping us with data collection.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Investigation, Visualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Data curation, Supervision, Investigation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Writing - original draft, Project administration</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Word count (<xref ref-type="fig" rid="fig1">Figure 1</xref>): summary data and mixed model linear regression coefficients and residuals.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53249-supp1-v2.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Analytical tone (<xref ref-type="fig" rid="fig2">Figure 2A</xref>): summary data, mixed model linear regression coefficients and residuals, and examples of reports with high and low scores for LIWC analytical tone.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53249-supp2-v2.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Clout (<xref ref-type="fig" rid="fig2">Figure 2B</xref>): summary data, mixed model linear regression coefficients and residuals, and examples of reports with high and low scores for LIWC clout.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53249-supp3-v2.docx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Authenticity (<xref ref-type="fig" rid="fig2">Figure 2C</xref>): summary data, mixed model linear regression coefficients and residuals, and examples of reports with high and low scores for LIWC authenticity.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53249-supp4-v2.docx"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>Sentiment/LIWC emotional tone (<xref ref-type="fig" rid="fig3">Figure 3A</xref>): summary data, mixed model linear regression coefficients and residuals, and examples of reports with high and low scores for sentiment (LIWC emotional tone).</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53249-supp5-v2.docx"/></supplementary-material><supplementary-material id="supp6"><label>Supplementary file 6.</label><caption><title>Sentiment/SentimentR score (<xref ref-type="fig" rid="fig3">Figure 3B</xref>): summary data, mixed model linear regression coefficients and residuals, and examples of reports with high and low scores for sentiment (SentimentR scores).</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53249-supp6-v2.docx"/></supplementary-material><supplementary-material id="supp7"><label>Supplementary file 7.</label><caption><title>Sentiment/Stanford CoreNLP score (<xref ref-type="fig" rid="fig3">Figure 3C</xref>): summary data, mixed model linear regression coefficients and residuals, and examples of reports with high and low scores for sentiment (Stanford CoreNLP score).</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53249-supp7-v2.docx"/></supplementary-material><supplementary-material id="supp8"><label>Supplementary file 8.</label><caption><title>Ten most frequent words found in peer review reports for general morality and the five moral foundation variables.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53249-supp8-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-53249-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The journal dataset required a data sharing agreement to be established between authors and publishers. A protocol on data sharing entitled 'TD1306 COST Action New frontiers of peer review (PEERE) PEERE policy on data sharing on peer review' was signed by all partners involved in this research on 1 March 2017, as part of a collaborative project funded by the EU Commission. The protocol established rules and practices for data sharing from a sample of scholarly journals, which included a specific data management policy, including data minimization, retention and storage, privacy impact assessment, anonymization, and dissemination. The protocol required that data access and use were restricted to the authors of this manuscript and data aggregation and report were done in such a way to avoid any identification of publishers, journals or individual records involved. The protocol was written to protect the interests of any stakeholder involved, including publishers, journal editors and academic scholars, who could be potentially acted by data sharing, use and release. The full version of the protocol is available on the peere.org website. To request additional information on the dataset and for any claim or objection, please contact the PEERE data controller at info@peere.org.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bornmann</surname> <given-names>L</given-names></name><name><surname>Wolf</surname> <given-names>M</given-names></name><name><surname>Daniel</surname> <given-names>H-D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Closed versus open reviewing of journal manuscripts: how far do comments differ in language use?</article-title><source>Scientometrics</source><volume>91</volume><fpage>843</fpage><lpage>856</lpage><pub-id pub-id-type="doi">10.1007/s11192-011-0569-5</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bravo</surname> <given-names>G</given-names></name><name><surname>Grimaldo</surname> <given-names>F</given-names></name><name><surname>López-Iñesta</surname> <given-names>E</given-names></name><name><surname>Mehmani</surname> <given-names>B</given-names></name><name><surname>Squazzoni</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The effect of publishing peer review reports on referee behavior in five scholarly journals</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>322</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-08250-2</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Casnici</surname> <given-names>N</given-names></name><name><surname>Grimaldo</surname> <given-names>F</given-names></name><name><surname>Gilbert</surname> <given-names>N</given-names></name><name><surname>Squazzoni</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attitudes of referees in a multidisciplinary journal: An empirical analysis</article-title><source>Journal of the Association for Information Science and Technology</source><volume>68</volume><fpage>1763</fpage><lpage>1771</lpage><pub-id pub-id-type="doi">10.1002/asi.23665</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falk Delgado</surname> <given-names>A</given-names></name><name><surname>Garretson</surname> <given-names>G</given-names></name><name><surname>Falk Delgado</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The language of peer review reports on articles published in the BMJ, 2014–2017: an observational study</article-title><source>Scientometrics</source><volume>120</volume><fpage>1225</fpage><lpage>1235</lpage><pub-id pub-id-type="doi">10.1007/s11192-019-03160-6</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fyfe</surname> <given-names>A</given-names></name><name><surname>Squazzoni</surname> <given-names>F</given-names></name><name><surname>Torny</surname> <given-names>D</given-names></name><name><surname>Dondio</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Managing the growth of peer review at the Royal Society journals, 1865-1965</article-title><source>Science, Technology &amp; Human Values</source><volume>45</volume><fpage>405</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1177/0162243919862868</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garg</surname> <given-names>N</given-names></name><name><surname>Schiebinger</surname> <given-names>L</given-names></name><name><surname>Jurafsky</surname> <given-names>D</given-names></name><name><surname>Zou</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Word embeddings quantify 100 years of gender and ethnic stereotypes</article-title><source>PNAS</source><volume>115</volume><fpage>E3635</fpage><lpage>E3644</lpage><pub-id pub-id-type="doi">10.1073/pnas.1720347115</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Glonti</surname> <given-names>K</given-names></name><name><surname>Hren</surname> <given-names>D</given-names></name><name><surname>Carter</surname> <given-names>S</given-names></name><name><surname>Schroter</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Linguistic features in peer reviewer reports: how peer reviewers communicate their recommendations</article-title><source>Proceedings of the International Congress on Peer Review and Scientific Publication</source><ext-link ext-link-type="uri" xlink:href="https://peerreviewcongress.org/prc17-0234">https://peerreviewcongress.org/prc17-0234</ext-link><date-in-citation iso-8601-date="2020-04-20">April 20, 2020</date-in-citation></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname> <given-names>J</given-names></name><name><surname>Haidt</surname> <given-names>J</given-names></name><name><surname>Nosek</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Liberals and conservatives rely on different sets of moral foundations</article-title><source>Journal of Personality and Social Psychology</source><volume>96</volume><fpage>1029</fpage><lpage>1046</lpage><pub-id pub-id-type="doi">10.1037/a0015141</pub-id><pub-id pub-id-type="pmid">19379034</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grimaldo</surname> <given-names>F</given-names></name><name><surname>Marušić</surname> <given-names>A</given-names></name><name><surname>Squazzoni</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Fragments of peer review: A quantitative analysis of the literature (1969-2015)</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0193148</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0193148</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haffar</surname> <given-names>S</given-names></name><name><surname>Bazerbachi</surname> <given-names>F</given-names></name><name><surname>Murad</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Peer review bias: a critical review</article-title><source>Mayo Clinic Proceedings</source><volume>94</volume><fpage>670</fpage><lpage>676</lpage><pub-id pub-id-type="doi">10.1016/j.mayocp.2018.09.004</pub-id><pub-id pub-id-type="pmid">30797567</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartley</surname> <given-names>J</given-names></name><name><surname>Pennebaker</surname> <given-names>JW</given-names></name><name><surname>Fox</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Abstracts, introductions and discussions: how far do they differ in style?</article-title><source>Scientometrics</source><volume>57</volume><fpage>389</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1023/A:1025008802657</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Hengel</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Publishing while female: are women held to higher standards? Evidence from peer review</article-title><publisher-name>University of Cambridge</publisher-name><ext-link ext-link-type="uri" xlink:href="https://www.repository.cam.ac.uk/handle/1810/270621">https://www.repository.cam.ac.uk/handle/1810/270621</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kacewicz</surname> <given-names>E</given-names></name><name><surname>Pennebaker</surname> <given-names>JW</given-names></name><name><surname>Davis</surname> <given-names>M</given-names></name><name><surname>Jeon</surname> <given-names>M</given-names></name><name><surname>Graesser</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pronoun use reflects standings in social hierarchies</article-title><source>Journal of Language and Social Psychology</source><volume>33</volume><fpage>125</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1177/0261927X13502654</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karačić</surname> <given-names>J</given-names></name><name><surname>Dondio</surname> <given-names>P</given-names></name><name><surname>Buljan</surname> <given-names>I</given-names></name><name><surname>Hren</surname> <given-names>D</given-names></name><name><surname>Marušić</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Languages for different health information readers: multitrait-multimethod content analysis of Cochrane systematic reviews textual summary formats</article-title><source>BMC Medical Research Methodology</source><volume>19</volume><elocation-id>75</elocation-id><pub-id pub-id-type="doi">10.1186/s12874-019-0716-x</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>CJ</given-names></name><name><surname>Sugimoto</surname> <given-names>CR</given-names></name><name><surname>Zhang</surname> <given-names>G</given-names></name><name><surname>Cronin</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Bias in peer review</article-title><source>Journal of the American Society for Information Science and Technology</source><volume>64</volume><fpage>2</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1002/asi.22784</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magua</surname> <given-names>W</given-names></name><name><surname>Zhu</surname> <given-names>X</given-names></name><name><surname>Bhattacharya</surname> <given-names>A</given-names></name><name><surname>Filut</surname> <given-names>A</given-names></name><name><surname>Potvien</surname> <given-names>A</given-names></name><name><surname>Leatherberry</surname> <given-names>R</given-names></name><name><surname>Lee</surname> <given-names>YG</given-names></name><name><surname>Jens</surname> <given-names>M</given-names></name><name><surname>Malikireddy</surname> <given-names>D</given-names></name><name><surname>Carnes</surname> <given-names>M</given-names></name><name><surname>Kaatz</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Are female applicants disadvantaged in National Institutes of Health peer review? Combining algorithmic text mining and qualitative methods to detect evaluative differences in R01 reviewers' critiques</article-title><source>Journal of Women's Health</source><volume>26</volume><fpage>560</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1089/jwh.2016.6021</pub-id><pub-id pub-id-type="pmid">28281870</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname> <given-names>DM</given-names></name><name><surname>Hancock</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Linguistic obfuscation in fraudulent science</article-title><source>Journal of Language and Social Psychology</source><volume>35</volume><fpage>435</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1177/0261927X15614605</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marsh</surname> <given-names>HW</given-names></name><name><surname>Jayasinghe</surname> <given-names>UW</given-names></name><name><surname>Bond</surname> <given-names>NW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Gender differences in peer reviews of grant applications: A substantive-methodological synergy in support of the null hypothesis model</article-title><source>Journal of Informetrics</source><volume>5</volume><fpage>167</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1016/j.joi.2010.10.004</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paltridge</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>The Discourse of Peer Review: Reviewing Submissions to Academic Journals</source><publisher-loc>London</publisher-loc><publisher-name>Palgrave Macmillan</publisher-name><pub-id pub-id-type="doi">10.1057/978-1-137-48736-0</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Pennebaker</surname> <given-names>JW</given-names></name><name><surname>Boyd</surname> <given-names>RL</given-names></name><name><surname>Jordan</surname> <given-names>K</given-names></name><name><surname>Blackburn</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The development and psychometric properties of LIWC2015</article-title><ext-link ext-link-type="uri" xlink:href="https://repositories.lib.utexas.edu/bitstream/handle/2152/31333/LIWC2015_LanguageManual.pdf">https://repositories.lib.utexas.edu/bitstream/handle/2152/31333/LIWC2015_LanguageManual.pdf</ext-link><date-in-citation iso-8601-date="2020-07-18">July 18, 2020</date-in-citation></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennebaker</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mind mapping: Using everyday language to explore social &amp; psychological processes</article-title><source>Procedia Computer Science</source><volume>118</volume><fpage>100</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2017.11.150</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="software"><person-group person-group-type="author"><collab>R Development Core Team</collab></person-group><year iso-8601-date="2017">2017</year><data-title>R: a language and environment for statistical computing</data-title><version designator="3.6.3">3.6.3</version><publisher-loc>Vienna, Austria</publisher-loc><publisher-name>R Foundation for Statistical Computing</publisher-name><ext-link ext-link-type="uri" xlink:href="https://www.r-project.org/">https://www.r-project.org/</ext-link></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Resnik</surname> <given-names>DB</given-names></name><name><surname>Elmore</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Ensuring the quality, fairness, and integrity of journal peer review: a possible role of editors</article-title><source>Science and Engineering Ethics</source><volume>22</volume><fpage>169</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1007/s11948-015-9625-5</pub-id><pub-id pub-id-type="pmid">25633924</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rinker</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>sentimentr: Calculate text polarity sentiment</data-title><source>Github</source><version designator="2.7.1"> version 2.7.1</version><ext-link ext-link-type="uri" xlink:href="http://github.com/trinker/sentimentr">http://github.com/trinker/sentimentr</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santamaría</surname> <given-names>L</given-names></name><name><surname>Mihaljević</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Comparison and benchmark of name-to-gender inference services</article-title><source>PeerJ Computer Science</source><volume>4</volume><elocation-id>e156</elocation-id><pub-id pub-id-type="doi">10.7717/peerj-cs.156</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sizo</surname> <given-names>A</given-names></name><name><surname>Lino</surname> <given-names>A</given-names></name><name><surname>Reis</surname> <given-names>LP</given-names></name><name><surname>Rocha</surname> <given-names>Á</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An overview of assessing the quality of peer review reports of scientific articles</article-title><source>International Journal of Information Management</source><volume>46</volume><fpage>286</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1016/j.ijinfomgt.2018.07.002</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Socher</surname> <given-names>R</given-names></name><name><surname>Perelygin</surname> <given-names>A</given-names></name><name><surname>Wu</surname> <given-names>J</given-names></name><name><surname>Chuang</surname> <given-names>J</given-names></name><name><surname>Manning</surname> <given-names>CD</given-names></name><name><surname>Ng</surname> <given-names>A</given-names></name><name><surname>Potts</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Recursive deep models for semantic compositionality over a sentiment treebank</article-title><conf-name>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</conf-name><fpage>1631</fpage><lpage>1642</lpage></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Squazzoni</surname> <given-names>F</given-names></name><name><surname>Grimaldo</surname> <given-names>F</given-names></name><name><surname>Marušić</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Publishing: Journals could share peer-review data</article-title><source>Nature</source><volume>546</volume><elocation-id>352</elocation-id><pub-id pub-id-type="doi">10.1038/546352a</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Squazzoni</surname> <given-names>F</given-names></name><name><surname>Ahrweiler</surname> <given-names>P</given-names></name><name><surname>Barros</surname> <given-names>T</given-names></name><name><surname>Bianchi</surname> <given-names>F</given-names></name><name><surname>Birukou</surname> <given-names>A</given-names></name><name><surname>Blom</surname> <given-names>HJJ</given-names></name><name><surname>Bravo</surname> <given-names>G</given-names></name><name><surname>Cowley</surname> <given-names>S</given-names></name><name><surname>Dignum</surname> <given-names>V</given-names></name><name><surname>Dondio</surname> <given-names>P</given-names></name><name><surname>Grimaldo</surname> <given-names>F</given-names></name><name><surname>Haire</surname> <given-names>L</given-names></name><name><surname>Hoyt</surname> <given-names>J</given-names></name><name><surname>Hurst</surname> <given-names>P</given-names></name><name><surname>Lammey</surname> <given-names>R</given-names></name><name><surname>MacCallum</surname> <given-names>C</given-names></name><name><surname>Marušić</surname> <given-names>A</given-names></name><name><surname>Mehmani</surname> <given-names>B</given-names></name><name><surname>Murray</surname> <given-names>H</given-names></name><name><surname>Nicholas</surname> <given-names>D</given-names></name><name><surname>Pedrazzi</surname> <given-names>G</given-names></name><name><surname>Puebla</surname> <given-names>I</given-names></name><name><surname>Rodgers</surname> <given-names>P</given-names></name><name><surname>Ross-Hellauer</surname> <given-names>T</given-names></name><name><surname>Seeber</surname> <given-names>M</given-names></name><name><surname>Shankar</surname> <given-names>K</given-names></name><name><surname>Van Rossum</surname> <given-names>J</given-names></name><name><surname>Willis</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Unlock ways to share data on peer review</article-title><source>Nature</source><volume>578</volume><fpage>512</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1038/d41586-020-00500-y</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Besselaar</surname> <given-names>P</given-names></name><name><surname>Sandström</surname> <given-names>U</given-names></name><name><surname>Schiffbaenker</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Studying grant decision-making: a linguistic analysis of review reports</article-title><source>Scientometrics</source><volume>117</volume><fpage>313</fpage><lpage>329</lpage><pub-id pub-id-type="doi">10.1007/s11192-018-2848-x</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Rooyen</surname> <given-names>S</given-names></name><name><surname>Godlee</surname> <given-names>F</given-names></name><name><surname>Evans</surname> <given-names>S</given-names></name><name><surname>Black</surname> <given-names>N</given-names></name><name><surname>Smith</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effect of open peer review on quality of reviews and on reviewers' recommendations: a randomised trial</article-title><source>BMJ</source><volume>318</volume><fpage>23</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1136/bmj.318.7175.23</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53249.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Hengel</surname><given-names>Erin</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p>Thank you for submitting your article &quot;Meta research: Large-scale language analysis of peer review reports reveals lack of moral bias&quot; to <italic>eLife</italic> for consideration as a Feature Article. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by the <italic>eLife</italic> Features Editor. One of the reviewers was Erin Hengel; the other two reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The paper presents an explorative study of the linguistic content of a very large set (roughly 500,000) of peer review reports. To our knowledge, this is the first paper addressing such a large data set on peer review and the potential biases often experienced by scholars of all fields. This is highly valuable to academic research as a whole. However, we do not believe the evidence warrants the authors' conclusion that peer review lacks moral bias (see below), and we suggest that the authors revise their paper to focus exclusively on the LIWC indicators (and conduct further research into moral bias, informed by the comments from referee #2 at the end of this email, with a view to writing a second manuscript for future submission). There are also a number of other issues that need to be addressed in a revised manuscript.</p><p>Essential revisions:</p><p>1. Reducing the emphasis on moral bias in the present manuscript will involve deleting the last 10 rows of table 1 (and similarly for tables 2, 3 and 4), and making changes to the text. The title also needs to be changed, but Figure 1 does not need to be changed.</p><p>2. The authors should analyze the data they have for word length, analytical thinking, clout, authenticity, emotional tone and morality in general in greater detail through regression analyses and/or multilevel models.</p><p>Here is what referee #1 said on this topic: I am concerned that the explorative approach misses a lot of underlying information. By comparing means of groups, the in-group variance is overlooked. Some of the observed effects might be large enough to hold some sort of universal &quot;truth&quot;, but for other cases, substantial effects might still exist within groups. My suggestion would be to rethink the design into one that allows for multidimensionality. One approach could be regression analysis, which would require a more strict type of test-design though. Another approach could be to build reviewer-profiles based on their characteristics. E.g. reviews with a high degree of clout and negative emotion, and low analytical thinking could be one type. Where is this type found? What characterizes it? Additionally, this would allow the authors to include information about the reviewer, e.g. are they always negative? This might also solve the baseline problem, as this would be a classification issue rather than a measurement issue.</p><p>Here is what referee #3 said on this topic: All the analyses are descriptive ANOVAs and the like (bivariate). Given the supposed quality of the data, I'd recommend they explore multilevel models. Then they can fix out differences by fields, journals, etc so we get a more clear sense of what's going on.</p><p>3. Please include a fuller description of your dataset and describe how representative/biased the sampling is by field and type of journal.</p><p>4. Please provide more information on what the LIWC measures and how it is calculated? It would be especially helpful if you showed several LIWC scores together with sample texts (preferably from your own database) to illustrate how well it analyses text. This can be shown in an appendix. If you can't show text from your own database due to privacy concerns, feel free to show passages from this report. (Alternatively, you could take a few reports from, say, the BMJ which makes referee reports publicly available for published papers.)</p><p>5. What are baseline effects? What kind of changes should we expect? E.g. when arguing that the language is cold and analytical, what is this comparable to? I would expect most scientific writing to be mostly in this category, and it should not be a surprise - I hope. It would be very useful for the reader to have some type of comparison.</p><p>6. Does the analysis allow for the fact that different subject areas will have different LIWC scores? The largest sample of reports comes from the physical sciences, which use single-blind review the most. Reports from this field are also shorter, slightly more analytic and display less Clout and Authenticity. I think your results are picking up this selection bias instead of representing actual differences between the two review processes.</p><p>Please discuss.</p><p>7. Please add a section on the limitations of your study. For example, there is no discussion of sample bias and representation really.</p><p>Also, LIWC is full of issues and problems (NLP has come a ways since LIWC arrived on the scene): do you use the standardized version of LIWC constructs with high cronbach alphas, or the raw versions with poor cronbach alphas?</p><p>Does the analysis distinguish between original research content and other forms of content (eg, editorials, opinion pieces, book reviews etc)?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53249.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[We repeat the reviewers’ points here in italic, and include our replies point by point in Roman.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. Reducing the emphasis on moral bias in the present manuscript will involve deleting the last 10 rows of table 1 (and similarly for tables 2, 3 and 4), and making changes to the text. The title also needs to be changed, but Figure 1 does not need to be changed.</p></disp-quote><p>Thank you for your comment. We revised the tables according to the reviewers’ recommendations and created new tables where we present multilevel description of the review reports based on the interaction effects of the reviewer recommendation, journal discipline, journal peer review type and reviewer gender. We created the tables for the five LIWC variables (word count, analytical tone, clout tone, authenticity and emotional tone), and two new additional measures (SentimentR- an R package that has its own dictionaries for the emotional tone of the text, and Stanford CoreNLP-a deep language analysis software, which served as the concurrent validity assessment of the tone variables from the LIWC package). All multilevel relations are now presented in new figures, which are the results of the mixed methods linear regression where we controlled the random effect of the journal, word count (except for LIWC word count) and article type. In light of the new results, and coherently with referee recommendations, we introduced a change in the title and replaced the Figure 1. We now have seven new graphs describing linguistic characteristics of the reviews between groups in the main text and eleven graphs presenting the moral variables in the Supplementary file.</p><disp-quote content-type="editor-comment"><p>2. The authors should analyze the data they have for word length, analytical thinking, clout, authenticity, emotional tone and morality in general in greater detail through regression analyses and/or multilevel models.</p></disp-quote><p>Thank you for your comment. As mentioned above, we performed a new analysis, where we used the mixed methods approach with reviewer recommendation, journal discipline, journal peer review type and reviewer gender as predictors (fixed factors) and different journals, word count and article type as the random factor, which would enable us to control the variations between journals (there were 61 journals in total from which some were more represented than others, and the majority of the articles were original research articles). We found significant interaction of the reviewer recommendation, journal’s field of research, the type of peer review and reviewer’s gender in each variable assessment, but we understand that this significance could be due to the large sample size. So, we presented figures with the within-group relations on standardized scales where we presented the differences between groups.</p><disp-quote content-type="editor-comment"><p>Here is what referee #1 said on this topic: I am concerned that the explorative approach misses a lot of underlying information. By comparing means of groups, the in-group variance is overlooked. Some of the observed effects might be large enough to hold some sort of universal &quot;truth&quot;, but for other cases, substantial effects might still exist within groups. My suggestion would be to rethink the design into one that allows for multidimensionality. One approach could be regression analysis, which would require a stricter type of test-design though. Another approach could be to build reviewer-profiles based on their characteristics. E.g. reviews with a high degree of clout and negative emotion, and low analytical thinking could be one type. Where is this type found? What characterizes it? Additionally, this would allow the authors to include information about the reviewer, e.g. are they always negative? This might also solve the baseline problem, as this would be a classification issue rather than a measurement issue.</p></disp-quote><p>Thank you for the comment. Excellent point. We re-performed the analysis accordingly. The mixed methods approach revealed that the majority of effects of the differences in the writing style of the reviews can be attributed to reviewer recommendations, much less to the journal’s field of research, the type of peer review type and reviewer’s gender. We tried to provide an overview of the general writing style in peer reviews by presenting relevant variables in the same graphs so that a reader can have an overview about what peer review characteristics predict different language styles.</p><disp-quote content-type="editor-comment"><p>Here is what referee #3 said on this topic: All the analyses are descriptive ANOVAs and the like (bivariate). Given the supposed quality of the data, I'd recommend they explore multilevel models. Then they can fix out differences by fields, journals, etc so we get a clearer sense of what's going on.</p></disp-quote><p>As explained above, we performed mixed methods approach where these effects were analysed jointly. The current analysis provides an overview of the interaction effects in peer review characteristics and sizes of the differences between them.</p><disp-quote content-type="editor-comment"><p>3. Please include a fuller description of your dataset and describe how representative/biased the sampling is by field and type of journal.</p></disp-quote><p>Thank you, we added this both to the methods and the limitations in the Discussion section.</p><disp-quote content-type="editor-comment"><p>4. Please provide more information on what the LIWC measures and how it is calculated? It would be especially helpful if you showed several LIWC scores together with sample texts (preferably from your own database) to illustrate how well it analyses text. This can be shown in an appendix. If you can't show text from your own database due to privacy concerns, feel free to show passages from this report. (Alternatively, you could take a few reports from, say, the BMJ which makes referee reports publicly available for published papers.)</p></disp-quote><p>LIWC has a dictionary with words associated with different tone, and it counts number of words for each tone type in a certain text. The LIWC output is the percentage of words from a tone category in the text. We now provided the calculation of the different tone variables in the Supplementary file, both for high and low levels of tone. The examples are anonymized.</p><disp-quote content-type="editor-comment"><p>5. What are baseline effects? What kind of changes should we expect? E.g. when arguing that the language is cold and analytical, what is this comparable to? I would expect most scientific writing to be mostly in this category, and it should not be a surprise - I hope. It would be very useful for the reader to have some type of comparison.</p></disp-quote><p>Another good point. The results in our study were similar to an unpublished study that focused on the analysis of the peer review linguistic characteristics, but on a much smaller sample and in only in a single journal (<ext-link ext-link-type="uri" xlink:href="https://peerreviewcongress.org/prc17-0234">https://peerreviewcongress.org/prc17-0234</ext-link>). However, with the new methodological approach we looked at the relationship of the linguistic characteristics and different aspects of peer review process and found important differences. The analytical tone was indeed predominant in all types of peer review reports, but we found differences in other linguistic characteristics. The new results are presented in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>6. Does the analysis allow for the fact that different subject areas will have different LIWC scores? The largest sample of reports comes from the physical sciences, which use single-blind review the most. Reports from this field are also shorter, slightly more analytic and display less Clout and Authenticity. I think your results are picking up this selection bias instead of representing actual differences between the two review processes.</p></disp-quote><p>Thank you for your comment. The dataset characteristics are now described in the limitations in the Discussion section and we are aware that there is a higher prevalence of journals from Physical sciences, double blind reviews and those which asked for revisions. However, the new analyses now include the interaction of peer review characteristics and so we introduced a better control for this selection bias.</p><disp-quote content-type="editor-comment"><p>7. Please add a section on the limitations of your study. For example, there is no discussion of sample bias and representation really.</p></disp-quote><p>As mentioned previously, we added a limitation section to the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Also, LIWC is full of issues and problems (NLP has come a ways since LIWC arrived on the scene): do you use the standardized version of LIWC constructs with high cronbach alphas, or the raw versions with poor cronbach alphas?</p></disp-quote><p>The LIWC version we used is the standardized version with high Cronbach alphas. This has now been clarified in the Methods section of the revised manuscript. We also analysed the data using Stanford CoreNLP deep learning tool in order to increase internal validity of our approach.</p><disp-quote content-type="editor-comment"><p>Does the analysis distinguish between original research content and other forms of content (eg, editorials, opinion pieces, book reviews etc)?</p></disp-quote><p>There were no book reviews in the dataset. However, we did make the distinction between the original articles and other formats (There was the total of 388,737 original articles and 83,972 of other types or articles of those included in the mixed model analyses), which is now described in the Methods. We used this as the random factor in the mixed model linear regression.</p></body></sub-article></article>