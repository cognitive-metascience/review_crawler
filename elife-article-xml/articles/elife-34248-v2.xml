<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">34248</article-id><article-id pub-id-type="doi">10.7554/eLife.34248</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Striatal action-value neurons reconsidered</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-102790"><name><surname>Elber-Dorozko</surname><given-names>Lotem</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1235-8651</contrib-id><email>lotem.elber@mail.huji.ac.il</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-103339"><name><surname>Loewenstein</surname><given-names>Yonatan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2577-2317</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">The Edmond &amp; Lily Safra Center for Brain Sciences</institution><institution>The Hebrew University of Jerusalem</institution><addr-line><named-content content-type="city">Jerusalem</named-content></addr-line><country>Israel</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Neurobiology, The Alexander Silberman Institute of Life Sciences</institution><institution>The Hebrew University of Jerusalem</institution><addr-line><named-content content-type="city">Jerusalem</named-content></addr-line><country>Israel</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">The Federmann Center for the Study of Rationality</institution><institution>The Hebrew University of Jerusalem</institution><addr-line><named-content content-type="city">Jerusalem</named-content></addr-line><country>Israel</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-1044"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Reviewing Editor</role><aff id="aff4"><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>31</day><month>05</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e34248</elocation-id><history><date date-type="received" iso-8601-date="2017-12-11"><day>11</day><month>12</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-05-13"><day>13</day><month>05</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Elber-Dorozko et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Elber-Dorozko et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-34248-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.34248.001</object-id><p>It is generally believed that during economic decisions, striatal neurons represent the values associated with different actions. This hypothesis is based on studies, in which the activity of striatal neurons was measured while the subject was learning to prefer the more rewarding action. Here we show that these publications are subject to at least one of two critical confounds. First, we show that even weak temporal correlations in the neuronal data may result in an erroneous identification of action-value representations. Second, we show that experiments and analyses designed to dissociate action-value representation from the representation of other decision variables cannot do so. We suggest solutions to identifying action-value representation that are not subject to these confounds. Applying one solution to previously identified action-value neurons in the basal ganglia we fail to detect action-value representations. We conclude that the claim that striatal neurons encode action-values must await new experiments and analyses.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>action-value</kwd><kwd>striatum</kwd><kwd>temporal correlations</kwd><kwd>statistical confounds</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>757/16</award-id><principal-award-recipient><name><surname>Loewenstein</surname><given-names>Yonatan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>CRC1080</award-id><principal-award-recipient><name><surname>Loewenstein</surname><given-names>Yonatan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000324</institution-id><institution>Gatsby Charitable Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Loewenstein</surname><given-names>Yonatan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The numerous reports in support of action-value representation in the striatum are based on statistical analyses that are subject to two critical confounds and, thus, this long-held belief of striatal action-value representation should be retested using different experiments and analyses.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p>There is a long history of operant learning experiments, in which a subject, human or animal, repeatedly chooses between actions and is rewarded according to its choices. A popular theory posits that the subject’s decisions in these tasks utilize estimates of the different <italic>action-values</italic>. These action-values correspond to the expected reward associated with each of the actions, and actions associated with a higher estimated action-value are more likely to be chosen (<xref ref-type="bibr" rid="bib60">Sutton and Barto, 1998</xref>). In recent years, there is a lot of interest in the neural mechanisms underlying this computation (<xref ref-type="bibr" rid="bib39">Louie and Glimcher, 2012</xref>; <xref ref-type="bibr" rid="bib55">Schultz, 2015</xref>). In particular, based on electrophysiological, functional magnetic resonance imaging (fMRI) and intervention experiments, it is now widely accepted that a population of neurons in the striatum represents these action-values, adding sway to this action-value theory (<xref ref-type="bibr" rid="bib4">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>; <xref ref-type="bibr" rid="bib16">Funamizu et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Guitart-Masip et al., 2012</xref>; <xref ref-type="bibr" rid="bib21">Her et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>; <xref ref-type="bibr" rid="bib25">2015a</xref>; <xref ref-type="bibr" rid="bib26">Ito and Doya, 2015b</xref>; <xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib30">Kim et al., 2012</xref>; <xref ref-type="bibr" rid="bib31">2007</xref>; <xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib34">Lee et al., 2015</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>; <xref ref-type="bibr" rid="bib59">Stalnaker et al., 2010</xref>; <xref ref-type="bibr" rid="bib61">Tai et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>; <xref ref-type="bibr" rid="bib69">Wunderlich et al., 2009</xref>). Here we challenge the evidence for action-value representation in the striatum by describing two major confounds in the interpretation of the data that have not yet been successfully addressed.</p><p>To identify neurons that represent the values the subject associates with the different actions, researchers have searched for neurons whose firing rate is significantly correlated with the average reward associated with exactly one of the actions. There are several ways of defining the average reward associated with an action. For example, the average reward can be defined by the reward schedule, for example, the probability of a reward associated with the action. Alternatively, one can adopt the subject’s perspective, and use the subject-specific history of rewards and actions in order to estimate the average reward. In particular, the Rescorla–Wagner model (equivalent to the standard ones-state Q-learning model) has been used to estimate action-values (<xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>). In this model, the value associated with an action <inline-formula><mml:math id="inf1"><mml:mi>i</mml:mi></mml:math></inline-formula> in trial <inline-formula><mml:math id="inf2"><mml:mi>t</mml:mi></mml:math></inline-formula>, termed <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, is an exponentially-weighted average of the rewards associated with this action in past trials:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ2"><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf4"><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> denote the choice and reward in trial <inline-formula><mml:math id="inf6"><mml:mi>t</mml:mi></mml:math></inline-formula>, respectively, and <inline-formula><mml:math id="inf7"><mml:mi>α</mml:mi></mml:math></inline-formula> is the learning rate.</p><p>The model also posits that in a two-alternative task, the probability of choosing an action is a sigmoidal function, typically softmax, of the difference of the action-values (see also [<xref ref-type="bibr" rid="bib58">Shteingart and Loewenstein, 2014</xref>]):<disp-formula id="equ3"><label>(2)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf8"><mml:mi>β</mml:mi></mml:math></inline-formula> is a parameter that determines the bias towards the action associated with the higher action-value. The parameters of the model, <inline-formula><mml:math id="inf9"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"><mml:mi>β</mml:mi></mml:math></inline-formula>, can be estimated from the behavior, allowing the researchers to compute <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> on a trial-by-trial basis.</p><p>In principle, one can identify the neurons that represent an action-value by identifying neurons for which the regression of the trial-by-trial spike count on one of the variables <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is statistically significant. Using this framework, electrophysiological studies have found that the firing rate of a substantial fraction of striatal neurons (12–40% for different significance thresholds) is significantly correlated with an action-value. These and similar results were considered as evidence that neurons in the striatum represent action-values (<xref ref-type="bibr" rid="bib16">Funamizu et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Her et al., 2016</xref>; <xref ref-type="bibr" rid="bib25">Ito and Doya, 2015a</xref>; <xref ref-type="bibr" rid="bib26">Ito and Doya, 2015b</xref>; <xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>).</p><p>In this paper we conduct a systematic literature search and conclude that the literature has, by and large, ignored two major confounds in this and in similar analyses. First, it is well-known that spurious correlations can emerge in correlation analysis if both variables have temporal correlations (<xref ref-type="bibr" rid="bib18">Granger and Newbold, 1974</xref>; <xref ref-type="bibr" rid="bib53">Phillips, 1986</xref>). Here we show that neurons can be erroneously classified as representing action-values when their firing rates are weakly temporally correlated. Second, it is also well-known that lack of a statistically significant result in the analysis does not imply lack of correlation. Because in standard analyses neurons are classified as representing action-values if they have a significant regression coefficient on <italic>exactly</italic> one action-value and because decision variables such as policy are correlated with both action-values, neurons representing other decision variables may be misclassified as representing action-values. We propose different approaches to address these issues. Applying one of them to recordings from the basal ganglia, we fail to identify any action-value representation there. Thus, we conclude that the hypothesis that striatal neurons represent action-values still remains to be tested by experimental designs and analyses that are not subject to these confounds. In the Discussion we address additional conceptual issues with identifying such a representation.</p><p>This paper discusses methodological problems that may also be of relevance in other fields of biology in general and neuroscience in particular. Nevertheless, the focus of this paper is a single scientific claim, namely, that action-value representation in the striatum is an established fact. Our criticism is restricted to the representation of action-values, and we do not make any claims regarding the possible representations of other decision variables, such as policy, chosen-value or reward-prediction-error. This we leave for future studies. Moreover, we do not make any claims about the possible representations of action-values elsewhere in the brain, although our results suggest caution when looking for such representations.</p><p>The paper is organized in the following way. We commence by describing a standard method for identifying action-value neurons. Next, we show that this method erroneously classifies simulated neurons, whose activity is temporally correlated, as representing action-values. We show that this confound brings into question the conclusion of many existing publications. Then, we propose different methods for identifying action-value neurons, that overcome this confound. Applying such a method to basal ganglia recordings, in which action-value neurons were previously identified, we fail to conclusively detect any action-value representations. We continue by discussing the second confound: neurons that encode the policy (the probability of choice) may be erroneously classified as representing action-value, even when the policy is the result of learning algorithms that are devoid of action-value calculation. Then we discuss a possible solution to this confound.</p><sec id="s1" sec-type="results"><title>Results</title><sec id="s1-1"><title>Identifying action-value neurons</title><p>We commence by examining the standard methods for identifying action-value neurons using a simulation of an operant learning experiment. We simulated a task, in which the subject repeatedly chooses between two alternative actions, which yield a binary reward with a probability that depends on the action. Specifically, each session in the simulation was composed of four blocks such that the probabilities of rewards were fixed within a block and varied between the blocks. The probabilities of reward in the blocks were (0.1,0.5), (0.9,0.5), (0.5,0.9) and (0.5,0.1) for actions 1 and 2, respectively (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The order of blocks was random and a block terminated when the more rewarding action was chosen more than 14 times within 20 consecutive trials (<xref ref-type="bibr" rid="bib25">Ito and Doya, 2015a</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.34248.002</object-id><label>Figure 1.</label><caption><title>Model of action-value neurons.</title><p>(<bold>A</bold>) Behavior of the model in an example session, composed of four blocks (separated by dashed vertical lines). The probabilities of reward for choosing actions 1 and 2 are denoted by the pair of numbers above the block. Black line denotes the probability of choosing action 1; vertical lines denote choices in individual trials, where red and blue denote actions 1 and 2, respectively, and long and short lines denote rewarded and unrewarded trials, respectively. (<bold>B</bold>) Neural activity. Firing rate (line) and spike-count (dots) of two example simulated action-value neurons in the session depicted in (<bold>A</bold>). The red and blue-labeled neurons represent <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, respectively. Black horizontal lines denote the mean spike count in the last 20 trials of the block. Error bars denote the standard error of the mean. The two asterisks denote p&lt;0.01 (rank sum test). (<bold>C</bold>) Values. Thick red and blue lines denote <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, respectively. Note that the firing rates of the two neurons in (<bold>B</bold>) are a linear function of these values. Thin red and blue lines denote the estimates of <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, respectively, based on the choices and rewards in (<bold>A</bold>). The similarity between the thick and thin lines indicates that the parameters of the model can be accurately estimated from the behavior (see also Materials and methods). (<bold>D</bold>) and (<bold>E</bold>) Population analysis. (<bold>D</bold>) Example of 500 simulated action-value neurons from randomly chosen sessions. Each dot corresponds to a single neuron and the coordinates correspond to the t-values of the regression of the spike counts on the estimated values of the two actions. Dashed lines at t=2 denote the significance boundaries. Color of dots denote significance: dark red and blue denote a significant regression coefficient on exactly one estimated action-value, action 1 or action 2, respectively; light blue – significant regression coefficients on both estimated action-values with similar signs (<inline-formula><mml:math id="inf20"><mml:mi>Σ</mml:mi><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>); orange - significant regression coefficients on both estimated action-values with opposite signs (<inline-formula><mml:math id="inf21"><mml:mi>Δ</mml:mi><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>); Black – no significant regression coefficients. The two simulated neurons in (<bold>B</bold>) are denoted by squares. (<bold>E</bold>) Fraction of neurons in each category, estimated from 20,000 simulated neurons in 1,000 sessions. Error bars denote the standard error of the mean. Dashed lines denote the naïve expected false positive rate from the significance threshold (see Materials and methods).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig1-v2"/></fig><p>To simulate learning behavior, we used the Q-learning framework (<xref ref-type="disp-formula" rid="equ1 equ3">Equations 1 and 2</xref> with <inline-formula><mml:math id="inf22"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf23"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn></mml:math></inline-formula> (taken from distributions reported in [<xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>]) and initial conditions <inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>). As demonstrated in <xref ref-type="fig" rid="fig1">Figure 1A</xref>, the model learned: the probability of choosing the more rewarding alternative increased over trials (black line). To model the action-value neurons, we simulated neurons whose firing rate is a linear function of one of the two Q-values and whose spike count in a 1 sec trial is randomly drawn from a corresponding Poisson distribution (see Materials and methods). The firing rates and spike counts of two such neurons, representing action-values 1 and 2, are depicted in <xref ref-type="fig" rid="fig1">Figure 1B</xref> in red and blue, respectively.</p><p>One standard method for identifying action-value neurons is to compare neurons' spike counts after learning, at the end of the blocks (horizontal bars in <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Considering the red-labeled Poisson neuron, the spike count in the last 20 trials of the second block, in which the probability of reward associated with action 1 was 0.9, was significantly higher than that count in the first block, in which the probability of reward associated with action 1 was 0.1 (p&lt;0.01; rank sum test). By contrast, there was no significant difference in the spike counts between the third and fourth blocks, in which the probability of reward associated with action 1 was equal (p=0.91; rank sum test). This is consistent with the fact that the red-labeled neuron was an action 1-value neuron: its firing rate was a linear function of the value of action 1 (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, red) Similarly for the blue labeled neuron, the spike counts in the last 20 trials of the first two blocks were not significantly different (p=0.92; rank sum test), but there was a significant difference in the counts between the third and fourth blocks (p&lt;0.001; rank sum test). These results are consistent with the probabilities of reward associated with action 2 and the fact that in our simulations, this neuron’s firing rate was modulated by the value of action 2 (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, blue).</p><p>This approach for identifying action-value neurons is limited, however, for several reasons. First, it considers only a fraction of the data, the last 20 trials in a block. Second, action-value neurons are not expected to represent the block average probabilities of reward. Rather, they will represent a subjective estimate, which is based on the subject-specific history of actions and rewards. Therefore, it is more common to identify action-value neurons by regressing the spike count on subjective action-values, estimated from the subject’s history of choices and rewards (<xref ref-type="bibr" rid="bib16">Funamizu et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Ito and Doya, 2015a</xref>; <xref ref-type="bibr" rid="bib26">Ito and Doya, 2015b</xref>; <xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>). Note that when studying behavior in experiments, we have no direct access to these estimated action-values, in particular because the values of the parameters <inline-formula><mml:math id="inf25"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:mi>β</mml:mi></mml:math></inline-formula> are unknown. Therefore, following common practice, we estimated the values of <inline-formula><mml:math id="inf27"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf28"><mml:mi>β</mml:mi></mml:math></inline-formula> from the model’s sequence of choices and rewards using maximum likelihood, and used the estimated learning rate (<inline-formula><mml:math id="inf29"><mml:mi>α</mml:mi></mml:math></inline-formula>) and the choices and rewards to estimate the action-values (thin lines in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, see Materials and methods). These estimates were similar to the true action-value, which underlay the model’s choice behavior (thick lines in <xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>Next, we regressed the spike count of each simulated neuron on the two estimated action-values from its corresponding session. As expected, the t-value of the regression coefficient of the red-labeled action 1-value neuron was significant for the estimated <inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> <inline-formula><mml:math id="inf31"><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>182</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>4.05</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> but not for the estimated <inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> <inline-formula><mml:math id="inf33"><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>182</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.27</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>. Similarly, the t-value of the regression coefficient of the blue-labeled action 2-value neuron was significant for the estimated <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> <inline-formula><mml:math id="inf35"><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>182</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>3.05</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> but not for the estimated <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> <inline-formula><mml:math id="inf37"><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>182</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.78</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>.</p><p>A population analysis of the t-values of the two regression coefficients is depicted in <xref ref-type="fig" rid="fig1">Figure 1D,E</xref>. As expected, a substantial fraction (42%) of the simulated neurons were identified as action-value neurons. Only 2% of the simulated neurons had significant regression coefficients with both action-values. Such neurons are typically classified as state <inline-formula><mml:math id="inf38"><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> or policy (also known as preference) <inline-formula><mml:math id="inf39"><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> neurons, if the two regression coefficients have the same or different signs, respectively (<xref ref-type="bibr" rid="bib25">Ito and Doya, 2015a</xref>). Note that despite the fact that by construction, all neurons were action-value neurons, not all of them were detected as such by this method. This failure occurred for two reasons. First, the estimated action-values are not identical to the true action-values, which determine the firing rates. This is because of the finite number of trials and the stochasticity of choice (note the difference, albeit small, between the thin and thick lines in <xref ref-type="fig" rid="fig1">Figure 1C</xref>). Second and more importantly, the spike count in a trial is only a noisy estimate of the firing rate because of the Poisson generation of spikes.</p><p>Several prominent studies have implemented the methods we described in this section and reported that a substantial fraction (10–40% depending on significance threshold) of striatal neurons represent action-values (<xref ref-type="bibr" rid="bib25">Ito and Doya, 2015a</xref>; <xref ref-type="bibr" rid="bib26">Ito and Doya, 2015b</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>). In the next two sections we show that these methods, and similar methods employed by other studies (<xref ref-type="bibr" rid="bib4">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>; <xref ref-type="bibr" rid="bib16">Funamizu et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Guitart-Masip et al., 2012</xref>; <xref ref-type="bibr" rid="bib21">Her et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>; <xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib30">Kim et al., 2012</xref>; <xref ref-type="bibr" rid="bib31">2007</xref>; <xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib59">Stalnaker et al., 2010</xref>; <xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>; <xref ref-type="bibr" rid="bib69">Wunderlich et al., 2009</xref>) are all subject to at least one of two major confounds.</p></sec><sec id="s1-2"><title>Confound 1 – temporal correlations</title><sec id="s1-2-1"><title>Simulated random-walk neurons are erroneously classified as action-value neurons</title><p>The red and blue-labeled neurons in <xref ref-type="fig" rid="fig1">Figure 1D</xref> were classified as action-value neurons because their t-values were improbable under the null hypothesis that the firing rate of the neuron is not modulated by action-values. The significance threshold (<italic>t</italic> = 2) was computed assuming that trials are independent in time. To see why this assumption is essential, we consider a case in which it is violated. <xref ref-type="fig" rid="fig2">Figure 2A</xref> depicts the firing rates and spike counts of two simulated Poisson neurons, whose firing rates follow a bounded Gaussian random-walk process:<disp-formula id="equ4"><label>(3)</label><mml:math id="m4"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:math></disp-formula></p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.34248.003</object-id><label>Figure 2.</label><caption><title>Erroneous detection of action-value representation in random-walk neurons.</title><p>(<bold>A</bold>) Two example random-walk neurons that appear as if they represent action-values. The red (top) and blue (bottom) lines denote the estimated action-values 1 and 2, respectively that were depicted in <xref ref-type="fig" rid="fig1">Figure 1C</xref>. Gray lines and gray dots denote the firing rates and the spike counts of two example random-walk neurons that were randomly assigned to this simulated session. Black horizontal lines denote the mean spike count in the last 20 trials of each block. Error bars denote the standard error of the mean. The two asterisks denote p&lt;0.01 (rank sum test). (<bold>B</bold>) and (<bold>C</bold>) Population analysis. Each random-walk neuron was regressed on the two estimated action-values, as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>. Numbers and legend are the same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>. The two random-walk neurons in (<bold>A</bold>) are denoted by squares in (<bold>B</bold>). Dashed lines in (<bold>B</bold>) at t=2 denote the significance boundaries. Dashed lines in (<bold>C</bold>) denote the naïve expected false positive rate from the significance threshold (see Materials and methods). (<bold>D</bold>) Fraction of random-walk neurons classified as action-value neurons (red), and classified as state neurons (<inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>Q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) or policy neurons (<inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) (green) as a function of the magnitude of the diffusion parameter of random-walk (<inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). Light red and light green are standard error of the mean. Dashed lines denote the results for <inline-formula><mml:math id="inf43"><mml:mi>σ</mml:mi></mml:math></inline-formula>=0.1, which is the value of the diffusion parameter used in (<bold>A</bold>)-(<bold>C</bold>). Initial firing rate for all neurons in the simulations is <inline-formula><mml:math id="inf44"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn></mml:math></inline-formula>Hz.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.004</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Erroneous detection of action-value representation in a model with covariance based synaptic plasticity.</title><p>(<bold>A</bold>) An example of operant learning by the covariance model (see Materials and methods). Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. (<bold>B</bold>) Two example covariance neurons that appear is if they represent action-values. The red (top) and blue (bottom) lines denote the calculated action-values 1 and 2, respectively, that were computed from the behavior of the model. Gray lines and gray dots denote the firing rates and the spike counts of two example covariance neurons. Black horizontal lines denote the mean spike count in the last 20 trials of each block. Error bars denote the standard error of the mean. The two asterisks denote p&lt;0.01 (rank sum test). Legend is the same as in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. (<bold>C</bold>) and (<bold>D</bold>) Population analysis. Same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>. Each simulated neuron was regressed on the computed action-values. The two simulated neurons in (<bold>B</bold>) are denoted by squares in (<bold>C</bold>). Results in (<bold>D</bold>) are based on 500 sessions with 2000 simulated neurons in a session. Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>. Dashed lines in (<bold>C</bold>) at t = 2 denote the significance boundaries. Dashed lines in (<bold>D</bold>) denote the naïve expected false positive rate from the significance threshold (see Materials and methods). Error bars denote the standard error of the mean. Following this standard approach, 43% of the covariance neurons were erroneously classified as representing action-values.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig2-figsupp1-v2"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.005</object-id><label>Figure 2—figure supplement 2.</label><caption><title>Erroneous detection of action-value neurons in unrelated experiments.</title><p>(<bold>A</bold>) and (<bold>B</bold>) motor cortex neurons (<bold>A</bold>) An example motor cortex neuron recorded in a BMI task, presented as if the sequence of spike counts of this neuron corresponds to the sequence of trials in a randomly chosen session of operant learning from the sessions used for the population analysis in <xref ref-type="fig" rid="fig1">Figure 1E</xref>. Gray dots denote the spike-counts. Black horizontal line denotes the mean spike counts in the last 20 trials of the assigned blocks. Error bars denote the standard error of the mean. The two asterisks denote p&lt;0.01 (rank sum test). For each neuron, we computed the t-values of the regression of the spike count on the two corresponding estimated action-values. The red line denotes the action-value whose t-value exceeded 2 (in absolute value). (<bold>B</bold>) Population analysis. Left scatter plot, the t-values of 89 neurons regressed on the estimated action-values of 89 randomly selected sessions (same as <xref ref-type="fig" rid="fig1">Figure 1D</xref>). The neuron in (<bold>A</bold>) is denoted by a square. Dashed lines at t = 2 denote the significance boundaries. Right bar chart, fraction of neurons classified in each category, estimated by regressing each of the 89 motor cortex neurons on 80 different estimated action-values from 40 randomly selected sessions. Dashed lines denote the naïve expected false positive rate from the significance threshold (see Materials and methods). Error bars denote the standard error of the mean. Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>. (<bold>C</bold>) and (<bold>D</bold>) auditory cortex neurons from (<xref ref-type="bibr" rid="bib22">Hershenhoren et al., 2014</xref>). (<bold>C</bold>) Same as in (<bold>A</bold>) for an auditory cortex neuron in an anesthetized rat responding to the presentation of pure tones. (<bold>D</bold>) Population analysis. Left scatter plot, the t-values of 82 recorded sessions from auditory neurons regressed on the estimated action-values of 82 randomly selected sessions (same as (<bold>B</bold>)). The neuron in (<bold>C</bold>) is denoted by a square. Right bar chart, fraction of neurons classified in each category, estimated by regressing 125 recorded sessions from auditory cortex neurons on 80 different estimated action-values from 40 randomly selected sessions (in each session, 34% of recordings were excluded on average, see Materials and methods). Error bars denote the standard error of the mean. Following this standard approach, 36% of the motor cortex neurons and 23% of the auditory cortex neurons were erroneously classified as representing action-values. These results demonstrate that the magnitude of non-stationarity in standard electrophysiological recordings is sufficient to result in an erroneous identification of neurons as representing action-values.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig2-figsupp2-v2"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.006</object-id><label>Figure 2—figure supplement 3.</label><caption><title>Erroneous detection of unrelated action-value representations in basal ganglia neurons.</title><p>Population analysis on basal ganglia neurons. Spike counts were regressed on estimated action-values that were created in the same experimental setting as in <xref ref-type="fig" rid="fig1">Figure 1</xref>. To compare with the number of blocks and trials used in the original analysis, we simulated sessions with more blocks, so that the original four blocks were repeated in random permutation, and with a weaker bias towards the higher-valued action (<inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). The average number of blocks and trials used in this analysis is 6 and 516.5, respectively. Left scatter plot, the t-values of the 214 neurons from (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>) in three different phases regressed on the estimated action-values from 642 randomly selected simulated sessions (same analysis as in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B,D</xref>). Dashed lines at t = 2 denote the significance boundaries. Right bar chart, fraction of neurons classified in each category, estimated by regressing the 214 neurons in three different phases on 80 different estimated action-values from 40 randomly selected sessions (see Materials and methods). Dashed lines denote the naïve expected false positive rate from the significance threshold (see Materials and methods). Error bars denote the standard error of the mean. Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>. This analysis erroneously classified 43% of the neurons as action-value neurons, despite the fact that these action-values were completely unrelated to the experimental session in which these neurons were recorded.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig2-figsupp3-v2"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.007</object-id><label>Figure 2—figure supplement 4.</label><caption><title>Spike count permutation (as in [<xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>]) does not resolve the temporal correlations confound.</title><p>Conducting this analysis using the four data sets and unrelated, simulated action-values we erroneously detect action-value representation in all data sets. (<bold>A</bold>) (<bold>B</bold>) (<bold>C</bold>) and (<bold>D</bold>) denote the random-walk neurons, motor cortex neurons, auditory cortex neurons and basal ganglia neurons, respectively. Left, t-values from regressions of the original spike-count on the estimated action-values. Green triangles denote significant modulation by action-value according to the permuted spike-count analysis (see Materials and methods). Legend otherwise is the same as in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. Dashed black lines at t=2 denote the significance boundaries that would be used in the standard analysis. Right, fraction of neurons significantly modulated by action-value according to the permuted spike-count analysis across the population (0.05, expected by chance, is denoted by a horizontal dashed line). Error bars are standard error of the mean. Number of neurons used in (<bold>A</bold>) (<bold>B</bold>) (<bold>C</bold>) and (<bold>D</bold>) is the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A–B</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C–D</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, respectively. Note that in all four cases the two t-values are correlated. This results from the correlation between <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> caused by the reward schedule in (<xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig2-figsupp4-v2"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.008</object-id><label>Figure 2—figure supplement 5.</label><caption><title>Autoregressive coefficients do not resolve the temporal correlations confound.</title><p>Following (<xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>), we simulated the experimental settings in (<xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>) to create simulated action-values (see also Meterials and methods on <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>, with the same experimental design, but with a different statistical analysis) and conducted a multiple linear regression analysis on each of the unrelated data sets, using the simulated action-values and the following regression model: <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the spike count in trial <inline-formula><mml:math id="inf50"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> are the estimated action-values in trial <inline-formula><mml:math id="inf53"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf54"><mml:mi>C</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the action chosen in trial <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf56"><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the reward in trial <inline-formula><mml:math id="inf57"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf58"><mml:mi>C</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>∙</mml:mo><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the interaction between choice and reward in trial <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where both are expressed as binary with values {-1,1}, <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the value of the action that was chosen on trial <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are the spike counts one, two and three trials prior to the current trial, <inline-formula><mml:math id="inf63"><mml:mi>ϵ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the residual error in trial <inline-formula><mml:math id="inf64"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf65"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the regression parameters. (<bold>A</bold>) (<bold>B</bold>) (<bold>C</bold>) and (<bold>D</bold>) denote the random-walk neurons, motor cortex neurons, auditory cortex neurons and basal ganglia neurons, respectively. Left, t-values from the regression of the spike-counts on <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in the regression model. The significance boundaries for the t-values, denoted by dashed lines, are 2.3, corresponding to p&lt;0.025. Right, fraction of neurons significantly modulated by action-value across the population (0.05, expected by chance, is denoted by a horizontal dashed line). Error bars are standard error of the mean. Number of neurons used in (<bold>A</bold>) (<bold>B</bold>) (<bold>C</bold>) and (<bold>D</bold>) is the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A–B</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C–D</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, respectively. Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. Note that in all four cases the two t-values are correlated. This results from the correlation between <inline-formula><mml:math id="inf68"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf69"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> caused by the reward schedule in (<xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig2-figsupp5-v2"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.009</object-id><label>Figure 2—figure supplement 6.</label><caption><title>Regression on reward probabilities does not resolve the temporal correlations confound.</title><p>Following (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>) for each of the four data-sets (<bold>A</bold>) random-walk (<bold>B</bold>) motor cortex (<bold>C</bold>) auditory cortex and (<bold>D</bold>) basal ganglia neurons, spike counts in the last 20 trials in each block (from randomly assigned simulated experimental settings, the assignment was the same as in the standard analysis in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A–B</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C–D</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> ) were regressed on reward probabilities (e.g., (0.5,0.9)) in those blocks. This is similar to the analysis in the individual examples of <xref ref-type="fig" rid="fig1">Figures 1B</xref> and <xref ref-type="fig" rid="fig2">2A</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>, and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C</xref> (in which two rank sum tests, and not regression, were used). Left of each panel denotes the t-values of the regressions of individual neurons (Dashed lines at t = 2 denote the significance boundaries) and right bar graphs denote the population statistics (Dashed lines denote the naïve expected false positive rate from the significance threshold, see Materials and methods). Number of neurons used in (<bold>A</bold>) (<bold>B</bold>) (<bold>C</bold>) and (<bold>D</bold>) is the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A–B</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C–D</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, respectively. Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>. Note that for this analysis we considered significance using the threshold of p&lt;0.05. By contrast, in (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>) the same analysis was used with a significance threshold of p&lt;0.01. For comparison, when considering the basal ganglia neurons with a significance threshold of p&lt;0.01, the number of neurons that are erroneously classified as action-value neurons decreases from 37 ± 3.3% to 26 ± 3%.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig2-figsupp6-v2"/></fig><fig id="fig2s7" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.010</object-id><label>Figure 2—figure supplement 7.</label><caption><title>Detrending analysis does not resolve the temporal correlations confound.</title><p>Following (<xref ref-type="bibr" rid="bib25">Ito and Doya, 2015a</xref>), we conducted a multiple linear regression analysis using unrelated action-values (the same action-values as in <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>) and the following regression model: <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>Where <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the spike count in trial <inline-formula><mml:math id="inf72"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf73"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf74"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> are the estimated action-values in trial <inline-formula><mml:math id="inf75"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf76"><mml:mi>C</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf77"><mml:mi>C</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> are the actions chosen in trial <inline-formula><mml:math id="inf78"><mml:mi mathvariant="normal">t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf79"><mml:mi mathvariant="normal">t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, respectively, <inline-formula><mml:math id="inf80"><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf81"><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> are the rewards in trial <inline-formula><mml:math id="inf82"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, respectively, <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the residual error in trial <inline-formula><mml:math id="inf85"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf86"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the regression parameters. (<bold>A</bold>) (<bold>B</bold>) (<bold>C</bold>) and (<bold>D</bold>) denote the random-walk neurons, motor cortex neurons, auditory cortex neurons and basal ganglia neurons, respectively. Left, t-values from regressions of the spike-counts on <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in the regression model. As in (<xref ref-type="bibr" rid="bib25">Ito and Doya, 2015a</xref>), the significance boundaries for the t-values, denoted by dashed black lines, are 2.64, corresponding to p&lt;0.01 (as opposed to p&lt;0.05 elsewhere). Right bar graphs denote the population statistics. Dashed lines denote the naïve expected false positive rate from the significance threshold (see Materials and methods). Note, however, that the significance criterion is more stringent and the expected total number of identified action-value neurons by chance is only 2%. Number of neurons used in (<bold>A</bold>) (<bold>B</bold>) (<bold>C</bold>) and (<bold>D</bold>) is the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A–B</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C–D</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, respectively. Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig2-figsupp7-v2"/></fig><fig id="fig2s8" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.011</object-id><label>Figure 2—figure supplement 8.</label><caption><title>Unbiased classification of action-value neurons does not resolve the temporal correlations confound.</title><p>Following (<xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>) (whose main focus was state-value representation), we considered an unbiased classification of action-value neurons. (<bold>A</bold>) (<bold>B</bold>) (<bold>C</bold>) and (<bold>D</bold>) denote the random-walk neurons, motor cortex neurons, auditory cortex neurons and basal ganglia neurons, respectively. The t-values for the different neurons are identical to <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref> (and unlike (<xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>) this analysis used only the last 20 trials in each block). The f-value of each neuron was computed from the regression and a neuron was considered as non-significant (black dot) if p&gt;0.01, denoted by the circle in the left panels. For the significant neurons, the dashed lines define eight equal-angle sectors, each corresponding to a different classification of the neuron. Right is the population analysis. Dashed lines denote the naïve expected false positive rate from the significance threshold (see Materials and methods). Note that the expected total number of identified significant neurons by chance is only 1%. Number of neurons used in (<bold>A</bold>) (<bold>B</bold>) (<bold>C</bold>) and (<bold>D</bold>) is the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A–B</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C–D</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, respectively. Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig2-figsupp8-v2"/></fig><fig id="fig2s9" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.012</object-id><label>Figure 2—figure supplement 9.</label><caption><title>Random intermingling of estimated action-values does not resolve the temporal correlations confound.</title><p>In the spirit of the experiment conducted by (<xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>) we simulated an experiment, in which two different trial types are marked by cues, randomly selected every trial, and action-values are learned separately for each cue. Specifically, each session in this new design was created by a random intermingling of two different, randomly-selected sessions from those analyzed in <xref ref-type="fig" rid="fig1">Figure 1</xref>. The number of trials in the intermingled session was equal to the number of trials in one of the two randomly-selected original sessions. As a result, we only used approximately the first half of each of the original sessions. We created 1000 such intermingled sessions. Next, we regressed the spike counts of neurons from each of the four data-sets (<bold>A</bold>) random-walk neurons, (<bold>B</bold>) motor cortex neurons (<bold>C</bold>) auditory cortex neurons and (<bold>D</bold>) basal ganglia neurons on the resulting intermingled estimated action-values. Left of each panel denotes the t-values of the regressions of individual neurons (Dashed lines at t = 2 denote the significance boundaries) and right bar graphs denote the population statistics (Dashed lines denote the naïve expected false positive rate from the significance threshold, see Materials and methods). Number of neurons used in (<bold>A</bold>) (<bold>B</bold>) (<bold>C</bold>) and (<bold>D</bold>) is the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A–B</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C–D</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, respectively. Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>. These results show that even when using a design where trials are chosen randomly, there can still be temporal correlations in the predictors of the model. In this case, this occurs because the temporal correlations in each estimated action-value still create temporal correlations in the intermingled vector.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig2-figsupp9-v2"/></fig><fig id="fig2s10" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.013</object-id><label>Figure 2—figure supplement 10.</label><caption><title>Increasing the number of blocks does not resolve the temporal correlations confound.</title><p>We repeated the standard analysis with eight blocks, so that the four blocks from the experiment in <xref ref-type="fig" rid="fig1">Figure 1A</xref> were repeated twice, each time in random permutation. The mean length of the sessions was 347 trials (standard deviation 65 trials). For each of the three data-sets (<bold>A</bold>) random-walk neurons, (<bold>B</bold>) motor cortex neurons and (<bold>C</bold>) auditory cortex neurons, the spike-counts were regressed on the longer estimated action-values from the 8-block sessions (for auditory cortex analysis only 676 sessions with 370 or fewer trials were used). Left of each panel denotes the t-values of the regressions of individual neurons (Dashed lines at t = 2 denote the significance boundaries) and right bar graphs denote the population statistics (Dashed lines denote the naïve expected false positive rate from the significance threshold, see Materials and methods). Number of neurons used in (<bold>A</bold>), (<bold>B</bold>) and (<bold>C</bold>) is the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A–B</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C–D</xref>, respectively. Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>. We did not perform this analysis on the basal ganglia neurons because we already used longer sessions in the original analysis for these recordings (see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig2-figsupp10-v2"/></fig></fig-group><p>where <inline-formula><mml:math id="inf89"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the firing rate in trial <inline-formula><mml:math id="inf90"><mml:mi>t</mml:mi></mml:math></inline-formula> (we consider epochs of 1 second as ‘trials’), <inline-formula><mml:math id="inf91"><mml:mi>z</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is a diffusion variable, randomly and independently drawn from a normal distribution with mean 0 and variance <inline-formula><mml:math id="inf92"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf93"><mml:msub><mml:mrow><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> denotes a linear-threshold function, <inline-formula><mml:math id="inf94"><mml:msub><mml:mrow><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:math></inline-formula> if <inline-formula><mml:math id="inf95"><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> and 0 otherwise.</p><p>These random-walk neurons are clearly not action-value neurons. Nevertheless, we tested them using the analyses depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref>. To that goal, we randomly matched the trials in the simulation of the random-walk neurons (completely unrelated to the task) to the trials in the simulation depicted in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. Then, we considered the spike counts of the random-walk neurons in the last 20 trials of each of the four blocks in <xref ref-type="fig" rid="fig1">Figure 1A</xref> (block being defined by the simulation of learning and is unrelated to the activity of the random-walk neurons). Surprisingly, when considering the top neuron in <xref ref-type="fig" rid="fig2">Figure 2A</xref> and utilizing the same analysis as in <xref ref-type="fig" rid="fig1">Figure 1B</xref>, we found that its spike count differed significantly between the first two blocks (p&lt;0.01, rank sum test) but not between the last two blocks (p=0.28, rank sum test), similarly to the simulated action 1-value neuron of <xref ref-type="fig" rid="fig1">Figure 1B</xref> (red). Similarly, the spike count of the bottom random-walk neuron matched that of a simulated action 2-value neuron (compare with the blue-labeled neuron in <xref ref-type="fig" rid="fig1">Figure 1B</xref>; <xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><p>Moreover, we regressed each vector of spike counts for 20,000 random-walk neurons on randomly matched estimated action-values from <xref ref-type="fig" rid="fig1">Figure 1E</xref> and computed the t-values (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). This analysis erroneously classified 42% of these random-walk neurons as action-value neurons (see <xref ref-type="fig" rid="fig2">Figure 2C</xref>). In particular, the top and bottom random-walk neurons of <xref ref-type="fig" rid="fig2">Figure 2A</xref> were identified as action-value neurons for actions 1 and 2, respectively (squares in <xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>To further quantify this result, we computed the fraction of random-walk neurons erroneously classified as action-value neurons as a function of the diffusion parameter <inline-formula><mml:math id="inf96"><mml:mi mathvariant="normal">σ</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). When <inline-formula><mml:math id="inf97"><mml:mi mathvariant="normal">σ</mml:mi></mml:math></inline-formula>=0, the spike counts of the neurons in the different trials are independent and the number of random-walk neurons classified as action-value neurons is slightly less than 10%, the fraction expected by chance from a significance criterion of 5% and two statistical tests, corresponding to the two action-values. The larger the value of <inline-formula><mml:math id="inf98"><mml:mi mathvariant="normal">σ</mml:mi></mml:math></inline-formula>, the higher the probability that a random-walk neuron will pass the selection criterion for at least one action-value and thus be erroneously classified as an action-value, state or policy neuron.</p><p>The excess action-value neurons in <xref ref-type="fig" rid="fig2">Figure 2</xref> emerged because the significance boundary in the statistical analysis was based on the assumption that the different trials are independent from each other. In the case of a regression of a random-walk process on an action-value related variable, this assumption is violated. The reason is that in this case, both predictor (action-value) and the dependent variable (spike count) slowly change over trials, the former because of the learning and the latter because of the random drift. As a result, the statistic, which relates these two signals, is correlated between trials, violating the independence-of-trials assumption of the test. Because of these dependencies, the expected variance of the statistic (be it average spike count in 20 trials or the regression coefficient), which is calculated under the independence-of-trials assumption, is an underestimate of the actual variance. Therefore, the fraction of random-walk neurons classified as action-value neurons increases with the magnitude of the diffusion, which is directly related to the magnitude of correlations between spike counts in proximate trials (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). The phenomenon of spurious significant correlations in time-series with temporal correlations has been described previously in the field of econometrics and a formal discussion of this issue can be found in (<xref ref-type="bibr" rid="bib18">Granger and Newbold, 1974</xref>; <xref ref-type="bibr" rid="bib53">Phillips, 1986</xref>).</p></sec><sec id="s1-2-2"><title>Is this confound relevant to the question of action-value representation in the striatum?</title><sec id="s1-2-2-1"><title>Is a random-walk process a good description of striatal neurons’ activity?</title><p>The Gaussian random-walk process is just an example of a temporally correlated firing rate and we do not argue that the firing rates of striatal neurons follow such a process. However, any other type of temporal correlations, for example, oscillations or trends, will violate the independence-of-trials assumption, and may lead to the erroneous classification of neurons as representing action-values. Such temporal correlations can also emerge from stochastic learning. For example, in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> we consider a model of operant leaning that is based on covariance based synaptic plasticity (<xref ref-type="bibr" rid="bib37">Loewenstein, 2008</xref>; <xref ref-type="bibr" rid="bib38">Loewenstein, 2010</xref>; <xref ref-type="bibr" rid="bib36">Loewenstein and Seung, 2006</xref>; <xref ref-type="bibr" rid="bib45">Neiman and Loewenstein, 2013</xref>) and competition (<xref ref-type="bibr" rid="bib3">Bogacz et al., 2006</xref>). Because such plasticity results in slow changes in the firing rates of the neurons, applying the analysis of <xref ref-type="fig" rid="fig1">Figure 1E</xref> to our simulations results in the erroneous classification of 43% of the simulated neurons as representing action-values. This is despite the fact that action-values are not computed as part of this learning, neither explicitly or implicitly.</p></sec><sec id="s1-2-2-2"><title>Are temporal correlations in neural recordings sufficiently strong to affect the analysis?</title><p>To test the relevance of this confound to experimentally-recorded neural activity, we repeated the analysis of <xref ref-type="fig" rid="fig2">Figure 2B,C</xref> on neurons recorded in two unrelated experiments: 89 neurons from extracellular recordings in the motor cortex of an awake monkey (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A–B</xref>) and 39 auditory cortex neurons recorded intracellularly in anaesthetized rats (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C–D</xref>; [<xref ref-type="bibr" rid="bib22">Hershenhoren et al., 2014</xref>]). We regressed the spike counts on randomly matched estimated action-values from <xref ref-type="fig" rid="fig1">Figure 1E</xref>. In both cases we erroneously classified neurons as representing action-value in a fraction comparable to that reported in the striatum (36 and 23%, respectively).</p></sec><sec id="s1-2-2-3"><title>Strong temporal correlations in the striatum</title><p>To test the relevance of this confound to striatal neurons, we considered previous recordings from neurons in the nucleus accumbens (NAc) and ventral pallidum (VP) of rats in an operant learning experiment (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>) and regressed their spike counts on simulated, unrelated action-values (using more blocks and trials than in <xref ref-type="fig" rid="fig1">Figure 1E</xref>, see Figure legend). Note that although the recordings were obtained during an operant learning task, the action-values that we used in the regression were obtained from simulated experiments and were completely unrelated to the true experimental settings. Again, we erroneously classified a substantial fraction of neurons (43%) as representing action-values, a fraction comparable to that reported in the striatum (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>).</p></sec><sec id="s1-2-2-4"><title>Haven't previous publications acknowledged this confound and successfully addressed it?</title><p>We conducted an extensive literature search to see whether previous studies have identified this confound and addressed it (see Materials and methods). Two studies noted that processes such as slow drift in firing rate may violate the independence-of-trials assumption of the statistical tests and suggested unique methods to address this problem (<xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>): one method (<xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>) relied on permutation of the spike counts within a block (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>, see Materials and methods) and another (<xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>), used spikes in previous trials as predictors (<xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>). However, both approaches still erroneously classify unrelated recorded and random-walk neurons as action-value neurons (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplements 4</xref> and <xref ref-type="fig" rid="fig2s5">5</xref>). The failure of both these approaches stems from the fact that a complete model of the learning-independent temporal correlations is lacking. As a result, these methods are unable to remove <italic>all</italic> the temporal correlations from the vector of spike-counts.</p><p>Our literature search yielded four additional methods that have been used to identify action-value neurons. However, as depicted in <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref> (corresponding to the analyses in [<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>]), <xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7</xref> (corresponding to the analysis in [<xref ref-type="bibr" rid="bib25">Ito and Doya, 2015a</xref>]), <xref ref-type="fig" rid="fig2s8">Figure 2—figure supplement 8</xref> (corresponding to the analysis in [<xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>]) and <xref ref-type="fig" rid="fig2s9">Figure 2—figure supplement 9</xref> (corresponding to a trial design experiment in [<xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>]), all these additional methods erroneously classify neurons from unrelated recordings and random-walk neurons as action-value neurons in numbers comparable to those reported in the striatum (<xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>–<xref ref-type="fig" rid="fig2s9">9</xref>). The fMRI analysis in (<xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>) focused on the difference between action-values rather than on the action-values themselves (see confound 2), and therefore we did not attempt to replicate it (and cannot attest to whether it is subject to the temporal correlations confound). We did, however, conduct the standard analysis on their unique experimental design - a trial-design experiment in which trials with different reward probabilities are randomly intermingled. Surprisingly, we erroneously detect action-value representation even when using this trial design (<xref ref-type="fig" rid="fig2s9">Figure 2—figure supplement 9</xref>). This erroneous detection occurs because in this analysis, the regression’s predictors are estimated action-values, which are temporally correlated. From this example it follows that even trial-design experiments may still be subject to the temporal correlations confound.</p></sec><sec id="s1-2-2-5"><title>Some previous publications used more blocks. Shouldn’t adding blocks solve the problem?</title><p>In <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref> we considered a learning task composed of four blocks with a mean length of 174 trials (standard deviation 43 trials). It is tempting to believe that experiments with more blocks and trials (e.g., [<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>; <xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>]) will be immune to this confound. The intuition is that the larger the number of trials, the less likely it is that a neuron that is not modulated by action-value (e.g., a random-walk neuron) will have a large regression coefficient on one of the action-values. Surprisingly, however, this intuition is wrong. In <xref ref-type="fig" rid="fig2s10">Figure 2—figure supplement 10</xref> we show that doubling the number of blocks, so that the original blocks are repeated twice, each time in a random order, does not decrease the fraction of neurons erroneously classified as representing action-values. For the case of random-walk neurons, it can be shown that, contrary to this intuition, the fraction of erroneously identified action-value neurons is expected to increase with the number of trials (<xref ref-type="bibr" rid="bib53">Phillips, 1986</xref>). This is because the expected variance of the regression coefficients under the null hypothesis is inversely proportional to the degrees of freedom, which increase with the number of trials. As a result, the threshold for classifying a regression coefficient as significant decreases with the number of trials.</p></sec></sec><sec id="s1-2-3"><title>Possible solutions to the temporal correlations confound</title><p>The temporal correlations confound has been acknowledged in the fMRI literature, and several methods have been suggested to address it, such as ‘prewhitening’ (<xref ref-type="bibr" rid="bib67">Woolrich et al., 2001</xref>). However, these methods require prior knowledge, or an estimate of the predictor-independent temporal correlations. Both are impractical for the slow time-scale of learning and therefore are not applicable in the experiments we discussed.</p><p>Another suggestion is to assess the level of autocorrelations between trials in the data and to use it to predict the expected fraction of erroneous classification of action-value neurons. However, using such a measure is problematic in the context of action-value representation because the autocorrelations relevant for the temporal correlations confound are those associated with the time-scale relevant for learning - tens of trials. Computing such autocorrelations in experiments of a few hundreds of trials introduces substantial biases (<xref ref-type="bibr" rid="bib32">Kohn, 2006</xref>; <xref ref-type="bibr" rid="bib46">Newbold and Agiakloglou, 1993</xref>). Moreover, even when these autocorrelations are computed, it is not clear exactly how they can be used to estimate the expected false positive rate for action-value classification.</p><p>Finally, it has been suggested that the temporal correlation confound can be addressed by using repeating blocks and removing neurons whose activity is significantly different in identical blocks (<xref ref-type="bibr" rid="bib2">Asaad et al., 2000</xref>; <xref ref-type="bibr" rid="bib40">Mansouri et al., 2006</xref>). We applied this method by applying a design in which the four blocks of <xref ref-type="fig" rid="fig1">Figure 1</xref> are repeated twice. However, even when this method was applied, a significant number of neurons were erroneously classified as representing action-values (Materials and methods). </p><p>We therefore propose two alternative approaches.</p><sec id="s1-2-3-1"><title>Permutation analysis</title><p>Trivially, an action-value neuron (or any task-related neuron) should be more strongly correlated with the action-value of the experimental session, in which the neuron was recorded, than with action-values of other sessions (recorded in different days). We propose to use this requirement in a permutation test, as depicted in <xref ref-type="fig" rid="fig3">Figure 3</xref>. We first consider the two simulated action-value neurons of <xref ref-type="fig" rid="fig1">Figure 1B</xref>. For each of the two neurons, we computed the t-values of the regression coefficients of the spike counts on each of the estimated action-values in all possible sessions (see Materials and methods). <xref ref-type="fig" rid="fig3">Figure 3A</xref> depicts the two resulting distributions of t-values. As a result of the temporal correlations, the 5% significance boundaries (vertical dashed lines), which are defined to be exceeded by exactly 5% of t-values in each distribution, are substantially larger (in absolute value) than 2, the standard significance boundaries. On this analysis, a neuron is significantly correlated with an action-value if the t-value of the regression on the action-value from its corresponding session exceeds the significance boundaries derived from the regression of its spike count on all possible action-values.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.34248.014</object-id><label>Figure 3.</label><caption><title>Permutation analysis.</title><p>(<bold>A</bold>) Red and blue correspond to red and blue - labeled neurons in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. Arrow-heads denote the t-values from the regressions on the estimated action-values from the session in which the neuron was simulated (depicted in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The red and blue histograms denote the t-values of the regressions of the spike-count on estimated action-values from <italic>different</italic> sessions in <xref ref-type="fig" rid="fig1">Figure 1E</xref> (Materials and methods). Dashed black lines denote the 5% significance boundary. In this analysis, the regression coefficient of neural activity on an action-value is significant if it exceeds these significance boundaries. Note that because of the temporal correlations, these significance boundaries are larger than ±2 (the significance boundaries in <xref ref-type="fig" rid="fig1">Figure 1</xref>,<xref ref-type="fig" rid="fig2">2</xref>). According to this permutation test the red-labeled but not the blue-labeled neuron is classified as an action-value neuron (<bold>B</bold>) Fraction of neurons classified in each category using the permutation analysis for the action-value neurons (green, <xref ref-type="fig" rid="fig1">Figure 1</xref>) and random-walk neurons (yellow, <xref ref-type="fig" rid="fig2">Figure 2</xref>).Dashed lines denote the naïve expected false positive rate from the significance threshold (Materials and methods). Error bars denote the standard error of the mean. The test correctly identifies 29% of actual action-value neurons as such, while classification of random-walk neurons was at chance level. Analysis was done on 10,080 action-value neurons and 10,077 random-walk neurons from 504 simulated sessions (<bold>C</bold>) Light orange, fraction of basal ganglia neurons from (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>) classified in each category when regressing the spike count of 214 basal ganglia neurons in three different experimental phases on the estimated action-values associated with their experimental session. This analysis classified 32% of neurons as representing action-values. Dark orange, fraction of basal ganglia neurons classified in each category when applying the permutation analysis. This test classified 3.6% of neurons as representing action-value. Dashed line denotes significance level of p&lt;0.01.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig3-v2"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.015</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Analyses of basal ganglia data using estimated action-values from the neurons' sessions.</title><p>(<bold>A</bold>) The standard analysis on the neuronal recordings taken from (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>), using action-values estimated from the behavior in the sessions in which the neurons were recorded. The top and bottom scatter plots are identical, except for the significance boundries. They depict the t-values from the results of the regression of 640 of 642 neuronal recordings (214 neurons <inline-formula><mml:math id="inf99"><mml:mo>×</mml:mo></mml:math></inline-formula> 3 phases) on the action-values that were estimated from the original experiment (see Materials and methods for estimation of action-values). Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref> (Dashed lines in the right panels at t=2, 2.64 denote the significance boundaries and the dashed lines on the left panels denote the naïve expected false positive rate from the significance threshold, see Materials and methods). This analysis is different from the one used in (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>), which was similar to the one described in (<xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>). Two neurons do not appear on the scatter plots, whose axes were bounded for ease of viewing. Their t-values were (2.44, 18.91) and (1.08, 16.96) for action-value 1 and 2, respectively. (<bold>B</bold>) For comparsion, we repeated the analysis using the random-walk neurons (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The fraction of erroenusly classified action-value neurons is comparable to that extracted from the experimental data. Remarkably, bias in favor of 'state'-representing neurons over 'policy'-representing neurons observed in the recorded neurons is also present in the random-walk neurons. This suggests that the overrepresentation of state neurons is not necessarily of biological significance.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig3-figsupp1-v2"/></fig></fig-group><p>Indeed, when considering the Top (red) simulated action 1-value neuron, we find that its spike count has a significant regression coefficient on the estimated <inline-formula><mml:math id="inf100"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> from its session (red arrow) but not on the estimated <inline-formula><mml:math id="inf101"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> (blue arrow). Importantly, because the significance boundary exceeds 2, this approach is less sensitive than the original one (<xref ref-type="fig" rid="fig1">Figure 1</xref>) and indeed, the regression coefficients of the Bottom simulated neuron (blue) do not exceed the significance level (red and blue arrows) and thus this analysis fails to identify it as an action-value neuron. Considering the population of simulated action-value neurons of <xref ref-type="fig" rid="fig1">Figure 1</xref>, this analysis identified 29% of the action-value neurons of <xref ref-type="fig" rid="fig1">Figure 1</xref> as such (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, green), demonstrating that this analysis can identify action-value neurons. When considering the random-walk neurons (<xref ref-type="fig" rid="fig2">Figure 2</xref>), this method classifies only approximately 10% of the random-walk neurons as action-value neurons, as predicted by chance (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, yellow). Similar results were obtained for the motor cortex and auditory cortex neurons (not shown).</p></sec><sec id="s1-2-3-2"><title>Permutation analysis of basal ganglia neurons</title><p>Importantly, this permutation method can also be used to reanalyze the activity of previously recorded neurons. To that goal, we considered the recordings reported in (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>). The results of their model-free method (<xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>) imply that approximately 23% of the recorded neurons represent action-values at different phases of the experiment. As a first step, we estimated the action-values and regressed the spike counts in the different phases of the experiment on the estimated action-values, as in <xref ref-type="fig" rid="fig1">Figure 1</xref> (activity in each phase is analyzed as if it is a different neuron; see Materials and methods). The results of this analysis implied that 32% of the neurons represent action values (p&lt;0.01) (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Next, we applied the permutation analysis. Remarkably, this analysis yielded that only 3.6% of the neurons have a significantly higher regression coefficient on an action-value from their session than on other action-values (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Similar results were obtained when performing a similar model-free permutation analysis (regression of spike counts in the last 20 trials of the block on reward probabilities, not shown). These results raise the possibility that all or much of the apparent action-value representation in (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>) is the result of the temporal correlations confound.</p></sec><sec id="s1-2-3-3"><title>Trial-design experiments</title><p>Another way of overcoming the temporal correlations confound is to use a trial design experiment. The idea is to randomly mix the reward probabilities, rather than use blocks as in <xref ref-type="fig" rid="fig1">Figure 1</xref>. For example, we propose the experimental design depicted in <xref ref-type="fig" rid="fig4">Figure 4A</xref>. Each trial is presented in one of four clearly marked contexts (color coded). The reward probabilities associated with the two actions are fixed within a context but differ between the contexts. Within each context the participant learns to prefer the action associated with a higher probability of reward. Naively, we can regress the spike counts on the action-values estimated from behavior, as in <xref ref-type="fig" rid="fig1">Figure 1</xref>. However, because the estimated action-values are temporally correlated, this regression is still subject to the temporal correlations confound (<xref ref-type="fig" rid="fig2s9">Figure 2—figure supplement 9</xref>). Alternatively, we can regress the spike counts on the reward probabilities. If the contexts are randomly mixed, then by construction, the reward probabilities are temporally independent. These reward probabilities are the objective action-values. After learning, the subjective action-values are expected to converge to these reward probabilities. Therefore, the reward probabilities can be used as proxies for the subjective action-values after a sufficiently large number of trials. It is thus possible to conduct a regression analysis on the spike counts at the end of the experiment, with reward probabilities as predictors that do not violate the independence assumption.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.34248.016</object-id><label>Figure 4.</label><caption><title>A possible solution for the temporal correlations confound that is based on a trial design.</title><p>(<bold>A</bold>) A Q-learning model was simulated in 1,000 sessions of 400 trials, where the original reward probabilities (same as in <xref ref-type="fig" rid="fig1">Figure 1A</xref>) were associated with different cues and appeared randomly. Learning was done separately for each cue. Top panel: The first 20 trials in an example session. Background colors denote the reward probabilities in each trial. Black circles denote the learned value of action-value 1 in each trial. Top and bottom black lines denote choices of action 1 and 2, respectively. Long and short lines denote rewarded and unrewarded trials, respectively. Bottom panels: Two examples of the grouping of trials with the same reward probabilities to show the continuity in learning. Note that the action-value changes only when action 1 is chosen because it is the action-value associated with action 1. (<bold>B</bold>) and (<bold>C</bold>) population analysis for action-value neurons. 20,000 action-value neurons were simulated from the model in (<bold>A</bold>), similarly to the action-value neurons in <xref ref-type="fig" rid="fig1">Figure 1</xref>. For each neuron, the spike-counts in the last 200 trials of the session were regressed on the reward probabilities (see Materials and methods). Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D–E</xref>. Dashed lines in (<bold>B</bold>) at t=2 denote the significance boundaries. Dashed lines in (<bold>C</bold>) denote the naïve expected false positive rate from the significance threshold (see Materials and methods). This analysis correctly identifies 59% of action-value neurons as such. (<bold>D</bold>) and (<bold>E</bold>) population analysis for random-walk neurons. 20,000 Random-walk neurons were simulated as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Same regression analysis as in (<bold>B</bold>) and (<bold>C</bold>). Only 8.5% of the random-walk neurons were erroneously classified as representing action-values (9.5% chance level).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig4-v2"/></fig><p>To demonstrate this method, we simulated learning in a session composed of 400 trials, randomly divided into 4 different contexts (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Learning followed the Q-learning equations (<xref ref-type="disp-formula" rid="equ1 equ3">Equations 1 and 2</xref>), independently for each context. Next, we simulated action-value neurons, whose firing rate is a linear function of the action-value in each trial (dots in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, upper panel). We regressed the spike counts of the neurons in the last 200 trials (approximately 50 trials in each context) on the corresponding reward probabilities (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Indeed, 59% of the neurons were classified this way as action-value neurons (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, 9.5% is chance level). By contrast, considering random-walk neurons, only 8.5% were erroneously classified as action-value neurons, a fraction expected by chance.</p><p>Three previous studies used trial-designs to search for action-value representation in the striatum (<xref ref-type="bibr" rid="bib4">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>; <xref ref-type="bibr" rid="bib30">Kim et al., 2012</xref>). In two of them (<xref ref-type="bibr" rid="bib4">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib30">Kim et al., 2012</xref>) the reward probabilities were explicitly cued and therefore their results can be interpreted in the framework of cue-values and not action-values (<xref ref-type="bibr" rid="bib50">Padoa-Schioppa, 2011</xref>). Moreover, all these studies focused on significant neural modulation by both action-values or by their difference, analyses that support state or policy representations (<xref ref-type="bibr" rid="bib25">Ito and Doya, 2015a</xref>). As discussed in details in the next section, policy representation can emerge without action-value representation (<xref ref-type="bibr" rid="bib8">Darshan et al., 2014</xref>; <xref ref-type="bibr" rid="bib12">Fiete et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Frémaux et al., 2010</xref>; <xref ref-type="bibr" rid="bib37">Loewenstein, 2008</xref>; <xref ref-type="bibr" rid="bib38">Loewenstein, 2010</xref>; <xref ref-type="bibr" rid="bib36">Loewenstein and Seung, 2006</xref>; <xref ref-type="bibr" rid="bib45">Neiman and Loewenstein, 2013</xref>; <xref ref-type="bibr" rid="bib57">Seung, 2003</xref>; <xref ref-type="bibr" rid="bib63">Urbanczik and Senn, 2009</xref>). Therefore, the results reported in (<xref ref-type="bibr" rid="bib4">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>; <xref ref-type="bibr" rid="bib30">Kim et al., 2012</xref>) cannot be taken as evidence for action-value representation in the striatum.</p></sec></sec></sec><sec id="s1-3"><title>Confound 2 – correlated decision variables</title><p>In the previous sections we demonstrated that irrelevant temporal correlations may lead to the erroneous classification of neurons as representing action-values, even if their activity is task-independent. Here we address an unrelated confound. We show that neurons that encode different decision variables, in particular policy, may be erroneously classified as representing action-values. For clarity, we will commence by discussing this caveat independently of the temporal correlations confound. Specifically, we show that neurons whose firing rate encodes the policy (probability of choice) may be erroneously classified as representing action-values, even when this policy emerged in the absence of any implicit or explicit action-value representation. We will conclude by discussing a possible solution that addresses this and the temporal correlations confounds.</p><sec id="s1-3-1"><title>Policy without action-value representation</title><p>It is well-known that operant learning can occur in the absence of any value computation, for example, as a result of direct-policy learning (<xref ref-type="bibr" rid="bib43">Mongillo et al., 2014</xref>). Several studies have shown that reward-modulated synaptic plasticity can implement direct-policy reinforcement learning (<xref ref-type="bibr" rid="bib8">Darshan et al., 2014</xref>; <xref ref-type="bibr" rid="bib12">Fiete et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Frémaux et al., 2010</xref>; <xref ref-type="bibr" rid="bib37">Loewenstein, 2008</xref>; <xref ref-type="bibr" rid="bib38">Loewenstein, 2010</xref>; <xref ref-type="bibr" rid="bib36">Loewenstein and Seung, 2006</xref>; <xref ref-type="bibr" rid="bib45">Neiman and Loewenstein, 2013</xref>; <xref ref-type="bibr" rid="bib57">Seung, 2003</xref>; <xref ref-type="bibr" rid="bib63">Urbanczik and Senn, 2009</xref>).</p><p>For concreteness, we consider a particular reinforcement learning algorithm, in which the probability of choice <inline-formula><mml:math id="inf102"><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> is determined by a single variable <inline-formula><mml:math id="inf103"><mml:mi>W</mml:mi></mml:math></inline-formula> that is learned in accordance with the REINFORCE learning algorithm (<xref ref-type="bibr" rid="bib66">Williams, 1992</xref>): <inline-formula><mml:math id="inf104"><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>W</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></inline-formula> where <inline-formula><mml:math id="inf105"><mml:mo>∆</mml:mo><mml:mi>W</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>∙</mml:mo><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>∙</mml:mo><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>∙</mml:mo><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:math></inline-formula>, where <inline-formula><mml:math id="inf106"><mml:mi>α</mml:mi></mml:math></inline-formula> is the learning rate, <inline-formula><mml:math id="inf107"><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the binary reward in trial <inline-formula><mml:math id="inf108"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is a binary variable indicating whether action 1 was chosen in trial <inline-formula><mml:math id="inf110"><mml:mi>t</mml:mi></mml:math></inline-formula>. In our simulations <inline-formula><mml:math id="inf111"><mml:mi>W</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf112"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.17</mml:mn></mml:math></inline-formula>. For biological implementation of this algorithm see (<xref ref-type="bibr" rid="bib38">Loewenstein, 2010</xref>; <xref ref-type="bibr" rid="bib57">Seung, 2003</xref>).</p><p>We tested this model in the experimental design of <xref ref-type="fig" rid="fig1">Figure 1</xref> (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). As expected, the model learned to prefer the action associated with a higher probability of reward, completing the four blocks within 228 trials on average (standard deviation 62 trials).</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.34248.017</object-id><label>Figure 5.</label><caption><title>Erroneous detection of action-value representation in policy neurons.</title><p>(<bold>A</bold>) Behavior of the model in an example session, same as in <xref ref-type="fig" rid="fig1">Figure 1A</xref> for the direct-policy model. (<bold>B</bold>) Red and blue lines denote ‘action-values’ 1 and 2, respectively, calculated from the choices and rewards in (<bold>A</bold>). Note that the model learned without any explicit or implicit calculation of action-values. The extraction of action-values in (<bold>B</bold>) is based on the fitting of <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> to the learning behavior. (<bold>C</bold>) Strong correlation between policy from the direct-policy algorithm and action-values extracted by fitting <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> to behavior. The three panels depict probability of choice as a function of the difference between the calculated action-values (left), ‘<inline-formula><mml:math id="inf113"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>’ (center) and ‘<inline-formula><mml:math id="inf114"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>’ (right). This correlation can cause policy neurons to be erroneously classified as representing action-values (<bold>D</bold>) and (<bold>E</bold>) Population analysis, same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref> for the policy neurons. Legend and number of neurons are also as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>. Dashed lines in (<bold>D</bold>) at t=2 denote the significance boundaries. Dashed lines in (<bold>E</bold>) denote the naïve expected false positive rate from the significance threshold (see Materials and methods).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig5-v2"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.34248.018</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Alternative analyses do not resolve the correlated decision variables confound.</title><p>(<bold>A</bold>) Regression analysis on policy neurons with choice as an added regressor. Following common experimental practice, we used the following regression model: <inline-formula><mml:math id="inf115"><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> where <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the spike count in trial <inline-formula><mml:math id="inf117"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf118"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf119"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> are the estimated action-values in trial <inline-formula><mml:math id="inf120"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf121"><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> is a binary variable indicating whether action 1 was chosen, <inline-formula><mml:math id="inf122"><mml:mi>ϵ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the residual error in trial <inline-formula><mml:math id="inf123"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf124"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the regression parameters. Simulated neurons are the same as in <xref ref-type="fig" rid="fig5">Figure 5E</xref>. Legend is the same as <xref ref-type="fig" rid="fig1">Figure 1D and E</xref> (Dashed lines in the right panels at t=2 denote the significance boundaries and the dashed lines on the left panels denote the naïve expected false positive rate from the significance threshold, see Materials and methods). (<bold>B</bold>) Unbiased analysis. Following (<xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>), we considered an analysis in which the probability of erroneous classification for data that is not related to the task is equal between action-value 1, action-value 2, state and policy. The f-value of each neuron was computed from the regression on the two calculated action-values and a neuron was considered as non-significant (black dot) if p&gt;0.05, denoted by the circle in the left figure. For the significant neurons, the dashed lines define 8 equal-angle sectors, each corresponding to a different classification of the neuron, similarly to <xref ref-type="fig" rid="fig2s8">Figure 2—figure supplement 8</xref>. The figure on the right is the population analysis (Dashed lines denote the naïve expected false positive rate from the significance threshold, see Materials and methods). Note that the fraction of neurons that is expected to be classified as action-value neurons by chance is only 2.5%. Simulated neurons are the same as in <xref ref-type="fig" rid="fig5">Figure 5E</xref>. Legend is the same as in <xref ref-type="fig" rid="fig1">Figure 1D and E</xref>. In the original paper, p&lt;0.01 was used. For p&lt;0.01 the analysis classifies 6% of neurons as action-value neurons (0.5% expected by chance) and 17% as <inline-formula><mml:math id="inf125"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Q</mml:mi></mml:math></inline-formula> neurons (0.25% expected by chance).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig5-figsupp1-v2"/></fig></fig-group></sec><sec id="s1-3-2"><title>Spike count of neurons representing policy are correlated with estimated <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></title><p>Despite the fact that the learning was value-independent, we can still fit a Q-learning model to the behavior, extract best-fit model parameters and compute action-values (see also <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The computed action-values are presented in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. Note that according to <xref ref-type="disp-formula" rid="equ3">Equation 2</xref>, the probability of choice is a monotonic function of the difference between <inline-formula><mml:math id="inf127"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf128"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. Therefore, we expect that the probability of choice will be correlated with the computed <inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf130"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, with opposite signs (<xref ref-type="fig" rid="fig5">Figure 5C</xref>).</p><p>We simulated policy neurons as Poisson neurons whose firing rate is a linear function of the policy <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (Materials and methods). Next, we regressed the spike counts of these neurons on the two action-values that were computed from behavior (same as in <xref ref-type="fig" rid="fig1">Figures 1D,E</xref> and <xref ref-type="fig" rid="fig2">2B,C</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C,D</xref>, – <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B,D</xref>, – <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). Indeed, as expected, 14% of the neurons were significantly correlated with both action values with opposite signs (chance level for each action value is 5%, naïve chance level for both with opposite signs is 0.125%, see Materials and methods), as depicted in <xref ref-type="fig" rid="fig5">Figure 5D,E</xref>. These results demonstrate that neurons representing value-independent policy can be erroneously classified as representing <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s1-3-3"><title>Neurons representing policy may be erroneously classified as action-value neurons</title><p>Surprisingly, 38% of policy neurons were significantly correlated with <italic>exactly</italic> one estimated action-value, and therefore would have been classified as action-value neurons in the standard method of analysis (9.5% chance level).</p><p>To understand why this erroneous classification emerged, we note that a neuron is classified as representing an action-value if its spike count is significantly correlated with one of the action values, but not with the other. The confound that led to the classification of policy neurons as representing action-values is that <italic>a lack of statistically significant correlation is erroneously taken to imply lack of correlation.</italic> All policy neurons are modulated by the probability of choice, a variable that is correlated with the difference in the two action-values. Therefore, this probability of choice is expected to be correlated with both action-values, with opposite signs. However, because the neurons are Poisson, the spike count of the neurons is a noisy estimate of the probability of choice. As a result, in most cases (86%), the regression coefficients do not cross the significance threshold for <italic>both</italic> action-values. More often (38%), only one of them crosses the significance threshold, resulting in an erroneous classification of the neurons as representing action values.</p></sec><sec id="s1-3-4"><title>Is this confound relevant to the question of action-value representation in the striatum?</title><sec id="s1-3-4-1"><title>If choice is included as a predictor, is policy representation still a relevant confound?</title><p>It is common, (although not ubiquitous) to attempt to differentiate action-value representation from choice representation by including choice as another regressor in the regression model (<xref ref-type="bibr" rid="bib4">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>; <xref ref-type="bibr" rid="bib16">Funamizu et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Her et al., 2016</xref>; <xref ref-type="bibr" rid="bib25">Ito and Doya, 2015a</xref>; <xref ref-type="bibr" rid="bib26">Ito and Doya, 2015b</xref>; <xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib30">Kim et al., 2012</xref>; <xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>). Such analyses may be expected to exclude policy neurons, whose firing rate is highly correlated with choice, from being classified as action-value neurons. However, repeating this analysis for the policy neurons of <xref ref-type="fig" rid="fig5">Figure 5</xref>, we still erroneously classify 36% of policy neurons as action-value neurons (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>).</p><p>An alternative approach has been to consider only those neurons whose spike count is not significantly correlated with choice (<xref ref-type="bibr" rid="bib59">Stalnaker et al., 2010</xref>; <xref ref-type="bibr" rid="bib69">Wunderlich et al., 2009</xref>). Repeating this analysis for the <xref ref-type="fig" rid="fig5">Figure 5</xref> policy neurons, we still find that 24% of the neurons are erroneously classified as action-value neurons (8% are classified as policy neurons).</p></sec><sec id="s1-3-4-2"><title>Is this confound the result of an analysis that is biased against policy representation?</title><p>The analysis depicted in <xref ref-type="fig" rid="fig1">Figures 1D,E</xref>, <xref ref-type="fig" rid="fig2">2B,C</xref>, <xref ref-type="fig" rid="fig4">4B–E</xref> and <xref ref-type="fig" rid="fig5">5D,E</xref> is biased towards classifying neurons as action-value neurons, at the expense of state or policy neurons, as noted by (<xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>). This is because action-value classification is based on a single significant regression coefficient whereas policy or state classification requires two significant regression coefficients. Therefore, (<xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>) have proposed an alternative approach. First, compute the statistical significance of the whole regression model for each neuron (using f-value). Then, classify those significant neurons according to the t-values corresponding to the two action-values (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>). Applying this analysis to the policy neurons of <xref ref-type="fig" rid="fig5">Figure 5</xref> with a detection threshold of 5% we find that indeed, this method is useful in detecting which decision variables are more frequently represented (its major use in [<xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>]): 25% of the neurons are classified as representing policy (1.25% expected by chance). Nevertheless, 12% of the neurons are still erroneously classified as action-value neurons (2.5% expected by chance; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>).</p></sec><sec id="s1-3-4-3"><title>Additional issues</title><p>In many cases, the term action-value was used, while the reported results were equally consistent with other decision variables. In some cases, significant correlation with both action-values (with opposite signs) or significant correlation with the difference between the action-values was used as evidence for ‘action-value representations’ (<xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>; <xref ref-type="bibr" rid="bib19">Guitart-Masip et al., 2012</xref>; <xref ref-type="bibr" rid="bib30">Kim et al., 2012</xref>; <xref ref-type="bibr" rid="bib31">2007</xref>; <xref ref-type="bibr" rid="bib59">Stalnaker et al., 2010</xref>). Similarly, other papers did not distinguish between neurons whose activity is significantly correlated with one action-value and those whose activity is correlated with both action-values (<xref ref-type="bibr" rid="bib16">Funamizu et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Her et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>). Finally, one study used a concurrent variable-interval schedule, in which the magnitudes of rewards associated with each action were anti-correlated (<xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>). In such a design, the two probabilities of reward depend on past choices and therefore, the objective values associated with the actions change on a trial-by-trial basis and are, in general, correlated.</p></sec></sec><sec id="s1-3-5"><title>A possible solution to the policy confound</title><p>The policy confound emerged because policy and action-values are correlated. To distinguish between the two possible representations, we should seek a variable that is correlated with the action-value but uncorrelated with the policy. Consider the sum of the two action-values. It is easy to see that <inline-formula><mml:math id="inf133"><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>∝</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula>. Therefore, if the variances of the two action-values are equal, their sum is uncorrelated with their difference. An action-value neuron is expected to be correlated with the sum of action-values. By contrast, a policy neuron, modulated by the difference in action-values is expected to be uncorrelated with this sum.</p><p>We repeated the simulations of <xref ref-type="fig" rid="fig4">Figure 4</xref> (which addresses the temporal correlations confound), considering three types of neurons: action-value neurons (of <xref ref-type="fig" rid="fig1">Figure 1</xref>), random-walk neurons (of <xref ref-type="fig" rid="fig2">Figure 2</xref>), and policy neurons (of <xref ref-type="fig" rid="fig5">Figure 5</xref>). As in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we considered the spike counts of the three types of neurons in the last 200 trials of the session, but now we regressed them on the <italic>sum</italic> of reward probabilities (state; in this experimental design the reward probabilities are also the objective action-values, which the subject learns). We found that only 4.5 and 6% of the random-walk and policy neurons, respectively, were significantly correlated with the sum of reward probabilities (5% chance level). By contrast, 47% of the action-value neurons were significantly correlated with this sum.</p><p>This method is able to distinguish between policy and action-value representations. However, it will fail in the case of state representation because both state and action-values are correlated with the sum of probabilities of reward. To dissociate between state and action-value representations, we can consider the difference in reward probabilities because this difference is correlated with the action-values but is uncorrelated with the state. Regressing the spike count on <italic>both</italic> the sum and difference of the probabilities of reward, a random-walk neuron is expected to be correlated with none, a policy neuron is expected to be correlated only with the difference, whereas an action-value neuron is expected to be correlated with both (this analysis is inspired by Fig. S8b in (<xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>) in which the predictors in the regression model were policy and state). We now classify a neuron that passes both significance tests as an action-value neuron. Indeed, for a significance threshold of p&lt;0.05 (for each test), only 0.2% of the random-walk neurons and 5% of the policy neurons were classified as action-value neurons. By contrast, 32% of the action-value neurons were classified as such (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Note that in this analysis only when more than 5% of the neurons are classified as action-value neurons we have support for the hypothesis that there is action-value representation rather than policy or state representation.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.34248.019</object-id><label>Figure 6.</label><caption><title>A possible solution for the policy and state confounds.</title><p>(<bold>A</bold>) The Q-learning model (<xref ref-type="disp-formula" rid="equ1 equ3">Equations 1 and 2</xref>) was simulated in 1,000 sessions of 400 trials each, where the reward probabilities were associated with different cues and were randomly chosen in each trial, as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Learning occurred separately for each cue. In each session 20 action-value neurons, whose firing rate is proportional to the action-values (as in <xref ref-type="fig" rid="fig1">Figure 1</xref>) were simulated. For each neuron, the spike-counts in the last 200 trials of each session were regressed on the sum of the reward probabilities (<inline-formula><mml:math id="inf134"><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>Q</mml:mi></mml:math></inline-formula>; state) and the difference of the reward probabilities (<inline-formula><mml:math id="inf135"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Q</mml:mi></mml:math></inline-formula>; policy, see Materials and methods). Each dot denotes the t-values of the two regression coefficients of each of 500 example neurons. Dashed lines at t=2 denote the significance boundaries. Neurons that had significant regression coefficients on <italic>both</italic> policy and state were identified as action-value neurons. Colors as in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. (<bold>B</bold>) Population analysis revealed that 32% of the action-value neurons were identified as such. Error bars are the standard error of the mean. Dashed black line denotes the expected false positive rate from randomly modulated neurons. Dashed gray line denotes the expected false positive rate from policy or state neurons (see Materials and methods) (<bold>C</bold>) Same as in (<bold>A</bold>) with random-walk neurons, numbers are as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. (<bold>D</bold>) Population analysis revealed that less than 1% of the random-walk neurons were erroneously classified as representing action-values. (<bold>E-F</bold>) To test the policy neurons, we simulated a direct-policy learning algorithm (as in <xref ref-type="fig" rid="fig5">Figure 5</xref>) in the same sessions as in (<bold>A-D</bold>). Learning occurred separately for each cue. In each session 20 policy neurons, whose firing rate is proportional to the probability of choice (as in <xref ref-type="fig" rid="fig5">Figure 5</xref>) were simulated. As in (<bold>A-D</bold>), the spike-counts in the last 200 trials of each session were regressed on the sum and difference of the reward probabilities. (<bold>E</bold>) Each dot denotes the t-values of the two regression coefficients of each of 500 example neurons. (<bold>F</bold>) Population analysis. As expected, only 5% of the policy neurons were erroneously classified as representing action-values.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-34248-fig6-v2"/></fig><p>A word of caution is that the analysis should be performed only after the learning converges. This is because stochastic fluctuations in the learning process may be reflected in the activities of neurons representing decision-related variables. As a result, policy or state-representing neurons may appear correlated with the orthogonal variables. For the same reason, any block-related heterogeneity in neural activity could also result in this confound (<xref ref-type="bibr" rid="bib48">O'Doherty, 2014</xref>).</p><p>To conclude, it is worthwhile repeating the key features of the analysis proposed in this section:</p><list list-type="order"><list-item><p>Trial design is necessary because otherwise temporal correlations in spike count may inflate the fraction of neurons that pass the significance tests.</p></list-item><list-item><p>Regression should be performed on reward probabilities (i.e., the objective action-values) and not on estimated action-values. The reason is that because the estimated action-values evolve over time, this trial design does not eliminate all temporal correlations between them (<xref ref-type="fig" rid="fig2s9">Figure 2—figure supplement 9</xref>).</p></list-item><list-item><p>Reward probabilities associated with the two actions should be chosen such that their variances should be equal. Otherwise policy or state neurons may be erroneously classified as action-value neurons.</p></list-item></list></sec></sec></sec><sec id="s2" sec-type="discussion"><title>Discussion</title><p>In this paper, we performed a systematic literature search to discern the methods that have been previously used to infer the representation of action-values in the striatum. We showed that none of these methods overcome two critical confounds: (1) neurons with temporal correlations in their firing rates may be erroneously classified as representing action-values and (2) neurons whose activity co-varies with other decision variables, such as policy, may also be erroneously classified as representing action-values. Finally, we discuss possible experiments and analyses that can address the question of whether neurons encode action-values.</p><sec id="s2-1"><title>Temporal correlations and action-value representations</title><p>It is well known in statistics that the regression coefficient between two independent slowly-changing variables is on average larger (in absolute value) than this coefficient when the series are devoid of a temporal structure. If these temporal correlations are overlooked, the probability of a false-positive is underestimated (<xref ref-type="bibr" rid="bib18">Granger and Newbold, 1974</xref>). When searching for action-value representation in a block design, then by construction, there are positive correlations in the predictor (action-values). Positive temporal correlations in the dependent variable (neural activity) will result in an inflation of the false-positive observations, compared with the naïve expectation.</p><p>This confound occurs only when there are temporal correlations in both the predictor and the dependent variable. In a trial design, in which the predictor is chosen independently in each trial and thus has no temporal structure, we do not expect this confound. However, when studying incremental learning, it is difficult to randomize the predictor in each trial, making the task of identifying neural correlates of learning, and specifically action-values, challenging. With respect to the dependent variable (neural activity), temporal correlations in BOLD signal and their consequences have been discussed (<xref ref-type="bibr" rid="bib1">Arbabshirani et al., 2014</xref>; <xref ref-type="bibr" rid="bib67">Woolrich et al., 2001</xref>). Considering electrophysiological recordings, there have been attempts to remove these correlations, for example, using previous spike counts as predictors (<xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>). However, these are not sufficient because they are unable to remove all task-independent temporal correlations (see also <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplements 4</xref>–<xref ref-type="fig" rid="fig2s10">10</xref>). When repeating these analyses, we erroneously classified a fraction of neurons as representing action-value that is comparable to that reported in the striatum. The probability of a false-positive identification of a neuron as representing action-value depends on the magnitude and type of temporal correlations in the neural activity. Therefore, we cannot predict the fraction of erroneously classified neurons expected in various experimental settings and brain areas.</p><p>One may argue that the fact that action-value representations are reported mostly in a specific brain area, namely the striatum, is an indication that their identification there is not a result of the temporal correlations confound. However, because different brain regions are characterized by different spiking statistics, we expect different levels of erroneous identification of action-value neurons in different parts of the brain and in different experimental settings. Indeed, the fraction of erroneously identified action-value neurons differed between the auditory and motor cortices (compare B and D within <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Furthermore, many studies reported action-value representation outside of the striatum, in brain areas including the supplementary motor area and presupplementary eye fields (<xref ref-type="bibr" rid="bib69">Wunderlich et al., 2009</xref>), the substantia nigra/ventral tegmental area (<xref ref-type="bibr" rid="bib19">Guitart-Masip et al., 2012</xref>) and ventromedial prefrontal cortex, insula and thalamus (<xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>).</p><p>Considering the ventral striatum, our analysis on recordings from (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>) indicates that the identification of action-value representations there may have been erroneous, resulting from temporally correlated firing rates (<xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). It should be noted that the fraction of action-value neurons reported in (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>) is low relative to other publications, a difference that has been attributed to the location of the recording in the striatum (ventral as opposed to dorsal). It would be interesting to apply this method to other striatal recordings (<xref ref-type="bibr" rid="bib25">Ito and Doya, 2015a</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>; <xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>). We were unable to directly analyze these recordings from the dorsal striatum because relevant raw data is not publicly available. However, previous studies have reported that the firing rates of dorsal-striatal neurons change slowly over time (<xref ref-type="bibr" rid="bib17">Gouvêa et al., 2015</xref>; <xref ref-type="bibr" rid="bib42">Mello et al., 2015</xref>). As a result, identification of apparent action-value representation in dorsal-striatal neurons may also be the result of this confound.</p><p>Temporal correlations naturally emerge in experiments composed of multiple trials. Participants become satiated, bored, tired, etc., which may affect neuronal activity. In particular, learning in operant tasks is associated, by construction, with variables that are temporally correlated. If neural activity is correlated with performance (e.g., accumulated rewards in the last several trials) then it is expected to have temporal correlations, which may lead to an erroneous classification of the neurons as representing action-values.</p></sec><sec id="s2-2"><title>Temporal correlations – beyond action-value representation</title><p>Action-values are not the only example of slowly-changing variables. Any variable associated with incremental learning, motivation or satiation is expected to be temporally correlated. Even 'benign' behavioral variables, such as the location of the animal or the activation of different muscles may change at relatively long time-scales. When recording neural activity related to these variables, any temporal correlations in the neural recording, be it in fMRI, electrophysiology or calcium imaging may result in an erroneous identification of correlates of these behavioral variables because of the temporal correlation confound.</p><p>In general, the temporal correlation confound can be addressed by using the permutation analysis of <xref ref-type="fig" rid="fig3">Figure 3</xref>, which can provide strong support to the claim that the activity of a particular neuron or voxel co-varies with the behavioral variable. Therefore, the permutation test is a general solution for scientists studying slow processes such as learning. More challenging, however, is precisely identifying what the activity of the neuron represents (for example an action-value or policy). There are no easy solutions to this problem and therefore caution should be applied when interpreting the data.</p></sec><sec id="s2-3"><title>Differentiating action-value from other decision variables</title><p>Another difficulty in identifying action-value neurons is that they are correlated with other decision variables such as policy, state or chosen-value. Therefore, finding a neuron that is significantly correlated with an action-value could be the byproduct of its being modulated by other decision variables, in particular policy. The problem is exacerbated by the fact that standard analyses (e.g., <xref ref-type="fig" rid="fig1">Figure 1D–E</xref>) are biased towards classifying neurons as representing action-values at the expense of policy or state.).</p><p>As shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>, policy representation can be ruled out by finding a representation that is orthogonal to policy, namely state representation. This solution leads us, however, to a serious conceptual issue. All analyses discussed so far are based on significance tests: we divide the space of hypothesis into the ‘scientific claim’ (e.g., neurons represent action-values) and the null hypothesis (e.g., neural activity is independent of the task). An observation that is not consistent with the null hypothesis is taken to support the alternative hypothesis.</p><p>The problem we faced with correlated variables is that the null hypothesis and the ‘scientific claim’ were not complementary. A neuron that represents policy is expected to be inconsistent with the null hypothesis that neural activity is independent of the task but it is not an action-value neuron. The solution proposed was to devise a statistical test that seeks to identify a representation that is correlated with action-value and is orthogonal to the policy hypothesis, in order to also rule out a policy representation.</p><p>However, this does not rule out other decision-related representations. A ‘pure’ action-value neuron is modulated only by <inline-formula><mml:math id="inf136"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> or by <inline-formula><mml:math id="inf137"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. A ‘pure’ policy neuron is modulated exactly by <inline-formula><mml:math id="inf138"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. More generally, we may want to consider the hypotheses that the neuron is modulated by a different combination of the action values, <inline-formula><mml:math id="inf139"><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mo>∙</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi><mml:mo>∙</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, where <italic>a</italic> and <italic>b</italic> are parameters. For every such set of parameters <italic>a</italic> and <italic>b</italic> we can devise a statistical test to reject this hypothesis by considering the direction that is orthogonal to the vector <inline-formula><mml:math id="inf140"><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. In principle, this procedure should be repeated for every pair of parameters <italic>a</italic> and <italic>b</italic> that in not consistent with the action-value hypothesis.</p><p>Put differently, in order to find neurons that represent action-values, we first need to <italic>define</italic> the set of parameters <italic>a</italic> and <italic>b</italic> such that a neuron whose activity is modulated by <inline-formula><mml:math id="inf141"><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mo>∙</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi><mml:mo>∙</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> will be considered as representing an action-value. Only after this (arbitrary) definition is given, can we construct a set of statistical tests that will rule out the competing hypotheses, namely will rule out <italic>all</italic> values of <italic>a</italic> and <italic>b</italic> that are not in this set. The analysis of <xref ref-type="fig" rid="fig6">Figure 6</xref> implicitly defined the set of <italic>a</italic> and <italic>b</italic> such that <inline-formula><mml:math id="inf142"><mml:mi>a</mml:mi><mml:mo>≠</mml:mo><mml:mi>b</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf143"><mml:mi>a</mml:mi><mml:mo>≠</mml:mo><mml:mo>-</mml:mo><mml:mi>b</mml:mi></mml:math></inline-formula> as the set of parameters that defines action-value representations. In practice, it is already very challenging to identify action-values using the procedure of <xref ref-type="fig" rid="fig6">Figure 6</xref> and going beyond it seems impractical. Therefore, studying the distribution of t-values across the population of neurons may be more useful when studying representations of decision variables than asking questions about the significance of individual neurons.</p><p>Importantly, the regression models described in this paper allow us to investigate only some types of representations, namely, linear combinations of the two action-values. However, value representations in learning models may fall outside of this regime. It has been suggested that in decision making, subjects calculate the ratio of action-values (<xref ref-type="bibr" rid="bib68">Worthy et al., 2008</xref>), or that subjects compute, for each action, the probability that it is associated with the highest value (<xref ref-type="bibr" rid="bib44">Morris et al., 2014</xref>). Our proposed solution cannot support or refute these alternative hypotheses. If these are taken as additional alternative hypotheses, a neuron should be classified as representing an action-value if its activity is also significantly modulated in the directions that are correlated with action-value and are orthogonal to these hypotheses. Clearly, it is never possible to construct an analysis that can rule out all possible alternatives.</p><p>We believe that the confounds that we described have been overlooked because the null hypothesis in the significance tests was not made explicit. As a result, the complementary hypothesis was not explicitly described and the conclusions drawn from rejecting the null hypothesis were too specific. That is, alternative plausible interpretations were ignored. It is important, therefore, to keep the alternative hypotheses explicit when analyzing the data, be it using significance tests or other methods, such as model comparison (<xref ref-type="bibr" rid="bib26">Ito and Doya, 2015b</xref>).</p></sec><sec id="s2-4"><title>Are action-value representations a necessary part of decision making?</title><p>One may argue that the question of whether neurons represent action-value, policy, state or some other correlated variable is not an interesting question. This is because all these correlated decision variables implicitly encode action-values. Even direct-policy models can be taken to implicitly encode action-values because policy is correlated with the difference between the action-values. However, we believe that the difference between action-value representation and representation of other variables is an important one, because it centers on the question of the computational model that underlies decision making in these tasks. Specifically, the implication of a finding that a population of neurons represents action-values is not that these neurons are involved somehow in decision making. Rather, we interpret this finding as supporting the hypothesis that action-values are explicitly computed in the brain, and that these action-values play a specific role in the decision making process. However, if the results are also consistent with various alternative computational models then this is not the case. Some consider action-value computation to be a necessary part of decision making. By contrast, however, we presented here two models of learning and decision making that do not entail this computation (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig5">Figure 5</xref>). Other examples are discussed in (<xref ref-type="bibr" rid="bib43">Mongillo et al., 2014</xref>; <xref ref-type="bibr" rid="bib58">Shteingart and Loewenstein, 2014</xref>) and references therein.</p></sec><sec id="s2-5"><title>Other indications for action-value representation</title><p>Several trial-design experiments have associated cues with upcoming rewards and reported representations of expected reward, the upcoming action, or the interaction of action and reward (<xref ref-type="bibr" rid="bib7">Cromwell and Schultz, 2003</xref>; <xref ref-type="bibr" rid="bib6">Cromwell et al., 2005</xref>; <xref ref-type="bibr" rid="bib20">Hassani et al., 2001</xref>; <xref ref-type="bibr" rid="bib23">Hori et al., 2009</xref>; <xref ref-type="bibr" rid="bib27">Kawagoe et al., 1998</xref>; <xref ref-type="bibr" rid="bib52">Pasquereau et al., 2007</xref>). Another trial-design experiment reported representation of offer-value and chosen-value in the orbitofrontal cortex (<xref ref-type="bibr" rid="bib49">Padoa-Schioppa and Assad, 2006</xref>). While such studies do not provide direct evidence for action-value representation, they do provide evidence for representation of closely related decision variables (but see [<xref ref-type="bibr" rid="bib48">O'Doherty, 2014</xref>]).</p><p>The involvement of the basal ganglia in general and the striatum in particular in operant learning, planning and decision-making is well documented (<xref ref-type="bibr" rid="bib10">Ding and Gold, 2010</xref>; <xref ref-type="bibr" rid="bib41">McDonald and White, 1993</xref>; <xref ref-type="bibr" rid="bib47">O'Doherty et al., 2004</xref>; <xref ref-type="bibr" rid="bib51">Palminteri et al., 2012</xref>; <xref ref-type="bibr" rid="bib55">Schultz, 2015</xref>; <xref ref-type="bibr" rid="bib61">Tai et al., 2012</xref>; <xref ref-type="bibr" rid="bib62">Thorn et al., 2010</xref>; <xref ref-type="bibr" rid="bib71">Yarom and Cohen, 2011</xref>). However, there are alternatives to the possibility that the firing rate of striatal neurons represents action-values. First, as discussed above, learning and decision making do not entail action-value representation. Second, it is possible that action-value is represented elsewhere in the brain. Finally, it is also possible that the striatum plays an essential role in learning, but that the representation of decision variables there is distributed and neural activity of single neurons could reflect a complex combination of value-related features, rather than ‘pure’ decision variables. Such complex representations are typically found in artificial neural networks (<xref ref-type="bibr" rid="bib70">Yamins and DiCarlo, 2016</xref>).</p></sec><sec id="s2-6"><title>Action-value representation in the striatum requires further evidence</title><p>Considering the literature, both confounds have been partially acknowledged. Moreover, there have been some attempts to address them. However, as discussed above, even when these confounds were acknowledged and solutions were proposed, these solutions do not prevent the erroneous identification of action-value representation (see <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplements 4</xref>, <xref ref-type="fig" rid="fig2s5">5</xref> and <xref ref-type="fig" rid="fig2s10">10</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We therefore conclude that to the best of our knowledge, all studies that have claimed to provide direct evidence that neuronal activity in the striatum is specifically modulated by action-value were either susceptible to the temporal correlations confound (<xref ref-type="bibr" rid="bib16">Funamizu et al., 2015</xref>; <xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>; <xref ref-type="bibr" rid="bib25">2015a</xref>; <xref ref-type="bibr" rid="bib26">Ito and Doya, 2015b</xref>; <xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>; <xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>), or reported results in a manner indistinguishable from policy (<xref ref-type="bibr" rid="bib4">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>; <xref ref-type="bibr" rid="bib16">Funamizu et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Guitart-Masip et al., 2012</xref>; <xref ref-type="bibr" rid="bib21">Her et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib30">Kim et al., 2012</xref>; <xref ref-type="bibr" rid="bib31">2007</xref>; <xref ref-type="bibr" rid="bib59">Stalnaker et al., 2010</xref>; <xref ref-type="bibr" rid="bib69">Wunderlich et al., 2009</xref>). Many studies presented action-value and policy representations separately, but were subject to the second confound (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>; <xref ref-type="bibr" rid="bib25">2015a</xref>; <xref ref-type="bibr" rid="bib26">Ito and Doya, 2015b</xref>; <xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>). Furthermore, it should be noted that not all studies investigating the relation between striatal activity and action-value representation have reported positive results. Several studies have reported that striatal activity is more consistent with direct-policy learning than with action-value learning (<xref ref-type="bibr" rid="bib14">FitzGerald et al., 2014</xref>; <xref ref-type="bibr" rid="bib35">Li and Daw, 2011</xref>) and one noted that lesions to the dorsal striatum do not impair action-value learning (<xref ref-type="bibr" rid="bib64">Vo et al., 2014</xref>).</p><p>Finally, we would like to emphasize that we do not claim that there is no representation of action-value in the striatum. Rather, our results show that special caution should be applied when relating neural activity to reinforcement-learning related variables. Therefore, the prevailing belief that neurons in the striatum represent action-values must await further tests that address the confounds discussed in this paper.</p></sec></sec><sec id="s3" sec-type="materials|methods"><title>Materials and methods</title><sec id="s3-1"><title>Literature search</title><p>In order to thoroughly examine the finding of action-value neurons in the striatum, we conducted a literature search to find all the different approaches used to identify action-value representation in the striatum and see whether they are subject to at least one of the two confounds we described here.</p><p>The key words ‘action-value’ and ‘striatum’ were searched for in Web-of-Knowledge, Pubmed and Google Scholar, returning 43, 21 and 980 results, respectively. In the first screening stage, we excluded all publications that did not report new experimental results (e.g., reviews and theoretical papers), focused on other brain regions, or did not address value-representation or learning. In the remaining publications, the abstract of the publication was read and the body of the article was searched for ‘action-value’ and ‘striatum’. After this step, articles in which it was possible to find description of action-value representation in the striatum were read thoroughly. The search included PhD theses, but none were found to report new relevant data, not found in papers. We identified 22 papers that directly related neural activity in the striatum to action-values. These papers included reports of single-unit recordings, fMRI experiments and manipulations of striatal activity.</p><p>Of these, two papers have used the term action-value to refer to the value of the <italic>chosen</italic> action (also known as chosen-value) (<xref ref-type="bibr" rid="bib9">Day et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Seo et al., 2012</xref>) and therefore we do not discuss them.</p><p>An additional study (<xref ref-type="bibr" rid="bib52">Pasquereau et al., 2007</xref>) used the expected reward and the chosen action as predictors of the neuronal activity and found neurons that were modulated by the expected reward, the chosen action and their interaction. The authors did not claim that these neurons represent action-values, but it is possible that these neurons were modulated by the values of specific actions. However, the representation of the value of the action when the action is not chosen is a crucial part of action-value representation which differentiates it from the representation of expected reward, and the values of the actions when they were not chosen were not analyzed in this study. Therefore, the results of this study cannot be taken as an indication for action-value representation, rather than other decision variables.</p><p>A second group of 11 papers did not distinguish between action-value and policy representations (<xref ref-type="bibr" rid="bib4">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib16">Funamizu et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Her et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib69">Wunderlich et al., 2009</xref>), or reported policy representation (<xref ref-type="bibr" rid="bib13">FitzGerald et al., 2012</xref>; <xref ref-type="bibr" rid="bib19">Guitart-Masip et al., 2012</xref>; <xref ref-type="bibr" rid="bib30">Kim et al., 2012</xref>; <xref ref-type="bibr" rid="bib31">2007</xref>; <xref ref-type="bibr" rid="bib59">Stalnaker et al., 2010</xref>) in the striatum and therefore their findings do not necessarily imply action-value representation, rather than policy representation in the striatum (see confound 2).</p><p>In two additional papers, it was shown that the activation of striatal neurons changes animals’ behavior, and the results were interpreted in the action-value framework (<xref ref-type="bibr" rid="bib34">Lee et al., 2015</xref>; <xref ref-type="bibr" rid="bib61">Tai et al., 2012</xref>). However, a change in policy does not entail an action-value representation (see, for example, <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Therefore, these papers were not taken as strong support to the striatal action-value representation hypothesis.</p><p>Finally, six papers correlated action-values, separately from other decision variables, with neuronal activity in the striatum (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>; <xref ref-type="bibr" rid="bib25">2015a</xref>; <xref ref-type="bibr" rid="bib26">Ito and Doya, 2015b</xref>; <xref ref-type="bibr" rid="bib33">Lau and Glimcher, 2008</xref>; <xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>; <xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>). All of them used electrophysiological recordings of single units in the striatum. From these papers, only one utilized an analysis which is not biased towards identifying action-value neurons at the expense of policy and state neurons (<xref ref-type="bibr" rid="bib65">Wang et al., 2013</xref>). All papers used block-design experiments where action-values are temporally correlated.</p><p>Taken together, we concluded that previous reports on action-value representation in the striatum could reflect the representation of other decision variables or temporal correlations in the spike count that are not related to action-value learning.</p></sec><sec id="s3-2"><title>The action-value neurons model (<xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig4">Figure 4</xref>)</title><p>To model neurons whose firing rate is modulated by an action-value, we considered neurons whose firing rate changes according to:<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mo>⋅</mml:mo><mml:mi>r</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf144"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the firing rate in trial <inline-formula><mml:math id="inf145"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf146"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn></mml:math></inline-formula>Hz is the baseline firing rate, <inline-formula><mml:math id="inf147"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the action-value associated with one of the actions <inline-formula><mml:math id="inf148"><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:mn>1,2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="inf149"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>2.35</mml:mn></mml:math></inline-formula>Hz is the maximal modulation and <inline-formula><mml:math id="inf150"><mml:mi>r</mml:mi></mml:math></inline-formula> denotes the neuron-specific level of modulation, drawn from a uniform distribution, <inline-formula><mml:math id="inf151"><mml:mi>r</mml:mi><mml:mo>~</mml:mo><mml:mi>U</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>. The spike count in a trial was drawn from a Poisson distribution, assuming a 1 sec-long trial.</p></sec><sec id="s3-3"><title>The policy neurons model (<xref ref-type="fig" rid="fig5">Figure 5</xref>)</title><p>To model neurons whose firing rate is modulated by the policy, we considered neurons whose firing rate changes according to:<disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mo>⋅</mml:mo><mml:mi>r</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf152"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the firing rate in trial <inline-formula><mml:math id="inf153"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf154"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn></mml:math></inline-formula>Hz is the baseline firing rate, <inline-formula><mml:math id="inf155"><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> is the probability of choosing action 1 in trial <inline-formula><mml:math id="inf156"><mml:mi>t</mml:mi></mml:math></inline-formula> that changes in accordance with REINFORCE (<xref ref-type="bibr" rid="bib66">Williams, 1992</xref>) (see also <xref ref-type="fig" rid="fig5">Figure 5</xref> and corresponding text). <inline-formula><mml:math id="inf157"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math></inline-formula>Hz is the maximal modulation and <inline-formula><mml:math id="inf158"><mml:mi>r</mml:mi></mml:math></inline-formula> denotes the neuron-specific level of modulation, drawn from a uniform distribution, <inline-formula><mml:math id="inf159"><mml:mi>r</mml:mi><mml:mo>~</mml:mo><mml:mi>U</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>. The spike count in a trial was drawn from a Poisson distribution, assuming a 1 sec-long trial.</p></sec><sec id="s3-4"><title>The covariance neurons model (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>)</title><p>In the covariance based plasticity model the decision-making network is composed of two populations of Poisson neurons: each neuron is characterized by its firing rate and the spike count of a neuron in a trial (1 sec) is randomly drawn from a Poisson distribution. The chosen action corresponds to the population that fires more spikes in a trial (<xref ref-type="bibr" rid="bib38">Loewenstein, 2010</xref>; <xref ref-type="bibr" rid="bib36">Loewenstein and Seung, 2006</xref>). At the end of the trial, the firing rate of each of the neurons (in the two population) is updated according to <inline-formula><mml:math id="inf160"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo>∙</mml:mo><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>∙</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></inline-formula>, where <inline-formula><mml:math id="inf161"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the firing rate in trial <inline-formula><mml:math id="inf162"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf163"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.07</mml:mn></mml:math></inline-formula> is the learning rate, <inline-formula><mml:math id="inf164"><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the reward delivered in trial <inline-formula><mml:math id="inf165"><mml:mi>t</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf166"><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> in our simulations) and <inline-formula><mml:math id="inf167"><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the measured (realized) firing rate in that trial, that is the spike count in the trial. The initial firing rate of all simulated neurons is 2.5Hz. The network model was tested in the operant learning task of <xref ref-type="fig" rid="fig1">Figure 1</xref>. A session was terminated (without further analysis) if the model was not able to choose the better option more than 14 out of 20 consecutive times for at least 200 trials in the same block. This occurred on 20% of the sessions. We simulated two populations of 1,000 neurons in 500 successful sessions. Note that because on average, the empirical firing rate is equal to the true firing rate, <inline-formula><mml:math id="inf168"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="⟩" open="⟨" separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></inline-formula>, changes in the firing rate are driven, on average, by the covariance of reward and the empirical firing rate: <inline-formula><mml:math id="inf169"><mml:mfenced close="⟩" open="⟨" separators="|"><mml:mrow><mml:mo>∆</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>≡</mml:mo><mml:mfenced close="⟩" open="⟨" separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo>∙</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></inline-formula>(<xref ref-type="bibr" rid="bib36">Loewenstein and Seung, 2006</xref>). The estimated action-values in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> were computed from the actions and rewards of the covariance model by assuming the Q-learning model (<xref ref-type="disp-formula" rid="equ1 equ3">Equations 1 and 2</xref>).</p></sec><sec id="s3-5"><title>The motor cortex recordings (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>)</title><p>The data in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A–B</xref> was recorded by Oren Peles in Eilon Vaadia's lab. It was recorded from one female monkey (Macaca fascicularis) at 3 years of age, using a 10 × 10 microelectrode array (Blackrock Microsystems) with 0.4 mm inter-electrode distance. The array was implanted in the arm area of M1, under anesthesia and aseptic conditions.</p><p>Behavioral Task: The Monkey sat in a behavioral setup, awake and performing a Brain Machine Interface (BMI) and sensorimotor combined task. Spikes and Local Field Potentials were extracted from the raw signals of 96 electrodes. The BMI was provided through real time communication between the data acquisition system and a custom-made software, which obtained the neural data, analyzed it and provided the monkey with the desired visual and auditory feedback, as well as the food reward. Each trial began with a visual cue, instructing the monkey to make a small hand movement to express alertness. The monkey was conditioned to enhance the power of beta band frequencies (20-30Hz) extracted from the LFP signal of 2 electrodes, receiving a visual feedback from the BMI algorithm. When a required threshold was reached, the monkey received one of 2 visual cues and following a delay period, had to report which of the cues it saw by pressing one of two buttons. Food reward and auditory feedback were delivered based on correctness of report. The duration of a trial was on average 14.2s. The inter-trial-interval was 3s following a correct trial and 5s after error trials. The data used in this paper, consists of spiking activity of 89 neurons recorded during the last second of inter-trial-intervals, taken from 600 consecutive trials in one recording session. Pairwise correlations were comparable to previously reported (<xref ref-type="bibr" rid="bib5">Cohen and Kohn, 2011</xref>), <inline-formula><mml:math id="inf170"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.047</mml:mn><mml:mo>±</mml:mo><mml:mn>0.17</mml:mn></mml:math></inline-formula> (SD), (<inline-formula><mml:math id="inf171"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.037</mml:mn><mml:mo>±</mml:mo><mml:mn>0.21</mml:mn></mml:math></inline-formula> for pairs of neurons recorded from the same electrode).</p><p>Animal care and surgical procedures complied with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and with guidelines defined by the Institutional Committee for Animal Care and Use at the Hebrew University.</p></sec><sec id="s3-6"><title>The auditory cortex recordings (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>)</title><p>The auditory cortex recordings appearing in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C–D</xref> are described in detail in (<xref ref-type="bibr" rid="bib22">Hershenhoren et al., 2014</xref>). In short, membrane potential was recorded intracellularly in the auditory cortex of halothane-anesthetized rats. The data consists of 125 experimental sessions recorded from 39 neurons. Each session consisted of 370 pure tone bursts. Tone duration was 50 ms with 5 ms linear rise/fall ramps. In the data presented here, trials began 50 ms prior to the onset of the tone burst. For each session, all trials were either 300 msec or 500 msec long. Trial length remained identical throughout a session and depended on smallest interval between two tones in each session. Spike events were identified following high pass filtering with a corner frequency of 30Hz. Local maxima that were larger than 60 times the median of the absolute deviation from the median (MAD) were classified as spikes. The data presented here consists only of the spike counts in each trial, rather than the full membrane potential trace.</p></sec><sec id="s3-7"><title>The basal ganglia recordings (<xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>)</title><p>The basal ganglia recordings that are analyzed in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> are described in detail in (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>). In short, rats performed a combination of a tone discrimination task and a reward-based free-choice task. Extracellular voltage was recorded in the behaving rats from the NAc and VP using an electrode bundle. Spike sorting was done using principal component analysis. In total, 148 NAc and 66 VP neurons across 52 sessions were used for analyses (In 18 of the 70 behavioral sessions there were no neural recordings).</p></sec><sec id="s3-8"><title>Estimation of action-values from choices and rewards</title><p>To imitate experimental procedures, we regressed the spike counts on estimates of the action-values, rather than the subjective action-values that underlay model behavior (to which the experimentalist has no direct access). For that goal, for each session, we assumed that <inline-formula><mml:math id="inf172"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula> and found the set of parameters <inline-formula><mml:math id="inf173"><mml:mover accent="true"><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf174"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> that yielded the estimated action-values that best fit the sequences of actions in each experiment by maximizing the likelihood of the sequence. Action-values were estimated from <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, using these estimated parameters and the sequence of actions and rewards. Overall, the estimated values of the parameters <inline-formula><mml:math id="inf175"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf176"><mml:mi>β</mml:mi></mml:math></inline-formula> were comparable to the actual values used: on average, <inline-formula><mml:math id="inf177"><mml:mover accent="true"><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.12</mml:mn> <mml:mi mathvariant="normal"/><mml:mo>±</mml:mo><mml:mn>0.09</mml:mn></mml:math></inline-formula> (standard deviation) and <inline-formula><mml:math id="inf178"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>2.6</mml:mn> <mml:mi mathvariant="normal"/><mml:mo>±</mml:mo><mml:mn>0.7</mml:mn></mml:math></inline-formula> (compare with <inline-formula><mml:math id="inf179"><mml:mi>α</mml:mi></mml:math></inline-formula>=0.1 and <inline-formula><mml:math id="inf180"><mml:mi>β</mml:mi></mml:math></inline-formula>=2.5).</p></sec><sec id="s3-9"><title>Exclusion of neurons</title><p>Following standard procedures (<xref ref-type="bibr" rid="bib54">Samejima et al., 2005</xref>), a sequence of spike-counts, either simulated or experimentally measured was excluded due to low firing rate if the mean spike count in all blocks was smaller than 1. This procedure excluded 0.02% (4/20,000) of the random-walk neurons and 0.03% (285/1,000,000) of the covariance-based plasticity neurons. Considering the auditory cortex recordings, we assigned each of the 125 spike counts to 40 randomly-selected sessions. 23% of the neural recordings (29/125) were excluded in all 40 sessions. Because blocks are defined differently in different sessions, some neural recordings were excluded only when assigned to some sessions but not others. Of the remaining 96 recordings, 14% of the recordings × sessions were also excluded. Similarly, considering the basal ganglia neurons, we assigned each of the 642 recordings (214 × 3 phases) to 40 randomly-selected sessions. 11% (74/(214 × 3)) of the recordings were excluded in all 40 sessions. Of the remaining 568 recordings, 9% of the recordings × sessions were also excluded. None of the simulated action-value neurons (0/20,000) or the motor cortex neurons (0/89) were excluded.</p></sec><sec id="s3-10"><title>Statistical analyses</title><p>The computation of the t-values of the regression of the spike counts on the estimated action-values (as in <xref ref-type="fig" rid="fig1">Figures 1</xref>, <xref ref-type="fig" rid="fig2">2</xref> and <xref ref-type="fig" rid="fig5">5</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, – <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, –<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>) was done using the following regression model:<disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf181"><mml:mi mathvariant="normal">s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the spike count in trial <inline-formula><mml:math id="inf182"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf183"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf184"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> are the estimated action-values in trial <inline-formula><mml:math id="inf185"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf186"><mml:mi>ϵ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the residual error in trial <inline-formula><mml:math id="inf187"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf188"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the regression parameters.</p><p>The computation of the t-values of the regression of the spike counts on the reward probabilities in the trial design experiment (as in <xref ref-type="fig" rid="fig4">Figure 4</xref>) was done using the following regression model:<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>R</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>R</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the trial. Only the last 200 trials of the session were anlyzed. <inline-formula><mml:math id="inf190"><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the mean spike count, <inline-formula><mml:math id="inf191"><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf192"><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> are the reward probabilities corresponding to action 1 or action 2, respectively (in this experimental design <inline-formula><mml:math id="inf193"><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:math></inline-formula> could be 0.1,0.5 or 0.9), <inline-formula><mml:math id="inf194"><mml:mi>ϵ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the residual error and <inline-formula><mml:math id="inf195"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the regression parameters.</p><p>The computation of the t-values of the regression of the spike counts on <italic>state</italic> and <italic>policy</italic> in a trial design experiment (as in <xref ref-type="fig" rid="fig6">Figure 6</xref>) was done using the following regression model:<disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>All variables and parameters are the same as in <xref ref-type="disp-formula" rid="equ8">Equation 7</xref></p><p>All regression analyses were done using <italic>regstats</italic> in MATLAB (version 2016A).</p><p>To compare the spike counts of the example neurons, in the last 20 trials of each block (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>; <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>; <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C</xref>; <xref ref-type="fig" rid="fig2">Figure 2A</xref>) we executed the Wilcoxon rank sum test, using <italic>ranksum</italic> in MATLAB. All tests were two-tailed.</p><p>Significance of t-values slightly depends on session length. For the session lengths we considered, 0.05 significance bounds varied between 1.962 and 1.991. For consistency, we chose a single conservative bound of 2. Similarly, 0.025 and 0.01 significance bounds were chosen to be 2.3 and 2.64, respectively.</p><p>For all significance boundaries the false positive thresholds were computed naively, that is, assuming the analysis is not confounded in any way and that the two predictors are not correlated with each other. For example, assuming the false positive rate from a single t-test for a significant regression coefficient is <inline-formula><mml:math id="inf196"><mml:mi>P</mml:mi></mml:math></inline-formula>, for the standard analysis, the false positive rate for each action-value classification was defined as <inline-formula><mml:math id="inf197"><mml:mi>P</mml:mi><mml:mo>∙</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, and the false positive rate was equal for state and policy classification and was defined as <inline-formula><mml:math id="inf198"><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig6">Figure 6</xref> the false positive rate computed for random-walk neurons was <inline-formula><mml:math id="inf199"><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> for each action-value classification, and the false positive rate computed for state or policy neurons was <inline-formula><mml:math id="inf200"><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> for each action-value classification.</p></sec><sec id="s3-11"><title>Permutation test (<xref ref-type="fig" rid="fig3">Figure 3</xref>)</title><p>For each action-value and random-walk neuron, we computed the t-values of the regressions of its spike-count on estimated action-values from the sessions of <xref ref-type="fig" rid="fig1">Figure 1E</xref>. Because the number of trials can affect the distribution of t-values, we only considered in our analysis the first 170 trials of the 504 sessions longer or equal to 170 trials. This number, which is approximately the median of the distribution of number of trials per session, was chosen as a compromise between the number of trials per session and number of sessions. When performing the permutation test on the basal ganglia data we included all recordings and only the first 332 trials in each session, which is the smallest number of trials used in a session in this dataset.</p><p>Two points are noteworthy. First, the distribution of the t-values of the regression of the spike count of a neuron on all action-values depends on the neuron (see difference between distributions in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Similarly, the distribution of the t-values of the regression of the spike counts of all neurons on an action-value depends on the action-value (not shown). Therefore, the analysis could be biased in favor (or against) finding action-value neurons if the number of neurons analyzed from each session (and therefore are associated with the same action-values) differs between sessions. Second, this analysis does not address the correlated decision variables confound.</p><p>Finally, we would like to point out that there is an alternative way of performing the permutation test, which is applicable when the number of sessions is small, while the number of neurons recorded in a session is large. Instead of comparing the t-values from the regression of a neuron on different action-values, one can compare the t-values from different neurons on the same action-value. However, this method is only applicable under the assumption that the temporal correlations that are not related to action-value in the neuronal activity are similar between sessions.</p></sec><sec id="s3-12"><title>Comparison with permuted spike counts (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>)</title><p>In <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref> we considered the experiment and analysis described in (<xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>). That experiment consisted of four blocks, each associated with a different pair of reward probabilities, (0.72, 0.12), (0.12, 0.72), (0.21, 0.63) and (0.63, 0.21), appearing in a random order, with the better option changing location with each block change. The number of trials in a block was preset, ranging between 35 and 45 with a mean of 40 (this is unlike the experiment described in <xref ref-type="fig" rid="fig1">Figure 1</xref>, in which termination of a block depended on performance).</p><p>First, we used <xref ref-type="disp-formula" rid="equ1 equ3">Equations 1 and 2</xref> to model learning behavior in this protocol. Then, we estimated the action-values according to choice and reward sequences, as in <xref ref-type="fig" rid="fig1">Figure 1</xref>. These estimated action-values were used for regression of the spike counts of the random-walk, motor cortex, auditory cortex, and basal ganglia neurons in the following way: each spike count sequence was randomly assigned to a particular pair of estimated action-values from one session. The spike count sequence was regressed on these estimated action-values. The resultant t-values were compared with the t-values of 1000 regressions of the spike-count, permuted within each block, on the same action-values. The p-value of this analysis was computed as the percentage of t-values from the permuted spike-counts that were higher in absolute value than the t-value from the regression of the original spike count. The significance boundary was set at p&lt;0.025 (<xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>). Neurons with at least one significant regression coefficient (rather than exactly one significant regression coefficient) were classified as action-value modulated neurons (<xref ref-type="bibr" rid="bib29">Kim et al., 2009</xref>).</p></sec><sec id="s3-13"><title>ANOVA tests for comparisons between blocks, excluding ‘drifting’ neurons</title><p>Following (<xref ref-type="bibr" rid="bib2">Asaad et al., 2000</xref>) we conducted an additional analysis with repeating blocks. We simulated learning behavior in the same experiment as in <xref ref-type="fig" rid="fig2s10">Figure 2—figure supplement 10</xref>. This experiment is composed of 8 blocks - the 4 blocks of <xref ref-type="fig" rid="fig1">Figure 1</xref>, repeated twice, in random permutation. We restricted our analysis to the 438 sessions with 332 trials or fewer (332 trials is the shortest session in the basal ganglia recording). Each spike count was analyzed 40 times, using 40 randomly-assigned sessions. For each block, we restricted the analysis to the neuronal activity in the last 20 trials of the block.</p><p>First, we conducted four one-way ANOVAs (using MATLAB’s anova1) to compare the neuronal activities in blocks associated with the same action-values (e.g., the neuronal activity in the two blocks, in which reward probabilities were (0.1,0.5)). Neurons were excluded from further analysis if we found a significant difference in their firing rates in at least one of these comparisons (df(columns)=1, df(error)=38, p&lt;0.1). This procedure excludes from further analysis ‘drifting’ neurons, whose spike count significantly varied in the session.</p><p>Next, for each action-value we conducted a one-way ANOVA (using MATLAB’s anova1), which compared the neuronal activity between the two blocks in which the action-value was 0.1 and the two blocks in which the action-value was 0.9 (df(columns)=1, df(error)=78, p&lt;0.01). We classified neurons as representing action-values if there was a significant difference between their firing rates for one action-value but not for the other.</p><p>Despite the removal of ‘drifting’ neurons, this analysis yielded an erroneous classification of action-value neurons in all datasets: random-walk neurons, 18%; motor cortex neurons, 12%; auditory cortex neurons, 5%; basal ganglia neurons, 9%. This is despite the fact that the expected false positive rate is only 2%. These results indicate that the exclusion of ‘drifting’ neurons as in (<xref ref-type="bibr" rid="bib2">Asaad et al., 2000</xref>) does not solve the temporal correlations confound.</p><p>Data from the motor cortex, auditory cortex, and basal ganglia was the same as in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplements 2</xref>–<xref ref-type="fig" rid="fig2s3">3</xref>. Data for random-walk included 1000 newly simulated neurons, using the same parameters as in <xref ref-type="fig" rid="fig2">Figure 2</xref> (this was done to create enough trials in each simulated spike count).</p></sec><sec id="s3-14"><title>Data and code availability</title><p>The data of the basal ganglia recordings from (<xref ref-type="bibr" rid="bib24">Ito and Doya, 2009</xref>) is available online at <ext-link ext-link-type="uri" xlink:href="https://groups.oist.jp/ncu/data">https://groups.oist.jp/ncu/data</ext-link> and was analyzed with permission from the authors. Motor cortex data (recorded by Oren Peles in Eilon Vaadia's lab) and auditory cortex data (taken from the recordings in (<xref ref-type="bibr" rid="bib22">Hershenhoren et al., 2014</xref>)) is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/lotem-elber/striatal-action-value-neurons-reconsidered-codes">https://github.com/lotem-elber/striatal-action-value-neurons-reconsidered-codes</ext-link> (<xref ref-type="bibr" rid="bib11">Elber-Dorozko and Loewenstein, 2018</xref>). The custom MATLAB scripts used to create simulated neurons and to analyze simulated and recorded neurons are also available at <ext-link ext-link-type="uri" xlink:href="https://github.com/lotem-elber/striatal-action-value-neurons-reconsidered-codes">https://github.com/lotem-elber/striatal-action-value-neurons-reconsidered-codes</ext-link> (<xref ref-type="bibr" rid="bib11">Elber-Dorozko and Loewenstein, 2018</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/striatal-action-value-neurons-reconsidered-codes">https://github.com/elifesciences-publications/striatal-action-value-neurons-reconsidered-codes</ext-link>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We are extremely grateful to Oren Peles, Eilon Vaadia and Uri Werner-Reiss for providing us with their motor cortex recordings, Bshara Awwad, Itai Hershenhoren, Israel Nelken for providing us with their auditory cortex recordings, Kenji Doya and Makoto Ito for providing us with their basal ganglia recordings, Mati Joshua, Gianluigi Mongillo, Jonathan Roiser and Roey Schurr for careful reading of the manuscript and helpful comments and Inbal Goshen, Hanan Shteingart and Wolfram Schultz for discussions.</p></ack><sec id="s4" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Supervision, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn></fn-group></sec><sec id="s5" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.34248.020</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-34248-transrepform-v2.docx"/></supplementary-material><sec id="s6" sec-type="datasets"><title>Data availability</title><p>The data of the basal ganglia recordings from (Ito and Doya 2009) is available online at https://groups.oist.jp/ncu/data and was analyzed with permission from the authors. Motor cortex data (recorded by Oren Peles in Eilon Vaadia's lab) and auditory cortex data (taken from the recordings in (Hershenhoren, Taaseh, Antunes, &amp; Nelken, 2014)) is available at https://github.com/lotem-elber/striatal-action-value-neurons-reconsidered-codes (Elber-Dorozko &amp; Loewenstein 2018). The custom MATLAB scripts used to create simulated neurons and to analyze simulated and recorded neurons are also available at https://github.com/lotem-elber/striatal-action-value-neurons-reconsidered-codes (copy archived at https://github.com/elifesciences-publications/striatal-action-value-neurons-reconsidered-codes).</p><p>The following previously published datasets were used:</p><p><related-object content-type="generated-dataset" id="dataset2" source-id="https://groups.oist.jp/ncu/data" source-id-type="uri"><collab collab-type="author">Ito M</collab><collab collab-type="author">Doya K</collab><year>2009</year><source>Validation of decision making models and analysis of decision variables in the rat basal ganglia.</source><ext-link ext-link-type="uri" xlink:href="https://groups.oist.jp/ncu/data">https://groups.oist.jp/ncu/data</ext-link><comment>Publicly available at OIST Groups website.</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arbabshirani</surname> <given-names>MR</given-names></name><name><surname>Damaraju</surname> <given-names>E</given-names></name><name><surname>Phlypo</surname> <given-names>R</given-names></name><name><surname>Plis</surname> <given-names>S</given-names></name><name><surname>Allen</surname> <given-names>E</given-names></name><name><surname>Ma</surname> <given-names>S</given-names></name><name><surname>Mathalon</surname> <given-names>D</given-names></name><name><surname>Preda</surname> <given-names>A</given-names></name><name><surname>Vaidya</surname> <given-names>JG</given-names></name><name><surname>Adali</surname> <given-names>T</given-names></name><name><surname>Calhoun</surname> <given-names>VD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Impact of autocorrelation on functional connectivity</article-title><source>NeuroImage</source><volume>102</volume><fpage>294</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.07.045</pub-id><pub-id pub-id-type="pmid">25072392</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaad</surname> <given-names>WF</given-names></name><name><surname>Rainer</surname> <given-names>G</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Task-specific neural activity in the primate prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>84</volume><fpage>451</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.84.1.451</pub-id><pub-id pub-id-type="pmid">10899218</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>E</given-names></name><name><surname>Moehlis</surname> <given-names>J</given-names></name><name><surname>Holmes</surname> <given-names>P</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks</article-title><source>Psychological Review</source><volume>113</volume><fpage>700</fpage><lpage>765</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id><pub-id pub-id-type="pmid">17014301</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname> <given-names>X</given-names></name><name><surname>Kim</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Heterogeneous coding of temporally discounted values in the dorsal and ventral striatum during intertemporal choice</article-title><source>Neuron</source><volume>69</volume><fpage>170</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.11.041</pub-id><pub-id pub-id-type="pmid">21220107</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MR</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Measuring and interpreting neuronal correlations</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>811</fpage><lpage>819</lpage><pub-id pub-id-type="doi">10.1038/nn.2842</pub-id><pub-id pub-id-type="pmid">21709677</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cromwell</surname> <given-names>HC</given-names></name><name><surname>Hassani</surname> <given-names>OK</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Relative reward processing in primate striatum</article-title><source>Experimental Brain Research</source><volume>162</volume><fpage>520</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-2223-z</pub-id><pub-id pub-id-type="pmid">15754177</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cromwell</surname> <given-names>HC</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Effects of expectations for different reward magnitudes on neuronal activity in primate striatum</article-title><source>Journal of Neurophysiology</source><volume>89</volume><fpage>2823</fpage><lpage>2838</lpage><pub-id pub-id-type="doi">10.1152/jn.01014.2002</pub-id><pub-id pub-id-type="pmid">12611937</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darshan</surname> <given-names>R</given-names></name><name><surname>Leblois</surname> <given-names>A</given-names></name><name><surname>Hansel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Interference and shaping in sensorimotor adaptations with rewards</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003377</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003377</pub-id><pub-id pub-id-type="pmid">24415925</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Day</surname> <given-names>JJ</given-names></name><name><surname>Jones</surname> <given-names>JL</given-names></name><name><surname>Carelli</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Nucleus accumbens neurons encode predicted and ongoing reward costs in rats</article-title><source>European Journal of Neuroscience</source><volume>33</volume><fpage>308</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2010.07531.x</pub-id><pub-id pub-id-type="pmid">21198983</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>L</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Caudate encodes multiple computations for perceptual decisions</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>15747</fpage><lpage>15759</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2894-10.2010</pub-id><pub-id pub-id-type="pmid">21106814</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Elber-Dorozko</surname> <given-names>L</given-names></name><name><surname>Loewenstein</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>striatal-action-value-neurons-reconsidered-codes</data-title><source>GitHub</source><version designator="37f6b8f">37f6b8f</version><ext-link ext-link-type="uri" xlink:href="https://github.com/lotem-elber/striatal-action-value-neurons-reconsidered-codes">https://github.com/lotem-elber/striatal-action-value-neurons-reconsidered-codes</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiete</surname> <given-names>IR</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>2038</fpage><lpage>2057</lpage><pub-id pub-id-type="doi">10.1152/jn.01311.2006</pub-id><pub-id pub-id-type="pmid">17652414</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>FitzGerald</surname> <given-names>TH</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Action-specific value signals in reward-related regions of the human brain</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>16417</fpage><lpage>16423</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3254-12.2012</pub-id><pub-id pub-id-type="pmid">23152624</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>FitzGerald</surname> <given-names>TH</given-names></name><name><surname>Schwartenbeck</surname> <given-names>P</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reward-related activity in ventral striatum is action contingent and modulated by behavioral relevance</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>1271</fpage><lpage>1279</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4389-13.2014</pub-id><pub-id pub-id-type="pmid">24453318</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frémaux</surname> <given-names>N</given-names></name><name><surname>Sprekeler</surname> <given-names>H</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional requirements for reward-modulated spike-timing-dependent plasticity</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>13326</fpage><lpage>13337</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6249-09.2010</pub-id><pub-id pub-id-type="pmid">20926659</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funamizu</surname> <given-names>A</given-names></name><name><surname>Ito</surname> <given-names>M</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name><name><surname>Kanzaki</surname> <given-names>R</given-names></name><name><surname>Takahashi</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Condition interference in rats performing a choice task with switched variable- and fixed-reward conditions</article-title><source>Frontiers in Neuroscience</source><volume>9</volume><elocation-id>27</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2015.00027</pub-id><pub-id pub-id-type="pmid">25741231</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouvêa</surname> <given-names>TS</given-names></name><name><surname>Monteiro</surname> <given-names>T</given-names></name><name><surname>Motiwala</surname> <given-names>A</given-names></name><name><surname>Soares</surname> <given-names>S</given-names></name><name><surname>Machens</surname> <given-names>C</given-names></name><name><surname>Paton</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Striatal dynamics explain duration judgments</article-title><source>eLife</source><volume>4</volume><elocation-id>e11386</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.11386</pub-id><pub-id pub-id-type="pmid">26641377</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granger</surname> <given-names>CWJ</given-names></name><name><surname>Newbold</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Spurious regressions in econometrics</article-title><source>Journal of Econometrics</source><volume>2</volume><fpage>111</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/0304-4076(74)90034-7</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guitart-Masip</surname> <given-names>M</given-names></name><name><surname>Huys</surname> <given-names>QJ</given-names></name><name><surname>Fuentemilla</surname> <given-names>L</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Duzel</surname> <given-names>E</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Go and no-go learning in reward and punishment: interactions between affect and effect</article-title><source>NeuroImage</source><volume>62</volume><fpage>154</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.04.024</pub-id><pub-id pub-id-type="pmid">22548809</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassani</surname> <given-names>OK</given-names></name><name><surname>Cromwell</surname> <given-names>HC</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Influence of expectation of different rewards on behavior-related neuronal activity in the striatum</article-title><source>Journal of Neurophysiology</source><volume>85</volume><fpage>2477</fpage><lpage>2489</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.85.6.2477</pub-id><pub-id pub-id-type="pmid">11387394</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Her</surname> <given-names>ES</given-names></name><name><surname>Huh</surname> <given-names>N</given-names></name><name><surname>Kim</surname> <given-names>J</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neuronal activity in dorsomedial and dorsolateral striatum under the requirement for temporal credit assignment</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>27056</elocation-id><pub-id pub-id-type="doi">10.1038/srep27056</pub-id><pub-id pub-id-type="pmid">27245401</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hershenhoren</surname> <given-names>I</given-names></name><name><surname>Taaseh</surname> <given-names>N</given-names></name><name><surname>Antunes</surname> <given-names>FM</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Intracellular correlates of stimulus-specific adaptation</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>3303</fpage><lpage>3319</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2166-13.2014</pub-id><pub-id pub-id-type="pmid">24573289</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hori</surname> <given-names>Y</given-names></name><name><surname>Minamimoto</surname> <given-names>T</given-names></name><name><surname>Kimura</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neuronal encoding of reward value and direction of actions in the primate putamen</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>3530</fpage><lpage>3543</lpage><pub-id pub-id-type="doi">10.1152/jn.00104.2009</pub-id><pub-id pub-id-type="pmid">19812294</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname> <given-names>M</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Validation of decision-making models and analysis of decision variables in the rat basal ganglia</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>9861</fpage><lpage>9874</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6157-08.2009</pub-id><pub-id pub-id-type="pmid">19657038</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname> <given-names>M</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Distinct neural representation in the dorsolateral, dorsomedial, and ventral parts of the striatum during fixed- and free-choice tasks</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>3499</fpage><lpage>3514</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1962-14.2015</pub-id><pub-id pub-id-type="pmid">25716849</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname> <given-names>M</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Parallel representation of value-based and finite State-Based strategies in the ventral and dorsal striatum</article-title><source>PLoS Computational Biology</source><volume>11</volume><elocation-id>e1004540</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004540</pub-id><pub-id pub-id-type="pmid">26529522</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawagoe</surname> <given-names>R</given-names></name><name><surname>Takikawa</surname> <given-names>Y</given-names></name><name><surname>Hikosaka</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Expectation of reward modulates cognitive signals in the basal ganglia</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>411</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1038/1625</pub-id><pub-id pub-id-type="pmid">10196532</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>H</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Signals for previous goal choice persist in the dorsomedial, but not dorsolateral striatum of rats</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>52</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2422-12.2013</pub-id><pub-id pub-id-type="pmid">23283321</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>H</given-names></name><name><surname>Sul</surname> <given-names>JH</given-names></name><name><surname>Huh</surname> <given-names>N</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Role of striatum in updating values of chosen actions</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>14701</fpage><lpage>14712</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2728-09.2009</pub-id><pub-id pub-id-type="pmid">19940165</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>S</given-names></name><name><surname>Cai</surname> <given-names>X</given-names></name><name><surname>Hwang</surname> <given-names>J</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Prefrontal and striatal activity related to values of objects and locations</article-title><source>Frontiers in Neuroscience</source><volume>6</volume><elocation-id>108</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2012.00108</pub-id><pub-id pub-id-type="pmid">22822390</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>YB</given-names></name><name><surname>Huh</surname> <given-names>N</given-names></name><name><surname>Lee</surname> <given-names>H</given-names></name><name><surname>Baeg</surname> <given-names>EH</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Encoding of action history in the rat ventral striatum</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>3548</fpage><lpage>3556</lpage><pub-id pub-id-type="doi">10.1152/jn.00310.2007</pub-id><pub-id pub-id-type="pmid">17942629</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kohn</surname> <given-names>AF</given-names></name></person-group><year iso-8601-date="2006">2006</year><chapter-title>Autocorrelation and Cross-Correlation Methods</chapter-title><person-group person-group-type="editor"><name><surname>Akay</surname> <given-names>M</given-names></name></person-group><source>Wiley Encyclopedia of Biomedical Engineering</source><publisher-name>John Wiley &amp; Sons</publisher-name><fpage>260</fpage><lpage>283</lpage><pub-id pub-id-type="doi">10.1002/9780471740360.ebs0094</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname> <given-names>B</given-names></name><name><surname>Glimcher</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Value representations in the primate striatum during matching behavior</article-title><source>Neuron</source><volume>58</volume><fpage>451</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.02.021</pub-id><pub-id pub-id-type="pmid">18466754</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>E</given-names></name><name><surname>Seo</surname> <given-names>M</given-names></name><name><surname>Dal Monte</surname> <given-names>O</given-names></name><name><surname>Averbeck</surname> <given-names>BB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Injection of a dopamine type 2 receptor antagonist into the dorsal striatum disrupts choices driven by previous outcomes, but not perceptual inference</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>6298</fpage><lpage>6306</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4561-14.2015</pub-id><pub-id pub-id-type="pmid">25904783</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>J</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Signals in human striatum are appropriate for policy update rather than value prediction</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>5504</fpage><lpage>5511</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6316-10.2011</pub-id><pub-id pub-id-type="pmid">21471387</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewenstein</surname> <given-names>Y</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Operant matching is a generic outcome of synaptic plasticity based on the covariance between reward and neural activity</article-title><source>PNAS</source><volume>103</volume><fpage>15224</fpage><lpage>15229</lpage><pub-id pub-id-type="doi">10.1073/pnas.0505220103</pub-id><pub-id pub-id-type="pmid">17008410</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewenstein</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Robustness of learning that is based on covariance-driven synaptic plasticity</article-title><source>PLoS Computational Biology</source><volume>4</volume><elocation-id>e1000007</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000007</pub-id><pub-id pub-id-type="pmid">18369414</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewenstein</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Synaptic theory of replicator-like melioration</article-title><source>Frontiers in Computational Neuroscience</source><volume>4</volume><elocation-id>17</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2010.00017</pub-id><pub-id pub-id-type="pmid">20617184</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname> <given-names>K</given-names></name><name><surname>Glimcher</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Efficient coding and the neural representation of value</article-title><source>Annals of the New York Academy of Sciences</source><volume>1251</volume><fpage>13</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2012.06496.x</pub-id><pub-id pub-id-type="pmid">22694213</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mansouri</surname> <given-names>FA</given-names></name><name><surname>Matsumoto</surname> <given-names>K</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Prefrontal cell activities related to monkeys' success and failure in adapting to rule changes in a Wisconsin card sorting test analog</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>2745</fpage><lpage>2756</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5238-05.2006</pub-id><pub-id pub-id-type="pmid">16525054</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDonald</surname> <given-names>RJ</given-names></name><name><surname>White</surname> <given-names>NM</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>A triple dissociation of memory systems: hippocampus, amygdala, and dorsal striatum</article-title><source>Behavioral Neuroscience</source><volume>107</volume><fpage>3</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1037/0735-7044.107.1.3</pub-id><pub-id pub-id-type="pmid">8447956</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mello</surname> <given-names>GB</given-names></name><name><surname>Soares</surname> <given-names>S</given-names></name><name><surname>Paton</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A scalable population code for time in the striatum</article-title><source>Current Biology</source><volume>25</volume><fpage>1113</fpage><lpage>1122</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.02.036</pub-id><pub-id pub-id-type="pmid">25913405</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname> <given-names>G</given-names></name><name><surname>Shteingart</surname> <given-names>H</given-names></name><name><surname>Loewenstein</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The misbehavior of reinforcement learning</article-title><source>Proceedings of the IEEE</source><volume>102</volume><fpage>528</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2014.2307022</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname> <given-names>RW</given-names></name><name><surname>Dezfouli</surname> <given-names>A</given-names></name><name><surname>Griffiths</surname> <given-names>KR</given-names></name><name><surname>Balleine</surname> <given-names>BW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Action-value comparisons in the dorsolateral prefrontal cortex control choice between goal-directed actions</article-title><source>Nature Communications</source><volume>5</volume><elocation-id>4390</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms5390</pub-id><pub-id pub-id-type="pmid">25055179</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neiman</surname> <given-names>T</given-names></name><name><surname>Loewenstein</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Covariance-based synaptic plasticity in an attractor network model accounts for fast adaptation in free operant learning</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>1521</fpage><lpage>1534</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2068-12.2013</pub-id><pub-id pub-id-type="pmid">23345226</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newbold</surname> <given-names>P</given-names></name><name><surname>Agiakloglou</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Bias in the sample autocorrelations of fractional noise</article-title><source>Biometrika</source><volume>80</volume><fpage>698</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1093/biomet/80.3.698</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Doherty</surname> <given-names>J</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Schultz</surname> <given-names>J</given-names></name><name><surname>Deichmann</surname> <given-names>R</given-names></name><name><surname>Friston</surname> <given-names>K</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title><source>Science</source><volume>304</volume><fpage>452</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1126/science.1094285</pub-id><pub-id pub-id-type="pmid">15087550</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Doherty</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The problem with value</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>43</volume><fpage>259</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2014.03.027</pub-id><pub-id pub-id-type="pmid">24726573</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name><name><surname>Assad</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neurons in the orbitofrontal cortex encode economic value</article-title><source>Nature</source><volume>441</volume><fpage>223</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1038/nature04676</pub-id><pub-id pub-id-type="pmid">16633341</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neurobiology of economic choice: a good-based model</article-title><source>Annual Review of Neuroscience</source><volume>34</volume><fpage>333</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-061010-113648</pub-id><pub-id pub-id-type="pmid">21456961</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname> <given-names>S</given-names></name><name><surname>Justo</surname> <given-names>D</given-names></name><name><surname>Jauffret</surname> <given-names>C</given-names></name><name><surname>Pavlicek</surname> <given-names>B</given-names></name><name><surname>Dauta</surname> <given-names>A</given-names></name><name><surname>Delmaire</surname> <given-names>C</given-names></name><name><surname>Czernecki</surname> <given-names>V</given-names></name><name><surname>Karachi</surname> <given-names>C</given-names></name><name><surname>Capelle</surname> <given-names>L</given-names></name><name><surname>Durr</surname> <given-names>A</given-names></name><name><surname>Pessiglione</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Critical roles for anterior insula and dorsal striatum in punishment-based avoidance learning</article-title><source>Neuron</source><volume>76</volume><fpage>998</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.017</pub-id><pub-id pub-id-type="pmid">23217747</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasquereau</surname> <given-names>B</given-names></name><name><surname>Nadjar</surname> <given-names>A</given-names></name><name><surname>Arkadir</surname> <given-names>D</given-names></name><name><surname>Bezard</surname> <given-names>E</given-names></name><name><surname>Goillandeau</surname> <given-names>M</given-names></name><name><surname>Bioulac</surname> <given-names>B</given-names></name><name><surname>Gross</surname> <given-names>CE</given-names></name><name><surname>Boraud</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Shaping of motor responses by incentive values through the basal ganglia</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>1176</fpage><lpage>1183</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3745-06.2007</pub-id><pub-id pub-id-type="pmid">17267573</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname> <given-names>PCB</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Understanding spurious regressions in econometrics</article-title><source>Journal of Econometrics</source><volume>33</volume><fpage>311</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1016/0304-4076(86)90001-1</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samejima</surname> <given-names>K</given-names></name><name><surname>Ueda</surname> <given-names>Y</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name><name><surname>Kimura</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Representation of action-specific reward values in the striatum</article-title><source>Science</source><volume>310</volume><fpage>1337</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1126/science.1115270</pub-id><pub-id pub-id-type="pmid">16311337</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuronal reward and decision signals: from theories to data</article-title><source>Physiological Reviews</source><volume>95</volume><fpage>853</fpage><lpage>951</lpage><pub-id pub-id-type="doi">10.1152/physrev.00023.2014</pub-id><pub-id pub-id-type="pmid">26109341</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seo</surname> <given-names>M</given-names></name><name><surname>Lee</surname> <given-names>E</given-names></name><name><surname>Averbeck</surname> <given-names>BB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Action selection and action value in frontal-striatal circuits</article-title><source>Neuron</source><volume>74</volume><fpage>947</fpage><lpage>960</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.037</pub-id><pub-id pub-id-type="pmid">22681697</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Learning in spiking neural networks by reinforcement of stochastic synaptic transmission</article-title><source>Neuron</source><volume>40</volume><fpage>1063</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00761-X</pub-id><pub-id pub-id-type="pmid">14687542</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shteingart</surname> <given-names>H</given-names></name><name><surname>Loewenstein</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reinforcement learning and human behavior</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>93</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.12.004</pub-id><pub-id pub-id-type="pmid">24709606</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stalnaker</surname> <given-names>TA</given-names></name><name><surname>Calhoon</surname> <given-names>GG</given-names></name><name><surname>Ogawa</surname> <given-names>M</given-names></name><name><surname>Roesch</surname> <given-names>MR</given-names></name><name><surname>Schoenbaum</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural correlates of stimulus-response and response-outcome associations in dorsolateral versus dorsomedial striatum</article-title><source>Frontiers in Integrative Neuroscience</source><volume>4</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2010.00012</pub-id><pub-id pub-id-type="pmid">20508747</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tai</surname> <given-names>LH</given-names></name><name><surname>Lee</surname> <given-names>AM</given-names></name><name><surname>Benavidez</surname> <given-names>N</given-names></name><name><surname>Bonci</surname> <given-names>A</given-names></name><name><surname>Wilbrecht</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Transient stimulation of distinct subpopulations of striatal neurons mimics changes in action value</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/nn.3188</pub-id><pub-id pub-id-type="pmid">22902719</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorn</surname> <given-names>CA</given-names></name><name><surname>Atallah</surname> <given-names>H</given-names></name><name><surname>Howe</surname> <given-names>M</given-names></name><name><surname>Graybiel</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Differential dynamics of activity changes in dorsolateral and dorsomedial striatal loops during learning</article-title><source>Neuron</source><volume>66</volume><fpage>781</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.04.036</pub-id><pub-id pub-id-type="pmid">20547134</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urbanczik</surname> <given-names>R</given-names></name><name><surname>Senn</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reinforcement learning in populations of spiking neurons</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>250</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1038/nn.2264</pub-id><pub-id pub-id-type="pmid">19219040</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vo</surname> <given-names>K</given-names></name><name><surname>Rutledge</surname> <given-names>RB</given-names></name><name><surname>Chatterjee</surname> <given-names>A</given-names></name><name><surname>Kable</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dorsal striatum is necessary for stimulus-value but not action-value learning in humans</article-title><source>Brain</source><volume>137</volume><fpage>3129</fpage><lpage>3135</lpage><pub-id pub-id-type="doi">10.1093/brain/awu277</pub-id><pub-id pub-id-type="pmid">25273995</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>AY</given-names></name><name><surname>Miura</surname> <given-names>K</given-names></name><name><surname>Uchida</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The dorsomedial striatum encodes net expected return, critical for energizing performance vigor</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>639</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1038/nn.3377</pub-id><pub-id pub-id-type="pmid">23584742</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning</article-title><source>Machine Learning</source><volume>8</volume><fpage>229</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1007/BF00992696</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Ripley</surname> <given-names>BD</given-names></name><name><surname>Brady</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Temporal autocorrelation in univariate linear modeling of FMRI data</article-title><source>NeuroImage</source><volume>14</volume><fpage>1370</fpage><lpage>1386</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0931</pub-id><pub-id pub-id-type="pmid">11707093</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Worthy</surname> <given-names>DA</given-names></name><name><surname>Maddox</surname> <given-names>WT</given-names></name><name><surname>Markman</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>NIH public access</article-title><source>Memory &amp; Cognition</source><volume>36</volume><fpage>1460</fpage><lpage>1469</lpage><pub-id pub-id-type="doi">10.3758/MC.36.8.1460</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wunderlich</surname> <given-names>K</given-names></name><name><surname>Rangel</surname> <given-names>A</given-names></name><name><surname>O'Doherty</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural computations underlying action-based decision making in the human brain</article-title><source>PNAS</source><volume>106</volume><fpage>17199</fpage><lpage>17204</lpage><pub-id pub-id-type="doi">10.1073/pnas.0901077106</pub-id><pub-id pub-id-type="pmid">19805082</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id><pub-id pub-id-type="pmid">26906502</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarom</surname> <given-names>O</given-names></name><name><surname>Cohen</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Putative cholinergic interneurons in the ventral and dorsal regions of the striatum have distinct roles in a two choice alternative association task</article-title><source>Frontiers in Systems Neuroscience</source><volume>5</volume><elocation-id>36</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2011.00036</pub-id><pub-id pub-id-type="pmid">21660109</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.34248.028</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Reviewing Editor</role><aff id="aff5"><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Striatal action-value neurons reconsidered&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>Elber-Dorozko and Loewenstein examine issues using trial-by-trial spike data to determine whether neural activity is associated with action-values. First, they note that standard regression inference can lead to false detection of action-value correlations when samples are not independent. They illustrate the prevalence of false detections using simulation and data with no plausible relation to action-values. Second, they note that different reinforcement learning models without action-value representations can yield significant action-value coding when analyzed using prior approaches. The authors present well-thought-out analyses and data that highlight analytic, experimental and conceptual difficulties in identifying action-value coding. Although both issues the authors examine have been acknowledged in the literature, and different approaches have been made to deal with or minimize their potential effects, the authors' paper represents an important synthesis and analysis that will likely lead to clearer future experiments.</p><p>However, there were several key concerns.</p><p>Firstly, the reviewers thought that there should be a more careful attention to the wider literature. In the reviewer discussion, this was thought to be of particular importance as you are making a technical point in a journal with a broad readership. It is particularly important that you are clear about which parts of the literature your results speak to.</p><p>For example:</p><p>1) The authors conclude that current methods of studying action-values are confounded, and propose that experiments should use actions whose reward values are not learned over time during the experiment, and instead are indicated by sensory cues with their values picked randomly on each trial.</p><p>However, the authors leave unmentioned the large literature of studies taking an alternate approach: recording striatal activity during planning and execution of instructed actions for cued reward outcomes, in which each trial randomizes the instructed action, the reward, or both. The authors should discuss what implications this literature has for whether the striatum encodes action-values vs. policies vs. other variables. Examples include the work from the labs of Schultz (e.g. Hassani et al., 2001; Cromwell and Schultz, 2003; Cromwell et al., 2005), Hikosaka (e.g. Kawagoe et al., 1998; Lauwereyns et al., Neuron 2004), and Kimura (e.g. Hori et al., 2009).</p><p>Most of these studies did not specifically claim that the activity they reported represents &quot;action-values&quot; in the reinforcement learning sense (and hence the present authors shouldn't feel obligated to try to debunk them), but they do seem highly relevant to the larger question the present authors raise. These studies did attempt to test whether neurons represented actions, values, and notably, their interaction (e.g. a cell whose activity scales more with action A's value than action B's value), which resembles the concept of &quot;action-value&quot;.</p><p>Also, these studies may be somewhat resistant to the authors' criticisms about confounds from temporal correlations (since rewards were either explicitly cued, or kept deterministic and stable in well-learned blocks of trials, rather than slowly fluctuating during extended learning) and confounds with action probability (since the actions were instructed and hence a priori equally probable on each trial).</p><p>Of particular interest, a paper by Pasquereau et al. (2007) seems to fulfill all the requirements the present authors set for a test of striatal action-value coding; if so, this seems worthy of mention. That study manipulated the reward value of four actions (up, down, left, right), randomly assigning their reward probabilities on each trial and indicating them with visual cues. Unfortunately, as the present authors note, the study did not explicitly analyze their results in the action-value vs. chosen-value framework. However, the paper did report that some neurons had significant action x value interactions – for example, a cell that is more active when planning rightward movements (action), with stronger activity when the planned movement was more valuable (value), and with this value-modulation greater for rightward movements than other movements (action x value). This is not a pure chosen-value signal as the present authors seem to claim that paper reported. One could argue that it contains a key feature of an action-value signal as the value modulation is strongest for one specific action.</p><p>2) The authors correctly pointed out that some earlier studies of action value used a suboptimal task design and their conclusions need to be more rigorously validated. However, in the broader field, the potential risk of &quot;drift&quot; in neural recording has been well recognized. For example, &quot;Neurophysiological experiments that compare activity across different blocks of trials must make efforts to be confident that any neural effects are not the result of artifacts of that design, such as slow-wave changes in neural activity over time.&quot; (Asaad et al., 2000). In the same Asaad et al. paper, a better design with repeated, alternated block types was used, similar in concept to randomized block design that the authors proposed here. Such designs have also been used in many neural studies of cognition – to name just two examples: value manipulations (Lauwereyns et al., 2002), rule manipulations (Mansouri et al., 2006). The problem thus seems relatively limited to one type of analysis that introduces temporal correlation across trials in an effort to estimate Q values. By the authors' account, this amounts to 5 papers from 3 different labs.</p><p>3) What about previous results arguing for prominence of a specific type of value representation? The authors touch on this, but it would be helpful to discuss specific results. In particular, the cited study of Wang et al., Nat Neuro 2003 reported that their unbiased angular measure of DMS value coding was distributed significantly non-uniformly, with net value (Σ<italic>Q</italic>) coding more prevalent than other types (their Figure 7). Whereas the null hypothesis simulations in this paper predict very different results, either a uniform distribution (Figure 2—figure supplement 8) or a dearth of Σ<italic>Q</italic> neurons (Figure 5—figure supplement 1). The authors should discuss whether this previous result can therefore still be interpreted as evidence of value coding (at least, net-value coding), rather than strictly policy coding, in the striatum.</p><p>(Also, it is odd that the authors cite Wang et al. as a study that &quot;claimed to provide direct evidence that neuronal activity in the striatum is specifically modulated by action-value&quot;, since the main result was specifically finding prevalence of net-value, <italic>not</italic> action-value coding).</p><p>4) I found the authors' choice of basal ganglia data misleading (Ito and Doya, 2009). First, because these data are recordings from the nucleus accumbens and ventral pallidum, which are not the first basal ganglia structures one thinks of as encoding action values. Second, because the original authors of the study from which the data was obtained noted that action value coding was low in these structures, leading them to suggest that action value coding was not the primary function of the nucleus accumbens and ventral pallidum. This is mentioned in the Results subsection “Permutation analysis of basal ganglia neurons”, but should be noted in the Discussion (the current text in the Results could probably just be moved).</p><p>5) Previous methods dealing with trial correlations have different success at reducing false positive rate of detecting action-values. In particular, the method of Wang et al. (2013) comes very close to attaining the correct size of the test for action-values. Indeed, it appears to be the only existing method from which one would reasonably conclude that the ventral striatal data set analyzed probably does not exhibit much action-value coding (2-3% above the expected 5%). I think it would be useful to have a figure in the main text comparing the different methods to the authors' permutation test (using for example, just the basal ganglia data set). In addition, Wang et al.’s method is also pretty good at identifying policy neurons, which is important because it could be applied retrospectively to existing data sets.</p><p>6) The authors' biggest suggestion for rigorously detecting a neural action value representation is &quot;Don't use a task with learning, use a trial-based design where subjects associate contexts with well-learned sets of action values&quot;. That is perfectly fine for scientists whose goal is specifically to test whether a brain area encodes action values. However, what about the many scientists whose explicit goal is to study neural representation of time-varying values, and hence <italic>need</italic> to use learning tasks? Many scientists are studying (1) the neural basis of value learning, (2) brain areas specifically involved in early learning (not well-trained performance), (3) motivational variables specifically present for time-varying action values not well-trained ones (e.g. volatility, certain forms of uncertainty, etc.). If the authors can give an approach that will let these scientists make accurate estimates of neural time-varying value coding during learning tasks, that would certainly be valuable to the field.</p><p>I feel like their methods could potentially be used to achieve this in a straightforward way (by combining their novel permutation test from Part 1 of their paper with their method of testing for correlation with both sum and difference of values from Part 2 of their paper). But they don't lay this out explicitly in their paper at present, since they are more focused on the narrower implication (&quot;Do striatal neurons encode action values?&quot;) rather than the broader implication of their results (&quot;In general, how can one properly measure time-varying action values?&quot;).</p><p>Secondly, the reviewers had some particular concerns about the action value vs. policy representation issue. For example:</p><p>1) Regarding the second confound of policy vs. action value:</p><p>- The authors seem to be arguing against a straw-man version of how to relate neural activity to behavior. Typically we infer the underlying computations by testing how well different hypothesized models can fit the behavior and then search for correlates of the most likely computation in the brain. The authors seem to test only how well the neural activity correlates with different hypothesized models.</p><p>- The proposed solution for distinguishing policy from action value also has a very high rate of false negative (78%).</p><p>2) I feel that the point the authors make about action-value vs. policy representations may actually be underselling the true extent of the confound, and so their proposed solution may not be sufficient. However, this all depends on how the authors want to define a 'policy neuron' vs. a 'value neuron', as I explain below. I think they should clarify this.</p><p>2.1) Their arguments seem to assume that neural policy representations are in the form of action probabilities, which can then be identified by the key signature that they relate to action-values in a 'relative' manner (e.g. an 'action 1' neuron that is correlated positively with <italic>Q</italic><sub>1</sub> must be correlated negatively with <italic>Q</italic><sub>2</sub>), and hence will be best fit as encoding Δ<italic>Q (Q</italic><sub>1</sub> – <italic>Q</italic><sub>2</sub>). However, depending on how they define 'policy', this may not be the case.</p><p>Notably, even for reinforcement learning agents that do not explicitly represent action-values, few of them directly learn a policy in its most raw form of a set of action probabilities. Instead, they represent the policy in a more abstract parameter space. The simplest parameterization is a vector of action strengths, one for each possible action. Then during a choice, the probability of each action is determined by applying a choice function (e.g. softmax) to the action strengths of the set of actions that are currently available. The choice's outcome is then used to do learning on the action strengths. This method is used by some traditional actor-critic agents (which represent state values and action strengths, but not action-values). My impression is that the authors' covariance-based model is similar, in that the variables that it updates when it learns are the input weights <italic>W</italic><sub>1</sub> and <italic>W</italic><sub>2</sub> to each pool (with one input weight for each action, thus being analogous to action strengths).</p><p>Note that in these models, the action <italic>strengths</italic> are not explicitly represented in a 'relative' manner; only the resulting action <italic>probabilities</italic> are (since the probabilities must sum to 1). It's not clear to me whether a neuron encoding an action strength would be classified as a 'policy neuron' or an 'action-value neuron' by the authors' current framework, nor is it clear to me which outcome the authors would prefer. I believe the dynamics of actor-critic learning would cause the action strengths to be somewhat 'relative' (e.g. the best action is nudged toward positive strength while all others are nudged to negative strength), but I'm not sure big this effect would be, or whether this would also occur for the authors' covariance-based model, or whether this would occur if &gt; 2 actions are available. It is possible that these types of learning tasks can't discriminate between action strengths (e.g. from an actor-critic) versus action-values (e.g. from a Q-learner). So, the authors should clarify whether they believe this is an important distinction for the present study.</p><p>2.2) Suppose we agree that neurons only count as coding the policy if they encode action probability (and not strength). Their proposed solution still seems model-dependent because it assumes that the policy is such that the probability of choosing an action is a function of the difference in action-values (<italic>Q</italic><sub>1</sub> – <italic>Q</italic><sub>2</sub>) and hence policy neurons can be identified as encoding Δ<italic>Q</italic> and not Σ<italic>Q</italic>. However, there is data suggesting that humans and animals are also influenced by ratios of reward rates rather than just differences (e.g. &quot;Ratio vs. difference comparators in choice&quot;, Gibbon and Fairhurst, J Exp Anal Behav 1994; &quot;Ratio and Difference Comparisons of Expected Reward in Decision Making Tasks&quot;, Worthy, Maddox, and Markman, 2008). If so, then policy neuron activity could be related to a ratio (e.g. <italic>Q</italic><sub>1</sub> / <italic>Q</italic><sub>2</sub>), which is correlated with both Δ<italic>Q</italic> and Σ<italic>Q</italic>.</p><p>Here is my proposed solution. It seems to me that if 'policy neurons' are equated to action probabilities, then the proper method of distinguishing policy from value coding would be to design a task that explicitly dissociates between the probability of choosing an action (encoded by policy neurons) and the action's value (encoded by action-value neurons). For instance, suppose an animal is found to choose based on the ratio of the reward rates, such that it chooses A 80% of the time when V(A) = 4*V(B). Then we can set up the following three trial types:</p><p>V(A), V(B), p(choose A)</p><p>8, 2, 80%</p><p>4, 1, 80%</p><p>4, 4, 50%</p><p>A neuron encoding V(A) should be twice as active on the first trial type as the other two trial types (since V(A) is twice as high), while a neuron encoding the policy p(choose A) should be equally active on the first two trial types (since p(choose A) = 80%). Of course, more trial types might be desired to further dissociate this from encoding of Σ<italic>Q</italic> and Δ<italic>Q</italic>. Also, note that this approach is model-dependent, because it requires a model of behavior to estimate the true p(action) on each trial (or else careful psychophysics to find two pairs of action-values that make the subject have the same action probabilities).</p><p>In general, to use this approach in a regression-based manner, one would (1) fit a model to behavior, (2) use the model to derive p(action,t) and V(action,t) for each action and each trial t, (3) fit neural activity as a function of those variables (and possibly others, such as the actually performed action, Σ<italic>Q</italic>, etc.), (4) test whether the neuron is significantly modulated by p(action), V(action), or both, controlling for temporal correlation using the authors' proposed method that uses task trajectories from other sessions as a control. Of course, if the model says that choice is indeed based on the value difference Δ<italic>Q</italic> as the authors currently assume, then this approach would simplify to the same one the authors currently propose.</p><p>Thirdly, the reviewers raised some questions about the corrections proposed and whether there in fact remained evidence for action value coding in the Basal Ganglia.</p><p>1) A critical assumption is that there exists temporal correlation strong enough to contaminate the analysis. It would be helpful to report the degree of this temporal correlation in the basal ganglia data set vs. the motor/auditory cortex data and the random walk model.</p><p>A figure, in the format of Figure 1D, showing the distribution of t-values for the actual basal ganglia data set analyzed with trial-matched Q estimates should be presented. This information is critical for effective comparisons to other data sets.</p><p>2) The authors proposed two possible solutions for this type of study. The first is to use a more stringent (and appropriate) criterion for significance, given the often wrongly assumed variance due to correlation. The permutation test is definitely in the right direction, particularly for reducing false positives. However, I am concerned by the really high rate of false negatives (~70% misses). &quot;Considering the population of simulated action-value neurons of Figure 1, this analysis identified 29% of the action-value neurons of Figure 1 as such&quot;. Considering other unaccountable variables in typical experiments, particularly that basal ganglia neurons may have mixed selectivity both at the population and single-neuron level, such a high false negative rate seems to carry high risks of missing a true representation.</p><p>3) The authors suggested randomized blocks as the second solution. In addition to my earlier point, by their own account, such a design is not new and has been implemented in three separate studies &gt;5 years ago. The authors pointed out some issues with those studies, which will need to be addressed in the future, but did not suggest any solutions.</p><p>4) The authors stated that the detrending analysis does not resolve the confound. However, judging from Figure 2—figure supplement 7, the detrending analysis resulted in ~29% significant Q modulation in the basal ganglia, in contrast to ~14% for random walk, ~12% for motor cortex and 10% for auditory cortex. Compared to other figures, which showed similar percentage for all four datasets, it seems that the basal ganglia data set is most robust to this analysis. Doesn't this support the idea of an action value representation in the basal ganglia?</p><p>5) The authors focus on statistical significance. Does examining the magnitude of the effects distinguish erroneous from &quot;real&quot; action value coding? It seems incomplete to only plot the t-values, which are important for understanding parameter precision, without presenting the parameters effect sizes. Can real action value coding be distinguished by effect sizes that were meaningfully large (i.e., substantive versus statistical significance)?</p><p>6) Along related lines, it seems like examining the pattern of effects is also useful. When comparing Figure 1D and Figure 2B, one can see that the erroneous detections included positive and negative Δ<italic>Q</italic> and Σ<italic>Q</italic> neurons, whereas for real detections (Figure 1D), there are much fewer of these neurons (by definition). All the erroneous detections generate spherical t-value plots, indicating that combinations of one or the other action value are independent. This seems not to be the case for real detections (in the authors simulations), nor in real data (Samejima et al., 2005). This suggests that any non-uniformity in detecting combinations of action value coding would be evidence that it is not erroneous (even if the type I error is not properly controlled).</p><p>7) The simulations in Figure 2 are useful, but it would be useful to translate the diffusion parameter (σ) of the random walk into an (auto) correlation. This would make it easier for a reader to interpret how this relates to real data.</p><p>8) Is the M1 data a proper control? It is hard to tell from the task description here. I wouldn't be able to replicate the task that was used given the description here. If that M1 data is published, a citation would be helpful. My concerns are whether it might have had unusually large temporal correlations and thus exaggerated the degree to which such correlations might confound action-value studies, due to either (1) having blocks of trials (as opposed to randomly interleaved trial types), (2) being a BMI task in which animals were trained to induce the recorded ensemble to emit specific long-duration activity patterns.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.34248.029</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>However, there were several key concerns.</p><p>Firstly, the reviewers thought that there should be a more careful attention to the wider literature. In the reviewer discussion, this was thought to be of particular importance as you are making a technical point in a journal with a broad readership. It is particularly important that you are clear about which parts of the literature your results speak to.</p><p>For example:</p><p>1) The authors conclude that current methods of studying action-values are confounded, and propose that experiments should use actions whose reward values are not learned over time during the experiment, and instead are indicated by sensory cues with their values picked randomly on each trial.</p><p>However, the authors leave unmentioned the large literature of studies taking an alternate approach: recording striatal activity during planning and execution of instructed actions for cued reward outcomes, in which each trial randomizes the instructed action, the reward, or both. The authors should discuss what implications this literature has for whether the striatum encodes action-values vs. policies vs. other variables. Examples include the work from the labs of Schultz (e.g. Hassani et al., 2001; Cromwell and Schultz, 2003; Cromwell et al., 2005), Hikosaka (e.g. Kawagoe et al., 1998; Lauwereyns et al., Neuron 2004), and Kimura (e.g. Hori et al., 2009).</p><p>Most of these studies did not specifically claim that the activity they reported represents &quot;action-values&quot; in the reinforcement learning sense (and hence the present authors shouldn't feel obligated to try to debunk them), but they do seem highly relevant to the larger question the present authors raise. These studies did attempt to test whether neurons represented actions, values, and notably, their interaction (e.g. a cell whose activity scales more with action A's value than action B's value), which resembles the concept of &quot;action-value&quot;.</p><p>Also, these studies may be somewhat resistant to the authors' criticisms about confounds from temporal correlations (since rewards were either explicitly cued, or kept deterministic and stable in well-learned blocks of trials, rather than slowly fluctuating during extended learning) and confounds with action probability (since the actions were instructed and hence a priori equally probable on each trial).</p><p>Of particular interest, a paper by Pasquereau et al. (2007) seems to fulfill all the requirements the present authors set for a test of striatal action-value coding; if so, this seems worthy of mention. That study manipulated the reward value of four actions (up, down, left, right), randomly assigning their reward probabilities on each trial and indicating them with visual cues. Unfortunately, as the present authors note, the study did not explicitly analyze their results in the action-value vs. chosen-value framework. However, the paper did report that some neurons had significant action x value interactions – for example, a cell that is more active when planning rightward movements (action), with stronger activity when the planned movement was more valuable (value), and with this value-modulation greater for rightward movements than other movements (action x value). This is not a pure chosen-value signal as the present authors seem to claim that paper reported. One could argue that it contains a key feature of an action-value signal as the value modulation is strongest for one specific action.</p></disp-quote><p>We agree with the reviewers that such trial designs, when trials are temporally independent, are not subject to the temporal correlation confound. We have added a paragraph about these papers and explained there why their findings cannot be used as a support to the striatal action-value representation hypothesis. In short, we do not doubt that the striatum plays an important role in decision making and learning. However, this finding, as well as the evidence in support of representation of other decision variables in the basal ganglia do not entail action-value representation in the striatum, as there are alternatives that are consistent with these findings. These points are clarified in the Discussion (Section “Other indications for action-value representation”).</p><p>Specifically regarding Pasquereau et al. (2007), we agree that the results are not consistent with pure chosen-value representation and changed the text accordingly. The finding that neurons are co-modulated by action and expected reward is indeed very interesting. However, it cannot be taken as evidence for action-value representation for several reasons. First, a policy neuron is also expected to be co-modulated by these two variables. Second, the example neurons in Figure 6 in that paper are clearly modulated by the value of <italic>other</italic> actions, which is inconsistent with the action-value hypothesis (no such quantitative analysis was performed at the population level). Finally, an essential test of action-value representation is that the value of the action is represented even when this action is not chosen. This was not tested in that paper (although in principle, it can be tested using existing data; The prediction of action-value representation is that the activity of that neuron is modulated by the value of the left target even when this target is not chosen). This is clarified, in short, in the “Literature search” section in the Materials and methods.</p><disp-quote content-type="editor-comment"><p>2) The authors correctly pointed out that some earlier studies of action value used a suboptimal task design and their conclusions need to be more rigorously validated. However, in the broader field, the potential risk of &quot;drift&quot; in neural recording has been well recognized. For example, &quot;Neurophysiological experiments that compare activity across different blocks of trials must make efforts to be confident that any neural effects are not the result of artifacts of that design, such as slow-wave changes in neural activity over time.&quot; (Asaad et al., 2000). In the same Asaad et al. paper, a better design with repeated, alternated block types was used, similar in concept to randomized block design that the authors proposed here. Such designs have also been used in many neural studies of cognition – to name just two examples: value manipulations (Lauwereyns et al., 2002), rule manipulations (Mansouri et al., 2006). The problem thus seems relatively limited to one type of analysis that introduces temporal correlation across trials in an effort to estimate Q values. By the authors' account, this amounts to 5 papers from 3 different labs.</p></disp-quote><p>In response to this comment, we examined the papers proposed by the reviewer. We found that this method does not resolve the temporal correlations confound, as described in the Results section about possible solutions to the first confound (section &quot;Possible solutions to the temporal correlations confound”) and in the Materials and methods section (the section “ANOVA tests for comparisons between blocks, excluding ‘drifting’ neurons”).</p><disp-quote content-type="editor-comment"><p>3) What about previous results arguing for prominence of a specific type of value representation? The authors touch on this, but it would be helpful to discuss specific results. In particular, the cited study of Wang et al., Nat Neuro 2003 reported that their unbiased angular measure of DMS value coding was distributed significantly non-uniformly, with net value (ΣQ) coding more prevalent than other types (their Figure 7). Whereas the null hypothesis simulations in this paper predict very different results, either a uniform distribution (Figure 2—figure supplement 8) or a dearth of ΣQ neurons (Figure 5—figure supplement 1). The authors should discuss whether this previous result can therefore still be interpreted as evidence of value coding (at least, net-value coding), rather than strictly policy coding, in the striatum.</p><p>(Also, it is odd that the authors cite Wang et al. as a study that &quot;claimed to provide direct evidence that neuronal activity in the striatum is specifically modulated by action-value&quot;, since the main result was specifically finding prevalence of net-value, not action-value coding).</p></disp-quote><p>We do not discuss the issue of non-uniform results in the paper but we agree that non-uniform results may be an indication of a true modulation by some variable. For example, if only neurons that are positively correlated with action-values are found (rather than negatively correlated with them) – this would be a strong indication for a modulation that is not caused by random fluctuations.</p><p>However, it is important to point out that small changes in the analysis may bias it in unexpected ways. In <xref ref-type="fig" rid="respfig1">Author response image 1</xref> we repeated the analysis of Wang et al., 2013 for the random-walk neurons. This analysis is slightly different form the one presented in Figure 2—figure supplement 8. There, we analyzed only the last 20 trials in each block (following Samejima et al. (2005), we now added a clarification in the figure legend). Wang et al. (2013) analyzed all the trials in a block except the first 10 and utilized 5-9 blocks. Analyzing all the trials in a block except the first 10 and utilizing 8 blocks (order of blocks as in Figure 2—figure supplement 10), surprisingly, we find a small, but significant bias towards representation of (𝛴𝑄) (p=2.9%), as in Wang et al., 2013.</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.34248.022</object-id><label>Author response image 1.</label><caption/><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-34248-resp-fig1-v2"/></fig><p>Importantly, we have not fully followed the experimental setting in Wang et al. (2013). Specifically, we were not sure what was their rule for a termination of a block and we used the Samejima et al. (2005) rule. Therefore, we are unsure about the consequence of the bias we now found to their conclusions. However, this analysis shows that a biased result is not always an indication of true modulation.</p><p>With respect to the second point, we agree that Wang et al.’s (2013) main point is that the dorsomedial striatum represents net-value (i.e., 𝛴𝑄). However, they do report that &quot;in the DMS, all categories of neuron types were represented above chance&quot; (p. 645). Nevertheless, we added this point in the legend of Figure 2—figure supplement 8, where the Wang et al. (2013) analysis is repeated.</p><p>(Computation of p-value: The p-value for the probability of receiving this fraction of state neurons was computed under the assumption that the significant neurons were distributed uniformly between classifications. If classification is uniform, the expected fraction of neurons in each category will be 10.11%. Here we classified 11.93% of the neurons as representing state. We used 20,000 neurons in 1000 different sessions. Taking 1000 sessions as the sample size, we calculated the probability of a binomial distribution with prob. 10.11% to yield more than 119 classifications in 1000 sessions).</p><disp-quote content-type="editor-comment"><p>4) I found the authors' choice of basal ganglia data misleading (Ito and Doya, 2009). First, because these data are recordings from the nucleus accumbens and ventral pallidum, which are not the first basal ganglia structures one thinks of as encoding action values. Second, because the original authors of the study from which the data was obtained noted that action value coding was low in these structures, leading them to suggest that action value coding was not the primary function of the nucleus accumbens and ventral pallidum. This is mentioned in the Results subsection “Permutation analysis of basal ganglia neurons”, but should be noted in the Discussion (the current text in the Results could probably just be moved).</p></disp-quote><p>We moved the text to the Discussion (section “Temporal correlations and action-value representations”, fourth paragraph).</p><disp-quote content-type="editor-comment"><p>5) Previous methods dealing with trial correlations have different success at reducing false positive rate of detecting action-values. In particular, the method of Wang et al. (2013) comes very close to attaining the correct size of the test for action-values. Indeed, it appears to be the only existing method from which one would reasonably conclude that the ventral striatal data set analyzed probably does not exhibit much action-value coding (2-3% above the expected 5%). I think it would be useful to have a figure in the main text comparing the different methods to the authors' permutation test (using for example, just the basal ganglia data set). In addition, Wang et al.’s method is also pretty good at identifying policy neurons, which is important because it could be applied retrospectively to existing data sets.</p></disp-quote><p>In an attempt to make our analyses as similar as possible to the original analyses we used different thresholds for significance for different methods. Specifically, in Wang et al. analysis we find that 7% – 8% of the basal ganglia neurons represent an action value, whereas only 0.25% are expected by chance. To clarify this, we added the significance threshold to the different figures to make this difference clear.</p><p>Regarding the analysis in Wang et al. (2013) on policy neurons, we address this question in the section “Is this confound the result of an analysis that is biased against policy representation?”. This analysis indeed yields more policy than action-value neurons, but still a fraction much larger than expected by chance of policy neurons is classified as action-value neurons.</p><p>With regards to the suggestion of adding the figure, we are unsure about the added value of such a figure. In the supplementary figures we demonstrate that all these methods erroneously classify neurons in the basal ganglia recordings as representing unrelated action-values. In view of these findings, we fear that using them to identify true action-values in those recordings may mislead the readers.</p><disp-quote content-type="editor-comment"><p>6) The authors' biggest suggestion for rigorously detecting a neural action value representation is &quot;Don't use a task with learning, use a trial-based design where subjects associate contexts with well-learned sets of action values&quot;. That is perfectly fine for scientists whose goal is specifically to test whether a brain area encodes action values. However, what about the many scientists whose explicit goal is to study neural representation of time-varying values, and hence need to use learning tasks? Many scientists are studying (1) the neural basis of value learning, (2) brain areas specifically involved in early learning (not well-trained performance), (3) motivational variables specifically present for time-varying action values not well-trained ones (e.g. volatility, certain forms of uncertainty, etc.). If the authors can give an approach that will let these scientists make accurate estimates of neural time-varying value coding during learning tasks, that would certainly be valuable to the field.</p><p>I feel like their methods could potentially be used to achieve this in a straightforward way (by combining their novel permutation test from Part 1 of their paper with their method of testing for correlation with both sum and difference of values from Part 2 of their paper). But they don't lay this out explicitly in their paper at present, since they are more focused on the narrower implication (&quot;Do striatal neurons encode action values?&quot;) rather than the broader implication of their results (&quot;In general, how can one properly measure time-varying action values?&quot;).</p></disp-quote><p>The paper addresses two confounds, that are somewhat orthogonal. The temporal correlation confound can be addressed using the permutation analysis of Figure 3, which can provide strong support to the claim that the activity of a particular neuron co-varies with learning. This is a general solution for scientists studying slow processes such as learning.</p><p>Precisely defining or interpreting what the activity of the neuron represents (for example an action-value or policy) is more challenging and in general, there are no easy solutions and caution should be applied when interpreting the data. We now discuss these points in the 'Temporal correlations – beyond action-value representation' section of the Discussion.</p><p>With respect to the proposed solution, to rule out policy representation, the analysis in Figure 6 includes a regression on an orthogonal variable – state. For the two variables to be orthogonal it is required mathematically that the two action-values will have the same variance (section “A possible solution to the policy confound”). This can be achieved in a controlled experiment where reward probabilities are used, but we cannot control for the variance of the action-values when we estimate them from behavior. Therefore, we could not find a way to combine the solution from Figure 3 with the regression analysis from Figure 6. However, in other cases, this may not be an issue, depending on the specific variable and question.</p><disp-quote content-type="editor-comment"><p>Secondly, the reviewers had some particular concerns about the action value vs. policy representation issue. For example:</p><p>1) Regarding the second confound of policy vs. action value:</p><p>- The authors seem to be arguing against a straw-man version of how to relate neural activity to behavior. Typically we infer the underlying computations by testing how well different hypothesized models can fit the behavior and then search for correlates of the most likely computation in the brain. The authors seem to test only how well the neural activity correlates with different hypothesized models.</p></disp-quote><p>We respectfully disagree with the review for two reasons:</p><p>First, the reviewer hints that because action-value based models best describe behavior, we should search for action-value representations. We would like to note that while the view that action-value based models best describe behavior is widespread, there is strong evidence that favors other models (e.g., Erev et al., Economic Theory, 2007, see also Shteingart and Loewenstein, 2014 for review). Therefore, it is still an open question whether action-value representation exists in the brain.</p><p>Second, policy representation (representation of the probability of choice) is likely to exist even if the brain computes action-values. If neurons represent policy, then they may be misclassified as representing action-values.</p><disp-quote content-type="editor-comment"><p>- The proposed solution for distinguishing policy from action value also has a very high rate of false negative (78%).</p></disp-quote><p>We agree with this point and we remedied the analysis to decrease its false negative rate. For true action-value neurons, the rate of correct detection vs. false negatives depends on the strength of their modulation by action-value, together with the power of the analysis.</p><p>We used neurons whose correct detection rate in the original analyses was comparable to the literature (~40%). The analysis in the previous version of the manuscript decreased this rate to 22%. It indeed suffered from limited power also because it only employed 80 trials. To increase the power of the analysis, we repeated the analysis using 400 trials in total (rather than the original 280 trials) and conducting the analysis on the last 200 trials. We now correctly classify 32% of action-value neurons as such (see Figure 6). Considering that the original analysis in Figure 1 was biased towards classifying neurons as representing action-value, rather than policy or state and that our new analysis requires passing two significance tests, we take this correct detection rate to be reasonable.</p><p>We changed Figures 4 and 6, together with their figure legends and descriptions of the analysis accordingly.</p><disp-quote content-type="editor-comment"><p>2) I feel that the point the authors make about action-value vs. policy representations may actually be underselling the true extent of the confound, and so their proposed solution may not be sufficient. However, this all depends on how the authors want to define a 'policy neuron' vs. a 'value neuron', as I explain below. I think they should clarify this.</p><p>2.1) Their arguments seem to assume that neural policy representations are in the form of action probabilities, which can then be identified by the key signature that they relate to action-values in a 'relative' manner (e.g. an 'action 1' neuron that is correlated positively with Q<sub>1</sub> must be correlated negatively with Q<sub>2</sub>), and hence will be best fit as encoding ΔQ (Q<sub>1</sub> – Q<sub>2</sub>). However, depending on how they define 'policy', this may not be the case.</p><p>Notably, even for reinforcement learning agents that do not explicitly represent action-values, few of them directly learn a policy in its most raw form of a set of action probabilities. Instead, they represent the policy in a more abstract parameter space. The simplest parameterization is a vector of action strengths, one for each possible action. Then during a choice, the probability of each action is determined by applying a choice function (e.g. softmax) to the action strengths of the set of actions that are currently available. The choice's outcome is then used to do learning on the action strengths. This method is used by some traditional actor-critic agents (which represent state values and action strengths, but not action-values). My impression is that the authors' covariance-based model is similar, in that the variables that it updates when it learns are the input weights W<sub>1</sub> and W<sub>2</sub> to each pool (with one input weight for each action, thus being analogous to action strengths).</p><p>Note that in these models, the action strengths are not explicitly represented in a 'relative' manner; only the resulting action probabilities are (since the probabilities must sum to 1). It's not clear to me whether a neuron encoding an action strength would be classified as a 'policy neuron' or an 'action-value neuron' by the authors' current framework, nor is it clear to me which outcome the authors would prefer. I believe the dynamics of actor-critic learning would cause the action strengths to be somewhat 'relative' (e.g. the best action is nudged toward positive strength while all others are nudged to negative strength), but I'm not sure big this effect would be, or whether this would also occur for the authors' covariance-based model, or whether this would occur if &gt; 2 actions are available. It is possible that these types of learning tasks can't discriminate between action strengths (e.g. from an actor-critic) versus action-values (e.g. from a Q-learner). So, the authors should clarify whether they believe this is an important distinction for the present study.</p></disp-quote><p>The reviewer is making an interesting and important point. An initial requirement for a neuron to be considered an action-value neuron, a policy neuron or any decision variable-neuron, is that it is significantly more correlated with these decision variables than with decision variables that are unrelated to the current task. The permutation analysis of Figure 3 can be used to find such neurons.</p><p>The question of which decision variable the neuron represents (assuming that it passed the permutation test) is a more difficult one. The reason is that the different decision variables are correlated. Moreover, because these variables are all some function of past actions and rewards, and relate to future choice, many existing and future decision-making models are expected to have modules whose activity correlates with these variables. One may argue that the question of whether neurons represent action-value, policy, state or some other correlated variable is not an interesting question. This is because all these correlated decision variables implicitly encode action-value. Even direct-policy models can be taken to implicitly encode action-value, because policy is correlated with the difference between the action-values. However, we believe that the difference between action-value representation and representation of other variables is an important one, because it centers on the question of the computational model that underlies decision-making in these tasks.</p><p>Often, reports of action-value representation are taken to support the hypothesis that action-values are explicitly computed in the brain, and that these action-values play a specific role in the decision making process. While other models may include no such calculation they can still include neuronal activity that correlates with action-value, as in the covariance-based plasticity model (at the level of the population). One proper way of ruling out competing hypotheses about the variables the neuronal activity correlates with is to test for significant correlations in directions that are correlated with action-value but are orthogonal to each of the competing hypotheses.</p><p>Clearly, one cannot attempt to rule out all possible hypotheses. However, even in the restricted framework of value-based Q-learning, a necessary condition for a neuron to be considered as representing an action-value is that it is not representing other decision variables <italic>of that model</italic> such as policy. Regarding alternative models for learning, clearly the more restrictive the characterization of the response properties of a neuron in the task, the more informative it is about the underlying neural computation.</p><p>We added a section in the Discussion titled “Are action-value representations a necessary part of decision making? “that addresses these issues.</p><disp-quote content-type="editor-comment"><p>2.2) Suppose we agree that neurons only count as coding the policy if they encode action probability (and not strength). Their proposed solution still seems model-dependent because it assumes that the policy is such that the probability of choosing an action is a function of the difference in action-values (Q<sub>1</sub> – Q<sub>2</sub>) and hence policy neurons can be identified as encoding ΔQ and not ΣQ. However, there is data suggesting that humans and animals are also influenced by ratios of reward rates rather than just differences (e.g. &quot;Ratio vs. difference comparators in choice&quot;, Gibbon and Fairhurst, J Exp Anal Behav 1994; &quot;Ratio and Difference Comparisons of Expected Reward in Decision Making Tasks&quot;, Worthy, Maddox, and Markman, 2008). If so, then policy neuron activity could be related to a ratio (e.g. Q<sub>1</sub> / Q<sub>2</sub>), which is correlated with both ΔQ and ΣQ.</p></disp-quote><p>We agree, but any analysis can only consider and compare the hypotheses that are explicitly acknowledged. We added a paragraph in the Discussion addressing this point (section “Differentiating action-value from other decision variables”, fifth paragraph).</p><disp-quote content-type="editor-comment"><p>Here is my proposed solution. It seems to me that if 'policy neurons' are equated to action probabilities, then the proper method of distinguishing policy from value coding would be to design a task that explicitly dissociates between the probability of choosing an action (encoded by policy neurons) and the action's value (encoded by action-value neurons). For instance, suppose an animal is found to choose based on the ratio of the reward rates, such that it chooses A 80% of the time when V(A) = 4*V(B). Then we can set up the following three trial types:</p><p>V(A), V(B), p(choose A)</p><p>8, 2, 80%</p><p>4, 1, 80%</p><p>4, 4, 50%</p><p>A neuron encoding V(A) should be twice as active on the first trial type as the other two trial types (since V(A) is twice as high), while a neuron encoding the policy p(choose A) should be equally active on the first two trial types (since p(choose A) = 80%). Of course, more trial types might be desired to further dissociate this from encoding of ΣQ and ΔQ. Also, note that this approach is model-dependent, because it requires a model of behavior to estimate the true p(action) on each trial (or else careful psychophysics to find two pairs of action-values that make the subject have the same action probabilities).</p><p>In general, to use this approach in a regression-based manner, one would (1) fit a model to behavior, (2) use the model to derive p(action,t) and V(action,t) for each action and each trial t, (3) fit neural activity as a function of those variables (and possibly others, such as the actually performed action, ΣQ, etc.), (4) test whether the neuron is significantly modulated by p(action), V(action), or both, controlling for temporal correlation using the authors' proposed method that uses task trajectories from other sessions as a control. Of course, if the model says that choice is indeed based on the value difference ΔQ as the authors currently assume, then this approach would simplify to the same one the authors currently propose.</p></disp-quote><p>This is an elegant experimental design and not unlike the one we consider in Figure 6. However, with respect to the proposed analysis, there are two important differences. One is the question of whether behavior is modulated by the ratio of reward rates, the difference of reward rates or a different function. In the paper we posited that it is the difference in the reward rates that modulates behavior when analyzing the data in the value-based framework. We agree, that it is possible that the ratio is a better predictor of behavior. Our choice followed that of the previous publications and is based on the assumption of the Q-learning model that the probability of choice is a monotonic function of the difference between action-values.</p><p>Second, in point 4, the reviewers propose to test the type of representation by looking for significant modulation or the lack of it. However, a non-significant result for one variable, is not an indication that it was not the modulator. As described in Figure 5, this can lead to confounds. Furthermore, policy and action-value will have shared variance, and so some of the modulation of the neuronal activity cannot be conclusively attributed to any of them. Therefore, it is better to use model comparison (likelihood) when considering the results of this analysis. In our manuscript we focus on significance tests that can rule out specific possibilities under the null hypothesis.</p><p>Note, that the design suggested by the reviewers can also be used to reject the hypothesis that neurons are policy neurons. For neurons whose activity differs significantly between the first two cases (p(choose A)=80%) the null hypothesis that they represent policy can be rejected. In the experimental design we simulate in the paper (Figure 6) this is like comparing the activity of neurons at the end of two blocks where the policy is similar (this is an assumption which can be tested empirically). We can compare the neural activity in (0.1, 0.5) with (0.5, 0.9), and the activity in (0.5, 0.1) with (0.9, 0.5). To rule out the possibility of state representation we should compare the activity and the end of the following blocks: compare (0.1, 0.5) with (0.5, 0.1), and (0.5, 0.9) with (0.9, 0.5). As the reviewers note, this is in fact exactly what we do in the analysis in Figure 6. We regress neuronal activity on state – sum(0.1, 0.5)=0.6, sum(0.5, 0.9)=1.4, sum(0.5, 0.1)=0.6, sum(0.9, 0.5)=1.4. This effectively compares activity in cases with the same policies in a regression model.</p><disp-quote content-type="editor-comment"><p>Thirdly, the reviewers raised some questions about the corrections proposed and whether there in fact remained evidence for action value coding in the Basal Ganglia.</p><p>1) A critical assumption is that there exists temporal correlation strong enough to contaminate the analysis. It would be helpful to report the degree of this temporal correlation in the basal ganglia data set vs. the motor/auditory cortex data and the random walk model.</p></disp-quote><p><xref ref-type="fig" rid="respfig2">Author response image 2</xref> shows a plot of the autocorrelation of the spike counts in each trial for the different data sets (averaged over the spike counts in each group; light-colors denote SEM; computed using MATLAB’s ‘autocorr’ function).</p><p>We believe that it is better to refrain from including this figure in the paper for two reasons: (1) The autocorrelations relevant for the temporal correlations confound are those associated with the time-scale relevant for learning, tens of trials. Computing such autocorrelations in experiments of a few hundreds of trials introduces substantial biases (Newbold and Agiakloglou, 1993; Kohn, 2006). This is also demonstrated in the negative autocorrelation of the random-walk spike counts, computed using sessions of 151-379 trials. Alternative measures for autocorrelation are also problematic when applied to small samples, see (Kohn, 2006). (2) We are not aware of theoretical mapping from the autocorrelation function to the temporal correlations confound. For example, considering the autocorrelations below, it is not clear how to compare the basal ganglia and the motor cortex datasets with respect to the temporal correlations confound when considering their autocorrelation functions. For these reasons, computing autocorrelation functions to quantify the temporal correlations confound may be misleading rather than useful.</p><p>We added a paragraph to the manuscript, describing the potential problems with the autocorrelation measure (section “Possible solutions to the temporal correlations confound”, second paragraph).</p><fig id="respfig2"><object-id pub-id-type="doi">10.7554/eLife.34248.023</object-id><label>Author response image 2.</label><caption/><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-34248-resp-fig2-v2"/></fig><disp-quote content-type="editor-comment"><p>A figure, in the format of Figure 1D, showing the distribution of t-values for the actual basal ganglia data set analyzed with trial-matched Q estimates should be presented. This information is critical for effective comparisons to other data sets.</p></disp-quote><p>We added Figure 3—figure supplement 1, which reports this information.</p><disp-quote content-type="editor-comment"><p>2) The authors proposed two possible solutions for this type of study. The first is to use a more stringent (and appropriate) criterion for significance, given the often wrongly assumed variance due to correlation. The permutation test is definitely in the right direction, particularly for reducing false positives. However, I am concerned by the really high rate of false negatives (~70% misses). &quot;Considering the population of simulated action-value neurons of Figure 1, this analysis identified 29% of the action-value neurons of Figure 1 as such&quot;. Considering other unaccountable variables in typical experiments, particularly that basal ganglia neurons may have mixed selectivity both at the population and single-neuron level, such a high false negative rate seems to carry high risks of missing a true representation.</p></disp-quote><p>The rate of misses of action-value neurons in our analysis depends on the parameters that we used to model these neurons. We used parameters such that the &quot;standard&quot; methods miss approximately 60% of the action value neurons. With the permutation test we miss approximately 70%. Other parameters would yield different rates of misses. If selectivity is weak then indeed, it will be more difficult to identify such neurons. However, a necessary condition for a neuron to be classified as a task-related neuron is that it is more correlated with decision variables in its corresponding session than with these decision variables in other sessions. We do not see a way around it even if this requirement is associated with a substantial rate of false identifications.</p><p>One approach to increase the power of any analysis will be to use as many trials as possible, as can be seen from the increase in the correct detection rate in Figure 6, caused by the addition of trials (we could not add trials in this analysis because we analyzed the original neurons of Figure 1). Another alternative is to consider population coding rather than to focus on individual neurons. This analysis is, however, beyond the scope of this paper.</p><disp-quote content-type="editor-comment"><p>3) The authors suggested randomized blocks as the second solution. In addition to my earlier point, by their own account, such a design is not new and has been implemented in three separate studies &gt;5 years ago. The authors pointed out some issues with those studies, which will need to be addressed in the future, but did not suggest any solutions.</p></disp-quote><p>We are not sure that we understand this comment. In our second solution we proposed randomized trials and not randomized blocks. If the reviewer relates to the similarity of our second solution to (Fitzgerald, Friston and Dolan, 2012) then crucially, we used reward probabilities in the analysis and not estimated action-values. This removes temporal correlations which are present when estimated action-values are used (see Figure 2—figure supplement 9). In addition, our analysis in Figure 6 rules out policy and state representations, which was not present in (Fitzgerald, Friston and Dolan, 2012). This last point is also relevant to (Cai et al., 2011 and Kim et al., 2012).</p><disp-quote content-type="editor-comment"><p>4) The authors stated that the detrending analysis does not resolve the confound. However, judging from Figure 2—figure supplement 7, the detrending analysis resulted in ~29% significant Q modulation in the basal ganglia, in contrast to ~14% for random walk, ~12% for motor cortex and 10% for auditory cortex. Compared to other figures, which showed similar percentage for all four datasets, it seems that the basal ganglia data set is most robust to this analysis. Doesn't this support the idea of an action value representation in the basal ganglia?</p></disp-quote><p>Originally, we were not clear enough on this issue. We’ve added clarifying sentences in the figure legends. The analysis of the basal ganglia data in Figure 2—figure supplement 7<italic>erroneously</italic> identified <italic>unrelated</italic> action-values from simulations. In fact, this analysis indicates that detrending is even <italic>less</italic> useful there than in other datasets.</p><disp-quote content-type="editor-comment"><p>5) The authors focus on statistical significance. Does examining the magnitude of the effects distinguish erroneous from &quot;real&quot; action value coding? It seems incomplete to only plot the t-values, which are important for understanding parameter precision, without presenting the parameters effect sizes. Can real action value coding be distinguished by effect sizes that were meaningfully large (i.e., substantive versus statistical significance)?</p></disp-quote><p>To address this comment, we compared the explained variance of the action-value and random-walk neurons used in our paper. Surprisingly, the explained variance of the random-walk neurons is <italic>higher</italic> than that of the true action-value neurons.</p><fig id="respfig3"><object-id pub-id-type="doi">10.7554/eLife.34248.024</object-id><label>Author response image 3.</label><caption/><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-34248-resp-fig3-v2"/></fig><p>One may argue that very high explained variance (say R<sup>2</sup> &gt; 0.25) can be used as conclusive evidence of action-value representation. However, we find that if the diffusion coefficient of the random-walk neurons is sufficiently large then a substantial fraction of the neurons will exhibit high values of R<sup>2</sup>. For example, with a diffusion coefficient of 0.5 31% of the random-walk neurons exhibit R<sup>2</sup> &gt; 0.25.</p><disp-quote content-type="editor-comment"><p>6) Along related lines, it seems like examining the pattern of effects is also useful. When comparing Figure 1D and Figure 2B, one can see that the erroneous detections included positive and negative ΔQ and ΣQ neurons, whereas for real detections (Figure 1D), there are much fewer of these neurons (by definition). All the erroneous detections generate spherical t-value plots, indicating that combinations of one or the other action value are independent. This seems not to be the case for real detections (in the authors simulations), nor in real data (Samejima et al., 2005). This suggests that any non-uniformity in detecting combinations of action value coding would be evidence that it is not erroneous (even if the type I error is not properly controlled).</p></disp-quote><p>We partially answer this question (question 3 in the first set of comments above). Some non-uniformities may indeed indicate that the result are not due to random modulations. However, even when dealing with random modulations we may see certain biases that are caused by the design of the analysis. Another example is Figure 2. There we find that in the random-walk dataset, the fraction of state neurons is larger than that of policy neurons. We shortly address the fact that the results may be biased towards a specific classification in some experimental designs in Figure 2—figure supplements 4, 5, and Figure 3—figure supplement 1.</p><disp-quote content-type="editor-comment"><p>7) The simulations in Figure 2 are useful, but it would be useful to translate the diffusion parameter (σ) of the random walk into an (auto) correlation. This would make it easier for a reader to interpret how this relates to real data.</p></disp-quote><p>As discussed above, we fear that presenting autocorrelations may be misleading. Particularly, the autocorrelations of the random-walk function for a finite (and small) number of trials, which is relevant for experiments is very different from the function obtained when the number of trials is large. This is depicted in <xref ref-type="fig" rid="respfig4">Author response image 4</xref>, where we compare the autocorrelation of the random-walk sessions of the paper, with the autocorrelation function of the same process, computed using 5,000 trials.</p><fig id="respfig4"><object-id pub-id-type="doi">10.7554/eLife.34248.025</object-id><label>Author response image 4.</label><caption/><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-34248-resp-fig4-v2"/></fig><disp-quote content-type="editor-comment"><p>8) Is the M1 data a proper control? It is hard to tell from the task description here. I wouldn't be able to replicate the task that was used given the description here. If that M1 data is published, a citation would be helpful. My concerns are whether it might have had unusually large temporal correlations and thus exaggerated the degree to which such correlations might confound action-value studies, due to either (1) having blocks of trials (as opposed to randomly interleaved trial types), (2) being a BMI task in which animals were trained to induce the recorded ensemble to emit specific long-duration activity patterns.</p></disp-quote><p>The motor cortex data was recorded in Eilon Vaadia’s lab and has not been published yet. We agree that the specific task the subject is performing may influence the overall firing rate or the temporal correlations in the neural activity and hence the false positive rates in the detection of action-value representation. However, we think it is unlikely that the recordings in this data set are an outlier in terms of autocorrelations. First, the monkey was extensively trained and all trials were identical, so there is nothing in the design of the task that suggests long-term correlations between trials. Second, the monkey was conditioned to enhance the power of beta band frequencies (20-30Hz). This frequency band is two orders of magnitude different than the time scale separating different trials (on average 14.2 seconds). Finally, we considered spike count <italic>prior</italic> to the beginning of the trials, while the monkey was still waiting for a GO signal.</p></body></sub-article></article>