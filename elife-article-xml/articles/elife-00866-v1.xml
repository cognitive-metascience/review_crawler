<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="article-commentary" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">00866</article-id><article-id pub-id-type="doi">10.7554/eLife.00866</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Insight</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Watching the brain in action</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-3283"><name><surname>Mahon</surname><given-names>Bradford Z</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="conf1"/><x> is at the </x><aff id="aff1"><institution content-type="dept">Department of Brain and Cognitive Sciences, and the Department of Neurosurgery</institution>, <institution>University of Rochester</institution>, <addr-line><named-content content-type="city">Rochester</named-content></addr-line>, <country>United States</country> <email>mahon@rcbi.rochester.edu</email></aff></contrib></contrib-group><pub-date date-type="pub" publication-format="electronic"><day>28</day><month>05</month><year>2013</year></pub-date><pub-date pub-type="collection"><year>2013</year></pub-date><volume>2</volume><elocation-id>e00866</elocation-id><permissions><copyright-statement>© 2013, Mahon</copyright-statement><copyright-year>2013</copyright-year><copyright-holder>Mahon</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-00866-v1.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="commentary-article" xlink:href="10.7554/eLife.00425"/><abstract><p>Functional magnetic resonance imaging has been used to identify the different networks in the brain that underpin the use of tools by humans.</p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>fMRI</kwd><kwd>tool use</kwd><kwd>intention</kwd><kwd>action</kwd><kwd>neuroscience</kwd><kwd>motor</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group></article-meta></front><body><boxed-text><p><bold>Related research article</bold> Gallivan JP, McLean DA, Valyear KF, Culham JC. 2013. Decoding the neural mechanisms of human tool use. <italic>eLife</italic> <bold>2</bold>:e00425. doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7554/eLife.00425">10.7554/eLife.00425</ext-link></p><p><bold>Image</bold> Certain brain regions respond preferentially to viewing tools and planning tool actions (red) or to viewing the human body and planning hand actions (green)</p><p><inline-graphic xlink:href="elife-00866-inf1-v1"/></p></boxed-text><p>In our daily lives, we interact with a vast array of objects—from pens and cups to hammers and cars. Whenever we recognize and use an object, our brain automatically accesses a wealth of background knowledge about the object’s structure, properties and functions, and about the movements associated with its use. We are also constantly observing our own actions as we engage with objects, as well as those of others. A key question is: how are these distinct types of information, which are distributed across different regions of the brain, integrated in the service of everyday behavior? Addressing this question involves specifying the internal organizational structure of the representations of each type of information, as well as the way in which information is exchanged or combined across different regions. Now, in <italic>eLife</italic>, Jody Culham at the University of Western Ontario (UWO) and co-workers report a significant advance in our understanding of these ‘big picture’ issues by showing how a specific type of information about object-directed actions is coded across the brain (<xref ref-type="bibr" rid="bib5">Gallivan et al., 2013b</xref>).</p><p>A great deal is known about which brain regions represent and process different types of knowledge about objects and actions (<xref ref-type="bibr" rid="bib10">Martin, 2007</xref>). For instance, visual information about the structure and form of objects, and of body parts, is represented in ventral and lateral temporal occipital regions (<xref ref-type="bibr" rid="bib6">Goodale and Milner, 1992</xref>). Visuomotor processing in support of object-directed action, such as reaching and grasping, is represented in dorsal occipital and posterior parietal regions (<xref ref-type="bibr" rid="bib3">Culham et al., 2003</xref>). Knowledge about how to manipulate objects according to their function is represented in inferior-lateral parietal cortex, and in premotor regions of the frontal lobe (<xref ref-type="bibr" rid="bib3">Culham et al., 2003</xref>; <xref ref-type="bibr" rid="bib8">Johnson-Frey, 2004</xref>).<fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Summary of the networks of brain regions that code for movements of hands and tools.</title><p>By comparing brain activation as subjects prepared to reach towards or grasp an object using their hands or a tool, Gallivan et al. identified four networks that code for distinct components of object-directed actions. Some brain regions code for planned actions that involve the hands but not tools (red), and others for actions that involve tools but not the hands (blue). A third set of regions codes for actions involving either the hands or tools, but uses different neural representations for each (pink). A final set of areas code the type of action to be performed, distinguishing between reaching towards an object as opposed to grasping it, irrespective of whether a tool or the hands alone are used (purple). The red lines represent the frontoparietal network implicated in hand actions, with the short dashes showing the subnetwork involved in reaching, and the long dashes, the subnetwork involved in grasping. The blue solid lines show the network implicated in tool use, while the green line connects areas comprising a subset of the perception network.</p></caption><graphic xlink:href="elife-00866-fig1-v1.tif"/><attrib>FIGURE CREDIT: IMAGE ADAPTED FROM FIGURE 7 IN <xref ref-type="bibr" rid="bib5">GALLIVAN ET AL., 2013B</xref></attrib></fig></p><p>Culham and colleagues—who are based at the UWO, Queen's University and the University of Missouri, and include Jason Gallivan as first author—focus their investigation on the neural substrates that underlie our ability to grasp objects. They used functional magnetic resonance imaging (fMRI) to scan the brains of subjects performing a task in which they had to alternate between using their hands or a set of pliers to reach towards or grasp an object. Ingeniously, the pliers were reverse pliers—constructed so that the business end opens when you close your fingers, and closes when your fingers open. This made it possible to dissociate the goal of each action (e.g., ‘grasp’) from the movements involved in its execution (since in the case of the pliers, ‘grasping’ is accomplished by opening the hand).</p><p>Gallivan et al. used multivariate analyses to test whether the pattern of responses elicited across a set of voxels (or points in the brain) when the participant reaches to touch an object can be distinguished from the pattern elicited across the same voxels when they grasp the object. In addition, they sought to identify three classes of brain regions: those that code grasping of objects with the hand (but not the pliers), those that code grasping of objects with the pliers (but not the hand), and those that have a common code for grasping with both the hand and the pliers (that is, a code for grasping that is independent of the specific movements involved).</p><p>One thing that makes this study particularly special is that Gallivan et al. performed their analyses on the fMRI data just ‘before’ the participants made an overt movement. In other words, they examined where in the brain the ‘intention’ to move is represented. Specifically, they asked: which brain regions distinguish between intentions corresponding to different types of object-directed actions? They found that certain regions decode upcoming actions of the hand but not the pliers (superior-parietal/occipital cortex and lateral occipital cortex), whereas other regions decode upcoming actions involving the pliers but not the hand (supramarginal gyrus and left posterior middle temporal gyrus). A third set of regions uses a common code for upcoming actions of both the hands and the pliers (subregions of the intraparietal sulcus and premotor regions of the frontal lobe).</p><p>The work of Gallivan et al. significantly advances our understanding of how the brain codes upcoming actions involving the hands. Research by a number of teams is converging to suggest that such actions activate regions of lateral occipital cortex that also respond to images of hands (<xref ref-type="bibr" rid="bib1">Astafiev et al., 2004</xref>; <xref ref-type="bibr" rid="bib12">Peelen and Downing, 2005</xref>; <xref ref-type="bibr" rid="bib11">Orlov et al., 2010</xref>; <xref ref-type="bibr" rid="bib2">Bracci et al., 2012</xref>). Moreover, a previous paper from Gallivan and colleagues reported that upcoming hand actions (grasping versus reaching with the fingers) can be decoded in regions of ventral and lateral temporal-occipital cortex that were independently defined as showing differential BOLD responses for different categories of objects (e.g., objects, scenes, body parts; <xref ref-type="bibr" rid="bib4">Gallivan et al., 2013a</xref>). Furthermore, the regions of lateral occipital cortex that respond specifically to images of hands are directly adjacent to those that respond specifically to images of tools, and also exhibit strong functional connectivity with areas of somatomotor cortex (<xref ref-type="bibr" rid="bib2">Bracci et al., 2012</xref>).</p><p>Taken together, these latest results and the existing literature point toward a model in which the connections between visual areas and somatomotor regions help to organize high level visual areas (<xref ref-type="bibr" rid="bib9">Mahon and Caramazza, 2011</xref>), and to integrate visual and motor information online to support object-directed action. An exciting issue raised by this study is the degree to which tools may have multiple levels of representation across different brain regions: some regions seem to represent tools as extensions of the human body (<xref ref-type="bibr" rid="bib7">Iriki et al., 1996</xref>), while other regions represent them as discrete objects to be acted upon by the body. The work of Gallivan et al. suggests a new way of understanding how these different representations of tools are combined in the service of everyday behavior.</p></body><back><fn-group content-type="competing-interest"><fn fn-type="conflict" id="conf1"><label>Competing interests:</label><p>The author declares that no competing interests exist.</p></fn></fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Astafiev</surname><given-names>SV</given-names></name><name><surname>Stanley</surname><given-names>CM</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name></person-group><year>2004</year><article-title>Extrastriate body area in human occipital cortex responds to the performance of motor actions</article-title><source>Nat Neurosci</source><volume>7</volume><fpage>542</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nn1241</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Cavina-Pratesi</surname><given-names>C</given-names></name><name><surname>Ietswaart</surname><given-names>M</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year>2012</year><article-title>Closely overlapping responses to tools and hands in left lateral occipitotemporal cortex</article-title><source>J Neurophysiol</source><volume>107</volume><fpage>1443</fpage><lpage>1456</lpage><pub-id pub-id-type="doi">10.1152/jn.00619.2011</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Culham</surname><given-names>JC</given-names></name><name><surname>Danckert</surname><given-names>SL</given-names></name><name><surname>De Souza</surname><given-names>JFX</given-names></name><name><surname>Gati</surname><given-names>JS</given-names></name><name><surname>Menon</surname><given-names>RS</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><year>2003</year><article-title>Visually guided grasping produces fMRI activation in dorsal but not ventral stream brain areas</article-title><source>Exp Brain Res</source><volume>153</volume><fpage>180</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1007/s00221-003-1591-5</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>Chapman</surname><given-names>CS</given-names></name><name><surname>McLean</surname><given-names>DA</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><year>2013a</year><article-title>Activity patterns in the category-selective occipitotemporal cortex predict upcoming motor actions</article-title><source>Eur J Neurosci</source><pub-id pub-id-type="doi">10.1111/ejn.12215</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>McLean</surname><given-names>DA</given-names></name><name><surname>Valyear</surname><given-names>KF</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><year>2013b</year><article-title>Decoding the neural mechanisms of human tool use</article-title><source>eLife</source><volume>2</volume><fpage>e00425</fpage><pub-id pub-id-type="doi">10.7554/eLife.00425</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name></person-group><year>1992</year><article-title>Separate visual pathways for perception and action</article-title><source>Trends Neurosci</source><volume>15</volume><fpage>20</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(92)90344-8</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iriki</surname><given-names>A</given-names></name><name><surname>Tanaka</surname><given-names>M</given-names></name><name><surname>Iwamura</surname><given-names>Y</given-names></name></person-group><year>1996</year><article-title>Coding of modified body schema during tool use by macaque postcentral neurones</article-title><source>Neuroreport</source><volume>7</volume><fpage>2325</fpage><lpage>2330</lpage><pub-id pub-id-type="doi">10.1097/00001756-199610020-00010</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson-Frey</surname><given-names>SH</given-names></name></person-group><year>2004</year><article-title>The neural bases of complex tool use in humans</article-title><source>Trends Cogn Sci</source><volume>8</volume><fpage>71</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2003.12.002</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahon</surname><given-names>BZ</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year>2011</year><article-title>What drives the organization of object knowledge in the brain?</article-title><source>Trends Cogn Sciences</source><volume>15</volume><fpage>97</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.01.004</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year>2007</year><article-title>The representation of object concepts in the brain</article-title><source>Ann Rev Psychol</source><volume>58</volume><fpage>25</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.57.102904.190143</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orlov</surname><given-names>T</given-names></name><name><surname>Makin</surname><given-names>TR</given-names></name><name><surname>Zohary</surname><given-names>E</given-names></name></person-group><year>2010</year><article-title>Topographic representation of the human body in the occipitotemporal cortex</article-title><source>Neuron</source><volume>68</volume><fpage>586</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.032</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><year>2005</year><article-title>Is the extrastriate body area involved in motor actions?</article-title><source>Nat Neurosci</source><volume>8</volume><fpage>125</fpage><comment>author reply 125–6</comment><pub-id pub-id-type="doi">10.1038/nn0205-125a</pub-id></element-citation></ref></ref-list></back></article>