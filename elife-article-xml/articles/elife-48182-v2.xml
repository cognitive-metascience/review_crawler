<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">48182</article-id><article-id pub-id-type="doi">10.7554/eLife.48182</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Short Report</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A neural mechanism for contextualizing fragmented inputs during naturalistic vision</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-80489"><name><surname>Kaiser</surname><given-names>Daniel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9007-3160</contrib-id><email>danielkaiser.net@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-143230"><name><surname>Turini</surname><given-names>Jacopo</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-143231"><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Psychology</institution><institution>University of York</institution><addr-line><named-content content-type="city">York</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Education and Psychology</institution><institution>Freie Universität Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Institute of Psychology</institution><institution>Goethe-Universität Frankfurt</institution><addr-line><named-content content-type="city">Frankfurt am Main</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Berlin School of Mind and Brain</institution><institution>Humboldt-Universität Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution>Bernstein Center for Computational Neuroscience Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing Editor</role><aff><institution>Peking University</institution><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>09</day><month>10</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e48182</elocation-id><history><date date-type="received" iso-8601-date="2019-05-03"><day>03</day><month>05</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-10-08"><day>08</day><month>10</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Kaiser et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Kaiser et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-48182-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.48182.001</object-id><p>With every glimpse of our eyes, we sample only a small and incomplete fragment of the visual world, which needs to be contextualized and integrated into a coherent scene representation. Here we show that the visual system achieves this contextualization by exploiting spatial schemata, that is our knowledge about the composition of natural scenes. We measured fMRI and EEG responses to incomplete scene fragments and used representational similarity analysis to reconstruct their cortical representations in space and time. We observed a sorting of representations according to the fragments' place within the scene schema, which occurred during perceptual analysis in the occipital place area and within the first 200 ms of vision. This schema-based coding operates flexibly across visual features (as measured by a deep neural network model) and different types of environments (indoor and outdoor scenes). This flexibility highlights the mechanism's ability to efficiently organize incoming information under dynamic real-world conditions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual perception</kwd><kwd>scene representation</kwd><kwd>real-world structure</kwd><kwd>fMRI/EEG</kwd><kwd>multivariate pattern analysis</kwd><kwd>deep neural network models</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>KA4683/2-1</award-id><principal-award-recipient><name><surname>Kaiser</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>CI241/1-1</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>CI241/3-1</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>ERC-2018-StG 803370</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>In scene-selective occipital cortex and within 200 ms of processing, visual inputs are sorted according to their typical spatial position within a scene.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>During natural vision, the brain continuously receives incomplete fragments of information that need to be integrated into meaningful scene representations. Here, we propose that this integration is achieved through contextualization: the brain uses prior knowledge about where information typically appears in a scene to meaningfully sort incoming information.</p><p>A format in which such prior knowledge about the world is represented in the brain is provided by schemata. First introduced to philosophy to explain how prior knowledge enables perception of the world (<xref ref-type="bibr" rid="bib37">Kant, 1781</xref>), schemata were later adapted by psychology (<xref ref-type="bibr" rid="bib3">Barlett, 1932</xref>; <xref ref-type="bibr" rid="bib53">Piaget, 1926</xref>) and computer science (<xref ref-type="bibr" rid="bib48">Minsky, 1975</xref>; <xref ref-type="bibr" rid="bib56">Rumelhart, 1980</xref>) as a means to formalize mechanisms enabling natural and artificial intelligence, respectively.</p><p>In the narrower context of natural vision, scene schemata represent knowledge about the typical composition of real-world environments (<xref ref-type="bibr" rid="bib45">Mandler, 1984</xref>). Scene schemata for example entail knowledge about the distribution of objects across scenes, where objects appear in particular locations across the scene and in particular locations with respect to other objects (<xref ref-type="bibr" rid="bib34">Kaiser et al., 2019a</xref>; <xref ref-type="bibr" rid="bib59">Torralba et al., 2006</xref>; <xref ref-type="bibr" rid="bib62">Võ et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Wolfe et al., 2011</xref>).</p><p>The beneficial role of such scene schemata was first investigated in empirical studies of human memory, where memory performance is boosted when scenes are configured in accordance with the schema (<xref ref-type="bibr" rid="bib8">Brewer and Treyens, 1981</xref>; <xref ref-type="bibr" rid="bib46">Mandler and Johnson, 1976</xref>; <xref ref-type="bibr" rid="bib47">Mandler and Parker, 1976</xref>).</p><p>Recently however, it has become clear that scene schemata not only organize memory contents, but also the contents of perception. For example, knowledge about the structure of the world can be used to generate predictions about a scene’s content (<xref ref-type="bibr" rid="bib2">Bar, 2009</xref>; <xref ref-type="bibr" rid="bib27">Henderson, 2017</xref>), or to efficiently organize the concurrent representation of multiple scene elements (<xref ref-type="bibr" rid="bib32">Kaiser et al., 2014</xref>; <xref ref-type="bibr" rid="bib35">Kaiser et al., 2019b</xref>). This position is reinforced by behavioral studies demonstrating a beneficial role of schema-congruent naturalistic stimuli across a variety of perceptual tasks, such as visual detection (<xref ref-type="bibr" rid="bib5">Biederman et al., 1982</xref>; <xref ref-type="bibr" rid="bib14">Davenport and Potter, 2004</xref>; <xref ref-type="bibr" rid="bib58">Stein et al., 2015</xref>) and visual search (<xref ref-type="bibr" rid="bib32">Kaiser et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">Torralba et al., 2006</xref>; <xref ref-type="bibr" rid="bib62">Võ et al., 2019</xref>).</p><p>Here, we put forward a novel function of scene schemata in visual processing: they support the contextualization of fragmented sensory inputs. If sensory inputs are indeed processed in relation to the schema context, scene fragments stemming from similar typical positions within the scene should be processed similarly and fragments stemming from different positions should be processed differently. Therefore, the neural representations of scene fragments should be sorted according to their typical place within the scene.</p><p>We tested two hypotheses about this sorting process. First, we hypothesized that this sorting occurs during perceptual scene analysis, which can be spatiotemporally pinpointed to scene-selective cortex (<xref ref-type="bibr" rid="bib1">Baldassano et al., 2016</xref>; <xref ref-type="bibr" rid="bib17">Epstein, 2014</xref>) and the first 250 ms of processing (<xref ref-type="bibr" rid="bib11">Cichy et al., 2017</xref>; <xref ref-type="bibr" rid="bib24">Harel et al., 2016</xref>). Second, given that schema-related effects in behavioral studies (<xref ref-type="bibr" rid="bib47">Mandler and Parker, 1976</xref>) are more robustly observed along the vertical dimension, where the scene structure is more rigid (i.e., the sky is almost always above the ground), we hypothesized that the cortical sorting of information should primarily occur along the vertical dimension.</p><p>To test these hypotheses, we used a novel visual paradigm in which participants were exposed to fragmented visual inputs, and recorded fMRI and EEG data to resolve brain activity in space and time.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In our study, we experimentally mimicked the fragmented nature of naturalistic visual inputs by dissecting scene images into position-specific fragments. Six natural scene images (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) were each split into six equally-sized fragments (three vertical × 2 horizontal), resulting in 36 conditions (six scenes × 6 fragments). In separate fMRI (n = 30) and EEG (n = 20) experiments, participants viewed these fragments at central fixation while performing an indoor/outdoor categorization task to ensure engagement with the stimulus (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Critically, this design allowed us to investigate whether the brain sorts the fragments with respect to their place in the schema in the absence of explicit location differences (<xref ref-type="fig" rid="fig1">Figure 1c</xref>).</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.48182.002</object-id><label>Figure 1.</label><caption><title>Experimental design and rationale of schema-based information sorting.</title><p>(<bold>a</bold>) The stimulus set consisted of six natural scenes (three indoor, three outdoor). Each scene was split into six rectangular fragments. (<bold>b</bold>) During the fMRI and EEG recordings, participants performed an indoor/outdoor categorization task on individual fragments. Notably, all fragments were presented at central fixation, removing explicit location information. (<bold>c</bold>) We hypothesized that the visual system sorts sensory input by spatial schemata, resulting in a cortical organization that is explained by the fragments’ within-scene location, predominantly in the vertical dimension: Fragments stemming from the same part of the scene should be represented similarly. Here we illustrate the hypothesized sorting in a two-dimensional space. A similar organization was observed in multi-dimensional scaling solutions for the fragments’ neural similarities (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> and <xref ref-type="video" rid="video1">Video 1</xref>). In subsequent analyses, the spatiotemporal emergence of the schema-based cortical organization was precisely quantified using representational similarity analysis (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.003</object-id><label>Figure 1—figure supplement 1.</label><caption><title>MDS visualization of neural RDMs 34 MDS visualization of neural RDMs.</title><p>(<bold>a/b</bold>) A multi-dimensional scaling (MDS) of the fragments’ neural similarity in OPA (<bold>a</bold>) and after 200 ms of processing (<bold>b</bold>) revealed a sorting according to vertical location, which was visible in a two-dimensional solution. This visualization suggests that schemata are a prominent organizing principle for representations in OPA and after 200 ms of vision. A time-resolved MDS for the EEG data can be found in <xref ref-type="video" rid="video1">Video 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig1-figsupp1-v2.tif"/></fig></fig-group><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-48182-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.48182.004</object-id><label>Video 1.</label><caption><title>Time-resolved MDS visualization of the neural RDMs.</title><p>To directly visualize the emergence of schematic coding from the neural data, we performed a multi-dimensional scaling (MDS) analysis, where the time-resolved neural RDMs (averaged across participants) were projected onto a two-dimensional space. The RDM time series was smoothed using a sliding averaging window (15 ms width). Computing MDS solutions across time yielded a movie (5 ms resolution), where fragments travel through an arbitrary space, eventually forming a meaningful organization. Notably, around 200 ms, a division into the three vertical locations can be observed.</p></caption></media><p>To quantify the sorting of fragments during cortical processing we used spatiotemporally resolved representational similarity analysis (<xref ref-type="bibr" rid="bib9">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib40">Kriegeskorte et al., 2008</xref>). We first extracted representational dissimilarity matrices (RDMs) from the fMRI and EEG data, which indexed pairwise dissimilarities of the fragments’ neural representations (for details on RDM construction see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). In the fMRI (<xref ref-type="fig" rid="fig2">Figure 2a</xref>), we extracted spatially-resolved neural RDMs from scene-selective occipital place area (OPA) and parahippocampal place area (PPA), and from early visual cortex (V1) (for temporal response profiles in these regions see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). In the EEG (<xref ref-type="fig" rid="fig2">Figure 2b</xref>), we extracted time-resolved neural RDMs from −200 ms to 800 ms relative to stimulus onset from posterior EEG electrodes (for other electrode groups see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplements 3</xref>–<xref ref-type="fig" rid="fig2s5">5</xref>).</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.48182.005</object-id><label>Figure 2.</label><caption><title>Spatial schemata determine cortical representations of fragmented scenes.</title><p>(<bold>a</bold>) To test where and when the visual system sorts incoming sensory information by spatial schemata, we first extracted spatially (fMRI) and temporally (EEG) resolved neural representational dissimilarity matrices (RDMs). In the fMRI, we extracted pairwise neural dissimilarities of the fragments from response patterns across voxels in the occipital place area (OPA), parahippocampal place area (PPA), and early visual cortex (V1). (<bold>b</bold>) In the EEG, we extracted pairwise dissimilarities from response patterns across electrodes at every time point from −200 ms to 800 ms with respect to stimulus onset. (<bold>c</bold>) We modelled the neural RDMs with three predictor matrices, which reflected their vertical and horizontal positions within the full scene, and their category (i.e., their scene or origin). (<bold>d</bold>) The fMRI data revealed a vertical-location organization in OPA, but not V1 and PPA. Additionally, the fragment’s category predicted responses in both scene-selective regions. (<bold>e</bold>) The EEG data showed that both vertical location and category predicted cortical responses rapidly, starting from around 100 ms. These results suggest that the fragments’ vertical position within the scene schema determines rapidly emerging representations in scene-selective occipital cortex. Significance markers represent p&lt;0.05 (corrected for multiple comparisons). Error margins reflect standard errors of the mean. In further analysis, we probed the flexibility of this schematic coding mechanism (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.006</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Details on neural dissimilarity construction.</title><p>Pairwise neural dissimilarity values were into representational dissimilarity matrices (RDMs), so that for every time point one 36 × 36 matrix containing estimates of neural dissimilarity was available. Here, an example RDM at 200 ms post-stimulus is shown, which exemplifies the ordering of fragment combinations for all RDMs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.007</object-id><label>Figure 2—figure supplement 2.</label><caption><title>fMRI response time courses.</title><p>(<bold>a</bold>) Functional MRI data were analyzed in three regions of interest (here shown on the right hemisphere): primary visual cortex (V1), occipital place area (OPA), and parahippocampal place area (PPA). Each of these ROIs showed reliable net responses to the fragments, peaking 3 TRs after stimulus onset. The activation time courses were baseline-corrected by subtracting the activation from the first two TRs. (<bold>b</bold>), GLM analysis across the response time course. Most prominently after 3 TRs, the neural organization in OPA was explained by the fragments’ vertical location, reflecting a neural coding in accordance with spatial schemata. Additionally, scene category predicted neural organization in OPA and PPA. Error margins reflect standard errors of the mean. Significance markers represent p&lt;0.05 (corrected for multiple comparisons across ROIs).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.008</object-id><label>Figure 2—figure supplement 3.</label><caption><title>Pairwise decoding across EEG electrode groups.</title><p>Based on previous studies on multivariate decoding of visual information, we restricted our main analysis to a group of posterior electrodes (where we expected the strongest effects). For comparison, we also analyzed data in central and anterior electrode groups. The central group consisted of 20 electrodes (C3, TP9, CP5, CP1, TP10, CP6, CP2, Cz, C4, C1, C5, TP7, CP3, CPz, CP4, TP8, (C6, C2, T7, T8) and the anterior group consisted of 26 electrodes (F3, F7, FT9, FC5, FC1, FT10, FC6, FC2, F4, F8, Fp2, AF7, AF3, AFz, F1, F5, FT7, FC3, FCz, FC4, FT8, F6, F2, AF4, AF8, Fpz). RDMs were constructed in an identical fashion to the posterior group used for the main analyses (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). We computed general discriminability of the 36 scene fragments in the three groups by averaging all off-diagonal elements of the RDMs. As expected, the resulting time courses of pair-wise discriminability revealed the strongest overall decoding in the posterior group, followed by the central and anterior groups. RSA results for these electrodes are found in <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4/5</xref>. Significance markers represent p&lt;0.05 (corrected for multiple comparisons). Error margins reflect standard errors of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.009</object-id><label>Figure 2—figure supplement 4.</label><caption><title>RSA using central electrodes.</title><p>(<bold>a/b</bold>) Repeating the main RSAs for the central electrode group yielded a similar pattern as the posterior group, revealing both vertical location information (from 85 ms to 485 ms) and category information (from 100 ms to 705 ms). (<bold>c/d</bold>) Removing DNN features abolished category information, but not vertical location information, most prominently between 185 ms and 350 ms. This result is consistent with the schematic coding observed for posterior signals. Significance markers represent p&lt;0.05 (corrected for multiple comparisons). Error margins reflect standard errors of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.010</object-id><label>Figure 2—figure supplement 5.</label><caption><title>RSA using anterior electrodes.</title><p>(<bold>a/b</bold>) Also responses recorded from the anterior group yielded both vertical location information (from 85 ms to 350 ms) and category information (from 165 ms to 610 ms). (<bold>c/d</bold>) In contrast to the other electrode groups, removing DNN features rendered location and category information insignificant, suggesting that they are not primarily linked to sources in frontal brain areas. This observation also excludes explanations based on oculomotor confounds. Significance markers represent p&lt;0.05 (corrected for multiple comparisons). Error margins reflect standard errors of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig2-figsupp5-v2.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.011</object-id><label>Figure 2—figure supplement 6.</label><caption><title>Vertical location effects across experiment halves.</title><p>We interpret the vertical location organization in the neural data as reflecting prior schematic knowledge about scene structure. Alternatively, however, the vertical location organization could in principle result from learning the composition of the scenes across the experiment. In the latter case, one would predict that vertical location effects should primarily occur late in the experiment (e.g., in the second half), and less so towards the beginning (e.g., in the first half). To test this, we split into halves both the fMRI data (three runs each) and the EEG data (first versus second half of trials) and for each half modeled the neural data as a function of the vertical and horizontal location and category predictors. (<bold>a</bold>) For the fMRI data, we found significant vertical location information in the OPA for in the first half (t[29]=3.46, p&lt;0.001, p<sub>corr</sub> &lt;0.05) and a trending effect for the second half (t[29] = 2.07, p = 0.024, p<sub>corr</sub> &gt;0.05). No differences between the splits were found in any region (all t&lt;0.90, p&gt;0.37). (<bold>b</bold>) For the EEG data, we also found very similar results for the two spits, with no significant differences emerging at any time point. Together, these results suggest that the vertical location organization cannot solely be explained by extensive learning over the course of the experiment. Significance markers represent p&lt;0.05 (corrected for multiple comparisons). Empty markers represent p&lt;0.05 (uncorrected). Error margins reflect standard errors of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig2-figsupp6-v2.tif"/></fig><fig id="fig2s7" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.012</object-id><label>Figure 2—figure supplement 7.</label><caption><title>Pairwise comparisons along the vertical axis.</title><p>To test whether vertical location information can be observed across all three vertical bins, we modelled the neural data as a function of the fragments’ vertical location, now separately for each pairwise comparison along the vertical axis (i.e., top versus bottom, top versus middle, and middle versus bottom). (<bold>a</bold>) For the fMRI data, we only found consistent evidence for vertical location information in the OPA: top versus bottom (t[29]=4.10, p&lt;0.001, p<sub>corr</sub> &lt;0.05), top versus middle (t[29]=2.13, p=0.021, p<sub>corr</sub> &gt;0.05), middle versus bottom (t[29]=2.06, p=0.024, p<sub>corr</sub> &gt;0.05). Although the effect was numerically bigger for top versus bottom, we did not find a significant difference between the three pairwise comparisons in OPA (F[2,58]=2.71, p=0.075). (<bold>b</bold>) For the EEG data, we found significant vertical location information for all three comparisons. Here, the middle-versus-bottom comparison yielded the weakest effect, which was significantly smaller than the effect for top versus bottom from 120 ms and 195 ms and significantly smaller than the effect for top versus middle from 110 ms to 285 ms. Together, these results suggest that schematic coding can be observed consistently across the different comparisons along the vertical axis, although comparisons including the top fragments yielded stronger effects. Significance markers represent p&lt;0.05 (corrected for multiple comparisons). Empty markers represent p&lt;0.05 (uncorrected). Error margins reflect standard errors of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig2-figsupp7-v2.tif"/></fig><fig id="fig2s8" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.013</object-id><label>Figure 2—figure supplement 8.</label><caption><title>Controlling for task difficulty.</title><p>(<bold>a</bold>) To control for task difficulty effects in the indoor/outdoor classification task, we computed paired t-tests between all pairs of fragments, separately for their associated accuracies and response times. We then constructed two predictor RDMs that contained the t-values of the pairwise tests between the fragments: For each pair of fragments, these t-values corresponded to dissimilarity in task difficulty (e.g., comparing two fragments associated with similarly short categorization response times would yield a low t-value, and thus low dissimilarity). This was done separately for the fMRI and EEG experiments (matrices from the EEG experiment are shown). The accuracy and response time RDMs were mildly correlated with the category RDM (fMRI: accuracy: r = 0.10, response time: r = 0.15; EEG: accuracy: r = 0.17, response time: r = 0.16), but not with the vertical location RDM (fMRI: both r &lt; 0.01, EEG: both r &lt; 0.01). After regressing out the task difficulty RDMs, we found highly similar vertical location and category information as in the previous analyses (<xref ref-type="fig" rid="fig3">Figure 3b/c</xref>). (<bold>b</bold>) In the fMRI, only category information in OPA was significantly reduced when task difficulty was accounted for. (<bold>c</bold>) In the EEG, towards the end of the epoch – when participants responded – location and category information were decreased. This shows that the effects of schematic coding – emerging around 200 ms after onset – cannot be explained by differences in task difficulty. The dashed significance markers represent significantly reduced information (compared to the main analyses, <xref ref-type="fig" rid="fig3">Figure 3b/c</xref>) at p&lt;0.05 (corrected for multiple comparisons).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig2-figsupp8-v2.tif"/></fig><fig id="fig2s9" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.014</object-id><label>Figure 2—figure supplement 9.</label><caption><title>Categorical versus Euclidean vertical location predictors.</title><p>We defined our vertical location predictor as categorical, assuming that top, middle, and bottom fragments are coded distinctly in the human brain. An alternative way of constructing the vertical location predictor is in terms of the fragments’ Euclidean distances, where fragments closer together along the vertical axis (e.g., top and middle) are represented more similarly than fragments further apart (e.g., top and bottom). (<bold>a</bold>) For the fMRI data, we found that the categorical and Euclidean predictors similarly explained the neural data, with no statistical differences between them (all t[29] &lt;1.15, p&gt;0.26). (<bold>b</bold>) For the EEG data, we found that both predictors explained the neural data well. However, the categorical predictor revealed significantly stronger vertical location information from 75 ms to 340 ms, suggesting that, at least in the EEG data, the differentiation along the vertical axis is more categorical in nature. Significance markers represent p&lt;0.05 (corrected for multiple comparisons). Error margins reflect standard errors of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig2-figsupp9-v2.tif"/></fig></fig-group><p>We then quantified schema effects using separate model RDMs for horizontal and vertical locations (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). These location RDMs reflected whether pairs of fragments shared the same location or not. We additionally constructed a category model RDM, which reflected whether pairs of fragments stemmed from the same scene or not.</p><p>Critically, if cortical information is indeed sorted with respect to scene schemata, we should observe a neural clustering of fragments that stem from the same within-scene location – in this case, the location RDM should predict a significant proportion of the representational organization in visual cortex.</p><p>To test this, we modeled neural RDMs as a function of the model RDMs using general linear models, separately for the fMRI and EEG data. The resulting beta weights indicated to which degree location and category information accounted for cortical responses in the three ROIs and across time.</p><p>The key observation was that the fragments’ vertical location predicted neural representations in OPA (t[29] = 4.12, p&lt;0.001, p<sub>corr</sub> &lt;0.05), but not in V1 and PPA (test statistics for all analyses and ROIs are reported in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>) (<xref ref-type="fig" rid="fig2">Figure 2d</xref>) and between 55 ms and 685 ms (peak: t[19] = 9.03, p&lt;0.001, p<sub>corr</sub> &lt;0.05) (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). This vertical-location organization was consistent across the first and second half of the experiments (see <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>) and across all pairwise comparisons along the vertical axis (see <xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7</xref>). No effects were observed for horizontal location, consistent with more rigid spatial scene structure in the vertical dimension (<xref ref-type="bibr" rid="bib47">Mandler and Parker, 1976</xref>). This result provides a first characterization of where and when incoming information is organized in accordance with scene schemata: in OPA and rapidly after stimulus onset, scene fragments are sorted according to their origin within the environment.</p><p>The schema-based organization co-exists with a prominent scene-category organization: In line with previous findings (<xref ref-type="bibr" rid="bib43">Lowe et al., 2018</xref>; <xref ref-type="bibr" rid="bib63">Walther et al., 2009</xref>), category was accurately predicted in OPA (t[29] = 3.12, p=0.002, p<sub>corr</sub> &lt;0.05) and PPA (t[29] = 4.26, p&lt;0.001, p<sub>corr</sub> &lt;0.05) (<xref ref-type="fig" rid="fig2">Figure 2d</xref>), and from 60 ms to 775 ms (peak: t[19] = 6.39, p&lt;0.001, p<sub>corr</sub> &lt;0.05) (<xref ref-type="fig" rid="fig2">Figure 2e</xref>).</p><p>To efficiently support vision in dynamic natural environments, schematic coding needs to be flexible with respect to visual properties of specific scenes. The absence of vertical location effects in V1 indeed highlights that schematic coding is not tied to the analysis of simple visual features. To more thoroughly probe this flexibility, we additionally conducted three complementary analyses (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.48182.015</object-id><label>Figure 3.</label><caption><title>Schematic coding operates flexibly across visual and conceptual scene properties.</title><p>(<bold>a</bold>) To determine the role of categorization-related visual features in this schematic organization, we regressed out RDMs obtained from 18 layers along the ResNet50 DNN before repeated the three-predictor general linear model (GLM) analysis (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). (<bold>b/c</bold>) Removing DNN features abolished category information in fMRI and EEG signals, but not vertical location information. (<bold>d</bold>) To test for generalization across different scene types, we restricted location predictor RDMs to comparisons across indoor and outdoor scenes. Due to this restriction, category could not be modelled. (<bold>e/f</bold>) In this analysis, vertical location still predicted neural organization in OPA and from 70 ms. (<bold>g</bold>) Finally, we combined the two analyses: we first regressed out DNN features prior and then modelled the neural RDMs using the restricted predictor RDMs (<bold>d</bold>). (<bold>h</bold>) In this analysis, we still found significant vertical location information in OPA. (<bold>i</bold>) Notably, vertical location information in the EEG signals was delayed to after 180 ms, suggesting that at this stage schematic coding becomes flexible to visual and conceptual attributes. Significance markers represent p&lt;0.05 (corrected for multiple comparisons). Error margins reflect standard errors of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.016</object-id><label>Figure 3—figure supplement 1.</label><caption><title>AlexNet as a model of visual categorization.</title><p>(<bold>a</bold>) In addition to the ResNet50 DNN, we also used the more widely used AlexNet DNN architecture (pretrained on the ImageNet dataset, implemented in the MatConvNet toolbox) as a model for visual categorization. AlexNet consists of 5 convolutional and three fully-connected layers. We created 8 RDMs, separately for each layer of the DNN. (<bold>b/c</bold>) Removing the AlexNet DNN features rendered category information non-significant in fMRI and EEG signals. However, we still found vertical location information in OPA and from 65 ms to 375 ms. (<bold>c–e</bold>) When additionally restricting the analysis to comparisons between indoor and outdoor scenes, the fragments’ vertical location still predicted neural activations in OPA and from 95 ms to 375 ms. In sum, these results are highly similar to the results obtained with the ResNet50 model (<xref ref-type="fig" rid="fig3">Figure 3b/c/h/i</xref>). Significance markers represent p&lt;0.05 (corrected for multiple comparisons). Error margins reflect standard errors of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.017</object-id><label>Figure 3—figure supplement 2.</label><caption><title>DNN model fit.</title><p>(<bold>a/b</bold>) Goodness of fit (R<sup>2</sup>) across ROIs (<bold>a</bold>) and time (<bold>b</bold>) of the GLMs used to regress out DNN features, obtained from ResNet50 (left) or AlexNet (right). For the EEG time series, mean R<sup>2</sup> across the baseline period were subtracted. Note that GLMs based on the ResNet50 RDMs had more predictor variables, which may contribute to their better fit. Error bars represent standard errors of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48182.018</object-id><label>Figure 3—figure supplement 3.</label><caption><title>Low-level control models.</title><p>We used three control models that explicitly account for low-level visual features: a pixel-dissimilarity model, GIST descriptors, and the fragments’ neural dissimilarity in V1. Critically, all three models did not account for the fragments’ vertical location organization. Moreover, unlike the DNN models, the low-level models were also unable to account for the fragments’ categorical organization. (<bold>a/b</bold>) Results after regressing out the pixel dissimilarity model, which captured the fragments’ pairwise dissimilarity in pixel space (i.e., 1- the correlation of their pixel values). (<bold>c/d</bold>) Results after regressing out the GIST model, which captured the fragments’ pairwise dissimilarity in GIST descriptors (i.e., in their global spatial envelope). (<bold>e/f</bold>) Results after regressing out the V1 model, which captured the fragments’ pairwise neural dissimilarity in V1 (i.e., the averaged RDM across participants) and thereby provides a brain-derived measure of low-level feature similarity. Significance markers represent p&lt;0.05 (corrected for multiple comparisons). Error margins reflect standard errors of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48182-fig3-figsupp3-v2.tif"/></fig></fig-group><p>First, we tested whether schematic coding is tolerant to stimulus features relevant for visual categorization. Categorization-related features were quantified using a deep neural network (DNN; ResNet50), which extracts such features similarly to the brain (<xref ref-type="bibr" rid="bib65">Wen et al., 2018</xref>). We removed DNN features by regressing out layer-specific RDMs constructed from DNN activations (see Materials and Methods for details) (<xref ref-type="fig" rid="fig3">Figure 3a</xref>); subsequently, we re-estimated location and category information.</p><p>After removing DNN features, category information was rendered non-significant in both fMRI and EEG signals. When directly comparing category information before and after removing the DNN features, we found reduced category information in PPA (t[29] = 2.48, p = 0.010, p<sub>corr</sub> &lt;0.05) and OPA (t[29] = 1.86, p = 0.036, p<sub>corr</sub> &gt;0.05), and a strong reduction of category information across time, from 75 ms to 775 ms (peak t[19] = 13.0, p&lt;0.001, p<sub>corr</sub> &lt;0.05). Together, this demonstrates that categorization-related brain activations are successfully explained by DNN features (<xref ref-type="bibr" rid="bib10">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib11">Cichy et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Groen et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib65">Wen et al., 2018</xref>), indicating the appropriateness of our DNN for modelling visual brain activations. Despite the suitability of our DNN model for modelling categorical brain responses, vertical location still accounted for the neural organization in OPA (t[29] = 2.37, p = 0.012, p<sub>corr</sub> &lt;0.05) (<xref ref-type="fig" rid="fig3">Figure 3b</xref>) and between 75 ms and 335 ms (peak: t[19] = 5.06, p&lt;0.001, p<sub>corr</sub> &lt;0.05) (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). Similar results were obtained using a shallower feed-forward DNN (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). This result suggests that schematic coding cannot be explained by categorization-related features extracted by DNN models.</p><p>DNN features are a useful control for flexibility regarding visual features, because they cover both low-level and high-level features, explaining variance across fMRI regions and across EEG processing time (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>; see also <xref ref-type="bibr" rid="bib10">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib23">Güçlü and van Gerven, 2015</xref>). However, to more specifically control for low-level features, we used two commonly employed low-level control models: pixel dissimilarity and GIST descriptors (<xref ref-type="bibr" rid="bib49">Oliva and Torralba, 2001</xref>). These models neither explained the vertical location organization nor the category organization in the neural data (see <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). Finally, as an even stronger control of the low-level features encoded in V1, we used the neural dissimilarity structure in V1 (i.e., the neural RDMs) as a control model, establishing an empirical neural measure of low-level features. With V1 housing precise low-level feature representations, this measure should very well capture the features extracted during the early processing of simple visual features. However, removing the V1 dissimilarity structure did neither abolish the schematic coding effects in the OPA nor in the EEG data (see <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). This shows that even if we had control models that approximated V1 representations extremely well – as well as the V1 representations approximate themselves – these models could not explain vertical location effects in downstream processing. Together, these results provide converging evidence that low-level feature processing cannot explain the schematic coding effects reported here.</p><p>Second, we asked whether schematic coding operates flexibly across visually diverse situations. To test this explicitly we restricted RDMs to comparisons between indoor and outdoor scenes, which vary substantially in visual characteristics (<xref ref-type="bibr" rid="bib60">Torralba and Oliva, 2003</xref>) (<xref ref-type="fig" rid="fig3">Figure 3d</xref>).</p><p>Vertical location still predicted cortical organization in OPA (t[29] = 3.05, p = 0.002, p<sub>corr</sub> &lt;0.05) (<xref ref-type="fig" rid="fig3">Figure 3e</xref>) and from 70 ms to 385 ms (peak: t[19] = 7.47, p&lt;0.001, p<sub>corr</sub> &lt;0.05) (<xref ref-type="fig" rid="fig3">Figure 3f</xref>). The generalization across indoor and outdoor scenes indicates that schematic coding operates similarly across radically different scenes, suggesting that the mechanism can similarly contextualize information across different real-life situations.</p><p>Finally, for a particularly strong test of flexibility, we tested for schematic coding after removing both DNN features and within-category comparisons (<xref ref-type="fig" rid="fig3">Figure 3g</xref>). In this analysis, OPA representations were still explained by the fragments’ vertical location (t[29] = 2.38, p = 0.012, p<sub>corr</sub> &lt;0.05) (<xref ref-type="fig" rid="fig3">Figure 3h</xref>). Notably, early schema effects were rendered non-significant, while vertical location still predicted representations after 180 ms (peak: t[19] = 4.41, p&lt;0.001, p<sub>corr</sub> &lt;0.05) (<xref ref-type="fig" rid="fig3">Figure 3i</xref>), suggesting a high degree of flexibility emerging at that time. Interestingly, across all analyses, vertical location information was exclusively found in OPA and always peaked shortly after 200 ms (see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>), suggesting that schematic coding occurs during early perceptual analysis of scenes.</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Together, our findings characterize a novel neural mechanism for contextualizing fragmented inputs during naturalistic vision. The mechanism exploits schemata to sort sensory inputs into meaningful representations of the environment. This sorting occurs during perceptual scene analysis in scene-selective OPA and within the first 200 ms of vision, and operates flexibly across changes in visual properties.</p><p>That schema-based coding can be localized to OPA is consistent with the region’s important role in visual scene processing. Transcranial magnetic stimulation studies suggest that OPA activation is crucial for various scene perception tasks, such as scene discrimination (<xref ref-type="bibr" rid="bib15">Dilks et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Ganaden et al., 2013</xref>), navigating through scenes (<xref ref-type="bibr" rid="bib31">Julian et al., 2016</xref>) and anticipating upcoming scene information (<xref ref-type="bibr" rid="bib21">Gandolfo and Downing, 2019</xref>). Functional MRI work suggest that computations in the OPA include the analysis of spatial scene layout (<xref ref-type="bibr" rid="bib16">Dillon et al., 2018</xref>; <xref ref-type="bibr" rid="bib29">Henriksson et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Lowe et al., 2017</xref>) and the parsing of local scene elements like objects and local surfaces (<xref ref-type="bibr" rid="bib36">Kamps et al., 2016</xref>). Future studies are needed to clarify which of these computations mediate the schema-based coding described here.</p><p>As the current study is limited to a small set of scenes, more research is needed to explore whether schema-based coding generalizes to more diverse contents. It is conceivable that schema-based coding constitutes a more general coding strategy that may generalize to other visual contents (such as faces; <xref ref-type="bibr" rid="bib28">Henriksson et al., 2015</xref>) and non-visual processing domains: when sensory information is fragmented and spatial information is unreliable, the brain may use schematic information to contextualize sensory inputs. This view is in line with Bayesian theories of perception where the importance of prior information for perceptual inference grows with the noisiness and ambiguity of the sensory information at hand (<xref ref-type="bibr" rid="bib19">Ernst and Banks, 2002</xref>; <xref ref-type="bibr" rid="bib39">Kersten et al., 2004</xref>).</p><p>The schema-based sorting of scene representations provides a mechanism for efficient communication between perceptual and cognitive systems: when scene information is formatted with respect to its role in the environment, it can be efficiently read out by downstream processes. This idea is consistent with the emerging view that cortical representations depend on functional interactions with the environment (<xref ref-type="bibr" rid="bib6">Bonner and Epstein, 2017</xref>; <xref ref-type="bibr" rid="bib22">Groen et al., 2018</xref>; <xref ref-type="bibr" rid="bib44">Malcolm et al., 2016</xref>; <xref ref-type="bibr" rid="bib52">Peelen and Downing, 2017</xref>). Under this view, formatting perceptual information according to real-world structure may allow cognitive and motor systems to efficiently read out visual information that is needed for different real-world tasks (e.g., immediate action versus future navigation). As the schema-based sorting of scene information happens already during early scene analysis, many high-level processes have access to this information.</p><p>Lastly, our results have implications for computational modelling of vision. While DNNs trained on categorization accurately capture the representational divide into different scene categories, they cannot explain the schema-based organization observed in the human visual system. Although this does not mean that visual features extracted by DNN models in principle are incapable of explaining schema-based brain representations, our results highlight that current DNN models of categorization do not use real-world structure in similar ways as the human brain. In the future, augmenting DNN training procedures with schematic information (<xref ref-type="bibr" rid="bib38">Katti et al., 2019</xref>) may improve their performance on real-world tasks and narrow the gap between artificial and biological neural networks.</p><p>To conclude, our findings provide the first spatiotemporal characterization of a neural mechanism for contextualizing fragmented visual inputs. By rapidly organizing visual information according to its typical role in the world, this mechanism may contribute to the optimal use of perceptual information for guiding efficient real-world behaviors, even when sensory inputs are incomplete or dynamically changing.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th valign="top">Reagent type <break/>(species) or resource</th><th valign="top">Designation</th><th valign="top">Source or reference</th><th valign="top">Identifiers</th><th valign="top">Additional <break/>information</th></tr></thead><tbody><tr><td valign="top">Software, algorithm</td><td valign="top">CoSMoMVPA</td><td valign="top"><xref ref-type="bibr" rid="bib51">Oosterhof et al., 2016</xref></td><td valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_014519">SCR_014519</ext-link></td><td valign="top">For data analysis</td></tr><tr><td valign="top">Software, algorithm</td><td valign="top">fieldtrip</td><td valign="top"><xref ref-type="bibr" rid="bib50">Oostenveld et al., 2011</xref></td><td valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_004849">SCR_004849</ext-link></td><td valign="top">For EEG data preprocessing</td></tr><tr><td valign="top">Software, algorithm</td><td valign="top">MATLAB</td><td valign="top">Mathworks Inc.</td><td valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001622">SCR_001622</ext-link></td><td valign="top">For stimulus delivery and data analysis</td></tr><tr><td valign="top">Software, algorithm</td><td valign="top">Psychtoolbox 3</td><td valign="top"><xref ref-type="bibr" rid="bib7">Brainard, 1997</xref></td><td valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002881">SCR_002881</ext-link></td><td valign="top">For stimulus delivery</td></tr><tr><td valign="top">Software, algorithm</td><td valign="top">SPM12</td><td valign="top"><ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/software/spm12/">www.fil.ion.ucl.ac.uk/spm/software/spm12/</ext-link></td><td valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_007037">SCR_007037</ext-link></td><td valign="top">For fMRI data preprocessing</td></tr></tbody></table></table-wrap><sec id="s4-1"><title>Participants</title><p>Thirty adults (mean age 23.9 years, <italic>SD</italic> = 4.4; 26 females) completed the fMRI experiment and twenty (mean age 24.0 years, SD = 4.3; 15 females) completed the EEG experiment. All participants had normal or corrected-to-normal vision. They all provided informed consent and received monetary reimbursement or course credits for their participation. All procedures were approved by the ethical committee of the Department of Education and Psychology at Freie Universität Berlin (reference 140/2017) and were in accordance with the Declaration of Helsinki.</p></sec><sec id="s4-2"><title>Stimuli</title><p>The stimulus set (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) consisted of fragments taken from three images of indoor scenes (bakery, classroom, kitchen) and three images of outdoor scenes (alley, house, farm). Each image was split horizontally into two halves, and each of the halves was further split vertically in three parts, so that for each scene six fragments were obtained. Participants were not shown the full scene images prior to the experiment.</p></sec><sec id="s4-3"><title>Experimental design</title><p>The fMRI and EEG designs were identical, unless otherwise noted. Stimulus presentation was controlled using the Psychtoolbox (<xref ref-type="bibr" rid="bib7">Brainard, 1997</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002881">SCR_002881</ext-link>). In each trial, one of the 36 fragments was presented at central fixation (7° horizontal visual angle) for 200 ms (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Participants were instructed to maintain central fixation and categorize each stimulus as an indoor or outdoor scene image by pressing one of two buttons.</p><p>In the fMRI experiment, the inter-trial interval was kept constant at 2,300 ms, irrespective of the participant’s response time. In the EEG experiment, after each response a green or red fixation dot was presented for 300 ms to indicate response correctness; participants were instructed to only blink after the feedback had occurred. Trials were separated by a fixation interval randomly varying between 1500 ms and 2000 ms.</p><p>In the fMRI, participants performed six identical runs. Within each run, each of the 36 scene fragments was shown four times, resulting in 144 trials. Additionally, each run contained 29 fixation trials, where only the central fixation dot was shown. Runs started and ended with brief fixation periods; the total run duration was 7:30 min. In the EEG, each of the 36 fragments was presented 40 times during the experiment, for a total of 1440 trials, divided into 10 runs. Three participants performed a shorter version of the experiment, with only 20 repetitions of each image (720 trials in total).</p><p>In both experiments, participants performed very well in the indoor/outdoor categorization task (fMRI: 94% correct, 658 ms mean response time, EEG: 96%, 606 ms). Differences in task difficulty across fragments were not related to the neural effects of interest (<xref ref-type="fig" rid="fig2s8">Figure 2—figure supplement 8</xref>).</p></sec><sec id="s4-4"><title>fMRI recording and preprocessing</title><p>MRI data was acquired using a 3T Siemens Tim Trio Scanner equipped with a 12-channel head coil. T2*-weighted gradient-echo echo-planar images were collected as functional volumes (TR = 2 s, TE = 30 ms, 70° flip angle, 3 mm<sup>3</sup> voxel size, 37 slices, 20% gap, 192 mm FOV, 64 × 64 matrix size, interleaved acquisition). Additionally, a T1-weighted image (MPRAGE; 1 mm<sup>3</sup> voxel size) was obtained as a high-resolution anatomical reference. During preprocessing, the functional volumes were realigned and coregistered to the T1 image, using MATLAB (RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_014519">SCR_014519</ext-link>) and SPM12 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/">www.fil.ion.ucl.ac.uk/spm/</ext-link>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_014519">SCR_014519</ext-link>).</p></sec><sec id="s4-5"><title>fMRI region of interest definition</title><p>We restricted our analyses to three regions of interest (ROIs). We defined scene-selective occipital place area (OPA; <xref ref-type="bibr" rid="bib15">Dilks et al., 2013</xref>) and parahippocampal place area (PPA; <xref ref-type="bibr" rid="bib18">Epstein and Kanwisher, 1998</xref>) using a functional group atlas (<xref ref-type="bibr" rid="bib30">Julian et al., 2012</xref>). As a control region, we defined early visual cortex (V1) using a probabilistic atlas (<xref ref-type="bibr" rid="bib64">Wang et al., 2015</xref>). All ROIs were defined in standard space and then inverse-normalized into individual-participant space. For each ROI, we concatenated the left- and right-hemispheric masks and performed analyses on the joint ROI.</p></sec><sec id="s4-6"><title>EEG recording and preprocessing</title><p>The EEG was recorded using an EASYCAP 64-channel system and a Brainvision actiCHamp amplifier. The electrodes were arranged in accordance with the standard 10–10 system. The data was recorded at a sampling rate of 1000 Hz and filtered online between 0.03 Hz and 100 Hz. All electrodes were referenced online to the Fz electrode. Offline preprocessing was performed in MATLAB, using the FieldTrip toolbox (<xref ref-type="bibr" rid="bib50">Oostenveld et al., 2011</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_004849">SCR_004849</ext-link>). The continuous EEG data were epoched into trials ranging from 200 ms before stimulus onset to 800 ms after stimulus onset, and baseline corrected by subtracting the mean of the pre-stimulus interval for each trial and channel separately. Trials containing movement-related artefacts were automatically identified and removed using the default automatic rejection procedure implemented in Fieldtrip. Channels containing excessive noise were removed based on visual inspection. Blinks and eye movement artifacts were identified and removed using independent components analysis and visual inspection of the resulting components. The epoched data were down-sampled to 200 Hz.</p></sec><sec id="s4-7"><title>Representational similarity analysis</title><p>To model the representational structure of the neural activity related to our stimulus set, we used representational similarity analysis (RSA; <xref ref-type="bibr" rid="bib40">Kriegeskorte et al., 2008</xref>). We first extracted neural RDMs separately for the fMRI and EEG experiments, and then used the same analyses to model their organization. To retrieve the fragments’ position within the original scene, as well their scene category, we used a regression approach, where we modeled neural dissimilarity as a linear combination of multiple predictors (<xref ref-type="bibr" rid="bib54">Proklova et al., 2016</xref>; <xref ref-type="bibr" rid="bib55">Proklova et al., 2019</xref>).</p><sec id="s4-7-1"><title>Constructing neural dissimilarity – fMRI</title><p>For the fMRI data, we used cross-validated correlations as a measure of pairwise neural dissimilarity. First, patterns for each ROI were extracted from the functional images corresponding to the trials of interest. After shifting the activation time course by 3 TRs (i.e., 6 s, accounting for the hemodynamic delay), we extracted voxel-wise activation values for each trial, from the TR that was closest to the stimulus onset on this trial (for results across 6 TRs with respect to trial onset, see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). To account for activation differences between runs, the mean activation across conditions was subtracted from each voxel’s values, separately for each run. For each ROI, response patterns across voxels were used to perform multivariate analyses using the CoSMoMVPA toolbox (<xref ref-type="bibr" rid="bib51">Oosterhof et al., 2016</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_014519">SCR_014519</ext-link>). For each TR separately, we performed correlation-based (<xref ref-type="bibr" rid="bib25">Haxby et al., 2001</xref>) multi-voxel pattern analyses (MVPA) for each pair of fragments. These analyses were cross-validated by repeatedly splitting the data into two equally-sized sets (i.e., half of the runs per set). For this analysis, we correlated the patterns across the two sets, both within-condition (i.e., the patterns stemming from the two same fragments and from different sets) and between-conditions (i.e., the patterns stemming from the two different fragments and from different sets). These correlations were Fisher-transformed. Then, we subtracted the within- and between-correlations to obtain a cross-validated correlation measure, where above-zero values reflect successful discrimination. This procedure was repeated for all possible splits of the six runs. Performing this MVPA for all pairs of fragments yielded a 36 × 36 representational dissimilarity matrix (RDM) for each ROI. RDMs’ entries reflected the neural dissimilarity between pairs of fragments (the diagonal remained empty).</p></sec><sec id="s4-7-2"><title>Constructing neural dissimilarity – EEG</title><p>For the EEG data, we used cross-validated classification accuracies as a measure of pairwise neural dissimilarity. We thus constructed RDMs across time by performing time-resolved multivariate decoding analyses (<xref ref-type="bibr" rid="bib13">Contini et al., 2017</xref>). RDMs were built by computing pair-wise decoding accuracy for all possible combinations of the 36 stimuli, using the CoSMoMVPA toolbox (<xref ref-type="bibr" rid="bib51">Oosterhof et al., 2016</xref>). As we expected the highest classification in sensors over visual cortex (<xref ref-type="bibr" rid="bib4">Battistoni et al., 2018</xref>; <xref ref-type="bibr" rid="bib33">Kaiser et al., 2016</xref>), only 17 occipital and posterior sensors (O1, O2, Oz, PO3, PO4, PO7, PO8, POz, P1, P2, P3, P4, P5, P6, P7, P8, Pz) were used in this analysis. We report results for other electrode groups in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplements 3</xref>–<xref ref-type="fig" rid="fig2s5">5</xref>. For each participant, classification was performed separately for each time point across the epoch (i.e., with 5 ms resolution). The analysis was performed in a pair-wise fashion: Linear discriminant analysis classifiers were always trained and tested on data from two conditions (e.g., the middle left part of the alley versus the top right part of the farm), using a leave-one-trial-out partitioning scheme. The training set consisted of all but one trials for each of the two conditions, while one trial for each of the two conditions was held back and used for classifier testing. This procedure was repeated until every trial was left out once. Classifier performance was averaged across these repetitions. The pairwise decoding analysis resulted in a 36-by-36 neural RDM for each time point. A schematic description of the RDM construction can be found in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p></sec><sec id="s4-7-3"><title>Location and category predictors</title><p>We predicted the neural RDMs in a general linear model (GLM; see below) with three different predictor RDMs (36 × 36 entries each) (<xref ref-type="fig" rid="fig2">Figure 2c</xref>): In the vertical location RDM, each pair of conditions is assigned either a value of 0, if the fragments stem from the same vertical location, or the value 1, if they stem from different vertical locations (for results with an alternative predictor RDM using Euclidean distances see <xref ref-type="fig" rid="fig2s9">Figure 2—figure supplement 9</xref>). In the horizontal location RDM, each pair of conditions is assigned either a value of 0, if the fragments stem from the same horizontal location, or a value of 1, if they stem from different horizontal locations. In the category RDM, each pair of conditions is assigned either a value of 0, if the fragments stem from the same scene, or a value of 1, if they stem from different scenes.</p><p>In an additional analysis, we sought to eliminate properties specific to either the indoor or outdoor scenes, respectively. We therefore constructed RDMs for horizontal and vertical location information which only contained comparisons between the indoor and outdoor scenes. These RDMs were constructed in the same way as explained above, but all comparisons within the same scene type of scene were removed (<xref ref-type="fig" rid="fig3">Figure 3d</xref>).</p></sec><sec id="s4-7-4"><title>Modelling neural dissimilarity</title><p>To reveal correspondences between the neural data and the predictor matrices, we used GLM analyses. Separately for each ROI (fMRI) or time point (EEG), we modelled the neural RDM as a linear function of the vertical location RDM, the horizontal location RDM, and the category RDM. Prior to each regression, the neural RDMs and predictor RDMs were vectorized by selecting all lower off-diagonal elements – the rest of the entries, including the diagonal, was discarded. Values for the neural RDMs were z-scored. Separately for each subject and each time point, three beta coefficients (i.e., regression weights) were estimated. By averaging across participants, we obtained time-resolved beta estimates for each predictor, showing how well each predictor explains the neural data over time.</p><p>Furthermore, we performed an additional GLM analysis with a vertical location predictor and a horizontal location predictor, where comparisons within indoor- and outdoor-scenes were removed (<xref ref-type="fig" rid="fig3">Figure 3d–f</xref>); these comparisons were also removed from the regression criterion. Using the same procedure as in the previous GLM analysis, we then estimated the beta coefficients for each predictor at each time point, separately for each subject. For this analysis, a category RDM could not be constructed, as all comparisons of fragments from the same scene were eliminated.</p></sec><sec id="s4-7-5"><title>Controlling for deep neural network features</title><p>To control for similarity in categorization-related visual features, we used a deep neural network (DNN) model. DNNs have recently become the state-of-the-art model of visual categorization, as they tightly mirror the neural organization of object and scene representations (<xref ref-type="bibr" rid="bib10">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib11">Cichy et al., 2017</xref>; <xref ref-type="bibr" rid="bib12">Cichy and Kaiser, 2019</xref>; <xref ref-type="bibr" rid="bib22">Groen et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib65">Wen et al., 2018</xref>). DNNs are similar to the brain as they are trained using excessive training material while dynamically adjusting the ‘tuning’ of their connections. Here, we used a DNN (see below) that has been trained to categorize objects across a large number of images and categories, therefore providing us with a high-quality model of how visual features are extracted for efficient categorization. By comparing DNNs activations and brain responses to the scene fragments, we could quantify to which extent features routinely extracted for categorization purposes account for schema-based coding in the human visual system.</p><p>In a two-step approach, we re-performed our regression analysis after removing the representational organization emerging from the DNN. First, we used a regression model to remove the contribution of the dissimilarity structure in the DNN model. This model included one predictor for each layer extracted from the DNN (i.e., one RDM for each processing step along the DNN). Estimating this model allowed us to remove the neural organization explained by the DNN while retaining what remains unexplained (in the regression residuals). Second, we re-ran the previous regression analyses (see above), but now the residuals of the DNN regression were used as the regression criterion, so that only the organization that remained unexplained by the DNN was modeled.</p><p>As a DNN model, we used a pre-trained version (trained on image categorization for the ImageNet challenge) of the ResNet50 model (<xref ref-type="bibr" rid="bib26">He et al., 2016</xref>), as implemented in MatConvNet (<xref ref-type="bibr" rid="bib61">Vedaldi and Lenc, 2015</xref>). This model’s deeper, residual architecture outperforms shallower models in approximating visual cortex organization (<xref ref-type="bibr" rid="bib65">Wen et al., 2018</xref>). ResNet50 consists of 16 blocks of residual layer modules, where information both passes through an aggregate of layers within the block, and bypasses the block; then the residual between the processed and the bypassing information is computed. Additionally, ResNet50 has one convolutional input layer, and one fully-connected output layer. Here, to not inflate the number of intercorrelated predictor variables, we only used the final layer of each residual block, and thus 18 layers in total (16 from the residual blocks, and the input and output layers). For each layer, an RDM was built using 1-correlation between the activations of all nodes in the layer, separately for each pair of conditions. For regressing out the DNN RDMs, we added one predictor for each available RDM. In <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, we show that an analysis using the AlexNet architecture (<xref ref-type="bibr" rid="bib41">Krizhevsky et al., 2012</xref>) yields comparable results; in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, we additionally provide information about the DNN model fit across regions and time points.</p></sec></sec><sec id="s4-8"><title>Statistical testing</title><p>For the fMRI data, we tested the regression coefficients against zero, using one-tailed, one-sample t-tests (i.e., testing the hypothesis that coefficients were greater than zero). Multiple-comparison correction was based on Bonferroni-corrections across ROIs. A complete report of all tests performed on the fMRI data can be found in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>. For the EEG data, we used a threshold-free cluster enhancement procedure (<xref ref-type="bibr" rid="bib57">Smith and Nichols, 2009</xref>) to identify significant effects across time. Multiple-comparison correction was based on a sign-permutation test (with null distributions created from 10,000 bootstrapping iterations) as implemented in CoSMoMVPA (<xref ref-type="bibr" rid="bib51">Oosterhof et al., 2016</xref>). The resulting statistical maps were thresholded at Z &gt; 1.64 (i.e., p&lt;0.05, one-tailed against zero). Additionally, we report the results of one-sided t-tests for all peaks effects. To estimate the reliability of onset and peak latencies we performed bootstrapping analyses, which are reported in Supplementary Items 2/3.</p></sec><sec id="s4-9"><title>Data availability</title><p>Data are publicly available on OSF (DOI.ORG/10.17605/OSF.IO/H3G6V).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>DK and RMC are supported by Deutsche Forschungsgemeinschaft (DFG) grants (KA4683/2-1, CI241/1-1, CI241/3-1). RMC is supported by a European Research Council Starting Grant (ERC-2018-StG).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Investigation, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants provided informed written consent. All procedures were approved by the ethical committee of the Department of Education and Psychology at Freie Universität Berlin (reference 140/2017) and were in accordance with the Declaration of Helsinki.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><object-id pub-id-type="doi">10.7554/eLife.48182.019</object-id><label>Supplementary file 1.</label><caption><title>Complete statistical report for fMRI results.</title><p>The table shows test statistics and p-values for all tests performed in the fMRI experiment (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>). Values reflect one-sided t-tests against zero. All p-values are uncorrected; in the main manuscript, only tests surviving Bonferroni-correction across the three ROIs (marked in color) are considered significant.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-48182-supp1-v2.docx"/></supplementary-material><supplementary-material id="supp2"><object-id pub-id-type="doi">10.7554/eLife.48182.020</object-id><label>Supplementary file 2.</label><caption><title>Estimating peak latencies.</title><p>The table shows means and standard deviations (in brackets) of peak latencies in ms for vertical location and category information in the main analyses (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>). To estimate the reliability of peaks and onsets (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>) of location and category information in the key analyses, we conducted a bootstrapping analysis. For this analysis, we choose 100 samples of 20 randomly chosen datasets (with possible repetitions). For each random sample, we computed peak and onset latencies; we then averaged the peak and onset latencies across the 100 samples. Peak latencies were defined as the highest beta estimate in the time course. Notably, the peak latency of vertical location information remained highly stable across analyses.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-48182-supp2-v2.docx"/></supplementary-material><supplementary-material id="supp3"><object-id pub-id-type="doi">10.7554/eLife.48182.021</object-id><label>Supplementary file 3.</label><caption><title>Estimating onset latencies.</title><p>The table shows means and standard deviations (in brackets) of onset latencies in ms for vertical location and category information in the main analyses (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>)). Onset latencies were quantified using the bootstrapping logic explained above (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>). Onsets were defined by first computing TFCE statistics for each random sample, with multiple-comparison correction based on 1000 null distributions. The onset latency for each sample was then defined as the first occurrence of three consecutive time points reaching significance (p&lt;0.05, corrected for multiple comparisons).</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-48182-supp3-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.48182.022</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-48182-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data are publicly available on OSF (<ext-link ext-link-type="uri" xlink:href="http://doi.org/10.17605/OSF.IO/H3G6V">http://doi.org/10.17605/OSF.IO/H3G6V</ext-link>), as indicated in the Materials and Methods section of the manuscript.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Turini</surname><given-names>J</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>A neural mechanism for contextualizing fragmented information during naturalistic vision</data-title><source>Open Science Framwork</source><pub-id assigning-authority="Open Science Framework" pub-id-type="archive" xlink:href="http://doi.org/10.17605/OSF.IO/H3G6V">10.17605/OSF.IO/H3G6V</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassano</surname> <given-names>C</given-names></name><name><surname>Esteva</surname> <given-names>A</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name><name><surname>Beck</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Two distinct Scene-Processing networks connecting vision and memory</article-title><source>Eneuro</source><volume>3</volume><elocation-id>ENEURO.0178-16.2016</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0178-16.2016</pub-id><pub-id pub-id-type="pmid">27822493</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The proactive brain: memory for predictions</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>364</volume><fpage>1235</fpage><lpage>1243</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0310</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlett</surname> <given-names>FC</given-names></name></person-group><year iso-8601-date="1932">1932</year><source>Remembering: A Study in Experimental and Social Psychology</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battistoni</surname> <given-names>E</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Hickey</surname> <given-names>C</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The time course of spatial attention during naturalistic visual search</article-title><source>Cortex</source><pub-id pub-id-type="doi">10.1016/j.cortex.2018.11.018</pub-id><pub-id pub-id-type="pmid">30563703</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname> <given-names>I</given-names></name><name><surname>Mezzanotte</surname> <given-names>RJ</given-names></name><name><surname>Rabinowitz</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Scene perception: detecting and judging objects undergoing relational violations</article-title><source>Cognitive Psychology</source><volume>14</volume><fpage>143</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(82)90007-X</pub-id><pub-id pub-id-type="pmid">7083801</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonner</surname> <given-names>MF</given-names></name><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Coding of navigational affordances in the human visual system</article-title><source>PNAS</source><volume>114</volume><fpage>4793</fpage><lpage>4798</lpage><pub-id pub-id-type="doi">10.1073/pnas.1618228114</pub-id><pub-id pub-id-type="pmid">28416669</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brewer</surname> <given-names>WF</given-names></name><name><surname>Treyens</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Role of schemata in memory for places</article-title><source>Cognitive Psychology</source><volume>13</volume><fpage>207</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(81)90008-6</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Khosla</surname> <given-names>A</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>27755</elocation-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id><pub-id pub-id-type="pmid">27282108</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>R M</given-names></name><name><surname>Khosla</surname> <given-names>A</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamics of scene representations in the human brain revealed by magnetoencephalography and deep neural networks</article-title><source>NeuroImage</source><volume>153</volume><fpage>346</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.03.063</pub-id><pub-id pub-id-type="pmid">27039703</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep neural networks as scientific models</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>305</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.01.009</pub-id><pub-id pub-id-type="pmid">30795896</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Contini</surname> <given-names>EW</given-names></name><name><surname>Wardle</surname> <given-names>SG</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Decoding the time-course of object recognition in the human brain: from visual features to categorical decisions</article-title><source>Neuropsychologia</source><volume>105</volume><fpage>165</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.02.013</pub-id><pub-id pub-id-type="pmid">28215698</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davenport</surname> <given-names>JL</given-names></name><name><surname>Potter</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Scene consistency in object and background perception</article-title><source>Psychological Science</source><volume>15</volume><fpage>559</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2004.00719.x</pub-id><pub-id pub-id-type="pmid">15271002</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dilks</surname> <given-names>DD</given-names></name><name><surname>Julian</surname> <given-names>JB</given-names></name><name><surname>Paunov</surname> <given-names>AM</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The occipital place area is causally and selectively involved in scene perception</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>1331</fpage><lpage>1336</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4081-12.2013</pub-id><pub-id pub-id-type="pmid">23345209</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dillon</surname> <given-names>MR</given-names></name><name><surname>Persichetti</surname> <given-names>AS</given-names></name><name><surname>Spelke</surname> <given-names>ES</given-names></name><name><surname>Dilks</surname> <given-names>DD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Places in the brain: bridging layout and object geometry in Scene-Selective cortex</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>2365</fpage><lpage>2374</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx139</pub-id><pub-id pub-id-type="pmid">28633321</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Neural systems for visual scene recognition</chapter-title><person-group person-group-type="editor"><name><surname>Bar</surname> <given-names>M</given-names></name><name><surname>Keveraga</surname> <given-names>K</given-names></name></person-group><source>Scene Vision</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname> <given-names>R</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A cortical representation of the local visual environment</article-title><source>Nature</source><volume>392</volume><fpage>598</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1038/33402</pub-id><pub-id pub-id-type="pmid">9560155</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname> <given-names>MO</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganaden</surname> <given-names>RE</given-names></name><name><surname>Mullin</surname> <given-names>CR</given-names></name><name><surname>Steeves</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Transcranial magnetic stimulation to the transverse occipital sulcus affects scene but not object processing</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>961</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00372</pub-id><pub-id pub-id-type="pmid">23410031</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gandolfo</surname> <given-names>M</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causal evidence for expression of perceptual expectations in Category-Selective extrastriate regions</article-title><source>Current Biology</source><volume>29</volume><fpage>2496</fpage><lpage>2500</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.06.024</pub-id><pub-id pub-id-type="pmid">31327721</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname> <given-names>IIA</given-names></name><name><surname>Greene</surname> <given-names>MR</given-names></name><name><surname>Baldassano</surname> <given-names>C</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name><name><surname>Beck</surname> <given-names>DM</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinct contributions of functional and deep neural network features to representational similarity of scenes in human brain and behavior</article-title><source>eLife</source><volume>7</volume><elocation-id>e32962</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32962</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname> <given-names>U</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id><pub-id pub-id-type="pmid">26157000</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harel</surname> <given-names>A</given-names></name><name><surname>Groen</surname> <given-names>II</given-names></name><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Deouell</surname> <given-names>LY</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The temporal dynamics of scene processing: a multifaceted EEG investigation</article-title><source>Eneuro</source><volume>3</volume><elocation-id>ENEURO.0139-16.2016</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0139-16.2016</pub-id><pub-id pub-id-type="pmid">27699208</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Furey</surname> <given-names>ML</given-names></name><name><surname>Ishai</surname> <given-names>A</given-names></name><name><surname>Schouten</surname> <given-names>JL</given-names></name><name><surname>Pietrini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep residual learning for image recognition</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2016.90</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Gaze control as prediction</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>15</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.11.003</pub-id><pub-id pub-id-type="pmid">27931846</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henriksson</surname> <given-names>L</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Faciotopy-A face-feature map with face-like topology in the human occipital face area</article-title><source>Cortex</source><volume>72</volume><fpage>156</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.06.030</pub-id><pub-id pub-id-type="pmid">26235800</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henriksson</surname> <given-names>L</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Rapid invariant encoding of scene layout in human OPA</article-title><source>Neuron</source><volume>103</volume><fpage>161</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.04.014</pub-id><pub-id pub-id-type="pmid">31097360</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Julian</surname> <given-names>JB</given-names></name><name><surname>Fedorenko</surname> <given-names>E</given-names></name><name><surname>Webster</surname> <given-names>J</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An algorithmic method for functionally defining regions of interest in the ventral visual pathway</article-title><source>NeuroImage</source><volume>60</volume><fpage>2357</fpage><lpage>2364</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.055</pub-id><pub-id pub-id-type="pmid">22398396</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Julian</surname> <given-names>JB</given-names></name><name><surname>Ryan</surname> <given-names>J</given-names></name><name><surname>Hamilton</surname> <given-names>RH</given-names></name><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The occipital place area is causally involved in representing environmental boundaries during navigation</article-title><source>Current Biology</source><volume>26</volume><fpage>1104</fpage><lpage>1109</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.02.066</pub-id><pub-id pub-id-type="pmid">27020742</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Stein</surname> <given-names>T</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Object grouping based on real-world regularities facilitates perception by reducing competitive interactions in visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>11217</fpage><lpage>11222</lpage><pub-id pub-id-type="doi">10.1073/pnas.1400559111</pub-id><pub-id pub-id-type="pmid">25024190</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The neural dynamics of attentional selection in natural scenes</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>10522</fpage><lpage>10528</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1385-16.2016</pub-id><pub-id pub-id-type="pmid">27733605</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Quek</surname> <given-names>GL</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Object vision in a structured world</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>672</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.04.013</pub-id><pub-id pub-id-type="pmid">31147151</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Häberle</surname> <given-names>G</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Cortical sensitivity to natural scene structure</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/613885</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamps</surname> <given-names>FS</given-names></name><name><surname>Julian</surname> <given-names>JB</given-names></name><name><surname>Kubilius</surname> <given-names>J</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name><name><surname>Dilks</surname> <given-names>DD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The occipital place area represents the local elements of scenes</article-title><source>NeuroImage</source><volume>132</volume><fpage>417</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.02.062</pub-id><pub-id pub-id-type="pmid">26931815</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kant</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1781">1781</year><source>Kritik Der Reinen Vernunf</source><publisher-name>Johann Friedrich Hartknoch</publisher-name></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katti</surname> <given-names>H</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Arun</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Machine vision benefits from human contextual expectations</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>2112</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-38427-0</pub-id><pub-id pub-id-type="pmid">30765753</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kersten</surname> <given-names>D</given-names></name><name><surname>Mamassian</surname> <given-names>P</given-names></name><name><surname>Yuille</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Object perception as bayesian inference</article-title><source>Annual Review of Psychology</source><volume>55</volume><fpage>271</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.55.090902.142005</pub-id><pub-id pub-id-type="pmid">14744217</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>ImageNet classification with deep convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1097</fpage><lpage>1105</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowe</surname> <given-names>MX</given-names></name><name><surname>Rajsic</surname> <given-names>J</given-names></name><name><surname>Gallivan</surname> <given-names>JP</given-names></name><name><surname>Ferber</surname> <given-names>S</given-names></name><name><surname>Cant</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural representation of geometry and surface properties in object and scene perception</article-title><source>NeuroImage</source><volume>157</volume><fpage>586</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.043</pub-id><pub-id pub-id-type="pmid">28647484</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowe</surname> <given-names>MX</given-names></name><name><surname>Rajsic</surname> <given-names>J</given-names></name><name><surname>Ferber</surname> <given-names>S</given-names></name><name><surname>Walther</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Discriminating scene categories from brain activity within 100 milliseconds</article-title><source>Cortex</source><volume>106</volume><fpage>275</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2018.06.006</pub-id><pub-id pub-id-type="pmid">30037637</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malcolm</surname> <given-names>GL</given-names></name><name><surname>Groen</surname> <given-names>IIA</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Making sense of Real-World scenes</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>843</fpage><lpage>856</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.09.003</pub-id><pub-id pub-id-type="pmid">27769727</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mandler</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="1984">1984</year><source>Stories, Scripts and Scenes: Aspects of Schema Theory</source><publisher-name>Taylor &amp; Francis</publisher-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mandler</surname> <given-names>JM</given-names></name><name><surname>Johnson</surname> <given-names>NS</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Some of the thousand words a picture is worth</article-title><source>Journal of Experimental Psychology: Human Learning and Memory</source><volume>2</volume><fpage>529</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.2.5.529</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mandler</surname> <given-names>JM</given-names></name><name><surname>Parker</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Memory for descriptive and spatial information in complex pictures</article-title><source>Journal of Experimental Psychology: Human Learning and Memory</source><volume>2</volume><fpage>38</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.2.1.38</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Minsky</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1975">1975</year><chapter-title>A framework for representing knowledge</chapter-title><person-group person-group-type="editor"><name><surname>Winston</surname> <given-names>P</given-names></name></person-group><source>The Psychology of Computer Vision</source><publisher-name>McGraw-Hill</publisher-name></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliva</surname> <given-names>A</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Modelling the shape of the scene: a holistic representation of the spatial envelope</article-title><source>International Journal of Computer Vision</source><volume>42</volume><fpage>145</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1023/A:1011139631724</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>CoSMoMVPA: multi-modal multivariate pattern analysis of neuroimaging data in matlab/GNU octave</article-title><source>Frontiers in Neuroinformatics</source><volume>10</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00027</pub-id><pub-id pub-id-type="pmid">27499741</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Category selectivity in human visual cortex: beyond visual object recognition</article-title><source>Neuropsychologia</source><volume>105</volume><fpage>177</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.03.033</pub-id><pub-id pub-id-type="pmid">28377161</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Piaget</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1926">1926</year><person-group person-group-type="editor"><name><surname>Keagan Paul</surname> <given-names>C</given-names></name></person-group><source>The Language and Thought of the Child</source><publisher-name>Trench Trubner &amp; Co</publisher-name></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname> <given-names>D</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Disentangling representations of object shape and object category in human visual cortex: the Animate-Inanimate distinction</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00924</pub-id><pub-id pub-id-type="pmid">26765944</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname> <given-names>D</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>MEG sensor patterns reflect perceptual but not categorical similarity of animate and inanimate objects</article-title><source>NeuroImage</source><volume>193</volume><fpage>167</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.028</pub-id><pub-id pub-id-type="pmid">30885785</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rumelhart</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="1980">1980</year><chapter-title>Schemata: the building blocks of cognition</chapter-title><person-group person-group-type="editor"><name><surname>Spiro</surname> <given-names>JR</given-names></name></person-group><source>Theoretical Issues in Reading Comprehension</source><publisher-name>CRC press</publisher-name></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><volume>44</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname> <given-names>T</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Interobject grouping facilitates visual awareness</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/15.8.10</pub-id><pub-id pub-id-type="pmid">26114673</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name><name><surname>Castelhano</surname> <given-names>MS</given-names></name><name><surname>Henderson</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search</article-title><source>Psychological Review</source><volume>113</volume><fpage>766</fpage><lpage>786</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.4.766</pub-id><pub-id pub-id-type="pmid">17014302</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Statistics of natural image categories</article-title><source>Network: Computation in Neural Systems</source><volume>14</volume><fpage>391</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_14_3_302</pub-id><pub-id pub-id-type="pmid">12938764</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vedaldi</surname> <given-names>A</given-names></name><name><surname>Lenc</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>MatConvNet – convolutional neural networks for Matlab</article-title><conf-name>ACM International Conference on Multimedia</conf-name></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Võ</surname> <given-names>ML</given-names></name><name><surname>Boettcher</surname> <given-names>SE</given-names></name><name><surname>Draschkow</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reading scenes: how scene grammar guides attention and aids perception in real-world environments</article-title><source>Current Opinion in Psychology</source><volume>29</volume><fpage>205</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.copsyc.2019.03.009</pub-id><pub-id pub-id-type="pmid">31051430</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname> <given-names>DB</given-names></name><name><surname>Caddigan</surname> <given-names>E</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name><name><surname>Beck</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Natural scene categories revealed in distributed patterns of activity in the human brain</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>10573</fpage><lpage>10581</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0559-09.2009</pub-id><pub-id pub-id-type="pmid">19710310</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Mruczek</surname> <given-names>RE</given-names></name><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Probabilistic maps of visual topography in human cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id><pub-id pub-id-type="pmid">25452571</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname> <given-names>H</given-names></name><name><surname>Shi</surname> <given-names>J</given-names></name><name><surname>Chen</surname> <given-names>W</given-names></name><name><surname>Liu</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep residual network predicts cortical representation and organization of visual features for rapid categorization</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>3752</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-22160-9</pub-id><pub-id pub-id-type="pmid">29491405</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname> <given-names>JM</given-names></name><name><surname>Võ</surname> <given-names>ML</given-names></name><name><surname>Evans</surname> <given-names>KK</given-names></name><name><surname>Greene</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual search in scenes involves selective and nonselective pathways</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>77</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.12.001</pub-id><pub-id pub-id-type="pmid">21227734</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48182.026</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing Editor</role><aff><institution>Peking University</institution><country>China</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;A neural mechanism for contextualizing fragmented inputs during naturalistic vision&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>This work combined fMRI, EEG and deep neural network model to investigate the neural basis of encoding of fragment location information (e.g., vertical, horizontal positions) across different scene categories. It addresses an important question about how the 'abstract' spatial layout that is not explicitly presented in the stimulus itself is represented in neural activities. The paper is clearly written and the approach and results are interesting and novel. However, there are some major issues brought up by the reviewers that need the authors to address and do additional analysis.</p><p>For a revision to be successful, you must address the following major issues:</p><p>Essential revisions:</p><p>1) The thirty-six figure segments come from only six natural images (3 indoor and 3 outdoor), which means the figure fragments would be repeatedly presented and be learned or memorized gradually. It is therefore hard to distinguish two interpretations – do the results reflect a true representation of spatial layout knowledge that would be automatically formed and could generalize to any natural images or do they derive from a learning and familiarization process after repeated exposure? A possible way to assess this is to divide the data into various stages and compare the early- and late-stage results. If the spatial layout knowledge is automatically represented regardless of learning, we would expect to see the same results in the early part. On the other hand, if it is indeed learning or memory process that induces the results, we would expect to see the pattern only in the late part but not in the early part.</p><p>2) Each scene picture was split into two halves horizontally and three parts vertically. Thus, there are confounding factors with regards to why the effect only occurred for vertical locations but not for horizontal locations. The authors should either collect new data or perform new analysis to address the issue.</p><p>3) The authors used DNN regression to confirm that the vertical position effect is not due to category-related information. However, the involvement of low-level features in discriminating vertical locations is still quite possible and could not be completely ruled out from the current analysis. For example, image segments at different vertical locations of natural scenes (upper, middle, lower) seem to be also associated with different low-level features (e.g., low spatial frequency for upper part, such as sky or ceiling, etc.). The authors could add additional analysis to clarify the confounds, for example, by creating a low-level dissimilarity design matrix, which would then predict involvement of V1 for the low-level features but not for vertical location, while the reverse for OPA.</p><p>4) It is hard to understand what exactly DNN features were removed. ResNet50 and AlexNet were used widely but these DNN models were trained by a very large set of images, whereas the present study only compares 6 specific images. The specific features to differentiate the 6 specific images may not be the same as the removed DNN features. The authors could show reconstructed images with DNN features removed and it is quite possible that human observes would still differentiate the 6 reconstructed images even when the DNN features are removed.</p><p>5) It is difficult to figure out what the authors were arguing was the mechanism or the consequence: Does knowledge/schema help sort incomplete information, or does the brain sort incomplete information so that we can extract knowledge? The paper is motivated by the former (&quot;…the brain uses prior knowledge about where information typically appears in a scene to meaningfully sort incoming information&quot;) but then ends by stating the latter (&quot;This mechanism empowers the visual brain to efficiently extract meaning from dynamic real-world environments, where it is confronted with sequences of incomplete visual snapshots&quot;). Please add clarifications.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;A neural mechanism for contextualizing fragmented inputs during naturalistic vision&quot; for further consideration at <italic>eLife</italic>.</p><p>The manuscript has been improved but there was still one issue that was misunderstood or not clearly addressed.</p><p>In the original third comment, the reviewer pointed out that the DNN regression analysis cannot convincingly rule out the involvement of low-level properties in vertical position effect and suggested to directly test the DNN features in V1 (&quot;The authors could add additional analysis to clarify the confounds, for example, by creating a low-level dissimilarity design matrix, which would then predict involvement of V1 for the low-level features but not for vertical location, while the reverse for OPA.&quot;). The authors have performed additional analysis by using 3 low-level control models, but again just examined whether the vertical position effect existed after regression using these models. The results indeed are not quite convincing and it seems that the regression even could not disrupt category representations as the previous DNN model showed. To explicitly address the concern, as the reviewer originally suggested, the authors should perform a new analysis to show that the DNN model does account for low-level property representation in V1, which would then support the claim that DNN regression could remove the low-level features.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48182.027</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The thirty-six figure segments come from only six natural images (3 indoor and 3 outdoor), which means the figure fragments would be repeatedly presented and be learned or memorized gradually. It is therefore hard to distinguish two interpretations – do the results reflect a true representation of spatial layout knowledge that would be automatically formed and could generalize to any natural images or do they derive from a learning and familiarization process after repeated exposure? A possible way to assess this is to divide the data into various stages and compare the early- and late-stage results. If the spatial layout knowledge is automatically represented regardless of learning, we would expect to see the same results in the early part. On the other hand, if it is indeed learning or memory process that induces the results, we would expect to see the pattern only in the late part but not in the early part.</p></disp-quote><p>This is an important point. We deliberately chose few scene fragments for the experiment to compute reliable neural RDMs from multiple image presentations. We are aware of this choice posing a limitation to the current study and we now explicitly acknowledge this limitation in the Discussion section:</p><p>“As the current study is limited to a small set of scenes, more research is needed to explore whether schema-based coding generalizes to more diverse contents.”</p><p>Given the limited number of stimuli, it is in principle possible that the fragments’ location-specific representations only emerged after many stimulus repetitions across the experiment. To exclude this possibility, we performed the suggested analysis and separately analyzed data from the first and second halves of both experiments. For these halves (fMRI: first three versus last three runs, EEG: first versus second half of trials), we re-performed the main analysis for these halves. Critically, we found a very similar pattern of results with no statistical differences between the first and second half of each experiment, suggesting that the effect cannot be explained by excessive learning during the experiment. The results of this analysis are reported in Figure 2—figure supplement 6.</p><disp-quote content-type="editor-comment"><p>2) Each scene picture was split into two halves horizontally and three parts vertically. Thus, there are confounding factors with regards to why the effect only occurred for vertical locations but not for horizontal locations. The authors should either collect new data or perform new analysis to address the issue.</p></disp-quote><p>We now performed analyses where we analyzed all pairwise comparisons along the vertical axis (i.e., top versus bottom, top versus middle, and middle versus bottom), so that in each analysis there were only fragments from two different vertical locations. These analyses reveal a consistent vertical location organization, replicating the overall effect for each pairwise comparison, while suggesting that some comparisons (the ones including the top fragments) may contribute more to the effect. Results from these analyses are reported in Figure 2—figure supplement 7.</p><disp-quote content-type="editor-comment"><p>3) The authors used DNN regression to confirm that the vertical position effect is not due to category-related information. However, the involvement of low-level features in discriminating vertical locations is still quite possible and could not be completely ruled out from the current analysis. For example, image segments at different vertical locations of natural scenes (upper, middle, lower) seem to be also associated with different low-level features (e.g., low spatial frequency for upper part, such as sky or ceiling, etc.). The authors could add additional analysis to clarify the confounds, for example, by creating a low-level dissimilarity design matrix, which would then predict involvement of V1 for the low-level features but not for vertical location, while the reverse for OPA.</p></disp-quote><p>Thank you for this suggestion. Although regressing out deep neural networks should also control for low-level features to a substantial degree (previous research has shown that early DNN layers correspond well with early visual processing and activations in V1; see Cichy et al., 2016; Güclü and van Gerven, 2015), we now added new analyses where we used three additional models that explicitly control for low-level features: a pixel dissimilarity model, GIST descriptors (Oliva and Torralba, 2001), and V1 dissimilarity (as a neural approximation of low-level features). We re-performed our regression analyses after regressing out RDMs obtained from each of these low-level models. These analyses show that also the low-level models could not explain the fragments’ vertical location organization; neither could they explain their categorical organization. This indicates that schematic coding cannot be accounted for by low-level features. The new control analyses are summarized in Figure 3—figure supplement 2.</p><disp-quote content-type="editor-comment"><p>4) It is hard to understand what exactly DNN features were removed. ResNet50 and AlexNet were used widely but these DNN models were trained by a very large set of images, whereas the present study only compares 6 specific images. The specific features to differentiate the 6 specific images may not be the same as the removed DNN features. The authors could show reconstructed images with DNN features removed and it is quite possible that human observes would still differentiate the 6 reconstructed images even when the DNN features are removed.</p></disp-quote><p>This is an interesting point. First, we would like to point out that the reason we used categorization DNNs here was primarily to have a good measure of features that are routinely extracted for successful categorization, which includes low-level images properties as well as high-level category-defining features. It is true that the DNNs used here were trained to extract these features from large image databases, of whose properties the few images used in the study only capture a small fraction. However, it is worth noting that this qualitatively resembles processing in the visual system: The types of features the visual system routinely extracts are defined by excessive experience with large varieties of inputs, but at any single moment, we only have a very limited amount of input from which we extract a subset of these features. We now stress the reasons for using pre-trained DNNs in the Materials and methods section:</p><p>“DNNs are similar to the brain as they are trained using excessive training material while dynamically adjusting the “tuning” of their connections. […] By comparing DNNs activations and brain responses to the scene fragments, we could quantify to which extent features routinely extracted for categorization purposes account for schema-based coding in the human visual system.”</p><p>Second, we would certainly predict that after removing DNN features from the images, participants would still be able to visually discriminate between the images. We conceive of this as a feature rather than as a problem – ultimately the location information observed in brain responses must stem from visual features of the images. The key finding here is that the feature organization in DNN models, despite their similarity to visual cortex representations, cannot account for the schematic coding observed in the brain. We now bring up this point in the Discussion:</p><p>“While DNNs trained on categorization accurately capture the representational divide into different scene categories, they cannot explain the schema-based organization observed in the human visual system. Although this does not mean that visual features extracted by DNN models in principle are incapable of explaining schema-based brain representations, our results highlight that current DNN models of categorization do not use real-world structure in similar ways as the human brain.”</p><p>Finally, we agree that the current analysis does not provide information on the exact features accounted for by the DNN. Reconstruction images after controlling for DNN features could indeed provide a useful avenue for delineating the features that are uniquely extracted by the DNN and scene-selective cortex, respectively. However, we think that this is beyond the scope of the current paper, because reliably defining these features would ultimately require a larger, more diverse set of images and the acquisition of new experimental data.</p><disp-quote content-type="editor-comment"><p>5) It is difficult to figure out what the authors were arguing was the mechanism or the consequence: Does knowledge/schema help sort incomplete information, or does the brain sort incomplete information so that we can extract knowledge? The paper is motivated by the former (&quot;…the brain uses prior knowledge about where information typically appears in a scene to meaningfully sort incoming information&quot;) but then ends by stating the latter (&quot;This mechanism empowers the visual brain to efficiently extract meaning from dynamic real-world environments, where it is confronted with sequences of incomplete visual snapshots&quot;). Please add clarifications.</p></disp-quote><p>The ending did indeed not reflect our interpretation very clearly. We re-worded the concluding paragraph to make it more consistent with the Introduction:</p><p>“To conclude, our findings provide the first spatiotemporal characterization of a neural mechanism for contextualizing fragmented visual inputs. By rapidly organizing visual information according to its typical role in the world, this mechanism may contribute to the optimal use of perceptual information for guiding efficient real-world behaviors, even when sensory inputs are incomplete or dynamically changing.”</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there was still one issue that was misunderstood or not clearly addressed.</p><p>In the original third comment, the reviewer pointed out that the DNN regression analysis cannot convincingly rule out the involvement of low-level properties in vertical position effect and suggested to directly test the DNN features in V1 (&quot;The authors could add additional analysis to clarify the confounds, for example, by creating a low-level dissimilarity design matrix, which would then predict involvement of V1 for the low-level features but not for vertical location, while the reverse for OPA.&quot;). The authors have performed additional analysis by using 3 low-level control models, but again just examined whether the vertical position effect existed after regression using these models. The results indeed are not quite convincing and it seems that the regression even could not disrupt category representations as the previous DNN model showed. To explicitly address the concern, as the reviewer originally suggested, the authors should perform a new analysis to show that the DNN model does account for low-level property representation in V1, which would then support the claim that DNN regression could remove the low-level features.</p></disp-quote><p>Thanks for giving us the opportunity to address the remaining comment in more detail. If we understand correctly, the main worry is that the DNN models used in our study are not good models of low-level feature coding in cortex (i.e., they so not match well with V1-level representations), and thus regressing out DNN features is not a convincing way of ruling out low-level features as the explanation behind the main effect of interest, i.e. the vertical location effect.</p><p>We strongly agree that controlling for low-level features is an important issue in the current study. We see that in the previous revision we did not make sufficiently clear the comprehensive way in which we control for the effect of low-level features, DNN feature regression being only one of them. We therefore expose our perspective in detail below and made changes to the manuscript that make our argumentation more explicit.</p><p>Our pieces of evidence against low-level features behind the main effect of interest are the following. Most directly, we show that cortical area V1 does not exhibit vertical location effects. The absence of the effect of interest in the core region in cortex coding for low-level features is often taken as strong evidence against an involvement of low-level features. This thinking applies in our study as well. We make this point explicit in the current version of the manuscript:</p><p>“To efficiently support vision in dynamic natural environments, schematic coding needs to be flexible with respect to visual properties of specific scenes. The absence of vertical location effects in V1 indeed highlights that schematic coding is not tied to the analysis of simple visual features.”</p><p>Naturally, we agree that more comprehensive and corroborative evidence is desirable. Therefore, we go beyond this single piece of evidence with a series of control analyses.</p><p>First, we show that only taking visually very different indoor and outdoor scenes into account does not remove vertical location effect. Thus, it is unlikely that the vertical location effect simply reflects low-level features.</p><p>Second, we remove DNN features (or features extracted by low-level control models such as the GIST descriptor) and find that this does not abolish the vertical location effects. The reviewer questions in particular whether our DNN model accurately captures low-level feature representations. We believe that it is a fair model – we show that the DNN does explain variance in all regions examined, including V1 (Figure 3—figure supplement 2).</p><p>The reviewer further notes that the V1 data (category and vertical location effects) look somewhat comparable before and after regressing out the control models and takes this as suggesting that these are bad models of V1. However, given that we show that the DNN does explain variance in V1, this is also parsimoniously explained by there being no meaningful category or location organization in the V1 data to begin with.</p><p>That being said, we fully agree with the reviewer that although the DNN used might be a fair model of V1-level features, it is certainly not the best possible model. Thus, one could still assume that if DNNs approximated low-level feature representations in V1 more faithfully (with more variance explained), removing these DNNs' features might abolish the vertical location organization in higher-level cortex (i.e., the OPA), too.</p><p>To exclude this possibility (i.e., that it's simply a very good match to low-level feature representations in V1 that's needed to remove the OPA organization), we conducted a third control analysis. In this analysis, we used the best possible match to the neural V1 data as a low-level model – the V1 data itself! Naturally, the empirical neural V1 data explains the data in V1 (Figure 3—figure supplement 3). Crucially, after removing the neural organization in V1, the vertical location effect in OPA still persisted, showing that even if we had a model that predicted V1 exceptionally well (as well as V1 predicts itself), this model could not account for the vertical location effect.</p><p>We believe that this analysis already offers both properties that the reviewer asks for: a very good model of V1-level representation (arguably the best possible match to the neural organization in V1), and a strong vertical location effect in higher-level regions when the model is regressed out.</p><p>Together, we thus believe that the remaining open question boils down to how well DNNs can, in principle, model V1 organization. This is an interesting topic, which surely needs investigation – but from our point of view without immediate implications for the current paper. After all, we empirically show that even if the DNN perfectly captured the V1 organization, our conclusions would hold.</p><p>We laid out this argument in the revised manuscript:</p><p>“DNN features are a useful control for flexibility towards visual features, because they cover both low-level and high-level visual features, explaining variance across fMRI regions and across EEG processing time (see Figure 3—figure supplement 2; see also Cichy et al., 2016; Gücli and van Gerven, 2015). […] Together, these results provide converging evidence that low-level feature processing cannot explain the schematic coding effects reported here.”</p></body></sub-article></article>