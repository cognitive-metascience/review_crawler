<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">51975</article-id><article-id pub-id-type="doi">10.7554/eLife.51975</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>Elements of a stochastic 3D prediction engine in larval zebrafish prey capture</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-157658"><name><surname>Bolton</surname><given-names>Andrew D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3059-7226</contrib-id><email>andrewdbolton@fas.harvard.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">†</xref></contrib><contrib contrib-type="author" id="author-158431"><name><surname>Haesemeyer</surname><given-names>Martin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2704-3601</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-158432"><name><surname>Jordi</surname><given-names>Josua</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-158433"><name><surname>Schaechtle</surname><given-names>Ulrich</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-158434"><name><surname>Saad</surname><given-names>Feras A</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-158435"><name><surname>Mansinghka</surname><given-names>Vikash K</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-84635"><name><surname>Tenenbaum</surname><given-names>Joshua B</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-3787"><name><surname>Engert</surname><given-names>Florian</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Center for Brain Science</institution><institution>Harvard University</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Brain and Cognitive Sciences</institution><institution>Massachusetts Institute of Technology</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Harvard University, Cambridge, United States</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>26</day><month>11</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e51975</elocation-id><history><date date-type="received" iso-8601-date="2019-09-18"><day>18</day><month>09</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-11-25"><day>25</day><month>11</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Bolton et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Bolton et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-51975-v2.pdf"/><abstract><p>The computational principles underlying predictive capabilities in animals are poorly understood. Here, we wondered whether predictive models mediating prey capture could be reduced to a simple set of sensorimotor rules performed by a primitive organism. For this task, we chose the larval zebrafish, a tractable vertebrate that pursues and captures swimming microbes. Using a novel naturalistic 3D setup, we show that the zebrafish combines position and velocity perception to construct a future positional estimate of its prey, indicating an ability to project trajectories forward in time. Importantly, the stochasticity in the fish’s sensorimotor transformations provides a considerable advantage over equivalent noise-free strategies. This surprising result coalesces with recent findings that illustrate the benefits of biological stochasticity to adaptive behavior. In sum, our study reveals that zebrafish are equipped with a recursive prey capture algorithm, built up from simple stochastic rules, that embodies an implicit predictive model of the world.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>prey capture</kwd><kwd>prediction</kwd><kwd>physical models</kwd><kwd>biological stochasticity</kwd><kwd>computation</kwd><kwd>animal cognition</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Zebrafish</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U19NS104653</award-id><principal-award-recipient><name><surname>Engert</surname><given-names>Florian</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Zebrafish implement a stochastic recursive algorithm during prey capture that reflects an implicit physical model of the world.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>It is becoming clear from recent ethological and neuroscience studies that remarkable capabilities in animals are often built by combining sets of more basic behaviors. For example, seemingly complicated behaviors like schooling in fish arise from stereotypic visuomotor rules being executed by members of the group, without reference to the emerging global pattern (<xref ref-type="bibr" rid="bib18">Couzin and Krause, 2003</xref>). Bees learning to pull strings for hidden rewards appear to be displaying insight and ingenuity, but are in fact instituting a set of observational and associative learning rules in sequence (<xref ref-type="bibr" rid="bib2">Alem et al., 2016</xref>). The synthesis of intelligence from the interaction of ‘mindless’ behavioral modules has long been a staple of computer science and artificial intelligence (<xref ref-type="bibr" rid="bib16">Brooks, 1991</xref>; <xref ref-type="bibr" rid="bib41">Minsky, 1988</xref>; <xref ref-type="bibr" rid="bib15">Braitenberg, 1986</xref>). However, a precise mechanistic explanation of how intelligent behavior is generated has remained elusive.</p><p>Intelligence itself can been defined as the practice of model-building about ourselves and our surroundings (<xref ref-type="bibr" rid="bib33">Lake et al., 2017</xref>). Indeed, humans and animals have evolved internal models that allow us to both predict ongoing dynamics in the environment and anticipate how our own actions give rise to consequences in the world (<xref ref-type="bibr" rid="bib10">Battaglia et al., 2013</xref>; <xref ref-type="bibr" rid="bib59">Ullman et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Baillargeon, 1987</xref>; <xref ref-type="bibr" rid="bib42">Mischiati et al., 2015</xref>; <xref ref-type="bibr" rid="bib14">Borghuis and Leonardo, 2015</xref>). Much progress has been made on the neurobiological and computational principles that could mediate these complex abilities, but how exactly internal models are built from component behavioral parts is unknown. Moreover, many theories of mind rely on deterministic digital logic, while the brain generates intelligence using noisy, stochastic units in neurons (e.g. <xref ref-type="bibr" rid="bib39">McCulloch and Pitts, 1943</xref>). Noise in <italic>any</italic> computing system is usually considered inconvenient and a nuisance to be overcome (<xref ref-type="bibr" rid="bib32">Körding and Wolpert, 2004</xref>). However, there is precedent for noisy sensory detection and stochastic movements working to the benefit of many animals. Crayfish and paddlefish, for instance, both take advantage of stochastic resonance to detect sparse prey and predators (<xref ref-type="bibr" rid="bib21">Douglass et al., 1993</xref>; <xref ref-type="bibr" rid="bib52">Russett et al., 1999</xref>). Beneficially stochastic foraging has been observed in animals ranging down to micro-organisms, while predated animals use mixed stochastic strategies to avoid predictability by predators (<xref ref-type="bibr" rid="bib29">Jensen, 2018</xref>).</p><p>Can internal models that mediate predictive abilities be reduced to a set of simple rules that are benefited by the stochasticity of neural systems? We posit that explicit physical intelligence is likely built upon a framework of more primitive sensorimotor behaviors that constitute an implicit model of how objects exist and move within the world (i.e. <xref ref-type="bibr" rid="bib16">Brooks, 1991</xref>). Therefore, characterizing the goals, algorithms, and advantages of animals possessing implicit models should provide insight into the evolution of more advanced forms of predictive knowledge (<xref ref-type="bibr" rid="bib55">Spelke and Hespos, 2018</xref>).</p><p>Here, we examine these questions through characterization and computational modeling of 3D prey capture sequences executed by the larval zebrafish. The larval zebrafish, a teleost with ~100,000 neurons, is a tractable model organism amenable to an array of modern neuroscience techniques (<xref ref-type="bibr" rid="bib5">Avella et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Dunn et al., 2016</xref>). Prey capture requires the zebrafish to select, pursue, and ultimately consume fast moving single-celled organisms swimming through its environment. We chose this model system and behavior for a multitude of reasons. Foremost, we hypothesized that the pursuit of fast-moving prey should be benefitted by internal models; the ability to extrapolate prey trajectories forward in time and the prediction of how each pursuit movement impacts prey position would both, a priori, appear to be helpful in capturing fast-moving objects (e.g. <xref ref-type="bibr" rid="bib61">Yoo et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Borghuis and Leonardo, 2015</xref>). Furthermore, there is precedent for the zebrafish constructing relatively complex behaviors from simple rules. For example, stabilization of position in turbulent streams is accomplished by instantiating a curl-detector for local water flow and an optomotor response (<xref ref-type="bibr" rid="bib48">Oteiza et al., 2017</xref>; <xref ref-type="bibr" rid="bib47">Naumann et al., 2016</xref>), while energy-efficient foraging emerges from the zebrafish’s innate tendency to locomote via alternating series of unidirectional turns (<xref ref-type="bibr" rid="bib22">Dunn et al., 2016</xref>). Finally, the zebrafish’s ongoing behavior is largely probabilistic, reflecting the stochasticity of its neural systems. For instance, the precise number of unidirectional turns in any spontaneous swimming stretch is stochastic, while turn magnitude in response to angular optic flow varies widely (<xref ref-type="bibr" rid="bib22">Dunn et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Naumann et al., 2016</xref>).</p><p>We build upon foundational studies in zebrafish prey capture (<xref ref-type="bibr" rid="bib49">Patterson et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Trivedi and Bollmann, 2013</xref>; <xref ref-type="bibr" rid="bib11">Bianco et al., 2011</xref>) by analyzing this behavior in its natural 3D setting, whereas previous studies have typically neglected vertical fish and prey movements. Moreover, we develop an experimental and computational framework that can simultaneously record fish and prey trajectories. This approach allowed us to accurately map the fish’s sensorimotor transformations in response to ongoing prey features, which are described in an egocentric spherical coordinate system that specifies the fish’s three-dimensional point of view. Particularly, we illustrate three main elements of the fish’s prey capture algorithm that reflect an implicit intuitive model of physics. First, sensorimotor transformations during prey capture are largely controlled by the azimuth angle, altitude angle and computed radial distance of prey before the fish initiates a pursuit movement. Second, all aspects of the fish’s 3D movement choices are strongly and proportionally modulated by the angular and radial velocity of its prey. Combining these two rules yields an emergent strategy whereby the fish predicts future prey locations and recursively halves the angle of attack. Third, we show that the speed of the fish’s recursive hunting strategy is benefited by noise in its sensorimotor transformations. Importantly, this stochasticity is graded, meaning that the further away a prey item is from the fish’s goal, the more variable the outcome of the fish’s choice becomes. Using a series of computational models and virtual prey capture simulations, we show that position perception, velocity projection, and graded variance are <italic>all</italic> essential for effective and energy-efficient prey capture performance. This suggests that the fish’s implicit models are, in fact, adaptive to the animal.</p><p>Overall, this work reveals that even the most complex behavior in larval zebrafish can be reduced to a set of simple rules. These rules coalesce to generate a stochastic recursive algorithm embodied by zebrafish during hunting, which ultimately reflects an implicit predictive model of the world.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Developing a 3D environment for elucidating hunting sequences in zebrafish</title><p>We first sought to characterize the sensorimotor transformations larval zebrafish implement when pursuing and capturing paramecia. Hunting sequences were recorded from 46 larval zebrafish using a behavioral setup that could simultaneously image the fish and its prey from the top and side at high resolution and speed (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Custom computer vision software was designed to reconstruct the fish’s 3D position and two of the principal axes, yaw and pitch (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), as well as the trajectories of paramecia in the environment (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). These reconstructions allowed us to spatially map prey position and velocity to an egocentric spherical coordinate system originating at the mouth of the fish. Hereby, each paramecium is assigned an azimuth, altitude and distance as a positional coordinate (‘Prey Az, Prey Alt, Prey Dist’), along with angular (Prey δAz / δt, Prey δAlt / δt) and radial (Prey δDist / δt) velocities with respect to the fish’s 3D point of view (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). To view the prey environment from the fish’s reconstructed 3D perspective, see <xref ref-type="video" rid="video1">Video 1</xref>.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>A novel 3D experimental paradigm for mapping prey trajectories to fish movement choices.</title><p>(<sc><bold>A</bold></sc>) 3D rendering of rig design and features. (<bold>B</bold>) Computer vision algorithms extract the continuous eye angle, yaw, pitch, and tail angle of the zebrafish. In every frame, prey are detected using a contour finding algorithm. (<bold>C</bold>) Prey contours from the two cameras are matched in time using a correlation and 3D distance-based algorithm, allowing 3D reconstruction of prey trajectories. (<bold>D</bold>) Prey features are mapped to a spherical coordinate system originating at the fish’s mouth. Altitude is positive above the fish, negative below. Azimuth is positive right of the fish, negative left. Distance is the magnitude of the vector pointing from the fish’s mouth to the prey.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Identificaiton of hunt sequences.</title><p>(<bold>A</bold>) Spectral clustering (scikit-learn) was used to cluster the continuous eye angle over each bout for both eyes. The initiation of hunt sequences was identified using Cluster three and deconvergence of the eyes was demarked by Cluster 1. (<bold>B</bold>) Hunt sequence types in the dataset. The user’s only role is to denote the last bout of the hunt sequence, characterize the sequence as a strike hit, strike miss, or abort, and note the chosen prey ID assigned by our automated prey reconstruction algorithm. The program then outputs the descriptors in B per hunt. ‘Collisions’ imply that the fish head has collided with the wall during the hunt (detected using fish COM and edge coordinates), preventing analysis of whether the fish would have struck or aborted. Collisions with unknown target are likely hunting of a paramecium reflection. Deconvergence, known target and unknown target, is the standard abort described previously (<xref ref-type="bibr" rid="bib30">Johnson et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Henriques et al., 2019</xref>), with ‘unknown targets’ being too ambiguous for the user to make a call on pursued prey ID. No deconvergence, known target are hunts where fish had initiated to and pursued a particular prey item, but clearly stopped pursuit on a particular bout not assigned to Cluster 1. ‘Probably not a hunt’ was a rare case where the fish converged, swam through the tank without choosing a prey, and did not deconverge within eight bouts. ‘Strike at Nothing’ was another rare case where the fish converged and struck without a prey item present. The fish did spend some time striking at immobile objects that were almost invariably residue stuck to the top of the tank. During strike hits, fish choose a prey and consume it, with strike misses typically a deflection of the prey off the fish’s mouth at hunt termination. The main manuscript is built off of strike hits and misses (which combined into a ‘strikes at known prey’ category would be the most common outcome), while Supplementary Figure 3’s abort algorithm is built from known target, deconverge and no-deconverge hunts. Collisions are simply an outcome of having a relatively small tank for parfocal imaging compared to the fish’s real environment; hunts resulting in collisions were not analyzed except for the initial choice.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Fish tend to choose the closest available prey when initiating hunt sequences.</title><p>(<bold>A</bold>) Colors are histograms of coordinates for prey chosen at hunt initiation, gray are all prey records [chosen + ignored. Note: XY records passing a threshold length and velocity that remain unpaired after prey reconstruction are assigned the Z-coordinate of the tank ceiling because live paramecia show anti-gravitaxis (Roberts 2010); we always noted that a subset of prey gather at the ceiling and never noticed coagulation of prey on the ground unless dead; results are very similar to those shown if ceiling assigned prey are not counted]. (<bold>B</bold>) Histograms showing the distribution of spherical velocities for chosen prey do not reveal a bias in magnitude or direction. (<bold>C</bold>) Count plots of distance rank for selected prey (0 = closest). (D) We virtually displaced fish coordinates at hunt initiation bouts into randomly recorded paramecium environments and asked whether the closest prey item in that environment shared azimuth and altitude features with prey that fish actually chose. Histograms of the closest prey in <italic>random</italic> prey environments and prey environments in which initiation actually occurred are plotted in red (for random condition, fish orientation and position at hunt initiation is projected into a different time during the experiment; left panel). Blue (az) and yellow (alt) histograms are chosen prey histograms from (<bold>A</bold>) for comparison. The closest prey item in a random environment does not show the same distribution as selected prey in (<bold>A</bold>), indicating that the closest prey does not necessarily have to share the altitude and azimuth features of chosen prey. This suggests that somewhat specific prey features are preferred for entry into the hunting state, although transition probabilities governing hunting mode entry are also at play (<xref ref-type="bibr" rid="bib30">Johnson et al., 2019</xref>; <xref ref-type="bibr" rid="bib40">Mearns et al., 2019</xref>).</p><p><supplementary-material id="fig1s2sdata1"><label>Figure 1—figure supplement 2—source data 1.</label><caption><title>Source data describing prey at hunt initiations.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-51975-fig1-figsupp2-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig1-figsupp2-v2.tif"/></fig></fig-group><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-51975-video1.mp4"><label>Video 1.</label><caption><title>In each instance, a hunt is shown from the top and side cameras simultaneously, followed by a virtual reality reconstruction of the fish’s point of view during the hunt.</title><p>The virtual reality reconstruction, built in Panda3D, is generated from 3D prey coordinates and unit vectors derived from the 3D position, pitch, and yaw of the fish.</p></caption></media><p>Larval zebrafish swim in discrete ‘bouts’, which consist of a pulse of velocity lasting ~200 ms, followed by a variable period of intermittent quiescence (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; <xref ref-type="bibr" rid="bib17">Budick and O'Malley, 2000</xref>). We took advantage of this unique facet of fish behavior to frame each bout performed during a hunting sequence as an individual ‘decision’ based on the spherical position and velocity of pursued prey. Specifically, we identified the start-time and end-time of each swim bout using fluctuations in tail variance and velocity (see Materials and methods). This allowed us to precisely understand how the fish transforms pre-bout prey features into movements that displace and rotate the fish to a new location in 3D space at the end of the bout.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Fish execute 3D movements based on the position of their prey.</title><p>(<bold>A</bold>) During hunt sequences, fish swim in bouts that can be detected using tail variance. Bouts can change the yaw, pitch, and position of the fish, while time between bouts is marked by quiescence. (<bold>B</bold>) Histograms showing the distributions of spherical prey positions when fish successfully ate a paramecium during a strike. (<bold>C, D</bold>) Regression fits between prey position and bout variables executed by the fish. (<bold>E, F</bold>) Features of sensorimotor transformations based on prey position: fish swim forward if prey are directly in front. Otherwise, if prey are on right, fish displace and rotate right; and vice versa. Fish displace downward if prey are at 0° altitude, but displace with no altitude change if prey are at 13.75°. In all schematics (<bold>C–F</bold>), positions and orientations at the beginning of the bout are represented by transparent fish, and by opaque fish at the end of the bout.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Source data for all bouts conducted in dataset.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-51975-fig2-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Prey capture algorithm during aborted hunt sequences.</title><p>(<bold>A</bold>) Regression fits between prey position and bout variables during hunt sequences ending in an abort. Gray points and lines represent the last three pursuit bouts before the quit bout occurs, colors are all bouts between initiation and the last 3. The algorithm strongly resembles <xref ref-type="fig" rid="fig2">Figure 2</xref> transformations at the beginning of hunt sequences that will eventually end in aborts, but goes awry in the last three bouts before quitting. (<bold>B</bold>) Pursuit bouts during abort sequences show modulation by velocity at inflection points similar to <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>. (<bold>C</bold>) Orange model is the same as <xref ref-type="fig" rid="fig4">Figure 4</xref> (Orange Model 2), which issues bouts based on multiple regression to prey position variables only. Green model is same as <xref ref-type="fig" rid="fig4">Figure 4</xref> (Green Model 3), which issues bouts based on multiple regression to prey position and velocity variables. However, both are fit using pursuit bouts during aborted sequences (outside of the last three bouts before quitting) instead of strike sequences (i.e. <xref ref-type="fig" rid="fig4">Figure 4</xref>). As with models fit on strike sequences, multiple regression using prey position and velocity outperforms position only regression due to proportional velocity modulation. Both models are fed the exact same prey trajectories as models in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Fitting on pursuit bouts during aborted sequences thus shows similar performance levels to models fit on strike sequences.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig2-figsupp1-v2.tif"/></fig></fig-group><p>Hunting sequences themselves consist of an initiation bout, multiple pursuit bouts, and a termination bout. Initiation bouts were identified by detecting whether the eyes on a given bout have converged. Eye convergence, which allows the use of stereovision by creating a small binocular zone, is a well-known correlate of hunting state entrance in zebrafish (<xref ref-type="bibr" rid="bib24">Gahtan et al., 2005</xref>; <xref ref-type="bibr" rid="bib11">Bianco et al., 2011</xref>). Hunt sequences were therefore identified by clustering the continuous eye angle record for both eyes during each bout (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). Hunt sequences were terminated on the bouts where fish either struck at their pursued prey or clearly quit pursuit. Quitting has been called an ‘aborted’ hunt in the literature (<xref ref-type="bibr" rid="bib28">Henriques et al., 2019</xref>; <xref ref-type="bibr" rid="bib30">Johnson et al., 2019</xref>), and most aborts in our dataset, as in other studies, corresponded to the cluster demarking deconvergence of the eyes (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>) and a return to an exploratory state. The average hunting sequence ending in a strike in our dataset lasted for five bouts (Interquartile Range = 4 to 7).</p></sec><sec id="s2-2"><title>Zebrafish typically choose the closest prey item when initiating a hunt sequence</title><p>Nearly all hunt sequences in our dataset began with the choice of a single prey item to pursue (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). The choice of prey item was straightforward. Fish almost invariably chose the closest paramecium in the environment conditioned on the fact that the paramecium was fairly close to its midline in azimuth and significantly above it in altitude (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>; μ<sub>az</sub> = 0.7° σ<sub>az</sub> = 48.4°; μ<sub>alt</sub> = 19.9° σ<sub>alt</sub> = 19.7°; μ<sub>dist</sub> = 3.4 mm σ<sub>dist</sub> = 1.6 mm). There was no particular bias of prey choice in terms of direction or magnitude of velocity (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>, bottom panels).</p></sec><sec id="s2-3"><title>Sensorimotor transformations during prey capture are largely controlled by Pre-Bout prey position</title><p>After choosing a prey item during an initiation bout, the fish engages in a series of pursuit bouts (see <xref ref-type="video" rid="video1">Video 1</xref>) that can each influence the position, yaw, and pitch of the animal (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Pursuit bouts are conducted until prey are positioned in a ‘strike zone’, which defines the termination condition for successful hunts in spherical coordinates relative to the fish: this zone is directly in front of, and considerably above the fish (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; avg. 0.9° Prey Az, 17.4° Prey Alt, .870 mm Prey Dist; <xref ref-type="bibr" rid="bib40">Mearns et al., 2019</xref>). We investigated whether displacements and rotations during pursuit bouts were influenced by 3D prey position before each bout. For the rest of this manuscript, only sensorimotor transformations during hunt sequences in which a strike was performed are described. However, the algorithm the fish uses during aborts is nearly identical (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>; note that the last three bouts in abort sequences tend to go awry for unknown reasons; <xref ref-type="bibr" rid="bib28">Henriques et al., 2019</xref>).</p><p>For every pursuit bout, we calculate an axis of motion in egocentric spherical coordinates along which fish displace during the bout. This axis is defined by an azimuth and an altitude angle (‘Bout Az’, ‘Bout Alt’), and the magnitude of displacement along this axis is termed ‘Bout Distance’. Because the axis of motion during bouts is not perfectly aligned to the axis of symmetry, yaw and pitch changes are independently described per bout (‘Bout ΔYaw’, ‘Bout ΔPitch’). Diagrams of rotation and displacement variables are provided alongside <xref ref-type="fig" rid="fig2">Figure 2C and D</xref>.</p><p>Each bout aspect is primarily controlled by the position of the prey relative to the fish immediately preceding bout initiation (<xref ref-type="fig" rid="fig2">Figure 2C and D</xref>). Regression fits show that Bout Az and Bout ΔYaw are well correlated to Prey Az, with negligible offset (0.3° and 0.48°). This transformation simply implies that fish displace and turn towards their prey. A similar linear relationship is seen when mapping Bout Alt and Bout ΔPitch of pursuit bouts to Prey Alt, but this time with significant negative offsets (−15.13° and −1.79°). These negative offsets imply that if Prey Alt before a pursuit bout is 0°, which one may preconceive as the fish’s ultimate ‘goal’, the fish will dive downwards by ~15° and rotate downward by ~2°, thereby consistently maintaining the prey above itself at the end of pursuit bouts. Schematics of these rules are shown in <xref ref-type="fig" rid="fig2">Figure 2E and F</xref>. Interestingly, these positional transformations reflect the fish’s preferred position in which to strike for prey consumption: with prey directly in front and significantly above the fish (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>Bout Distance along the axis of motion established by Bout Az and Bout Alt is a more complex variable and will be addressed below.</p></sec><sec id="s2-4"><title>All 3D movements during prey capture are strongly modulated by prey velocity</title><p>Fish must capture prey that can move very quickly (in our assay, 74% 3D vector velocity &gt;3 paramecium lengths per second, 27% &gt; 6 paramecium lengths per second; avg. Prey δAz / δt = 29°/s, Prey δAlt / δt = 25°/s; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). We therefore surmised that prey velocity perception should be required for prey capture. Previous studies had suggested that kinematics of zebrafish movements change from slow to fast bouts based on whether prey are approaching or swimming away from the fish (<xref ref-type="bibr" rid="bib49">Patterson et al., 2013</xref>). Our 3D setup allowed us to conduct a detailed analysis of prey velocity perception in all planes.</p><p>We find that every movement the fish performs during prey capture is strongly and proportionally influenced by both the radial and angular velocity of its prey (<xref ref-type="fig" rid="fig3">Figure 3</xref>). All angular bout variables are amplified if prey are moving away from the fish (<xref ref-type="fig" rid="fig3">Figure 3</xref> light colors) and dampened when prey are moving towards the fish (<xref ref-type="fig" rid="fig3">Figure 3</xref> dark colors; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). Modulation of bout features is <italic>proportional</italic> to the velocity of the prey, as it is well fit by multiple linear regression transforming position <italic>and</italic> velocity of prey into fish bout variables (see <xref ref-type="fig" rid="fig3">Figure 3</xref> legend).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Prey velocity biases all aspects of fish's bout choices.</title><p>(<bold>A, B</bold>) Light colors depict each bout variable if prey are moving away from the fish while dark colors indicate that prey are moving toward the fish in the same 5° window of space. A pattern emerges of dampening movements when prey velocity is towards the fish, and amplifying movements when prey velocity is away from the fish. Point locations are means with error bars representing 95% CIs. Multiple regression fitting of bout variables to prey position and velocity in all planes confirm and quantify the dampening and amplification (azimuth velocity moving left to right of the fish is positive, altitude velocity upward is positive). Bout Az is biased by .251 <italic>* Prey</italic> δAz / δt (.219 - .283 95% CI), Bout ΔYaw by .054 <italic>* Prey</italic> δAz / δt (.046-.061 95% CI), Bout Alt by .300 <italic>* Prey</italic> δAlt / δt (.268 - .331 95% CI), and Bout ΔPitch by .031 <italic>* Prey</italic> δAlt / δt (.024 - .039 95% CI). Dividing these coefficients by the average bout length (0.176 s) yields the projected positional coefficients described in the text. (<bold>C</bold>) Bout Distance is linearly proportional to Prey Distance but only within 4 mm of the fish, with breakdown in relationship above 6 mm. Likewise, dampening of Bout Distance when Prey δDist / δt &lt; 0 (prey approaching radially), and amplifying when Prey δDist / δt &gt; 0 (prey moving afar), occurs in two windows: 0–1 mm, and 1–2 mm from the fish. Overall, multiple regression finds: <italic>Bout Distance = 0.105 * Prey Dist</italic> + .053 <italic>* Prey</italic> δDist / δt (95% CIs = 0.094-.116, .034-.071), which reflects the fact that ~70% of pursuit bouts occur when prey are within 2 mm. (*: p&lt;0.05/6, Bonferroni corrected two-tailed t-tests. p-values: 0–1 mm = 2.7 * 10<sup>−6</sup>, 1–2 mm: 0.0016, 2–3 mm: 0.38, 3–4 mm: 0.73, 4–6 mm: .00046, 6+: 0.92).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Distribution of prey velocity and example prey velocity-based biasing of bout features.</title><p>(<bold>A</bold>) Distribution of 3D velocities of prey hunted by fish in our study. Mean azimuth and altitude velocities (absolute value) are shown in reference to the fish, averaged over 80 ms before pursuit bout initiation, which is the time interval of all velocity calculations implemented here. (<bold>B</bold>) Example histograms for two 5° windows of prey space, as shown in <xref ref-type="fig" rid="fig3">Figure 3A,B</xref>. Again, light colors indicate that prey are moving away from the fish and dark moving toward. If prey are 0–5° to the right of the fish, but swimming toward the fish in azimuth, the fish will actually turn and displace left, predicting that the prey will cross its midline. If swimming away, fish turn and displace to the right. In a window 15–20° above the fish in altitude, the fish will actually ‘wait’ for prey with downward velocity (i.e. Bout Alt ~0°), anticipating that the prey will arrive in the strike zone. Otherwise if prey are moving upwards, fish displace and rotate upwards.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig3-figsupp1-v2.tif"/></fig></fig-group><p>We next wondered whether velocity perception constitutes a prediction of future prey position. Given that most bouts zebrafish make during hunts are 176 ms long (IQR = 144 ms to 208 ms), we can calculate from each prey velocity the change in prey position that would occur during an average bout. We find that zebrafish transform <bold>projected</bold> prey position changes by 1.43 for Bout Az,. 31 for Bout ΔYaw, 1.70 for Bout Alt, and. 18 for Bout ΔPitch (see <xref ref-type="fig" rid="fig3">Figure 3</xref> legend). Critically, these values closely approximate the coefficients describing bout transformations to prey position itself in <xref ref-type="fig" rid="fig2">Figure 2</xref>. This is the first hint that the fish has reduced the problem of position prediction to adding velocity multiplied by bout time to its current prey position percept. In this sense, the fish are performing Euler’s Method of approximating a future position based on its instantaneous derivative.</p><p>The final bout variable to address, Bout Distance, is more nuanced than the other four bout features. The linear relationship between Bout Distance and Prey Distance, in general, is only strong when prey are &lt;4 mm from the fish (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Bout Distance is significantly modulated by radial prey velocity (δDist / δt; see <xref ref-type="fig" rid="fig1">Figure 1D</xref>) when prey are within 2 mm (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). In this spatial window, radial velocity of prey coming toward the fish dampens Bout Distance while radial velocity moving away from the fish amplifies Bout Distance. Indeed, multiple regression finds an overall correlation between Bout Distance and Prey Distance with significant dampening when Prey δDist / δt &lt; 0 and amplification of Bout Distance when Prey δDist / δt &gt; 0 (<xref ref-type="fig" rid="fig3">Figure 3C</xref> legend for coefficients).</p></sec><sec id="s2-5"><title>Computational models of prey capture behavior show efficiency and success arise from velocity perception</title><p>Next, we asked to which degree the use of prey velocity contributes to the animal’s ability to efficiently and successfully capture prey. To that end we constructed a Virtual Prey Capture Simulation Environment in which computational models of fish behavior with different ‘powers’ can be pitted against each other. Each of these models varied along two axes: how the fish perceives its prey and how it transforms prey perception into movements. The relative powers possessed by each model allowed us to assess how the fish balances prey capture speed and accuracy with the energy required for such a movement sequence. We started by re-creating paramecium trajectories of 225 hunt sequences initiated by the fish that resulted in a real-life strike (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, Materials and methods). From the initial conditions of these trajectories, we launched five models that transform current paramecium features into 3D bouts as the prey moves through the environment.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Virtual prey capture simulation reveals necessity of velocity perception.</title><p>(<bold>A</bold>) A virtual prey capture environment mimicking the prey capture tank was generated to test three different types of models, and six models overall, described in the schematic. Models control a virtual fish consisting of a 3D position (red dot) and a 3D unit vector pointing in the direction of the fish’s heading. Virtual fish are started at the exact position and rotation where fish initiated hunts in the dataset. Prey trajectories are launched that reconstruct the real paramecium movements that occurred during hunts. The virtual fish moves in bouts timed to real life bouts, and if the prey enters the strike zone (defined by the distributions in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, Materials and methods), the hunt is terminated. (<bold>B</bold>) Barplots (total #) and box plots (median and quartiles) showing performance of all six models in success (# Strikes), energy use per hunt sequence, and how many bouts each model performed during the hunt (a metric of hunt speed). (<bold>C</bold>) KDE plots showing the distribution of Post-Bout Prey Az and Post-Bout Prey Alt distributions for each model during virtual hunts. Dotted lines demark the strike zone mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig4-v2.tif"/></fig><p>Each model initiates a bout at the precise moment when the real fish initiated a bout during the sequence, and model sequences are terminated when the prey enters the virtual strike zone (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, Materials and methods). Moreover, we compare all models to the real 3D fish trajectory (the ‘real fish’ model, <xref ref-type="fig" rid="fig4">Figure 4</xref> Model 1 [Blue]), both assuring that our 3D bout and strike-zone characterizations recapitulate real-life performance, and allowing us to compare post-bout coordinates of all models to the characterization of real bouts.</p><p>The capabilities of each model are as follows (<xref ref-type="fig" rid="fig4">Figure 4A</xref>): First, a multiple regression model was fit on only positional features of the paramecium (Model 2 [Orange]). This model linearly transforms current position of the prey into 3D bout features according to the regression fits in <xref ref-type="fig" rid="fig2">Figure 2C,D</xref> (and the bout distance fit from 3C). Model 3 (Green), a multiple regression model fit on both the position and velocity of prey, accounts for the amplification and dampening of bout features by prey velocity in <xref ref-type="fig" rid="fig3">Figure 3</xref>; position transforms are thus linearly biased by the velocity coefficients described in the <xref ref-type="fig" rid="fig3">Figure 3</xref> legend.</p><p>Models 4–6 are ‘Choice’ models which can draw from a distribution of 1782 pursuit bouts conducted by fish in our dataset during sequences ending in a strike. This bout pool can be thought of as the ‘pursuit repertoire’ of larval zebrafish. Model 4 (Red) simply assembles random bouts drawn from the bout pool into a hunt sequence. Model 5 (Purple) chooses the ideal bout from the pool, with each bout scored on its achievement in reducing the prey’s azimuth, altitude, and distance to the mean values of the strike zone (see Materials and methods). Model 5 does <italic>not</italic> have access to the velocity of the prey, meaning that it will zero in on the <italic>pre-bout</italic> prey position. Lastly, Model 6 (Gold) has the same capabilities to choose ideally as Model 5, but will extrapolate the current prey velocity and add its time multiplied bias to the current position. Therefore, Model 6 can predict the future paramecium position at the end of the bout, but chooses ideally instead of linearly.</p><p>To compare the models, we describe four facets of their performance intended to score raw hunting success as well as energetic cost: First, how many times out of 225 the model achieved success in placing the virtual prey in its strike zone. Second, how much total energy was expended by the bout combination used during the hunt (see Materials and methods). Third, how many total bout choices the model made per hunt as a metric for capture speed (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). And lastly, the Post-Bout Prey Az and Alt prey coordinate for each transformation was plotted to illustrate how well each model does in reducing prey coordinates to the strike zone (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Performance of each model for an example prey trajectory can be viewed in <xref ref-type="video" rid="video2">Video 2</xref>.</p><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-51975-video2.mp4"><label>Video 2.</label><caption><title>Virtual prey capture simulation environment.</title><p>Each model from <xref ref-type="fig" rid="fig4">Figure 4</xref> begins at the same position and orientation and is given the task of hunting the same paramecium trajectory. In this representative hunt sequence, every model except the Random Choice and Multiple Regression (Position Only) models consume the prey (indicated by red STRIKE flash). As is typical in the simulations, the Ideal Choice (Position) model lags the Ideal Choice (Velocity) model by one bout.</p></caption></media><p>The first clear result is that the velocity-based regression Model 3 (Green) improves hunting success over the position-only regression Model 2 (Orange) by 65%. Moreover, the average number of bouts is one less for the velocity model, matching the average of the real fish (Blue). This indicates that the velocity information processed by the fish, which allows projection of future prey coordinates, is critical for both its success rate and speed in capture. When examining the Post-Bout Prey Az for the regression models, the velocity-based Model three shows a tighter distribution around 0°, indicating that it is closer to the strike zone on average than position-only Model 2 (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, right top panels). This is also true for Post-Bout Prey Alt, where the green plot shows a stronger bias toward the strike zone.</p><p>Velocity information is also critical to the performance of ideal choice models. As expected, the random choice Model 4 (Red) performs very poorly, indicating that although similar ‘types’ of pursuit bouts are chosen, success can only be gained by accounting for prey features. Model 5 (Purple), interestingly, does not outperform the real fish in terms of success (3% worse) or average speed of capture, and expends significantly more energy. Model 5 therefore issues high energy bouts (i.e. bouts that strongly rotate and displace the fish), but without any average improvement over what the fish actually did, owing to the high velocity of the average prey item (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). Model 6 (Gold), however, by accounting for prey velocity and choosing ideally, improves success rate over Model 1 (Real Fish) and 5 (Ideal Position) by 14% and 17%, reduces the average number of required bouts by 1, and more effectively reduces Post-Bout Prey Az and Alt to the strike-zone (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, bottom panels). Nevertheless, Model 6 expends the most energy of any model per hunt sequence, meaning that fully ideal choice comes at an energetic cost.</p><p>We therefore conclude that position perception improves performance over issuing random pursuit bouts with no reference to the prey, and that velocity information in all formats improves model performance over position perception alone. Of note, the high energy usage of the ideal models relative to the real fish argues against the natural implementations of these seemingly optimal strategies. Lastly, although the real fish takes fewer bouts to reach the target than the regression models (#2, #3), it requires slightly more total energy to do so. This implies that a modicum of additional energy is expended per bout, and we speculate that the generation of stochasticity in the real fish’s algorithm (described below) is to blame.</p></sec><sec id="s2-6"><title>Pre-Bout to Post-Bout prey coordinate transformation reveals a canonical hunting strategy</title><p>We next wondered which of the numerous prey capture strategies observed in nature arises from the rules implemented in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>. For example, a strategy of immediately zeroing the angle of attack is used by many predators and is called ‘pure pursuit’. Other common strategies include ‘deviated pursuit’, where a constant angle of attack is maintained throughout the hunting sequence. We find that larval zebrafish use neither of the two, but rather reduce Prey Az and Prey Alt by a factor of 0.5 at each bout. In the first two panels of <xref ref-type="fig" rid="fig5">Figure 5A</xref> (blue and yellow), the post bout angle of attack is plotted against the pre bout angle, and a constant ratio of ~0.5, regardless of prey velocity direction, is apparent in the slopes of the regression lines. This strategy of reducing prey angle in both planes to a fixed proportion on each bout is schematically depicted in <xref ref-type="fig" rid="fig5">Figure 5B</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>. Notably, when prey are below the fish, the slope changes to 0.9, meaning that the fish is only getting 10% closer in altitude per bout. This poor performance for negative Prey Alt is consistent with our observation that hunt initiations are triggered almost exclusively by prey located above the fish. In fact, 92.4% of all bouts in our dataset occur when prey are above 0° Alt (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>A cannonical 'halving' strategy emerges.</title><p>(<bold>A</bold>) Regression plots showing relationships between pre-bout prey coordinates and post-bout prey coordinates. Dark colors, prey are moving toward the fish. Light colors, prey are moving away from the fish. 95% CI on azimuth transforms’ y-intercept includes 0°. Top right panel fit on all distances &lt; 6 mm (see <xref ref-type="fig" rid="fig3">Figure 3C</xref>). (<bold>B</bold>) Schematic showing recursive halving of the angle of attack during pursuit. (<bold>C</bold>) Regression slopes are constant across the hunt sequence; color coded to 5A. (<bold>D</bold>) Pseudocode describing the recursive prey capture algorithm that transforms according to 5A until it arrives at the strike zone. The combined (both velocity directions) distance transform is .<italic>84 * Pre-Bout Prey Dist</italic> -. <italic>0125 mm = Post Bout Prey Dist.</italic> The azimuth transform is .<italic>53 * Pre-Bout Prey Az = Post Bout Prey Az</italic>. The altitude transformation is .<italic>54 * Pre-Bout Prey Alt</italic> + <italic>8.34° = Post Bout Prey Alt</italic>. Implementing these equations recursively will terminate the algorithm at 18.1° Prey Alt (since .<italic>54 *</italic> 18.1° <italic>+ 8.34° =</italic> 18.1°) and 0<italic>°</italic> Prey Az (.<italic>53 *</italic> 0° = 0°), which aligns precisely with the strike zone described in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. See Appendix for full pseudocode of all sub-functions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Schematic showing that the fish will reduce the post-bout angle of attack to the same value regardless of whether prey is moving towards or away from the fish (see <xref ref-type="fig" rid="fig5">Figure 5A</xref>).</title><p>This strategy emerges from the position transforms in <xref ref-type="fig" rid="fig2">Figure 2</xref> and the modulation of rotation and displacement by prey velocity in <xref ref-type="fig" rid="fig3">Figure 3</xref> (Bout ΔYaw, pink wheel; Bout Az, blue wheel).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Regression fits between Pre-Bout and Post-Bout Prey Alt differ depending on whether prey altitude is positive or negative before the bout.</title><p>Most bouts in the dataset (&gt;92%) occur when prey are above the fish. This change when prey cross 0° Alt is accounted for in all algorithms (see Appendix).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Transformation by the <italic>initiation</italic> bout of Pre-Bout to Post-Bout Prey Az and Alt.</title><p>The initiation bout is a large angle turn that divides azimuth more than a pursuit bout; altitude is also significantly more reduced by the initiation bout. All regression models in simulations therefore use an independent regression fit to initiation bouts to start every simulated hunt.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig5-figsupp3-v2.tif"/></fig></fig-group><p>Next, we analyzed how much the radial distance to the prey object is reduced as a consequence of each pursuit bout. We find that on average fish become 16% closer to the prey per bout, or in other words, the Prey Distance is scaled by 0.84 at every iteration. This reduction, however, is significantly less for prey objects that move radially away from the fish (0.87) versus when prey move toward it (0.81; <xref ref-type="fig" rid="fig5">Figure 5A</xref>, light red vs dark red regression fits), which is consistent with the fact that the fish does not modulate Bout Distance based on radial prey velocity when prey are further than 2 mm away (see <xref ref-type="fig" rid="fig3">Figure 3C</xref>). However, once prey are maneuvered to within 2 mm, radial velocity is taken into consideration and the two fits converge on similar outcomes. Importantly, 69% of pursuit bouts occur when prey are 0–2 mm from the fish, indicating that Bout Distance is typically modulated by radial prey velocity, and that fish can most often achieve a preferred Post-Bout Prey Distance after a bout is completed.</p><p>To summarize, because fish account for prey velocity in all directions (<xref ref-type="fig" rid="fig3">Figure 3</xref>), they are on average capable of achieving a fixed proportional reduction of Prey Az, Alt, and Dist during pursuit bouts. These proportions are consistent from the beginning to the end of hunting sequences (<xref ref-type="fig" rid="fig5">Figure 5C</xref>); therefore, this data reflects a ‘deterministic recursive algorithm’ for prey capture: The fish recursively transforms current prey coordinates into more favorable prey coordinates by a fixed scale factor in all planes until the strike zone is attained (<xref ref-type="fig" rid="fig5">Figure 5D</xref>).</p></sec><sec id="s2-7"><title>Graded stochasticity in Pre-Bout to Post-Bout prey coordinates benefits hunting efficiency</title><p>Interestingly, we noticed a clear graded increase in variance of the post-bout coordinate in all spherical planes as pre-bout coordinates trended away from the strike zone (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, <xref ref-type="fig" rid="fig6">Figure 6A</xref> left panels, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). This suggested to us that the fish is implementing a stochastic recursive algorithm rather than the deterministic recursion described in <xref ref-type="fig" rid="fig5">Figure 5D</xref>.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Graded stochasticity in sensorimotor transformations improves hunting performance.</title><p>(<bold>A</bold>) KDE plots of post-bout variable distributions for pre-bout input coordinates described in the legend, color-coded to the KDEs (i.e. the blue KDE in Az is the distribution of all Post-Bout Prey Az given that Pre-Bout Prey Az is 5°). Real data is binned in 5° windows for angles and. 25 mm bins for distance. DPMM-generated post-bout variables are directly simulated from the model 5000 times, conditioned on the pre-bout value in the legend. (<bold>B</bold>) The median performance of the DPMMs embedded in a recursive loop (stochastic recursion algorithm; run 200 times per initial prey position) typically ties or outperforms the deterministic recursion model, which transforms with the same pre-bout to post-bout slopes as the DPMMs. Pink line is the performance of the real fish. Right panel: KDE plot of data from 6A, showing that the stochastic algorithm approaches the speed of the real fish. (<bold>C, D</bold>) A Graded Variance Algorithm where proportional noise is injected into each choice (equations below) is applied 500 times per initial distance (0.1 mm to 10 mm, .1 mm steps) or azimuth (10° to 200° in steps of 2°). Termination condition is a window from. 1 mm to 1 mm for distance, −10° to 10° for azimuth (see Appendix for full algorithm). Deterministic recursion algorithm (<xref ref-type="fig" rid="fig5">Figure 5D</xref>) is also run on each initial azimuth and distance with .<italic>53 * az</italic> for azimuth <italic>and</italic> .<italic>84 * dist</italic> - .<italic>0125 mm</italic> as the fixed transforms. Graded Variance uses these exact transforms as the mean while injecting graded noise: σ<sub>d</sub> = <italic>0</italic>.<italic>137 * dist + 0.034 mm</italic>; σ<sub>az</sub> = .<italic>36 * az + 7.62°</italic>, which were fit using linear regression on samples generated by our Bayesian model (6B). (<bold>C</bold>) shows examples of Graded Variance performance (‘stochastic’) vs. deterministic performance for an example start distance of 3.8 mm. (<bold>D</bold>) is a barplot comparing the deterministic performance for each input distance and azimuth to the median Graded Variance (‘stochastic’) performance where we directly injected noise. Average performance using noise injection typically ties or defeats deterministic choices by one bout.</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Generators for BayesDB simulations.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-51975-fig6-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Standard deviation is plotted for each individual fish per five degree bin of prey space, indicating that graded stochasticity is not simply observed in the pooled bout population, but at the level of single fish.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51975-fig6-figsupp1-v2.tif"/></fig></fig-group><p>In order to capture the stochasticity of pre-bout to post-bout prey transforms made by zebrafish during pursuit, we chose to use probabilistic generative models (Dirichlet Process Mixture Models: ‘DPMMs’). These models are Bayesian, non-parametric models which avoid the key problem in the statistical modeling field of having to arbitrarily specify the number of variables that best characterizes your data (<xref ref-type="bibr" rid="bib25">Gershman and Blei, 2012</xref>). They can be thought of as ‘probabilistic programs’ that accurately mimic fish choices given a particular pre-bout prey coordinate (<xref ref-type="bibr" rid="bib26">Goodman et al., 2008</xref>; <xref ref-type="bibr" rid="bib35">Mansinghka et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Cusumano-Towner et al., 2019</xref>). Using this framework, we sought to uncover whether a stochastically implemented version of the recursive hunting algorithm was beneficial to the fish. To that end, we pitted a stochastic algorithm defined by our Bayesian model against the deterministic algorithm (5D) and compared performances of both models to each other and to the performance of the real fish (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Remarkably, the stochastic model outperformed the deterministic model in terms of capture speed (# Bouts to Capture, <xref ref-type="fig" rid="fig6">Figure 6B</xref>; Wilcoxon signed rank = 7.5 * 10<sup>−17</sup>). This suggested that the fish’s strategy of graded stochastic transforms centered on a preferred post-bout value is actually beneficial versus accurately achieving a fixed, preferred post-bout value. Moreover, although everything except initial prey position has been abstracted away, the performance of the stochastic model approaches that of the real fish (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, right panel).</p><p>To confirm that graded stochasticity in our Bayesian model was responsible for its increased prey capture speed, we simply injected proportional noise into deterministic bout choices and asked whether we could speed up prey capture (<xref ref-type="fig" rid="fig6">Figure 6C,D</xref>). This was clearly the case (Wilcoxon Signed Rank: 1.87 * 10<sup>−9</sup> azimuth, 3.96 * 10<sup>−14</sup> distance); common scenarios that we observed in these simulations are illustrated in <xref ref-type="fig" rid="fig6">Figure 6C</xref>. The deterministic algorithm definitively achieves the strike zone in a fixed number of transforms (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, top), but injecting proportional noise can either directly improve on deterministic choices (‘Beneficial’ panel, 6C), start poorly but then recover and outpace deterministic choices (‘Recovery’, 6C), or perform detrimentally (‘Detrimental’, 6C). Nevertheless, the average performance when injecting proportional noise is typically equivalent or better by one bout (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). Given that successful hunts in our dataset had an interquartile range of 4 to 7 total bouts, improving by one bout constitutes an average 14–25% gain in capture speed. We therefore conclude that the fish’s graded stochasticity produces performance that is curiously beneficial to the fish while hunting prey.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In our study, we uncovered three basic rules that larval zebrafish implement while hunting their fast-moving prey: 1) prey position linearly governs the aspects of five degrees of freedom in which fish can rotate or translate through the water: rotation in yaw and pitch, as well as lateral, vertical and radial displacement; 2) prey velocity modulates all of these aspects of 3D motion and allows the fish to project prey position forward in time; and 3) prey coordinate transformation operates via graded variance based on prey proximity to the strike zone. The first two rules are interesting, since their 3D features and specific contingencies have not been examined in zebrafish, and it certainly was not clear that position prediction was already implemented at the larval stage. This general strategy of using velocity to increase prey capture efficiency, however, is implemented in many hunting organisms such as dragon flies and salamanders (e.g. <xref ref-type="bibr" rid="bib14">Borghuis and Leonardo, 2015</xref>), while superposition of position and motion information has been observed in <italic>Drosophila</italic> (<xref ref-type="bibr" rid="bib8">Bahl et al., 2013</xref>). The third rule, which describes the benefits of stochasticity in these hunting algorithms has, to our knowledge, not been described in any trajectory prediction assay to date.</p><p>As we show, the aforesaid rules work in tandem to generate excellent, energy efficient performance in prey capture (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig6">6</xref>), which is the most complex behavior that larval zebrafish perform and would appear to require elements of physical knowledge. We focus our discussion on how these rules apply to related studies on the neural mechanisms of prey capture, how examining prey capture at two levels of abstraction was beneficial to our study, and how the fish’s algorithms appear to be built around the inherent constraints of its own body.</p><sec id="s3-1"><title>From algorithms to neurobiology</title><p>With respect to the neural implementations of the algorithms we describe, the transformation of angular prey <italic>position</italic> into informative neural activity is encapsulated to a large extent by the abundant research related to retinotopic maps (e.g. <xref ref-type="bibr" rid="bib56">Sperry, 1963</xref>; <xref ref-type="bibr" rid="bib4">Apter, 1946</xref>; <xref ref-type="bibr" rid="bib45">Muto et al., 2013</xref>; but see <xref ref-type="bibr" rid="bib6">Avitan et al., 2016</xref>). The encoding of distance to an object has been well studied in mammalian visual neuroscience and is primarily focused on binocular disparity allowing stereoscopic comparison of each eye’s retinotopic map (<xref ref-type="bibr" rid="bib20">DeAngelis et al., 1998</xref>), with some studies focusing on monocular motion parallax (e.g. <xref ref-type="bibr" rid="bib46">Nadler et al., 2008</xref>). Neuronal encoding of object speed, however, requires more sophisticated circuitry and much less is known about its implementation during prey capture. Speed sensitive neurons that discriminate between ‘fast’ and ‘slow’ prey-like stimuli have been uncovered in the zebrafish optic tectum (<xref ref-type="bibr" rid="bib12">Bianco and Engert, 2015</xref>), while ‘small field’ tectal neurons that respond to velocity have been found in toads (<xref ref-type="bibr" rid="bib23">Ewert, 1987</xref>).</p><p>Unlike these and other reports studying velocity during prey capture (<xref ref-type="bibr" rid="bib58">Trivedi and Bollmann, 2013</xref>; <xref ref-type="bibr" rid="bib49">Patterson et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Monroy and Nishikawa, 2011</xref>), we specifically contend that velocity perception is used to point-estimate a future prey position, and that the fish conducts bouts to achieve this estimate, on average, by biasing their prey position-controlled movements. We show that the perception and projection of velocity is key to prey capture success, and that without it, the azimuth and altitude coordinates of prey after bouts are less likely to lie near the strike zone (<xref ref-type="fig" rid="fig4">Figure 4</xref>). This type of predictive use of velocity is reminiscent of elegant behavioral studies that have illustrated trajectory prediction in salamanders and dragonflies (<xref ref-type="bibr" rid="bib35">Mansinghka et al., 2015</xref>; <xref ref-type="bibr" rid="bib14">Borghuis and Leonardo, 2015</xref>). Quantitative descriptions of such complex algorithms are an absolute necessity for generating hypotheses about neural implementations (<xref ref-type="bibr" rid="bib37">Marr, 1982</xref>), and virtual prey capture setups for head fixed larvae provide promising inroads for testing our work at the neural level (<xref ref-type="bibr" rid="bib12">Bianco and Engert, 2015</xref>; <xref ref-type="bibr" rid="bib6">Avitan et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Trivedi and Bollmann, 2013</xref>). Relatedly, monkeys also predict future locations of virtual prey using Newtonian physical attributes in a very similar paradigm to that shown here. In the monkey brain, these attributes are all reflected by neural activity in the dorsal anterior cingulate, which has no known homolog in the zebrafish, nor in other simple animals that use predictive prey models (<xref ref-type="bibr" rid="bib61">Yoo et al., 2019</xref>). This suggests that in more primitive organisms the necessary computations are executed in earlier evolved brain areas which also might play an essential role in the primate.</p><p>The stochasticity that gives rise to the graded variance we describe can have several biological sources. It will be important to unravel whether the neural command signals arriving at muscles grow in variance with increased amplitude or if instead the muscles receive fixed input from the brain for a given prey condition, but themselves respond with graded noise. If the source of the variance is largely of a neuronal nature, then it would be interesting to study where exactly in the pathway from sensory areas to motor neurons such noise starts to appear (<xref ref-type="bibr" rid="bib57">Stern et al., 2017</xref>). Further elucidation of the fish’s probabilistic strategy should eventually integrate our findings into more general theories describing the utility of noisy biological behavior (see <xref ref-type="bibr" rid="bib29">Jensen, 2018</xref>; <xref ref-type="bibr" rid="bib60">Wiesenfeld and Moss, 1995</xref>, for review) and intentionally probabilistic circuit structures for solving computational problems (<xref ref-type="bibr" rid="bib34">Mansinghka et al., 2008</xref>).</p></sec><sec id="s3-2"><title>Strategic behavior arises from simple behavioral rules</title><p>The stochastic recursive algorithm in <xref ref-type="fig" rid="fig6">Figure 6</xref> describes the progression of pre-bout to post-bout prey coordinates without explicitly accounting for prey velocity or specifically executing fish movements. This transformation pattern, which reveals a preferred future prey position and thus a trajectory prediction ability, emerges from the execution of the position and velocity-based rules described in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>. Zebrafish prey capture in <xref ref-type="fig" rid="fig6">Figure 6C</xref> has, in fact, been reduced to a single input and a recursive series of stochastic divisions with a termination condition, which largely recapitulates the performance of the fish (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, DPMM). It is unlikely that we would have found such a straightforward description of proportionality and stochasticity at the lower level of abstraction (i.e. in the fish’s actual sensorimotor transformations), because pre-bout to post-bout prey transformation is a formulation of fish movements along five different axes acting simultaneously. Describing prey capture in this way allowed us to assess the goals of the fish on each bout (i.e. <xref ref-type="bibr" rid="bib37">Marr, 1982</xref>), revealed that the fish possess an implicit model of how objects move in the world, and may lead to descriptions about how fish are evaluating their own performance during prey capture.</p><p>With regard to the benefits of the zebrafish’s strategy, the proportional reduction of angle and distance saves energy at the expense of speed (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Ideal bout choice improved speed of prey capture in our modeling data (<xref ref-type="fig" rid="fig4">Figure 4</xref>, Model 6); but it did so at the price of spending almost twice as much energy per paramecium captured. This suggests that the increase in feeding rate that the ideal model would afford seems not to be essential for providing an adaptive advantage. Interestingly, the inherent stochasticity in the algorithm significantly improves the speed of capture (<xref ref-type="fig" rid="fig6">Figure 6</xref>) while adding only a modicum of energy expenditure (<xref ref-type="fig" rid="fig4">Figure 4</xref>: comparison of Real Fish Model 1 vs. deterministic regression Model 3). This suggests that the fish has evolved a proper balance between energy expenditure and speed of capture. On the whole, the evolution of an efficient algorithm for prey capture in the zebrafish is in agreement with the theme of efficient behaviors arising from simple rules. However, quantifying the fish’s overall energy consumption in a context where they often quit is difficult: energy consumption should therefore be revisited, incorporating work on the decay of the prey capture algorithm in the last three pursuit bouts of aborted sequences (e.g. <xref ref-type="bibr" rid="bib28">Henriques et al., 2019</xref>; <xref ref-type="bibr" rid="bib30">Johnson et al., 2019</xref>).</p><p>With respect to hunting schemes, predatory animals have evolved a variety of strategies to optimize pursuit and intercept prey. Tiger beetles, for example, engage in pure pursuit where the angle of attack is kept constant at zero degrees (<xref ref-type="bibr" rid="bib27">Haselsteiner et al., 2014</xref>). Salamanders, on the other hand, lead the trajectory of their prey (<xref ref-type="bibr" rid="bib14">Borghuis and Leonardo, 2015</xref>). Dragonflies and falcons often utilize a strategy of maintaining a constant line of sight which affords the benefit of motion camouflage (<xref ref-type="bibr" rid="bib31">Kane and Zamani, 2014</xref>, <xref ref-type="bibr" rid="bib43">Mizutani et al., 2003</xref>; but see <xref ref-type="bibr" rid="bib42">Mischiati et al., 2015</xref>). Relevantly, dragonflies also implement an implicit predictive model of their prey as well as a model of the effects of their own body movements on prey drift, which foreshadowed the possible use of predictive models across animals with small brains (<xref ref-type="bibr" rid="bib42">Mischiati et al., 2015</xref>). Larval zebrafish have been assumed to engage in pure pursuit, the simplest and most heuristic of these strategies. However, we find that the strategy used by these animals is more complex and reflects an implicit predictive model of where prey will be at a specified time in the future. Furthermore, the quantal nature of the zebrafish’s swim bouts allowed us to uncover that the angle of attack is recursively and stochastically reduced by an average proportion until the prey enters a terminal strike zone.</p></sec><sec id="s3-3"><title>Embodied physical knowledge</title><p>One branch of artificial intelligence research advocates against a central processing unit where relevant computation occurs in favor of a distributed network of sensorimotor transformations, tuned to the capabilities of the body, that can accomplish the goals of the system (<xref ref-type="bibr" rid="bib16">Brooks, 1991</xref>). Our study reinforces these sentiments and suggests that approaching the study of the brain without considering its embodiment may be precarious.</p><p>Specifically, interesting relationships in the data we provide suggest that the fish’s algorithms are built around the capabilities and constraints of its body. First, the amount of ‘fixed noise’ in the fish’s azimuth transformations is 7.64°. This is the standard deviation of Post-Bout Prey Az given a Pre-Bout Prey Az coordinate of 0°, the minimum of graded stochasticity observed in <xref ref-type="fig" rid="fig6">Figure 6A</xref>. The standard deviation of the strike zone itself is 7.2° in azimuth. We contend that the similarity between these two numbers suggests that the fish’s strike zone is constructed to deal with noise that the fish cannot overcome in its motor program. If the prey is at, for example, 5° azimuth, performing another bout to get to the very center of the strike zone (~0° Az) would in many cases worsen the Post-Bout Prey Az coordinate due to fixed noise. Perfection is the enemy of good in this case. In this sense, the command to end pursuit and issue a strike is triggered by a visual releasing stimulus that evolved due to the fish’s own bodily constraints. This is akin to the idea of embodied cognition (<xref ref-type="bibr" rid="bib38">Maturana and Varela, 1987</xref>). Further evidence for embodied knowledge comes from the bias in the fish’s responses to prey altitude. The fish’s algorithm for transforming prey altitude biases the prey to ~18° above the fish, which aligns almost perfectly with the mean altitude coordinate at which they strike (17.4°, <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig5">5</xref>). The mechanics of the fish jaw necessitate this: to open its jaw widely for paramecium entrance into the mouth, the fish <italic>must</italic> tilt its head up due to torsional constraints (<xref ref-type="bibr" rid="bib40">Mearns et al., 2019</xref>). Therefore, the fish’s entire sensation of prey altitude and its method of keeping the prey above it by biasing its bouts downward emerge from the way its jaw co-operates with the rest of its head. Also of note is that graded variance is minimal for altitude transformations at Pre-Bout Alt = 20° (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), whereas the azimuth minimum is at 0°; this is also likely a function of its jaw features, which allows the least motor noise when the prey are located in the ideal strike position.</p><p>Finally, it is tempting to speculate that, in addition to possessing an implicit model of how objects move, the zebrafish is also equipped with a second forward model that predicts how its own body movements should give rise to expected sensory input (<xref ref-type="fig" rid="fig5">Figure 5</xref>). One way to determine the existence of this model is to test whether fish can adjust the gain of their movements in settings where they do not consistently achieve their preferred post-bout outcomes (i.e. <xref ref-type="bibr" rid="bib1">Ahrens et al., 2012</xref>).</p><p>All things considered, the implicit predictive model of 3D prey motion shown in this study is: 1) embodied by the fish’s stochastic recursive algorithm 2) shaped by the constraints and capabilities of the fish and 3) formulated by the interaction of three simple rules. These rules transform position of prey into fish movements, bias the vigor of fish movements based on prey velocity, and inject proportional noise into each sensorimotor transformation. Importantly, these more nuanced features of the fish’s hunting algorithm would not have been revealed without examining prey capture in its more naturalistic 3D setting, which we believe has laid a groundwork for future studies examining the ontogeny, plasticity, and neural implementation of prey capture algorithms and physical knowledge in general (e.g. <xref ref-type="bibr" rid="bib7">Avitan et al., 2017</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th>Reagent type <break/>(species) or resource</th><th>Designation</th><th>Source or reference</th><th>Identifiers</th><th>Additional <break/>information</th></tr></thead><tbody><tr><td>Software, algorithm</td><td>BayesDB</td><td>arxiv</td><td>arxiv:1512.05006</td><td><ext-link ext-link-type="uri" xlink:href="http://probcomp.csail.mit.edu/software/bayesdb/">http://probcomp.csail.mit.edu/software/bayesdb/</ext-link></td></tr><tr><td>Strain, strain background (<italic>Danio rerio</italic>)</td><td><italic>WIK</italic></td><td><italic>ZFIN</italic></td><td>ZFIN_ZDB-GENO-010531–2</td><td/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Animals</title><p>Experiments were conducted according to the guidelines of the National Institutes of Health and were approved by the Standing Committee on the Use of Animals in Research of Harvard University. All experiments were performed on dpf 7–8 larval zebrafish of the WIK strain. Fish were raised in an automated system where they were delivered paramecia twice a day from dpf4 onward. Importantly, fish in the system experienced a full range of paramecium movement due to the height of the water in their home tank (~6’). Fish were fasted for 4–6 hr before experiments.</p></sec><sec id="s4-2"><title>Behavioral setup</title><p>After the fasting period, fish were added with ~100 paramecia (<italic>Paramecia Multimicronucleatum</italic>) in the dark to a 2 cm x 2 cm x 2 cm cube tank made of clear acrylic capped with coverglass. 3.56 megapixel images were simultaneously acquired from the top and side of the tank using two Point Grey Grasshopper 3 NIR cameras; the cameras were synchronized by a TTL pulse triggered by a Pyboard microcontroller at 62.5 Hz. Custom acquisition code was written using C# with the EmguCV library for high-speed video-writing.</p><p>Camera positions were calibrated by using known reference points (i.e. body features of the fish) for the shared plane of the cameras. Identification of known object positions in both planes was extremely accurate (~200 micron mean error), calculated by average position of the fish’s eye center in both cameras over all experiments. This allowed accurate reconstruction of 3D prey and fish features (see below).</p><p>For the duration of the experiment, fish were illuminated with an infrared LED array, and after 2 min in the dark, fish were exposed to a uniform white LED which commenced prey capture. Fish hunted in the white light illuminated condition for 8 min before the experiment was terminated. Fish that did not consume more than one paramecium over the 8 min experiment were discarded for analysis (46/53 fish passed this criteria).</p></sec><sec id="s4-3"><title>Behavioral analysis</title><p>Custom Python software using the OpenCV library was written to extract the body features of the fish (eye convergence angle, tail curvature, yaw, 3D position) and the position of each paramecium in the XY and XZ planes. Pitch was calculated by taking the 2D vertical angle in the side camera and fitting a cone to the fish using the yaw angle from the top camera. The tail angle of the fish was fed to a bout detection algorithm that returned frames where swim bouts were initiated and terminated using tail angle variance and bout velocity.</p><p>Hunt sequences were identified by spectral clustering (scikit-learn) the continuous eye angle of both eyes over each swim bout for all fish into five clusters. One of the five clusters showed clear convergence of both eyes at bout initiation (‘hunt initiation cluster’), while a second cluster showed clear deconvergence (‘hunt termination cluster’, see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). Custom annotation software cycled through frames marked as hunt initiations and allowed the user to terminate hunt sequences on frames where the fish consumed its prey or clearly quit hunting. Most hunt terminations coincided with the hunt termination cluster.</p><p>Upon identifying the frame boundaries of hunt sequences, a 3D prey trajectory reconstruction algorithm was applied that matched prey discovered in the two separate cameras. This is a nontrivial task because the two cameras only overlap in one axis; any two prey items that share similar values in the overlapping plane must be separated using dynamics in time. We therefore matched prey trajectories from the top and side using correlation of velocity profiles and post-hoc 3D position similarity. In this way, each prey item is assigned an ID for a given hunt sequence; the user is required to specify the prey ID that is struck at on strikes, and the ‘best guess’ prey ID that fish pursue during abort sequences. All prey trajectories for each hunt sequence are mapped to a spherical coordinate system based on unit vectors fit from the fish’s XYZ position, pitch, and yaw, with its origin at the fish’s oral cavity. Manual quality control for mistakes in fish characterization or prey reconstruction was applied rarely by eliminating hunt sequences from analysis showing clear mistakes from the computer vision algorithms.</p></sec><sec id="s4-4"><title>Regression fitting and modeling environment</title><p>All regression models were fit with Generalized Linear Model tools using the Python StatsModels package. When regression fits are shown in figures, we used the Seaborn library in Python, and 95% CIs for fits are represented as light shaded regions behind the regression line.</p><p>Ideal choice models used in the Virtual Prey Capture Simulation Environment cycled through bouts combined in a ‘bout pool’ from all 46 zebrafish that were performed during sequences that ended in a strike. Each bout during Choice was pre-filtered for bout duration before scoring for prey closeness to the strike zone; ideal bouts could not be shorter than the bout chosen by the fish at that juncture, and could not extend past the time of the next bout chosen by the fish.</p><p>Prey trajectories used in the modeling environment were selected from real capture sequences where the fish struck at the prey and the prey was swimming (&gt;330 microns per second; 89% of all hunted prey records pass velocity criteria, which through inspection distinguishes swimming from floating prey). All virtual hunt reconstructions were initiated with the virtual fish and prey items in the exact same positions and orientations as when the real sequences were initiated. Energy consumption in the virtual environment was calculated under the assumption that the head to center of mass distance for a larval zebrafish is .53 mm (as measured in ImageJ) and the mass of the fish is 1 mg (<xref ref-type="bibr" rid="bib5">Avella et al., 2012</xref>). Rotational energy of yaw and pitch and kinetic energy of center of mass displacement were added for each bout. Strike zone achievement was defined by the 95% CI on the angular position of a prey item during successful strikes (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), likewise conditioned on the radial distance being less than two standard deviations from the mean.</p><p>Both regression and ideal models choose initiation bouts and pursuit bouts independently. The first bout of regression models is fit on only initiation bouts, and the first bout of choice models is chosen from the pool of all initiation plus pursuit bouts. This is largely because initiation bout transformations are significantly different from pursuits (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>).</p></sec><sec id="s4-5"><title>Abstracted models and Bayesian nonparametric methods</title><p>Pseudocode describing the transformations of pre-bout to post-bout paramecium locations (Appendix, <xref ref-type="fig" rid="fig5">Figure 5D</xref>) were written according to the method of <xref ref-type="bibr" rid="bib51">Russell and Norvig (2010)</xref>. The Appendix contains all pseudocode required to implement the deterministic and stochastic choices made in <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>.</p><p>We inferred mixture models in <xref ref-type="fig" rid="fig6">Figure 6</xref> from empirical data (Pre-Bout Prey Az, Alt, Dist and Post-Bout Prey Az Alt, Dist for all pursuit bouts in the dataset) using a non-parametric Bayesian prior called a Dirichlet Process Mixture Model (DPMM) (<xref ref-type="bibr" rid="bib50">Rasmussen, 1999</xref>; <xref ref-type="bibr" rid="bib3">Antoniak, 1974</xref>; <xref ref-type="bibr" rid="bib36">Mansinghka et al., 2016</xref>). In order to accurately reflect realistic, stochastic pre-bout to post-bout transformations, our model choice had to be multivariate, heteroskedastic, and include multi-modal probability distributions over pursuit choices. While our linear parametric models (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) captured the average transformation made by the fish in multiple velocity conditions, analytically tractable model families are unable to qualitatively capture the above phenomena. DPMMs can approximate a broad class of multivariate distributions without requiring a priori specification of the number of components in the mixture model. The mixture models generated via a DPMM prior can be converted to probabilistic programs for inference to generate the kinds of conditional simulations used in <xref ref-type="fig" rid="fig6">Figure 6</xref> (<xref ref-type="bibr" rid="bib53">Saad et al., 2019</xref>). In this representation, each pre-bout to post-bout prey transformation made by a zebrafish can be thought of as arising from a program that first chooses a prototypical transform (corresponding to a component in the mixture), and then generates a random transform from a distribution over transforms associated with the prototype. We used the BayesDB software library (<xref ref-type="bibr" rid="bib35">Mansinghka et al., 2015</xref>; <xref ref-type="bibr" rid="bib54">Saad and Mansinghka, 2016</xref>) to implement the computations needed to build these models and generate conditional simulations. BayesDB simulations were embedded inside a recursive loop that take an initial prey position as input and output the number of bouts until striking (see PREYCAPTURE algorithm in <xref ref-type="fig" rid="fig5">Figure 5D</xref> with STOCHASTIC_TRANSFORM substitution). When comparing deterministic and stochastic models in <xref ref-type="fig" rid="fig6">Figure 6</xref>, the initiation bout for both models was equal and deterministic; only pursuit bouts differed between deterministic and DPMM models. The deterministic model transformed using the average slopes of 10,000 samples generated from the DPMMs to isolate stochastic effects. For validation of noise injection in <xref ref-type="fig" rid="fig6">Figure 6C and D</xref>, the GRADED_VARIANCE algorithm in the Appendix was used.</p></sec><sec id="s4-6"><title>Data and software availability</title><p>All software related to behavioral analysis, modeling, and virtual prey capture simulation is freely available at <ext-link ext-link-type="uri" xlink:href="http://www.github.com/larrylegend33/PreycapMaster">www.github.com/larrylegend33/PreycapMaster</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/PreycapMaster">https://github.com/elifesciences-publications/PreycapMaster</ext-link>; <xref ref-type="bibr" rid="bib13">Bolton, 2019</xref>). The software is licensed under a GNU General Public License 3.0. Source data for analysis and simulations is enclosed as ‘Source Data’ in relevant figures. Source Data for <xref ref-type="fig" rid="fig2">Figure 2</xref> contains all pursuit bouts analyzed in the dataset; it was used to construct <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref>, <xref ref-type="fig" rid="fig5">5</xref> and <xref ref-type="fig" rid="fig6">6A</xref>, and is accompanied by instructions for running queries. Source Data for <xref ref-type="fig" rid="fig6">Figure 6</xref> contains the generators for simulating from the DPMMs in <xref ref-type="fig" rid="fig6">Figure 6</xref>. Using the code at <ext-link ext-link-type="uri" xlink:href="http://www.github.com/larrylegend33/PreycapMaster">www.github.com/larrylegend33/PreycapMaster</ext-link> and the generators in Source Data – <xref ref-type="fig" rid="fig6">Figure 6</xref> requires obtaining the BayesDB software package, which is freely available at <ext-link ext-link-type="uri" xlink:href="http://probcomp.csail.mit.edu/software/bayesdb/">http://probcomp.csail.mit.edu/software/bayesdb/</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The authors thank Martha Constantine-Paton, Mehmet Fatih Yanik, Misha Ahrens, Rory Kirchner, Rob Johnson, Lilach Avitan, Roy Harpaz, Kirsten Bolton, and Elizabeth Spelke for conversations on the project. Yarden Katz, Olivia McGinnis, and Hanna Zwaka provided helpful advice on the manuscript. Armin Bahl and Kristian Herrera provided advice and assistance with 3D rendering. This work was funded by a U19 grant from the National Institutes of Health.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Methodology</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Methodology</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Methodology</p></fn><fn fn-type="con" id="con4"><p>Software, Methodology</p></fn><fn fn-type="con" id="con5"><p>Software</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Software, Supervision</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Supervision</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Supervision, Funding acquisition, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: Experiments were conducted according to the guidelines of the National Institutes of Health and were approved by the Standing Committee on the Use of Animals in Research of Harvard University. Animals were handled according IACUC protocol #2729.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-51975-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All software related to behavioral analysis, modeling, and virtual prey capture simulation is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/larrylegend33/PreycapMaster">https://github.com/larrylegend33/PreycapMaster</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/PreycapMaster">https://github.com/elifesciences-publications/PreycapMaster</ext-link>). The software is licensed under a GNU General Public License 3.0. Source data for analysis and simulations is enclosed as &quot;Source Data&quot; in relevant figures. Source Data for Figure 2 contains all pursuit bouts analyzed in the dataset; it was used to construct Figures 2, 3, 5, and 6A, and is accompanied by instructions for running queries. Source Data for Figure 6 contains the generators for simulating from the DPMMs in Figure 6. Using the code at <ext-link ext-link-type="uri" xlink:href="https://github.com/larrylegend33/PreycapMaster">https://github.com/larrylegend33/PreycapMaster</ext-link> and the generators in Source Data - Figure 6 requires obtaining the BayesDB software package, which is freely available at <ext-link ext-link-type="uri" xlink:href="http://probcomp.csail.mit.edu/software/bayesdb/">http://probcomp.csail.mit.edu/software/bayesdb/</ext-link>.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname> <given-names>MB</given-names></name><name><surname>Li</surname> <given-names>JM</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Robson</surname> <given-names>DN</given-names></name><name><surname>Schier</surname> <given-names>AF</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name><name><surname>Portugues</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Brain-wide neuronal dynamics during motor adaptation in zebrafish</article-title><source>Nature</source><volume>485</volume><fpage>471</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1038/nature11057</pub-id><pub-id pub-id-type="pmid">22622571</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alem</surname> <given-names>S</given-names></name><name><surname>Perry</surname> <given-names>CJ</given-names></name><name><surname>Zhu</surname> <given-names>X</given-names></name><name><surname>Loukola</surname> <given-names>OJ</given-names></name><name><surname>Ingraham</surname> <given-names>T</given-names></name><name><surname>Søvik</surname> <given-names>E</given-names></name><name><surname>Chittka</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Associative mechanisms allow for social learning and cultural transmission of string pulling in an insect</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002564</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002564</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antoniak</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Mixtures of dirichlet processes with applications to bayesian nonparametric problems</article-title><source>The Annals of Statistics</source><volume>2</volume><fpage>1152</fpage><lpage>1174</lpage><pub-id pub-id-type="doi">10.1214/aos/1176342871</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Apter</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="1946">1946</year><article-title>Eye movements following strychninization of the superior colliculus of cats</article-title><source>Journal of Neurophysiology</source><volume>9</volume><fpage>73</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1152/jn.1946.9.2.73</pub-id><pub-id pub-id-type="pmid">21019913</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avella</surname> <given-names>MA</given-names></name><name><surname>Place</surname> <given-names>A</given-names></name><name><surname>Du</surname> <given-names>SJ</given-names></name><name><surname>Williams</surname> <given-names>E</given-names></name><name><surname>Silvi</surname> <given-names>S</given-names></name><name><surname>Zohar</surname> <given-names>Y</given-names></name><name><surname>Carnevali</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Lactobacillus rhamnosus accelerates zebrafish backbone calcification and gonadal differentiation through effects on the GnRH and IGF systems</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e45572</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0045572</pub-id><pub-id pub-id-type="pmid">23029107</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avitan</surname> <given-names>L</given-names></name><name><surname>Pujic</surname> <given-names>Z</given-names></name><name><surname>Hughes</surname> <given-names>NJ</given-names></name><name><surname>Scott</surname> <given-names>EK</given-names></name><name><surname>Goodhill</surname> <given-names>GJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Limitations of neural map topography for decoding spatial information</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>5385</fpage><lpage>5396</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0385-16.2016</pub-id><pub-id pub-id-type="pmid">27170134</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avitan</surname> <given-names>L</given-names></name><name><surname>Pujic</surname> <given-names>Z</given-names></name><name><surname>Mölter</surname> <given-names>J</given-names></name><name><surname>Van De Poll</surname> <given-names>M</given-names></name><name><surname>Sun</surname> <given-names>B</given-names></name><name><surname>Teng</surname> <given-names>H</given-names></name><name><surname>Amor</surname> <given-names>R</given-names></name><name><surname>Scott</surname> <given-names>EK</given-names></name><name><surname>Goodhill</surname> <given-names>GJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spontaneous activity in the zebrafish tectum reorganizes over development and is influenced by visual experience</article-title><source>Current Biology</source><volume>27</volume><fpage>2407</fpage><lpage>2419</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.06.056</pub-id><pub-id pub-id-type="pmid">28781054</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bahl</surname> <given-names>A</given-names></name><name><surname>Ammer</surname> <given-names>G</given-names></name><name><surname>Schilling</surname> <given-names>T</given-names></name><name><surname>Borst</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Object tracking in motion-blind flies</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>730</fpage><lpage>738</lpage><pub-id pub-id-type="doi">10.1038/nn.3386</pub-id><pub-id pub-id-type="pmid">23624513</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baillargeon</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Young infants' reasoning about the physical and spatial properties of a hidden object</article-title><source>Cognitive Development</source><volume>2</volume><fpage>179</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/S0885-2014(87)90043-8</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battaglia</surname> <given-names>PW</given-names></name><name><surname>Hamrick</surname> <given-names>JB</given-names></name><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Simulation as an engine of physical scene understanding</article-title><source>PNAS</source><volume>110</volume><fpage>18327</fpage><lpage>18332</lpage><pub-id pub-id-type="doi">10.1073/pnas.1306572110</pub-id><pub-id pub-id-type="pmid">24145417</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname> <given-names>IH</given-names></name><name><surname>Kampff</surname> <given-names>AR</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prey capture behavior evoked by simple visual stimuli in larval zebrafish</article-title><source>Frontiers in Systems Neuroscience</source><volume>5</volume><pub-id pub-id-type="doi">10.3389/fnsys.2011.00101</pub-id><pub-id pub-id-type="pmid">22203793</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname> <given-names>IH</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visuomotor transformations underlying hunting behavior in zebrafish</article-title><source>Current Biology</source><volume>25</volume><fpage>831</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.01.042</pub-id><pub-id pub-id-type="pmid">25754638</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bolton</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Prey Capture Analysis and Modeling</data-title><version designator="12e2efd">12e2efd</version><publisher-name>Github</publisher-name><ext-link ext-link-type="uri" xlink:href="https://github.com/larrylegend33/PreycapMaster">https://github.com/larrylegend33/PreycapMaster</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borghuis</surname> <given-names>BG</given-names></name><name><surname>Leonardo</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The role of motion extrapolation in amphibian prey capture</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>15430</fpage><lpage>15441</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3189-15.2015</pub-id><pub-id pub-id-type="pmid">26586829</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Braitenberg</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="1986">1986</year><source> Vehicles:experiments in Synthetic Psychology</source><publisher-loc>London</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brooks</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Intelligence without representation</article-title><source>Artificial Intelligence</source><volume>47</volume><fpage>139</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/0004-3702(91)90053-M</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Budick</surname> <given-names>SA</given-names></name><name><surname>O'Malley</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Locomotor repertoire of the larval zebrafish: swimming, turning and prey capture</article-title><source>The Journal of Experimental Biology</source><volume>203</volume><fpage>2565</fpage><lpage>2579</lpage><pub-id pub-id-type="pmid">10934000</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Couzin</surname> <given-names>ID</given-names></name><name><surname>Krause</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Self-Organization and collective behavior in vertebrates</article-title><source>Advances in the Study of Behavior</source><volume>32</volume><fpage>1</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/s0065-3454(03)01001-5</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cusumano-Towner</surname> <given-names>MF</given-names></name><name><surname>Saad</surname> <given-names>FA</given-names></name><name><surname>Lew</surname> <given-names>AK</given-names></name><name><surname>Mansinghka</surname> <given-names>VK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Gen: a general-purpose probabilistic programming system with programmable inference</article-title><conf-name>Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation</conf-name><fpage>221</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1145/3314221.3314642</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Cumming</surname> <given-names>BG</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical area MT and the perception of stereoscopic depth</article-title><source>Nature</source><volume>394</volume><fpage>677</fpage><lpage>680</lpage><pub-id pub-id-type="doi">10.1038/29299</pub-id><pub-id pub-id-type="pmid">9716130</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Douglass</surname> <given-names>JK</given-names></name><name><surname>Wilkens</surname> <given-names>L</given-names></name><name><surname>Pantazelou</surname> <given-names>E</given-names></name><name><surname>Moss</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Noise enhancement of information transfer in crayfish mechanoreceptors by stochastic resonance</article-title><source>Nature</source><volume>365</volume><fpage>337</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1038/365337a0</pub-id><pub-id pub-id-type="pmid">8377824</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname> <given-names>TW</given-names></name><name><surname>Mu</surname> <given-names>Y</given-names></name><name><surname>Narayan</surname> <given-names>S</given-names></name><name><surname>Randlett</surname> <given-names>O</given-names></name><name><surname>Naumann</surname> <given-names>EA</given-names></name><name><surname>Yang</surname> <given-names>CT</given-names></name><name><surname>Schier</surname> <given-names>AF</given-names></name><name><surname>Freeman</surname> <given-names>J</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name><name><surname>Ahrens</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain-wide mapping of neural activity controlling zebrafish exploratory locomotion</article-title><source>eLife</source><volume>5</volume><elocation-id>e12741</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12741</pub-id><pub-id pub-id-type="pmid">27003593</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ewert</surname> <given-names>J-P</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Neuroethology of releasing mechanisms: prey-catching in toads</article-title><source>Behavioral and Brain Sciences</source><volume>10</volume><fpage>337</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1017/S0140525X00023128</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gahtan</surname> <given-names>E</given-names></name><name><surname>Tanger</surname> <given-names>P</given-names></name><name><surname>Baier</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Visual prey capture in larval zebrafish is controlled by identified reticulospinal neurons downstream of the tectum</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>9294</fpage><lpage>9303</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2678-05.2005</pub-id><pub-id pub-id-type="pmid">16207889</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Blei</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A tutorial on bayesian nonparametric models</article-title><source>Journal of Mathematical Psychology</source><volume>56</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2011.08.004</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goodman</surname> <given-names>N</given-names></name><name><surname>Mansinghka</surname> <given-names>VK</given-names></name><name><surname>Roy</surname> <given-names>D</given-names></name><name><surname>Bonawitz</surname> <given-names>K</given-names></name><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Church: a universal language for generative models</article-title><conf-name>Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence</conf-name><fpage>220</fpage><lpage>229</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haselsteiner</surname> <given-names>AF</given-names></name><name><surname>Gilbert</surname> <given-names>C</given-names></name><name><surname>Wang</surname> <given-names>ZJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Tiger beetles pursue prey using a proportional control law with a delay of one half-stride</article-title><source>Journal of the Royal Society Interface</source><volume>11</volume><elocation-id>20140531</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2014.0531</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henriques</surname> <given-names>PM</given-names></name><name><surname>Rahman</surname> <given-names>N</given-names></name><name><surname>Jackson</surname> <given-names>SE</given-names></name><name><surname>Bianco</surname> <given-names>IH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Nucleus isthmi is required to sustain target pursuit during visually guided Prey-Catching</article-title><source>Current Biology</source><volume>29</volume><fpage>1771</fpage><lpage>1786</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.04.064</pub-id><pub-id pub-id-type="pmid">31104935</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jensen</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>Behavioral Stochasticity</chapter-title><source>Encyclopedia of Animal Cognition and Behavior</source><publisher-name>Springer</publisher-name><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-47829-6_1520-1</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>RE</given-names></name><name><surname>Linderman</surname> <given-names>S</given-names></name><name><surname>Panier</surname> <given-names>T</given-names></name><name><surname>Wee</surname> <given-names>CL</given-names></name><name><surname>Song</surname> <given-names>E</given-names></name><name><surname>Herrera</surname> <given-names>KJ</given-names></name><name><surname>Miller</surname> <given-names>A</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Probabilistic models of larval zebrafish behavior: structure on many scales</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/672246</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kane</surname> <given-names>SA</given-names></name><name><surname>Zamani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Falcons pursue prey using visual motion cues: new perspectives from animal-borne cameras</article-title><source>Journal of Experimental Biology</source><volume>217</volume><fpage>225</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1242/jeb.092403</pub-id><pub-id pub-id-type="pmid">24431144</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname> <given-names>KP</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Bayesian integration in sensorimotor learning</article-title><source>Nature</source><volume>427</volume><fpage>244</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1038/nature02169</pub-id><pub-id pub-id-type="pmid">14724638</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lake</surname> <given-names>BM</given-names></name><name><surname>Ullman</surname> <given-names>TD</given-names></name><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Building machines that learn and think like people</article-title><source>Behavioral and Brain Sciences</source><volume>40</volume><pub-id pub-id-type="doi">10.1017/S0140525X16001837</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Mansinghka</surname> <given-names>V</given-names></name><name><surname>Jonas</surname> <given-names>E</given-names></name><name><surname>Tenenbaum</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>Stochastic Digital Circuits for Probabilistic Inference (Technical Report)</source><publisher-name>Massachussets Institute of Technology</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mansinghka</surname> <given-names>V</given-names></name><name><surname>Shafto</surname> <given-names>P</given-names></name><name><surname>Jonas</surname> <given-names>E</given-names></name><name><surname>Petschulat</surname> <given-names>C</given-names></name><name><surname>Gasner</surname> <given-names>M</given-names></name><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>CrossCat: a fully bayesian nonparametric method for analyzing heterogeneous, high dimensional data</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1512.01272">https://arxiv.org/abs/1512.01272</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mansinghka</surname> <given-names>V</given-names></name><name><surname>Shafto</surname> <given-names>P</given-names></name><name><surname>Jonas</surname> <given-names>E</given-names></name><name><surname>Petschulat</surname> <given-names>C</given-names></name><name><surname>Gasner</surname> <given-names>M</given-names></name><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>CrossCat: a fully bayesian nonparametric method for analyzing heterogeneous, high dimensional data</article-title><source>Journal of Machine Learning Research: JMLR</source><volume>17</volume><fpage> 4760</fpage><lpage> 4808</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Vision: A Computational Investigation Into the Human Representation and Processing of Visual Information</source><publisher-name>Henry Holt and Co</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/9780262514620.001.0001</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Maturana</surname> <given-names>HR</given-names></name><name><surname>Varela</surname> <given-names>FJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><source>The Tree of Knowledge: The Biological Roots of Human Understanding</source><publisher-name>Shambhala Publications</publisher-name></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCulloch</surname> <given-names>WS</given-names></name><name><surname>Pitts</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1943">1943</year><article-title>A logical calculus of the ideas immanent in nervous activity</article-title><source>The Bulletin of Mathematical Biophysics</source><volume>5</volume><fpage>115</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1007/BF02478259</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mearns</surname> <given-names>DS</given-names></name><name><surname>Semmelhack</surname> <given-names>JL</given-names></name><name><surname>Donovan</surname> <given-names>JC</given-names></name><name><surname>Baier</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deconstructing hunting behavior reveals a tightly coupled stimulus-response loop</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/656959</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Minsky</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1988">1988</year><source>Society of Mind</source><publisher-name>Simon and Schuster</publisher-name></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mischiati</surname> <given-names>M</given-names></name><name><surname>Lin</surname> <given-names>HT</given-names></name><name><surname>Herold</surname> <given-names>P</given-names></name><name><surname>Imler</surname> <given-names>E</given-names></name><name><surname>Olberg</surname> <given-names>R</given-names></name><name><surname>Leonardo</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Internal models direct dragonfly interception steering</article-title><source>Nature</source><volume>517</volume><fpage>333</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1038/nature14045</pub-id><pub-id pub-id-type="pmid">25487153</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mizutani</surname> <given-names>A</given-names></name><name><surname>Chahl</surname> <given-names>JS</given-names></name><name><surname>Srinivasan</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Insect behaviour: motion camouflage in dragonflies</article-title><source>Nature</source><volume>423</volume><pub-id pub-id-type="doi">10.1038/423604a</pub-id><pub-id pub-id-type="pmid">12789327</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monroy</surname> <given-names>JA</given-names></name><name><surname>Nishikawa</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prey capture in frogs: alternative strategies, biomechanical trade-offs, and hierarchical decision making</article-title><source>Journal of Experimental Zoology Part A: Ecological Genetics and Physiology</source><volume>315A</volume><fpage>61</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1002/jez.601</pub-id><pub-id pub-id-type="pmid">20309849</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muto</surname> <given-names>A</given-names></name><name><surname>Ohkura</surname> <given-names>M</given-names></name><name><surname>Abe</surname> <given-names>G</given-names></name><name><surname>Nakai</surname> <given-names>J</given-names></name><name><surname>Kawakami</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Real-time visualization of neuronal activity during perception</article-title><source>Current Biology</source><volume>23</volume><fpage>307</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.12.040</pub-id><pub-id pub-id-type="pmid">23375894</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadler</surname> <given-names>JW</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A neural representation of depth from motion parallax in macaque visual cortex</article-title><source>Nature</source><volume>452</volume><fpage>642</fpage><lpage>645</lpage><pub-id pub-id-type="doi">10.1038/nature06814</pub-id><pub-id pub-id-type="pmid">18344979</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naumann</surname> <given-names>EA</given-names></name><name><surname>Fitzgerald</surname> <given-names>JE</given-names></name><name><surname>Dunn</surname> <given-names>TW</given-names></name><name><surname>Rihel</surname> <given-names>J</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>From Whole-Brain data to functional circuit models: the zebrafish optomotor response</article-title><source>Cell</source><volume>167</volume><fpage>947</fpage><lpage>960</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2016.10.019</pub-id><pub-id pub-id-type="pmid">27814522</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oteiza</surname> <given-names>P</given-names></name><name><surname>Odstrcil</surname> <given-names>I</given-names></name><name><surname>Lauder</surname> <given-names>G</given-names></name><name><surname>Portugues</surname> <given-names>R</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A novel mechanism for mechanosensory-based rheotaxis in larval zebrafish</article-title><source>Nature</source><volume>547</volume><fpage>445</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1038/nature23014</pub-id><pub-id pub-id-type="pmid">28700578</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname> <given-names>BW</given-names></name><name><surname>Abraham</surname> <given-names>AO</given-names></name><name><surname>MacIver</surname> <given-names>MA</given-names></name><name><surname>McLean</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visually guided gradation of prey capture movements in larval zebrafish</article-title><source>Journal of Experimental Biology</source><volume>216</volume><fpage>3071</fpage><lpage>3083</lpage><pub-id pub-id-type="doi">10.1242/jeb.087742</pub-id><pub-id pub-id-type="pmid">23619412</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rasmussen</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The infinite gaussian mixture model</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>554</fpage><lpage>560</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/1745-the-infinite-gaussian-mixture-model">https://papers.nips.cc/paper/1745-the-infinite-gaussian-mixture-model</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Russell</surname> <given-names>S</given-names></name><name><surname>Norvig</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Artificial Intelligence a Modern Approach </source><edition>Third Edition</edition><publisher-loc>Upper Saddle River</publisher-loc><publisher-name>Prentice Hall</publisher-name></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russett</surname> <given-names>DF</given-names></name><name><surname>Wilkens</surname> <given-names>LA</given-names></name><name><surname>Moss</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Use of behavioural stochastic resonance by paddle fish for feeding</article-title><source>Nature</source><volume>402</volume><pub-id pub-id-type="doi">10.1038/46279</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saad</surname> <given-names>FA</given-names></name><name><surname>Cusumano-Towner</surname> <given-names>MF</given-names></name><name><surname>Schaechtle</surname> <given-names>U</given-names></name><name><surname>Rinard</surname> <given-names>MC</given-names></name><name><surname>Mansinghka</surname> <given-names>VK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bayesian synthesis of probabilistic programs for automatic data modeling</article-title><source>Proceedings of the ACM on Programming Languages</source><volume>3</volume><fpage>1</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1145/3290350</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Saad</surname> <given-names>F</given-names></name><name><surname>Mansinghka</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Probabilistic data analysis with probabilistic programming</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1608.05347">http://arxiv.org/abs/1608.05347</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Spelke</surname> <given-names>E</given-names></name><name><surname>Hespos</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>Continuity, Competence, and the Object Concept</chapter-title><source>Language, Brain and Cognitive Development: Essays in Honor of Jacques Mehler</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sperry</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Chemoaffinity in the orderly growth of nerve fiber patterns and connections</article-title><source>PNAS</source><volume>50</volume><fpage>703</fpage><lpage>710</lpage><pub-id pub-id-type="doi">10.1073/pnas.50.4.703</pub-id><pub-id pub-id-type="pmid">14077501</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stern</surname> <given-names>S</given-names></name><name><surname>Kirst</surname> <given-names>C</given-names></name><name><surname>Bargmann</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuromodulatory control of Long-Term behavioral patterns and individuality across development</article-title><source>Cell</source><volume>171</volume><fpage>1649</fpage><lpage>1662</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.10.041</pub-id><pub-id pub-id-type="pmid">29198526</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trivedi</surname> <given-names>CA</given-names></name><name><surname>Bollmann</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visually driven chaining of elementary swim patterns into a goal-directed motor sequence: a virtual reality study of zebrafish prey capture</article-title><source>Frontiers in Neural Circuits</source><volume>7</volume><pub-id pub-id-type="doi">10.3389/fncir.2013.00086</pub-id><pub-id pub-id-type="pmid">23675322</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullman</surname> <given-names>TD</given-names></name><name><surname>Spelke</surname> <given-names>E</given-names></name><name><surname>Battaglia</surname> <given-names>P</given-names></name><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mind games: game engines as an architecture for intuitive physics</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>649</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.05.012</pub-id><pub-id pub-id-type="pmid">28655498</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiesenfeld</surname> <given-names>K</given-names></name><name><surname>Moss</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Stochastic resonance and the benefits of noise: from ice ages to crayfish and SQUIDs</article-title><source>Nature</source><volume>373</volume><fpage>33</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1038/373033a0</pub-id><pub-id pub-id-type="pmid">7800036</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yoo</surname> <given-names>SBM</given-names></name><name><surname>Tu</surname> <given-names>JC</given-names></name><name><surname>Piantadosi</surname> <given-names>S</given-names></name><name><surname>Hayden</surname> <given-names>BY</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The neural basis of predictive pursuit</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/694604</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td><bold>function</bold> STRIKE(<italic>prey_coordinate</italic>) <bold>returns</bold> <italic>true</italic> if prey in strike zone and <italic>false</italic> otherwise <break/>      <bold>inputs:</bold> <italic>prey_coordinate</italic>, a percept of the current prey position <break/>    <bold>local variables:</bold> <italic>strikezone</italic>, 95% CI of strike probability based on 2B norm fits, or bounds of a fixed window for single coordinate <break/>     <bold>if</bold> <italic>prey_coordinate</italic> in <italic>strikezone</italic> <bold>then return</bold> <italic>true</italic> <break/>     <bold>else return</bold> <italic>false</italic></td></tr></tbody></table></table-wrap></p><p><table-wrap id="inlinetable2" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td><bold>function</bold> ALT_COEFFS(<italic>prey_alt</italic>) <bold>returns</bold> <italic>alt_coefficients,</italic> a list containing a slope and y-intercept for deterministic transform of alt coordinates <break/>    <bold>inputs:</bold> <italic>prey_alt</italic>, a percept of the current altitude of the prey item <break/>    <bold>local variables:</bold> <italic>alt_slope</italic> | <italic>prey_alt_positive</italic> = .54 <break/>             <italic>alt_yint</italic> | <italic>prey_alt_positive</italic> = 8.34<sup>o</sup> <break/>             <italic>alt_slope</italic> | <italic>prey_alt_negative</italic> = .92 <break/>             <italic>alt_yint</italic> | <italic>prey_alt_negative</italic> = 7.03 <sup>o</sup> <break/> <break/><break/>    <bold>if</bold> <italic>prey_alt</italic> &gt; 0, <bold>then return</bold> [<italic>alt_slope</italic> | <italic>prey_alt_positive</italic>, <italic>alt_yint</italic> | prey_alt_positive] <break/>    <bold>else return</bold> [<italic>alt_slope</italic> | <italic>prey_alt_negative</italic>, <italic>alt_yint</italic> | <italic>prey_alt_negative</italic>]</td><td valign="middle"><break/><break/><break/><break/>from <xref ref-type="fig" rid="fig5">Figure 5B</xref></td></tr></tbody></table></table-wrap></p><p><table-wrap id="inlinetable3" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td><bold>function</bold> DETERMINISTIC_TRANSFORM(<italic>prey_coordinate</italic>) <bold>returns</bold> <italic>new_prey_coordinate,</italic> <break/>    <bold>inputs:</bold> <italic>prey_coordinate</italic>, a percept of the current spherical prey coordinate as a list [‘az’, ‘alt’, ‘dist’] <break/>    <bold>local variables:</bold> <italic>az_slope</italic> = .53 <break/>             <italic>alt_slope</italic> <break/>             <italic>alt_yint</italic> from Figure 5A <break/>             <italic>dist_slope</italic> = .84 <break/>             <italic>dist_yint</italic> = -.0125 mm <break/>             <italic>new_prey_coordinate,</italic> the new spherical prey position after transform <break/>    <italic>alt_slope</italic>, <italic>alt_yint</italic> ←ALT_COEFFS(<italic>prey_position</italic>[‘alt’]) <break/>    <italic>new_prey_coordinate</italic> ←[<italic>prey_coordinate</italic>[‘az’] * <italic>az_slope</italic>, <break/>                 <italic>prey_coordinate</italic>[‘alt’] * <italic>alt_slope</italic> + <italic>alt_yint</italic>, <break/>                 <italic>prey_coordinate</italic>[‘dist’] * <italic>dist_slope</italic> + <italic>dist_yint</italic>] <break/>    <bold>return</bold> <italic>new_prey_coordinate</italic></td><td valign="middle"><break/><break/><break/><break/><break/>from <xref ref-type="fig" rid="fig5">Figure 5A</xref></td></tr></tbody></table></table-wrap></p><p><table-wrap id="inlinetable4" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td><bold>function</bold> PREYCAPTURE(<italic>prey_coordinate, bout_counter</italic>) <bold>returns</bold> <italic>bout_counter</italic>, the number of bouts required for capture <break/>    <bold>inputs:</bold> <italic>prey_coordinate</italic>, a percept of the current prey in spherical coordinates <break/>        <italic>bout_counter</italic>,the number of bouts the agent has performed since hunt initiation <break/> <break/><break/>    <bold>if</bold> STRIKE(<italic>prey_coordinate</italic>): <break/>         <italic>bout_counter</italic> ←<italic>bout_counter</italic> + 1 <break/>         <bold>return</bold> <italic>bout_counter</italic> <break/>    <bold>else:</bold> <break/>        <italic>prey_coordinate </italic>← DETERMINISTIC_TRANSFORM(<italic>prey_position</italic>) <break/>        <italic>bout_counter</italic> ← <italic>bout_counter</italic> + 1 <break/>        PREYCAPTURE(<italic>prey_coordinate</italic>, <italic>bout_counter</italic>)</td><td valign="middle"><break/><break/><break/><break/><break/><break/><break/><break/><break/><break/><break/><break/><break/>** replace DETERMINISTIC_TRANSFORM(prey_position)<break/>with STOCHASTIC_TRANSFORM(prey_position) to sample DPMM <break/>(see Figure 6), which implements graded variance</td></tr></tbody></table></table-wrap></p><p><table-wrap id="inlinetable5" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td><bold>function</bold> GRADED_VARIANCE(<italic>prey_coordinate, bout_counter, dist_or_angle</italic>) <bold>returns</bold> <italic>bout_counter</italic>, the number of bouts required for capture <break/>   <bold>inputs:</bold> <italic>prey_coordinate</italic>, a percept of the current prey position in spherical coordinates <break/>        <italic>bout_counter,</italic> the number of bouts the agent has performed since hunt initiation <break/>        <italic>dist_or_angle</italic>, string representing whether input is a distance or an azimuth angle <break/>    <bold>local_variables:</bold> <break/>       µ a value representing the average transform <break/>       σ the standard deviation of the average transform that decreases with proximity to the strike zone <break/>    <bold>if</bold> STRIKE(<italic>prey_coordinate</italic>): <break/>        <italic>bout_counter ← bout_counter</italic> + 1 <break/>        <bold>return</bold> <italic>bout_counter</italic> <break/>    <bold>else:</bold> <break/>       <bold>if</bold> <italic>dist_or_angle</italic> == ‘angle’: <break/>         µ<italic>←</italic>53* <italic>prey_coordinate</italic> <break/>         σ <italic>←</italic> .36 * <italic>prey_coordinate</italic> + 7.62° <break/>       <bold>if</bold> <italic>dist_or_angle</italic> == ‘distance’: <break/>         µ<italic>←</italic>84* <italic>prey_coordinate</italic> - .0125 mm <break/>         σ <italic>←</italic> 0.137 * <italic>prey_coordinate</italic> + 0.034 mm <break/>       <italic>prey_coordinate ←</italic> GAUSSIAN_DRAW(µ, σ) <break/>       <italic>bout_counter ← bout_counter</italic> + 1 <break/>       GRADED_VARIANCE(<italic>prey_coordinate, bout_counter, dist_or_ang</italic>)</td></tr></tbody></table></table-wrap></p></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.51975.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Washbourne</surname><given-names>Phillip</given-names> </name><role>Reviewer</role><aff><institution>University of Oregon</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Combes</surname><given-names>Stacey A</given-names></name><role>Reviewer</role><aff><institution>University of California, Davis</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This paper presents a highly detailed account of the kinematics of larval zebrafish hunting paramecia and compares these to various computational models to infer key aspects of zebrafish pursuit strategy and control. The work employs a free behavioral assay in which zebrafish can pursue their prey in 3D space, and takes advantage of the distinct pursuit &quot;bouts&quot; that zebrafish engage in during prey pursuits, unlike many other predators that display continuous pursuits. The most novel and exciting aspect of this work is the finding that the stochastic (&quot;noisy&quot;) pursuit kinematics displayed by live zebrafish actually perform better than &quot;ideal&quot; alternatives that are modeled, in terms of striking a beneficial balance between capture success, speed, and energetic expenditure. The authors conclude their study with a well-written Discussion that touches on many potential causes and implications of this beneficial, stochastic pursuit strategy in 3D. This work will likely be of interest to researchers interested in prey capture writ large, as well as those interested in the role of noise and strategy in animal behavior, and points towards novel models to understand goal-directed behavior in animals.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Elements of a stochastic 3D prediction engine in larval zebrafish prey capture&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Ronald Calabrese as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Phillip Washbourne (Reviewer #1); Stacey A Combes (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>As the reviews were largely positive on the technical aspects of the work, most of the necessary revisions detailed below relate to how the work is presented. Most broadly, there was a consensus amongst the reviewers that the writing was somewhat unclear and jargon-full at times, making it less accessible to the broad <italic>eLife</italic> readership than it could be, so the manuscript could greatly benefit from additional copy editing and tightening (including potentially making some figures supplementary). The manuscript also would greatly benefit from additional exposition explaining the technical choices made (including, for example, the DPMM) and the consequences of these choices over other possibilities.</p><p>All of that being said, all reviewers expressed excitement about the scientific results presented in the work, and we look forward to seeing the revised submission.</p><p>Essential revisions:</p><p>- As the reviews were largely positive on the technical aspects of the work, most of the necessary revisions detailed below relate to how the work is presented. Most broadly, there was a consensus amongst the reviewers that the writing was somewhat unclear and jargon-full at times, making it less accessible to the broad <italic>eLife</italic> readership than it could be, so the manuscript could greatly benefit from additional copy editing and tightening (including potentially making some additional figures supplementary).</p><p>- The manuscript also would greatly benefit from additional exposition explaining the technical choices made (including, for example, the DPMM) and the consequences of these choices over other possibilities.</p><p>- The finding that larval zebrafish use both the position and velocity of their prey to predict future prey locations has been shown in many other animals, including invertebrates such as dragonflies. The reviewers didn't find this result particularly surprising, although there was less familiarity with the zebrafish prey capture literature amongst the reviewers than with other studies on other predator-prey systems – so perhaps this overturns some long-standing assumptions about the capabilities of larval zebrafish. If this is the case, the authors should emphasize more strongly why these findings are surprising. Otherwise, the findings concerning the benefit of &quot;noisy&quot; pursuit trajectories should receive more of the emphasis in the Abstract, conclusions, etc.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.51975.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>- As the reviews were largely positive on the technical aspects of the work, most of the necessary revisions detailed below relate to how the work is presented. Most broadly, there was a consensus amongst the reviewers that the writing was somewhat unclear and jargon-full at times, making it less accessible to the broad eLife readership than it could be, so the manuscript could greatly benefit from additional copy editing and tightening (including potentially making some additional figures supplementary).</p></disp-quote><p>As you’ll see we have quite extensively tightened the text and we paid particular attention in focusing on the main message and on de-jargonizing. We think it’s much, much better now and more readily accessible to a broad audience.</p><disp-quote content-type="editor-comment"><p>- The manuscript also would greatly benefit from additional exposition explaining the technical choices made (including, for example, the DPMM) and the consequences of these choices over other possibilities.</p></disp-quote><p>We have added to the Results section where we give a more detailed explanation for our choices of models; and we also moved the more technical aspects to the Materials and methods.</p><disp-quote content-type="editor-comment"><p>- The finding that larval zebrafish use both the position and velocity of their prey to predict future prey locations has been shown in many other animals, including invertebrates such as dragonflies. The reviewers didn't find this result particularly surprising, although there was less familiarity with the zebrafish prey capture literature amongst the reviewers than with other studies on other predator-prey systems – so perhaps this overturns some long-standing assumptions about the capabilities of larval zebrafish. If this is the case, the authors should emphasize more strongly why these findings are surprising. Otherwise, the findings concerning the benefit of &quot;noisy&quot; pursuit trajectories should receive more of the emphasis in the Abstract, conclusions, etc.</p></disp-quote><p>We thank the reviewers for this helpful suggestion. Indeed, we do not find this result particularly surprising since a lot of animals do use combined velocity and position estimations to guide their behavior. We now explicitly cite and discuss work from the dragonfly and salamander literature that addresses this point. However, we note that this question was unresolved in larval zebrafish and it is therefore clearly important and interesting (if not surprising) that larval zebrafish do this, too. Also, velocity perception is fundamental to our recursive algorithm and is as such an essential component. We have modified manuscript to make this clearer and we also shifted the emphasis onto the noisy pursuit strategy, citing additional biological stochasticity papers in the Introduction, emphasizing the stochasticity in the Abstract, and adding a section on its novelty in the Discussion.</p></body></sub-article></article>