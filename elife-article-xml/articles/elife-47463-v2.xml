<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">47463</article-id><article-id pub-id-type="doi">10.7554/eLife.47463</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>One-shot learning and behavioral eligibility traces in sequential decision making</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-137289"><name><surname>Lehmann</surname><given-names>Marco P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5274-144X</contrib-id><email>marco.lehmann@alumni.epfl.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140175"><name><surname>Xu</surname><given-names>He A</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140176"><name><surname>Liakoni</surname><given-names>Vasiliki</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2599-1424</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-125147"><name><surname>Herzog</surname><given-names>Michael H</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-3010"><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-103462"><name><surname>Preuschoff</surname><given-names>Kerstin</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Brain-Mind-Institute, School of Life Sciences</institution><institution>École Polytechnique Fédérale de Lausanne</institution><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">School of Computer and Communication Sciences</institution><institution>École Polytechnique Fédérale de Lausanne</institution><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Laboratory of Psychophysics, School of Life Sciences</institution><institution>École Polytechnique Fédérale de Lausanne</institution><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff><aff id="aff4"><label>4</label><institution>Swiss Center for Affective Sciences, University of Geneva</institution><addr-line><named-content content-type="city">Geneva</named-content></addr-line><country>Switzerland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>11</day><month>11</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e47463</elocation-id><history><date date-type="received" iso-8601-date="2019-04-05"><day>05</day><month>04</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-11-01"><day>01</day><month>11</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Lehmann et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Lehmann et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-47463-v2.pdf"/><abstract><p>In many daily tasks, we make multiple decisions before reaching a goal. In order to learn such sequences of decisions, a mechanism to link earlier actions to later reward is necessary. Reinforcement learning (RL) theory suggests two classes of algorithms solving this credit assignment problem: In classic temporal-difference learning, earlier actions receive reward information only after multiple repetitions of the task, whereas models with eligibility traces reinforce entire sequences of actions from a single experience (one-shot). Here, we show one-shot learning of sequences. We developed a novel paradigm to <italic>directly</italic> observe which actions and states along a multi-step sequence are reinforced after a single reward. By focusing our analysis on those states for which RL with and without eligibility trace make qualitatively distinct predictions, we find direct behavioral (choice probability) and physiological (pupil dilation) signatures of reinforcement learning with eligibility trace across multiple sensory modalities.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>eligibility trace</kwd><kwd>human learning</kwd><kwd>sequential decision making</kwd><kwd>pupillometry</kwd><kwd>reward prediction error</kwd><kwd>reinforcement learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>CRSII2 147636 (Sinergia)</award-id><principal-award-recipient><name><surname>Lehmann</surname><given-names>Marco P</given-names></name><name><surname>Xu</surname><given-names>He A</given-names></name><name><surname>Liakoni</surname><given-names>Vasiliki</given-names></name><name><surname>Herzog</surname><given-names>Michael H</given-names></name><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name><name><surname>Preuschoff</surname><given-names>Kerstin</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>CRSII2 200020 165538</award-id><principal-award-recipient><name><surname>Lehmann</surname><given-names>Marco P</given-names></name><name><surname>Liakoni</surname><given-names>Vasiliki</given-names></name><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>Human Brain Project (SGA2) 785907</award-id><principal-award-recipient><name><surname>Herzog</surname><given-names>Michael H</given-names></name><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>268 689 MultiRules</award-id><principal-award-recipient><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>Human Brain Project (SGA1) 720270</award-id><principal-award-recipient><name><surname>Herzog</surname><given-names>Michael H</given-names></name><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Human learning relies on short-term memories (eligibility traces) which provide a mechanism to reinforce sequences of actions from a single reward (one-shot).</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In games, such as chess or backgammon, the players have to perform a sequence of many actions before a reward is received (win, loss). Likewise in many sports, such as tennis, a sequence of muscle movements is performed until, for example, a successful hit is executed. In both examples, it is impossible to immediately evaluate the goodness of a single action. Hence the question arises: How do humans learn sequences of actions from delayed reward?</p><p>Reinforcement learning (RL) models (<xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>) have been successfully used to describe reward-based learning in humans (<xref ref-type="bibr" rid="bib41">Pessiglione et al., 2006</xref>; <xref ref-type="bibr" rid="bib20">Gläscher et al., 2010</xref>; <xref ref-type="bibr" rid="bib15">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib35">Niv et al., 2012</xref>; <xref ref-type="bibr" rid="bib37">O'Doherty et al., 2017</xref>; <xref ref-type="bibr" rid="bib51">Tartaglia et al., 2017</xref>). In RL, an action (e.g. moving a token or swinging the arm) leads from an old state (e.g. configuration of the board, or position of the body) to a new one. Here, we grouped RL theories into two different classes. The first class, containing classic Temporal-Difference algorithms (such as <italic>TD-0</italic> <xref ref-type="bibr" rid="bib49">Sutton, 1988</xref>) cannot support one-shot learning of long sequences, because multiple repetitions of the task are needed before reward information arrives at states far away from the goal. Instead, one-shot learning requires algorithms that keep a memory of past states and actions making them eligible for later, that is delayed reinforcement. Such a memory is a key feature of the second class of RL theories – called <italic>RL with eligibility trace</italic> –, which includes algorithms with explicit eligibility traces (<xref ref-type="bibr" rid="bib49">Sutton, 1988</xref>; <xref ref-type="bibr" rid="bib54">Watkins, 1989</xref>; <xref ref-type="bibr" rid="bib56">Williams, 1992</xref>; <xref ref-type="bibr" rid="bib40">Peng and Williams, 1996</xref>; <xref ref-type="bibr" rid="bib47">Singh and Sutton, 1996</xref>) and related reinforcement learning models (<xref ref-type="bibr" rid="bib54">Watkins, 1989</xref>; <xref ref-type="bibr" rid="bib33">Moore and Atkeson, 1993</xref>; <xref ref-type="bibr" rid="bib7">Blundell et al., 2016</xref>; <xref ref-type="bibr" rid="bib32">Mnih et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>).</p><p>Eligibility traces are well-established in computational models (<xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>), and supported by synaptic plasticity experiments (<xref ref-type="bibr" rid="bib57">Yagishita et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">He et al., 2015</xref>; <xref ref-type="bibr" rid="bib6">Bittner et al., 2017</xref>; <xref ref-type="bibr" rid="bib17">Fisher et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Gerstner et al., 2018</xref>). However, it is unclear whether humans show one-shot learning, and a direct test of predictions that are manifestly different between the classes of RL models with and without eligibility trace has never been performed. Multi-step sequence learning with delayed feedback (<xref ref-type="bibr" rid="bib20">Gläscher et al., 2010</xref>; <xref ref-type="bibr" rid="bib15">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Walsh and Anderson, 2011</xref>; <xref ref-type="bibr" rid="bib51">Tartaglia et al., 2017</xref>) offers a way to directly compare the two, because the two classes of RL models make <italic>qualitatively</italic> different predictions. Our question can therefore be reformulated more precisely: Is there evidence for RL with eligibility trace in the form of one-shot learning? In other words, are actions and states more than one step away from the goal, reinforced after a single rewarded experience? And if eligibility traces play a role, how many states and actions are reinforced by a single reward?</p><p>To answer these questions, we designed a novel sequential learning task to directly observe which actions and states of a multi-step sequence are reinforced. We exploit that after a single reward, models of learning without eligibility traces (our null hypothesis) and with eligibility traces (alternative hypothesis) make qualitatively distinct predictions about changes in action-selection bias and in state evaluation (<xref ref-type="fig" rid="fig1">Figure 1</xref>). This qualitative difference in the second episode (i.e. after a single reward) allows us to draw conclusions about the presence or absence of eligibility traces independently of specific model fitting procedures and independently of the choice of physiological correlates, be it EEG, fMRI, or pupil responses. We therefore refer to these qualitative differences as ’direct’ evidence.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design and hypothesis.</title><p>(<bold>a</bold>) Typical state-action sequences of the first two episodes. At each state, participants execute one of two actions, ’a’ or ’b’, leading to the next state. Here, the participant discovered the goal state after randomly choosing three actions: ’b’ in state S (Start), ’a’ in D2 (two actions from the goal), and ’b’ in D1 (one action from the goal). Episode 1 terminated at the rewarding goal state. Episode 2 started in a new state, Y. Note that D2 and D1 already occurred in episode 1. In this example, the participant repeated the actions which led to the goal in episode 1 (’a’ at D2 and ’b’ at D1). (<bold>b</bold>) Reinforcement learning models make predictions about such behavioral biases, and about learned properties (such as action value <inline-formula><mml:math id="inf1"><mml:mi>Q</mml:mi></mml:math></inline-formula>, state value <inline-formula><mml:math id="inf2"><mml:mi>V</mml:mi></mml:math></inline-formula> or TD-errors, denoted as <inline-formula><mml:math id="inf3"><mml:mi>x</mml:mi></mml:math></inline-formula>) presumably observable as changes in a physiological measure (e.g. pupil dilation). Null Hypothesis: In RL without eligibility traces, only the state-action pair immediately preceding a reward is reinforced, leading to a bias at state D1, but not at D2 (50%-line). Similarly, the state value of D2 does not change and therefore the physiological response at the D2 in episode 2 (solid red line) should not differ from episode 1 (dashed black line). Alternative Hypothesis: RL with eligibility traces reinforces decisions further back in the state-action history. These models predict a behavioral bias at D1 and D2, and a learning-related physiological response at the onset of these states after a single reward. The effects may be smaller at state D2 because of decay factors in models with eligibility traces.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47463-fig1-v2.tif"/></fig><p>We measure changes in action-selection bias from behavior and changes in state evaluation from a physiological signal, namely the pupil dilation. Pupil responses have been previously linked to decision making, and in particular to variables that reflect changes in state value such as expected reward, reward prediction error, surprise, and risk (<xref ref-type="bibr" rid="bib36">O'Doherty et al., 2003</xref>; <xref ref-type="bibr" rid="bib27">Jepma and Nieuwenhuis, 2011</xref>; <xref ref-type="bibr" rid="bib38">Otero et al., 2011</xref>; <xref ref-type="bibr" rid="bib42">Preuschoff et al., 2011</xref>). By focusing our analysis on those states for which the two hypotheses make distinct predictions after a <italic>single</italic> reward (’one-shot’), we find direct behavioral and physiological signatures of reinforcement learning with eligibility trace. The observed one-shot learning sheds light on a long-standing question in human reinforcement learning (<xref ref-type="bibr" rid="bib8">Bogacz et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Walsh and Anderson, 2011</xref>; <xref ref-type="bibr" rid="bib53">Walsh and Anderson, 2012</xref>; <xref ref-type="bibr" rid="bib55">Weinberg et al., 2012</xref>; <xref ref-type="bibr" rid="bib51">Tartaglia et al., 2017</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Since we were interested in one-shot learning, we needed an experimental multi-step action paradigm that allowed a comparison of behavioral and physiological measures between episode 1 (before any reward) and episode 2 (after a single reward). Our learning environment had six states plus a goal G (<xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>) identified by clip-art images shown on a computer screen in front of the participants. It was designed such that participants were likely to encounter in episode 2 the same states D1 (one step away from the goal) and/or D2 (two steps away) as in episode 1 (<xref ref-type="fig" rid="fig1">Figure 1 (a)</xref>). In each state, participants chose one out of two actions, ’a’ or ’b’, and explored the environment until they discovered the goal G (the image of a reward) which terminated the episode. The participants were instructed to complete as many episodes as possible within a limited time of 12 min (Materials and methods).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>A single delayed reward reinforces state-action associations.</title><p>(<bold>a</bold>) Structure of the environment: six states, two actions, rewarded goal G. Transitions (arrows) were predefined, but actions were attributed to transitions <italic>during</italic> the experiment. Unbeknownst to the participants, the first actions always led through the sequence S (Start), D2 (two steps before goal), D1 (one step before goal) to G (Goal). Here, the participant chose actions ’b’, ’a’, ’b’ (underlined boldface). (<bold>b</bold>) Half of the experiments, started episode 2 in X, always leading to D1, where we tested if the action rewarded in episode 1 was repeated. (<bold>c</bold>) In the other half of experiments, we tested the decision bias in episode 2 at D2 (’a’ in this example) by starting from Y. (<bold>d</bold>) The same structure was implemented in three conditions. In the <italic>spatial</italic> condition (22 participants, <italic>top</italic> row in Figures (<bold>d</bold>), (<bold>e</bold>) and (<bold>f</bold>)), each state is identified by a fixed location (randomized across participants) of a checkerboard, flashed for a 100 ms on the screen. Participants only see one checkerboard at a time; the red arrows and state identifiers S, D2, D1, G are added to the figure to illustrate a first episode. In the <italic>sound</italic> condition (15 participants, <italic>middle</italic> row), states are represented by unique short sounds. In the c<italic>lip-art</italic> condition (12 participants, <italic>bottom</italic> row), a unique image is used for each state. (<bold>e</bold>) Action selection bias in state D1, in episode 2, averaged across all participants. (f) In all three conditions the action choices at D2 were significantly different from chance level (dashed horizontal line) and biased toward the actions that have led to reward in episode 1. Error bars: SEM, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mmultiscripts><mml:mi>p</mml:mi><mml:mprescripts/><mml:none/><mml:mo>*</mml:mo></mml:mmultiscripts><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:mrow><mml:mmultiscripts><mml:mi>p</mml:mi><mml:mprescripts/><mml:none/><mml:mrow><mml:mo>*</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo>*</mml:mo><mml:mo>*</mml:mo></mml:mrow></mml:mrow></mml:mmultiscripts><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>. For clarity, actions are labeled ’a’ and ’b’ in (<bold>e</bold>) and (<bold>f</bold>), consistent with panels (<bold>a</bold>) - (<bold>c</bold>), even though actual choices of participants varied.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47463-fig2-v2.tif"/></fig><p>The first set of predictions applied to the state D1 which served as a control if participants were able to learn, and assign value to, states or actions. Both classes of algorithms, with or without eligibility trace, predicted that effects of learning after the first reward should be reflected in the action choice probability during a subsequent visit of state D1 (<xref ref-type="fig" rid="fig1">Figure 1 (b)</xref>). For estimated effect size, see subsection Q-lambda model predictions in 'Methods. Furthermore, any physiological variable that correlates with variables of reinforcement learning theories, such as action value <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, state value <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, or TD-error, should increase at the second encounter of D1. To assess this effect of learning, we measured the pupil dilation, a known physiological marker for learning-related signals (<xref ref-type="bibr" rid="bib36">O'Doherty et al., 2003</xref>; <xref ref-type="bibr" rid="bib27">Jepma and Nieuwenhuis, 2011</xref>; <xref ref-type="bibr" rid="bib38">Otero et al., 2011</xref>; <xref ref-type="bibr" rid="bib42">Preuschoff et al., 2011</xref>). The advantage of our hypothesis-driven approach was that we did not need to make assumptions about the neurophysiological mechanisms causing pupil changes. Comparing the pupil dilation at state D1 in episode 1 to episode 2 (<xref ref-type="fig" rid="fig1">Figure 1(b)</xref>, null hypothesis <italic>and</italic> alternative), provided a baseline for the putative effect.</p><p>Our second set of predictions concerned state D2. RL without eligibility trace (null hypothesis) such as <italic>TD-0</italic>, predicted that the action choice probability at D2 during episode 2 should be at 50 percent, since information about the reward at the goal state G cannot ‘travel’ two steps. However, the class of RL with eligibility trace (alternative hypothesis) predicted an increase in the probability of choosing the correct action, that is the one leading toward the goal (For estimated effect size, see subsection Q-lambda model predictions in <italic>Methods</italic>). The two hypotheses also made different predictions about the pupil response to the onset of state D2. Under the null hypothesis, the evaluation of the state D2 could not change after a single reward. In contrast, learning with eligibility trace predicted a change in state evaluation, presumably reflected in pupil dilation (<xref ref-type="fig" rid="fig1">Figure 1(b)</xref>).</p><p>Participants could freely choose actions, but in order to maximize encounters with states D1 and D2, we assigned actions to state transitions ’on the fly’. In the first episode, all participants started in state S (<xref ref-type="fig" rid="fig1">Figure 1 (a)</xref> and <xref ref-type="fig" rid="fig2">2(a)</xref>) and chose either action 'a' or 'b'. Independently of their choice and unbeknownst to the participants, the first action brought them always to state D2, two steps away from the goal. Similarly, in D2, participants could freely choose an action but always transitioned to D1, and with their third action, to G. These initial actions determined the assignment of state-action pairs to state transitions for all remaining episodes in this environment. For example, if, during the first episode, a participant had chosen action 'a' in state D2 to initiate the transition to D1, then action 'a' brought this participant in all future encounters of D2 to D1, whereas action 'b' brought her from D2 to Z (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In episode 2, half of the participants started from state Y. Their first action always brought them to D2, which they had already seen once during the first episode. The other half of the participants started in state X and their first action brought them to D1 (<xref ref-type="fig" rid="fig2">Figure 2 (b)</xref>). Participants who started episode 2 in state X started episode 3 in state Y and vice versa. In episodes 4 to 7, the starting states were randomly chosen from {S, D2, X, Y, Z}. After seven episodes, we considered the task as solved, and the same procedure started again in a new environment (see Materials and methods for the special cases of repeated action sequences). This task design allowed us to study human learning in specific and controlled state sequences, without interfering with the participant’s free choices.</p><sec id="s2-1"><title>Behavioral evidence for one-shot learning</title><p>As expected, we found that the action taken in state D1 that led to the rewarding state G was reinforced after episode 1. Reinforcement was visible as an action bias toward the correct action when D1 was seen again in episode 2 (<xref ref-type="fig" rid="fig2">Figure 2 (e)</xref>). This action bias is predicted by many different RL algorithms including the early theories of <xref ref-type="bibr" rid="bib43">Rescorla and Wagner (1972)</xref>.</p><p>Importantly, we also found a strong action bias in state D2 in episode 2: participants repeated the correct action (the one leading toward the goal) in 85% of the cases. This strong bias is significantly different from chance level 50% (p&lt;0.001; <xref ref-type="fig" rid="fig2">Figure 2 (f)</xref>), and indicates that participants learned to assign a positive value to the correct state-action pair after a <italic>single exposure</italic> to state D2 and a <italic>single reward</italic> at the end of episode 1. In other words, we found evidence for one-shot learning in a state two steps away from goal in a multi-step decision task.</p><p>This is compatible with our alternative hypothesis, that is the broad class of RL ’with eligibility trace’, (<xref ref-type="bibr" rid="bib49">Sutton, 1988</xref>; <xref ref-type="bibr" rid="bib54">Watkins, 1989</xref>; <xref ref-type="bibr" rid="bib56">Williams, 1992</xref>; <xref ref-type="bibr" rid="bib33">Moore and Atkeson, 1993</xref>; <xref ref-type="bibr" rid="bib40">Peng and Williams, 1996</xref>; <xref ref-type="bibr" rid="bib47">Singh and Sutton, 1996</xref>; <xref ref-type="bibr" rid="bib32">Mnih et al., 2016</xref>; <xref ref-type="bibr" rid="bib7">Blundell et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>) that keep explicit or implicit memories of past state-action pairs (see Discussion). However, it is not compatible with the null hypothesis, that is RL ’without eligibility trace’. In both classes of algorithms, action biases or values that reflect the expected future reward are assigned to states. In RL ’without eligibility trace’, however, value information collected in a single action step is shared only between neighboring states (for example between states G and D1), whereas in RL ’with eligibility trace’ value information can reach state D2 after a single episode. Importantly, the above argument is both fundamental and qualitative in the sense that it does not rely on any specific choice of parameters or implementation details of an algorithm. Our finding can be interpreted as a signature of a behavioral eligibility trace in human multi-step decision making and complements the well-established synaptic eligibility traces observed in animal models (<xref ref-type="bibr" rid="bib57">Yagishita et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">He et al., 2015</xref>; <xref ref-type="bibr" rid="bib6">Bittner et al., 2017</xref>; <xref ref-type="bibr" rid="bib17">Fisher et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Gerstner et al., 2018</xref>).</p><p>We wondered whether the observed one-shot learning in our multi-step decision task depended on the choice of stimuli. If clip-art images helped participants to construct an imaginary story (e.g. with the method of loci; <xref ref-type="bibr" rid="bib58">Yates, 1966</xref>) in order to rapidly memorize state-action associations, the effect should disappear with other stimuli. We tested participants in environments where states were defined by acoustic stimuli (2nd experiment: <italic>sound</italic> condition) or by the spatial location of a black-and-white rectangular grid on the grey screen (3rd experiment: <italic>spatial</italic>condition; see <xref ref-type="fig" rid="fig2">Figure 2</xref> and Materials and methods). Across all conditions, results were qualitatively similar (<xref ref-type="fig" rid="fig2">Figure 2 (f)</xref>): not only the action directly leading to the goal (i.e. the action in D1) but also the correct action in state D2 were chosen in episode 2 with a probability significantly different from random choice. This behavior is consistent with the class of RL with eligibility trace, and excludes all algorithms in the class of RL without eligibility trace.</p><p>Even though results are consistent across different stimuli, we cannot exclude that participants simply memorize state-action associations independently of the rewards. To exclude a reward-independent memorization strategy, we performed a control experiment in which we tested the action-bias at state D2 (see <xref ref-type="fig" rid="fig3">Figure 3</xref>) in the absence of a reward. In a design similar to the <italic>clip-art</italic> condition (<xref ref-type="fig" rid="fig1">Figure 1 (a)</xref>), the participants freely chose actions that moved them through a defined, non-rewarded, sequence of states (namely S-D2-D1-N-Y-D2, see <xref ref-type="fig" rid="fig3">Figure 3 (b)</xref> during the first episode. By design of the control experiment, participants reach the state D2 twice before they encounter any reward. Upon their second visit of state D2, we measured whether participants repeated the same action as during their first visit. Such a repetition bias could be explained if participants tried to memorize and repeat state-action associations even in the absence of a reward between the two visits. In the control experiment we observed a weak non-significant (p=0.45) action-repetition bias of only 56% (<xref ref-type="fig" rid="fig3">Figure 3 (c)</xref> in contrast to the main experiment (with a reward between the first and second encounter of state D2) where we observed a repetition bias of 85%. These results indicate that earlier rewards influence the action choice when a state is encountered a second time.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Control experiment without reward.</title><p>(<bold>a</bold>) Sequence of the first six state-action pairs in the first control experiment. The state D2 is visited twice and the number of states between the two visits is the same as in the main experiment. The original goal state has been replaced by a non-rewarded state N. The control experiment focuses on the behavior during the second visit of state D2, further state-action pairs are not relevant for this analysis. (<bold>b</bold>) The structure of the environment has been kept as close as possible to the main experiment (<xref ref-type="fig" rid="fig2">Figure 2 (a)</xref>). (<bold>c</bold>) Ten participants performed a total of 32 repetitions of this control experiment. Participants show an average action-repetition bias of 56%. This bias is not significantly different from the 50% chance level (<inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.45</mml:mn></mml:mrow></mml:math></inline-formula>) and much weaker than the 85% observed in the main experiment (<xref ref-type="fig" rid="fig2">Figure 2 (f)</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47463-fig3-v2.tif"/></fig></sec><sec id="s2-2"><title>Reinforcement learning with eligibility trace is reflected in pupil dilation</title><p>We then investigated the time-series of the pupil diameter. Both, the null and the alternative hypothesis predict a change in the evaluation of state D1, when comparing the second with the first encounter. Therefore, if the pupil dilation indeed serves as a proxy for a learning-related state evaluation (be it <inline-formula><mml:math id="inf9"><mml:mi>Q</mml:mi></mml:math></inline-formula>-value, <inline-formula><mml:math id="inf10"><mml:mi>V</mml:mi></mml:math></inline-formula>-value, or TD-error); we should observe a difference between the pupil response to the onset of state D1 before (episode 1) and after (episode 2) a single reward.</p><p>We extracted (Materials and methods) the time-series of the pupil diameter, focused on the interval [0s, 3s] after the onset of states D2 or D1, and averaged the data across participants and environments (<xref ref-type="fig" rid="fig4">Figure 4</xref>, black traces). We observed a significant change in the pupil dilatory response to stimulus D1 between episode 1 (black curve) and episode 2 (red curve). The difference was computed per time point (paired samples t-test); significance levels were adjusted to control for false discovery rate (FDR, <xref ref-type="bibr" rid="bib4">Benjamini and Hochberg, 1995</xref>) which is a conservative measure given the temporal correlations of the pupillometric signal. This result suggests that participants change the evaluation of D1 after a single reward and that this change is reflected in pupil dilation.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Pupil dilation reflects one-shot learning.</title><p>(<bold>a</bold>) Pupil responses to state D1 are larger during episode 2 (red curve) than during episode 1 (black). (b) Pupil responses to state D2 are larger during episode 2 (red curve) than during episode 1 (black). Top row: <italic>spatial</italic>, middle row: <italic>sound</italic>, bottom row: <italic>clip-art</italic> condition. Pupil diameter averaged across all participants in units of standard deviation (z-score, see Materials and methods), aligned at stimulus onset and plotted as a function of time since stimulus onset. Thin lines indicate the pupil signal ± SEM. Green lines indicate the time interval during which the two curves differ significantly (<inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>). Significance was reached at a time <inline-formula><mml:math id="inf12"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which depends on the condition and the state: <italic>spatial</italic> D1:<inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>730</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (22, 131, 85); <italic>spatial</italic> D2: <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1030</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (22, 137,130) <italic>sound</italic> D1: <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1470</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (15, 34, 19); <italic>sound</italic> D2: <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1280</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (15, 35, 33); <italic>clip-art</italic> D1: <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>970</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (12, 39, 19); <italic>clip-art</italic> D2: <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>980</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (12, 45, 41); (Numbers in brackets: number of participants, number of pupil traces in episode 1 or 2, respectively). (<bold>c</bold>) Participant-specific mean pupil dilation at state D2 (averaged over the interval (1000 ms, 2500 ms)) before (black dot) and after (red dot) the first reward. Grey lines connect values of the same participant. Differences between episodes are significant (paired t-test, p-values indicated in the Figure).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47463-fig4-v2.tif"/></fig><p>Importantly, the pupil dilatory response to the state D2 was also significantly stronger in episode 2 than in episode 1. Therefore, if pupil diameter is correlated with the state value <inline-formula><mml:math id="inf19"><mml:mi>V</mml:mi></mml:math></inline-formula>, the action value <inline-formula><mml:math id="inf20"><mml:mi>Q</mml:mi></mml:math></inline-formula>, the TD-error, or a combination thereof, then the class of <italic>RL without eligibility trace</italic> must be excluded as an explanation of the pupil response (i.e. we can reject the null hypothesis in <xref ref-type="fig" rid="fig1">Figure 1</xref>). </p><p>However, before drawing such a conclusion we controlled for correlations of pupil response with other parameters of the experiment. First, for visual stimuli, pupil responses changed with stimulus luminance. The rapid initial contraction of the pupil observed in the <italic>clip-art</italic> condition (bottom row in <xref ref-type="fig" rid="fig4">Figure 4</xref>) was a response to the 300 ms display of the images. In the <italic>spatial</italic> condition, this initial transient was absent, but the difference in state D2 between episode 1 and episode 2 were equally significant. For the <italic>sound</italic> condition, in which stimuli were longer on average (Materials and methods), the significant separation of the curves occurred slightly later than in the other two conditions. A paired t-test of differences showed that, across all three conditions, pupil dilation changes significantly between episodes 1 and 2 (<xref ref-type="fig" rid="fig4">Figure 4(c)</xref>; paired t-test, p&lt;0.001 for the <italic>spatial</italic> condition, p&lt;0.01 for the two others). Since in all three conditions luminance is identical in episodes 1 and 2, luminance cannot explain the observed differences.</p><p>Second, we checked whether the differences in the pupil traces could be explained by the novelty of a state during episode 1, or familiarity with the state in episode 2 (<xref ref-type="bibr" rid="bib38">Otero et al., 2011</xref>), rather than by reward-based learning. In a further control experiment, a different set of participants saw a sequence of states, replayed from the main experiment. In order to ensure that participants were focusing on the state sequence and engaged in the task, they had to push a button in each state (freely choosing either ’a’ or ’b’), and count the number of states from start to goal. Stimuli, timing and data analysis were the same as in the main experiment. The strong difference after <inline-formula><mml:math id="inf21"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>1000</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> in state D2, that we observed in <xref ref-type="fig" rid="fig4">Figure 4 (b)</xref>, was absent in the control experiment (<xref ref-type="fig" rid="fig5">Figure 5</xref>) indicating that the significant differences in pupil dilation in response to state D2 cannot be explained by novelty or familiarity alone. The findings in the control experiment also exclude other interpretations of correlations of pupil diameter such as memory formation in the absence of reward.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Pupil dilation during the second control experiment.</title><p>In the second control experiment, different participants passively observed state sequences which were recorded during the main experiment. Data analysis was the same as for the main experiment. (<bold>a</bold>) Pupil time course after state onset (<inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) of state D1 (before goal). (<bold>b</bold>) State D2 (two before goal). Black traces show the pupil dilation during episode one, red traces during episode two. At state D1 in the <italic>clip-art</italic> condition, the pupil time course shows a separation similar to the one observed in the main experiment. This suggest that participants may recognize the clip-art image that appears just before the final image. Importantly in state D2, the pupil time course during episode 2 is qualitatively different from the one in the main experiment (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47463-fig5-v2.tif"/></fig><p>In summary, across three different stimulus modalities, the single reward received at the end of the first episode strongly influenced the pupil responses to the same stimuli later in episode 2. Importantly, this effect was observed not only in state D1 (one step before the goal) but also in state D2 (two steps before the goal). Furthermore, a mere engagement in button presses while observing a sequence of stimuli, as in the control experiment, did not evoke the same pupil responses as the main task. Together these results suggested that the single reward at the end of the first episode triggered increases in pupil diameter during later encounters of the same state. The increases observed in state D1 are consistent with an interpretation that pupil diameter reflects state value <inline-formula><mml:math id="inf23"><mml:mi>V</mml:mi></mml:math></inline-formula>, action value <inline-formula><mml:math id="inf24"><mml:mi>Q</mml:mi></mml:math></inline-formula>, or TD error - but do not inform us whether <inline-formula><mml:math id="inf25"><mml:mi>Q</mml:mi></mml:math></inline-formula>-value, <inline-formula><mml:math id="inf26"><mml:mi>V</mml:mi></mml:math></inline-formula>-value, or TD-error are estimated by the brain using RL with or without eligibility trace. However, the fact that very similar changes are also observed in state D2 excludes the possibility that the learning-related contribution to the pupil diameter can be predicted by <italic>RL without eligibility trace</italic>.</p><p>While our experiment was not designed to identify whether the pupil response reflects TD-errors or state values, we tried to address this question based on a model-driven analysis of the pupil traces. First, we extracted all pupil responses after the onset of non-goal states and calculated the TD-error (according to the best-fitting model, <italic>Q-λ</italic>, see next section) of the corresponding state transition. We found that the pupil dilation was much larger after transitions with high TD-error compared to transitions with zero TD-error (<xref ref-type="fig" rid="fig6">Figure 6 (a)</xref> and Materials and methods). Importantly, these temporal profiles of the pupil responses to states with high TD-error had striking similarities across the three experimental conditions, whereas the mean response time course was different across the three conditions (<xref ref-type="fig" rid="fig6">Figure 6 (c)</xref>. This suggests that the underlying physiological process causing the TD-error-driven component in the pupil responses was invariant to stimulation details. Second, a statistical analysis including data with low, medium, and high TD-error confirmed the correlation of pupil dilation with TD error (see subsection regression analysis in methods). Third, a further qualitative analysis revealed that TD-error, rather than value itself, was a factor modulating pupil dilation (<xref ref-type="fig" rid="fig6">Figure 6 (b)</xref>.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Reward prediction error (RPE) at non-goal states modulates pupil dilation.</title><p>Pupil traces (in units of standard deviation) from all states except G were aligned at state onset (<inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) and the mean pupil response <inline-formula><mml:math id="inf28"><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> was subtracted (see Materials and methods). (<bold>a</bold>) The deviation from the mean is shown for states where the model predicts <inline-formula><mml:math id="inf29"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (black, dashed) and for states where the model predicts <inline-formula><mml:math id="inf30"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>≥</mml:mo><mml:msup><mml:mn>80</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> percentile (solid, blue). Shaded areas: ± SEM. Thus the pupil dilation reflects the RPE predicted by a reinforcement learning model that spreads value information to nonrewarded states via eligibility traces. (<bold>b</bold>) To qualitatively distinguish pupil correlations with RPE from correlations with state values <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we started from the following observation: the model predicts that RPE decreases over the course of learning (due to convergence), while the state values <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> increase (due to spread of value information). We wanted to observe this qualitative difference in the pupil dilations of subsequent visits of the <italic>same</italic> state. We selected pairs of visits <inline-formula><mml:math id="inf33"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for which the RPE decreased while <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> increased and extracted the pupil measurements of the two visits (again, mean <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is subtracted). The dashed, black curves show the average pupil trace during the <inline-formula><mml:math id="inf37"><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> visit of a state. The solid black curves correspond to the next visit (<inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) of the same state. In the spatial condition, the two curves significantly (<inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>) separate at <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (indicated by the green line). All three conditions show the same trend (with strong significance in the spatial condition), compatible with a positive correlation of pupil response with RPE, but not with state value <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>c</bold>) The mean pupil dilation <inline-formula><mml:math id="inf42"><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is different in each condition, whereas the learning related deviations from the mean (in (<bold>a</bold>) and (<bold>b</bold>)) have similar shapes.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47463-fig6-v2.tif"/></fig></sec><sec id="s2-3"><title>Estimation of the time scale of the behavioral eligibility trace using reinforcement learning models</title><p>Given the behavioral and physiological evidence for <italic>RL with eligibility trace</italic>, we wondered whether our findings are consistent with earlier studies (<xref ref-type="bibr" rid="bib8">Bogacz et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib51">Tartaglia et al., 2017</xref>) where several variants of reinforcement learning algorithms were fitted to the experimental data. We considered algorithms with and (for comparison) without eligibility trace. Eligibility traces <inline-formula><mml:math id="inf43"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be modeled as a memory of past state-action pairs <inline-formula><mml:math id="inf44"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in an episode. At the beginning of each episode all twelve eligibility trace values (two actions for each of the six decision states) were set to <inline-formula><mml:math id="inf45"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. At each discrete time step <inline-formula><mml:math id="inf46"><mml:mi>n</mml:mi></mml:math></inline-formula>, the eligibility of the current state-action pair was set to 1, while that of all others decayed by a factor <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula> according to <xref ref-type="bibr" rid="bib47">Singh and Sutton (1996)</xref><disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mtext>if </mml:mtext><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>γ</mml:mi><mml:mi>λ</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The parameter <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> exponentially discounts a distal reward, as commonly described in neuroeconomics (<xref ref-type="bibr" rid="bib21">Glimcher and Fehr, 2013</xref>) and machine learning (<xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>); the parameter <inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is called the decay factor of the eligibility trace. The limit case <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is interpreted as no memory and represents an instance of <italic>RL without eligibility trace</italic>. Even though the two parameters <inline-formula><mml:math id="inf51"><mml:mi>γ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf52"><mml:mi>λ</mml:mi></mml:math></inline-formula> appear as a product in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> so that the decay of the eligibility trace depends on both, they have different effects in spreading the reward information from one state to the next (cf. <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> in Materials and methods). After many trials, the <inline-formula><mml:math id="inf53"><mml:mi>V</mml:mi></mml:math></inline-formula>-values of states, or <inline-formula><mml:math id="inf54"><mml:mi>Q</mml:mi></mml:math></inline-formula>-values of actions, approach final values which only depend on <inline-formula><mml:math id="inf55"><mml:mi>γ</mml:mi></mml:math></inline-formula>, but not on <inline-formula><mml:math id="inf56"><mml:mi>λ</mml:mi></mml:math></inline-formula>. Given a parameter <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the choice of <inline-formula><mml:math id="inf58"><mml:mi>λ</mml:mi></mml:math></inline-formula> determines how far value information spreads in a single trial. Note that for <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (<italic>RL without eligibility trace</italic>); <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> assigns an eligibility <inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to state D1 in the first episode at the moment of the transition to the goal (while the eligibility at state D2 is 0). These values of eligibility traces lead to a spread of reward information from the goal to state D1, but not to D2, at the end of the first episode in models without eligibilty trace (cf. <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> and subsection <italic>Q-λ model predictions</italic> in methods), hence the qualitative argument for episodes 1 and 2 as sketched in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p><p>We considered eight common algorithms to explain the behavioral data: Four algorithms belonged to the class of <italic>RL with eligibility traces</italic>. The first two, <italic>SARSA-λ</italic> and <italic>Q-λ</italic> (see Materials and methods, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>) implement a memory of past state-action pairs by an eligibility trace as defined in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>; as a member of the Policy-Gradient family, we implemented a variant of <italic>Reinforce</italic> (<xref ref-type="bibr" rid="bib56">Williams, 1992</xref>; <xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>), which memorizes all state-action pairs of an episode. A fourth algorithm with eligibility trace is the 3-step Q-learning algorithm (<xref ref-type="bibr" rid="bib54">Watkins, 1989</xref>; <xref ref-type="bibr" rid="bib32">Mnih et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>), which keeps memory of past states and actions over three steps (see Discussion and Materials and methods). From the model-based family of RL, we chose the <italic>Forward Learner</italic> (<xref ref-type="bibr" rid="bib20">Gläscher et al., 2010</xref>), which memorizes not state-action pairs, but learns a state-action-next-state model, and uses it for offline updates of action-values. The <italic>Hybrid Learner</italic> (<xref ref-type="bibr" rid="bib20">Gläscher et al., 2010</xref>) combines the <italic>Forward Learner</italic> with <italic>SARSA-0</italic>. As a control, we chose two algorithms belonging to the class of <italic>RL without eligibility traces</italic> (thus modeling the null hypothesis): <italic>SARSA-0</italic> and <italic>Q-0</italic>.</p><p>We found that the four RL algorithms with eligibility trace explained human behavior better than the <italic>Hybrid Learner</italic>, which was the top-scoring among all other RL algorithms. Cross-validation confirmed that our ranking based on the Akaike Information Criterion (AIC, <xref ref-type="bibr" rid="bib1">Akaike, 1974</xref>; see Materials and methods) was robust. According to the Wilcoxon rank-sum test, the probability that the <italic>Hybrid Learner</italic> ranks better than one of the three RL algorithms with explicit eligibility traces was below 14% in each of the conditions and below 0.1% for the aggregated data (<inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="table" rid="table1">Table 1</xref> and Materials and methods). The models <italic>Q</italic>-λ and <italic>SARSA</italic>-λ with eligbility trace performed each significantly better than the corresponding models <italic>Q-0</italic> and <italic>SARSA-0</italic> without eligbility trace.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Models with eligibility trace explain behavior significantly better than alternative models.</title><p>Four reinforcement learning models with eligibility trace (<italic>Q-λ</italic>, REINFORCE, <italic>SARSA-λ</italic>, <italic>3-step-Q</italic>); two model-based algorithms (<italic>Hybrid</italic>, <italic>Forward Learner</italic>), two RL models without eligibility trace (<italic>Q-0</italic>, <italic>SARSA-0</italic>), and a null-model (<italic>Biased Random</italic>, Materials and methods) were fitted to the human behavior, separately for each experimental condition (<italic>spatial, sound, clip-art</italic>). Models with eligibility trace ranked higher than those without (lower Akaike Information Criterion, AIC, evaluated on all participants performing the condition). <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> indicates the <italic>normalized Akaike weights</italic> (<xref ref-type="bibr" rid="bib13">Burnham and Anderson, 2004</xref>), values &lt; 0.01 are not added to the table. Note that only models with eligibility trace have <inline-formula><mml:math id="inf63"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>. The ranking is stable as indicated by the sum of <inline-formula><mml:math id="inf64"><mml:mi>k</mml:mi></mml:math></inline-formula> rankings (column <italic>rank sum</italic>) on test data, in <inline-formula><mml:math id="inf65"><mml:mi>k</mml:mi></mml:math></inline-formula>-fold crossvalidation (Materials and methods). P-values refer to the following comparisons: P(a): Each model in the <italic>with eligibility trace</italic> group was compared with the best model <italic>without eligibility trace</italic> (<italic>Hybrid</italic> in all conditions); models for which the comparison is significant are shown in bold. P(b): <italic>Q-0</italic> compared with <italic>Q-λ</italic>. P(c): <italic>SARSA-0</italic> compared with <italic>SARSA-λ</italic>. P(d): <italic>Biased Random</italic> compared with the second last model, which is <italic>Forward Learner</italic> in the <italic>clip-art</italic> condition and <italic>SARSA-0</italic> in the two others. In the <italic>Aggregated</italic> column, we compared the same pairs of models, taking into account all ranks across the three conditions. All algorithms with eligibility trace explain the human behavior better (p(e)&lt;.001) than algorithms without eligibility trace. Differences among the four models with eligibility trace are not significant. In each comparison, <inline-formula><mml:math id="inf66"><mml:mi>k</mml:mi></mml:math></inline-formula> pairs of individual ranks are used to compare pairs of models and obtain the indicated p-values (Wilcoxon rank-sum test, Materials and methods).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" valign="top">Condition</th><th align="left" colspan="2" valign="top">Spatial</th><th align="left" colspan="2" valign="top">Sound</th><th align="left" colspan="2" valign="top">Clip-art</th><th align="left" valign="top">Aggregated</th></tr></thead><tbody><tr><th align="center" colspan="2">Model</th><td align="left" valign="top">AIC</td><td align="right" valign="top">Rank Sum (k = 11)</td><td align="left" valign="top">AIC</td><td align="right" valign="top">Rank Sum (k = 7)</td><td align="left" valign="top">AIC</td><td align="right" valign="top">Rank Sum (k = 7)</td><td align="left" valign="bottom">all ranks</td></tr><tr><td align="left" rowspan="4" valign="middle">With elig tr.</td><td align="left" valign="top">Q-λ</td><td align="left" valign="top"><inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mn mathvariant="bold">6470.2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1.00</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">=</mml:mo><mml:mn mathvariant="bold">.003</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">24</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mn mathvariant="bold">1489.1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>0.23</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">=</mml:mo><mml:mn mathvariant="bold">.015</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">20</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mn>1234.8</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>0.27</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.062</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msup><mml:mn mathvariant="bold">64</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">e</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">&lt;</mml:mo><mml:mn mathvariant="bold">.001</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="top">Reinforce</td><td align="left" valign="top"><inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mn mathvariant="bold">6508.7</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">=</mml:mo><mml:mn mathvariant="bold">.016</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">35</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mn mathvariant="bold">1486.8</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>0.74</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">=</mml:mo><mml:mn mathvariant="bold">.015</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">10</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mn>1239.2</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.109</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mn mathvariant="bold">67</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">e</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">&lt;</mml:mo><mml:mn mathvariant="bold">.001</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="top">3-step-Q</td><td align="left" valign="top"><inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mn mathvariant="bold">6488.8</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">=</mml:mo><mml:mn mathvariant="bold">.013</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">33</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mn mathvariant="bold">1494.3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">=</mml:mo><mml:mn mathvariant="bold">.046</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">26</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mn mathvariant="bold">1236.6</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>0.11</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">=</mml:mo><mml:mn mathvariant="bold">.015</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">16</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mn mathvariant="bold">71</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">e</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">&lt;</mml:mo><mml:mn mathvariant="bold">.001</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="top">SARSA-λ</td><td align="left" valign="top"><inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mn mathvariant="bold">6502.4</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">=</mml:mo><mml:mn mathvariant="bold">.003</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">36</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mn>1495.2</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.040</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>30</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mn mathvariant="bold">1233.2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>0.59</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">=</mml:mo><mml:mn mathvariant="bold">.015</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">16</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mn mathvariant="bold">82</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">e</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo><mml:mo mathvariant="bold">&lt;</mml:mo><mml:mn mathvariant="bold">.001</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" rowspan="2" valign="middle">Model based</td><td align="left" valign="top">Hybrid</td><td align="left" valign="top"><inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>6536.6</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>61</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>1498.3</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>43</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>1271.3</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>33</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mn>137</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>.001</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="top">Forward Learner</td><td align="left" valign="top"><inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>6637.5</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>79</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>1500.6</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>41</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>1316.3</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>48</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>168</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" rowspan="2" valign="middle">Without elig tr.</td><td align="left" valign="top">Q-0</td><td align="left" valign="top"><inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>6604.0</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.003</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>60</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>1518.6</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.046</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>39</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>1292.0</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.015</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>51</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>150</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>.001</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="top">SARSA-0</td><td align="left" valign="top"><inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>6643.3</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.001</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>68</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>1520.2</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.093</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>43</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>1289.5</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.015</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>46</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>157</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>.001</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left"/><td align="left" valign="top">Biased Random</td><td align="left" valign="top"><inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>7868.3</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.001</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>99</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>1866.1</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.015</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>63</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>1761.1</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>.015</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="right" valign="top"><inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn>63</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top"><inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>225</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>.001</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>Since the ranks of the four RL algorithms with eligibility traces were not significantly different, we focused on one of these, viz. <italic>Q-λ</italic>. We wondered whether the parameter λ that characterizes the decay of the eligibility trace in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> could be linked to a time scale. To answer this question, we proceeded in two steps. First, we analyzed the human behavior in discrete time steps corresponding to state transitions. We found that the best fitting values (maximum likelihood, see Materials and methods) of the eligibility trace parameter λ were 0.81 in the <italic>clip-art</italic>, 0.96 in the <italic>sound</italic>, and 0.69 in the <italic>spatial</italic> condition (see <xref ref-type="fig" rid="fig7">Figure 7</xref>). These values are all significantly larger than zero (p&lt;0.001) indicating the presence of an eligibility trace consistent with our findings in the previous subsections.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Eligibility for reinforcement decays with a time-scale <inline-formula><mml:math id="inf130"><mml:mi>τ</mml:mi></mml:math></inline-formula> in the order of 10 s.</title><p>The behavioral data of each experimental condition constrain the free parameters of the model <italic>Q-λ</italic> to the ranges indicated by the blue histograms (see methods) (<bold>a</bold>) Distribution over the eligibility trace parameter λ in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> (discrete time steps). Vertical black lines indicate the values that best explain the data (maximum likelihood, see Materials and methods). All values are significantly different from zero. (<bold>b</bold>) Modeling eligibility in continuous time with a time-dependent decay (Materials and methods, <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>), instead of a discrete per-step decay. The behavioral data constrains the time-scale parameter <inline-formula><mml:math id="inf131"><mml:mi>τ</mml:mi></mml:math></inline-formula> to around 10 s. Values in the column <italic>All</italic> are obtained by fitting λ and <inline-formula><mml:math id="inf132"><mml:mi>τ</mml:mi></mml:math></inline-formula> to the aggregated data of all conditions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47463-fig7-v2.tif"/></fig><p>In a second step, we modeled the same action sequence in continuous time, taking into account the measured inter-stimulus interval (ISI) which was the sum of the reaction time plus a random delay of 2.5 to 4 seconds after the push-buttons was pressed. The reaction times were similar in the <italic>spatial</italic>- and <italic>clip-art</italic> condition, and slightly longer in the <italic>sound</italic> condition with the following 10%, 50% and 90% percentiles: <italic>spatial</italic>: [0.40, 1.19, 2.73], <italic>clip-art</italic>: [0.50, 1.11, 2.57], <italic>sound</italic>: [0.67, 1.45, 3.78] seconds. In this continuous-time version of the eligibility trace model, both the discount factor <inline-formula><mml:math id="inf133"><mml:mi>γ</mml:mi></mml:math></inline-formula> and the decay factor <inline-formula><mml:math id="inf134"><mml:mi>λ</mml:mi></mml:math></inline-formula> were integrated into a single time constant <inline-formula><mml:math id="inf135"><mml:mi>τ</mml:mi></mml:math></inline-formula> that describes the decay of the memory of past state-action associations in continuous time. We found maximum likelihood values for <inline-formula><mml:math id="inf136"><mml:mi>τ</mml:mi></mml:math></inline-formula> around 10s (<xref ref-type="fig" rid="fig7">Figure 7</xref>), corresponding to 2 to 3 inter-stimulus intervals. This implies that an action taken 10s before a reward was reinforced and associated with the state in which it was taken – even if one or several decisions happened in between (see Discussion).</p><p>Thus eligibility traces, that is memories of past state-action pairs, decay over about 10s and can be linked to a reward occurring during that time span.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Eligibility traces provide a mechanism for learning temporally extended action sequences from a single reward (one-shot). While one-shot learning is a well-known phenomenon for tasks such as image recognition (<xref ref-type="bibr" rid="bib48">Standing, 1973</xref>; <xref ref-type="bibr" rid="bib9">Brady et al., 2008</xref>) and one-step decision making (<xref ref-type="bibr" rid="bib16">Duncan and Shohamy, 2016</xref>; <xref ref-type="bibr" rid="bib22">Greve et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Rouhani et al., 2018</xref>) it has so far not been linked to Reinforcement Learning (RL) with eligibility traces in multi-step decision making.</p><p>In this study, we asked whether humans use eligibility traces when learning long sequences from delayed feedback. We formulated mutually exclusive hypotheses, which predict directly observable changes in behavior and in physiological measures when learning with or without eligibility traces. Using a novel paradigm, we could reject the null hypothesis of learning without eligibility trace in favor of the alternative hypothesis of learning with eligibility trace.</p><p>Our multi-step decision task shares aspects with earlier work in the neurosciences (<xref ref-type="bibr" rid="bib41">Pessiglione et al., 2006</xref>; <xref ref-type="bibr" rid="bib20">Gläscher et al., 2010</xref>; <xref ref-type="bibr" rid="bib15">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Walsh and Anderson, 2011</xref>; <xref ref-type="bibr" rid="bib35">Niv et al., 2012</xref>; <xref ref-type="bibr" rid="bib37">O'Doherty et al., 2017</xref>), but overcomes their limitations (i) by using a recurrent graph structure of the environment that enables relatively long episodes (<xref ref-type="bibr" rid="bib51">Tartaglia et al., 2017</xref>), and (ii) by implementing an ’on-the-fly’ assignment rule for state-action transitions during the first episodes. This novel design allows the study of human learning in specific and controlled conditions, without interfering with the participant’s free choices.</p><p>A difficulty in the study of eligibility traces, is that in the relatively simple tasks typically used in animal (<xref ref-type="bibr" rid="bib39">Pan et al., 2005</xref>) or human (<xref ref-type="bibr" rid="bib8">Bogacz et al., 2007</xref>; <xref ref-type="bibr" rid="bib23">Gureckis and Love, 2009</xref>; <xref ref-type="bibr" rid="bib15">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Walsh and Anderson, 2011</xref>; <xref ref-type="bibr" rid="bib55">Weinberg et al., 2012</xref>; <xref ref-type="bibr" rid="bib51">Tartaglia et al., 2017</xref>) studies, the two hypotheses make qualitatively different predictions only during the first episodes: At the end of the first episode, algorithms in the class of <italic>RL without eligibility trace</italic> update only the value of state D1 (but not of D2, see <xref ref-type="fig" rid="fig1">Figure 1</xref>, Null hypothesis). Then, this value of D1 will drive learning at state D2 when the participants move from D2 to D1 during episode 2. In contrast, algorithms in the class of <italic>RL with eligibility trace</italic>, update D2 already during episode one. Therefore, only during episode 2, the behavioral data permits a clean, qualitative dissociation between the two classes. On the other hand, the fact that for most episodes, the differences are not qualitative, is the reason why eligibility trace contributions have typically been statistically inferred from many trials through model selection (<xref ref-type="bibr" rid="bib39">Pan et al., 2005</xref>; <xref ref-type="bibr" rid="bib8">Bogacz et al., 2007</xref>; <xref ref-type="bibr" rid="bib23">Gureckis and Love, 2009</xref>; <xref ref-type="bibr" rid="bib15">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Walsh and Anderson, 2011</xref>; <xref ref-type="bibr" rid="bib51">Tartaglia et al., 2017</xref>). Here, by a specific task design and a focus on episodes 1 and 2, we provided directly observable, qualitative, evidence for learning with eligibility traces from behavior and pupil data without the need of model selection.</p><p>In the quantitative analysis, RL models with eligibility trace explained the behavioral data significantly better than the best tested RL models without. There are, however, in the reinforcement learning literature, several alternative algorithms that would also account for one-shot learning but do not rely on the explicit eligibility traces formulated in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. First, <inline-formula><mml:math id="inf137"><mml:mi>n</mml:mi></mml:math></inline-formula>-step reinforcement learning algorithms (<xref ref-type="bibr" rid="bib54">Watkins, 1989</xref>; <xref ref-type="bibr" rid="bib32">Mnih et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>) compare the value of a state not with that of its direct neighbor but of neighbors that are <inline-formula><mml:math id="inf138"><mml:mi>n</mml:mi></mml:math></inline-formula> steps away. These algorithms are closely related to eligibility traces and in certain cases even mathematically equivalent (<xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>). Second, reinforcement learning algorithm with storage of past sequences (<xref ref-type="bibr" rid="bib33">Moore and Atkeson, 1993</xref>; <xref ref-type="bibr" rid="bib7">Blundell et al., 2016</xref>; <xref ref-type="bibr" rid="bib32">Mnih et al., 2016</xref>) enable the offline replay of the first episode so as to update values of states far away from the goal. While these approaches are formally different from eligibility traces, they nevertheless implement the idea of eligibility traces as memory of past state-action pairs (<xref ref-type="bibr" rid="bib14">Crow, 1968</xref>; <xref ref-type="bibr" rid="bib18">Frémaux and Gerstner, 2015</xref>), albeit in a different algorithmic framework. For example, prioritized sweeping with small backups (<xref ref-type="bibr" rid="bib46">Seijen and Sutton, 2013</xref>) is an offline algorithm that is, if applied to our deterministic environment after the end of the first episode, equivalent to both episodic control (<xref ref-type="bibr" rid="bib11">Brea, 2017</xref>) and an eligibility trace. Interestingly, the two model-based algorithms (<italic>Forward Learner</italic> and <italic>Hybrid</italic>) would in principle be able to explain one-shot learning since reward information is spread, after the first episode, throughout the model, via offline <inline-formula><mml:math id="inf139"><mml:mi>Q</mml:mi></mml:math></inline-formula>-value updates. Nevertheless, when behavioral data from our experiments were fitted across all seven episodes, the two model-based algorithms performed significantly worse than the RL models with explicit eligibility traces. Since our experimental design does not allow us to distinguish between these different algorithmic implementations of closely related ideas, we put them all in the class of RL with eligibility traces.</p><p>Importantly, RL algorithms with explicit eligibility traces (<xref ref-type="bibr" rid="bib49">Sutton, 1988</xref>; <xref ref-type="bibr" rid="bib56">Williams, 1992</xref>; <xref ref-type="bibr" rid="bib40">Peng and Williams, 1996</xref>; <xref ref-type="bibr" rid="bib26">Izhikevich, 2007</xref>; <xref ref-type="bibr" rid="bib18">Frémaux and Gerstner, 2015</xref>) can be mapped to known synaptic and circuit mechanisms (<xref ref-type="bibr" rid="bib57">Yagishita et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">He et al., 2015</xref>; <xref ref-type="bibr" rid="bib6">Bittner et al., 2017</xref>; <xref ref-type="bibr" rid="bib17">Fisher et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Gerstner et al., 2018</xref>). A time scale of the eligibility trace of about 10s in our experiments is in the range of, but a bit longer than those observed for dopamine modulated plasticity in the striatum (<xref ref-type="bibr" rid="bib57">Yagishita et al., 2014</xref>), serotonin and norepinephrine modulated plasticity in the cortex (<xref ref-type="bibr" rid="bib25">He et al., 2015</xref>), or complex-spike plasticity in hippocampus (<xref ref-type="bibr" rid="bib6">Bittner et al., 2017</xref>), but shorter than the time scales of minutes reported in hippocampus (<xref ref-type="bibr" rid="bib12">Brzosko et al., 2017</xref>). The basic idea for the relation of eligibility traces as in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> to experiments on synaptic plasticity is that choosing action <inline-formula><mml:math id="inf140"><mml:mi>a</mml:mi></mml:math></inline-formula> in state <inline-formula><mml:math id="inf141"><mml:mi>s</mml:mi></mml:math></inline-formula> leads to co-activation of neurons and leaves a trace at the synapses connecting those neurons. A later phasic neuromodulator signal will transform the trace into a change of the synapses so that taking action <inline-formula><mml:math id="inf142"><mml:mi>a</mml:mi></mml:math></inline-formula> in state <inline-formula><mml:math id="inf143"><mml:mi>s</mml:mi></mml:math></inline-formula> becomes more likely in the future (<xref ref-type="bibr" rid="bib14">Crow, 1968</xref>; <xref ref-type="bibr" rid="bib26">Izhikevich, 2007</xref>; <xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>; <xref ref-type="bibr" rid="bib19">Gerstner et al., 2018</xref>). Neuromodulator signals could include dopamine (<xref ref-type="bibr" rid="bib45">Schultz, 2015</xref>), but reward-related signals could also be conveyed, together with novelty or attention-related signals, by other modulators (<xref ref-type="bibr" rid="bib18">Frémaux and Gerstner, 2015</xref>).</p><p>Since in our paradigm the inter-stimulus interval (ISI) was not systematically varied, we cannot distinguish between an eligibility trace with purely time-dependent, exponential decay, and one that decays discretely, triggered by events such as states or actions. Future research needs to show whether the decay is event-triggered or defined by molecular characteristics, independent of the experimental paradigm.</p><p>Our finding that changes of pupil dilation correlate with reward-driven variables of reinforcement learning (such as value or TD error) goes beyond the changes linked to state recognition reported earlier (<xref ref-type="bibr" rid="bib38">Otero et al., 2011</xref>; <xref ref-type="bibr" rid="bib30">Kucewicz et al., 2018</xref>). Also, since non-luminance related pupil diameter is influenced by the neuromodulator norepinephrine (<xref ref-type="bibr" rid="bib28">Joshi et al., 2016</xref>) while reward-based learning is associated with the neuromodulator dopamine (<xref ref-type="bibr" rid="bib45">Schultz, 2015</xref>), our findings suggest that the roles, and regions of influence, of neuromodulators could be mixed (<xref ref-type="bibr" rid="bib18">Frémaux and Gerstner, 2015</xref>; <xref ref-type="bibr" rid="bib5">Berke, 2018</xref>) and less well segregated than suggested by earlier theories.</p><p>From the qualitative analysis of the pupillometric data of the main experiment (<xref ref-type="fig" rid="fig5">Figure 5</xref>), together with those of the control experiment (<xref ref-type="fig" rid="fig5">Figure 5</xref>), we concluded that changes in pupil dilation reflected a learned, reward-related property of the state. In the context of decision making and learning, pupil dilation is most frequently associated with violation of an expectation in the form of a reward prediction error or stimulus prediction error as in an oddball-task (<xref ref-type="bibr" rid="bib34">Nieuwenhuis et al., 2011</xref>). However, our experimental paradigm was not designed to decide whether pupil diameter correlates stronger with state values or TD-errors. Nevertheless, a more systematic analysis (see Materials and methods and <xref ref-type="fig" rid="fig6">Figure 6</xref>) suggests that correlation of pupil dilation with TD-errors is stronger than correlation with state values.</p><sec id="s3-1"><title>Conclusion</title><p>Eligibility traces are a fundamental factor underlying the human capability of quick learning and adaptation. They implement a memory of past state-action associations and are a crucial element to efficiently solve the credit assignment problem in complex tasks (<xref ref-type="bibr" rid="bib26">Izhikevich, 2007</xref>; <xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>; <xref ref-type="bibr" rid="bib19">Gerstner et al., 2018</xref>). The present study provides both qualitative and quantitative evidence for one-shot sequence-learning with eligibility traces. The correlation of the pupillometric signals with an RL algorithm with eligibility traces suggests that humans not only exploit memories of past state-action pairs in behavior but also assign reward-related values to these memories. The consistency and similarity of our findings across three experimental conditions suggests that the underlying cognitive, or neuromodulatory, processes are independent of the stimulus modality. It is an interesting question for future research to actually identify the neural implementation of these memory traces.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental conditions</title><p>We implemented three different experimental conditions based on the same Markov Decision Process (MDP) of <xref ref-type="fig" rid="fig2">Figure 2(a)</xref>. The conditions only differed in the way the states were presented to the participants. Furthermore, in order to collect enough samples from early trials, where the learning effects are strongest, participants did not perform one long experiment. Instead, after completing seven episodes in the same environment, the experiment paused for 45 s while participants were instructed to close and relax their eyes. Then the experiment restarted with a new environment: the transition graph was reset, a different, unused, stimulus was assigned to each state, and the participant had to explore and learn the new environment. We instructed the participants to reach the goal state as often as possible within a limited time (12 min in the <italic>sound</italic> and <italic>clip-art</italic> condition, 20 min in the <italic>spatial</italic> condition). On average, they completed 48.1 episodes (6.9 environments) in the <italic>spatial</italic> condition , 19.4 episodes (2.7 environments) in the <italic>sound</italic> condition and 25.1 episodes (3.6 environments) in the <italic>clip-art</italic> condition.</p><p>In the <italic>spatial</italic> condition, each state was defined by the location (on an invisible circle) on the screen of a 100 × 260 pixels checkerboard image, flashed for 100 ms, <xref ref-type="fig" rid="fig2">Figure 2(d)</xref>. The goal state was represented by the same rectangular checkerboard, but rotated by 90°. The checkerboard had the same average luminance as the grey background screen. In each new environment, the states were randomly assigned to locations and the checkerboards were rotated (states: 260 × 100 pixels checkerboard, goal: 100 × 260).</p><p>In the <italic>sound</italic> condition, each state was represented by a unique acoustic stimulus (tones and natural sounds) of 300 ms to 600 ms duration. New, randomly chosen, stimuli were used in each environment. At the goal state an applause was played. An experimental advantage of the <italic>sound</italic> condition is that a change in the pupil dilation cannot stem from a luminance change but must be due to a task-specific condition.</p><p>In the <italic>clip-art</italic> condition, each state was represented by a unique 100 × 100 pixel clip-art image that appeared for 300 ms in the center of the screen. For each environment, a new set of images was used, except for the goal state which was always the same (a person holding a trophy) in all experiments.</p><p>The screen resolution was 1920 × 1080 pixels. In all three conditions, the background screen was grey with a fixation cross in the center of the screen. It was rotated from + to × to signal to the participants when to enter their decision by pressing one of two push-buttons (one in the left and the other in the right hand). No lower or upper bound was imposed on the reaction time. The next state appeared after a random delay of 2.5s to 4s after the push-buttons was pressed. Prior to the actual learning task, they performed a few trials to check they all understood the instructions. While the participants performed the <italic>sound</italic>- and <italic>clip-art</italic> conditions, we recorded the pupil diameter using an SMI iViewX high speed video-based eye tracker (recorded at 500 Hz, down-sampled to 100 Hz for the analysis by averaging over five samples). From participants performing the <italic>spatial</italic> condition, we recorded the pupil diameter using a 60 Hz Tobii Pro tracker. An eye tracker calibration protocol was run for each participant. All experiments were implemented using the Psychophysics Toolbox (<xref ref-type="bibr" rid="bib10">Brainard, 1997</xref>).</p><p>The number of participants performing the task was: <italic>sound</italic>: 15; <italic>clip-art</italic>: 12; <italic>spatial</italic>: 22 participants; Control <italic>sound</italic>: 9; Control <italic>clip-art</italic>: 10; Control <italic>spatial</italic>: 12. The participants were recruited from the EPFL students pool. They had normal or corrected-to-normal vision. Experiments were conducted in accordance with the Helsinki declaration and approved by the ethics commission of the Canton de Vaud (164/14 Titre: Aspects fondamentaux de la reconnaissance des objets : protocole général). All participants were informed about the general purpose of the experiment and provided written, informed consent. They were told that they could quit the experiment at any time they wish.</p></sec><sec id="s4-2"><title>Pupil data processing</title><p>Our data processing pipeline followed recommendations described in <xref ref-type="bibr" rid="bib31">Mathôt et al. (2017)</xref>. Eye blinks (including 100 ms before, and 150 ms after) were removed and short blocks without data (up to 500 ms) were linearly interpolated. In all experiments, participants were looking at a fixation cross which reduces artifactual pupil-size changes (<xref ref-type="bibr" rid="bib31">Mathôt et al., 2017</xref>). For each environment, the time-series of the pupil diameter during the seven episodes was extracted and then normalized to zero-mean, unit variance. This step renders the measurements comparable across participants and environments. We then extracted the pupil recordings at each state from 200 ms before to 3000 ms after each state onset and applied subtractive baseline correction where the baseline was taken as the mean in the interval (<inline-formula><mml:math id="inf144"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>100</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf145"><mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>100</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>]. Taking the <inline-formula><mml:math id="inf146"><mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>100</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> into account does not interfere with event-specific effects because they develop only later (&gt;220 ms according to <xref ref-type="bibr" rid="bib31">Mathôt et al., 2017</xref>); but a symmetric baseline reduces small biases when different traces have different slopes around t = 0 ms. We considered event-locked pupil responses with z-values outside ±3 as outliers and excluded them from the main analysis. We also excluded pupil traces with less than 50% eye-tracker data within the time window of interest, because very short data fragments do not provide information about the characteristic time course of the pupil trace after stimulus onset. As a control, <xref ref-type="fig" rid="fig8">Figure 8</xref> shows that the conclusions of our study are not affected if we drop the two conditions and include all data.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Results including low-quality pupil traces.</title><p>We repeated the pupil data analysis at the crucial state D2including all data (including traces with less than 50% of data within the 3s window and with z-values outside ±3). Gray curves in the background show all recorded pupil traces. The highlighted blue curves show a few, randomly selected, low-quality pupil traces. Including these traces does not affect the result.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47463-fig8-v2.tif"/></fig></sec><sec id="s4-3"><title>Action assignment in the Markov Decision Process</title><p>Actions in the graph of <xref ref-type="fig" rid="fig2">Figure 2</xref> were assigned to transitions during the first few actions as explained in the main text. However, our learning experiment would become corrupted if participants would discover that in the first episode any three actions lead to the goal. First, such knowledge would bypass the need to actually learn state-action associations, and second, the knowledge of ‘distance-to-goal’ implicitly provides reward information even before seeing the goal state. We avoided the learning of the latent structure by two manipulations: First, if in episode 1 of a new environment a participant repeated the exact same action sequence as in the previous environment, or if they tried trivial action sequences (a-a-a or b-b-b); the assignment of the third action led from state D1 to Z, rather than to the Goal. This was the case in about 1/3 of the first episodes (<italic>spatial</italic>: 48/173, <italic>sound</italic>: 20/53 <italic>clip-art</italic>: 23/49). The manipulation further implied that participants had to make decisions against their potential left/right bias. Second, an additional state H (not shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>) was added in episode 1 in some environments (<italic>spatial</italic>: 23/173, <italic>sound</italic>: 6/53 <italic>clip-art</italic>: 8/49). Participants then started from H (always leading to S) and the path length to goal was four steps. Interviews after the experiment showed that no participant became aware of the experimental manipulation and, importantly, they did not notice that they could reach the goal with a random action sequence in episode 1.</p></sec><sec id="s4-4"><title>Reinforcement Learning models</title><p>For the RL algorithm <inline-formula><mml:math id="inf147"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula> (see Algorithm 1); four quantities are important: the reward <inline-formula><mml:math id="inf148"><mml:mi>r</mml:mi></mml:math></inline-formula>; the value <inline-formula><mml:math id="inf149"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of a state-action association such as taking action ’b’ in state D2; the value <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the state itself, defined as the larger of the two <inline-formula><mml:math id="inf151"><mml:mi>Q</mml:mi></mml:math></inline-formula>-values in that state, that is <inline-formula><mml:math id="inf152"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>max</mml:mi><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:msub><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>; and the TD-error (also called Reward Prediction Error or RPE) calculated at the end of the <inline-formula><mml:math id="inf153"><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> action after the transition from state <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf155"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>RPE</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>→</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mo rspace="4.2pt">⋅</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf156"><mml:mi>γ</mml:mi></mml:math></inline-formula> is the discount factor and <inline-formula><mml:math id="inf157"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the estimate of the discounted future reward that can maximally be collected when starting from state <inline-formula><mml:math id="inf158"><mml:mi>s</mml:mi></mml:math></inline-formula>. Note that RPE is different from reward. In our environment a reward occurs only at the transition from state D1 to state G whereas reward prediction errors occur in episodes 2–7 also several steps before the reward location is reached.</p><p>The table of values <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is initialized at the beginning of an experiment and then updated by combining the RPE and the eligibility traces <inline-formula><mml:math id="inf160"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> defined in the main text (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>);<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>←</mml:mo><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf161"><mml:mi>α</mml:mi></mml:math></inline-formula> is the learning rate. Note that <italic>all</italic> <inline-formula><mml:math id="inf162"><mml:mi>Q</mml:mi></mml:math></inline-formula>-values are updated, but changes in <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are proportional to the eligibility of the state-action pair <inline-formula><mml:math id="inf164"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In the literature the table <inline-formula><mml:math id="inf165"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is often initialized with zero, but since some participants pressed the left (or right) button more often than the other one, we identified for each participant the preferred action <inline-formula><mml:math id="inf166"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and initialized <inline-formula><mml:math id="inf167"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with a small bias <inline-formula><mml:math id="inf168"><mml:mi>b</mml:mi></mml:math></inline-formula>, adapted to the data.</p><p>Action selection exploits the <inline-formula><mml:math id="inf169"><mml:mi>Q</mml:mi></mml:math></inline-formula>-values of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> using a softmax criterion with temperature <inline-formula><mml:math id="inf170"><mml:mi>T</mml:mi></mml:math></inline-formula>:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:munder><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>As an alternative to the eligibility trace defined in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, where the eligibility decays at each discrete time-step, we also modeled a decay in continuous time, defined as<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+2.8pt"><mml:mi>if</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>and zero otherwise. Here, <inline-formula><mml:math id="inf171"><mml:mi>t</mml:mi></mml:math></inline-formula> is the time stamp of the current discrete step, and <inline-formula><mml:math id="inf172"><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the time stamp of the last time a state-action pair <inline-formula><mml:math id="inf173"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> has been selected. The discount factor <inline-formula><mml:math id="inf174"><mml:mi>γ</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> is kept, while in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> a potential discounting is absorbed into the single parameter <inline-formula><mml:math id="inf175"><mml:mi>τ</mml:mi></mml:math></inline-formula>.</p><p>Our implementation of <italic>Reinforce</italic> followed the pseudo-code of <italic>REINFORCE: Monte-Carlo Policy-Gradient Control</italic> (<italic>without baseline</italic>) (<xref ref-type="bibr" rid="bib50">Sutton and Barto, 2018</xref>), Chapter 13.3) which updates the action-selection probabilities at the end of each episode. This requires the algorithm to keep a (non-decaying) memory of the complete state-action history of each episode. We refer to <xref ref-type="bibr" rid="bib40">Peng and Williams (1996)</xref>, <xref ref-type="bibr" rid="bib20">Gläscher et al. (2010)</xref> and <xref ref-type="bibr" rid="bib50">Sutton and Barto (2018)</xref> for the pseudo-code and in-depth discussions of all algorithms.</p></sec><sec id="s4-5"><title>Parameter fit and model selection</title><p>The main goal of this study was to test the null-hypothesis ’RL without eligibility traces’ from the behavioral responses at states D1 and D2 (<xref ref-type="fig" rid="fig2">Figure 2(e) and (f)</xref>). By the design of the experiment, we collected relatively many data points from the early phase of learning, but only relatively few episodes in total. This contrasts with other RL studies, where participants typically perform longer experiments with hundreds of trials. As a result, the behavioral data we collected from each single participant is not sufficient to reliably extract the values of the model-parameters on a participant-by-participant basis. To find the most likely values of model parameters, we therefore pooled the behavioral recordings of all participants into one data set D.</p><p>Each learning model <inline-formula><mml:math id="inf176"><mml:mi>m</mml:mi></mml:math></inline-formula> is characterized by a set of parameters <inline-formula><mml:math id="inf177"><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For example, our implementation of the <italic>Q-λ</italic> algorithm has five free parameters: the eligibility trace decay <inline-formula><mml:math id="inf178"><mml:mi>λ</mml:mi></mml:math></inline-formula>; the learning rate <inline-formula><mml:math id="inf179"><mml:mi>α</mml:mi></mml:math></inline-formula>; the discount rate <inline-formula><mml:math id="inf180"><mml:mi>γ</mml:mi></mml:math></inline-formula>; the softmax temperature <inline-formula><mml:math id="inf181"><mml:mi>T</mml:mi></mml:math></inline-formula>; and the bias <inline-formula><mml:math id="inf182"><mml:mi>b</mml:mi></mml:math></inline-formula> for the preferred action. For each model <inline-formula><mml:math id="inf183"><mml:mi>m</mml:mi></mml:math></inline-formula>, we were interested in the posterior distribution <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> over the free parameters <inline-formula><mml:math id="inf185"><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:math></inline-formula>, conditioned on the behavioral data of all participants <inline-formula><mml:math id="inf186"><mml:mi>D</mml:mi></mml:math></inline-formula>. This distribution was approximated by sampling using the Metropolis-Hastings Markov Chain Monte Carlo (MCMC) algorithm (<xref ref-type="bibr" rid="bib24">Hastings, 1970</xref>). For sampling, MCMC requires a function <inline-formula><mml:math id="inf187"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which is proportional to <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Choosing a uniform prior <inline-formula><mml:math id="inf189"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and exploiting that <inline-formula><mml:math id="inf190"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is independent of <inline-formula><mml:math id="inf191"><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:math></inline-formula>, we can directly use the model likelihood <inline-formula><mml:math id="inf192"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>∝</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We calculated the likelihood <inline-formula><mml:math id="inf193"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the data as the joint probability of all action selection probabilities obtained by evaluating the model (<xref ref-type="disp-formula" rid="equ1 equ2 equ3 equ4">Equations 1, 2, 3, and 4</xref> in the case of <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) given a parameter sample <inline-formula><mml:math id="inf195"><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:math></inline-formula>. The log likelihood (LL) of the data under the model is<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mpadded><mml:mrow><mml:mpadded width="+1.7pt"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:munderover></mml:mpadded><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the sum is taken over all participants <inline-formula><mml:math id="inf196"><mml:mi>p</mml:mi></mml:math></inline-formula>, all environments <inline-formula><mml:math id="inf197"><mml:mi>j</mml:mi></mml:math></inline-formula>, and all actions <inline-formula><mml:math id="inf198"><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> a participant has taken in the environment <inline-formula><mml:math id="inf199"><mml:mi>j</mml:mi></mml:math></inline-formula>.</p><p>For each model, we collected <inline-formula><mml:math id="inf200"><mml:mrow><mml:msup><mml:mn>100</mml:mn><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></inline-formula> parameter samples (burn-in: 1500; keeping only every <inline-formula><mml:math id="inf201"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> sample; 50 random start positions; proposal density: Gaussian with <inline-formula><mml:math id="inf202"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.004</mml:mn></mml:mrow></mml:math></inline-formula> for temperature <inline-formula><mml:math id="inf203"><mml:mi>T</mml:mi></mml:math></inline-formula> and bias <inline-formula><mml:math id="inf204"><mml:mi>b</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf205"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.008</mml:mn></mml:mrow></mml:math></inline-formula> for all other parameters). From the samples we chose the <inline-formula><mml:math id="inf206"><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msup></mml:math></inline-formula> which maximizes the log likelihood (LL), calculated the <inline-formula><mml:math id="inf207"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and ranked the models accordingly. The <inline-formula><mml:math id="inf208"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of each model is shown in <xref ref-type="table" rid="table1">Table 1</xref>, alongside with the Akaike weights <inline-formula><mml:math id="inf209"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The latter can be interpreted as the probability that the model <inline-formula><mml:math id="inf210"><mml:mi>m</mml:mi></mml:math></inline-formula> is the best model for the data (<xref ref-type="bibr" rid="bib13">Burnham and Anderson, 2004</xref>). Note that the parameter vector <inline-formula><mml:math id="inf211"><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msup></mml:math></inline-formula> could be found by a hill-climbing algorithm toward the optimum, but such an algorithm does not give any indication about the uncertainty. Here, we obtained an approximate conditional posterior distribution <inline-formula><mml:math id="inf212"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each component <inline-formula><mml:math id="inf213"><mml:mi>i</mml:mi></mml:math></inline-formula> of the parameter vector <inline-formula><mml:math id="inf214"><mml:msup><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:math></inline-formula> (cf. <xref ref-type="fig" rid="fig9">Figure 9</xref>). We estimated this posterior for a given parameter <inline-formula><mml:math id="inf215"><mml:mi>i</mml:mi></mml:math></inline-formula> by selecting only the 1% of all samples falling into a small neighborhood: <inline-formula><mml:math id="inf216"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. We determined <inline-formula><mml:math id="inf217"><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:math></inline-formula> such that along each dimension <inline-formula><mml:math id="inf218"><mml:mi>j</mml:mi></mml:math></inline-formula>, the same percentage of samples was kept (about 22%) and the overall number of samples was 1000.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Fitting results: behavioral data constrained the free parameters of <italic>Q-λ</italic>.</title><p>(<bold>a</bold>) For each experimental condition a distribution over the five free parameters is estimated by sampling. The blue histograms show the approximate conditional posterior for each parameter (see Materials and methods). Vertical black lines indicate the values of the five-parameter sample that best explains the data (maximum likelihood, ML). The bottom row (All) shows the distribution over λ when fitted to the aggregated data of all conditions, with other parameters fixed to the indicated value (mean over the three conditions). (<bold>b</bold>) Estimation of a time dependent decay (<inline-formula><mml:math id="inf219"><mml:mi>τ</mml:mi></mml:math></inline-formula> instead of λ) as defined in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47463-fig9-v2.tif"/></fig><p>One problem using the AIC for model selection stems from the fact that there are considerable behavioral differences across participants and the AIC model selection might change for a different set of participants. This is why we validated the model ranking using <inline-formula><mml:math id="inf220"><mml:mi>k</mml:mi></mml:math></inline-formula>-fold cross-validation. The same procedure as before (fitting, then ranking according to AIC) was repeated <inline-formula><mml:math id="inf221"><mml:mi>K</mml:mi></mml:math></inline-formula> times, but now we used only a subset of participants (training set) to fit <inline-formula><mml:math id="inf222"><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:math></inline-formula> and then calculated the <inline-formula><mml:math id="inf223"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and the <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> on the remaining participants (test set). We created the <inline-formula><mml:math id="inf225"><mml:mi>K</mml:mi></mml:math></inline-formula> folds such that each participant appears in exactly one test set and in <inline-formula><mml:math id="inf226"><mml:mrow><mml:mi>K</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> training sets. Also, we kept these splits fixed across models, and evaluated each model on the same split into training and test set. In each fold <inline-formula><mml:math id="inf227"><mml:mi>k</mml:mi></mml:math></inline-formula>, the models were sorted with respect to <inline-formula><mml:math id="inf228"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, yielding <inline-formula><mml:math id="inf229"><mml:mi>K</mml:mi></mml:math></inline-formula> lists of ranks. In order to evaluate whether the difference between two models is significant, we compared their ranking in each fold (Wilcoxon rank-sum test on K matched pairs, p-values shown in <xref ref-type="table" rid="table1">Table 1</xref>). The cross-validation results were summarized by summing the <inline-formula><mml:math id="inf230"><mml:mi>K</mml:mi></mml:math></inline-formula> ranks (<xref ref-type="table" rid="table1">Table 1</xref>). The best rank sum a model could obtain is <inline-formula><mml:math id="inf231"><mml:mi>K</mml:mi></mml:math></inline-formula>, and is obtained if it achieved the first rank in each of the <inline-formula><mml:math id="inf232"><mml:mi>K</mml:mi></mml:math></inline-formula> folds.</p></sec><sec id="s4-6"><title><inline-formula><mml:math id="inf233"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula> model predictions</title><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><tbody><tr valign="top"><td colspan="3"><bold>Algorithm 1</bold> <italic>Q-λ</italic> (and related models): <break/>For <italic>SARSA-λ</italic> we replace the expression <inline-formula><mml:math id="inf234"><mml:mrow><mml:mrow><mml:msub><mml:mi>max</mml:mi><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:msub><mml:mo>⁡</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in line 9 by <inline-formula><mml:math id="inf235"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the action taken in the next state <inline-formula><mml:math id="inf237"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. For <italic>Q-0</italic> and <italic>SARSA-0</italic> we set λ to zero.</td></tr><tr valign="top"><td>1:</td><td colspan="2">Algorithm Parameters: learning rate <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, discount factor <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, eligibility trace decay factor <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, temperature <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of softmax policy <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">b</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for preferred action <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td></tr><tr valign="top"><td>2:</td><td colspan="2">Initialize <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <break/>For preferred action <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> set <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr valign="top"><td>3:</td><td>       <bold>for</bold> each episode <bold>do</bold></td></tr><tr valign="top"><td>4:</td><td colspan="2">              Initialize state <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr valign="top"><td>5:</td><td colspan="2">              Initialize step <inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr valign="top"><td>6:</td><td colspan="2">              <bold>while</bold> <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is not terminal <bold>do</bold></td></tr><tr valign="top"><td>7:</td><td colspan="2">                    Choose action <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> from <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> with softmax policy <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> derived from <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr valign="top"><td>8:</td><td colspan="2">                    Take action <inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and observe <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr valign="top"><td>9:</td><td colspan="2">                    <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">R</mml:mi><mml:mi mathvariant="italic">P</mml:mi><mml:mi mathvariant="italic">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr valign="top"><td>10:</td><td colspan="2">                    <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr valign="top"><td>11:</td><td colspan="2">                    <bold>for all</bold> <inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>do</bold></td></tr><tr valign="top"><td>12:</td><td colspan="2">                           <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr valign="top"><td>13:</td><td colspan="2">                           <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mi>γ</mml:mi><mml:mi>λ</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr valign="top"><td>14</td><td colspan="2">                     <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>The <italic>Q-λ</italic> model (see Algorithm 1), and related models like <italic>ARSA-λ</italic>, have previously been used to explain human data. We used those published results, in particular the parameter values from <xref ref-type="bibr" rid="bib20">Gläscher et al. (2010)</xref>, <xref ref-type="bibr" rid="bib15">Daw et al. (2011)</xref> and <xref ref-type="bibr" rid="bib51">Tartaglia et al. (2017)</xref>, to estimate the effect size, as well as the reliability of the result. The published parameter values have a high variance: they differ across participants and across tasks. We therefore simulated different agents, each with its own parameters, sampled independently from a uniform distribution in the following ranges: <inline-formula><mml:math id="inf266"><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf267"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf268"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf269"><mml:mrow><mml:mi>T</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.125</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (corresponding to an inverse temperature <inline-formula><mml:math id="inf270"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>), and <inline-formula><mml:math id="inf271"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. We then simulated episodes 1 and 2 of the experiment, applied the <inline-formula><mml:math id="inf272"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula> model to calculate the action-selection bias (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) when the agents visit states <inline-formula><mml:math id="inf273"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf274"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and also <inline-formula><mml:math id="inf275"><mml:mi>S</mml:mi></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig10">Figure 10(c)</xref> during episode 2, and sampled a binary decision (action ’a’ or action ’b’) according to the model’s bias. In the same way as in the main behavioral experiment, each agent repeated the experiment four times and we estimated the empirical action-selection bias as the mean of the (simulated) behavioral data over all repetitions of all agents. This mean value depends on the actual realizations of the random variables and its uncertainty is higher when fewer samples are available. We therefore repeated the simulation of <inline-formula><mml:math id="inf276"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> agents 1000 times and plotted the distribution of the empirical means in <xref ref-type="fig" rid="fig10">Figure 10(d)</xref>. The same procedure was repeated for <inline-formula><mml:math id="inf277"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> agents, showing a smaller standard deviation. The simulations showed a relatively large (simulated) effect size at states <inline-formula><mml:math id="inf278"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf279"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. Furthermore, as expected, the action bias decays as a function of the delay between the action and the final reward in episode 1. We then compared the <inline-formula><mml:math id="inf280"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula> model with a member of the class of <italic>RL without eligibility trace</italic>. When the parameter <inline-formula><mml:math id="inf281"><mml:mi>λ</mml:mi></mml:math></inline-formula>, which controls the decay of the eligibility trace, is set to 0, <inline-formula><mml:math id="inf282"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula> turns into <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>−</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (Q-L<italic>earning without eligibility trace</italic> and we can use it to compare the two classes of RL without changing other parameters. Thus, we repeated the simulation for this case (<inline-formula><mml:math id="inf284"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf285"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>) which shows the model predictions under our null hypothesis. <xref ref-type="fig" rid="fig10">Figure 10(d)</xref> shows the qualitative difference between the two classes of RL.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Simulated experiment.</title><p>( <italic>Q-λ </italic>model). (<bold>a</bold>) and (<bold>b</bold>): Task structure (same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>). Simulated agents performed episodes 1 and 2 and we recorded the decisions at states D1 and D2 in episode 2. (<bold>c</bold>): Additionally, we also simulated the model’s behavior at state S, by extending the structure of the (simulated) experiment with a new state R, leading to S. (<bold>d</bold>): We calculated the action-selection bias at states D1, D2 and S during episode 2 from the behavior of <inline-formula><mml:math id="inf286"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> (blue) and <inline-formula><mml:math id="inf287"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> (green) simulated agents. The effect size (observed during episode 2 and visualized in panel (<bold>d</bold>)) decreases when (in episode 1) the delay between taking the action and receiving the reward increases. It is thereby smallest at state S. When setting the model’s eligibility trace parameter <inline-formula><mml:math id="inf288"><mml:mi>λ</mml:mi></mml:math></inline-formula> to 0(red, no ET), the effect at state D1 is not affected (see <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) while at D2 and S the behavior was not reinforced. Horizontal dashed line: chance level 50%. Errorbars: standard deviation of the simulated effect when estimating 1000 times the mean bias from <inline-formula><mml:math id="inf289"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf290"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> simulated agents with individually sampled model parameters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47463-fig10-v2.tif"/></fig></sec><sec id="s4-7"><title>Regression analysis</title><p>The reward prediction error (RPE, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) used for a comparison with pupil data was obtained by applying the algorithm <italic>Q-λ</italic> with the optimal (maximum likelihood) parameters. We chose <italic>Q-λ</italic> for regression because, first, it explained the behavior best across the three conditions and, second, it evaluates the outcome of an action at the onset of the next state (rather than at the selection of the next action as in <italic>SARSA-λ</italic>) which enabled us to compare the model with the pupil traces triggered at the onset of the next state.</p><p>In a first, qualitative, analysis, we split data of all state transitions of all articipants into two groups: all the state transitions where the model predicts an RPE of zero and the twenty percent of state transitions where the model predicts the largest RPE (<xref ref-type="fig" rid="fig6">Figure 6(a)</xref>. We found that the pupil responses looked very different in the two groups, across all three modalities.</p><p>In a second, rigorous, statistical analysis, we tested whether pupil responses were correlated with the RPE across all RPE values, not just those in the two groups with zero and very high RPE. In our experiment, only state G was rewarded; at nongoal states, the RPE depended solely on learned <inline-formula><mml:math id="inf291"><mml:mi>Q</mml:mi></mml:math></inline-formula>-values (<inline-formula><mml:math id="inf292"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). Note that at the first state of each episode the RPE is not defined. We distinguished these three cases in the regression analysis by defining two events 'Start' and 'Goal', as well as a parametric modulation by the reward prediction error at intermediate states. From <xref ref-type="fig" rid="fig5">Figure 5</xref>, we expected significant modulations in the time window <inline-formula><mml:math id="inf293"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>500</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>2500</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> after stimulus onset. We mapped <inline-formula><mml:math id="inf294"><mml:mi>t</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf295"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mn>1500</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>1000</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and used orthogonal Legendre polynomials <inline-formula><mml:math id="inf296"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> up to order <inline-formula><mml:math id="inf297"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig11">Figure 11</xref>) as basis functions on the interval <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. We use the indices <inline-formula><mml:math id="inf299"><mml:mi>p</mml:mi></mml:math></inline-formula> for participant and <inline-formula><mml:math id="inf300"><mml:mi>n</mml:mi></mml:math></inline-formula> for the <inline-formula><mml:math id="inf301"><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> state-on event. With a noise term <inline-formula><mml:math id="inf302"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf303"><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> for the overall mean pupil dilation at <inline-formula><mml:math id="inf304"><mml:mi>t</mml:mi></mml:math></inline-formula>, the regression model for the pupil measurements <inline-formula><mml:math id="inf305"><mml:mi>y</mml:mi></mml:math></inline-formula> is<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>5</mml:mn></mml:munderover><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>→</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mpadded width="+2.8pt"><mml:msub><mml:mi>β</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mpadded><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the participant-independent parameters <inline-formula><mml:math id="inf306"><mml:msub><mml:mi>β</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> were fitted to the experimental data (one independent analysis for each experimental condition). The models for ‘tart state’ and ‘oal state’ are analogous and obtained by replacing the real valued <inline-formula><mml:math id="inf307"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> by a 0/1 indicator for the respective events. By this design, we obtained three uncorrelated regressors with six parameters each.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Detailed results of regression analysis and permutation tests.</title><p>The regressors are <italic>top</italic>: Start state event, <italic>middle</italic>: Goal state event and <italic>bottom</italic>: Reward Prediction Error. We extracted the time course of the pupil dilation in (500 ms, 2500 ms) after state onset for each of the conditions, <italic>clip-art</italic>, <italic>sound</italic> and <italic>spatial</italic>, using Legendre polynomials <inline-formula><mml:math id="inf308"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of orders k = 0 to k = 5 (top row) as basis functions. The extracted weights <inline-formula><mml:math id="inf309"><mml:msub><mml:mi>β</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> (cf. <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>) are shown in each column below the corresponding Legendre polynomial as vertical bars with color indicating the level of significance (red, statistically significant at p&lt;0.05/6 (Bonferroni); orange, p&lt;0.05; black, not significant). Blue histograms summarize shuffled samples obtained by 1000 permutations. Black curves in the leftmost column show the fits with all six Legendre Polynomials, while the red curve is obtained by summing only over the few Legendre Polynomials with significant <inline-formula><mml:math id="inf310"><mml:mi>β</mml:mi></mml:math></inline-formula>. Note the similarity of the pupil responses across conditions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47463-fig11-v2.tif"/></fig><p>Using the regression analysis sketched here, we quantified the qualitative observations suggested by (<xref ref-type="fig" rid="fig6">Figure 6</xref>) and found a significant parametric modulation of the pupil dilation by reward prediction errors at non-goal states (<xref ref-type="fig" rid="fig11">Figure 11</xref>). The extracted modulation profile reached a maximum at around 1–1.5 s ( 1300 ms in the <italic>clip-art</italic>, 1100 ms in the <italic>sound</italic> and 1400 ms in the <italic>spatial</italic> condition); with a strong mean effect size (<inline-formula><mml:math id="inf311"><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> in <xref ref-type="fig" rid="fig11">Figure 11</xref>) of 0.48 (<inline-formula><mml:math id="inf312"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>), 0.41 (<inline-formula><mml:math id="inf313"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.008</mml:mn></mml:mrow></mml:math></inline-formula>) and 0.35 (<inline-formula><mml:math id="inf314"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>), respectively.</p><p>We interpret the pupil traces at the start and the end of each episode (<xref ref-type="fig" rid="fig11">Figure 11</xref>) as markers for additional cognitive processes beyond reinforcement learning which could include correlations with cognitive load (<xref ref-type="bibr" rid="bib3">Beatty, 1982</xref>; <xref ref-type="bibr" rid="bib29">Kahneman and Beatty, 1966</xref>), recognition memory (<xref ref-type="bibr" rid="bib38">Otero et al., 2011</xref>), attentional effort (<xref ref-type="bibr" rid="bib2">Alnæs et al., 2014</xref>), exploration (<xref ref-type="bibr" rid="bib27">Jepma and Nieuwenhuis, 2011</xref>), and encoding of memories (<xref ref-type="bibr" rid="bib30">Kucewicz et al., 2018</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research was supported by Swiss National Science Foundation (no. CRSII2 147636 and no. 200020 165538), by the European Research Council (grant agreement no. 268 689, MultiRules), and by the European Union Horizon 2020 Framework Program under grant agreement no. 720270 and no. 785907 (Human Brain Project, SGA1 and SGA2)</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Validation, Investigation, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Investigation, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Experiments were conducted in accordance with the Helsinki declaration and approved by the ethics commission of the Canton de Vaud (164/14 Titre: Aspects fondamentaux de la reconnaissance des objets : protocole général). All participants were informed about the general purpose of the experiment and provided written, informed consent. They were told that they could quit the experiment at any time they wish.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-47463-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The datasets generated during the current study are available on Dryad (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.j7h6f69">https://doi.org/10.5061/dryad.j7h6f69</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Lehmann</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>HA</given-names></name><name><surname>Liakoni</surname><given-names>V</given-names></name><name><surname>Herzog</surname><given-names>MH</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Preuschoff</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Data from: One-shot learning and behavioral eligibility traces in sequential decision making</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.j7h6f69</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akaike</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>A new look at the statistical model identification</article-title><source>IEEE Transactions on Automatic Control</source><volume>19</volume><fpage>716</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1109/TAC.1974.1100705</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alnæs</surname> <given-names>D</given-names></name><name><surname>Sneve</surname> <given-names>MH</given-names></name><name><surname>Espeseth</surname> <given-names>T</given-names></name><name><surname>Endestad</surname> <given-names>T</given-names></name><name><surname>van de Pavert</surname> <given-names>SHP</given-names></name><name><surname>Laeng</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pupil size signals mental effort deployed during multiple object tracking and predicts brain activity in the dorsal attention network and the locus coeruleus</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/14.4.1</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beatty</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Task-evoked pupillary responses, processing load, and the structure of processing resources</article-title><source>Psychological Bulletin</source><volume>91</volume><fpage>276</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.91.2.276</pub-id><pub-id pub-id-type="pmid">7071262</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname> <given-names>Y</given-names></name><name><surname>Hochberg</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berke</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What does dopamine mean?</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>787</fpage><lpage>793</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0152-y</pub-id><pub-id pub-id-type="pmid">29760524</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bittner</surname> <given-names>KC</given-names></name><name><surname>Milstein</surname> <given-names>AD</given-names></name><name><surname>Grienberger</surname> <given-names>C</given-names></name><name><surname>Romani</surname> <given-names>S</given-names></name><name><surname>Magee</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Behavioral time scale synaptic plasticity underlies CA1 place fields</article-title><source>Science</source><volume>357</volume><fpage>1033</fpage><lpage>1036</lpage><pub-id pub-id-type="doi">10.1126/science.aan3846</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Blundell</surname> <given-names>C</given-names></name><name><surname>Uria</surname> <given-names>B</given-names></name><name><surname>Pritzel</surname> <given-names>A</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Ruderman</surname> <given-names>A</given-names></name><name><surname>Leibo</surname> <given-names>JZ</given-names></name><name><surname>Rae</surname> <given-names>J</given-names></name><name><surname>Wierstra</surname> <given-names>D</given-names></name><name><surname>Hassabis</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Model-free episodic control</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1606.04460">https://arxiv.org/abs/1606.04460</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>McClure</surname> <given-names>SM</given-names></name><name><surname>Li</surname> <given-names>J</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Montague</surname> <given-names>PR</given-names></name><name><surname>Read Montague</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Short-term memory traces for action bias in human reinforcement learning</article-title><source>Brain Research</source><volume>1153</volume><fpage>111</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2007.03.057</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brady</surname> <given-names>TF</given-names></name><name><surname>Konkle</surname> <given-names>T</given-names></name><name><surname>Alvarez</surname> <given-names>GA</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual long-term memory has a massive storage capacity for object details</article-title><source>PNAS</source><volume>105</volume><fpage>14325</fpage><lpage>14329</lpage><pub-id pub-id-type="doi">10.1073/pnas.0803390105</pub-id><pub-id pub-id-type="pmid">18787113</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Brea</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Is prioritized sweeping the better episodic control?</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1606.04460">https://arxiv.org/abs/1606.04460</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brzosko</surname> <given-names>Z</given-names></name><name><surname>Zannone</surname> <given-names>S</given-names></name><name><surname>Schultz</surname> <given-names>W</given-names></name><name><surname>Clopath</surname> <given-names>C</given-names></name><name><surname>Paulsen</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sequential neuromodulation of hebbian plasticity offers mechanism for effective reward-based navigation</article-title><source>eLife</source><volume>6</volume><elocation-id>e27756</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.27756</pub-id><pub-id pub-id-type="pmid">28691903</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burnham</surname> <given-names>KP</given-names></name><name><surname>Anderson</surname> <given-names>DR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Multimodel inference: understanding AIC and BIC in model selection</article-title><source>Sociological Methods and Research</source><volume>33</volume><fpage>261</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1177/0049124104268644</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crow</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Cortical synapses and reinforcement: a hypothesis</article-title><source>Nature</source><volume>219</volume><fpage>736</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.1038/219736a0</pub-id><pub-id pub-id-type="pmid">5667068</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Seymour</surname> <given-names>B</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Model-based influences on humans' choices and striatal prediction errors</article-title><source>Neuron</source><volume>69</volume><fpage>1204</fpage><lpage>1215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id><pub-id pub-id-type="pmid">21435563</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname> <given-names>KD</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Memory states influence value-based decisions</article-title><source>Journal of Experimental Psychology: General</source><volume>145</volume><fpage>1420</fpage><lpage>1426</lpage><pub-id pub-id-type="doi">10.1037/xge0000231</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname> <given-names>SD</given-names></name><name><surname>Robertson</surname> <given-names>PB</given-names></name><name><surname>Black</surname> <given-names>MJ</given-names></name><name><surname>Redgrave</surname> <given-names>P</given-names></name><name><surname>Sagar</surname> <given-names>MA</given-names></name><name><surname>Abraham</surname> <given-names>WC</given-names></name><name><surname>Reynolds</surname> <given-names>JNJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reinforcement determines the timing dependence of corticostriatal synaptic plasticity in vivo</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>334</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00394-x</pub-id><pub-id pub-id-type="pmid">28839128</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frémaux</surname> <given-names>N</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules</article-title><source>Frontiers in Neural Circuits</source><volume>9</volume><elocation-id>85</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2015.00085</pub-id><pub-id pub-id-type="pmid">26834568</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstner</surname> <given-names>W</given-names></name><name><surname>Lehmann</surname> <given-names>M</given-names></name><name><surname>Liakoni</surname> <given-names>V</given-names></name><name><surname>Corneil</surname> <given-names>D</given-names></name><name><surname>Brea</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Eligibility traces and plasticity on behavioral time scales: experimental support of NeoHebbian Three-Factor learning rules</article-title><source>Frontiers in Neural Circuits</source><volume>12</volume><elocation-id>53</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2018.00053</pub-id><pub-id pub-id-type="pmid">30108488</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gläscher</surname> <given-names>J</given-names></name><name><surname>Daw</surname> <given-names>N</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>O'Doherty</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning</article-title><source>Neuron</source><volume>66</volume><fpage>585</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.04.016</pub-id><pub-id pub-id-type="pmid">20510862</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Glimcher</surname> <given-names>PW</given-names></name><name><surname>Fehr</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Neuroeconomics: Decision Making and the Brain</source><publisher-name>Elsevier Inc</publisher-name><pub-id pub-id-type="doi">10.1016/C2011-0-05512-6</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname> <given-names>A</given-names></name><name><surname>Cooper</surname> <given-names>E</given-names></name><name><surname>Kaula</surname> <given-names>A</given-names></name><name><surname>Anderson</surname> <given-names>MC</given-names></name><name><surname>Henson</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Does prediction error drive one-shot declarative learning?</article-title><source>Journal of Memory and Language</source><volume>94</volume><fpage>149</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2016.11.001</pub-id><pub-id pub-id-type="pmid">28579691</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gureckis</surname> <given-names>TM</given-names></name><name><surname>Love</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Short-term gains, long-term pains: how cues about state aid learning in dynamic environments</article-title><source>Cognition</source><volume>113</volume><fpage>293</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2009.03.013</pub-id><pub-id pub-id-type="pmid">19427635</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hastings</surname> <given-names>WK</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Monte carlo sampling methods using markov chains and their applications</article-title><source>Biometrika</source><volume>57</volume><fpage>97</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1093/biomet/57.1.97</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Huertas</surname> <given-names>M</given-names></name><name><surname>Hong</surname> <given-names>SZ</given-names></name><name><surname>Tie</surname> <given-names>X</given-names></name><name><surname>Hell</surname> <given-names>JW</given-names></name><name><surname>Shouval</surname> <given-names>H</given-names></name><name><surname>Kirkwood</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct eligibility traces for LTP and LTD in cortical synapses</article-title><source>Neuron</source><volume>88</volume><fpage>528</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.037</pub-id><pub-id pub-id-type="pmid">26593091</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Izhikevich</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Dynamical Systems in Neuroscience : The Geometry of Excitability and Bursting</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jepma</surname> <given-names>M</given-names></name><name><surname>Nieuwenhuis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Pupil diameter predicts changes in the exploration-exploitation trade-off: evidence for the adaptive gain theory</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>1587</fpage><lpage>1596</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21548</pub-id><pub-id pub-id-type="pmid">20666595</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname> <given-names>S</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Kalwani</surname> <given-names>RM</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Relationships between pupil diameter and neuronal activity in the locus coeruleus, Colliculi, and cingulate cortex</article-title><source>Neuron</source><volume>89</volume><fpage>221</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.028</pub-id><pub-id pub-id-type="pmid">26711118</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahneman</surname> <given-names>D</given-names></name><name><surname>Beatty</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Pupil diameter and load on memory</article-title><source>Science</source><volume>154</volume><fpage>1583</fpage><lpage>1585</lpage><pub-id pub-id-type="doi">10.1126/science.154.3756.1583</pub-id><pub-id pub-id-type="pmid">5924930</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kucewicz</surname> <given-names>MT</given-names></name><name><surname>Dolezal</surname> <given-names>J</given-names></name><name><surname>Kremen</surname> <given-names>V</given-names></name><name><surname>Berry</surname> <given-names>BM</given-names></name><name><surname>Miller</surname> <given-names>LR</given-names></name><name><surname>Magee</surname> <given-names>AL</given-names></name><name><surname>Fabian</surname> <given-names>V</given-names></name><name><surname>Worrell</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pupil size reflects successful encoding and recall of memory in humans</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>4949</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-23197-6</pub-id><pub-id pub-id-type="pmid">29563536</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mathôt</surname> <given-names>S</given-names></name><name><surname>Fabius</surname> <given-names>J</given-names></name><name><surname>Van Heusden</surname> <given-names>E</given-names></name><name><surname>Van der Stigchel</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Safe and sensible baseline correction of pupil-size data</article-title><source>PeerJ Preprints</source><ext-link ext-link-type="uri" xlink:href="https://peerj.com/preprints/2725">https://peerj.com/preprints/2725</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mnih</surname> <given-names>V</given-names></name><name><surname>Badia</surname> <given-names>AP</given-names></name><name><surname>Mirza</surname> <given-names>M</given-names></name><name><surname>Graves</surname> <given-names>A</given-names></name><name><surname>Lillicrap</surname> <given-names>T</given-names></name><name><surname>Harley</surname> <given-names>T</given-names></name><name><surname>Silver</surname> <given-names>D</given-names></name><name><surname>Kavukcuoglu</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Asynchronous methods for deep reinforcement learning</article-title><conf-name>Proceedings of the 33rd International Conference on Machine Learning, PMLR 48</conf-name><fpage>1928</fpage><lpage>1937</lpage><ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v48/mniha16.html">http://proceedings.mlr.press/v48/mniha16.html</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>AW</given-names></name><name><surname>Atkeson</surname> <given-names>CG</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Prioritized sweeping: reinforcement learning with less data and less time</article-title><source>Machine Learning</source><volume>13</volume><fpage>103</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1007/BF00993104</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieuwenhuis</surname> <given-names>S</given-names></name><name><surname>De Geus</surname> <given-names>EJ</given-names></name><name><surname>Aston-Jones</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The anatomical and functional relationship between the P3 and autonomic components of the orienting response</article-title><source>Psychophysiology</source><volume>48</volume><fpage>162</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2010.01057.x</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Edlund</surname> <given-names>JA</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>O'Doherty</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural Prediction Errors Reveal a Risk-Sensitive Reinforcement-Learning Process in the Human Brain</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>551</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5498-10.2012</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Doherty</surname> <given-names>JP</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Friston</surname> <given-names>K</given-names></name><name><surname>Critchley</surname> <given-names>H</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Temporal difference models and reward-related learning in the human brain</article-title><source>Neuron</source><volume>38</volume><fpage>329</fpage><lpage>337</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00169-7</pub-id><pub-id pub-id-type="pmid">12718865</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Doherty</surname> <given-names>JP</given-names></name><name><surname>Cockburn</surname> <given-names>J</given-names></name><name><surname>Pauli</surname> <given-names>WM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning, reward, and decision making</article-title><source>Annual Review of Psychology</source><volume>68</volume><fpage>73</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010416-044216</pub-id><pub-id pub-id-type="pmid">27687119</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otero</surname> <given-names>SC</given-names></name><name><surname>Weekes</surname> <given-names>BS</given-names></name><name><surname>Hutton</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Pupil size changes during recognition memory</article-title><source>Psychophysiology</source><volume>48</volume><fpage>1346</fpage><lpage>1353</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2011.01217.x</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname> <given-names>WX</given-names></name><name><surname>Schmidt</surname> <given-names>R</given-names></name><name><surname>Wickens</surname> <given-names>JR</given-names></name><name><surname>Hyland</surname> <given-names>BI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Dopamine cells respond to predicted events during classical conditioning: evidence for eligibility traces in the reward-learning network</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>6235</fpage><lpage>6242</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1478-05.2005</pub-id><pub-id pub-id-type="pmid">15987953</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname> <given-names>J</given-names></name><name><surname>Williams</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Incremental multi-step Q-learning</article-title><source>Machine Learning</source><volume>22</volume><fpage>283</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1007/BF00114731</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessiglione</surname> <given-names>M</given-names></name><name><surname>Seymour</surname> <given-names>B</given-names></name><name><surname>Flandin</surname> <given-names>G</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dopamine-dependent prediction errors underpin reward-seeking behaviour in humans</article-title><source>Nature</source><volume>442</volume><fpage>1042</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1038/nature05051</pub-id><pub-id pub-id-type="pmid">16929307</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preuschoff</surname> <given-names>K</given-names></name><name><surname>'t Hart</surname> <given-names>BM</given-names></name><name><surname>Einhäuser</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Pupil dilation signals surprise: evidence for noradrenaline's Role in Decision Making</article-title><source>Frontiers in Neuroscience</source><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3389/fnins.2011.00115</pub-id><pub-id pub-id-type="pmid">21994487</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rescorla</surname> <given-names>RA</given-names></name><name><surname>Wagner</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="1972">1972</year><chapter-title>A theory of Pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement</chapter-title><source>Classical Conditioning II: Current Research and Theory</source><publisher-name> Appleton Century Crofts</publisher-name></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouhani</surname> <given-names>N</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dissociable effects of surprising rewards on learning and memory</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>44</volume><fpage>1430</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1037/xlm0000518</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuronal reward and decision signals: from theories to data</article-title><source>Physiological Reviews</source><volume>95</volume><fpage>853</fpage><lpage>951</lpage><pub-id pub-id-type="doi">10.1152/physrev.00023.2014</pub-id><pub-id pub-id-type="pmid">26109341</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Seijen</surname> <given-names>HV</given-names></name><name><surname>Sutton</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Planning by prioritized sweeping with small backups</article-title><conf-name>Proceedings of the 30th International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname> <given-names>SP</given-names></name><name><surname>Sutton</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Reinforcement learning with replacing eligibility traces</article-title><source>Machine Learning</source><volume>22</volume><fpage>123</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1007/BF00114726</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Standing</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Learning 10,000 pictures</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>25</volume><fpage>207</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1080/14640747308400340</pub-id><pub-id pub-id-type="pmid">4515818</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Learning to predict by the methods of temporal differences</article-title><source>Machine Learning</source><volume>3</volume><fpage>9</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1007/BF00115009</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tartaglia</surname> <given-names>EM</given-names></name><name><surname>Clarke</surname> <given-names>AM</given-names></name><name><surname>Herzog</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>What to choose next? A paradigm for testing human sequential decision making</article-title><source>Frontiers in Psychology</source><volume>8</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2017.00312</pub-id><pub-id pub-id-type="pmid">28326050</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walsh</surname> <given-names>MM</given-names></name><name><surname>Anderson</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Learning from delayed feedback: neural responses in temporal credit assignment</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>11</volume><fpage>131</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.3758/s13415-011-0027-0</pub-id><pub-id pub-id-type="pmid">21416212</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walsh</surname> <given-names>MM</given-names></name><name><surname>Anderson</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Learning from experience: event-related potential correlates of reward processing, neural adaptation, and behavioral choice</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>36</volume><fpage>1870</fpage><lpage>1884</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2012.05.008</pub-id><pub-id pub-id-type="pmid">22683741</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Watkins</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Learning from delayed rewards</article-title><publisher-name>Cambridge University</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weinberg</surname> <given-names>A</given-names></name><name><surname>Luhmann</surname> <given-names>CC</given-names></name><name><surname>Bress</surname> <given-names>JN</given-names></name><name><surname>Hajcak</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Better late than never? the effect of feedback delay on ERP indices of reward processing</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>12</volume><fpage>671</fpage><lpage>677</lpage><pub-id pub-id-type="doi">10.3758/s13415-012-0104-z</pub-id><pub-id pub-id-type="pmid">22752976</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning</article-title><source>Machine Learning</source><volume>8</volume><fpage>229</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1007/BF00992696</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yagishita</surname> <given-names>S</given-names></name><name><surname>Hayashi-Takagi</surname> <given-names>A</given-names></name><name><surname>Ellis-Davies</surname> <given-names>GC</given-names></name><name><surname>Urakubo</surname> <given-names>H</given-names></name><name><surname>Ishii</surname> <given-names>S</given-names></name><name><surname>Kasai</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A critical time window for dopamine actions on the structural plasticity of dendritic spines</article-title><source>Science</source><volume>345</volume><fpage>1616</fpage><lpage>1620</lpage><pub-id pub-id-type="doi">10.1126/science.1255514</pub-id><pub-id pub-id-type="pmid">25258080</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yates</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="1966">1966</year><source>Art of Memory</source><publisher-name>Routledge and Kegan Paul</publisher-name></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47463.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Gläscher</surname><given-names>Jan</given-names> </name><role>Reviewer</role></contrib><contrib contrib-type="reviewer"><name><surname>Jepma</surname><given-names>Marieke</given-names> </name><role>Reviewer</role></contrib><contrib contrib-type="reviewer"><name><surname>Ponte Costa</surname><given-names>Rui</given-names> </name><role>Reviewer</role><aff><institution>University of Bristol</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Reinforcement learning models come in two flavors: with and without eligibility traces. Whereas the former require multiple repetitions, the latter models enable reinforcement of entire sequences of actions from a single experience (i.e., one-shot learning). In this paper, the authors use a novel experimental design to explore one-shot learning and eligibility traces during human decision-making. Using pupillary and behavioral responses, as well as computational modeling, the authors show evidence for the existence of one-shot learning and that eligibility traces are a plausible computational mechanism by which this is accomplished. These findings will have broad implications for learning mechanisms that support human decision making.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;One-shot learning and behavioral eligibility traces in sequential decision making&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Jan Gläscher (Reviewer #1), Marieke Jepma (Reviewer #2), and Rui Ponte Costa (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This paper presents behavioral and psychophysiological evidence for eligibility traces in human reinforcement learning (RL). The experimental paradigm used here provides a novel way to discriminate between RL with vs. without eligibility traces. The key results show that behavioral and pupillometry responses are in line with eligibility traces, which is supported by computational modeling. These main results are replicated across three different experiments.</p><p>All reviewers agreed that this is a well-written paper that addresses an interesting and important question. They also found the approach clean and hypothesis-driven, the results convincing, and the modeling comprehensive. After discussion, the reviewers also agreed on a number of issues that would need to be addressed. These are summarized below under “Essential revisions”. Most importantly, all reviewers agreed that the previous literature should be discussed more comprehensively and that some of the results and analytical choices require more discussion. There was some disagreement among reviewers whether an additional control experiment would be necessary, however, all reviewers agreed that adding such data would substantially strengthen the paper.</p><p>Essential revisions:</p><p>1) The study does a good job in ruling out alternative explanations of the primary finding of one-shot learning. However, one crucial question is left unanswered. Does one-shot learning occur because of the experienced reward or because of the specific action sequence in the first trial? In other words, could one-shot learning occur without a reinforcer at the end of the first trial? The role of reward is essential for the eligibility trace argument because eligibility traces work on the reward prediction error (RPE) and if there is no reward, there is no RPE, and hence nothing to learn (with or without eligibility traces). The cleanest way to answer this question is to run a small control experiment in which some subjects get rewarded at the end of the first trial and others do not get a reward. The behavioral prediction from this experiment would be that if there is no reward, there is no bias in action selection at D2. This additional control experiment would be the most appropriate way to address this point. However, the authors might find another clever way to answer the question convincingly. (Some reviewers noted that reaching the end of a trial in itself may act as a reinforcer and that in this case the suggested control experiment may not work as intended.) At the very least this issue needs a thorough discussion.</p><p>2) The authors pooled the data of all participants into one data set before fitting the models, and used cross-validation to deal with potential individual differences. A hierarchical Bayesian approach seems a better way to deal with individual differences. However, redoing the modeling analyses was not considered essential to support the main conclusions, but it would be good if the authors could discuss why they chose to pool the data.</p><p>3) For the novelty of the present work to be properly evaluated, the authors need to better contrast their work with previous work in the Introduction and Discussion. Also, parts of this study could be better presented and made more solid to improve its readability by a general audience and further support the results. Suggestions in this direction are given below.</p><p>3.1) The authors state that their work provides &quot;clear.… signatures&quot; and that it &quot;solves a long-standing question in human reinforcement learning for which so far only indirect evidence was available&quot; and that &quot;a direct test between the classes of RL models with and without eligibility traces has never been performed&quot;. This claim is perhaps too strong. Specially given that there are a number of studies (e.g. Walsh and Anderson, 2011, 2012; Weinberg et al., 2012 and Pan et al., 2005) that have touched (maybe not as directly) on this issue, performing both behavioral studies and comparing different computational models. These studies should be briefly reviewed in the Introduction and clearly stated what is novel in this new study. Also, please tone down the conclusions regarding &quot;clear&quot; and &quot;solves&quot; a problem.</p><p>3.2) Previous studies have looked at more direct signals such as ERPs and single-unit recordings (Pan et al., 2005), which provide a more direct measurement of putative eligibility traces. Pupil dilation is an interesting signal to look at, but it is known to correlate with many behavioral signals as discussed by the authors (e.g. expected reward, reward prediction error, surprise and risk). So it is not clear how this signal can directly or clearly support the claims. The authors do a good job in showing that pupil dilation is better correlated with TD-error than with other factors, but how these results relate to ERP and single unit recordings should be discussed.</p><p>3.3) Central to this work is comparing models with and without eligibility traces. This comparison should be better illustrated. At present this is done in Table 1 and in schematic form in Figure 1B, rather than exact results from the models. Given how central this is to the paper, it would be better to use a figure for this: illustrating the different model predictions, explicitly, and plotting the model selection scores. For the model selection scores, please show the score as an evidence ratio (or similar; see for use cases Costa et al. 2013 Frontiers; Turkheimer et al. 2003 J. Cereb. Blood Flow Metab; Nakagawa and Hauber, 2011 Neurosci. Biobeh. Rev), which is a relative ranking of the AIC weights.</p><p>4) Participants were allowed to solve more than two episodes, however, the paper only highlights the first two. Is the reason for this that only the first two episodes are clearly testing for eligibility traces? In any way, eligibility trace models predict that learning decays as function of time from the reward (older actions would exhibit weaker learning). As the authors may already have some of this data in place, it would be interesting to show them to further support the point, or at least discuss it.</p><p>5) It would be important to also show the results without removing some of the pupil responses as indicated in the Materials and methods (or with a less strict method). For instance, the authors could gradually vary the two exclusion criteria that they use (&lt;50% eye-tracker data, z-values outside +/-3) and show how their key results vary as a function of that. This could be a new supplementary figure. The results seem robust enough, but this would make the study more complete.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47463.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The study does a good job in ruling out alternative explanations of the primary finding of one-shot learning. However, one crucial question is left unanswered. Does one-shot learning occur because of the experienced reward or because of the specific action sequence in the first trial? In other words, could one-shot learning occur without a reinforcer at the end of the first trial? The role of reward is essential for the eligibility trace argument because eligibility traces work on the reward prediction error (RPE) and if there is no reward, there is no RPE, and hence nothing to learn (with or without eligibility traces). The cleanest way to answer this question is to run a small control experiment in which some subjects get rewarded at the end of the first trial and others do not get a reward. The behavioral prediction from this experiment would be that if there is no reward, there is no bias in action selection at D2. This additional control experiment would be the most appropriate way to address this point. However, the authors might find another clever way to answer the question convincingly. (Some reviewers noted that reaching the end of a trial in itself may act as a reinforcer and that in this case the suggested control experiment may not work as intended.) At the very least this issue needs a thorough discussion.</p></disp-quote><p>As suggested, we performed a new control experiment. Its design is very close to the current experiment, but now, participants visit the crucial state D2 a second time before receiving a reward. The new Figure 5 “Control Experiment without reward” shows the task-design and the results. The results are discussed in the subsection “Behavioral evidence for one-shot learning”. In short, in the absence of reward, we observe only a weak, non-significant, action repetition bias.</p><disp-quote content-type="editor-comment"><p>2) The authors pooled the data of all participants into one data set before fitting the models, and used cross-validation to deal with potential individual differences. A hierarchical Bayesian approach seems a better way to deal with individual differences. However, redoing the modeling analyses was not considered essential to support the main conclusions, but it would be good if the authors could discuss why they chose to pool the data.</p></disp-quote><p>The main goal of this study was to test (and ideally reject) the null-hypothesis “RL without eligibility traces” from the behavioral responses at states D1 and D2 (Figures 2E and F). By the design of the experiment, we collected relatively many data points from the early phase of learning, but only relatively few episodes in total. This contrasts with other RL studies, where participants typically perform longer experiments with hundreds of trials. As a result, the behavioral data we collected from each single participant is not sufficient to reliably extract the values of the free model-parameters for each participant and, consequently, a meaningful log-likelihood cannot be calculated on the basis of individual data.</p><p>On the other hand, the pooled behavioral data rejects the null-hypothesis based on the crucial episodes one and two. We then performed an additional statistical analysis of the pooled data to calculate the support for each of the candidate models. While the data is not sufficient to make strong claims about the models used by the participants, the results support the rejection of the null-hypothesis.</p><disp-quote content-type="editor-comment"><p>3) For the novelty of the present work to be properly evaluated, the authors need to better contrast their work with previous work in the Introduction and Discussion. Also, parts of this study could be better presented and made more solid to improve its readability by a general audience and further support the results. Suggestions in this direction are given below.</p><p>3.1) The authors state that their work provides &quot;clear.… signatures&quot; and that it &quot;solves a long-standing question in human reinforcement learning for which so far only indirect evidence was available&quot; and that &quot;a direct test between the classes of RL models with and without eligibility traces has never been performed&quot;. This claim is perhaps too strong. Specially given that there are a number of studies (e.g. Walsh and Anderson, 2011 and Anderson, 2012; Weinberg et al.. 2012 and Pan et al., 2005) that have touched (maybe not as directly) on this issue, performing both behavioral studies and comparing different computational models. These studies should be briefly reviewed in the Introduction and clearly stated what is novel in this new study. Also, please tone down the conclusions regarding &quot;clear&quot; and &quot;solves&quot; a problem.</p><p>3.2) Previous studies have looked at more direct signals such as ERPs and single-unit recordings (Pan et al., 2005), which provide a more direct measurement of putative eligibility traces. Pupil dilation is an interesting signal to look at, but it is known to correlate with many behavioral signals as discussed by the authors (e.g. expected reward, reward prediction error, surprise and risk). So it is not clear how this signal can directly or clearly support the claims. The authors do a good job in showing that pupil dilation is better correlated with TD-error than with other factors, but how these results relate to ERP and single unit recordings should be discussed.</p></disp-quote><p>We extended the Introduction by defining the term “direct evidence”. We make it clearer that by “direct” we do not refer to a particular (direct) neurophysiological signal like ERP.</p><p>Instead, our study “directly” exploits the qualitatively different predictions made by the classes of RL algorithms. Then, in the Discussion, we added a new paragraph which contrasts our “direct evidence” with previous studies which provided support for eligibility traces based on a statistical approach.</p><p>In the Introduction we write:</p><p>“This qualitative difference in the second episode (i.e., after a single reward) allows us to draw conclusions about the presence or absence of eligibility traces independently of specific model fitting procedures and independently of the choice of physiological correlates, be it EEG, fMRI, or pupil responses. We therefore refer to these qualitative differences as 'direct' evidence.”</p><p>In the Discussion we write:</p><p>“A difficulty in the study of eligibility traces, is that in the relatively simple tasks typically used in animal (Pan et al., 2005) or human (Daw et al., 2011; Tartaglia, Clarke and Herzog, 2017; Walsh and Anderson, 2011; Bogacz et al., 2007; Weinberg et al., 2012; Gureckis and Love, 2009) studies, the two hypotheses make qualitatively different predictions only during the first episodes: At the end of the first episode, algorithms in the class of RL without eligibility trace update only the value of state D1 (but not of D2. see Figure 1, Null hypothesis). […] Here, by a specific task design and a focus on episodes one and two, we provided directly observable, qualitative, evidence for learning with eligibility traces from behavior and pupil data without the need of model selection.”</p><disp-quote content-type="editor-comment"><p>3.3) Central to this work is comparing models with and without eligibility traces. This comparison should be better illustrated. At present this is done in Table 1 and in schematic form in Figure 1B, rather than exact results from the models. Given how central this is to the paper, it would be better to use a figure for this: illustrating the different model predictions, explicitly, and plotting the model selection scores. For the model selection scores, please show the score as an evidence ratio (or similar; see for use cases Costa et al. 2013 Frontiers; Turkheimer et al. 2003 J. Cereb. Blood Flow Metab; Nakagawa and Hauber, 2011 Neurosci. Biobeh. Rev), which is a relative ranking of the AIC weights.</p></disp-quote><p>We added a relative measure to Table 1: the Akaike weight wAIC of a model can be interpreted as a probability that it is the best model for the data.</p><p>We also added a new paragraph “Q − λ model predictions” and a new Figure 9 “Simulated experiment”, showing a model prediction for different parameter values. We simulated the behavior of artificial agents using a range of previously published parameters and also with the special case λ=0 (no eligibility trace). As suggested by the reviewers, this new figure illustrates the decay of the action-bias as a function of the delay, and also provides a direct comparison between two RL models: Q – λ (with ET) and Q – 0 (without ET).</p><disp-quote content-type="editor-comment"><p>4) Participants were allowed to solve more than two episodes, however, the paper only highlights the first two. Is the reason for this that only the first two episodes are clearly testing for eligibility traces? In any way, eligibility trace models predict that learning decays as function of time from the reward (older actions would exhibit weaker learning). As the authors may already have some of this data in place, it would be interesting to show them to further support the point, or at least discuss it.</p></disp-quote><p>The new paragraph in the Discussion (see also comment on Essential revision #3.1) explains the focus on episodes one and two more clearly.</p><disp-quote content-type="editor-comment"><p>5) It would be important to also show the results without removing some of the pupil responses as indicated in the Materials and methods (or with a less strict method). For instance, the authors could gradually vary the two exclusion criteria that they use (&lt;50% eye-tracker data, z-values outside +/-3) and show how their key results vary as a function of that. This could be a new supplementary figure. The results seem robust enough, but this would make the study more complete.</p></disp-quote><p>At the end of the subsection “Pupil data processing”, we refer to a new Figure 7 “Results including low-quality pupil traces” which includes <italic>all</italic> pupil traces even those where only partial data was available or where z-values lie outside +/-3z. The result of our statistical analysis is not affected by this data.</p><p>Our standard analysis in the main text of the paper (which excludes traces with values outside +/-3z) still incorporates almost all pupil traces, but helps to exclude a few outliers which could be caused by technical artefacts.</p><p>The second rule in the standard analysis of the main text, which excludes traces with less than 50% of data within the time-window, is more delicate. Reducing the threshold to, for example, 10% makes us include very short data fragments which do not provide information about the characteristic time course of the pupil trace after stimulus onset.</p><p>On the other hand, requiring 90% of pupil data during the 3 seconds after stimulus onset would remove many traces because eye-blinks or short periods of lost eye-tracking are quite common. Requiring 50% of data seemed a reasonable compromise.</p></body></sub-article></article>