<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">00699</article-id><article-id pub-id-type="doi">10.7554/eLife.00699</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Segregation of complex acoustic scenes based on temporal coherence</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-2973"><name><surname>Teki</surname><given-names>Sundeep</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-4307"><name><surname>Chait</surname><given-names>Maria</given-names></name><xref ref-type="aff" rid="aff2"/><xref ref-type="fn" rid="equal-contrib">†</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-4871"><name><surname>Kumar</surname><given-names>Sukhbinder</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="aff" rid="aff3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-3578"><name><surname>Shamma</surname><given-names>Shihab</given-names></name><xref ref-type="aff" rid="aff4"/><xref ref-type="aff" rid="aff5"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-5099"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="aff" rid="aff3"/><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Wellcome Trust Centre for Neuroimaging</institution>, <institution>University College London</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff2"><institution content-type="dept">UCL Ear Institute</institution>, <institution>University College London</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff3"><institution content-type="dept">Institute of Neuroscience</institution>, <institution>Newcastle University</institution>, <addr-line><named-content content-type="city">Newcastle upon Tyne</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff4"><institution content-type="dept">The Institute for Systems Research</institution>, <institution>University of Maryland</institution>, <addr-line><named-content content-type="city">College Park</named-content></addr-line>, <country>United States</country></aff><aff id="aff5"><institution content-type="dept">Département d’études cognitive</institution>, <institution>Ecole Normale Supérieure</institution>, <addr-line><named-content content-type="city">Paris</named-content></addr-line>, <country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Angelaki</surname><given-names>Dora</given-names></name><role>Reviewing editor</role><aff><institution>Baylor College of Medicine</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>sundeep.teki@gmail.com</email></corresp><fn fn-type="con" id="equal-contrib"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="pub" publication-format="electronic"><day>23</day><month>07</month><year>2013</year></pub-date><pub-date pub-type="collection"><year>2013</year></pub-date><volume>2</volume><elocation-id>e00699</elocation-id><history><date date-type="received"><day>04</day><month>03</month><year>2013</year></date><date date-type="accepted"><day>16</day><month>06</month><year>2013</year></date></history><permissions><copyright-statement>© 2013, Teki et al</copyright-statement><copyright-year>2013</copyright-year><copyright-holder>Teki et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-00699-v1.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="commentary" xlink:href="10.7554/eLife.01136"/><abstract><object-id pub-id-type="doi">10.7554/eLife.00699.001</object-id><p>In contrast to the complex acoustic environments we encounter everyday, most studies of auditory segregation have used relatively simple signals. Here, we synthesized a new stimulus to examine the detection of coherent patterns (‘figures’) from overlapping ‘background’ signals. In a series of experiments, we demonstrate that human listeners are remarkably sensitive to the emergence of such figures and can tolerate a variety of spectral and temporal perturbations. This robust behavior is consistent with the existence of automatic auditory segregation mechanisms that are highly sensitive to correlations across frequency and time. The observed behavior cannot be explained purely on the basis of adaptation-based models used to explain the segregation of deterministic narrowband signals. We show that the present results are consistent with the predictions of a model of auditory perceptual organization based on temporal coherence. Our data thus support a role for temporal coherence as an organizational principle underlying auditory segregation.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00699.001">http://dx.doi.org/10.7554/eLife.00699.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.00699.002</object-id><title>eLife digest</title><p>Even when seated in the middle of a crowded restaurant, we are still able to distinguish the speech of the person sitting opposite us from the conversations of fellow diners and a host of other background noise. While we generally perform this task almost effortlessly, it is unclear how the brain solves what is in reality a complex information processing problem.</p><p>In the 1970s, researchers began to address this question using stimuli consisting of simple tones. When subjects are played a sequence of alternating high and low frequency tones, they perceive them as two independent streams of sound. Similar experiments in macaque monkeys reveal that each stream activates a different area of auditory cortex, suggesting that the brain may distinguish acoustic stimuli on the basis of their frequency.</p><p>However, the simple tones that are used in laboratory experiments bear little resemblance to the complex sounds we encounter in everyday life. These are often made up of multiple frequencies, and overlap—both in frequency and in time—with other sounds in the environment. Moreover, recent experiments have shown that if a subject hears two tones simultaneously, he or she perceives them as belonging to a single stream of sound even if they have different frequencies: models that assume that we distinguish stimuli from noise on the basis of frequency alone struggle to explain this observation.</p><p>Now, Teki, Chait, et al. have used more complex sounds, in which frequency components of the target stimuli overlap with those of background signals, to obtain new insights into how the brain solves this problem. Subjects were extremely good at discriminating these complex target stimuli from background noise, and computational modelling confirmed that they did so via integration of both frequency and temporal information. The work of Teki, Chait, et al. thus offers the first explanation for our ability to home in on speech and other pertinent sounds, even amidst a sea of background noise.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00699.002">http://dx.doi.org/10.7554/eLife.00699.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>auditory scene analysis</kwd><kwd>temporal coherence</kwd><kwd>psychophysic</kwd><kwd>segregation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>WT091681MA</award-id><principal-award-recipient><name><surname>Teki</surname><given-names>Sundeep</given-names></name><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>093292/Z/10/Z</award-id><principal-award-recipient><name><surname>Chait</surname><given-names>Maria</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 DC 07657</award-id><principal-award-recipient><name><surname>Shamma</surname><given-names>Shihab</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution>Deafness Research UK</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Chait</surname><given-names>Maria</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Experiments with realistic acoustic stimuli have revealed that humans distinguish salient sounds from background noise by integrating frequency and temporal information.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In our daily lives, we are constantly exposed to complex acoustic environments composed of multiple sound sources, for instance, while shopping in crowded markets or listening to an orchestra. Although we do it effortlessly, the separation of such mixtures of sounds into perceptually distinct sound sources is a highly complex task. In spite of being a topic of intense investigation for several decades, the neural bases of auditory object formation and segregation still remain to be fully explained (<xref ref-type="bibr" rid="bib11">Cherry, 1953</xref>; <xref ref-type="bibr" rid="bib41">McDermott, 2009</xref>; <xref ref-type="bibr" rid="bib29">Griffiths et al., 2012</xref>).</p><p>The most commonly used signal for probing auditory perceptual organization is a sequence of two pure tones alternating in time that, under certain conditions, can ‘stream’ or segregate into two sources (<xref ref-type="bibr" rid="bib65">van Noorden, 1975</xref>; <xref ref-type="bibr" rid="bib9">Bregman, 1990</xref>). Much work using these streaming signals has been carried out to elucidate the neural substrates and computations that underlie auditory segregation (<xref ref-type="bibr" rid="bib47">Moore and Gockel, 2012</xref>; <xref ref-type="bibr" rid="bib59">Snyder et al., 2012</xref>). In a series of seminal experiments, Fishman and colleagues recorded multi-unit activity from the auditory cortex of macaques in response to a simple streaming sequence (<xref ref-type="bibr" rid="bib24">Fishman et al., 2001</xref>; <xref ref-type="bibr" rid="bib23">2004</xref>). For large frequency differences and fast presentation rates, which promote two distinct perceptual streams, they observed spatially segregated responses to the two tones. This pattern of segregated cortical activation, proposed to underlie the streaming percept, has since been widely replicated (e.g., <xref ref-type="bibr" rid="bib4">Bee and Klump, 2004</xref>; <xref ref-type="bibr" rid="bib5">2005</xref>; <xref ref-type="bibr" rid="bib43">Micheyl et al., 2007a</xref>; <xref ref-type="bibr" rid="bib6">Bidet-Caulet and Bertrand, 2009</xref>) and attributed to basic physiological principles of frequency selectivity, forward masking and neural adaptation (<xref ref-type="bibr" rid="bib25">Fishman and Steinschneider, 2010a</xref>). These properties are considered to contribute to stream segregation by promoting the activation of distinct neuronal populations in the primary auditory cortex (A1) that are well separated along the tonotopic axis (<xref ref-type="bibr" rid="bib40">McCabe and Denham, 1997</xref>; <xref ref-type="bibr" rid="bib10">Carlyon, 2004</xref>; <xref ref-type="bibr" rid="bib43">Micheyl et al., 2007a</xref>; <xref ref-type="bibr" rid="bib47">Moore and Gockel, 2012</xref>). Human imaging studies that directly correlated the perceptual representation of streaming sequences with brain responses also support the correspondence between the streaming percept and the underlying neural activity in A1 (<xref ref-type="bibr" rid="bib31">Gutschalk et al., 2005</xref>; <xref ref-type="bibr" rid="bib58">Snyder et al., 2006</xref>; <xref ref-type="bibr" rid="bib68">Wilson et al., 2007</xref>; <xref ref-type="bibr" rid="bib15">Cusack, 2005</xref>). However, similar effects have also been demonstrated in the auditory nerve, suggesting that processes contributing to segregation might occur earlier in the ascending auditory pathway rather than be mediated exclusively by the auditory cortex (<xref ref-type="bibr" rid="bib3">Beauvois and Meddis, 1991</xref>; <xref ref-type="bibr" rid="bib53">Pressnitzer et al., 2008</xref>).</p><p>A major drawback of the streaming paradigm is that it uses relatively simple, temporally regular narrowband signals which do not capture the rich spectrotemporal complexity of natural acoustic scenes. Moving beyond streaming, Kidd and colleagues developed a spectrally rich signal referred to as the ‘informational masking’ (IM) stimulus (<xref ref-type="bibr" rid="bib35">Kidd et al., 1994</xref>, <xref ref-type="bibr" rid="bib36">1995</xref>, <xref ref-type="bibr" rid="bib38">2011</xref>; <xref ref-type="bibr" rid="bib37">Kidd and Mason, 2003</xref>). IM refers to a type of non-energetic or central masking that is associated with an increase in detection thresholds due to stimulus uncertainty and target-masker similarity that is distinct from peripheral energetic masking (<xref ref-type="bibr" rid="bib52">Pollack, 1975</xref>; <xref ref-type="bibr" rid="bib19">Durlach et al., 2003</xref>). These multi-tone masking experiments required listeners to detect tonal target signals in the presence of simultaneous multi-tone maskers, often separated by a ‘spectral protection region’ (a certain frequency region around the target with little masker energy) that promoted the perceptual segregation of the target from the masker tones. Results demonstrate that target detection is critically dependent on the width of the spectral protection region, and the ‘density’ of the maskers (<xref ref-type="bibr" rid="bib44">Micheyl et al., 2007b</xref>; <xref ref-type="bibr" rid="bib32">Gutschalk et al., 2008</xref>; <xref ref-type="bibr" rid="bib22">Elhilali et al., 2009b</xref>), and has been hypothesized to rely on the same adaptation-based mechanisms as proposed in the context of simple streaming signals (<xref ref-type="bibr" rid="bib44">Micheyl et al., 2007b</xref>).</p><p>In contrast, the sounds we are required to segregate in everyday life are distinct from the narrowband targets used in streaming and IM paradigms; they are often broadband with multiple frequency components that are temporally correlated and overlap with other signals in the environment (<xref ref-type="bibr" rid="bib42">McDermott and Simoncelli, 2011</xref>). Indeed, the ability of models inspired by such paradigms to explain segregation is currently under debate. Recently, Elhilali et al. (2009a) demonstrated that when the two tones in a streaming signal are presented synchronously, listeners perceive the sequence as one stream irrespective of the frequency separation between the two tones, a result that is inconsistent with predictions based on adaptation-based models. Instead, the authors argued that in addition to separation in feature space, temporal coherence between different elements in the scene is essential for segregation such that temporally incoherent patterns tend to result in a segregated percept while temporal coherence promotes integration (<xref ref-type="bibr" rid="bib56">Shamma et al., 2011</xref>; <xref ref-type="bibr" rid="bib26">Fishman and Steinschneider, 2010b</xref>; <xref ref-type="bibr" rid="bib46">Micheyl et al., 2013a</xref>,<xref ref-type="bibr" rid="bib45">b</xref>).</p><p>To investigate systematically the emergence of an auditory object from a random stochastic background, we developed a new stimulus (Stochastic figure-ground; SFG) consisting of coherent (‘figure’) and randomly varying (‘background’) components that overlap in spectrotemporal space and vary only in their statistics of fluctuation (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; <xref ref-type="bibr" rid="bib62">Teki et al., 2011</xref>). The components comprising the figure vary from trial to trial so that it can be extracted only by integrating across both frequency and time. The appearance of a brief figure embedded in background components thus simulated perception of a coherent object in noisy listening environments. Two spectrotemporal dimensions of the figure were manipulated in each experiment—the ‘coherence’, or the number of repeating components, and the ‘duration’, or the number of chords that comprised the figure.<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.00699.003</object-id><label>Figure 1.</label><caption><title>Examples of Stochastic Figure-Ground stimuli.</title><p>All stimuli in this example contain four identical frequency components (only for illustrative purposes: these were selected randomly in the experiments) with F<sub>coh</sub> = 1016.7 Hz, 2033.4 Hz, 3046.7 Hz, and 4066.8 Hz repeated over 6 chords and indicated by the black arrows. The figure is bound by a black rectangle in each stimulus. (<bold>A</bold>) <italic>Chord duration of 50 ms</italic>: stimulus comprises of 40 consecutive chords each of duration 50 ms with a total duration of 2000 ms. (<bold>B</bold>) <italic>Chord duration of 25 ms</italic>: stimulus comprises of 40 consecutive chords each of duration 25 ms with a total duration of 1000 ms. (<bold>C</bold>) <italic>Ramped figures</italic>: stimulus comprises of 40 consecutive chords each of duration 50 ms each (like <bold>A</bold>) but the frequency components comprising the figure increase in frequency in steps of 2*<italic>I</italic> or 5*<italic>I</italic>, where <italic>I</italic> = 1/24<sup>th</sup> of an octave, represents the resolution of the frequency pool. (<bold>D</bold>) <italic>Isolated figures</italic>: stimulus comprises only of the ‘figure present’ portion without any chords preceding or following the figure. The duration of the stimulus is given by the number of chords. (<bold>E</bold>) <italic>Chords interrupted by noise</italic>: stimulus comprises of 40 consecutive chords alternating with 40 chords comprising of loud, masking broadband white noise, each 50 ms in duration. In experiment 6b, the duration of the noise was varied from 100 ms to 500 ms (see ‘Materials and methods’).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00699.003">http://dx.doi.org/10.7554/eLife.00699.003</ext-link></p></caption><graphic xlink:href="elife-00699-fig1-v1.tif"/></fig></p><p>We used psychophysics to examine listeners’ ability to extract complex figures and tested segregation behavior in the context of various spectral and temporal perturbations. Our results demonstrate that listeners are remarkably sensitive to the emergence of such figures (<xref ref-type="fig" rid="fig2">Figure 2</xref>) and can withstand a variety of stimulus manipulations designed to potentially disturb spectrotemporal integration (<xref ref-type="fig" rid="fig1 fig4">Figures 1B–E and 4</xref>). We also show that a model based on the detection of temporal coherence across frequency channels (<xref ref-type="bibr" rid="bib56">Shamma et al., 2011</xref>, <xref ref-type="bibr" rid="bib55">2013</xref>) accounts for the psychophysical data (<xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The work demonstrates an automatic, highly robust segregation mechanism that is sensitive to temporal correlations across frequency channels.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Experiment 1: chord duration of 50 ms</title><p>In experiment 1, the basic SFG stimulus sequence was used to probe figure-detection performance (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; see ‘Materials and methods’). Listeners’ responses were evaluated to obtain d′ for each combination of coherence and duration of the figure. The results (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) show a clear effect of increasing coherence and duration. Hit rates (not shown) mirror d′ with listeners achieving mean hit rates of 93 ± 2% for the most salient coherence/duration combination. It is notable that the patterns were very brief (longest figure duration was 7 chords or 350 ms), yet very high levels of performance were observed (and without extensive practice). This is consistent with the idea that this task based on the SFG stimulus taps low-level, finely tuned segregation mechanisms.</p></sec><sec id="s2-2"><title>Experiment 2: figure identification</title><p>What underlies this sensitivity? Since ‘figure-absent’ and ‘figure-present’ signals were controlled for overall number of components (see ‘Materials and methods’), a global power increase per se associated with the emergence of the figure, can be discounted as a potential cue. However, it is possible that the decisions of the listeners are based on other changes within the stimulus, for example, the emergence of a figure might be associated with a change in the temporal modulation rate of a few frequency channels. The purpose of experiment 2 was to investigate whether the detection of figures involves a specific figure-ground decomposition, namely whether the figure components are grouped together as a detectable ‘perceptual object’ distinct from the background components, or whether listeners were rather just detecting some low-level changes within the ongoing stimulus. To address this issue, we created stimulus triplets with different background patterns in which each stimulus contained a figure but where figure components were identical in two out of the three signals. Listeners were required to identify the ‘odd’ signal that contained a different figure from the other two signals with identical figures in this AXB psychophysical paradigm (see ‘Materials and methods’ for details). Results (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) indicate that for the very short figure duration (4 chords, or 200 ms) listeners had difficulty with this discrimination task (d′ = 0.31 ± 0.18; not significantly different from 0: p=0.12, t = 1.72), but that performance increased significantly for a figure duration of 8 chords (400 ms; d′ = 1.75 ± 0.34) and reached ceiling for a figure duration of 12 chords (600 ms; d′ = 2.93 ± 0.26). This pattern of results indicates that figure detection in these stimuli is associated with a segregation mechanism whereby coherent components are grouped together as a distinct perceptual object.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.00699.004</object-id><label>Figure 2.</label><caption><title>Behavioral performance in the basic and figure identification task.</title><p>The d′ for experiments 1 (<bold>A</bold>; ‘chord duration of 50 ms’; n = 9) and 2 (<bold>B</bold>; ‘figure identification’; n = 9) are plotted on the ordinate and the duration of the figure (in terms of number of 50 ms long chords) is shown along the abscissa. The coherence of the different stimuli in experiment 1 is color coded according to the legend (inset) while the coherence in experiment 2 was fixed and equal to six. The AXB figure identification task was different from the single interval alternative forced choice experiments: listeners were required to discriminate a stimulus with an ‘odd’ figure from two other stimuli with identical figure components. Error bars signify one standard error of the mean (SEM).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00699.004">http://dx.doi.org/10.7554/eLife.00699.004</ext-link></p></caption><graphic xlink:href="elife-00699-fig2-v1.tif"/></fig></p><sec id="s2-2-1"><title>Temporal coherence modeling</title><p>It is difficult to account for listeners’ performance in experiments 1 and 2 based on the standard, adaptation-based models proposed in the context of the streaming paradigm (<xref ref-type="bibr" rid="bib43">Micheyl et al., 2007a</xref>; <xref ref-type="bibr" rid="bib25">Fishman and Steinschneider, 2010a</xref>). The figure and background in the SFG stimuli overlap in frequency space, thus challenging segregation based on activation of spatially distinct neuronal populations in A1. Furthermore, the data clearly indicate that performance strongly depends on the number of simultaneously repeating frequency components, suggesting a mechanism that is able to integrate across widely spaced frequency channels, an element missing in previous models based on streaming. Instead, the psychophysical data are consistent with a recently proposed model of auditory object formation based on analysis of temporal coherence (<xref ref-type="bibr" rid="bib56">Shamma et al., 2011</xref>).</p><p>The temporal coherence model is based on the idea that a perceptual ‘stream’ emerges when a group of (frequency) channels are coherently activated against the backdrop of other uncorrelated channels (<xref ref-type="bibr" rid="bib57">Shamma and Micheyl, 2010</xref>). In our stimuli, the ‘figure’ (defined by the correlated tones) perceptually stands out against a background of random uncorrelated tones. The temporal coherence model postulates that the figure becomes progressively more salient with more correlated tones in the different frequency channels. To measure this coherence, we computed a correlation matrix across all channels of the spectrogram. In principle, the correlation between the activations of any two channels at time <italic>t</italic> should be computed over a certain time window in the past, of a duration that is commensurate with the rates of tone presentations in the channels; this may range roughly between 2 Hz and 40 Hz depending on the experimental session. Consequently, to estimate the perceptual saliency of the figure segment in our stimuli, we computed the correlation matrix simultaneously for a range of temporal resolutions, and then reported the largest correlation values as explained in more detail below and in ‘Materials and methods’.</p><p>The computations incorporated a spectrotemporal analysis postulated to take place in the auditory cortex (<xref ref-type="bibr" rid="bib12">Chi et al., 2005</xref>). Specifically, temporal modulations in the spectrogram channels were first analyzed with a range of constant-Q modulation filters centered at rates ranging from 2 Hz to 40 Hz (computing in effect a wavelet transform for each channel). The correlation matrix ‘at each rate’ is then defined as the product of all channel pairs derived from the same rate filters (see ‘Materials and methods’ and <xref ref-type="fig" rid="fig3">Figure 3</xref>). The maximum correlation values from each matrix were then averaged and was assumed to reflect the coherence of the activity in the spectrogram channels, and hence the saliency of the figure interval. Note that, as expected, the rate at which the maximum correlations occurred for the different experiments (reported in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) approximately matched the rate of the tones presented during the figure interval.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.00699.005</object-id><label>Figure 3.</label><caption><title>Temporal coherence modeling of SFG stimuli.</title><p>The protocol for temporal coherence analysis is demonstrated here for experiment 5. The procedure was identical for modeling the other experiments. A stimulus containing a figure (here with coherence = 4) as indicated by the arrows (<bold>A</bold>) and another, background only (figure absent) stimulus (<bold>B</bold>) was applied as input to the temporal coherence model. The model performs multidimensional feature analysis at the level of the auditory cortex followed by temporal coherence analysis which generates a coherence matrix for each stimulus as shown in <bold>C</bold> and <bold>D</bold> respectively. The coherence matrix for the stimulus with figure present contains significantly higher cross-correlation values (off the diagonal; enclosed in white square) between the channels comprising repeating frequencies as indicated by the two orthogonal sets of white arrows in <bold>C</bold>. A magnified plot of the coherence matrix for the figure stimulus is shown in <bold>E</bold> where the cross-correlation peaks are highlighted in white boxes. The strength of the cross-correlation is indicated by the heat map next to each figure. The stimulus without a figure, that is, which does not contain any repeating frequencies, does not contain significant cross-correlations. This process is repeated for 500 iterations (N<sub>iter</sub>) for all combinations of coherence and duration. The differences between these two coherence matrices were quantified by computing the maximum cross-correlation for each set of coherence matrices for the figure and the ground stimuli respectively. Temporal coherence was calculated as the difference between the average maxima for the figure and the ground stimuli respectively. The resultant model response is shown for each combination of coherence and duration in <bold>F</bold>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00699.005">http://dx.doi.org/10.7554/eLife.00699.005</ext-link></p></caption><graphic xlink:href="elife-00699-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.00699.006</object-id><label>Figure 3—Figure supplement 1.</label><caption><title>Temporal coherence models for other SFG stimuli.</title><p>The output of the temporal coherence modeling procedure is shown for the remaining psychophysical experiments: (<bold>A</bold>) experiment 2 with 25 ms chords modeled at a rate of 40 Hz; (<bold>B</bold>, <bold>C</bold>) experiments 4a and 4b with ramped figures with step size of 2 and 5 respectively modeled at a rate of 10 Hz; (<bold>D</bold>) experiment 5 with isolated 50 ms chords modeled at a rate of 20 Hz; (<bold>E</bold>, <bold>F</bold>) experiments 6a and 6b with chords interrupted by noise of duration 50 ms and 300 ms modeled at 20 Hz and 3.33 Hz respectively.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00699.006">http://dx.doi.org/10.7554/eLife.00699.006</ext-link></p></caption><graphic xlink:href="elife-00699-fig3-figsupp1-v1.tif"/></fig></fig-group></p><p><xref ref-type="fig" rid="fig3">Figure 3</xref> illustrates the modeling procedure and results for stimuli from experiment 1 (see ‘Materials and methods’ for details of the model). The model successfully accounted for the behavioral data in that, an average cross-correlation based measure was able to systematically distinguish ‘figure-present’ from ‘figure-absent’ (or background) stimuli in a manner that mirrored the behavioral responses. The model’s measure of temporal coherence showed a similar profile and increased with the coherence and the duration of the figure for the different experimental conditions (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). This constitutes the first demonstration of the validity of the temporal coherence model for segregation in complex acoustic scenes.<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.00699.007</object-id><label>Figure 4.</label><caption><title>Behavioral performance in the psychophysics experiments.</title><p>The d′ for experiment 3, 4a (thick lines; ramp step = 2), 4b (thin lines, ramp step = 5), 5, 6a and 6b are shown here, as labeled in each figure (n = 10 for all conditions). The abscissa represents the duration of the figure (<bold>A</bold>–<bold>D</bold>) and the duration of the masking noise in <bold>E</bold>. Note that the maximum duration value in experiments 4a and 4b is larger (9 chords) than in the other experiments. Error bars signify one SEM.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.00699.007">http://dx.doi.org/10.7554/eLife.00699.007</ext-link></p></caption><graphic xlink:href="elife-00699-fig4-v1.tif"/></fig></p></sec></sec><sec id="s2-3"><title>Experiment 3: chord duration of 25 ms</title><p>In experiment 3, the length of each chord in the SFG stimulus was halved to 25 ms, thereby reducing the corresponding durations of the figure and the stimulus (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Here, we aimed to test whether figure-detection performance would be affected by such temporal scaling, that is, whether performance would vary as a function of the total duration of the figure (twice as long in experiment 1 vs experiment 2) or the number of repeating chords that comprised the figure (same in experiments 1 and 2).</p><p>Behavioral results (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) reveal good performance, as in experiment 1. Listeners achieved hit rates of 92 ± 3% for the highest coherence/duration combination used. An ANOVA with coherence and duration as within-subject factors and experimental condition (50 ms vs 25 ms chords) as a between-subject factor revealed no significant effect of condition (F<sub>1,15</sub> = 2; p=0.174), suggesting that performance largely depends on the number of repeating chords irrespective of the time scale. Finally, as expected, model predictions were consistent with the experimental findings. Thus, correlations across the spectrogram channels remained significant, but now occurred at higher rates than in experiment 1 (40 Hz vs 20 Hz), reflecting the faster rate of tone presentations in the figure (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>).</p></sec><sec id="s2-4"><title>Experiment 4: ramped figures</title><p>In the preceding experiments, figure components were identical across several chords. In experiment 4, we manipulated the figure components such that they were not identical across chords but rather ramped, that is, increasing in frequency from one chord to the next (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). The components in the frequency pool used to generate the SFG signals are separated equally by 1/24<sup>th</sup> of an octave; and in the following two experiments we increased the frequency steps from one chord to the next by two times (experiment 4A; <xref ref-type="fig" rid="fig4">Figure 4B</xref>—thick lines) or five times (experiment 4B; <xref ref-type="fig" rid="fig4">Figure 4B</xref>—thin lines) the frequency resolution (i.e., 2/24<sup>th</sup> octave and 5/24<sup>th</sup> octave respectively).</p><p>Performance in these experiments was robust (maximum hit-rates of 0.97 and 0.83 were obtained for figures with coherence equal to 8 and duration equal to 7 for the two ramp levels of 2 and 5 respectively) and a comparison with experiment 1 using an ANOVA with coherence and duration as within-subject factors and experimental condition (repeating vs ramp size 2 vs ramp size 5) as a between-subject factor revealed a significant effect of condition: F<sub>2,25</sub> = 19; p&lt;0.001. Performance was significantly worse for the ramp = 5 vs ramp = 2 condition (F<sub>1,18</sub> = 21, p&lt;0.001), but, remarkably, listeners exhibited above-chance performance even for the steeper slope. This suggests that the segregation mechanisms in question are more susceptible to spectral than temporal perturbations (as in experiments 3, and 6 below) but can still integrate over dynamically changing, rather than fixed, figure components. Finally, as with previous experiments, there were significant correlations among the channels predicting the saliency of the figure. However, the optimal rate at which the correlations occurred here was slightly lower (at 10 Hz; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B,C</xref>) than that of experiment 1 (20 Hz), perhaps because two 50 ms chords are integrated as a single unit to define the ramp.</p></sec><sec id="s2-5"><title>Experiment 5: isolated figures</title><p>The stimuli in previous experiments consisted of a sequence of ‘background-only’ chords, prior to the onset of the figure, and another sequence of ‘background-only’ chords after figure offset. From first principles, segregation could be considered to be mediated by adaptation to the ongoing background statistics and detection of the figure as a deviation from this established pattern. In order to test this hypothesis, in experiment 5, we removed the ‘background’ context which preceded the occurrence of the figure (<xref ref-type="fig" rid="fig1">Figure 1D</xref>; see ‘Materials and methods’ section). The stimulus consisted simply of the chords which contained a figure (between 3 and 7 chords).</p><p>Similar to previous experiments, the results (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) show a marked effect of coherence and duration, and performance improved with increasing salience of the figures with listeners reaching hit rates of 89 ± 5% for the most salient condition. To evaluate behavior with respect to experiment 1, an ANOVA with coherence and duration as within-subject factors and experimental condition (with background vs no background) as a between-subject factor was used which yielded no significant effect of condition: F<sub>1,16</sub> = 0.033; p=0.859, suggesting that the ‘background-only’ chords which preceded the figure did not affect performance.</p><p>Modeling for this experiment replicated the results of experiment 1 in that the correlations increased with the coherence and duration of the figure and showed maximum response at 20 Hz (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1D</xref>), corresponding to the rate of presentation of the chords comprising the figure.</p></sec><sec id="s2-6"><title>Experiment 6a: chords interrupted by noise</title><p>In experiment 6, we incorporated 50 ms of loud, broadband masking noise between successive 50 ms long SFG chords (<xref ref-type="fig" rid="fig1">Figure 1E</xref>), in an attempt to disrupt binding of temporally successive components. If figure detection is accomplished by low level mechanisms which are sensitive to a power increase within certain frequency bands, the addition of the noise bursts would disrupt performance by introducing large power fluctuations across the entire spectrum, thus reducing the overall power differences between channels.</p><p>The results (<xref ref-type="fig" rid="fig4">Figure 4D</xref>) show good behavioral performance (maximum hit rate of 0.93 was obtained for the most salient condition) which varied parametrically with the coherence and duration of the figure. An ANOVA with coherence and duration as within-subject factors and experimental condition (50 ms repeating chords vs 50 ms chords alternating with white noise) as a between-subject factor revealed no significant effect of condition (F<sub>1,17</sub> = 0.004; p=0.953). Interleaving the noise bursts between successive chords does not therefore affect performance.</p><p>Model predictions in this experiment (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1E,F</xref>) are broadly consistent with the findings in that detection became easier with more coherent tones, and with longer figure intervals. The reason is simply because the noise weakens but does not eliminate the correlation among the tones, at least when computed at slower rates.</p></sec><sec id="s2-7"><title>Experiment 6b: chords interrupted by extended noise</title><p>A natural question that arises from the preceding experiment is—what are the temporal limits or the range over which such a higher-order mechanism operates?</p><p>In order to test this question, we gradually varied the duration of the intervening noise bursts between stimulus chords in a set of three related experiments with different durations of noise for a particular combination of coherence (6) and duration (6; see ‘Materials and methods’). Results (<xref ref-type="fig" rid="fig4">Figure 4E</xref>) indicate robust performance for all durations of noise up to 300 ms and surprisingly, supra-threshold performance (d′ = 1.00 ± 0.30; significantly different from 0: p=0.01; t = 3.29) for a noise duration of 500 ms. This remarkable ability of listeners to integrate coherent patterns over 3 s long (in the case of 500 ms noise bursts) suggests that the underlying higher-order mechanisms are very robust over such long time windows. Model predictions of these findings are still possible if correlations are measured over longer windows (or slower rates—e.g., 3.33 Hz as in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1F</xref>).</p><p>Temporal windows of integration, as long as 500 ms, have rarely been reported in the context of auditory object formation in complex scenes such as those examined here. The results suggest the existence of a central mechanism that is not affected by interfering broadband noise that integrates repeating pure tone components as belonging to a distinct object over multiple time scales. The long temporal windows here implicate cortical mechanisms at or beyond primary auditory cortex (see e.g., <xref ref-type="bibr" rid="bib49">Overath et al., 2008</xref> demonstrating a range of ‘cortical windows’ between 20 ms and 300 ms).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We demonstrate fast detection with minimal training of a novel figure-from-ground stimulus comprising an overlapping figure and ground segment where, like natural stimuli, the figure has multiple components that are temporally coherent: they start and stop together. We show that such figures can be detected as objects that can be distinguished, consistent with a high-level mechanism for object detection as opposed to the simple detection of frequency or modulation cues (experiment 2). The mechanism scales in time, in that detection depends on the number of components over time rather than their absolute duration (experiment 3) and can still operate if the object is defined by slowly changing frequency trajectories rather than a fixed set of frequencies (experiment 4). It is robust to interruption of the figure by interleaved noise (experiment 5) and can operate over a remarkably long timescale (experiment 6). Modeling based on temporal coherence demonstrated a similar profile to the behavioral results and constitutes the first demonstration of the compatibility of the temporal coherence model with segregation in complex acoustic scenes.</p><sec id="s3-1"><title>Segregation based on temporal coherence</title><p>The temporal coherence model proposes that segregation is determined not only based on separation in feature-space but rather by the temporal relationship between different elements in the scene, such that temporally coherent elements are grouped together, while temporally incoherent channels with independent fluctuation profiles are perceived as belonging to separate sources (<xref ref-type="bibr" rid="bib56">Shamma et al., 2011</xref>). Specifically, the model incorporates two stages: firstly, a feature analysis stage that performs multidimensional feature analysis by distinct populations of neurons in the auditory cortex that are tuned to a range of temporal modulation rates and spectral resolution scales. Auditory features such as pitch, timbre and loudness are computed by different neuronal groups at this initial stage, the output of which is fed to a second stage that involves analysis of temporal coherence.</p><p>Elhilali et al. carried out work implicating a critical role for temporal coherence in the assignment of common elements within a stream: they showed that a pair of synchronous repeating tones produces the same coherent pattern of modeled central activity as a single stream irrespective of the frequency separation between them, suggesting that temporal coherence is an important factor governing segregation (<xref ref-type="bibr" rid="bib21">Elhilali et al., 2009a</xref>). This was substantiated by direct neurophysiological recordings from ferret auditory cortex which showed that synchronous and alternating cortical responses were equally segregated despite their perceptual differences, and hence that the temporal factors are more important in inducing the one and two streams percept (<xref ref-type="bibr" rid="bib57">Shamma and Micheyl, 2010</xref>; <xref ref-type="bibr" rid="bib54">Pressnitzer et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Shamma et al., 2011</xref>, <xref ref-type="bibr" rid="bib55">2013</xref>).</p><p>In the case of the more complex SFG stimuli, the modeling results suggest that temporal coherence is modulated as a function of the coherence and the duration of the figure in a manner similar to the modulation of figure detection performance. Although this is not causal evidence in favor of the model, it behaves similar to human listeners in complex acoustic conditions as used here. The data suggest temporal coherence as a correlate of stimulus salience by which the brain picks out the most important sounds in busy auditory scenes: a process that may not be computed by dedicated structures but could be achieved by binding across distributed feature channels without significant changes in ensemble activity. Similar accounts of binding in vision based on coherence of the temporal structure have been put forward previously (e.g., <xref ref-type="bibr" rid="bib60">Sporns et al., 1991</xref>; <xref ref-type="bibr" rid="bib1">Alais et al., 1998</xref>; <xref ref-type="bibr" rid="bib8">Blake and Lee, 2005</xref>).</p></sec><sec id="s3-2"><title>Neural bases of temporal coherence analysis</title><p>It is still not known how temporal coherence may be computed and which brain areas perform these computations. Temporal coherence may be implemented by neurons that show strong sensitivity to temporal coherence across distant frequency channels, or by neurons that act as multiplexers and are more selective to particular combinations of inputs (<xref ref-type="bibr" rid="bib21">Elhilali et al., 2009a</xref>; <xref ref-type="bibr" rid="bib56">Shamma et al., 2011</xref>). Elhilali et al. (2009a) sought such cells in the primary auditory cortex of the ferret but were unable to demonstrate any in passively listening animals but have preliminary evidence in behaving ferrets (<xref ref-type="bibr" rid="bib55">Shamma et al., 2013</xref>).</p><p>Although previous brain imaging studies have identified activity in A1 that was correlated with the streaming percept (e.g., <xref ref-type="bibr" rid="bib31">Gutschalk et al., 2005</xref>; <xref ref-type="bibr" rid="bib58">Snyder et al., 2006</xref>; <xref ref-type="bibr" rid="bib68">Wilson et al., 2007</xref>), we found no evidence of modulation of BOLD signal in A1 as a function of figure emergence in a passive listening paradigm (<xref ref-type="bibr" rid="bib62">Teki et al., 2011</xref>). However, we found activity in the intraparietal sulcus (IPS) to be strongly modulated by the salience of the figure, similar to the modulation of temporal coherence observed here. The IPS activation likely reflect bottom-up stimulus-driven processing of figures and is consistent with accumulating literature which suggests that areas outside the conventional auditory system, such as the parietal cortex may have a role in auditory segregation (<xref ref-type="bibr" rid="bib15">Cusack, 2005</xref>; <xref ref-type="bibr" rid="bib18">Dykstra et al., 2011</xref>). Although not relevant to the passive fMRI experiment (<xref ref-type="bibr" rid="bib62">Teki et al., 2011</xref>), attention also influences segregation. In this regard, the parietal cortex is in an ideal position to integrate both bottom-up auditory input as it receives auditory input from the temporoparietal cortex (<xref ref-type="bibr" rid="bib50">Pandya and Kuypers, 1969</xref>; <xref ref-type="bibr" rid="bib17">Divac et al., 1977</xref>; <xref ref-type="bibr" rid="bib33">Hyvärinen, 1982</xref>; <xref ref-type="bibr" rid="bib13">Cohen, 2009</xref>) as well as top-down attentional input from the prefrontal cortex (<xref ref-type="bibr" rid="bib69">Andersen et al., 1985</xref>; <xref ref-type="bibr" rid="bib2">Barbas and Mesulam, 1981</xref>; <xref ref-type="bibr" rid="bib51">Petrides and Pandya, 1984</xref>; <xref ref-type="bibr" rid="bib61">Stanton et al., 1995</xref>). IPS is associated with both bottom-up and top-down attention and is a key structure implicated in saliency map models of visual search (<xref ref-type="bibr" rid="bib39">Koch and Ullman, 1985</xref>; <xref ref-type="bibr" rid="bib34">Itti and Koch, 2001</xref>; <xref ref-type="bibr" rid="bib66">Walther and Koch, 2007</xref>) where low-level feature maps may combine with top-down cognitive biases to represent a global saliency map (<xref ref-type="bibr" rid="bib28">Gottlieb et al., 1998</xref>; <xref ref-type="bibr" rid="bib27">Geng and Mangun, 2009</xref>; <xref ref-type="bibr" rid="bib7">Bisley and Goldberg, 2010</xref>). IPS (and its monkey homologue) has been implicated in mediating object representations, binding of sensory features within and across different modalities, as well as attentional selection.</p><p>We hypothesize that IPS may represent a neural correlate of the figure percept where the representation will depend on the salience of auditory figures. In our model, this perceptual representation depends on the computation of temporal coherence across multiple frequencies that are initially represented in the auditory cortex. Neurophysiological recordings from parietal neurons might in future determine whether such sensory analysis (before perceptual representation) involves parietal neurons or is established in auditory cortex first.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Stochastic figure-ground stimulus</title><p>We developed a new stimulus (Stochastic figure-ground [SFG] stimulus; <xref ref-type="bibr" rid="bib62">Teki et al., 2011</xref>) to model naturally complex situations characterized by a figure and background that overlap in feature space that are only distinguishable by their fluctuation statistics. Contrary to previously used signals, the spectrotemporal properties of the figure vary from trial to trial and the figure can only be extracted by binding the spectral components that comprise the figure across frequency and time.</p><p><xref ref-type="fig" rid="fig1">Figure 1</xref> presents examples of the SFG stimulus which consists of a sequence of random chords, each 50 ms in duration with 0 ms inter-chord-interval, presented for a total duration of 2000 ms (40 consecutive chords). Each chord contains a random number (average: 10 and varying between 5 and 15) of pure tone components. The spectral components are randomly selected from a set of 129 frequencies equally spaced on a logarithmic scale between 179 Hz and 7246 Hz such that the separation between successive components is 1/24<sup>th</sup> of an octave. The onset and offset of each chord are shaped by a 10 ms raised-cosine ramp. In half of these stimuli, a random number of tones are repeated across a certain number of consecutive chords (e.g., in <xref ref-type="fig" rid="fig1">Figure 1</xref>, four components marked by arrows are repeated across 6 chords) which results in the percept of a ‘figure’ that readily pops out of the random tonal background. To eliminate correlation between the number of figure and background components, the figure was realized by first generating the random background and then adding additional, repeating components to the relevant chords. To avoid the problem that the interval containing the figure might, on average, also contain more frequency components, and to prevent listeners from relying on this feature in performing the figure detection task, the remaining 50% of the stimuli (those containing no figure) also included additional tonal components, which were added over a variable number (2–7) of consecutive chords at the same time as when a figure would have appeared. But these additional components changed from chord to chord and did not form a coherent figure.</p><p>In the present study, we parametrically varied the number of consecutive chords over which the tones were repeated (‘duration’) and the number of repeated frequency components (‘coherence’). The onset of the figure was jittered between 15 and 20 chords (750–1000 ms) post stimulus onset.</p></sec><sec id="s4-2"><title>Participants</title><p>All participants tested in this set of experiments reported normal hearing and had no history of audiological or neurological disorders. Experimental procedures were approved by the research ethics committee of University College London (Project ID number: 1490/002), and written informed consent was obtained from each participant. For each experiment we report the number of listeners whose data is included in the final analysis. In each experiment, a few listeners (2–3) were excluded from analysis because of their inability to perform the task. 9 listeners (2 females; aged between 20 and 47 years; mean age: 26.9 years) took part in experiment 1. 9 listeners (6 females; aged between 22 and 28 years; mean age: 23.8 years) participated in experiment 2 based on the AXB design. 10 listeners (5 females; aged between 20 and 36 years; mean age: 25.7 years) took part in experiment 3. 10 listeners (5 females; aged between 23 and 31 years; mean age: 26.8 years) participated in experiment 6a. 27 listeners (Group 1: 9 listeners; 5 females, aged between 19 and 27 years; mean age: 21.1 years; Group 2: 10 listeners; 3 females; aged between 19 and 25 years; mean age: 21.3 years; Group 3: 8 listeners; 3 females; aged between 19 and 29 years; mean age: 22.4 years) participated in experiment 6b. 10 listeners (6 females; aged between 21 and 34 years, and mean age of 24.7 years) participated in experiment 4a with ramp step equal to 2 and another group of 10 listeners (3 females; aged between 20 and 30 years and mean age of 24.5 years) took part in experiment 4b with ramp step of 5. 10 listeners (5 females; aged between 22 and 31 years, mean age: 24.8 years) participated in experiment 5.</p></sec><sec id="s4-3"><title>Stimuli</title><p>SFG stimuli in experiment 1 consisted of a sequence of 50 ms chords with 0 ms inter-chord interval and 2 s duration (40 consecutive chords). The coherence of the figure varied between 1, 2, 4, 6 or 8 and the duration of the figure ranged from 2 to 7 chords. Stimuli for all combinations of coherence and duration were presented in a separate block (total of 30 blocks) where 50% of the trials (50 trials per block) contained a figure.</p><p>The stimuli in experiment 2 consisted of 50 ms chords and a figure coherence value of 6. Figure duration varied between 4, 8 and 12 (in separate blocks). Stimuli, all containing a figure, were presented in triplets as in an AXB design (e.g., <xref ref-type="bibr" rid="bib30">Goldinger, 1998</xref>). The background patterns were different in all three signals but two of them (either A and X or B and X) contained identical figure components. Listeners were required to indicate the ‘odd’ figure (A or B) by pressing a button. Three blocks of 60 trials each were presented for each duration condition.</p><p>Stimuli in experiment 3 were identical to those in experiment 1 except that chord duration was reduced to 25 ms. The coherence of the figure varied between 2, 4, 6 or 8 and the duration of the figure ranged from 2–7 chords resulting in a total of 24 blocks.</p><p>In experiment 4a and 4b, stimuli were similar to those in experiment 1 except that in this condition, the successive frequencies comprising the figure were not identical from one chord to the next but increased across chords in steps of <italic>2*I</italic> or <italic>5*I</italic>, where <italic>I</italic> = 1/24<sup>th</sup> of an octave is the resolution of the frequency pool used to create the SFG stimulus. The coherence of the figure was 4, 6, or 8 and duration was 5, 7 or 9 chords resulting in a total of 9 blocks for each condition. Note that in this experiment, the maximum duration of the figure (9 chords) is longer than the maximum duration of the figure in the remaining experiments (7 chords).</p><p>The stimuli for experiment 5 were same as in experiment 1 except that they comprised of the figure only (3–7 chords or 150–350 ms) without any chords that preceded or succeeded the figure as in previous experiments. The coherence of the figure was 2, 4, 6, or 8 chords and this resulted in a total of 20 blocks.</p><p>For experiment 6a, we modified the SFG stimulus so that successive chords were separated by 50 ms broadband noise burst. The loudness of the noise was set to a level 12 dB above the level of the stimulus chords. The coherence of the figure was 2, 4, 6 or 8 and the duration of the figure ranged from 3 to 7 chords resulting in a total of 20 blocks.</p><p>The stimuli in experiment 6b were identical to the previous experiment save for the following differences: (a) coherence and duration were fixed at a value of 6; (b) the duration of the noise was varied in three different experiments in increasing order: group 1: 50, 100, 150 ms; group 2: 100, 200, 250 ms; group 3: 100, 300, 500 ms respectively. The 100 ms condition was chosen as an anchor and only those participants who performed above a threshold of d′ = 1.5 in this condition were selected for the whole experiment.</p></sec><sec id="s4-4"><title>Procedure</title><p>Prior to the study, training was provided which consisted of listening to trials with no figures, easy-to-detect figures, difficult-to-detect figures and one practice block of fifty mixed trials. For the main experiment, the value of coherence and duration was displayed before the start of each block and participants were instructed to press a button as soon as they heard a figure pop out of the random background (for the brief figures used here, these sounded like a ‘warble’ in the on-going random pattern). Feedback was provided. Blocks with different values of coherence and duration were presented in a pseudorandom order. The participants self-paced the experiment and the study lasted approximately an hour and a half. The procedure was identical across all experiments.</p></sec><sec id="s4-5"><title>Analysis</title><p>Participants’ responses were measured in terms of sensitivity (d prime, or d′). We also report hit rates for certain conditions as mean ± one standard error.</p></sec><sec id="s4-6"><title>Apparatus</title><p>All stimuli were created online using MATLAB 7.5 software (The Mathworks Inc., Natick, MA) at a sampling rate of 44.1 kHz and 16 bit resolution. Sounds were delivered diotically through Sennheiser HD555 headphones (Sennheiser, Germany) and presented at a comfortable listening level of 60–70 dB SPL (self adjusted by each listener). Presentation of the stimuli was controlled using Cogent (<ext-link ext-link-type="uri" xlink:href="http://www.vislab.ucl.ac.uk/cogent.php">http://www.vislab.ucl.ac.uk/cogent.php</ext-link>). Listeners were tested individually in an acoustically shielded sound booth. The apparatus was identical for all experiments.</p><sec id="s4-6-1"><title>Temporal coherence model</title><p>The temporal coherence model consists of two distinct stages. The first stage takes the spectrogram of the stimulus as its input and simulates spectral analysis performed at the level of the cochlea and temporal integration at the level of the auditory cortex (<xref ref-type="bibr" rid="bib12">Chi et al., 2005</xref>; <xref ref-type="bibr" rid="bib20">Elhilali and Shamma, 2008</xref>; <xref ref-type="bibr" rid="bib21">Elhilali et al., 2009a</xref>; <xref ref-type="bibr" rid="bib56">Shamma et al., 2011</xref>). This temporal integration is achieved through a bank of bandpass filters that are tuned to different physiologically plausible parameters that capture the rich variety of spectrotemporal receptive fields (STRFs) found in A1. This is realized by using a range of physiologically valid temporal modulation rates (from slow to fast: 2–32 Hz) and spectral resolution scales (narrow to broad: 0.125 to 8 cycles per octave).</p><p>The next level of the model incorporates a ‘coherence analysis’ stage which computes a ‘windowed’ correlation between each pair of channels by taking the product of the filter outputs corresponding to the different channels. A dynamic coherence matrix which consists of the cross-correlation values as a function of time is thus obtained. The off-diagonal elements of the matrix indicate the presence of coherence across different channels, that is, positively correlated activity and offer insight into the perceptual representation of the stimulus.</p></sec></sec><sec id="s4-7"><title>Procedure</title><p>The temporal coherence model was run for a range of temporal modulation rates: 2.5, 5, 10 and 20 Hz for experiments 1, 4, 5, and 6a and 5, 10, 20 and 40 Hz for experiment 3 respectively. Additionally, we used a rate of 3.33 Hz corresponding to the rate of presentation of 300 ms white noise segments in experiment 6b. These rates cover the range of physiological temporal modulation rates observed in the auditory cortex. A single spectral resolution scale of 8 cycles per octave (corresponding to the bandwidth of streaming; 4 cycles per octave for experiment 4b where larger frequency steps are required to extract a ramped figure) was used.</p><p>The analysis was conducted by entering the SFG stimulus for each experimental condition to the input stage of the model. For experiments 1 and 3, the entire stimulus duration was fed to the model input and for the remaining experiments a stimulus without the pre- and post-figure chords was entered. This was based on the prediction that the background chords before and after the figure onset contribute little to the cross-correlation matrix unlike the chords comprising the figure. The simulations were performed separately for the stimuli containing a figure and without a figure and repeated across 500 iterations. To establish differences between the resultant coherence matrices, we computed the maximum value of the cross-correlation across all time points. This spectral decomposition helps us to examine which channels are correlated with each other (hence, the channels with repeating figure components could possibly be bound together as one object, or the ‘figure’), and not significantly correlated with each other (hence, the channels with random correlation between channels may not be perceived as a single object). The difference in the average values of the maxima between the figure and the ground stimuli was calculated as the model response and plotted like the psychophysical curves to obtain model responses (see <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Deborah Williams, Aiysha Siddiq, Nicolas Barascud, Madhurima Dey and Michael Savage for data collection.</p></ack><sec sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>ST, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>MC, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>SK, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con4"><p>SS, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con5"><p>TDG, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Experimental procedures were approved by the research ethics committee of University College London (Project ID number: 1490/002), and written informed consent was obtained from each participant.</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Blake</surname><given-names>R</given-names></name><name><surname>Lee</surname><given-names>SH</given-names></name></person-group><year>1998</year><article-title>Visual features that vary together over time group together over space</article-title><source>Nat Neurosci</source><volume>1</volume><fpage>160</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1038/414</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>RA</given-names></name><name><surname>Asanuma</surname><given-names>C</given-names></name><name><surname>Cowan</surname><given-names>WM</given-names></name></person-group><year>1985</year><article-title>Callosal and prefrontal associational projecting cell populations in area 7A of the macaque monkey: a study using retrogradely transported fluorescent dyes</article-title><source>J Comp Neurol</source><volume>232</volume><fpage>443</fpage><lpage>455</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbas</surname><given-names>H</given-names></name><name><surname>Mesulam</surname><given-names>M</given-names></name></person-group><year>1981</year><article-title>Organization of afferent input to subdivisions of area 8 in the rhesus monkey</article-title><source>J Comp Neurol</source><volume>200</volume><fpage>407</fpage><lpage>431</lpage><pub-id pub-id-type="doi">10.1002/cne.902000309</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauvois</surname><given-names>MW</given-names></name><name><surname>Meddis</surname><given-names>R</given-names></name></person-group><year>1991</year><article-title>A computer model of auditory stream segregation</article-title><source>Q J Exp Psychol A</source><volume>43</volume><fpage>517</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1080/14640749108400985</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bee</surname><given-names>MA</given-names></name><name><surname>Klump</surname><given-names>GM</given-names></name></person-group><year>2004</year><article-title>Primitive auditory stream segregation: a neurophysiological study in the songbird forebrain</article-title><source>J Neurophysiol</source><volume>92</volume><fpage>1088</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1152/jn.00884.2003</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bee</surname><given-names>MA</given-names></name><name><surname>Klump</surname><given-names>GM</given-names></name></person-group><year>2005</year><article-title>Auditory stream segregation in the songbird forebrain: effects of time intervals on responses to interleaved tone sequences</article-title><source>Brain Behav Evol</source><volume>66</volume><fpage>197</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1159/000087854</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bidet-Caulet</surname><given-names>A</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name></person-group><year>2009</year><article-title>Neurophysiological mechanisms involved in auditory perceptual organization</article-title><source>Front Neurosci</source><volume>3</volume><fpage>182</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.3389/neuro.01.025.2009</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bisley</surname><given-names>JW</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year>2010</year><article-title>Attention, intention, and priority in the parietal lobe</article-title><source>Ann Rev Neurosci</source><volume>33</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-060909-152823</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blake</surname><given-names>R</given-names></name><name><surname>Lee</surname><given-names>SH</given-names></name></person-group><year>2005</year><article-title>The role of temporal structure in human vision</article-title><source>Behav Cogn Neurosci Rev</source><volume>4</volume><fpage>21</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1177/1534582305276839</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bregman</surname><given-names>AS</given-names></name></person-group><year>1990</year><source>Auditory scene analysis: the perceptual organization of sound</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlyon</surname><given-names>RP</given-names></name></person-group><year>2004</year><article-title>How the brain separates sounds</article-title><source>Trends Cog Sci</source><volume>8</volume><fpage>465</fpage><lpage>471</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.08.008</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cherry</surname><given-names>EC</given-names></name></person-group><year>1953</year><article-title>Some experiments on the recognition of speech, with one and two ears</article-title><source>J Acoust Soc Am</source><volume>25</volume><fpage>975</fpage><lpage>979</lpage><pub-id pub-id-type="doi">10.1121/1.1907229</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname><given-names>T</given-names></name><name><surname>Ru</surname><given-names>P</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year>2005</year><article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title><source>J Acoust Soc Am</source><volume>118</volume><fpage>887</fpage><lpage>906</lpage><pub-id pub-id-type="doi">10.1121/1.1945807</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>YE</given-names></name></person-group><year>2009</year><article-title>Multimodal activity in the parietal cortex</article-title><source>Hear Res</source><volume>258</volume><fpage>100</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2009.01.011</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cusack</surname><given-names>R</given-names></name></person-group><year>2005</year><article-title>The intraparietal sulcus and perceptual organization</article-title><source>J Cogn Neurosci</source><volume>17</volume><fpage>641</fpage><lpage>651</lpage><pub-id pub-id-type="doi">10.1162/0898929053467541</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Divac</surname><given-names>I</given-names></name><name><surname>Lavail</surname><given-names>JH</given-names></name><name><surname>Rakic</surname><given-names>P</given-names></name><name><surname>Winston</surname><given-names>KR</given-names></name></person-group><year>1977</year><article-title>Heterogeneous afferents to the inferior parietal lobule of the rhesus monkey revealed by the retrograde transport method</article-title><source>Brain Res</source><volume>123</volume><fpage>197</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(77)90474-7</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durlach</surname><given-names>NI</given-names></name><name><surname>Mason</surname><given-names>CR</given-names></name><name><surname>Kidd</surname><given-names>G</given-names><suffix>Jnr</suffix></name><name><surname>Arbogast</surname><given-names>TL</given-names></name><name><surname>Colburn</surname><given-names>HS</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name></person-group><year>2003</year><article-title>Note on informational masking</article-title><source>J Acoust Soc Am</source><volume>113</volume><fpage>2984</fpage><lpage>2987</lpage><pub-id pub-id-type="doi">10.1121/1.1570435</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dykstra</surname><given-names>AR</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name><name><surname>Thesen</surname><given-names>T</given-names></name><name><surname>Carlson</surname><given-names>CE</given-names></name><name><surname>Doyle</surname><given-names>W</given-names></name><name><surname>Madsen</surname><given-names>JR</given-names></name><etal/></person-group><year>2011</year><article-title>Widespread brain areas engaged during a classical auditory streaming task revealed by intracranial EEG</article-title><source>Front Hum Neurosci</source><volume>5</volume><fpage>74</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2011.00074</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year>2008</year><article-title>A cocktail party with a cortical twist: how cortical mechanisms contribute to sound segregation</article-title><source>J Acoust Soc Am</source><volume>124</volume><fpage>3751</fpage><lpage>3771</lpage><pub-id pub-id-type="doi">10.1121/1.3001672</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Ma</surname><given-names>L</given-names></name><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year>2009a</year><article-title>Temporal coherence in the perceptual organization and cortical representation of auditory scenes</article-title><source>Neuron</source><volume>61</volume><fpage>317</fpage><lpage>329</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.12.005</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Xiang</surname><given-names>J</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year>2009b</year><article-title>Interaction between attention and bottom-up saliency mediates the representation of foreground and background in an auditory scene</article-title><source>PLOS Biol</source><volume>7</volume><fpage>e1000129</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.1000129</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fishman</surname><given-names>YI</given-names></name><name><surname>Steinschneider</surname><given-names>M</given-names></name></person-group><year>2010a</year><article-title>Formation of auditory streams</article-title><person-group person-group-type="editor"><name><surname>Rees</surname><given-names>A</given-names></name><name><surname>Palmer</surname><given-names>AR</given-names></name></person-group><source>The Oxford handbook of auditory science</source><publisher-loc>Oxford, UK</publisher-loc><publisher-name>Oxford University Press</publisher-name><fpage>215</fpage><lpage>246</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fishman</surname><given-names>YI</given-names></name><name><surname>Steinschneider</surname><given-names>M</given-names></name></person-group><year>2010b</year><article-title>Neural correlates of auditory scene analysis based on inharmonicity in the monkey primary auditory cortex</article-title><source>J Neurosci</source><volume>30</volume><fpage>12480</fpage><lpage>12494</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1780-10.2010</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fishman</surname><given-names>YI</given-names></name><name><surname>Arezzo</surname><given-names>JC</given-names></name><name><surname>Steinschneider</surname><given-names>M</given-names></name></person-group><year>2004</year><article-title>Auditory stream segregation in monkey auditory cortex: effects of frequency separation, presentation rate, and tone duration</article-title><source>J Acoust Soc Am</source><volume>116</volume><fpage>1656</fpage><lpage>1670</lpage><pub-id pub-id-type="doi">10.1121/1.1778903</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fishman</surname><given-names>YI</given-names></name><name><surname>Reser</surname><given-names>DH</given-names></name><name><surname>Arezzo</surname><given-names>JC</given-names></name><name><surname>Steinschneider</surname><given-names>M</given-names></name></person-group><year>2001</year><article-title>Neural correlates of auditory stream segregation in primary auditory cortex of the awake monkey</article-title><source>Hear Res</source><volume>151</volume><fpage>167</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(00)00224-0</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geng</surname><given-names>JJ</given-names></name><name><surname>Mangun</surname><given-names>GR</given-names></name></person-group><year>2009</year><article-title>Anterior intraparietal sulcus is sensitive to bottom-up attention driven by stimulus salience</article-title><source>J Cogn Neurosci</source><volume>21</volume><fpage>1584</fpage><lpage>1601</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21103</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldinger</surname><given-names>SD</given-names></name></person-group><year>1998</year><article-title>Echoes of echoes? an episodic theory of lexical access</article-title><source>Psych Reviews</source><volume>105</volume><fpage>251</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.105.2.251</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottlieb</surname><given-names>JP</given-names></name><name><surname>Kusunoki</surname><given-names>M</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year>1998</year><article-title>The representation of visual salience in monkey parietal cortex</article-title><source>Nature</source><volume>391</volume><fpage>481</fpage><lpage>484</lpage><pub-id pub-id-type="doi">10.1038/35135</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Overath</surname><given-names>T</given-names></name></person-group><year>2012</year><article-title>Identification tasks I: auditory object analysis</article-title><person-group person-group-type="editor"><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Overath</surname><given-names>T</given-names></name><name><surname>Popper</surname><given-names>AN</given-names></name><name><surname>Fay</surname><given-names>RR</given-names></name></person-group><source>Human auditory cortex</source><publisher-loc>New York</publisher-loc><publisher-name>Springer Science+Business Media</publisher-name><fpage>199</fpage><lpage>224</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutschalk</surname><given-names>A</given-names></name><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Melcher</surname><given-names>JR</given-names></name><name><surname>Rupp</surname><given-names>A</given-names></name><name><surname>Scherg</surname><given-names>M</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><year>2005</year><article-title>Neuromagnetic correlates of streaming in human auditory cortex</article-title><source>J Neurosci</source><volume>25</volume><fpage>5382</fpage><lpage>5388</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0347-05.2005</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutschalk</surname><given-names>A</given-names></name><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><year>2008</year><article-title>Neural correlates of auditory perceptual awareness under informational masking</article-title><source>PLOS Biol</source><volume>6</volume><fpage>e138</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.0060138</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hyvärinen</surname><given-names>J</given-names></name></person-group><year>1982</year><source>The parietal cortex of monkey and man</source><publisher-loc>Berlin</publisher-loc><publisher-name>Springer-Verlag</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itti</surname><given-names>L</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year>2001</year><article-title>Computational modeling of visual attention</article-title><source>Nat Rev Neurosci</source><volume>2</volume><fpage>194</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1038/35058500</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kidd</surname><given-names>G</given-names></name><name><surname>Mason</surname><given-names>CR</given-names></name><name><surname>Deliwala</surname><given-names>PS</given-names></name><name><surname>Woods</surname><given-names>WS</given-names></name><name><surname>Colburn</surname><given-names>HS</given-names></name></person-group><year>1994</year><article-title>Reducing informational masking by sound segregation</article-title><source>J Acoust Soc Am</source><volume>95</volume><fpage>3475</fpage><lpage>3480</lpage><pub-id pub-id-type="doi">10.1121/1.410023</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kidd</surname><given-names>G</given-names></name><name><surname>Mason</surname><given-names>CR</given-names></name><name><surname>Dai</surname><given-names>H</given-names></name></person-group><year>1995</year><article-title>Discriminating coherence in spectro-temporal patterns</article-title><source>J Acoust Soc Am</source><volume>97</volume><fpage>3782</fpage><lpage>3790</lpage><pub-id pub-id-type="doi">10.1121/1.413107</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kidd</surname><given-names>G</given-names></name><name><surname>Mason</surname><given-names>CR</given-names></name></person-group><year>2003</year><article-title>Multiple bursts, multiple looks, and stream coherence in the release from informational masking</article-title><source>J Acoust Soc Am</source><volume>114</volume><fpage>2835</fpage><lpage>2845</lpage><pub-id pub-id-type="doi">10.1121/1.1621864</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kidd</surname><given-names>G</given-names></name><name><surname>Richards</surname><given-names>VM</given-names></name><name><surname>Streeter</surname><given-names>T</given-names></name><name><surname>Mason</surname><given-names>CR</given-names></name></person-group><year>2011</year><article-title>Contextual effects in the identification of nonspeech auditory patterns</article-title><source>J Acoust Soc Am</source><volume>130</volume><fpage>3926</fpage><lpage>3938</lpage><pub-id pub-id-type="doi">10.1121/1.3658442</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Ullman</surname><given-names>S</given-names></name></person-group><year>1985</year><article-title>Shifts in selective visual attention: towards the underlying neural circuitry</article-title><source>Hum Neurobiol</source><volume>4</volume><fpage>219</fpage><lpage>227</lpage><ext-link ext-link-type="uri" xlink:href="http://papers.klab.caltech.edu/104/1/200.pdf">http://papers.klab.caltech.edu/104/1/200.pdf</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCabe</surname><given-names>SL</given-names></name><name><surname>Denham</surname><given-names>MJ</given-names></name></person-group><year>1997</year><article-title>A model of auditory streaming</article-title><source>J Acoust Soc Am</source><volume>101</volume><fpage>1611</fpage><lpage>1621</lpage><pub-id pub-id-type="doi">10.1121/1.418176</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year>2009</year><article-title>The cocktail party problem</article-title><source>Curr Biol</source><volume>19</volume><fpage>1024</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.09.005</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year>2011</year><article-title>Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis</article-title><source>Neuron</source><volume>71</volume><fpage>926</fpage><lpage>940</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.032</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Carlyon</surname><given-names>RP</given-names></name><name><surname>Gutschalk</surname><given-names>A</given-names></name><name><surname>Melcher</surname><given-names>JR</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name><name><surname>Rauschecker</surname><given-names>JP</given-names></name><etal/></person-group><year>2007a</year><article-title>The role of auditory cortex in the formation of auditory streams</article-title><source>Hear Res</source><volume>229</volume><fpage>116</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2007.01.007</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><year>2007b</year><person-group person-group-type="editor"><name><surname>Kollmeier</surname><given-names>B</given-names></name><name><surname>Klump</surname><given-names>G</given-names></name><name><surname>Hohmann</surname><given-names>V</given-names></name><name><surname>Langemann</surname><given-names>U</given-names></name><name><surname>Mauermann</surname><given-names>M</given-names></name><name><surname>Uppenkamp</surname><given-names>S</given-names></name><name><surname>Verhey</surname><given-names>J</given-names></name></person-group><source>Hearing – from basic research to application</source><publisher-loc>Berlin</publisher-loc><publisher-name>Springer</publisher-name><fpage>267</fpage><lpage>274</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Kreft</surname><given-names>H</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><year>2013b</year><article-title>Temporal coherence versus harmonicity in auditory stream formation</article-title><source>J Acoust Am Soc</source><volume>133</volume><fpage>188</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1121/1.4789866</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Hanson</surname><given-names>C</given-names></name><name><surname>Demany</surname><given-names>L</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><year>2013a</year><article-title>Auditory stream segregation for alternating and synchronous tones</article-title><source>J Exp Psychol Hum Percept Perform</source><comment>[Epub ahead of print]</comment><pub-id pub-id-type="doi">10.1037/a0032241</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>BCJ</given-names></name><name><surname>Gockel</surname><given-names>HE</given-names></name></person-group><year>2012</year><article-title>Properties of auditory stream formation</article-title><source>Phil Trans R Soc</source><volume>367</volume><fpage>919</fpage><lpage>931</lpage><pub-id pub-id-type="doi">10.1098/rstb.2011.0355</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Näätänen</surname><given-names>R</given-names></name><name><surname>Paavilainen</surname><given-names>P</given-names></name><name><surname>Rinne</surname><given-names>T</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name></person-group><year>2007</year><article-title>The mismatch negativity (MMN) in basic research of central auditory processing: a review</article-title><source>Clin Neurophys</source><volume>118</volume><fpage>2544</fpage><lpage>2590</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2007.04.026</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Overath</surname><given-names>T</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>von Kreigstein</surname><given-names>K</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name></person-group><year>2008</year><article-title>Encoding of spectral correlation over time in auditory cortex</article-title><source>J Neurosci</source><volume>28</volume><fpage>13268</fpage><lpage>13273</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4596-08.2008</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandya</surname><given-names>DN</given-names></name><name><surname>Kuypers</surname><given-names>HGJM</given-names></name></person-group><year>1969</year><article-title>Cortico-cortical connections in the rhesus monkey</article-title><source>Brain Res</source><volume>13</volume><fpage>13</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(69)90141-3</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petrides</surname><given-names>M</given-names></name><name><surname>Pandya</surname><given-names>DN</given-names></name></person-group><year>1984</year><article-title>Projections to the frontal cortex from the posterior parietal region in the rhesus monkey</article-title><source>J Comp Neurol</source><volume>228</volume><fpage>105</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1002/cne.902280110</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollack</surname><given-names>I</given-names></name></person-group><year>1975</year><article-title>Auditory informational masking</article-title><source>J Acoust Soc Am</source><volume>57</volume><fpage>S5</fpage><pub-id pub-id-type="doi">10.1121/1.1995329</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pressnitzer</surname><given-names>D</given-names></name><name><surname>Sayles</surname><given-names>M</given-names></name><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Winter</surname><given-names>IM</given-names></name></person-group><year>2008</year><article-title>Perceptual organization of sound begins in the auditory periphery</article-title><source>Curr Biol</source><volume>18</volume><fpage>1124</fpage><lpage>1128</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.06.053</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pressnitzer</surname><given-names>D</given-names></name><name><surname>Suied</surname><given-names>C</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year>2011</year><article-title>Auditory scene analysis: the sweet music of ambiguity</article-title><source>Front Hum Neurosci</source><volume>5</volume><fpage>158</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2011.00158</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Ma</surname><given-names>L</given-names></name><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Oxenham</surname><given-names>A</given-names></name><name><surname>Pressnitzer</surname><given-names>D</given-names></name><etal/></person-group><year>2013</year><article-title>Temporal coherence and the streaming of complex sounds</article-title><source>Adv Exp Med Biol</source><volume>787</volume><fpage>535</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1007/978-1-4614-1590-9_59</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Micheyl</surname><given-names>C</given-names></name></person-group><year>2011</year><article-title>Temporal coherence and attention in auditory scene analysis</article-title><source>Trends Neurosci</source><volume>34</volume><fpage>114</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2010.11.002</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Micheyl</surname><given-names>C</given-names></name></person-group><year>2010</year><article-title>Behind the scenes of auditory perception</article-title><source>Curr Opin Neurobiol</source><volume>20</volume><fpage>361</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.03.009</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname><given-names>JS</given-names></name><name><surname>Alain</surname><given-names>C</given-names></name><name><surname>Picton</surname><given-names>TW</given-names></name></person-group><year>2006</year><article-title>Effects of attention on neuroelectric correlates of auditory stream segregation</article-title><source>J Cogn Neurosci</source><volume>18</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1162/089892906775250021</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname><given-names>JS</given-names></name><name><surname>Gregg</surname><given-names>MK</given-names></name><name><surname>Weintraub</surname><given-names>DM</given-names></name><name><surname>Alain</surname><given-names>C</given-names></name></person-group><year>2012</year><article-title>Attention, awareness, and the perception of auditory scenes</article-title><source>Front Psychol</source><volume>3</volume><fpage>15</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00015</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Tononi</surname><given-names>G</given-names></name><name><surname>Edelman</surname><given-names>GM</given-names></name></person-group><year>1991</year><article-title>Modeling perceptual grouping and figure-ground segregation by means of active reentrant connections</article-title><source>Proc Natl Acad Sci USA</source><volume>88</volume><fpage>129</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1073/pnas.88.1.129</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanton</surname><given-names>GB</given-names></name><name><surname>Bruce</surname><given-names>CJ</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year>1995</year><article-title>Topography of projections to posterior cortical areas from the macaque frontal eye fields</article-title><source>J Comp Neurol</source><volume>353</volume><fpage>291</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1002/cne.903530210</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teki</surname><given-names>S</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>von Kriegstein</surname><given-names>K</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name></person-group><year>2011</year><article-title>Brain bases for auditory stimulus-driven figure-ground segregation</article-title><source>J Neurosci</source><volume>31</volume><fpage>164</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3788-10.2011</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>van Noorden</surname><given-names>LPAS</given-names></name></person-group><year>1975</year><source>Temporal coherence in the perception of tone sequences</source><publisher-loc>Eindhoven</publisher-loc><publisher-name>University of Technology</publisher-name></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>DB</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year>2007</year><article-title>Attention in hierarchical models of object recognition</article-title><source>Prog Brain Res</source><volume>165</volume><fpage>57</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(06)65005-X</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>EC</given-names></name><name><surname>Melcher</surname><given-names>JR</given-names></name><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Gutschalk</surname><given-names>A</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><year>2007</year><article-title>Cortical FMRI activation to sequences of tones alternating in frequency: relationship to perceived rate and streaming</article-title><source>J Neurophysiol</source><volume>97</volume><fpage>2230</fpage><lpage>2238</lpage><pub-id pub-id-type="doi">10.1152/jn.00788.2006</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.00699.008</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Angelaki</surname><given-names>Dora</given-names></name><role>Reviewing editor</role><aff><institution>Baylor College of Medicine</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://www.elifesciences.org/the-journal/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for sending your work entitled “Segregation of complex acoustic scenes based on temporal coherence” for consideration at <italic>eLife</italic>. Your article has been favorably evaluated by a Senior editor and 3 reviewers, one of whom is a member of our Board of Reviewing Editors, and one of whom, Mitchell Steinschneider, wants to reveal his identity.</p><p>The Reviewing editor and the other reviewers discussed their comments before we reached this decision, and the Reviewing editor has assembled the following comments to help you prepare a revised submission.</p><p>This is a well-written psychoacoustical study demonstrating that subjects are able to “hear out” a sequence of tones embedded in a complex acoustic environment of randomly presented tones. In contrast to other studies of this kind, there are no protective spectral regions where the background tones are omitted. The authors demonstrate that after initial training, most subjects are able to segregate the foreground sound pattern (one sound source) from the background sound source. Studies were appropriately performed with adequate controls. Indeed, it is remarkable that segregation of the foreground tone sequence still occurs despite introduction of interleaved bursts of white noise between the foreground tones. The duration of the white noise could extend up to 500 ms. These findings are clearly beyond the segregation capacities of core auditory cortex and the authors rightfully suggest non-classical auditory cortical processing stations as being the most likely generators of this percept. Further, the authors concisely provide modeling data and evidence that the temporal coherence model is the most appropriate model for explaining how this foreground/background segregation develops.</p><p>While the psychoacoustic results are interesting, the reviewers were disappointed by the integration between data and modeling.</p><p>1) First, the motivation for each modification of stimulus parameters should be explained more clearly. What would their model predict for the manipulation?</p><p>2) Second, it is unclear why the model simulations are not shown for all experimental manipulations, but only for number of tones and frequency components. It would be nice to see what the model predicts for the ramped SFG.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.00699.009</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>1) First, the motivation for each modification of stimulus parameters should be explained more clearly. What would their model predict for the manipulation</italic>?</p><p>We agree with this comment and we have restructured the manuscript to better integrate the psychophysics and the modeling, and to provide a clear motivation for the simulation parameters for each experiment. See the Results section for a better description of the modeling, starting “The temporal coherence model is based on the idea that a perceptual “stream” emerges when a group of (frequency) channels are coherently activated against the backdrop of other uncorrelated channels (Shamma et al, 2010).”</p><p><italic>2) Second, it is unclear why the model simulations are not shown for all experimental manipulations, but only for number of tones and frequency components. It would be nice to see what the model predicts for the ramped SFG</italic>.</p><p>In addition to the modeling of the first (“basic SFG”) experiment (<xref ref-type="fig" rid="fig3">Figure 3</xref>), we have now performed and included simulations for the remaining experiments (new <xref ref-type="fig" rid="fig3s1">Figure 3–figure supplement 1</xref>) demonstrating results qualitatively similar to those in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></body></sub-article></article>