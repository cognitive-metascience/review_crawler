<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">50476</article-id><article-id pub-id-type="doi">10.7554/eLife.50476</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>Overtone focusing in biphonic tuvan throat singing</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-139383"><name><surname>Bergevin</surname><given-names>Christopher</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4529-399X</contrib-id><email>cberge@yorku.ca</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-151207"><name><surname>Narayan</surname><given-names>Chandan</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-151208"><name><surname>Williams</surname><given-names>Joy</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-151209"><name><surname>Mhatre</surname><given-names>Natasha</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3618-306X</contrib-id><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-151210"><name><surname>Steeves</surname><given-names>Jennifer KE</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7487-4646</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-174790"><name><surname>Bernstein</surname><given-names>Joshua GW</given-names></name><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-151211"><name><surname>Story</surname><given-names>Brad</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6530-8781</contrib-id><email>bstory@email.arizona.edu</email><xref ref-type="aff" rid="aff10">10</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Physics and Astronomy, York University</institution><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff><aff id="aff2"><label>2</label><institution>Centre for Vision Research, York University</institution><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff><aff id="aff3"><label>3</label><institution>Fields Institute for Research in Mathematical Sciences</institution><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff><aff id="aff4"><label>4</label><institution>Kavli Institute of Theoretical Physics, University of California</institution><addr-line><named-content content-type="city">Santa Barbara</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution>Languages, Literatures and Linguistics, York University</institution><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff><aff id="aff6"><label>6</label><institution>York MRI Facility, York University</institution><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff><aff id="aff7"><label>7</label><institution>Biology, Western University</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>Canada</country></aff><aff id="aff8"><label>8</label><institution>Psychology, York University</institution><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff><aff id="aff9"><label>9</label><institution>National Military Audiology &amp; Speech Pathology Center, Walter Reed National Military Medical Center</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff><aff id="aff10"><label>10</label><institution>Speech, Language, and Hearing Sciences, University of Arizona</institution><addr-line><named-content content-type="city">Tucson</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><role>Reviewing Editor</role><aff><institution>University of Newcastle</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>02</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e50476</elocation-id><history><date date-type="received" iso-8601-date="2019-07-24"><day>24</day><month>07</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-01-31"><day>31</day><month>01</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Bergevin et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Bergevin et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-50476-v3.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="commentary" xlink:href="10.7554/eLife.55749"/><abstract><p>Khoomei is a unique singing style originating from the republic of Tuva in central Asia. Singers produce two pitches simultaneously: a booming low-frequency rumble alongside a hovering high-pitched whistle-like tone. The biomechanics of this biphonation are not well-understood. Here, we use sound analysis, dynamic magnetic resonance imaging, and vocal tract modeling to demonstrate how biphonation is achieved by modulating vocal tract morphology. Tuvan singers show remarkable control in shaping their vocal tract to narrowly focus the harmonics (or overtones) emanating from their vocal cords. The biphonic sound is a combination of the fundamental pitch and a focused filter state, which is at the higher pitch (1–2 kHz) and formed by merging two formants, thereby greatly enhancing sound-production in a very narrow frequency range. Most importantly, we demonstrate that this biphonation is a phenomenon arising from linear filtering rather than from a nonlinear source.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>The republic of Tuva, a remote territory in southern Russia located on the border with Mongolia, is perhaps best known for its vast mountainous geography and the unique cultural practice of “throat singing”. These singers simultaneously create two different pitches: a low-pitched drone, along with a hovering whistle above it. This practice has deep cultural roots and has now been shared more broadly via world music performances and the 1999 documentary Genghis Blues.</p><p>Despite many scientists being fascinated by throat singing, it was unclear precisely how throat singers could create two unique pitches. Singing and speaking in general involves making sounds by vibrating the vocal cords found deep in the throat, and then shaping those sounds with the tongue, teeth and lips as they move up the vocal tract and out of the body. Previous studies using static images taken with magnetic resonance imaging (MRI) suggested how Tuvan singers might produce the two pitches, but a mechanistic understanding of throat singing was far from complete.</p><p>Now, Bergevin et al. have better pinpointed how throat singers can produce their unique sound. The analysis involved high quality audio recordings of three Tuvan singers and dynamic MRI recordings of the movements of one of those singers. The images showed changes in the singer’s vocal tract as they sang inside an MRI scanner, providing key information needed to create a computer model of the process.</p><p>This approach revealed that Tuvan singers can create two pitches simultaneously by forming precise constrictions in their vocal tract. One key constriction occurs when tip of the tongue nearly touches a ridge on the roof of the mouth, and a second constriction is formed by the base of the tongue. The computer model helped explain that these two constrictions produce the distinctive sounds of throat singing by selectively amplifying a narrow set of high frequency notes that are made by the vocal cords. Together these discoveries show how very small, targeted movements of the tongue can produce distinctive sounds.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Tuvan throat singing</kwd><kwd>acoustic phonetics</kwd><kwd>speech biomechanics</kwd><kwd>biphonation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>RGPIN-430761-2013</award-id><principal-award-recipient><name><surname>Bergevin</surname><given-names>Christopher</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>In light of the mysteries underlying the biphonic nature of Tuvan throat song, information from multiple modalities is combined to explain how this remarkable phenomenon is achieved biomechanically.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In the years preceding his death, Richard Feynman had been attempting to visit the small republic of Tuva located in geographic center of Asia (<xref ref-type="bibr" rid="bib24">Leighton, 2000</xref>). A key catalyst came from Kip Thorne, who had gifted him a record called <italic>Melody tuvy</italic>, featuring a Tuvan singing in a style known as Khoomei, or Xöömij. Although he was never successful in visiting Tuva, Feynman was nonetheless captivated by Khoomei, which can be best described as a high-pitched tone, similar to a whistle carrying a melody, hovering above a constant booming low-frequency rumble. This is a form of biphonation, or in Feynman’s own words, &quot;a man with two voices&quot;. Khoomei, now a part of the UNESCO Intangible Cultural Heritage of Humanity, is characterized as &quot;the simultaneous performance by one singer of a held pitch in the lower register and a melody … in the higher register&quot; (<xref ref-type="bibr" rid="bib2">Aksenov, 1973</xref>). How, indeed, does one singer produce two pitches at one time? Even today, the biophysical underpinnings of this biphonic human vocal style are not fully understood.</p><p>Normally, when a singer voices a song or speech, their vocal folds vibrate at a fundamental frequency (<inline-formula><mml:math id="inf1"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>), generating oscillating airflow, forming the so-called <italic>source</italic>. This vibration is not, however, simply sinusoidal, as it also produces a series of harmonics tones (i.e., integer multiples of <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>) (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Harmonic frequencies in this sound above <inline-formula><mml:math id="inf3"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> are called overtones. Upon emanating from the vocal folds, they are then sculpted by the vocal tract, which acts as a spectral <italic>filter</italic>. The vocal-tract filter has multiple resonances that accentuate certain clusters of overtones, creating <italic>formants</italic>. When speaking, we change the shape of our vocal tract to shift formants in systematic ways characteristic of vowel and consonant sounds. Indeed, singing largely uses vowel-like sounds (<xref ref-type="bibr" rid="bib47">Story, 2016</xref>). In most singing, the listener perceives only a single pitch associated with the <inline-formula><mml:math id="inf4"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> of the vocal production, with the formant resonances determining the timbre. Khoomei has two strongly emphasized pitches: a low-pitch drone associated with the <inline-formula><mml:math id="inf5"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, plus a melody carried by variation in the higher frequency formant that can change independently (<xref ref-type="bibr" rid="bib23">Kob, 2004</xref>). Two possible loci for this biphonic property are the <italic>source</italic> and/or the <italic>filter</italic>.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Frequency spectra for three different singers transitioning from normal to biphonic singing.</title><p>Vertical white lines in the spectrograms (left column) indicate the time point for the associated spectrum in the right column. Transition points from normal to biphonic singing state are denoted by the red triangle. The fundamental frequency (<inline-formula><mml:math id="inf6"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>) of the song is indicated by a peak in the spectrum marked by a green square. Overtones, which represent integral multiples of this frequency, are also indicated (black circles). Estimates of the formant structure are shown by overlaying a red dashed line and each formant peak is marked by an x. Note that the vertical scale is in decibels (e.g., a 120 dB difference is a million-fold difference in pressure amplitude). See also <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> and <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref> for further quantification of these waveforms. The associated waveforms can be accessed in the Appendix [T1_3short.wav, T2_5short.wav, T3_2shortA.wav].</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-fig1-v3.tif"/></fig><p>A source-based explanation could involve different mechanisms, such as two vibrating nonlinear sound sources in the syrinx of birds, which produce multiple notes that are harmonically unrelated (<xref ref-type="bibr" rid="bib15">Fee et al., 1998</xref>; <xref ref-type="bibr" rid="bib55">Zollinger et al., 2008</xref>). Humans however are generally considered to have only a single source, the vocal folds. But there are an alternative possibilities: for instance, the source could be nonlinear and produce harmonically-unrelated sounds. For example, aerodynamic instabilities are known to produce biphonation (<xref ref-type="bibr" rid="bib30">Mahrt et al., 2016</xref>). Further, Khoomei often involves dramatic and sudden transitions from simple tonal singing to biophonation (see <xref ref-type="fig" rid="fig1">Figure 1</xref> and the Appendix for associated audio samples). Such abrupt changes are often considered hallmarks of physiological nonlinearity (<xref ref-type="bibr" rid="bib17">Goldberger et al., 2002</xref>), and vocal production can generally be nonlinear in nature (<xref ref-type="bibr" rid="bib20">Herzel and Reuter, 1996</xref>; <xref ref-type="bibr" rid="bib31">Mergell and Herzel, 1997</xref>; <xref ref-type="bibr" rid="bib16">Fitch et al., 2002</xref>; <xref ref-type="bibr" rid="bib49">Suthers et al., 2006</xref>). Therefore it remains possible that biphonation arises from nonlinear source considerations.</p><p>Vocal tract shaping, a filter-based framework, provides an alternative explanation for biphonation. In one seminal study of Tuvan throat singing, Levin and Edgerton examined a wide variety of song types and suggested that there were three components at play. The first two (‘tuning a harmonic’ relative to the filter and lengthening the closed phase of the vocal fold vibration) represented a coupling between source and filter. But it was the third, narrowing of the formant, that appeared crucial. Yet, the authors offered little empirical justification for how these effects are produced by the vocal tract shape in the presented radiographs. Thus it remains unclear how the high-pitched formant in Khoomei was formed (<xref ref-type="bibr" rid="bib18">Grawunder, 2009</xref>). Another study (<xref ref-type="bibr" rid="bib1">Adachi and Yamada, 1999</xref>) examined a throat singer using magnetic resonance imaging (MRI) and captured static images of the vocal tract shape during singing. These images were then used in a computational model to produce synthesized song. Adachi and Yamada argued that a &quot;rear cavity&quot; was formed in the vocal tract and its resonance was essential to biphonation. However, their MRI data reveal limited detail since they were static images of singers already in the biphonation state. Small variations in vocal tract geometry can have pronounced effects on produced song (<xref ref-type="bibr" rid="bib41">Story et al., 1996</xref>) and data from static MRI would reveal little about how and which parts of the vocal tract change shape as the singers transition from simple tonal song to biphonation. To understand which features of vocal tract morphology are crucial to biophonation, a dynamic description of vocal tract morphology would be required.</p><p>Here we study the dynamic changes in the vocal tracts of multiple expert practitioners from Tuva as they produce Khoomei. We use MRI to acquire volumetric 3D shape of the vocal tract of a singer during biphonation. Then, we capture the dynamic changes in a midsagittal slice of the vocal tract as singers transition from tonal to biphonic singing while making simultaneous audio recordings of the song. We use these empirical data to guide our use of a computational model, which allows us to gain insight into which features of vocal tract morphology are responsible for the singing phonetics observed during biophonic Khoomei song (e.g., <xref ref-type="bibr" rid="bib47">Story, 2016</xref>). We focus specifically on the Sygyt (or Sigit) style of Khoomei (<xref ref-type="bibr" rid="bib2">Aksenov, 1973</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Audio recordings</title><p>We made measurements from three Tuvan singers performing Khoomei in the Sygyt style (designated as T1–T3) and one (T4) in a non-Sygyt style. Songs were analyzed using short-time Fourier transforms (STFT), which provide detailed information in both temporal and spectral domains. We recorded the singers transitioning from normal singing into biphonation, <xref ref-type="fig" rid="fig1">Figure 1</xref> showing this transition for three singers. The <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> of their song is marked in the figure (approximately 140 Hz for subject T2, 164 Hz for both T1 and T3) and the overtone structure appears as horizontal bands. Varying degrees of vibrato can be observed, dependent upon the singer (<xref ref-type="fig" rid="fig1">Figure 1</xref>; see also longer spectrograms in <xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref> and <xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7</xref>). Most of the energy in their song is concentrated in the overtones and no subharmonics (i.e., peaks at half-integer multiples of <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>) are observed. In contrast to these three singers, singer T4 performing in a non-Sygyt style exhibited a fundamental frequency of approximately 130 Hz, although significant energy additionally appears around 50–55 Hz, well below an expected subharmonic (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>).</p><p>If we take a slice, that is a time-point from the spectrogram and plot the spectrum, we can observe the peaks to infer the formant structure from this representation of the sound (red-dashed lines in <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). As the singers transition from normal singing to biphonation, we see that the formant structure changes significantly and the positions of formant peaks shift dramatically and rapidly. Note that considering time points before and after the transitions also provides an internal control for both normal and focused song types (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). Once in the biphonation mode, all three singers demonstrate overtones in a narrow spectral band around 1.5–2 kHz; we refer to this as the <italic>focused state</italic>. Specifically, <xref ref-type="fig" rid="fig1">Figure 1</xref> shows that not only is just a single or small group of overtones accentuated, but also that nearby ones are greatly attenuated: ±1 overtones are as much 15–35 dB and ±2 overtones are 35–65 dB below the central overtone. Whereas the energy in the low-frequency region associated with the first formant (below 500 Hz) is roughly constant between the normal-singing and focused states, there is a dramatic change in the spectrum for the higher formants above 500 Hz. In normal singing (i.e., prior to the focused state), spectral energy is distributed across several formants between 500 and 4000 Hz. In the focused state after the transition, the energy above 500 Hz becomes narrowly focused in the 1.5–2 kHz region, generating a whistle-like pitch that carries the song melody.</p><p>To assess the degree of focus objectively and quantitatively, we computed an energy ratio <inline-formula><mml:math id="inf9"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that characterizes the relative degree of energy brought into a narrow band against the energy spread over the full spectrum occupied by human speech (see Materials and methods). In normal speech and singing, for <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">z</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, typically <inline-formula><mml:math id="inf11"><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula> is small (i.e., energy is spread across the spectrum, not <italic>focused</italic> into that narrow region between 1 and 2 kHz). For the Tuvan singers, prior to a transition into a focused state, <inline-formula><mml:math id="inf12"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is similarly small. However once the transition occurs (red triangle in <xref ref-type="fig" rid="fig1">Figure 1</xref>), those values are large (upwards of 0.5 and higher) and sustained across time (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref> and <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>). For one of the singers (T2) the situation was more complex, as he created multiple focused formants (<xref ref-type="fig" rid="fig1">Figure 1</xref> middle panels and <xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref>, <xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8</xref>). The second focused state was not explicitly dependent upon the first: The first focused state clearly moves and transitions between approximately 1.5–2 kHz (by 30%) while the second focused state remains constant at approximately 3–3.5 kHz (changing less than 1%). Thus the focused states are not harmonically related. Unlike the other singers, T2 not only has a second focused state, but also had more energy in the higher overtones (<xref ref-type="fig" rid="fig1">Figure 1</xref>). As such, singer T2 also exhibited a different <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula> time course, which took on values that could be relatively large even prior to the transition. This may be because he took multiple ways to approach the transition into a focused state (e.g., <xref ref-type="fig" rid="app1fig9">Appendix 1—figure 9</xref>).</p><p>Plotting spectra around the transition from normal to biphonation singing in a waterfall plot indicates that the sharp focused filter is achieved by merging two broader formants together (<inline-formula><mml:math id="inf14"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> in <xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="bibr" rid="bib23">Kob, 2004</xref>). This transition into the focused state is fast (∼40–60 ms), as are the shorter transitions within the focused state where the singer melodically changes the filter that forms the whistle-like component of their song (<xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>A waterfall plot representing the spectra at different time points as singer T2 transitions from normal singing into biphonation (T2_3short.wav).</title><p>The superimposed arrows are color-coded to help visualize how the formants change about the transition, chiefly with F3 shifting to merge with F2. This plot also indicates the second focused state centered just above 3 kHz is a sharpened F4 formant.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-fig2-v3.tif"/></fig></sec><sec id="s2-2"><title>Vocal tract MRI</title><p>While we can infer the shape of the formants in Khoomei by examining audio recordings, such analysis is not conclusive in explaining the mechanism used to achieve these formants. The working hypothesis was that vocal tract shape determines these formants. Therefore, it was crucial to examine the shape and dynamics of the vocal tract to determine whether the acoustic measurements are consistent with this hypothesis. To accomplish this, we obtained MRI data from one of the singers (T2) that are unique in two regards. First, there are two types of MRI data reported here: steady-state volumetric data <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="app1fig18">Appendix 1—figure 18</xref>) and dynamic midsagittal images at several frames per second that capture changes in vocal tract position (<xref ref-type="fig" rid="fig4">Figure 4A–B</xref> and <xref ref-type="fig" rid="app1fig20">Appendix 1—figure 20</xref>). Second is that the dynamic data allow us to examine vocal tract changes as song transitions into a focused state (e.g., <xref ref-type="fig" rid="app1fig20">Appendix 1—figure 20</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>3-D reconstruction of volumetric MRI data taken from singer T2 (Run3; see Appendix, including <xref ref-type="fig" rid="app1fig18">Appendix 1—figure 18</xref>).</title><p>(<bold>A</bold>) Example of MRI data sliced through three different planes, including a pseudo-3D plot. Airspaces were determined manually (green areas behind tongue tip, red for beyond). Basic labels are included: L – lips, J – jaw, To– tongue, AR – alveolar ridge, V – velum, E – epiglottis, Lx – larynx, and Tr – trachea. The shadow from the dental post is visible in the axial view on the left hand side and stops near the midline leaving that view relatively unaffected. (<bold>B</bold>) Reconstructed airspace of the vocal tract from four different perspectives. The red circle highlights the presence of the piriform sinuses (<xref ref-type="bibr" rid="bib9">Dang and Honda, 1997</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-fig3-v3.tif"/></fig><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Analysis of vocal tract configuration during singing.</title><p>(<bold>A</bold>) 2D measurement of tract shape. The inner and outer profiles were manually traced, whereas the centerline (white dots) was found with an iterative bisection technique. The distance from the inner to outer profile was measured along a line perpendicular to each point on the centerline (thin white lines). (<bold>B</bold>) Collection of cross-distance measurements plotted as a function of distance from the glottis. Area function can be computed directly from these values and is derived by assuming the cross-distances to be equivalent diameters of circular cross-sections (see Materials and methods). (<bold>C</bold>) Schematic indicating associated modeling assumptions, including vocal tract configuration as in panel B (adapted from <xref ref-type="bibr" rid="bib7">Bunton et al. (2013)</xref>, under a Creative Commons CC-BY license, <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>). (<bold>D</bold>) Model frequency response calculated from the associated area function stemming from panels B and C. Each labeled peak can be considered a formant frequency and the dashed circle indicates merging of formants F2 and F3.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-fig4-v3.tif"/></fig><p>The human vocal tract begins at the vocal folds and ends at the lips. Airflow produced by the vocal cords sets the air-column in the tract into vibration, and its acoustics determine the sound that emanates from the mouth. The vocal tract is effectively a tube-like cavity whose shape can be altered by several articulators: the jaw, lips, tongue, velum, epiglottis, larynx and trachea (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Producing speech or song requires that the shape of the vocal tract, and hence its acoustics, are precisely controlled (<xref ref-type="bibr" rid="bib47">Story, 2016</xref>).</p><p>Several salient aspects of the vocal tract during the production of Khoomei can be observed in the volumetric MRI data. The most important feature however, is that there are two distinct and relevant constrictions when in the focused state, corresponding roughly to the uvula and alveolar ridge. Additionally, the vocal tract is expanded in the region just anterior to the alveolar ridge (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The retroflex position of the tongue tip and blade produces a constriction at 14 cm, and also results in the opening of this sublingual space. It is the degree of constriction at these two locations that is hypothesized to be the primary mechanism for creating and controlling the frequency at which the formant is <italic>focused</italic>.</p></sec><sec id="s2-3"><title>Modeling</title><p>Having established that the shape of vocal tract during Khoomei does indeed have two constrictions, consistent with observations from other groups, the primary goals of our modeling efforts were to use the dynamic MRI data as morphological benchmarks and capture the merging of formants to create the focused states as well as the dynamic transitions into them. Our approach was to use a well-established linear &quot;source/filter&quot; model (e.g., <xref ref-type="bibr" rid="bib40">Stevens, 2000</xref>) that includes known energy losses (<xref ref-type="bibr" rid="bib39">Sondhi and Schroeter, 1987</xref>; <xref ref-type="bibr" rid="bib42">Story et al., 2000</xref>; <xref ref-type="bibr" rid="bib46">Story, 2013</xref>). Here, the vibrating vocals folds act as the broadband sound source (with the <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and associated overtone cascade), while resonances of the vocal tract, considered as a series of 1-D concatenated tubes of variable uniform radius, act as a primary filter. We begin with a first order assumption that the system behaves linearly, which allows us for a simple multiplicative relationship between the source and filter in the spectral domain (e.g., <xref ref-type="fig" rid="app1fig10">Appendix 1—figure 10</xref>).</p><p>Acoustic characteristics of the vocal tract can be captured by transforming the three-dimensional configuration (<xref ref-type="fig" rid="fig3">Figure 3</xref>) into a tube with variation in its cross-sectional area from the glottis to the lips (<xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig5">Figure 5</xref>). This representation of the vocal tract shape is called an <italic>area function</italic>, and allows for calculation of the corresponding frequency response function (from which the formant frequencies can be determined) with a one-dimensional wave propagation algorithm. Although the area function can be obtained directly from a 3D vocal tract reconstruction (e.g., <xref ref-type="bibr" rid="bib41">Story et al., 1996</xref>), the 3D reconstructions of the Tuvan singer’s vocal tract were affected by a large shadow from a dental post (e.g., see <xref ref-type="fig" rid="fig4">Figure 4</xref>) and were not amenable to detailed measurements of cross-sectional area. Instead, a cross-sectional area function was measured from the midsagittal slice of the 3D image set (see Materials and methods and Appendix for details). Thus, the MRI data provided crucial bounds for model parameters: the locations of primary constrictions and thereby the associated area functions.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Results of changing vocal tract morphology in the model by perturbing the baseline area function <inline-formula><mml:math id="inf17"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to demonstrate the merging of formants <inline-formula><mml:math id="inf18"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>, atop two separate overtones as apparent in the two columns of panels A and B.</title><p>(<bold>A</bold>) The frames from dynamic MRI with red and blue dashed circles highlighting the location of the key vocal tract constrictions. (<bold>B</bold>) Model-based vocal tract shapes stemming from the MRI data, including both the associated area functions (top inset) and frequency response functions (bottom inset). <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:msub></mml:math></inline-formula> indicates the constriction near the alveolar ridge while <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>C</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> the constriction near the uvula in the upper pharynx. (<bold>C</bold>) Waveform and corresponding spectrogram of audio from singer T2 (a spectrogram from the model is shown in <xref ref-type="fig" rid="app1fig14">Appendix 1—figure 14</xref>). Note that the merged formants lie atop either the 7th overtone (i.e., <inline-formula><mml:math id="inf22"><mml:mrow><mml:mn>8</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) or the 11th (i.e., <inline-formula><mml:math id="inf23"><mml:mrow><mml:mn>12</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-fig5-v3.tif"/></fig><p>The frequency response functions derived from the above static volumetric MRI data (e.g., <xref ref-type="fig" rid="fig4">Figure 4D</xref>) indicate that two formants <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> cluster together, thus enhancing both their amplitudes. Clearly, if <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> could be driven closer together in frequency, they would merge and form a single formant with unusually high amplitude. We hypothesize that this mechanism could be useful for effectively amplifying a specific overtone, such that it becomes a prominent acoustic feature in the sound produced by a singer, specifically the high frequency component of Khoomei.</p><p>Next, we used the model in conjunction with time-resolved MRI data to investigate how the degree of constriction and expansion at different locations along the vocal tract axis could be a mechanism for controlling the transition from normal to overtone singing and the pitch while in the focused state. These results are summarized in <xref ref-type="fig" rid="fig5">Figure 5</xref> (further details are in the Appendix). While the singers are in the normal song mode, there are no obvious strong constrictions in their vocal tracts (e.g., <xref ref-type="fig" rid="app1fig11">Appendix 1—figure 11</xref>). After they transition, in each MRI from the focused state, we observe a strong constriction near the alveolar ridge. We also observe a constriction near the uvula in the upper pharynx, but the degree of constriction here varies. If we examine the simultaneous audio recordings, we find that variations in this constriction are co-variant with the frequency of the focused formant. From this, we surmise that the mechanism for controlling the enhancement of voice harmonics is the degree of constriction near the alveolar ridge in the oral cavity (labeled <inline-formula><mml:math id="inf28"><mml:msub><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:msub></mml:math></inline-formula> in <xref ref-type="fig" rid="fig5">Figure 5</xref>), which affects the proximity of <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> to each other (<xref ref-type="fig" rid="app1fig12">Appendix 1—figure 12</xref>). Additionally, the degree of constriction near the uvula in the upper pharynx (<inline-formula><mml:math id="inf31"><mml:msub><mml:mi>C</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula>) controls the actual frequency at which <inline-formula><mml:math id="inf32"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf33"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> converge (<xref ref-type="fig" rid="app1fig13">Appendix 1—figure 13</xref>). Other parts of the vocal tract, specifically the expansion anterior to <inline-formula><mml:math id="inf34"><mml:msub><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:msub></mml:math></inline-formula>, may also contribute since they also show small co-variations with the focused formant frequency (<xref ref-type="fig" rid="app1fig14">Appendix 1—figure 14</xref>). Further, a dynamic implementation of the model, as shown in <xref ref-type="fig" rid="app1fig14">Appendix 1—figure 14</xref>, reasonably captures the rapid transition into/out of the focused state as shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Taken together, the model confirms and explains how these articulatory changes give rise to the observed acoustic effects.</p><p>To summarize, an overtone singer could potentially ‘play’ (i.e., select) various harmonics of the voice source by first generating a tight constriction in the oral cavity near the alveolar ridge, and then modulating the degree of constriction in the uvular region of the upper pharynx to vary the position of the focused formant, thereby generating a basis for melodic structure.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>This study has shown that Tuvan singers performing Sygyt-style Khoomei exercise precise control of the vocal tract to effectively merge multiple formants together. They morph their vocal tracts so to create a sustained <italic>focused</italic> state that effectively filters an underlying stable array of overtones. This focused filter greatly accentuates energy of a small subset of higher order overtones primarily in the octave-band spanning 1–2 kHz, as quantified by an energy ratio <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Some singers are even capable of producing additional foci at higher frequencies. Below, we argue that a linear framework (i.e., source/filter model, <xref ref-type="bibr" rid="bib40">Stevens, 2000</xref>) appears sufficient to capture this behavior including the sudden transitions into a focused state, demonstrating that nonlinearities are not a priori essential. That is, since the filter characteristics are highly sensitive to vocal tract geometry, precise biomechanical motor control of the singers is sufficient to achieve a focused state without invoking nonlinearities or a second source as found in other vocalization types (e.g., <xref ref-type="bibr" rid="bib20">Herzel and Reuter, 1996</xref>; <xref ref-type="bibr" rid="bib15">Fee et al., 1998</xref>). Lastly, we describe several considerations associated with how focused overtone song produces such a salient percept by virtue of a pitch decoherence.</p><sec id="s3-1"><title>Source or filter?</title><p>The notion of a focused state is mostly consistent with vocal tract filter-based explanations for biphonation in previous studies (e.g., <xref ref-type="bibr" rid="bib6">Bloothooft et al., 1992</xref>; <xref ref-type="bibr" rid="bib13">Edgerton et al., 1999</xref>; <xref ref-type="bibr" rid="bib1">Adachi and Yamada, 1999</xref>; <xref ref-type="bibr" rid="bib18">Grawunder, 2009</xref>), where terms such as an ‘interaction of closely spaced formants’, ‘reinforced harmonics’, and ‘formant melting’ were used. In addition, the merging of multiple formants is closely related to the ‘singer’s formant’, which is proposed to arise around 3 kHz due to formants <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>–<inline-formula><mml:math id="inf37"><mml:msub><mml:mi>F</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math></inline-formula> combining (<xref ref-type="bibr" rid="bib47">Story, 2016</xref>), though this is typically broader and less prominent than the focused states exhibited by the Tuvans. Our results explain how this occurs and are also broadly consistent with <xref ref-type="bibr" rid="bib1">Adachi and Yamada (1999)</xref> in that a constricted ‘rear cavity’ is crucial. However, we find that this rear constriction determines the pitch of the focused formant, whereas it is the ‘front cavity’ constriction near the alveolar ridge that produces the focusing effect (i.e., merging of formants <inline-formula><mml:math id="inf38"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>).</p><p>Further, the present data appear in several ways inconsistent with conclusions from previous studies of Khoomei, especially those that center on effects that arise from changes in the source. Three salient examples are highlighted. First, we observed overtone structure to be highly stable, though some vibrato may be present. This contrasts the claim by <xref ref-type="bibr" rid="bib25">Levin and Edgerton (1999)</xref> that “(t)o tune a harmonic, the vocalist adjusts the fundamental frequency of the buzzing sound produced by the vocal folds, so as to bring the harmonic into alignment with a formant’. That is, we see no evidence for the overtone ‘ladder’ being lowered or lifted as they suggested (note in <xref ref-type="fig" rid="fig1">Figure 1</xref>, <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is held nearly constant). Further, this stability argues against a transition into a different mode of glottal pulse generation, which could allow for a ‘second source’ (<xref ref-type="bibr" rid="bib31">Mergell and Herzel, 1997</xref>). Second, a single sharply defined harmonic alone is not sufficient to get the salient perception of a focused state, as had been suggested by <xref ref-type="bibr" rid="bib25">Levin and Edgerton (1999)</xref>. Consider <xref ref-type="fig" rid="app1fig9">Appendix 1—figure 9</xref>, especially at the 4 s mark, where the voicing is ‘pressed'. <italic>Pressed</italic> phonation, also referred to as ventricular voice, occurs when glottal flow is affected by virtue of tightening the laryngeal muscles such that the ventricular folds are brought into vibration. This has the perceptual effect of adding a degree of roughness to the voice sound (<xref ref-type="bibr" rid="bib29">Lindestad et al., 2001</xref>; <xref ref-type="bibr" rid="bib14">Edmondson and Esling, 2006</xref>). There, a harmonic at 1.51 kHz dominates (i.e., the two flanking overtones are approximately 40 dB down), yet the song has not yet perceptibly transitioned. It is not until the cluster of overtones at 3–3.5 kHz is brought into focus that the perceptual effect becomes salient, perhaps because prior to the 5.3 s mark the broadband nature of those frequencies effectively masks the first focused state. Third, we do not observe subharmonics, which contrasts a prior claim (<xref ref-type="bibr" rid="bib29">Lindestad et al., 2001</xref>) that ”(t)his combined voice source produces a very dense spectrum of overtones suitable for overtone enhancement’. However, that study was focused on a different style of song called ‘Kargyraa’, which does not exhibit as clearly a focused state as in Sygyt.</p></sec><sec id="s3-2"><title>Linear versus nonlinear mechanisms</title><p>An underlying biophysical question is whether focused overtone song arises from inherently linear or nonlinear processes. Given that Khoomei consists of the voicing of two or more pitches at once and exhibits dramatic and fast transitions from normal singing to biphonation, nonlinear phenomena may seem like an obvious candidate (<xref ref-type="bibr" rid="bib20">Herzel and Reuter, 1996</xref>). It should be noted that <xref ref-type="bibr" rid="bib20">Herzel and Reuter (1996)</xref> go so far to define <italic>biphonation</italic> explicitly through the lens of nonlinearity. We relax such a definition and argue for a perceptual basis for delineating the boundaries of biphonation. Certain frog species exhibit biphonation, and it has been suggested that their vocalizations can arise from complex nonlinear oscillatory regimes of separate elastically coupled masses (<xref ref-type="bibr" rid="bib49">Suthers et al., 2006</xref>). Further, the appearance of abrupt changes in physiological systems (as seen in <xref ref-type="fig" rid="fig1">Figure 1</xref>) has been argued to be a flag for nonlinear mechanisms (<xref ref-type="bibr" rid="bib17">Goldberger et al., 2002</xref>); for example, by virtue of progression through a bifurcation.</p><p>Our results present two lines of evidence that argue against Sygyt-style Khoomei arising primarily from a nonlinear process. First, the underlying harmonic structure of the vocal fold source appears highly stable through the transition into the focused state (<xref ref-type="fig" rid="fig1">Figure 1</xref>). There is little evidence of subharmonics. A source spectral structure that is comprised of an <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and integral harmonics would suggest a primarily linear source mechanism. Second is that our modeling efforts, which are chiefly linear in nature, reasonably account for the sudden and salient transition. That is, the model is readily sufficient to capture the characteristic that small changes in the vocal tract can produce large changes in the filter. Thereby, precise and fast motor control of the articulators in a linear framework accounts for the transitions into and out of the focused state. Thus, in essence, Sygyt-style Khoomei could be considered a linear means to achieve biphonation. Connecting back to nonlinear phonation mechanisms in non-mammals, our results provide further context for how human song production and perception may be similar and/or different relative to that of non-humans (e.g., <xref ref-type="bibr" rid="bib12">Doolittle et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Kingsley et al., 2018</xref>).</p><p>Nevertheless, features that appear transiently in spectrograms do provide hints of source nonlinearity, such as the brief appearance of subharmonics in some instances (<xref ref-type="fig" rid="app1fig15">Appendix 1—figure 15B</xref>). This provides an opportunity to address the limitations of the current modeling efforts and to highlight future considerations. We suggest that further analysis (e.g., <xref ref-type="bibr" rid="bib50">Theiler et al., 1992</xref>; <xref ref-type="bibr" rid="bib54">Tokuda et al., 2002</xref>; <xref ref-type="bibr" rid="bib21">Kantz and Schreiber, 2004</xref>) of Khoomei audio recordings may help to inform the model and might better capture focused filter sharpness and the origin of secondary focused states. Several potential areas for improvement are: nonlinear source–filter coupling (<xref ref-type="bibr" rid="bib52">Titze et al., 2008</xref>); a detailed model of glottal dynamics (e.g., ratio of open/closed phases in glottal flow [<xref ref-type="bibr" rid="bib18">Grawunder, 2009</xref>; <xref ref-type="bibr" rid="bib27">Li and Hou, 2017</xref>], and periodic vibrations in <inline-formula><mml:math id="inf42"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>); inclusion of piriform sinuses as side-branch resonators (<xref ref-type="bibr" rid="bib9">Dang and Honda, 1997</xref>; <xref ref-type="bibr" rid="bib53">Titze and Story, 1997</xref>); inclusion of the 3-D geometry; and detailed study of the front cavity (e.g., lip movements) that may be used by the singer to maintain control of the focused state and to make subtle manipulations.</p></sec><sec id="s3-3"><title>Perceptual consequences of overtone focusing</title><p>Although this study did not directly assess the percept associated with these vocal productions, the results raise pressing questions about how the spectro-temporal signatures of biphonic Khoomei described here create the classical perception of Sygyt-style Khoomei as two distinct sounds (<xref ref-type="bibr" rid="bib2">Aksenov, 1973</xref>). The first, the low-pitched drone, which is present during both the normal singing and the focused-state biphonation intervals, reflects the pitch associated with <inline-formula><mml:math id="inf43"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, extracted from the harmonic representation of the stimulus. It is well established that the perceived pitch of a broadband sound comprised of harmonics reflects the <inline-formula><mml:math id="inf44"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> derived primarily from the perceptually resolved harmonics up to about <inline-formula><mml:math id="inf45"><mml:mrow><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib4">Bernstein and Oxenham, 2003</xref>). The frequency resolution of the peripheral auditory system is such that these low-order harmonics are individually resolved by the cochlea, and it appears that such filtering is an important prerequisite for pitch extraction associated with that common <inline-formula><mml:math id="inf46"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. The second sound, the high-pitched melody, is present only during the focused-state intervals and probably reflects a pitch associated with the focused formant. An open question, however, is why this focused formant would be perceived incoherently as a separate pitch (<xref ref-type="bibr" rid="bib38">Shamma et al., 2011</xref>), when it contains harmonics at multiples of <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. The auditory system tends to group together concurrent harmonics into a single perceived object with a common pitch (<xref ref-type="bibr" rid="bib36">Roberts et al., 2015</xref>), and the multiple formants of a sung or unsung voice are not generally perceived as separate sounds from the low harmonics.</p><p>The fact that the focused formant is so narrow apparently leads the auditory system to interpret this sound as if it were a separate tone, independent of the low harmonics associated with the drone percept, thereby effectively leading to a pitch decoherence. This perceptual separation could be attributable to a combination of both bottom-up (i.e., cochlear) and top-down (i.e., perceptual) factors. From the bottom-up standpoint, even if the focused formant is broad enough to encompass several harmonic components, the fact that it consists of harmonics at or above 10 <inline-formula><mml:math id="inf48"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> (i.e., the 1500 Hz formant frequency represents the 10th harmonic of a 150 Hz <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>) means that these harmonics will not be spectrally resolved by cochlear filtering (<xref ref-type="bibr" rid="bib4">Bernstein and Oxenham, 2003</xref>). Instead, the formant will be represented as a single spectral peak, similar to the representation of a single pure tone at the formant frequency. Although the interaction of harmonic components at this cochlear location will generate amplitude modulation at a rate equal to the <inline-formula><mml:math id="inf50"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib33">Plack and Oxenham, 2005</xref>), it has been argued that a common <inline-formula><mml:math id="inf51"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is a weak cue for binding low- and high-frequency formants (<xref ref-type="bibr" rid="bib8">Culling and Darwin, 1993</xref>). Rather, other top-down mechanisms of auditory-object formation may play a more important role in generating a perception of two separate objects in Khoomei. For example, the rapid onsets of the focused formant may enhance its perceptual separation from the constant drone (<xref ref-type="bibr" rid="bib10">Darwin, 1984</xref>). Further, the fact that the focused formant has a variable frequency (i.e., frequency modulation, or FM) while the drone maintains a constant <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is another difference that could facilitate their perceptual separation. Although it has been argued that FM differences between harmonic sounds generally have little influence on their perceived separation (<xref ref-type="bibr" rid="bib11">Darwin, 2005</xref>), others have reported enhanced separation in the special case in which one complex was static and the other had applied FM (<xref ref-type="bibr" rid="bib48">Summerfield and Culling, 1992</xref>) – similar to the first and second formants during the Tuvan focused state.</p><p>The perceptual separation of the two sounds in the Tuvan song might be further affected by a priori expectations about the spectral qualities of vocal formants (<xref ref-type="bibr" rid="bib5">Billig et al., 2013</xref>). Because a narrow formant occurs so rarely in natural singing and speech, the auditory system might be pre-disposed against perceiving it as a phonetic element, limiting its perceptual integration with the other existing formants. Research into ‘sine-wave speech’ provides some insights into this phenomenon. When three or four individual frequency-modulated sinusoids are presented at formant frequencies in lieu of natural formants, listeners can, with sufficient training, perceive the combination as speech (<xref ref-type="bibr" rid="bib34">Remez et al., 1981</xref>). Nevertheless, listeners largely perceive these unnatural individual pure tones as separate auditory objects (<xref ref-type="bibr" rid="bib35">Remez et al., 2001</xref>), much like the focused formant in Khoomei. Further research exploring these considerations would help close the production–perception circle underlying the unique percept arising from Tuvan throat song.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Acoustical recordings</title><p>Recordings were made at York University (Toronto, ON, Canada) in a double-walled acoustic isolation booth (IAC) using a Zoom H5 24-bit digital recorder and an Audio-Technica P48 condenser microphone. A sample rate of 96 kHz was used. Spectral analysis was done using custom-coded software in Matlab. Spectrograms were typically computed using 4096 point window segments with 95% fractional overlap and a Hamming window. Harmonics (black circles in <xref ref-type="fig" rid="fig1">Figure 1</xref>) were estimated using a custom-coded peak-picking algorithm. Estimated formant trends (red dashed lines in <xref ref-type="fig" rid="fig1">Figure 1</xref>) were determined using a lowpass interpolating filter built into Matlab’s digital signal processing toolbox with a scaling factor of 10. From this trend, the peak-picking was reapplied to determine ‘formant’ frequencies (red 'x's in <xref ref-type="fig" rid="fig1">Figure 1</xref>). This process could be repeated across the spectrogram to track overtone and formant frequency/strength effectively, as shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>.</p><p>To quantify the focused states, we developed a dimension-less measure <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to represent the energy ratio of that spanning a frequency range <inline-formula><mml:math id="inf54"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> relative to the entire spectral output. This can be readily computed from the spectrogram data as follows. First take a ‘slice’ from the spectrogram and convert spectral magnitude to linear ordinate and square it (as intensity is proportional to pressure squared). Then integrate across frequency, first for a limited range spanning <inline-formula><mml:math id="inf55"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> (e.g., 1–2 kHz) and then for a broader range of <inline-formula><mml:math id="inf56"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> (e.g., 0–8 kHz; 8 kHz is a suitable maximum as there is little acoustic energy in vocal output above this frequency). The ratio of these two is then defined as <inline-formula><mml:math id="inf57"><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula>, and takes on values between 0 and 1. This can be expressed more explicitly as:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">𝑑</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:msubsup><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">𝑑</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf58"><mml:mi>P</mml:mi></mml:math></inline-formula> is the magnitude of the scaled sound pressure, <inline-formula><mml:math id="inf59"><mml:mi>f</mml:mi></mml:math></inline-formula> is frequency, and <inline-formula><mml:math id="inf60"><mml:msub><mml:mi>f</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf61"><mml:msub><mml:mi>f</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:math></inline-formula> are filter limits for considering the focused state. The choice of <inline-formula><mml:math id="inf62"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> kHz has the virtue of spanning an octave, which also closely approximates the ‘seventh octave’ from about C6 to C7. <inline-formula><mml:math id="inf63"><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula> did not depend significantly upon the length of the fast Fourier transform (FFT) window. Values of <inline-formula><mml:math id="inf64"><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula> for the waveforms used in <xref ref-type="fig" rid="fig1">Figure 1</xref> are shown in <xref ref-type="fig" rid="app1fig2">Appendix 1—figures 2</xref> and <xref ref-type="fig" rid="app1fig3">3</xref>.</p></sec><sec id="s4-2"><title>MRI acquisition and volumetric analysis</title><p>MRI images were acquired at the York MRI Facility on a 3.0 Tesla MRI scanner (Siemens Magnetom TIM Trio, Erlangen, Germany), using a 12-channel head coil and a neck array. Data were collected with the approval of the York University Institutional Review Board. The participant was fitted with an MRI compatible noise-cancelling microphone (Optoacoustics, Mazor, Israel) mounted directly above the lips. The latency of the microphone and noise-cancelling algorithm was 24 ms. Auditory recordings were made in QuickTime on an iMac during the scans to verify performance.</p><p>Images were acquired using one of two paradigms, static or dynamic. Static images were acquired using a T1-weighted 3D gradient echo sequence in the sagittal orientation with 44 slices centered on the vocal tract, TR = 2.35 ms, TE = 0.97 ms, flip angle = 8 degrees, FoV = 300 mm, and a voxel dimension of 1.2 × 1.2×1.2 mm. Total acquisition time was 11 s. The participant was instructed to begin singing a tone, and to hold it in a steady state for the duration of the scan. The scan was started immediately after the participant began to sing and had reached a steady state. Audio recordings verified a consistent tone for the duration of the scan. Dynamic images were acquired using a 2D gradient echo sequence. A single 10.0 mm thick slice was positioned in a sagittal orientation along the midline of the vocal tract, TR = 4.6 ms, TE = 2.04 ms, flip angle = 8 degrees, FoV = 250 mm, and a voxel dimension of 2.0 × 2.0×10.0 mm. One hundred measurements were taken for a scan duration of 27.75 s. The effective frame rate of the dynamic images was 3.6 Hz. Audio recordings were started just prior to scanning. Only subject T2 participated in the MRI recordings. The participant was instructed to sing a melody for the duration of the scan, and took breaths as needed.</p><p>For segmentation (<xref ref-type="fig" rid="fig3">Figure 3</xref>), 3D MRI images (Run1; see Appendix) were loaded into Slicer (version 4.6.2 r25516). The air-space in the oral cavity was manually segmented using the segmentation module, identified and painted in slice by slice. Careful attention was paid to the parts of the oral cavity that were affected by the artifact from the dental implant. The air cavity was manually repainted to be approximately symmetric in this region using the coronal and axial view (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Once completely segmented, the sections were converted into a 3D model and exported as a STL file. This mesh file was imported into MeshLab (v1.3.4Beta) for cleaning and repairing the mesh. The surface of the STL was converted to be continuous by removing non-manifold faces and then smoothed using depth and Laplacian filters. The mesh was then imported into Meshmixer where further artifacts were removed. This surface-smoothed STL file was finally reimported into Slicer, generating the display in <xref ref-type="fig" rid="fig3">Figure 3B</xref>.</p></sec><sec id="s4-3"><title>Computational modeling</title><p>Measurement of the cross-distance function is illustrated in <xref ref-type="fig" rid="fig4">Figure 4</xref>. The inner and outer profiles of the vocal tract were first determined by manual tracing of the midsagittal image. A 2D iterative bisection algorithm (<xref ref-type="bibr" rid="bib45">Story, 2007</xref>) was then used to find the centerline within the profiles extending from the glottis to the lips, as shown by the white dots in <xref ref-type="fig" rid="fig4">Figure 4A</xref>. Perpendicular to each point on the centerline, the distance from the inner to outer profiles was measured to generate the cross-distance function shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref>; the corresponding locations of the anatomic landmarks shown in the midsagittal image are also indicated on the cross-distance function.</p><p>The cross-distance function, <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, can be transformed to an approximate area function, <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with the relation <inline-formula><mml:math id="inf67"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mi>α</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf68"><mml:mi>k</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf69"><mml:mi>α</mml:mi></mml:math></inline-formula> are a scaling factor and exponent, respectively. If the elements of <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are considered to be diameters of a circular cross-section, <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. Although other values of <inline-formula><mml:math id="inf73"><mml:mi>k</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf74"><mml:mi>α</mml:mi></mml:math></inline-formula> have been proposed to account for the complex shape of the vocal tract cross-section (<xref ref-type="bibr" rid="bib19">Heinz and Stevens, 1964</xref>; <xref ref-type="bibr" rid="bib28">Lindblom and Sundberg, 1971</xref>; <xref ref-type="bibr" rid="bib32">Mermelstein, 1973</xref>), there is no agreement on a fixed set of numbers for each parameter. Hence, the circular approximation was used in this study to generate an estimate of the area function. In <xref ref-type="fig" rid="fig4">Figure 4C</xref>, the area function is plotted as its tubular equivalent, where the radii <inline-formula><mml:math id="inf75"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> were rotated about an axis to generate circular sections from the glottis to the lips.</p><p>The associated frequency response of that area function is shown in <xref ref-type="fig" rid="fig4">Figure 4D</xref> and was calculated with a transmission line approach (<xref ref-type="bibr" rid="bib39">Sondhi and Schroeter, 1987</xref>; <xref ref-type="bibr" rid="bib42">Story et al., 2000</xref>), which included energy losses due to yielding walls, viscosity, heat conduction, and acoustic radiation at the lips. Side branches such the piriform sinuses were not considered in detail in this study. The first five formant frequencies (resonances), <inline-formula><mml:math id="inf76"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, were determined by finding the peaks in the frequency response functions with a peak-picking algorithm (<xref ref-type="bibr" rid="bib51">Titze et al., 1987</xref>) and are located at 400, 1065, 1314, 3286, and 4029 Hz, respectively.</p><p>To examine changes in pitch, a particular vocal tract configuration was manually ‘designed (<xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref>) such that it included constrictive and expansive regions at locations similar to those measured from the singer (i.e., <xref ref-type="fig" rid="fig4">Figure 4</xref>), but to a less extreme degree. We henceforth denote this area function as <inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and it generates a frequency response with widely spaced formant frequencies (<inline-formula><mml:math id="inf78"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>529</mml:mn><mml:mo>,</mml:mo><mml:mn>1544</mml:mn><mml:mo>,</mml:mo><mml:mn>2438</mml:mn><mml:mo>,</mml:mo><mml:mn>3094</mml:mn><mml:mo>,</mml:mo><mml:mn>4236</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo> <mml:mtext>Hz</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>), essentially a neutral vowel. In many of the audio signals recorded from the singer, the fundamental frequency, <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> (i.e., the vibratory frequency of the vocal folds), was typically about 150 Hz. The singer then appeared to enhance one of the harmonics in the approximate range of <inline-formula><mml:math id="inf80"><mml:mrow><mml:mn>8</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>12</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Taking the 12th harmonic (<inline-formula><mml:math id="inf81"><mml:mrow><mml:mrow><mml:mn>12</mml:mn><mml:mo>×</mml:mo><mml:mn>150</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:mn>1800</mml:mn></mml:mrow></mml:math></inline-formula> Hz) as an example target frequency (dashed line in the frequency response shown in <xref ref-type="fig" rid="fig5">Figure 5c</xref>), the area function <inline-formula><mml:math id="inf82"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was iteratively perturbed by the acoustic-sensitivity algorithm described in <xref ref-type="bibr" rid="bib44">Story (2006)</xref> until <inline-formula><mml:math id="inf83"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> converged on 1800 Hz and became a single formant peak in the frequency response. Additional details on the perturbation process leading into <xref ref-type="fig" rid="fig5">Figure 5</xref> are detailed in the Appendix.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>A heartfelt thank you to Huun Huur Tu, without whom this study would not have been possible. Input/suggestions from Ralf Schlueter, Greg Huber, Dorothea Kolossa, Chris Rozell, Tuomas Virtanen, and the reviewers are gratefully acknowledged. Support from York University, the Fields Institute for Research in Mathematical Sciences, and the Kavli Institute of Theoretical Physics is also gratefully acknowledged. CB was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) Grant RGPIN-430761–2013. The identification of specific products or scientific instrumentation does not constitute endorsement or implied endorsement on the part of the author, Department of Defense, or any component agency. The views expressed in this article are those of the authors and do not reflect the official policy of the Department of Army/Navy/Air Force, the Department of Defense, or the U.S. Government.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Investigation</p></fn><fn fn-type="con" id="con3"><p>Investigation, Methodology</p></fn><fn fn-type="con" id="con4"><p>Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con5"><p>Investigation, Methodology</p></fn><fn fn-type="con" id="con6"><p>Investigation, Writing - original draft</p></fn><fn fn-type="con" id="con7"><p>Software, Formal analysis, Investigation, Visualization, Methodology</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: Data were collected with approval of the York University Institutional Review Board (IRB protocol to Prof Jennifer Steeves) This study was approved by the Human Participants Review Board of the Office of Research Ethics at York University (certificate #2017-132) and adhered to the tenets of the Declaration of Helsinki. All participants gave informed written consent and consent to publish prior to their inclusion in the study.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-50476-transrepform-v3.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data files (audio and imaging), as well as the relevant analysis software, are available via <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.cvdncjt14">https://doi.org/10.5061/dryad.cvdncjt14</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Bergevin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Overtone focusing in biphonic Tuvan throat singing</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.cvdncjt14</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adachi</surname> <given-names>S</given-names></name><name><surname>Yamada</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>An acoustical study of sound production in biphonic singing, xöömij</article-title><source>The Journal of the Acoustical Society of America</source><volume>105</volume><fpage>2920</fpage><lpage>2932</lpage><pub-id pub-id-type="doi">10.1121/1.426905</pub-id><pub-id pub-id-type="pmid">10335641</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aksenov</surname> <given-names>AN</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Tuvin folk music</article-title><source>Asian Music</source><volume>4</volume><fpage>7</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.2307/833827</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Bergevin</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Overtone focusing in biphonic Tuvan throat singing</data-title><source>Dryad Digital Repository</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.cvdncjt14">https://doi.org/10.5061/dryad.cvdncjt14</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname> <given-names>JG</given-names></name><name><surname>Oxenham</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Pitch discrimination of diotic and dichotic tone complexes: harmonic resolvability or harmonic number?</article-title><source>The Journal of the Acoustical Society of America</source><volume>113</volume><fpage>3323</fpage><lpage>3334</lpage><pub-id pub-id-type="doi">10.1121/1.1572146</pub-id><pub-id pub-id-type="pmid">12822804</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Billig</surname> <given-names>AJ</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name><name><surname>Deeks</surname> <given-names>JM</given-names></name><name><surname>Monstrey</surname> <given-names>J</given-names></name><name><surname>Carlyon</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Lexical influences on auditory streaming</article-title><source>Current Biology</source><volume>23</volume><fpage>1585</fpage><lpage>1589</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.06.042</pub-id><pub-id pub-id-type="pmid">23891107</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bloothooft</surname> <given-names>G</given-names></name><name><surname>Bringmann</surname> <given-names>E</given-names></name><name><surname>van Cappellen</surname> <given-names>M</given-names></name><name><surname>van Luipen</surname> <given-names>JB</given-names></name><name><surname>Thomassen</surname> <given-names>KP</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Acoustics and perception of overtone singing</article-title><source>The Journal of the Acoustical Society of America</source><volume>92</volume><fpage>1827</fpage><lpage>1836</lpage><pub-id pub-id-type="doi">10.1121/1.403839</pub-id><pub-id pub-id-type="pmid">1401528</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bunton</surname> <given-names>K</given-names></name><name><surname>Story</surname> <given-names>BH</given-names></name><name><surname>Titze</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Estimation of vocal tract area functions in children based on measurement of lip termination area and inverse acoustic mapping</article-title><source>Proceedings of Meetings on Acoustics</source><volume>19</volume><elocation-id>060054</elocation-id><pub-id pub-id-type="doi">10.1121/1.4799532</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Culling</surname> <given-names>JF</given-names></name><name><surname>Darwin</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Perceptual separation of simultaneous vowels: within and across-formant grouping by F0</article-title><source>The Journal of the Acoustical Society of America</source><volume>93</volume><fpage>3454</fpage><lpage>3467</lpage><pub-id pub-id-type="doi">10.1121/1.405675</pub-id><pub-id pub-id-type="pmid">8326071</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dang</surname> <given-names>J</given-names></name><name><surname>Honda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Acoustic characteristics of the piriform Fossa in models and humans</article-title><source>The Journal of the Acoustical Society of America</source><volume>101</volume><fpage>456</fpage><lpage>465</lpage><pub-id pub-id-type="doi">10.1121/1.417990</pub-id><pub-id pub-id-type="pmid">9000736</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darwin</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Perceiving vowels in the presence of another sound: constraints on Formant perception</article-title><source>The Journal of the Acoustical Society of America</source><volume>76</volume><fpage>1636</fpage><lpage>1647</lpage><pub-id pub-id-type="doi">10.1121/1.391610</pub-id><pub-id pub-id-type="pmid">6520301</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Darwin</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title>Pitch and Auditory Grouping</chapter-title><person-group person-group-type="editor"><name><surname>Plank</surname> <given-names>C. J</given-names></name><name><surname>Fay</surname> <given-names>R. R</given-names></name><name><surname>Oxenham</surname> <given-names>A. J</given-names></name><name><surname>Popper</surname> <given-names>A. N</given-names></name></person-group><source>Handbook of Auditory Research</source><volume>24</volume><publisher-name>Springer</publisher-name><fpage>278</fpage><lpage>305</lpage></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doolittle</surname> <given-names>EL</given-names></name><name><surname>Gingras</surname> <given-names>B</given-names></name><name><surname>Endres</surname> <given-names>DM</given-names></name><name><surname>Fitch</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Overtone-based pitch selection in hermit thrush song: unexpected convergence with scale construction in human music</article-title><source>PNAS</source><volume>111</volume><fpage>16616</fpage><lpage>16621</lpage><pub-id pub-id-type="doi">10.1073/pnas.1406023111</pub-id><pub-id pub-id-type="pmid">25368163</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edgerton</surname> <given-names>ME</given-names></name><name><surname>Bless</surname> <given-names>D</given-names></name><name><surname>Thibeault</surname> <given-names>S</given-names></name><name><surname>Fagerholm</surname> <given-names>M</given-names></name><name><surname>Story</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The acoustic analysis of reinforced harmonics</article-title><source>The Journal of the Acoustical Society of America</source><volume>105</volume><elocation-id>1329</elocation-id><pub-id pub-id-type="doi">10.1121/1.426220</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edmondson</surname> <given-names>JA</given-names></name><name><surname>Esling</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The valves of the throat and their functioning in tone, vocal register and stress: laryngoscopic case studies</article-title><source>Phonology</source><volume>23</volume><fpage>157</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1017/S095267570600087X</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fee</surname> <given-names>MS</given-names></name><name><surname>Shraiman</surname> <given-names>B</given-names></name><name><surname>Pesaran</surname> <given-names>B</given-names></name><name><surname>Mitra</surname> <given-names>PP</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The role of nonlinear dynamics of the syrinx in the vocalizations of a songbird</article-title><source>Nature</source><volume>395</volume><fpage>67</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1038/25725</pub-id><pub-id pub-id-type="pmid">12071206</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitch</surname> <given-names>WT</given-names></name><name><surname>Neubauer</surname> <given-names>J</given-names></name><name><surname>Herzel</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Calls out of Chaos: the adaptive significance of nonlinear phenomena in mammalian vocal production</article-title><source>Animal Behaviour</source><volume>63</volume><fpage>407</fpage><lpage>418</lpage><pub-id pub-id-type="doi">10.1006/anbe.2001.1912</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldberger</surname> <given-names>AL</given-names></name><name><surname>Amaral</surname> <given-names>LAN</given-names></name><name><surname>Hausdorff</surname> <given-names>JM</given-names></name><name><surname>Ivanov</surname> <given-names>PC</given-names></name><name><surname>Peng</surname> <given-names>C-K</given-names></name><name><surname>Stanley</surname> <given-names>HE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Fractal dynamics in physiology: alterations with disease and aging</article-title><source>PNAS</source><volume>99</volume><fpage>2466</fpage><lpage>2472</lpage><pub-id pub-id-type="doi">10.1073/pnas.012579499</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Grawunder</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>On the Physiology of Voice Production in South-Siberian Throat Singing: Analysis of Acoustic and Electrophysiological Evidences</source><publisher-name>Frank &amp; Timme</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinz</surname> <given-names>JM</given-names></name><name><surname>Stevens</surname> <given-names>KN</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>On the derivation of area functions and acoustic spectra from cinéradiographic films of speech</article-title><source>The Journal of the Acoustical Society of America</source><volume>36</volume><fpage>1037</fpage><lpage>1038</lpage><pub-id pub-id-type="doi">10.1121/1.2143313</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herzel</surname> <given-names>H</given-names></name><name><surname>Reuter</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Biphonation in voice signals,</article-title><source>American Institute of Physics</source><volume>375</volume><fpage>644</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.1063/1.51002</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kantz</surname> <given-names>H</given-names></name><name><surname>Schreiber</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Nonlinear Time Series Analysis</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511755798</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingsley</surname> <given-names>EP</given-names></name><name><surname>Eliason</surname> <given-names>CM</given-names></name><name><surname>Riede</surname> <given-names>T</given-names></name><name><surname>Li</surname> <given-names>Z</given-names></name><name><surname>Hiscock</surname> <given-names>TW</given-names></name><name><surname>Farnsworth</surname> <given-names>M</given-names></name><name><surname>Thomson</surname> <given-names>SL</given-names></name><name><surname>Goller</surname> <given-names>F</given-names></name><name><surname>Tabin</surname> <given-names>CJ</given-names></name><name><surname>Clarke</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Identity and novelty in the avian syrinx</article-title><source>PNAS</source><volume>115</volume><fpage>10209</fpage><lpage>10217</lpage><pub-id pub-id-type="doi">10.1073/pnas.1804586115</pub-id><pub-id pub-id-type="pmid">30249637</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kob</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Analysis and modelling of overtone singing in the sygyt style</article-title><source>Applied Acoustics</source><volume>65</volume><fpage>1249</fpage><lpage>1259</lpage><pub-id pub-id-type="doi">10.1016/j.apacoust.2004.04.010</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Leighton</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Tuva or Bust!: Richard Feynman’s Last Journey</source><publisher-name>WW Norton &amp; Company</publisher-name></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levin</surname> <given-names>TC</given-names></name><name><surname>Edgerton</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The throat singers of tuva</article-title><source>Scientific American</source><volume>281</volume><fpage>80</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/scientificamerican0999-80</pub-id><pub-id pub-id-type="pmid">10467751</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Levin</surname> <given-names>TC</given-names></name><name><surname>Süzükei</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Where Rivers and Mountains Sing: Sound, Music, and Nomadism in Tuva and Beyond</source><publisher-name>Indiana University Press</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>G</given-names></name><name><surname>Hou</surname> <given-names>Q</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The physiological basis of chinese höömii generation</article-title><source>Journal of Voice</source><volume>31</volume><elocation-id>e16</elocation-id><pub-id pub-id-type="doi">10.1016/j.jvoice.2016.03.007</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindblom</surname> <given-names>BE</given-names></name><name><surname>Sundberg</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Acoustical consequences of lip, tongue, jaw, and larynx movement</article-title><source>The Journal of the Acoustical Society of America</source><volume>50</volume><fpage>1166</fpage><lpage>1179</lpage><pub-id pub-id-type="doi">10.1121/1.1912750</pub-id><pub-id pub-id-type="pmid">5117649</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindestad</surname> <given-names>PA</given-names></name><name><surname>Södersten</surname> <given-names>M</given-names></name><name><surname>Merker</surname> <given-names>B</given-names></name><name><surname>Granqvist</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Voice source characteristics in mongolian “throat singing” studied with high-speed imaging technique, acoustic spectra, and inverse filtering</article-title><source>Journal of Voice</source><volume>15</volume><fpage>78</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/S0892-1997(01)00008-X</pub-id><pub-id pub-id-type="pmid">12269637</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahrt</surname> <given-names>E</given-names></name><name><surname>Agarwal</surname> <given-names>A</given-names></name><name><surname>Perkel</surname> <given-names>D</given-names></name><name><surname>Portfors</surname> <given-names>C</given-names></name><name><surname>Elemans</surname> <given-names>CP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mice produce ultrasonic vocalizations by intra-laryngeal planar impinging jets</article-title><source>Current Biology</source><volume>26</volume><fpage>R880</fpage><lpage>R881</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.08.032</pub-id><pub-id pub-id-type="pmid">27728788</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mergell</surname> <given-names>P</given-names></name><name><surname>Herzel</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Modelling biphonation — The role of the vocal tract</article-title><source>Speech Communication</source><volume>22</volume><fpage>141</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1016/S0167-6393(97)00016-2</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mermelstein</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Articulatory model for the study of speech production</article-title><source>The Journal of the Acoustical Society of America</source><volume>53</volume><fpage>1070</fpage><lpage>1082</lpage><pub-id pub-id-type="doi">10.1121/1.1913427</pub-id><pub-id pub-id-type="pmid">4697807</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Plack</surname> <given-names>CJ</given-names></name><name><surname>Oxenham</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title><italic>Pitch</italic></chapter-title><source>The Psychophysics of Pitch</source><publisher-name>Springer</publisher-name><fpage>7</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1007/0-387-28958-5_2</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remez</surname> <given-names>RE</given-names></name><name><surname>Rubin</surname> <given-names>PE</given-names></name><name><surname>Pisoni</surname> <given-names>DB</given-names></name><name><surname>Carrell</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Speech perception without traditional speech cues</article-title><source>Science</source><volume>212</volume><fpage>947</fpage><lpage>949</lpage><pub-id pub-id-type="doi">10.1126/science.7233191</pub-id><pub-id pub-id-type="pmid">7233191</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remez</surname> <given-names>RE</given-names></name><name><surname>Pardo</surname> <given-names>JS</given-names></name><name><surname>Piorkowski</surname> <given-names>RL</given-names></name><name><surname>Rubin</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>On the bistability of sine wave analogues of speech</article-title><source>Psychological Science</source><volume>12</volume><fpage>24</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00305</pub-id><pub-id pub-id-type="pmid">11294224</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roberts</surname> <given-names>B</given-names></name><name><surname>Summers</surname> <given-names>RJ</given-names></name><name><surname>Bailey</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Acoustic source characteristics, across-formant integration, and speech intelligibility under competitive conditions</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>41</volume><fpage>680</fpage><lpage>691</lpage><pub-id pub-id-type="doi">10.1037/xhp0000038</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanguineti</surname> <given-names>V</given-names></name><name><surname>Laboissière</surname> <given-names>R</given-names></name><name><surname>Ostry</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A dynamic biomechanical model for neural control of speech production</article-title><source>The Journal of the Acoustical Society of America</source><volume>103</volume><fpage>1615</fpage><lpage>1627</lpage><pub-id pub-id-type="doi">10.1121/1.421296</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname> <given-names>SA</given-names></name><name><surname>Elhilali</surname> <given-names>M</given-names></name><name><surname>Micheyl</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Temporal coherence and attention in auditory scene analysis</article-title><source>Trends in Neurosciences</source><volume>34</volume><fpage>114</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2010.11.002</pub-id><pub-id pub-id-type="pmid">21196054</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sondhi</surname> <given-names>M</given-names></name> <name><surname>Schroeter</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>A hybrid time-frequency domain articulatory speech synthesizer</article-title><source>IEEE Transactions on Acoustics, Speech, and Signal Processing</source><volume>35</volume><fpage>955</fpage><lpage>967</lpage><pub-id pub-id-type="doi">10.1109/TASSP.1987.1165240</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stevens</surname> <given-names>KN</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Acoustic Phonetics</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Story</surname> <given-names>BH</given-names></name><name><surname>Titze</surname> <given-names>IR</given-names></name><name><surname>Hoffman</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Vocal tract area functions from magnetic resonance imaging</article-title><source>The Journal of the Acoustical Society of America</source><volume>100</volume><fpage>537</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1121/1.415960</pub-id><pub-id pub-id-type="pmid">8675847</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Story</surname> <given-names>BH</given-names></name><name><surname>Laukkanen</surname> <given-names>AM</given-names></name><name><surname>Titze</surname> <given-names>IR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Acoustic impedance of an artificially lengthened and constricted vocal tract</article-title><source>Journal of Voice</source><volume>14</volume><fpage>455</fpage><lpage>469</lpage><pub-id pub-id-type="doi">10.1016/S0892-1997(00)80003-X</pub-id><pub-id pub-id-type="pmid">11130104</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Story</surname> <given-names>BH</given-names></name><name><surname>Titze</surname> <given-names>IR</given-names></name><name><surname>Hoffman</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The relationship of vocal tract shape to three voice qualities</article-title><source>The Journal of the Acoustical Society of America</source><volume>109</volume><fpage>1651</fpage><lpage>1667</lpage><pub-id pub-id-type="doi">10.1121/1.1352085</pub-id><pub-id pub-id-type="pmid">11325134</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Story</surname> <given-names>BH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Technique for &quot;tuning&quot; vocal tract area functions based on acoustic sensitivity functions</article-title><source>The Journal of the Acoustical Society of America</source><volume>119</volume><fpage>715</fpage><lpage>718</lpage><pub-id pub-id-type="doi">10.1121/1.2151802</pub-id><pub-id pub-id-type="pmid">16521730</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Story</surname> <given-names>BH</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Time dependence of vocal tract modes during production of vowels and vowel sequences</article-title><source>The Journal of the Acoustical Society of America</source><volume>121</volume><fpage>3770</fpage><lpage>3789</lpage><pub-id pub-id-type="doi">10.1121/1.2730621</pub-id><pub-id pub-id-type="pmid">17552726</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Story</surname> <given-names>BH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Phrase-level speech simulation with an airway modulation model of speech production</article-title><source>Computer Speech &amp; Language</source><volume>27</volume><fpage>989</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1016/j.csl.2012.10.005</pub-id><pub-id pub-id-type="pmid">23503742</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Story</surname> <given-names>BH</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>The vocal tract in singing</chapter-title><person-group person-group-type="editor"><name><surname>Welch</surname> <given-names>G</given-names></name><name><surname>Howard</surname> <given-names>D</given-names></name><name><surname>Nix</surname> <given-names>J</given-names></name></person-group><source>The Oxford Handbook of Singing</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/oxfordhb/9780199660773.013.012</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>Q</given-names></name><name><surname>Culling</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Auditory segregation of competing voices: absence of effects of FM or AM coherence</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>336</volume><fpage>357</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1098/rstb.1992.0069</pub-id><pub-id pub-id-type="pmid">1354375</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suthers</surname> <given-names>RA</given-names></name><name><surname>Narins</surname> <given-names>PM</given-names></name><name><surname>Lin</surname> <given-names>WY</given-names></name><name><surname>Schnitzler</surname> <given-names>HU</given-names></name><name><surname>Denzinger</surname> <given-names>A</given-names></name><name><surname>Xu</surname> <given-names>CH</given-names></name><name><surname>Feng</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Voices of the dead: complex nonlinear vocal signals from the larynx of an ultrasonic frog</article-title><source>Journal of Experimental Biology</source><volume>209</volume><fpage>4984</fpage><lpage>4993</lpage><pub-id pub-id-type="doi">10.1242/jeb.02594</pub-id><pub-id pub-id-type="pmid">17142687</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theiler</surname> <given-names>J</given-names></name><name><surname>Eubank</surname> <given-names>S</given-names></name><name><surname>Longtin</surname> <given-names>A</given-names></name><name><surname>Galdrikian</surname> <given-names>B</given-names></name><name><surname>Doyne Farmer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Testing for nonlinearity in time series: the method of surrogate data</article-title><source>Physica D: Nonlinear Phenomena</source><volume>58</volume><fpage>77</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/0167-2789(92)90102-S</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Titze</surname> <given-names>IR</given-names></name><name><surname>Horii</surname> <given-names>Y</given-names></name><name><surname>Scherer</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Some technical considerations in voice perturbation measurements</article-title><source>Journal of Speech, Language, and Hearing Research</source><volume>30</volume><fpage>252</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1044/jshr.3002.252</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Titze</surname> <given-names>I</given-names></name><name><surname>Riede</surname> <given-names>T</given-names></name><name><surname>Popolo</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Nonlinear source–filter coupling in phonation: Vocal exercises</article-title><source>The Journal of the Acoustical Society of America</source><volume>123</volume><fpage>1902</fpage><lpage>1915</lpage><pub-id pub-id-type="doi">10.1121/1.2832339</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Titze</surname> <given-names>IR</given-names></name><name><surname>Story</surname> <given-names>BH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Acoustic interactions of the voice source with the lower vocal tract</article-title><source>The Journal of the Acoustical Society of America</source><volume>101</volume><fpage>2234</fpage><lpage>2243</lpage><pub-id pub-id-type="doi">10.1121/1.418246</pub-id><pub-id pub-id-type="pmid">9104025</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tokuda</surname> <given-names>I</given-names></name><name><surname>Riede</surname> <given-names>T</given-names></name><name><surname>Neubauer</surname> <given-names>J</given-names></name><name><surname>Owren</surname> <given-names>MJ</given-names></name><name><surname>Herzel</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Nonlinear analysis of irregular animal vocalizations</article-title><source>The Journal of the Acoustical Society of America</source><volume>111</volume><fpage>2908</fpage><lpage>2919</lpage><pub-id pub-id-type="doi">10.1121/1.1474440</pub-id><pub-id pub-id-type="pmid">12083224</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zollinger</surname> <given-names>SA</given-names></name><name><surname>Riede</surname> <given-names>T</given-names></name><name><surname>Suthers</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Two-voice complexity from a single side of the syrinx in northern mockingbird Mimus polyglottos vocalizations</article-title><source>Journal of Experimental Biology</source><volume>211</volume><fpage>1978</fpage><lpage>1991</lpage><pub-id pub-id-type="doi">10.1242/jeb.014092</pub-id><pub-id pub-id-type="pmid">18515729</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><p>This appendix contains supporting information for the document <italic>Overtone focusing in biphonic Tuvan throat singing</italic> by Bergevin et al. Citations here refer to the bibliography of the main document. First (<bold><italic>Methodological considerations</italic></bold>), we include several methodological components associated with the quantitative analysis of the waveforms, helping illustrate different approaches towards characterizing the acoustic data and rationale underlying control measures. Second (<italic><bold>Additional waveform analyses</bold></italic>), we include additional plots to support results and discussion in the main text. For example, different spectrograms are presented, as are analyses for additional waveforms. This section also helps to provide additional context for a second independent focused state. The third section (<italic><bold>Additional modeling analysis figures</bold></italic>) details theoretical components leading into the results of the computational model and how the MRI data constrain the key parameters, justifying arguments surrounding the notion of formant merging. Fourth (<italic><bold>Instability in focused state</bold></italic>), some speculative discussion and basic modeling aspects are presented with regard to the notion of instabilities present in the motor control of the focused state. In the fifth section (<italic><bold>Additional MRI analysis figures</bold></italic>), images stemming from the MRI data are presented. Last, the final three sections detail accessing the acoustic waveforms, MRI data files, and waveform analysis (Matlab-based) software via an online repository.</p><sec id="s8" sec-type="appendix"><title>Methodological considerations</title><sec id="s8-1"><title>Overtone and formant tracking</title><p>To facilitate quantification of the waveforms, we custom-coded a peak-picking/tracking algorithm to analyze the time-frequency representations produced by the spectrogram. <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> shows an example of the tracking of the overtones (red dots) and formants (grayscale dots; intensity coded by relative magnitude as indicated by the colorbar). This representation provides an alternative view (compared to <xref ref-type="fig" rid="fig1">Figure 1</xref>) to help demonstrate that, by and large, the overtone structure is highly consistent throughout, while the formant structure varies significantly across the transition. </p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Same as <xref ref-type="fig" rid="fig1">Figure 1</xref> (middle left panel; subject T2, same sound file as shown in the middle panel of <xref ref-type="fig" rid="fig1">Figure 1</xref>), except with overtones and estimated formant structure tracked across time.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig1-v3.tif"/></fig></sec></sec><sec id="s9" sec-type="appendix"><title>Quantifying focused states</title><p><xref ref-type="fig" rid="app1fig2">Appendix 1—figures 2</xref> and <xref ref-type="fig" rid="app1fig3">3</xref> show calculation of the energy ratio <inline-formula><mml:math id="inf85"><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula> used as a means to quantify the degree of focus. For <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>, the waveforms are the same as those shown in <xref ref-type="fig" rid="fig1">Figure 1</xref> (those with slightly different axis limits). In general, we found that <inline-formula><mml:math id="inf86"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> provided a clear means to distinguish the focused state, as values were close to zero prior to the transition and larger/sustained beyond the transition. Singer T2 was an exception. <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref> is for singer T2, using the same file (i.e., the transition point into the focused state at between 6 and 7 s in this figure is the same as that shown in the middle panel of <xref ref-type="fig" rid="fig1">Figure 1</xref>), but with an expanded timescale to illustrate the larger <inline-formula><mml:math id="inf87"><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula> values prior to the transition. This is due to the relatively large amount of energy present between 2.5–4 kHz. We also explored <inline-formula><mml:math id="inf88"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> values in a wide range of phonetic signals, such as child and adult vocalizations, other singing styles (e.g., opera), non-Tuvan singers (e.g., Westerners) performing ‘overtone singing’, and older recordings of Tuvan singers. In general, it was observed that <inline-formula><mml:math id="inf89"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was relatively large and sustained across time for focused overtone song, whereas the value was close to zero and/or highly transient for other vocalizations.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Same data/layout as in <xref ref-type="fig" rid="fig1">Figure 1</xref> but now showing <inline-formula><mml:math id="inf90"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as defined in the 'Materials and methods'.</title><p>These plots show the energy ratio focused between 1–2 kHz. Vertical red dashed lines indicate approximate time of transition into the focused state. An expanded timescale is also shown for singer T2 (middle panel) in <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig2-v3.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Similar to <xref ref-type="fig" rid="fig2">Figure 2</xref> for singer T2 (middle panel), except an expanded time scale is shown to demonstrate the earlier dynamics as this singer approaches the focused state (see T2_5longer.wav).</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig3-v3.tif"/></fig></sec><sec id="s10" sec-type="appendix"><title>Control measurements</title><p>The waveforms from the Tuvan singers provide an intrinsic degree of <italic>control</italic> (i.e., voicing not in the focused state). Similar to <xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref> shows the spectra prior to the transition into the focused state. Although relatively narrow harmonics can be observed, they tend to occur below 1 kHz. Such is consistent with our calculations of <inline-formula><mml:math id="inf91"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>: prior to a transition into a focused state, this value is close to zero. The exception is singer T2, who instead shows a relatively large amount of energy about 1.8–3 kHz that may have some sort of masking effect (see 'Discussion' in the main text,and the 'Pressed transition' section below). In addition, Tuvan singer T4, who used a non-Sygyt style (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>) , can also effectively be considered a ‘control’.</p><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Stemming directly from <xref ref-type="fig" rid="fig1">Figure 1</xref>, the right-hand column now shows a spectrum from a time point prior to transition into the focused state (as denoted by the vertical black lines in the left column).</title><p>The shape of the spectra from <xref ref-type="fig" rid="fig1">Figure 1</xref> is also included for reference.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig4-v3.tif"/></fig><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Spectrogram for singer T4 singing in non-Sygyt style (first song segment of T2_4shortA.wav sound file).</title><p>For the spectrogram, 4096 point windows were used for the fast Fourier transform (FFT) with 95% fractional overlap and a Hamming window.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig5-v3.tif"/></fig></sec><sec id="s11" sec-type="appendix"><title>Additional waveform analyses</title><sec id="s11-1"><title>Other spectrograms</title><p><xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref> shows spectrogram from singer T4 (T4_shortA.wav) singing in non-Sygyt style. While producing a distinctive sound, note the relative lack of energy above approximately 1 kHz. <xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref> shows a spectrogram from singer T2 (T2_5.wav) over a longer timescale than that shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Similarly for <xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7</xref>, but for singer T1. Both of these plots provide a spectral-temporal view of how the singer maintains and modulates the song over the course of a single exhalation. Note both the sudden transitions into different maintained pitches and the briefer transient excursions.</p><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Spectrogram of the entire T2_5.wav sound file.</title><p>The sample rate was 96 kHz. The analysis parameters used were the same as those used for <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig6-v3.tif"/></fig><fig id="app1fig7" position="float"><label>Appendix 1—figure 7.</label><caption><title>Spectrogram of the first song segment of the T1_3.wav sound file.</title><p>The analysis parameters used were the same as those for <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig7-v3.tif"/></fig></sec><sec id="s11-2"><title>Second independent focused state</title><p><xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8</xref> shows another example of a transition in Sygyt-style song for singer T2, clearly showing a second focused state about 3–3.5 kHz. Two aspects merit highlighting. First, the spectral peaks are not harmonically related: at <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>4.5</mml:mn></mml:mrow></mml:math></inline-formula> s, the first focused state is at 1.36 kHz and the other at 3.17 kHz (far from to 2.72 kHz as expected). Second, during the singer-induced pitch change at 3.85 s, the two peaks do not move in unison. Although not ruling out correlations between the two focused states, these observations suggest that they are not simply nor strongly related to one another.</p><fig id="app1fig8" position="float"><label>Appendix 1—figure 8.</label><caption><title>Singer T2's transition into a focused state.</title><p>Note that while the first focused state transitions from approximately 1.36 to 1.78 kHz, the second state remains nearly constant, decreasing only slightly from 3.32 to 3.17 kHz (T2_1shortB.wav).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig8-v3.tif"/></fig></sec><sec id="s11-3"><title>Pressed transition</title><p><xref ref-type="fig" rid="app1fig9">Appendix 1—figure 9</xref> shows a spectrogram and several spectral slices for the sound file in which the voicing was ‘pressed’ (<xref ref-type="bibr" rid="bib1">Adachi and Yamada, 1999</xref>; <xref ref-type="bibr" rid="bib14">Edmondson and Esling, 2006</xref>) prior to the transition into the focused state. That is, prior to the 1.8 s mark, voicing is relatively normal. But after that point (prior to the transition into the focused state around 5.4 s, substantial energy appears between 2–4 kHz along with a degree of vibrato. Note, however, that there is no change to the overall overtone structure (e.g., no emergence of subharmonics). The spectrum at <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>4.0</mml:mn><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, prior to the transition, provides a useful comparison back to <xref ref-type="bibr" rid="bib26">Levin and Süzükei (2006)</xref>. Specifically, one particular overtone is singled out and highly focused, yet the broadband cluster of overtones about 2.5–4 kHz effectively mask it. It is not until about the 5.4 s mark, when those higher overtones are also brought into focus, that a salient perception of the Sygyt-style emerges.</p><fig id="app1fig9" position="float"><label>Appendix 1—figure 9.</label><caption><title>Spectrogram of singer T2 exhibiting pressed voicing heading into transition to focused state (T2_2short.wav).</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig9-v3.tif"/></fig></sec><sec id="s11-4"><title>Additional modeling analysis figures</title><p>The measurement of the cross-distance function (as described in the 'Materials and methods'), along with calculation of the frequency response from an estimate of the area function, suggested that constrictions of the vocal tract in the region of the uvula and alveolar ridge may play a significant role in controlling the spectral focus generated by the convergence of <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf95"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>. Assuming that an overtone singer configures the vocal tract to merge these two formants deliberately such that, together, they enhance the amplitude of a selected harmonic of the voice source, the aim was to investigate how the vocal tract can be systematically shaped with precisely placed constrictions and expansions to both merge <inline-formula><mml:math id="inf96"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf97"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> into a focused cluster and move the cluster along the frequency axis to allow for selection of a range of voice harmonics.</p><p><xref ref-type="fig" rid="app1fig11">Appendix 1—figure 11b</xref> shows the same area function as that in <xref ref-type="fig" rid="app1fig11">Appendix 1—figure 11a</xref> (see 'Materials and methods') but plotted by extending the equivalent radius of each cross-sectional area, outward and inward, along a line perpendicular to the centerline measured from the singer (see <xref ref-type="fig" rid="fig4">Figure 4A</xref>), resulting in an inner and outer outline of the vocal tract shape as indicated by the thick black lines. The measured centerline is also shown in the figure, along with anatomic landmarks. As this does not represent a true midsagittal plane, it will be referred to here as a <italic>pseudo-midsagittal</italic> plot (<xref ref-type="bibr" rid="bib43">Story et al., 2001</xref>).</p><fig id="app1fig10" position="float"><label>Appendix 1—figure 10.</label><caption><title>Overview of source/filter theory, as advanced by <xref ref-type="bibr" rid="bib40">Stevens (2000)</xref>.</title><p>The left column shows normal phonation, whereas the right indicates one example of a focused state.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig10-v3.tif"/></fig><fig id="app1fig11" position="float"><label>Appendix 1—figure 11.</label><caption><title>Setup of the baseline vocal tract configuration used in the modeling study.</title><p>(<bold>a</bold>) The area function (<inline-formula><mml:math id="inf98"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) is in the lower panel and its frequency response is in the upper panel. (<bold>b</bold>) The area function from (<bold>a</bold>) is shown as a pseudo-midsagittal plot (see text).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig11-v3.tif"/></fig><p><xref ref-type="fig" rid="app1fig12">Appendix 1—figure 12a</xref> shows the new area function and frequency response generated by the perturbation process, whereas the pseudo-midsagittal plot is shown in <xref ref-type="fig" rid="app1fig12">Appendix 1—figure 12b</xref>. Relative to the shape of <inline-formula><mml:math id="inf99"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (shown as the thin gray line), the primary modification is a severe constriction imposed between 12.5–13.5 cm from the glottis, essentially at the alveolar ridge. Although the line thickness might suggest that the vocal tract is occluded in this region, the minimum cross-sectional area is 0.09 cm<sup><sub>2</sub></sup>. There is also a more moderate constriction at about 5 cm from the glottis, and a slight expansion between 7–10.5 cm from the glottis. The frequency response in upper panel of <xref ref-type="fig" rid="app1fig12">Appendix 1—figure 12a</xref> demonstrates that the new area function was successful in driving <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> together to form a single formant peak centered at 1800 Hz, which is at least 15 dB higher in amplitude than any of the other formants. Exactly the same process was used to generate area functions for which <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> converge on the target harmonic frequencies: <inline-formula><mml:math id="inf104"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>9</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>11</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1200</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>1350</mml:mn><mml:mo>,</mml:mo><mml:mn>1500</mml:mn><mml:mo>,</mml:mo><mml:mn>1650</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> Hz, respectively. The results, along with those from the previous figure for <inline-formula><mml:math id="inf105"><mml:mrow><mml:mn>12</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, are shown in <xref ref-type="fig" rid="app1fig13">Appendix 1—figure 13</xref>. The collection of frequency responses in the upper panel of <xref ref-type="fig" rid="app1fig13">Appendix 1—figure 13b</xref> shows that <inline-formula><mml:math id="inf106"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf107"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> successfully converged to become one formant peak in each of the cases, and their locations on the frequency axis are centered around the specified target frequencies. The corresponding area functions in the lower panel suggests that the constriction between 12.5–13.5 cm from the glottis (alveolar ridge region) is present in roughly the same form for all five cases. By contrast, an increasingly severe constriction must be imposed in the region between 6–8.5 cm from the glottis (uvular region) in order to shift the target frequency (i.e., the frequency at which <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> converge) downward through progression of specified harmonics. Coincident with this constriction is a progressively larger expansion between 14–15.5 cm from the glottis, which probably assists in positioning the focal regions of <inline-formula><mml:math id="inf110"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf111"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> downward. It can also be noted that the area function that generates a focus at <inline-formula><mml:math id="inf112"><mml:mrow><mml:mn>8</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (1200 Hz; thinnest line) is most similar to the one generated from the cross-distance measurements (i.e., <xref ref-type="fig" rid="fig4">Figure 4c</xref>). In both, there are constrictions located at about 7.5 cm and 13 cm from the glottis; the expansions in the lower pharynx and oral cavity are also quite similar. The main difference is the greater expansion of the region between 8–13 cm from the glottis in the acoustically derived area function.</p><p>On the basis of the results, a mechanism for controlling the enhancement of voice harmonics can be proposed: the degree of constriction near the alveolar ridge in the oral cavity (labeled <inline-formula><mml:math id="inf113"><mml:msub><mml:mi>C</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> in <xref ref-type="fig" rid="fig5">Figure 5</xref> of the main text) controls the proximity of <inline-formula><mml:math id="inf114"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf115"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> to each other, whereas the degree of constriction near the uvula in the upper pharynx, <inline-formula><mml:math id="inf116"><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula>, controls the frequency at which <inline-formula><mml:math id="inf117"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf118"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> converge (the expansion anterior to <inline-formula><mml:math id="inf119"><mml:msub><mml:mi>C</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> may also contribute). Thus, an overtone singer could potentially ‘play’ (i.e., select) various harmonics of the voice source by first generating a tight constriction in the oral cavity near the alveolar ridge to generate the focus of <inline-formula><mml:math id="inf120"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf121"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>, and then modulating the degree of constriction in the uvular region of the upper pharynx to position the focus on a selected harmonic.</p><p>This proposed mechanism of controlling the spectral focus is supported by observation of vocal tract changes based on dynamic MRI data sets. Using this approach, midsagittal movies of the Tuvan singer were acquired in which each image represented approximately 275 ms. Shown in <xref ref-type="fig" rid="fig5">Figure 5</xref> is a comparison of vocal tract configurations derived with the acoustic-sensitivity algorithm (middle panels) to image frames from an MRI-based movie (upper panels) associated with the points in time indicated by the vertical lines superimposed across the waveform and spectrogram in the lower part of the figure. The image frames were chosen such that they appeared to be representative of the singer placing the spectral focus at <inline-formula><mml:math id="inf122"><mml:mrow><mml:mn>8</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (left) and <inline-formula><mml:math id="inf123"><mml:mrow><mml:mn>12</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (right), respectively, based on the evidence available in the spectrogram. The model-based vocal tract shape in the upper left panel, derived for a spectral focus of <inline-formula><mml:math id="inf124"><mml:mrow><mml:mn>8</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (1200 Hz), exhibits a fairly severe constriction in the uvular region, similar to the constrictive effect that can be seen in the corresponding image frame (middle left). Likewise, the vocal tract shape derived for a spectral focus of <inline-formula><mml:math id="inf125"><mml:mrow><mml:mn>12</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (1800 Hz) (upper right) and the image frame just below it both demonstrate an absence of a uvular constriction. Thus, the model-based approach generated vocal tract shapes that appear to possess characteristics similar to those produced by the singer, and provides support for the proposed mechanism of spectral focus control.</p><fig id="app1fig12" position="float"><label>Appendix 1—figure 12.</label><caption><title>Results of perturbing the baseline area function <inline-formula><mml:math id="inf126"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> so that <inline-formula><mml:math id="inf127"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf128"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> converge on 1800 Hz.</title><p>(<bold>a</bold>) Perturbed area function (thick black line) and the corresponding frequency response; for comparison, the baseline area function is also shown (thin gray line). The frequency response shows the convergence of <inline-formula><mml:math id="inf129"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf130"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> into one high amplitude peak centered around 1800 Hz. (<bold>b</bold>) Pseudo-midsagittal plot of the perturbed area function (thick black line) and the baseline area function (thin gray line).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig12-v3.tif"/></fig><fig id="app1fig13" position="float"><label>Appendix 1—figure 13.</label><caption><title>Results of perturbing the baseline area function <inline-formula><mml:math id="inf131"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> so that <inline-formula><mml:math id="inf132"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf133"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> converge on 1200, 1350, 1500, 1650, and 1800 Hz.</title><p>(<bold>A</bold>) Perturbed area functions and corresponding frequency responses; line thicknesses and gray scale are matched in the upper and lower panels. (<bold>B</bold>) Pseudo-midsagittal plot of the perturbed area functions. The circled regions (dotted) denote constrictions that control the proximity of <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> to each other and the frequency at which they converge.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig13-v3.tif"/></fig></sec><sec id="s11-5"><title>Second focused state</title><p>Given that singer T2 was the subject for the MRI scans and uniquely exhibited a second focused state (e.g., <xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8</xref>), the model was also utilized to explore how multiple states could be achieved. Two possibilities appear to be the sharpening of formant F4 alone, or the merging of F4 and F5 (<xref ref-type="fig" rid="app1fig14">Appendix 1—figure 14</xref>). However, it is unclear how reasonable those vocal tract configurations may be and further study is required.</p><fig id="app1fig14" position="float"><label>Appendix 1—figure 14.</label><caption><title>Similar to <xref ref-type="fig" rid="fig5">Figure 5</xref>, but additional manipulations were considered to create a second focused state by merging F4 and F5, as exhibited by singer T2 (see middle row in <xref ref-type="fig" rid="fig1">Figure 1</xref>).</title><p>In addition, the spectrogram shown here is from the model (not the singer’s audio). See also <xref ref-type="fig" rid="app1fig20">Appendix 1—figure 20</xref> for connections back to dynamic MRI data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig14-v3.tif"/></fig></sec><sec id="s11-6"><title>Animations and synthesized song</title><p>Animations and audio clips demonstrating various quantitative aspects of the model are included in the data files posted to <ext-link ext-link-type="uri" xlink:href="http://datadryad.org">datadryad.org</ext-link>. Specifically they are:</p><p>o Animation (no sound) of vocal tract changes during transition into focused state and subsequent pitch changes – <italic><monospace>Medley 0 to5 1 cluster.mp4Audioclipof simulatedsong</monospace></italic></p><p><italic><monospace>o Audio clip of simulated song - Medley 0 to5 1 cluster s im.wav</monospace></italic></p></sec><sec id="s11-7"><title>Instability in focused state</title><p><xref ref-type="fig" rid="app1fig15">Appendix 1—figure 15</xref> and <xref ref-type="fig" rid="app1fig16">Appendix 1—figure 16</xref> show that brief transient instabilities in the focused state can and do regularly occur. Specifically, it can be observed that there are brief transient lapses while the singer is maintaining the focused overtone condition, thereby providing insight into how focus is actively maintained. One possible explanation is control by virtue of biomechanical feedback, where the focused state can effectively be considered to be an unstable equilibrium point, akin to balancing a ruler vertically on the palm of your hand. An alternative consideration might be that singers learn to create additional quasi-stable equilibrium points (e.g., <xref ref-type="fig" rid="app1fig17">Appendix 1—figure 17</xref>). The sudden transitions observed (<xref ref-type="fig" rid="fig1">Figure 1</xref>) could then be likened to two-person cheerleading moves such as a ‘cupie’, where one person standing on the ground suddenly throws another up vertically and has them balancing atop their shoulders or upward-stretched hands. A simple proposed model for the transition into the focused state is shown in <xref ref-type="fig" rid="app1fig17">Appendix 1—figure 17</xref>. There, a stable configuration of the vocal tract would be the low point (pink ball). Learning to achieve a focused state would give rise to additional stable equilibria (red ball), which may be more difficult to maintain. Considerations along these lines, combined with a model for biomechanical control (e.g., <xref ref-type="bibr" rid="bib37">Sanguineti et al., 1998</xref>), can lead to testable predictions specific to when a highly experienced singer is maintaining balance about the transition point into/out of a focused state (e.g., T2_4.wav audio file).</p><fig id="app1fig15" position="float"><label>Appendix 1—figure 15.</label><caption><title>Brief instability in the focused state.</title><p>(<bold>A</bold>) Spectrogram of singer T3 during period during which the focused state briefly falters (T3_2shortB.wav, extracted from around the 33 s mark of T3_2.wav). (<bold>B</bold>) Spectral slices taken at two different time points (vertical white lines in panel A at 0.2 and 0.96 s), the latter falling in the transient unstable state. Note that while there is little change in <inline-formula><mml:math id="inf136"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> between the two periods (170 Hz versus 164 Hz), the unstable period shows a period doubling such that the subharmonic (i.e., <inline-formula><mml:math id="inf137"><mml:msub><mml:mi>f</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>/2) and associated overtones are now present, indicative of nonlinear phonation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig15-v3.tif"/></fig><fig id="app1fig16" position="float"><label>Appendix 1—figure 16.</label><caption><title>Spectrogram of singer T2 (T2_1shortA.wav) about a transition into a focused state.</title><p>Note that there is a slight instability around 4.5 s.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig16-v3.tif"/></fig><fig id="app1fig17" position="float"><label>Appendix 1—figure 17.</label><caption><title>Schematic illustrating a simple possible mechanical analogy (ball confined to a potential well) for the transition into a focused state.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig17-v3.tif"/></fig></sec></sec><sec id="s12" sec-type="appendix"><title>Additional MRI analysis figures</title><sec id="s12-1"><title>Volumetric data</title><p>An example of the volumetric data (arranged as tiled midsagittal slices) is shown in <xref ref-type="fig" rid="app1fig18">Appendix 1—figure 18</xref>. Note that the NMR artifact resulting from the presence of a dental post is apparently lateralized to one side.</p><p><xref ref-type="fig" rid="app1fig19">Appendix 1—figure 19</xref> shows a spectrogram of audio segment (extracted from Run3Vsound.wav) associated with the volumetric scan shown in <xref ref-type="fig" rid="app1fig18">Appendix 1—figure 18</xref>. Segments both with and without the scanner noise are shown.</p></sec><sec id="s12-2"><title>Vocal tract shape and associated spectrograms</title><p>Examples of the vocal tract taken during the dynamic MRI runs (i.e., midsagittal only) are shown for very different representative time points in <xref ref-type="fig" rid="app1fig20">Appendix 1—figure 20</xref>.</p><fig id="app1fig18" position="float"><label>Appendix 1—figure 18.</label><caption><title>Mosaic of single slices from the volumetric MRI scan (Run3) of subject T2 during focused overtone state.</title><p>Spectrogram of corresponding audio shown in <xref ref-type="fig" rid="app1fig19">Appendix 1—figure 19</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig18-v3.tif"/></fig><fig id="app1fig19" position="float"><label>Appendix 1—figure 19.</label><caption><title>Spectrogram of steady-state overtone voicing assocaited with the volumetric scan shown in <xref ref-type="fig" rid="app1fig18">Appendix 1—figure 18</xref>.</title><p>Two different one-second segments are shown: the top segment shows images there were made during the scan (and thus includes acoustic noise from the scanner during image acquisition), while the bottom segment shows images made just after scan ends but while the subject continues to sing.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig19-v3.tif"/></fig><fig id="app1fig20" position="float"><label>Appendix 1—figure 20.</label><caption><title>Representative movie frames and their corresponding spectra for singer T2, as input into modeling parameters (e.g., <xref ref-type="fig" rid="fig5">Figure 5</xref>).</title><p>The corresponding Appendix data files are DynamicRun2S.mov (MRI images) and DynamicRun2sound.wav (spectra; see also DynamicRun2SGrid.pdf). The top row shows a ‘low pitch’ (first) focused state at about 1.3 kHz whereas the bottom row shows a ‘high’ pitch at approximately 1.9 kHz. Note a key change is that the back of the tongue moves forward to shift from the low to the high pitch. Thin gray bars are added to the spectra to help to highlight the frequency difference. The legend is the same as that shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50476-app1-fig20-v3.tif"/></fig></sec></sec><sec id="s13" sec-type="appendix"><title>Data</title><p>All data relevant to the study have been placed in the online repository – <ext-link ext-link-type="uri" xlink:href="https://datadryad.org/stash">https://datadryad.org/stash (</ext-link><xref ref-type="bibr" rid="bib3">Bergevin, 2020</xref>). Below is a list of the data placed there, along with a brief description (see 'Materials and methods' section for additional details).</p></sec><sec id="s14" sec-type="appendix"><title>Acoustic data</title><p>All waveforms were obtained at a sample rate of 96 kHz and a bit-depth of 24 bits.</p><list list-type="bullet"><list-item><p>T1_1.wav</p></list-item><list-item><p>T1_2.wav</p></list-item><list-item><p>T1_3.wav</p></list-item><list-item><p>T1_3short.wav</p></list-item><list-item><p>T2_1.wav</p></list-item><list-item><p>T2_1shortA.wav</p></list-item><list-item><p>T2_1shortB.wav</p></list-item><list-item><p>T2_1shortC.wav</p></list-item><list-item><p>T2_2.wav</p></list-item><list-item><p>T2_2short.wav</p></list-item><list-item><p>T2_3.wav</p></list-item><list-item><p>T2_4.wav</p></list-item><list-item><p>T2_5.wav</p></list-item><list-item><p>T2_5longer.wav</p></list-item><list-item><p>T2_5short.wav</p></list-item><list-item><p>T3_2.wav</p></list-item><list-item><p>T3_2shortA.wav</p></list-item><list-item><p>T3_2shortB.wav</p></list-item><list-item><p>T4_1.wav</p></list-item><list-item><p>T4_1shortA.wav</p></list-item></list></sec><sec id="s15" sec-type="appendix"><title>MRI data</title><p>* Images Images were only obtained from singer T2. Note that all image data are saved as DICOM files (i.e., .dcm) :</p><list list-type="bullet"><list-item><p>Volumetric Run1</p></list-item><list-item><p>Volumetric Run2</p></list-item><list-item><p>Volumetric Run3</p></list-item><list-item><p>Dynamic midsagittal Run1</p></list-item><list-item><p>Dynamic midsagittal Run2</p></list-item><list-item><p>Dynamic midsagittal Run3</p></list-item></list><p>* Audio recordings acquired during MRI acquisition (see 'Materials and methods').</p><list list-type="bullet"><list-item><p>Vol. Run1 audio</p></list-item><list-item><p>Vol. Run2 audio</p></list-item><list-item><p>Vol. Run3 audio</p></list-item><list-item><p>Dyn. Run1 audio</p></list-item><list-item><p>Dyn. Run2 audio</p></list-item><list-item><p>Dyn. Run3 audio</p></list-item></list><p>* MRI Midsagittal movies with sound were also created by animating the frames in Matlab and syncing the recorded audio via Wondershare Filmora. They are saved as .mov files (Apple QuickTime Movie files):</p><list list-type="bullet"><list-item><p>Dyn. Run1 video</p></list-item><list-item><p>Dyn. Run2 video</p></list-item><list-item><p>Dyn. Run3 video</p></list-item></list><p>To facilitate connecting movie frames back to the associated sound produced by singer T2 at that moment, the movies include frame numbers. Those have been labeled on the corresponding time location in the spectrograms (see red labels at top):</p><list list-type="bullet"><list-item><p>Dyn. Run1 spectrogram</p></list-item><list-item><p>Dyn. Run2 spectrogram</p></list-item><list-item><p>Dyn. Run3 spectrogram</p></list-item></list><p>* Segmented volumetric data files (like those shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>), data saved as STL files (i.e., .stl):</p><list list-type="bullet"><list-item><p>Segmented data (T2)</p></list-item></list></sec><sec id="s16" sec-type="appendix"><title>Software and synthesized song</title><p>Simulations and waveform analysis were implemented in Matlab. The TubeTalker software is provided ‘as is’:</p><list list-type="bullet"><list-item><p>Code to analyze general aspects of the waveforms (e.g., <xref ref-type="fig" rid="fig1">Figure 1</xref> spectrograms)</p></list-item><list-item><p>Code to quantify <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>e</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula> time course (e.g., <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>)</p></list-item><list-item><p>TubeTalker (zipped file, 7 MB)</p></list-item></list></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.50476.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><role>Reviewing Editor</role><aff><institution>University of Newcastle</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Tuvan throat singing, in which people are able to simultaneously produce and independently control two different distinct pitches using the human vocal apparatus, has fascinated hearing and speech researchers for decades. This careful study examines the acoustics of the produced sound and offers new insights into why the produced sound results in two distinct, separately controllable pitches.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Overtone focusing in biphonic Tuvan throat singing&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another, and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary</p><p>We enjoyed this work addressing mechanisms by which throat singers produce dual pitches that assesses the mechanism for this in terms of the ways in which the vocal tract is precisely controlled based on MRI videos. The work mentions other biological examples of dual fundamentals in songbirds for the broad <italic>eLife</italic> audience. One of the issues that came up in discussion was the control for normal vocalisations without biphonation, but I think the authors make a reasonable case that the singers act as their own controls. Basically, the work shows that the dual pitch mechanism is associated with changes in the vocal tract morphology based on two constrictions that merge second and third formants and is associated with what they call a 'focussed state' in which the harmonics at 1.5kHz to 2kHz are accentuated. The idea as I understand it is that this accentuates a single harmonic of the fundamental glottal pulse rate so that a new high frequency component of Khoomei emerges that is in effect perceptually 'released' from the harmonic series to allow the emergence of the high pitched whistling part of the song.</p><p>Major comments</p><p>1) From first principles, dual pitch singing could be achieved by a different type of glottal pulse generation in the larynx so that two vibration modes were present (as in avian syrinx). This is not the mechanism suggested here, and it is hard to see how the anatomy and physiology of the human larynx might allow this, but this has not been directly examined in the MRI work. The authors carried out a careful acoustic analysis which shows only one harmonic series before and after transitions to throat singling (without shifting), which I think is adequate. But they might comment on the other possible mechanism for biological readers, if only to dismiss it.</p><p>2) Both reviewers thought the discussion of the basis for the perceived dual pitch was not clear. The authors discuss differences in cochlear mechanisms between low frequency regions and high frequency regions. More effort could be made to explain how the dual pitch, which is attributed to a type of spectral emphasis, can be reconciled with current models of pitch perception. The fundamental for the singers assessed was ~150Hz so that the &gt;1.5 kHz region will be unresolved (H10 and above). The greatest contribution to the salience of the low pitch will be the resolved harmonics at frequencies below the focus region, which are well represented. The high-frequency harmonics will usually contribute (weakly) to the low pitch based on the temporal firing patterns due to merged harmonics in frequency bands. The authors appear to be arguing that a different spectral pitch emerges in the high frequency focussed region, distinct from that associated with the lower harmonics.</p><p>3) The argument about decreased phase locking at high frequency was not convincing: this occurs in a much higher frequency region that the focussed region. The argument that the high pitch was not easily explained by a non-linear distortion was convincing.</p><p>4) In conclusion, we thought the work nicely shows the changes in vocal tract morphology and associated spectrum as an explanation for the dual pitch, but more teasing out of mechanism for the dual pitch perception is required in a way that might be accessible to readers.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Overtone focusing in biphonic Tuvan throat singing&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by a member of our Board of Reviewing Editors and Barbara Shinn-Cunningham as the Senior Editor.</p><p>We are afraid we are still not satisfied with the discussion of the basis for the dual pitch at the end of the Discussion. The authors have demonstrated a region of spectral focus as a proposed mechanism for the new pitch. But we still do not understand why the focussed overtones produce a different pitch. They are still harmonics of the same fundamental and interactions between them in this unresolved region would be expected to produce beating at the same frequency as the fundamental, in the absence of non-linear mechanisms. We also do not understand what the additional relevance of a decrease in phase locking in this region would be that the authors highlight. Are the authors claiming that the focused region produces spectral excitation in a region without the usual coding of beating between harmonics (because of decreased phase locking) and that this is the cause of the new pitch? If so an explicit suggestion along those lines might help readers who are familiar with conventional pitch models.</p><p><italic>eLife</italic> does not usually encourage multiple rounds of revision but this is a critical point in the interpretation of an interesting study, and I would encourage a revision with a much shorter final section of Discussion that explains a clear hypothesis related to the cause of the new pitch.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.50476.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Major comments</p><p>1) From first principles, dual pitch singing could be achieved by a different type of glottal pulse generation in the larynx so that two vibration modes were present (as in avian syrinx). This is not the mechanism suggested here, and it is hard to see how the anatomy and physiology of the human larynx might allow this, but this has not been directly examined in the MRI work. The authors carried out a careful acoustic analysis which shows only one harmonic series before and after transitions to throat singling (without shifting), which I think is adequate. But they might comment on the other possible mechanism for biological readers, if only to dismiss it.</p></disp-quote><p>We attempted to further clarify the point (that we saw no evidence for a nonlinear source mechanism) and added an additional line of text as per the suggestion.</p><disp-quote content-type="editor-comment"><p>2) Both reviewers thought the discussion of the basis for the perceived dual pitch was not clear. The authors discuss differences in cochlear mechanisms between low frequency regions and high frequency regions. More effort could be made to explain how the dual pitch, which is attributed to a type of spectral emphasis, can be reconciled with current models of pitch perception. The fundamental for the singers assessed was ~150Hz so that the &gt;1.5 kHz region will be unresolved (H10 and above). The greatest contribution to the salience of the low pitch will be the resolved harmonics at frequencies below the focus region, which are well represented. The high-frequency harmonics will usually contribute (weakly) to the low pitch based on the temporal firing patterns due to merged harmonics in frequency bands. The authors appear to be arguing that a different spectral pitch emerges in the high frequency focussed region, distinct from that associated with the lower harmonics.</p></disp-quote><p>This criticism was given particularly serious thought and consideration. As a result, we totally rewrote this section to make the proposed ideas clearer, as well as accessible to a broad readership. We tried to find a better balance between issues/questions related to pitch coding and those to cochlear mechanics.</p><disp-quote content-type="editor-comment"><p>3) The argument about decreased phase locking at high frequency was not convincing: this occurs in a much higher frequency region that the focussed region. The argument that the high pitch was not easily explained by a non-linear distortion was convincing.</p></disp-quote><p>As alluded to in the comments above, we clarified the nature of the argument (re phase locking) by expanding upon the discussion of pitch coding. While a reasonable degree of phase locking would still be expected around the 1.5-2 kHz region, this is also where temporal coding starts to fall off dramatically (e.g., Verschooten et al., 2018, PLoS Biol.). That facet, that in the 1-2 kHz region of the human cochlea the fidelity of timing information changes, is what is relevant to the narrative thread here.</p><disp-quote content-type="editor-comment"><p>4) In conclusion, we thought the work nicely shows the changes in vocal tract morphology and associated spectrum as an explanation for the dual pitch, but more teasing out of mechanism for the dual pitch perception is required in a way that might be accessible to readers.</p></disp-quote><p>See comments above.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>[…]</p><p>eLife does not usually encourage multiple rounds of revision but this is a critical point in the interpretation of an interesting study, and I would encourage a revision with a much shorter final section of Discussion that explains a clear hypothesis related to the cause of the new pitch.</p></disp-quote><p>As we would like to see this work published with <italic>eLife</italic>, we have drastically truncated the highlighted section to create “a much shorter final section” as suggested. Given our lack of expertise in pitch perception, coupled with our appreciation for the comments raised, we instead (succinctly) reframed through the lens of looking ahead at future work. Specifically, we include only what we think are some quite interesting and provocative parallels we have observed between Sygyt song and cochlear mechanics. We believe that providing this as a summary to the narrative will help stimulate crosstalk between emerging viewpoints in cochlear mechanics and central processing (e.g., pitch perception).</p><p>As such, hopefully we have a more streamlined “story” that will be sufficient for</p><p>publication. We believe the rest of the work paints a clear picture as to how the</p><p>morphology leads to biphonation and that can stand on its own without over speculation on other facets.</p></body></sub-article></article>