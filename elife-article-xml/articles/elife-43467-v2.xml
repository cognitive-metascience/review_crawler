<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">43467</article-id><article-id pub-id-type="doi">10.7554/eLife.43467</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Short Report</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Emotional faces guide the eyes in the absence of awareness</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-125273"><name><surname>Vetter</surname><given-names>Petra</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6516-4637</contrib-id><email>petra.vetter@rhul.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-126058"><name><surname>Badde</surname><given-names>Stephanie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4005-5503</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-13068"><name><surname>Phelps</surname><given-names>Elizabeth A</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-39101"><name><surname>Carrasco</surname><given-names>Marisa</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1002-9056</contrib-id><email>marisa.carrasco@nyu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Psychology, Center for Neural Science</institution><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Psychology</institution><institution>Royal Holloway, University of London</institution><addr-line><named-content content-type="city">Egham</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Psychology</institution><institution>Harvard University</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Goodale</surname><given-names>Melvyn</given-names></name><role>Reviewing Editor</role><aff><institution>Western University</institution><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Ivry</surname><given-names>Richard B</given-names></name><role>Senior Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>08</day><month>02</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e43467</elocation-id><history><date date-type="received" iso-8601-date="2018-11-07"><day>07</day><month>11</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2019-02-07"><day>07</day><month>02</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Vetter et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Vetter et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-43467-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.43467.001</object-id><p>The ability to act quickly to a threat is a key skill for survival. Under awareness, threat-related emotional information, such as an angry or fearful face, has not only perceptual advantages but also guides rapid actions such as eye movements. Emotional information that is suppressed from awareness still confers perceptual and attentional benefits. However, it is unknown whether suppressed emotional information can directly guide actions, or whether emotional information has to enter awareness to do so. We suppressed emotional faces from awareness using continuous flash suppression and tracked eye gaze position. Under successful suppression, as indicated by objective and subjective measures, gaze moved towards fearful faces, but away from angry faces. Our findings reveal that: (1) threat-related emotional stimuli can guide eye movements in the absence of visual awareness; (2) threat-related emotional face information guides distinct oculomotor actions depending on the type of threat conveyed by the emotional expression.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>eye movements</kwd><kwd>emotion</kwd><kwd>awareness</kwd><kwd>faces</kwd><kwd>continuous flash suppression</kwd><kwd>threat</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>VE 739/1-1</award-id><principal-award-recipient><name><surname>Vetter</surname><given-names>Petra</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NIH-RO1-EY016200</award-id><principal-award-recipient><name><surname>Carrasco</surname><given-names>Marisa</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>BA 5600/1-1</award-id><principal-award-recipient><name><surname>Badde</surname><given-names>Stephanie</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>When emotional faces are rendered invisible, our eyes look towards fearful faces and away from angry faces.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Detecting and reacting to potential threats in the environment is an essential skill for survival. Emotional information that indicates threat in the environment, such as a fearful or angry face, confers perceptual and attentional advantages compared to neutral information (<xref ref-type="bibr" rid="bib17">Carretié, 2014</xref>; <xref ref-type="bibr" rid="bib66">Pourtois et al., 2013</xref>; <xref ref-type="bibr" rid="bib90">Vuilleumier, 2005</xref>): Emotional information enhances visual sensitivity (<xref ref-type="bibr" rid="bib64">Phelps et al., 2006</xref>; <xref ref-type="bibr" rid="bib28">Fox et al., 2000</xref>), potentiates effects of attention on visual sensitivity (<xref ref-type="bibr" rid="bib26">Ferneyhough et al., 2013</xref>; <xref ref-type="bibr" rid="bib12">Bocanegra and Zeelenberg, 2009</xref>; <xref ref-type="bibr" rid="bib13">Bocanegra and Zeelenberg, 2011a</xref>; <xref ref-type="bibr" rid="bib14">Bocanegra and Zeelenberg, 2011b</xref>; <xref ref-type="bibr" rid="bib61">Ohman et al., 2001</xref>; <xref ref-type="bibr" rid="bib49">Lundqvist and Ohman, 2005</xref>) and on appearance (<xref ref-type="bibr" rid="bib6">Barbot and Carrasco, 2018</xref>), and gains preferential access to awareness (<xref ref-type="bibr" rid="bib2">Amting et al., 2010</xref>; <xref ref-type="bibr" rid="bib95">Yang et al., 2007</xref>; <xref ref-type="bibr" rid="bib55">Milders et al., 2006</xref>; <xref ref-type="bibr" rid="bib31">Hedger et al., 2015</xref>). The advantages of emotional information extend to actions. Eye (<xref ref-type="bibr" rid="bib3">Bannerman et al., 2009a</xref>; <xref ref-type="bibr" rid="bib4">Bannerman et al., 2009b</xref>; <xref ref-type="bibr" rid="bib5">Bannerman et al., 2010</xref>; <xref ref-type="bibr" rid="bib60">Nummenmaa et al., 2009</xref>) - or pointing (<xref ref-type="bibr" rid="bib88">Valk et al., 2015</xref>) movements towards emotional –especially threat-related stimuli– are facilitated, whereas saccades away from these are delayed (<xref ref-type="bibr" rid="bib9">Belopolsky et al., 2011</xref>; <xref ref-type="bibr" rid="bib40">Kissler and Keil, 2008</xref>). Further, emotional information influences gaze trajectories, that is directly guides eye movements. Usually, people’s eyes are attracted more towards emotional than neutral faces (<xref ref-type="bibr" rid="bib58">Mogg et al., 2007</xref>; <xref ref-type="bibr" rid="bib42">Kret et al., 2013a</xref>; <xref ref-type="bibr" rid="bib43">Kret et al., 2013b</xref>), yet, sometimes gaze is repelled from threat-related –angry or fearful– faces (<xref ref-type="bibr" rid="bib35">Hunnius et al., 2011</xref>; <xref ref-type="bibr" rid="bib7">Becker and Detweiler-Bedell, 2009</xref>; <xref ref-type="bibr" rid="bib70">Schmidt et al., 2012</xref>).</p><p>Remarkably, the recognition of a face’s emotional expression does not require awareness of the face. (Some authors refer to ‘awareness’ and ‘consciousness’ interchangeably; in line with a recent review on the dissociations of perception and eye movements for neutral stimuli (<xref ref-type="bibr" rid="bib74">Spering and Carrasco, 2015</xref>), we use the term awareness and operationally define it as the ability to make an explicit perceptual report). Cortically blind individuals are able to correctly ‘guess’ the emotions of faces they cannot see (<xref ref-type="bibr" rid="bib20">de Gelder et al., 2005</xref>; <xref ref-type="bibr" rid="bib19">de Gelder et al., 1999</xref>; <xref ref-type="bibr" rid="bib62">Pegna et al., 2005</xref>; <xref ref-type="bibr" rid="bib78">Tamietto et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Bertini et al., 2013</xref>; <xref ref-type="bibr" rid="bib77">Striemer et al., 2017</xref>). In neurologically intact observers, emotional — especially threat-related — information they remain entirely unaware of, is prioritized over neutral information (<xref ref-type="bibr" rid="bib32">Hedger et al., 2016</xref>), facilitates visual discrimination (<xref ref-type="bibr" rid="bib11">Bertini et al., 2017</xref>), and alters subsequent perceptual (<xref ref-type="bibr" rid="bib1">Almeida et al., 2013</xref>) and discrimination judgments differentially for the unseen emotion (<xref ref-type="bibr" rid="bib97">Zhan and de Gelder, 2018</xref>). Emotional stimuli observers are unaware of also elicit physiological reactions, for example changes in skin conductance (<xref ref-type="bibr" rid="bib24">Esteves et al., 1994</xref>), facial muscle activity and pupil dilation (<xref ref-type="bibr" rid="bib78">Tamietto et al., 2009</xref>), and activate subcortical structures, for example the amygdala, pulvinar, basal ganglia and superior colliculus (<xref ref-type="bibr" rid="bib80">Tamietto and de Gelder, 2010</xref>; <xref ref-type="bibr" rid="bib36">Jiang and He, 2006</xref>; <xref ref-type="bibr" rid="bib84">Troiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib85">Troiani and Schultz, 2013</xref>), as well as cortical structures (<xref ref-type="bibr" rid="bib63">Pessoa and Adolphs, 2010</xref>). Furthermore, suppressed emotional stimuli influence oculomotor response times; when observers saccade towards color patches to report a mask’s color, saccades initiation is slower departing from masked angry faces than from masked happy faces (<xref ref-type="bibr" rid="bib83">Terburg et al., 2012</xref>). Despite this evidence indicating perceptual and attentional advantages, it is unknown whether suppressed emotional information can also directly guide actions or whether we have to become aware of the threat before we can act upon it.</p><p>Eye movements are the ideal testing ground for the relation between suppressed emotional information and actions, as recent evidence has shown that eye movements and visual awareness for neutral stimuli can be dissociated (<xref ref-type="bibr" rid="bib74">Spering and Carrasco, 2015</xref>). Whether and how oculomotor actions are guided by unaware emotional information can be investigated by suppressing stimuli from awareness while measuring eye movements. The dissociation of eye movements and visual awareness reflects situations in which the direction of eye movements change in response to particular visual features, such as orientation or motion direction, even though observers are unable to report these features because they are not aware of them (<xref ref-type="bibr" rid="bib74">Spering and Carrasco, 2015</xref>; <xref ref-type="bibr" rid="bib73">Spering and Carrasco, 2012</xref>; <xref ref-type="bibr" rid="bib72">Spering et al., 2011</xref>; <xref ref-type="bibr" rid="bib68">Rothkirch et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Glasser and Tadin, 2014</xref>; <xref ref-type="bibr" rid="bib44">Kuhn and Land, 2006</xref>; <xref ref-type="bibr" rid="bib71">Simoncini et al., 2012</xref>; <xref ref-type="bibr" rid="bib81">Tavassoli and Ringach, 2010</xref>). In other words, suppressed neutral stimuli determine the trajectory of eye movements in the absence of awareness. Moreover, visual attention can modulate the processing of neutral stimuli and the eye movements they elicit even when observers are unaware of them (<xref ref-type="bibr" rid="bib73">Spering and Carrasco, 2012</xref>).</p><p>Angry and fearful faces convey different types of threat, and elicit different reactions under awareness. Fearful faces indicate a potential indirect threat in the environment of the fearful person without indicating the source of the threat, suggesting more information is needed to appraise the situation (<xref ref-type="bibr" rid="bib18">Davis et al., 2011</xref>). This ambiguity leads to increased amygdala activation (<xref ref-type="bibr" rid="bib91">Whalen et al., 2013</xref>), heightens visual attention to the seen fearful stimulus location (<xref ref-type="bibr" rid="bib64">Phelps et al., 2006</xref>; <xref ref-type="bibr" rid="bib26">Ferneyhough et al., 2013</xref>) and attracts eye movements that enable detailed processing of the face and its environment (<xref ref-type="bibr" rid="bib58">Mogg et al., 2007</xref>). Angry faces indicate direct threats (a person with an angry expression looking directly at the observer) leading to avoidance (<xref ref-type="bibr" rid="bib70">Schmidt et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">Marsh et al., 2005</xref>) or freezing (<xref ref-type="bibr" rid="bib67">Roelofs et al., 2010</xref>) responses. When looking at angry faces observers show a stronger startle reflex (<xref ref-type="bibr" rid="bib75">Springer et al., 2007</xref>), stronger activation of the torso muscles (<xref ref-type="bibr" rid="bib34">Huis In 't Veld et al., 2014</xref>), widening of the pupils (<xref ref-type="bibr" rid="bib43">Kret et al., 2013b</xref>), and higher activation in brain areas associated with defense preparation (<xref ref-type="bibr" rid="bib93">Williams et al., 2005</xref>; <xref ref-type="bibr" rid="bib65">Pichon et al., 2009</xref>; <xref ref-type="bibr" rid="bib41">Kret et al., 2011</xref>) than when looking at fearful faces. Threat processing is considered to evolve along two distinctive neuronal pathways, a fast sub-cortical and a slower cortical route (<xref ref-type="bibr" rid="bib46">LeDoux, 1998</xref>). There is some evidence that fear information, relative to anger information, may be preferentially processed along the fast, subcortical route (<xref ref-type="bibr" rid="bib50">Luo et al., 2007</xref>). This rich body of research suggests differential processing of fear and anger under awareness. Yet, when observers are presented with unmasked angry and fearful faces, both attract (<xref ref-type="bibr" rid="bib58">Mogg et al., 2007</xref>; <xref ref-type="bibr" rid="bib42">Kret et al., 2013a</xref>) or repel (<xref ref-type="bibr" rid="bib35">Hunnius et al., 2011</xref>; <xref ref-type="bibr" rid="bib7">Becker and Detweiler-Bedell, 2009</xref>) their attention and eye gaze equally strongly. Of note, there is only circumstantial evidence for distinct effects of emotional faces in the absence of awareness: Fearful faces have preferential access to awareness (<xref ref-type="bibr" rid="bib95">Yang et al., 2007</xref>; <xref ref-type="bibr" rid="bib55">Milders et al., 2006</xref>; <xref ref-type="bibr" rid="bib84">Troiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib37">Jusyte et al., 2015</xref>), whereas angry faces remain suppressed longer (<xref ref-type="bibr" rid="bib37">Jusyte et al., 2015</xref>; <xref ref-type="bibr" rid="bib96">Zhan et al., 2015</xref>), both compared to neutral faces presented during continuous-flash suppression (CFS).</p><p>Our study had two goals: (1) To test whether threat-related emotional stimuli can guide eye movements in the absence of visual awareness. Were suppressed emotional faces to guide eye movements, their trajectory would depend on the location of the emotional stimuli. (2) To test whether these oculomotor actions in the absence of awareness are distinct for different threat-related facial emotions. Were threat-related emotional information to guide eye-movements, the trajectories could be similar or differential, that is depend on the nature of the threat, direct — angry faces — or indirect — fearful faces.</p><p>We investigated these critical open questions by assessing whether different threat-related stimuli can direct oculomotor actions in the absence of awareness, as indicated by objective and subjective measures, and whether those actions are similar or differential by contrasting eye movements elicited by angry faces and fearful faces.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We rendered emotional faces unaware using continuous flash suppression (<xref ref-type="bibr" rid="bib87">Tsuchiya and Koch, 2005</xref>; <xref ref-type="bibr" rid="bib25">Fang and He, 2005</xref>). Low contrast face stimuli located in one of four quadrants of the visual display were presented to the non-dominant eye and a high contrast flickering mask was presented to the dominant eye (<xref ref-type="fig" rid="fig1">Figure 1</xref>). As such, the viewer is only aware of the flickering mask, but not of the stimulus presented to the non-dominant eye. In this way, continuous flash suppression suppresses stimuli for up to several seconds, enabling the measurement of eye movements in the absence of awareness (<xref ref-type="bibr" rid="bib68">Rothkirch et al., 2012</xref>; <xref ref-type="bibr" rid="bib51">Madipakkam et al., 2016</xref>).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.43467.002</object-id><label>Figure 1.</label><caption><title>Stimuli and Experimental Design.</title><p>(<bold>A</bold>) Continuous flash suppression was used to suppress low-contrast emotional face stimuli from awareness: when a high-contrast colored flickering mask is presented to the dominant eye (using a stereoscope) the viewer will not be aware of the picture presented to the non-dominant eye for several seconds. (<bold>B</bold>) Face stimuli with either a neutral, angry, or fearful expression (10 identities, five male, five female; see Materials and methods) were presented to the non-dominant eye. Faces were placed in any of the 4 quadrants of the stimulus field, either upright or upside down (to control for low-level visual features). (<bold>C</bold>) After a mandatory fixation period of 200 ms, the face stimulus was gradually faded in for 500 ms and fully presented to the non-dominant eye for 1000 ms while the flickering mask was continuously presented to the dominant eye. The mask was displayed for a further 200 ms to prevent aftereffects of the face stimulus. At the end of each trial, participants indicated the location of the face stimulus, its emotional expression, and its visibility by a button press.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-43467-fig1-v2.tif"/><permissions><copyright-statement>© 1998 Karolinska Institutet. All rights reserved</copyright-statement><copyright-year>1998</copyright-year><copyright-holder>Karolinska Institutet</copyright-holder><license><license-p>Face stimuli in panels A, CB, and C are reproduced, with permission, from The Karolinska directed emotional faces (KDEF) (<xref ref-type="bibr" rid="bib48">Lundqvist et al., 1998</xref>). These images are not available under CC-BY and are exempt from the CC-BY 4.0 license. They may not be redistributed or shared without written consent from the copyright holder (Karolinska Institutet, Psychology section). The ID for the face depicted in this figure is AM10 and the IDs for the faces used in experiments are AM05, AM06, AM08, AM10, AM17, AF01, AF09, AF14, AF19, and AF20.</license-p></license></permissions></fig><p>To investigate the extent to which the different emotional facial expressions are processed and whether they affect eye movements differentially, we used two negative-valence expressions, <italic>fearful</italic> faces, indicating an indirect threat in the environment, and <italic>angry</italic> faces, indicating a direct threat from the depicted person. We also used <italic>neutral</italic> faces to control for the effects of face stimuli on eye movements that are unrelated to threat. To rule out the possibility that eye movements could have been simply triggered by low-level visual features (e.g., contrast differences between the eye and mouth regions), we also presented the same stimulus set upside down, as done in previous studies (<xref ref-type="bibr" rid="bib64">Phelps et al., 2006</xref>; <xref ref-type="bibr" rid="bib12">Bocanegra and Zeelenberg, 2009</xref>; <xref ref-type="bibr" rid="bib6">Barbot and Carrasco, 2018</xref>) (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>We determined full suppression of visual awareness based on subjective visibility ratings, whose validity we verified for each observer using objective measures. This verification is essential as subjective measures alone could reflect criterion rather than discriminability differences (<xref ref-type="bibr" rid="bib32">Hedger et al., 2016</xref>; <xref ref-type="bibr" rid="bib76">Sterzer et al., 2014</xref>). After each trial, participants (a) judged the location and emotional expression of the face stimulus and (b) rated its visibility (<xref ref-type="fig" rid="fig2">Figure 2</xref>). We analyzed only those trials in which subjective visibility was zero, <italic>and</italic> only data from participants’ whose objective performance across trials with subjective zero visibility was at chance level for each of the two tasks, location and emotional expression, matching their subjective impression.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.43467.003</object-id><label>Figure 2.</label><caption><title>Objective and subjective measures of awareness of face stimuli presented under continuous flash suppression (CFS; circles) or on top of the flickering mask (squares).</title><p>Proportion correct values for the position (y-axis) and emotion (x-axis) tasks are shown for each subjective rating of visibility (note that not all visibility ratings were selected in every condition). The area of the symbols corresponds to the average number of trials with the respective visibility rating per participant. Grey lines indicate chance level. Error bars show standard errors of the mean (SEM).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-43467-fig2-v2.tif"/></fig><p>Despite face stimuli being fully suppressed from awareness, participants moved their eyes towards fearful faces and away from angry faces, compared to neutral faces. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows gaze position relative to the face stimulus as a function of time. Most locations within the display were further away from the face than the fixation point. Thus, the distance between gaze position and face increased for all faces when participants moved their eyes after an initial mandatory fixation period. At ~400 ms after full display was reached, distance of gaze position to fearful faces decreased compared to neutral faces, indicating an orienting of gaze towards fearful faces. In contrast, distance of gaze position to angry faces increased compared to the distance to neutral faces, indicating gaze aversion from angry faces. This pattern of results was not observed with upside-down presented faces. The absence of an effect for upside-down presented faces rules out the possibility that low-level visual features were responsible for the effect observed with upright presented emotional faces, a confound not controlled for in previous studies (<xref ref-type="bibr" rid="bib15">Bodenschatz et al., 2018</xref>) (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.43467.004</object-id><label>Figure 3.</label><caption><title>Time course of gaze distance from target.</title><p>Mean distance of gaze position from the center of the face stimulus is plotted for all time points (±1 SEM shaded area) separately for upright (top panel) and upside down (bottom panel) presented face stimuli displaying fearful (blue), angry (red), or neutral (grey) emotions. Gaze data were included only for trials in which faces were fully suppressed from awareness (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). After the fixation period, mean gaze distance increased in all conditions, as participants moved their eyes freely, and most areas of the display were located further away from the target than the initially-fixated center point. After ~400 ms of full stimulus display, mean gaze distance to upright fearful faces decreased, indicating an orienting towards fearful faces. In contrast, mean gaze distance to upright angry faces increased, indicating gaze aversion from an angry face. Grey bars represent significant clusters (<italic>p </italic>&lt; 0.001) of adjacent time points (1 ms temporal resolution). At all time points within a cluster, a significant difference in distance between the two respective emotional expressions (colored textboxes) emerged (<italic>p </italic>&lt; 0.05, corrected for multiple comparisons).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-43467-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.43467.005</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Time course of gaze distance from target overlaid with time course of average gaze distance from possible target locations for trials in which no target was presented (green).</title><p>This line closely resembles the timeline for trials in which a neutral upright or any upside-down face was presented.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-43467-fig3-figsupp1-v2.tif"/></fig></fig-group><p>To determine the preferentially viewed location for each emotion, we contrasted the spatial distribution of dwell times for upright faces with their upside-down counterparts; the latter provided a suitable baseline as they evoked no emotion-specific eye movements (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This dwell time analysis (<xref ref-type="fig" rid="fig4">Figure 4</xref>) showed that, observers looked significantly longer towards the location at which an upright fearful face was displayed: dwell times at the target location increased by 57% compared to trials with upside-down presented fearful faces. In contrast, observers looked significantly longer towards an adjacent location when an upright compared to an upside-down angry face was displayed, demonstrating a consistent and lasting gaze aversion from an angry face. No effect emerged for neutral faces.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.43467.006</object-id><label>Figure 4.</label><caption><title>Spatial distribution of mean dwell time differences.</title><p>Differences in dwell time between upright and upside down presented faces across the stimulus field, divided into four quadrants and three eccentricities. Data were aligned such that the target position is in the upper right quadrant (red square). Dwell time differences show an orienting towards the position of upright fearful faces (upright 62.82 ± 10.80 ms SEM, upside down 36.37 ± 6.45 ms; red dots: <italic>p</italic> &lt; 0.05, corrected for multiple comparisons), and an aversion of gaze away from the position of upright angry faces (upright 51.57 ± 8.72 ms; upside down 33.62 ± 4.36 ms), both compared to upside-down presented faces of the same emotion.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-43467-fig4-v2.tif"/></fig></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We found that in the absence of visual awareness, observers’ eye movements were attracted towards fearful faces and repelled away from angry faces, compared to neutral faces. These novel results revealed that threat-related emotional faces can <italic>guide oculomotor actions</italic> without entering awareness. Our objective performance measures (<xref ref-type="fig" rid="fig2">Figure 2</xref>, red dots) validate the subjective visibility ratings, and together indicate that the emotional faces were suppressed from perceptual awareness. The absence of an effect with upside-down presented angry faces precludes confounds with low-level feature differences between faces of different emotional expressions.</p><p>Remarkably, the elicited eye movements were specific to the displayed facial emotion regarding the type of threat it conveyed. These distinct consequences on oculomotor actions demonstrate <italic>qualitative processing</italic> of the emotional expression, rather than mere detection of emotional stimuli in the absence of awareness. Previous studies on emotional face perception predominantly demonstrated preferential emergence of fearful faces into awareness (<xref ref-type="bibr" rid="bib32">Hedger et al., 2016</xref>), which could simply indicate unspecific arousal caused by emotional content.</p><p><italic>How can emotional stimuli directly guide oculomotor actions in the absence of awareness?</italic> For neutral stimuli, dissociations between visual awareness and eye movements, revealing more sensitive processing of information, have been related to the involvement of a fast subcortical retinocollicular pathway (<xref ref-type="bibr" rid="bib74">Spering and Carrasco, 2015</xref>). Furthermore, processing of unaware emotional visual information has been associated with a subcortical pathway involving the amygdala, pulvinar, and superior colliculus (<xref ref-type="bibr" rid="bib80">Tamietto and de Gelder, 2010</xref>), as well as structural connections between the amygdala and cortical motor‐related areas (<xref ref-type="bibr" rid="bib30">Grèzes et al., 2014</xref>). The superior colliculus plays a crucial role in the control of voluntary and involuntary eye movements (<xref ref-type="bibr" rid="bib8">Bell and Munoz, 2008</xref>). Thus, in the absence of awareness, orienting of eye movements towards and away from emotional faces could be mediated by these subcortical structures, in addition to the potential involvement of a cortical pathway (<xref ref-type="bibr" rid="bib63">Pessoa and Adolphs, 2010</xref>). A possible mechanism could be that the amygdala assesses facial emotional expression, and the superior colliculus guides eye movements according to a fight, flight, or freeze response.</p><p><italic>Why did participants look towards fearful faces, but avert their gaze away from angry faces, in the absence of awareness?</italic> As mentioned in the Introduction, under awareness these two threat-related emotions have differential perceptual consequences (<xref ref-type="bibr" rid="bib43">Kret et al., 2013b</xref>; <xref ref-type="bibr" rid="bib53">Marsh et al., 2005</xref>; <xref ref-type="bibr" rid="bib75">Springer et al., 2007</xref>; <xref ref-type="bibr" rid="bib34">Huis In 't Veld et al., 2014</xref>) and may be mediated by different neural pathways (<xref ref-type="bibr" rid="bib50">Luo et al., 2007</xref>). In the absence of awareness, before this study, there had been only circumstantial evidence for differential processing of angry and fearful faces according to the time they take to break into awareness (<xref ref-type="bibr" rid="bib95">Yang et al., 2007</xref>; <xref ref-type="bibr" rid="bib55">Milders et al., 2006</xref>; <xref ref-type="bibr" rid="bib84">Troiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib37">Jusyte et al., 2015</xref>; <xref ref-type="bibr" rid="bib96">Zhan et al., 2015</xref>). With respect to eye movements, under awareness, both angry and fearful faces have shown to elicit similar eye movement responses, either attraction (<xref ref-type="bibr" rid="bib58">Mogg et al., 2007</xref>; <xref ref-type="bibr" rid="bib42">Kret et al., 2013a</xref>) or repulsion (<xref ref-type="bibr" rid="bib35">Hunnius et al., 2011</xref>; <xref ref-type="bibr" rid="bib7">Becker and Detweiler-Bedell, 2009</xref>). In the absence of awareness, gaze avoidance from masked angry faces had been demonstrated only indirectly with eyes starting instructed movements towards a response target more slowly in the presence of an angry than a happy face (<xref ref-type="bibr" rid="bib82">Terburg et al., 2011</xref>). Instead, our results, directly measured with eye tracking, show uninstructed, sustained gaze avoidance to an angry face but sustained gaze attraction to a fearful face, while the faces are entirely suppressed from awareness. Fearful faces are more ambiguous and directing our eyes towards them may be an automatic response to try to gather more information (<xref ref-type="bibr" rid="bib3">Bannerman et al., 2009a</xref>; <xref ref-type="bibr" rid="bib58">Mogg et al., 2007</xref>; <xref ref-type="bibr" rid="bib18">Davis et al., 2011</xref>). In contrast, angry faces pose a direct threat and directing our eyes away from them may be an automatic avoidance response (<xref ref-type="bibr" rid="bib18">Davis et al., 2011</xref>).</p><p>Comparing our finding of differential eye movements in the absence of awareness with the previous findings of similar eye movements under awareness (<xref ref-type="bibr" rid="bib58">Mogg et al., 2007</xref>; <xref ref-type="bibr" rid="bib42">Kret et al., 2013a</xref>; <xref ref-type="bibr" rid="bib35">Hunnius et al., 2011</xref>; <xref ref-type="bibr" rid="bib7">Becker and Detweiler-Bedell, 2009</xref>) suggest that different factors may influence eye movements under aware and unaware conditions. Across studies, perceptual (<xref ref-type="bibr" rid="bib45">Lapate et al., 2016</xref>), attentional (<xref ref-type="bibr" rid="bib57">Mogg et al., 1994</xref>; <xref ref-type="bibr" rid="bib27">Fox, 1996</xref>) and physiological (<xref ref-type="bibr" rid="bib79">Tamietto et al., 2015</xref>) effects of emotional information either differ qualitatively or tend to be stronger in the absence than in the presence of awareness (<xref ref-type="bibr" rid="bib22">Diano et al., 2017</xref>). Moreover, awareness modulates the time course (<xref ref-type="bibr" rid="bib47">Liddell et al., 2004</xref>) and locus (<xref ref-type="bibr" rid="bib79">Tamietto et al., 2015</xref>) of neural activations elicited by threat-related information, as well as functional connectivity between the amygdala and pre-frontal areas (<xref ref-type="bibr" rid="bib2">Amting et al., 2010</xref>; <xref ref-type="bibr" rid="bib45">Lapate et al., 2016</xref>; <xref ref-type="bibr" rid="bib94">Williams et al., 2006</xref>). Two different mechanisms might cause threat-related emotions guiding actions differentially in the absence, but not in the presence of awareness: a) the perceived intensity of the threat might be stronger in the absence of awareness (<xref ref-type="bibr" rid="bib45">Lapate et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Liddell et al., 2004</xref>), which in turn may facilitate actions specifically tailored towards the nature of the threat, indirect or direct; b) cognitive mechanisms might suppress automatic actions upon threat-related information only under awareness (<xref ref-type="bibr" rid="bib91">Whalen et al., 2013</xref>; <xref ref-type="bibr" rid="bib57">Mogg et al., 1994</xref>), but not in its absence, as in our present findings. Several factors may help overwrite automatic eye movements towards fearful or away from angry faces, ranging from mere adherence to the task instructions (<xref ref-type="bibr" rid="bib7">Becker and Detweiler-Bedell, 2009</xref>) to regulation of the social communication entailed in holding or averting gaze (<xref ref-type="bibr" rid="bib82">Terburg et al., 2011</xref>; <xref ref-type="bibr" rid="bib23">Emery, 2000</xref>; <xref ref-type="bibr" rid="bib39">Kendon, 1967</xref>).</p><p>To conclude, our results provide the first evidence that emotional information humans are unaware of can differentially modulate their eye movements without entering awareness.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Twelve participants (four females, mean age 24.3 years) were included in the final analysis. All participants took part in exchange for course credits, indicated normal or corrected-to-normal vision, and signed an informed consent form. The experiment was conducted according to the guidelines of the Declaration of Helsinki and approved by the ethics committee of New York University.</p></sec><sec id="s4-2"><title>Stimuli and apparatus</title><p>We selected emotional face stimuli (neutral, angry, and fearful expressions, all looking straight ahead) from 10 different identities (five male, five female) from the Karolinska Directed Emotional Faces database (<xref ref-type="bibr" rid="bib48">Lundqvist et al., 1998</xref>). The face images were cropped into oval shapes to remove hair, and their edges were smoothed to blend in with the grey background. Face stimuli were rendered low contrast, equated for overall luminance and contrast, and matched with the luminance of the background using the SHINE toolbox (<xref ref-type="bibr" rid="bib92">Willenbockel et al., 2010</xref>) in MATLAB. Face images (1.79° x 2.70° visual angle) were presented either upright or upside down (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) to control for low level visual features independent of emotional expression. The emotional expressions of upside down presented faces can elicit differential effects when these are presented for several seconds (<xref ref-type="bibr" rid="bib54">McKelvie, 1995</xref>; <xref ref-type="bibr" rid="bib16">Calvo and Nummenmaa, 2008</xref>; <xref ref-type="bibr" rid="bib21">Derntl et al., 2009</xref>; <xref ref-type="bibr" rid="bib59">Narme et al., 2011</xref>), but with shorter presentation times emotion-specific effects are limited to upright presented faces (<xref ref-type="bibr" rid="bib64">Phelps et al., 2006</xref>; <xref ref-type="bibr" rid="bib12">Bocanegra and Zeelenberg, 2009</xref>; <xref ref-type="bibr" rid="bib6">Barbot and Carrasco, 2018</xref>). Faces were displayed in any of the 4 quadrants of the stimulus field (counterbalanced) to allow eye movements to be directed to different parts of the visual field. For continuous flash suppression, a set of colorful, high contrast flickering masks were created with randomly chosen shapes and colors, and each set of masks was created anew for each trial. Importantly, no mask image was ever exactly the same as another; therefore systematic features of the mask could not have directed eye movements. Mask images were displayed at a frequency of 28.3 Hz (<xref ref-type="bibr" rid="bib38">Kaunitz et al., 2014</xref>). To help fuse the two images (<xref ref-type="fig" rid="fig1">Figure 1</xref>) while viewed through a stereoscope, black and white bars framed the stimulus field for each eye (4.67° x 6.23°). A mirror stereoscope (ScreenScope, ASC Scientific, Carlsbad, USA) was used to display different images to each eye. The flickering mask was always presented to the dominant eye (as determined with a hole-in-card test (<xref ref-type="bibr" rid="bib56">Miles, 1930</xref>) at the start of the experiment for each participant) and the low contrast face stimulus was presented to the non-dominant eye to achieve best possible suppression. Stimuli were created and presented using MATLAB and the Cogent toolbox (Wellcome Department of Imaging Neuroscience, University College London) and displayed on a CRT monitor (IBM P260, 85 Hz, resolution 1280 × 1024). Participants were seated 114 cm from the monitor, placing their head on a chin rest and looking through the stereoscope. Eye movements were recorded with an infrared eyetracking system at 1000 Hz (EyeLink 1000, SR Research, Ottawa, Canada).</p></sec><sec id="s4-3"><title>Tasks</title><p>During mask presentation, participants were instructed to look for a face hidden behind the flickering mask. After each trial, they answered three questions by button press (maximum allowed response time was 2 s for each question). 1) Position task: Indicate the quadrant in which the picture was presented. 2) Expression task: Indicate the emotional expression of the face as angry, fearful, or neutral. 3) Visibility rating: Rate the visibility of the face stimulus as either ‘not at all seen’, ‘brief glimpse’, ‘almost clear’, or ‘very clear ‘(perceptual awareness scale (<xref ref-type="bibr" rid="bib69">Sandberg et al., 2010</xref>)). Two variants of numbering the quadrants and of assigning the keys to the three emotional expressions were counterbalanced across participants. The order of position and expression task was counterbalanced across participants, but the visibility rating was always administered last. The position and the expression tasks served as objective measures of awareness, that is determined whether participants were able to detect and identify the face stimulus above chance level. The visibility rating served as subjective measure of awareness. We only analyzed data from participants’ whose performance in the localization and expression tasks was at chance when subjective visibility was rated zero (see Data Exclusion).</p></sec><sec id="s4-4"><title>Procedure</title><p>The experiment comprised 288 trials divided into three blocks to allow participants to rest and to re-calibrate the eye tracker. In 240 trials, the face stimulus was presented to the non-dominant eye and the flickering mask to the dominant eye to suppress the face image from visual awareness. In 24 trials, the face image was displayed on top of the mask presented to the dominant eye, so that the face stimulus was clearly visible (while overall visual stimulation intensity remained the same as during suppressed presentation). These non-suppressed trials were introduced as positive control, to ensure that participants paid attention and reported the image’s position and emotion correctly when it was visible. In another 24 trials, only the mask was presented, with no face. These catch trials were included to measure baseline eye movement patterns and face position and emotion ratings without any face stimulus presentation to the non-dominant eye. Suppressed, non-suppressed, and catch trials were presented in random order, as were the face images’ emotional expression, identity, orientation, and position.</p><p>Each trial started with a mandatory fixation period of 200 ms (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), that is the trial was only initiated after participants maintained gaze at a central fixation cross for at least 200 ms. Then the flickering mask appeared to the dominant eye, and the face stimulus, presented to the non-dominant eye, was gradually faded in for 500 ms, to make the suppression more effective (<xref ref-type="bibr" rid="bib86">Tsuchiya et al., 2006</xref>). The face stimulus was subsequently displayed to the non-dominant eye for 1000 ms before it disappeared. The mask was displayed for another 200 ms to prevent afterimages of the face stimulus.</p></sec><sec id="s4-5"><title>Data exclusion</title><p>The suppressive effects of continuous flash suppression vary inter- and intra-individually (<xref ref-type="bibr" rid="bib33">Hesselmann et al., 2016</xref>). We employed subjective and objective measures to ensure that only trials in which the face stimulus was indeed suppressed from awareness were included in the analysis. First, only trials with a subjective visibility rating of zero (‘not seen at all’) were included in the analysis. Second, to validate these subjective judgments, participants’ average accuracy in the emotion and in the position task was calculated separately for each level of subjective face visibility. Of special interest were trials in which participants reported to not have seen the stimulus (<xref ref-type="fig" rid="fig2">Figure 2</xref>, red circles). In the emotion task, participants were biased towards responding ‘neutral’ when they indicated that they had not seen the face (while at the same time they were at chance level for the position task; <xref ref-type="fig" rid="fig2">Figure 2</xref>). Yet, when averaging across trials all participants’ proportion correct in the emotion task was close to chance level. In the position task, two participants performed clearly above chance level (78% and 67% correct position responses) even though they indicated to not have seen the stimulus. Data from these two participants were excluded from all further analyses. Two additional participants indicated to not have seen the face in only a low percentage of trials, thus, their data were too sparse for the analysis of gaze position over time (see below) and these participants were excluded from analyses of suppressed trials as well. For two participants we had only partial data due to problems with the hardware, these were excluded from the analysis. Additionally, less than 1% of the remaining trials were excluded from analysis because less than 25% of the recorded gaze positions were located inside the stimulus field (for example, because the participant repeatedly blinked during the trial).</p></sec><sec id="s4-6"><title>Data analysis</title><p>For the time series analysis (<xref ref-type="fig" rid="fig3">Figure 3</xref>), gaze position was evaluated as a function of time. To compensate for fluctuations in the eye-tracking device, single trial measurements were baseline corrected by subtracting the respective participants’ average gaze coordinates during the fixation period from all measurements during the respective trial. To be able to analyze trials probing any of the four possible stimulus locations together, we calculated the Euclidian distance between gaze position and the center of the target quadrant for each time point in a trial. Mean distances were calculated per participant, time point, emotion, and face orientation. For each time point, these mean distances were compared between pairs of emotions, separately for each face orientation. For 87.82% of analyzed time points we had data from all participants, for the remaining 12.18% of time-points data were missing from at least one of the participants. At α = 0.05 the chances for false positive results, that is, significant test results without an underlying difference are at 5%. Each analyzed time segment comprised 1000 time points, resulting in a prediction of 50 false positive results per segment. To control for this alpha inflation, we employed permutation-based cluster mass tests (<xref ref-type="bibr" rid="bib52">Maris and Oostenveld, 2007</xref>): In step 1, pairwise paired t-tests were calculated for each time point from 500 ms after trial onset onwards, that is each time point at which the face stimulus was fully displayed. Six t-tests for paired samples were conducted at each time point, comparing the average distance between gaze and target within participants for all three possible pairings of the three emotional expressions, separately for upright and upside down presented faces. These t-tests were considered significant, if the absolute t-value was larger than 2. In step 2, clusters of adjacent time points with significant, equally directed differences between emotions were identified separately for each of the six comparisons (emotion pair x face orientation). That is, one temporal cluster comprised all sampled timepoints from start till end of the cluster and the t-tests conducted in step 1 had revealed a significant difference between the tested two emotions for all of these timepoints. For each emotion pair and face orientation the largest temporal cluster was determined based on the summed t-values of all time points within the cluster. These clusters were tested for significance by comparing these summed t-values to those of clusters derived from 1000 random permutations of the data. For each permutation, the emotion labels of a participant’s gaze-target distances were swapped with a probability of 50%. Labels were swapped within but not across participants, and within one permutation the same permutated labels were used for all time points. Step 1 was repeated on the permuted data, and the largest cluster was extracted and compared to the corresponding cluster of the original data. A cluster was considered significant if clusters of the same or larger size occurred in less than 5% of the randomly permuted datasets. By doing so, we determined the probability that the original cluster of adjacent time points with significant differences between emotions could occur by chance.</p><p>Further, the distribution of dwell times across the stimulus field, that is, the cumulative time spent fixating a location on the screen, was analyzed (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The analysis focused again on successfully suppressed trials, and – based on the results of the time series analysis – was restricted to gaze positions recorded at least 500 ms after full stimulus onset. Gaze position was transformed into polar coordinates and discretized by dividing the visual display into three equally spaced rings and four wedges. To allow averaging of trials across the four possible stimulus locations, the coordinates were rotated until the face was located in the upper-right quadrant. Average dwell times in each segment were compared between trials with upright versus upside down presented faces using paired t-tests, and this comparison was conducted separately for each emotion (step 1). Having established segments with significant differences in dwell time between trials with upright and upside down presented faces, we again used permutation cluster tests to avoid alpha inflation due to multiple comparisons (step 2). The clusters were now defined in space rather than time, that is, clusters were defined as adjacent segments with significant differences in the same direction. Minimal cluster size was one segment. Again, to evaluate the strength of a cluster, summed t-values were determined for each cluster within the original data and within 1000 permutations of the data. For the permutations, we randomly re-assigned the labels upright and upside down within each participant. The largest summed t-value of each permutation was extracted and the p-value of the largest cluster in the original data was determined based on the percentile of its summed t-value within this distribution. Thus, a spatial cluster (consisting of 1 or more adjacent segments of the stimulus space) was considered significant if the summed size of the effect within the cluster was not exceeded by more than 5% of clusters found in random permutations of the data.</p><p>Data and source files are available online (<xref ref-type="bibr" rid="bib89">Vetter et al., 2019</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/EyeMovementsSuppressedEmotionalFaces">https://github.com/elifesciences-publications/EyeMovementsSuppressedEmotionalFaces</ext-link>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgments</title><p>This study was supported by research fellowships from the Deutsche Forschungsgemeinschaft to PV (VE 739/1–1) and SB (BA5600/1-1) and by a grant from NIH-RO1-EY016200 to MC. We thank Jasmine Pan and Maura LaBrecque for help with data collection, members of the Carrasco Lab for discussions and comments on the manuscript, and Miriam Spering for comments on the manuscript.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Investigation, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Funding acquisition, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants took part in the experiment in exchange for course credits and signed an informed consent form. The experiment was conducted according to the guidelines of the Declaration of Helsinki and approved by the ethics committee of New York University (IRB# 13-9582).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.43467.007</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-43467-transrepform-v2.pdf"/></supplementary-material><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Source data and all analyses are available on Github (<ext-link ext-link-type="uri" xlink:href="https://github.com/StephBadde/EyeMovementsSuppressedEmotionalFaces">https://github.com/StephBadde/EyeMovementsSuppressedEmotionalFaces</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/EyeMovementsSuppressedEmotionalFaces">https://github.com/elifesciences-publications/EyeMovementsSuppressedEmotionalFaces</ext-link>).</p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Almeida</surname> <given-names>J</given-names></name><name><surname>Pajtas</surname> <given-names>PE</given-names></name><name><surname>Mahon</surname> <given-names>BZ</given-names></name><name><surname>Nakayama</surname> <given-names>K</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Affect of the unconscious: visually suppressed angry faces modulate our decisions. <italic>cognitive, affective, &amp;</italic></article-title><source>Behavioral Neuroscience</source><volume>13</volume><fpage>94</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.3758/s13415-012-0133-7</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amting</surname> <given-names>JM</given-names></name><name><surname>Greening</surname> <given-names>SG</given-names></name><name><surname>Mitchell</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Multiple mechanisms of consciousness: the neural correlates of emotional awareness</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>10039</fpage><lpage>10047</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6434-09.2010</pub-id><pub-id pub-id-type="pmid">20668188</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bannerman</surname> <given-names>RL</given-names></name><name><surname>Milders</surname> <given-names>M</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name><name><surname>Sahraie</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009a</year><article-title>Orienting to threat: faster localization of fearful facial expressions and body postures revealed by saccadic eye movements</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>276</volume><fpage>1635</fpage><lpage>1641</lpage><pub-id pub-id-type="doi">10.1098/rspb.2008.1744</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bannerman</surname> <given-names>RL</given-names></name><name><surname>Milders</surname> <given-names>M</given-names></name><name><surname>Sahraie</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009b</year><article-title>Processing emotional stimuli: comparison of saccadic and manual choice-reaction times</article-title><source>Cognition &amp; Emotion</source><volume>23</volume><fpage>930</fpage><lpage>954</lpage><pub-id pub-id-type="doi">10.1080/02699930802243303</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bannerman</surname> <given-names>RL</given-names></name><name><surname>Milders</surname> <given-names>M</given-names></name><name><surname>Sahraie</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attentional bias to brief threat-related faces revealed by saccadic eye movements</article-title><source>Emotion</source><volume>10</volume><fpage>733</fpage><lpage>738</lpage><pub-id pub-id-type="doi">10.1037/a0019354</pub-id><pub-id pub-id-type="pmid">21038958</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbot</surname> <given-names>A</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Emotion and anxiety potentiate the way attention alters visual appearance</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>e5938</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-23686-8</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becker</surname> <given-names>MW</given-names></name><name><surname>Detweiler-Bedell</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Short article: early detection and avoidance of threatening faces during passive viewing</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>62</volume><fpage>1257</fpage><lpage>1264</lpage><pub-id pub-id-type="doi">10.1080/17470210902725753</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname> <given-names>AH</given-names></name><name><surname>Munoz</surname> <given-names>DP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Activity in the superior colliculus reflects dynamic interactions between voluntary and involuntary influences on orienting behaviour</article-title><source>European Journal of Neuroscience</source><volume>28</volume><fpage>1654</fpage><lpage>1660</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2008.06393.x</pub-id><pub-id pub-id-type="pmid">18691327</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belopolsky</surname> <given-names>AV</given-names></name><name><surname>Devue</surname> <given-names>C</given-names></name><name><surname>Theeuwes</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Angry faces hold the eyes</article-title><source>Visual Cognition</source><volume>19</volume><fpage>27</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1080/13506285.2010.536186</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertini</surname> <given-names>C</given-names></name><name><surname>Cecere</surname> <given-names>R</given-names></name><name><surname>Làdavas</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>I am blind, but I &quot;see&quot; fear</article-title><source>Cortex</source><volume>49</volume><fpage>985</fpage><lpage>993</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2012.02.006</pub-id><pub-id pub-id-type="pmid">22480404</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bertini</surname> <given-names>C</given-names></name><name><surname>Cecere</surname> <given-names>R</given-names></name><name><surname>Làdavas</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Unseen fearful faces facilitate visual discrimination in the intact field</chapter-title><source>Neuropsychologia</source><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bocanegra</surname> <given-names>BR</given-names></name><name><surname>Zeelenberg</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Emotion improves and impairs early vision</article-title><source>Psychological Science</source><volume>20</volume><fpage>707</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02354.x</pub-id><pub-id pub-id-type="pmid">19422624</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bocanegra</surname> <given-names>BR</given-names></name><name><surname>Zeelenberg</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Emotion-induced trade-offs in spatiotemporal vision</article-title><source>Journal of Experimental Psychology: General</source><volume>140</volume><fpage>272</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.1037/a0023188</pub-id><pub-id pub-id-type="pmid">21443382</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bocanegra</surname> <given-names>BR</given-names></name><name><surname>Zeelenberg</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>Emotional cues enhance the attentional effects on spatial and temporal resolution</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>18</volume><fpage>1071</fpage><lpage>1076</lpage><pub-id pub-id-type="doi">10.3758/s13423-011-0156-z</pub-id><pub-id pub-id-type="pmid">21901512</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bodenschatz</surname> <given-names>CM</given-names></name><name><surname>Kersting</surname> <given-names>A</given-names></name><name><surname>Suslow</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Effects of briefly presented masked emotional facial expressions on gaze behavior: an Eye-Tracking study</article-title><source>Psychological Reports</source><volume>35</volume><fpage>003329411878904</fpage><pub-id pub-id-type="doi">10.1177/0033294118789041</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvo</surname> <given-names>MG</given-names></name><name><surname>Nummenmaa</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Detection of emotional faces: salient physical features guide effective visual search</article-title><source>Journal of Experimental Psychology: General</source><volume>137</volume><fpage>471</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1037/a0012771</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carretié</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Exogenous (automatic) attention to emotional stimuli: a review</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>14</volume><fpage>1228</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.3758/s13415-014-0270-2</pub-id><pub-id pub-id-type="pmid">24683062</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname> <given-names>FC</given-names></name><name><surname>Somerville</surname> <given-names>LH</given-names></name><name><surname>Ruberry</surname> <given-names>EJ</given-names></name><name><surname>Berry</surname> <given-names>AB</given-names></name><name><surname>Shin</surname> <given-names>LM</given-names></name><name><surname>Whalen</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A tale of two negatives: differential memory modulation by threat-related facial expressions</article-title><source>Emotion</source><volume>11</volume><fpage>647</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1037/a0021625</pub-id><pub-id pub-id-type="pmid">21668114</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Gelder</surname> <given-names>B</given-names></name><name><surname>Vroomen</surname> <given-names>J</given-names></name><name><surname>Pourtois</surname> <given-names>G</given-names></name><name><surname>Weiskrantz</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Non-conscious recognition of affect in the absence of striate cortex</article-title><source>NeuroReport</source><volume>10</volume><fpage>3759</fpage><lpage>3763</lpage><pub-id pub-id-type="doi">10.1097/00001756-199912160-00007</pub-id><pub-id pub-id-type="pmid">10716205</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Gelder</surname> <given-names>B</given-names></name><name><surname>Morris</surname> <given-names>JS</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Unconscious fear influences emotional awareness of faces and voices</article-title><source>PNAS</source><volume>102</volume><fpage>18682</fpage><lpage>18687</lpage><pub-id pub-id-type="doi">10.1073/pnas.0509179102</pub-id><pub-id pub-id-type="pmid">16352717</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derntl</surname> <given-names>B</given-names></name><name><surname>Seidel</surname> <given-names>EM</given-names></name><name><surname>Kainz</surname> <given-names>E</given-names></name><name><surname>Carbon</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Recognition of emotional expressions is affected by inversion and presentation time</article-title><source>Perception</source><volume>38</volume><fpage>1849</fpage><lpage>1862</lpage><pub-id pub-id-type="doi">10.1068/p6448</pub-id><pub-id pub-id-type="pmid">20192133</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diano</surname> <given-names>M</given-names></name><name><surname>Celeghin</surname> <given-names>A</given-names></name><name><surname>Bagnis</surname> <given-names>A</given-names></name><name><surname>Tamietto</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Amygdala response to emotional stimuli without awareness: facts and interpretations</article-title><source>Frontiers in Psychology</source><volume>7</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2016.02029</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emery</surname> <given-names>NJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The eyes have it: the neuroethology, function and evolution of social gaze</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>24</volume><fpage>581</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.1016/S0149-7634(00)00025-7</pub-id><pub-id pub-id-type="pmid">10940436</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteves</surname> <given-names>F</given-names></name><name><surname>Parra</surname> <given-names>C</given-names></name><name><surname>Dimberg</surname> <given-names>U</given-names></name><name><surname>Ohman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Nonconscious associative learning: pavlovian conditioning of skin conductance responses to masked fear-relevant facial stimuli</article-title><source>Psychophysiology</source><volume>31</volume><fpage>375</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1994.tb02446.x</pub-id><pub-id pub-id-type="pmid">10690918</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname> <given-names>F</given-names></name><name><surname>He</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Cortical responses to invisible objects in the human dorsal and ventral pathways</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1380</fpage><lpage>1385</lpage><pub-id pub-id-type="doi">10.1038/nn1537</pub-id><pub-id pub-id-type="pmid">16136038</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferneyhough</surname> <given-names>E</given-names></name><name><surname>Kim</surname> <given-names>MK</given-names></name><name><surname>Phelps</surname> <given-names>EA</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Anxiety modulates the effects of emotion and attention on early vision</article-title><source>Cognition &amp; Emotion</source><volume>27</volume><fpage>166</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1080/02699931.2012.689953</pub-id><pub-id pub-id-type="pmid">22784014</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Selective processing of threatening words in anxiety: the role of awareness</article-title><source>Cognition &amp; Emotion</source><volume>10</volume><fpage>449</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1080/026999396380114</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname> <given-names>E</given-names></name><name><surname>Lester</surname> <given-names>V</given-names></name><name><surname>Russo</surname> <given-names>R</given-names></name><name><surname>Bowles</surname> <given-names>RJ</given-names></name><name><surname>Pichler</surname> <given-names>A</given-names></name><name><surname>Dutton</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Facial expressions of emotion: are angry faces detected more efficiently?</article-title><source>Cognition &amp; Emotion</source><volume>14</volume><fpage>61</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1080/026999300378996</pub-id><pub-id pub-id-type="pmid">17401453</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname> <given-names>DM</given-names></name><name><surname>Tadin</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Modularity in the motion system: independent oculomotor and perceptual processing of brief moving stimuli</article-title><source>Journal of Vision</source><volume>14</volume><fpage>28</fpage><pub-id pub-id-type="doi">10.1167/14.3.28</pub-id><pub-id pub-id-type="pmid">24665091</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grèzes</surname> <given-names>J</given-names></name><name><surname>Valabrègue</surname> <given-names>R</given-names></name><name><surname>Gholipour</surname> <given-names>B</given-names></name><name><surname>Chevallier</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A direct amygdala-motor pathway for emotional displays to influence action: a diffusion tensor imaging study</article-title><source>Human Brain Mapping</source><volume>35</volume><fpage>5974</fpage><lpage>5983</lpage><pub-id pub-id-type="doi">10.1002/hbm.22598</pub-id><pub-id pub-id-type="pmid">25053375</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hedger</surname> <given-names>N</given-names></name><name><surname>Adams</surname> <given-names>WJ</given-names></name><name><surname>Garner</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fearful faces have a sensory advantage in the competition for awareness</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>41</volume><fpage>1748</fpage><lpage>1757</lpage><pub-id pub-id-type="doi">10.1037/xhp0000127</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hedger</surname> <given-names>N</given-names></name><name><surname>Gray</surname> <given-names>KL</given-names></name><name><surname>Garner</surname> <given-names>M</given-names></name><name><surname>Adams</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Are visual threats prioritized without awareness? A critical review and meta-analysis involving 3 behavioral paradigms and 2696 observers</article-title><source>Psychological Bulletin</source><volume>142</volume><fpage>934</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1037/bul0000054</pub-id><pub-id pub-id-type="pmid">27123863</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hesselmann</surname> <given-names>G</given-names></name><name><surname>Darcy</surname> <given-names>N</given-names></name><name><surname>Ludwig</surname> <given-names>K</given-names></name><name><surname>Sterzer</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Priming in a shape task but not in a category task under continuous flash suppression</article-title><source>Journal of Vision</source><volume>16</volume><fpage>17</fpage><pub-id pub-id-type="doi">10.1167/16.3.17</pub-id><pub-id pub-id-type="pmid">26885629</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huis In 't Veld</surname> <given-names>EM</given-names></name><name><surname>van Boxtel</surname> <given-names>GJ</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The body action coding system II: muscle activations during the perception and expression of emotion</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>8</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.3389/fnbeh.2014.00330</pub-id><pub-id pub-id-type="pmid">25294993</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunnius</surname> <given-names>S</given-names></name><name><surname>de Wit</surname> <given-names>TC</given-names></name><name><surname>Vrins</surname> <given-names>S</given-names></name><name><surname>von Hofsten</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Facing threat: infants' and adults' visual scanning of faces with neutral, happy, sad, angry, and fearful emotional expressions</article-title><source>Cognition &amp; Emotion</source><volume>25</volume><fpage>193</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1080/15298861003771189</pub-id><pub-id pub-id-type="pmid">21432667</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>Y</given-names></name><name><surname>He</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical responses to invisible faces: dissociating subsystems for facial-information processing</article-title><source>Current Biology</source><volume>16</volume><fpage>2023</fpage><lpage>2029</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.08.084</pub-id><pub-id pub-id-type="pmid">17055981</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jusyte</surname> <given-names>A</given-names></name><name><surname>Mayer</surname> <given-names>SV</given-names></name><name><surname>Künzel</surname> <given-names>E</given-names></name><name><surname>Hautzinger</surname> <given-names>M</given-names></name><name><surname>Schönenberg</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Unemotional traits predict early processing deficit for fearful expressions in young violent offenders: an investigation using continuous flash suppression</article-title><source>Psychological Medicine</source><volume>45</volume><fpage>285</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1017/S0033291714001287</pub-id><pub-id pub-id-type="pmid">25066013</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaunitz</surname> <given-names>LN</given-names></name><name><surname>Fracasso</surname> <given-names>A</given-names></name><name><surname>Skujevskis</surname> <given-names>M</given-names></name><name><surname>Melcher</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Waves of visibility: probing the depth of inter-ocular suppression with transient and sustained targets</article-title><source>Frontiers in Psychology</source><volume>5</volume><fpage>804</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00804</pub-id><pub-id pub-id-type="pmid">25126081</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kendon</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>Some functions of gaze-direction in social interaction</article-title><source>Acta Psychologica</source><volume>26</volume><fpage>22</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/0001-6918(67)90005-4</pub-id><pub-id pub-id-type="pmid">6043092</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kissler</surname> <given-names>J</given-names></name><name><surname>Keil</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Look-don't look! how emotional pictures affect pro- and anti-saccades</article-title><source>Experimental Brain Research</source><volume>188</volume><fpage>215</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1007/s00221-008-1358-0</pub-id><pub-id pub-id-type="pmid">18368396</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kret</surname> <given-names>ME</given-names></name><name><surname>Pichon</surname> <given-names>S</given-names></name><name><surname>Grèzes</surname> <given-names>J</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Similarities and differences in perceiving threat from dynamic faces and bodies. an fMRI study</article-title><source>NeuroImage</source><volume>54</volume><fpage>1755</fpage><lpage>1762</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.08.012</pub-id><pub-id pub-id-type="pmid">20723605</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kret</surname> <given-names>ME</given-names></name><name><surname>Stekelenburg</surname> <given-names>JJ</given-names></name><name><surname>Roelofs</surname> <given-names>K</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Perception of face and body expressions using electromyography, pupillometry and gaze measures</article-title><source>Frontiers in Psychology</source><volume>4</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00028</pub-id><pub-id pub-id-type="pmid">23403886</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kret</surname> <given-names>ME</given-names></name><name><surname>Roelofs</surname> <given-names>K</given-names></name><name><surname>Stekelenburg</surname> <given-names>JJ</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Emotional signals from faces, bodies and scenes influence observers' face expressions, fixations and pupil-size</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.3389/fnhum.2013.00810</pub-id><pub-id pub-id-type="pmid">24391567</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhn</surname> <given-names>G</given-names></name><name><surname>Land</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>There's more to magic than meets the eye</article-title><source>Current Biology</source><volume>16</volume><fpage>R950</fpage><lpage>R951</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.10.012</pub-id><pub-id pub-id-type="pmid">17113372</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lapate</surname> <given-names>RC</given-names></name><name><surname>Rokers</surname> <given-names>B</given-names></name><name><surname>Tromp</surname> <given-names>DP</given-names></name><name><surname>Orfali</surname> <given-names>NS</given-names></name><name><surname>Oler</surname> <given-names>JA</given-names></name><name><surname>Doran</surname> <given-names>ST</given-names></name><name><surname>Adluru</surname> <given-names>N</given-names></name><name><surname>Alexander</surname> <given-names>AL</given-names></name><name><surname>Davidson</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Awareness of emotional stimuli determines the behavioral consequences of amygdala activation and Amygdala-Prefrontal connectivity</article-title><source>Scientific Reports</source><volume>6</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1038/srep25826</pub-id><pub-id pub-id-type="pmid">27181344</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>LeDoux</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><chapter-title>The emotional brain</chapter-title><source>The Mysterious Underpinnings of Emotional Life</source><publisher-name>Simon and Schuster</publisher-name></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liddell</surname> <given-names>BJ</given-names></name><name><surname>Williams</surname> <given-names>LM</given-names></name><name><surname>Rathjen</surname> <given-names>J</given-names></name><name><surname>Shevrin</surname> <given-names>H</given-names></name><name><surname>Gordon</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A temporal dissociation of subliminal versus supraliminal fear perception: an event-related potential study</article-title><source>Journal of Cognitive Neuroscience</source><volume>16</volume><fpage>479</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1162/089892904322926809</pub-id><pub-id pub-id-type="pmid">15072682</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lundqvist</surname> <given-names>D</given-names></name><name><surname>Flykt</surname> <given-names>A</given-names></name><name><surname>Öhman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>The Karolinska Directed Emotional Faces – KDEF, CD ROM from Department of Clinical Neuroscience, Psychology Section, Karolinska Institutet</source><publisher-name>Karolinska Institutet</publisher-name><pub-id pub-id-type="isbn">91-630-7164-9</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundqvist</surname> <given-names>D</given-names></name><name><surname>Ohman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Emotion regulates attention: the relation between facial configurations, facial emotion, and visual attention</article-title><source>Visual Cognition</source><volume>12</volume><fpage>51</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1080/13506280444000085</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>Q</given-names></name><name><surname>Holroyd</surname> <given-names>T</given-names></name><name><surname>Jones</surname> <given-names>M</given-names></name><name><surname>Hendler</surname> <given-names>T</given-names></name><name><surname>Blair</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural dynamics for facial threat processing as revealed by gamma band synchronization using MEG</article-title><source>NeuroImage</source><volume>34</volume><fpage>839</fpage><lpage>847</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.09.023</pub-id><pub-id pub-id-type="pmid">17095252</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madipakkam</surname> <given-names>AR</given-names></name><name><surname>Rothkirch</surname> <given-names>M</given-names></name><name><surname>Wilbertz</surname> <given-names>G</given-names></name><name><surname>Sterzer</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Probing the influence of unconscious fear-conditioned visual stimuli on eye movements</article-title><source>Consciousness and Cognition</source><volume>46</volume><fpage>60</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2016.09.016</pub-id><pub-id pub-id-type="pmid">27684607</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marsh</surname> <given-names>AA</given-names></name><name><surname>Ambady</surname> <given-names>N</given-names></name><name><surname>Kleck</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The effects of fear and anger facial expressions on approach- and avoidance-related behaviors</article-title><source>Emotion</source><volume>5</volume><fpage>119</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1037/1528-3542.5.1.119</pub-id><pub-id pub-id-type="pmid">15755225</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKelvie</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Emotional expression in upside-down faces: evidence for configurational and componential processing</article-title><source>British Journal of Social Psychology</source><volume>34</volume><fpage>325</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8309.1995.tb01067.x</pub-id><pub-id pub-id-type="pmid">7551775</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milders</surname> <given-names>M</given-names></name><name><surname>Sahraie</surname> <given-names>A</given-names></name><name><surname>Logan</surname> <given-names>S</given-names></name><name><surname>Donnellon</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Awareness of faces is modulated by their emotional meaning</article-title><source>Emotion</source><volume>6</volume><fpage>10</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1037/1528-3542.6.1.10</pub-id><pub-id pub-id-type="pmid">16637746</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miles</surname> <given-names>WR</given-names></name></person-group><year iso-8601-date="1930">1930</year><article-title>Ocular dominance in human adults</article-title><source>The Journal of General Psychology</source><volume>3</volume><fpage>412</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1080/00221309.1930.9918218</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mogg</surname> <given-names>K</given-names></name><name><surname>Bradley</surname> <given-names>BP</given-names></name><name><surname>Hallowell</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Attentional bias to threat: roles of trait anxiety, stressful events, and awareness</article-title><source>The Quarterly Journal of Experimental Psychology Section A</source><volume>47</volume><fpage>841</fpage><lpage>864</lpage><pub-id pub-id-type="doi">10.1080/14640749408401099</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mogg</surname> <given-names>K</given-names></name><name><surname>Garner</surname> <given-names>M</given-names></name><name><surname>Bradley</surname> <given-names>BP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Anxiety and orienting of gaze to angry and fearful faces</article-title><source>Biological Psychology</source><volume>76</volume><fpage>163</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2007.07.005</pub-id><pub-id pub-id-type="pmid">17764810</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narme</surname> <given-names>P</given-names></name><name><surname>Bonnet</surname> <given-names>AM</given-names></name><name><surname>Dubois</surname> <given-names>B</given-names></name><name><surname>Chaby</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Understanding facial emotion perception in Parkinson's disease: the role of configural processing</article-title><source>Neuropsychologia</source><volume>49</volume><fpage>3295</fpage><lpage>3302</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2011.08.002</pub-id><pub-id pub-id-type="pmid">21856319</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nummenmaa</surname> <given-names>L</given-names></name><name><surname>Hyönä</surname> <given-names>J</given-names></name><name><surname>Calvo</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Emotional scene content drives the saccade generation system reflexively</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>35</volume><fpage>305</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1037/a0013626</pub-id><pub-id pub-id-type="pmid">19331490</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohman</surname> <given-names>A</given-names></name><name><surname>Lundqvist</surname> <given-names>D</given-names></name><name><surname>Esteves</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The face in the crowd revisited: a threat advantage with schematic stimuli</article-title><source>Journal of Personality and Social Psychology</source><volume>80</volume><fpage>381</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.80.3.381</pub-id><pub-id pub-id-type="pmid">11300573</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pegna</surname> <given-names>AJ</given-names></name><name><surname>Khateb</surname> <given-names>A</given-names></name><name><surname>Lazeyras</surname> <given-names>F</given-names></name><name><surname>Seghier</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Discriminating emotional faces without primary visual cortices involves the right amygdala</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>24</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1038/nn1364</pub-id><pub-id pub-id-type="pmid">15592466</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessoa</surname> <given-names>L</given-names></name><name><surname>Adolphs</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Emotion processing and the amygdala: from a 'low road' to 'many roads' of evaluating biological significance</article-title><source>Nature Reviews Neuroscience</source><volume>11</volume><fpage>773</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1038/nrn2920</pub-id><pub-id pub-id-type="pmid">20959860</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phelps</surname> <given-names>EA</given-names></name><name><surname>Ling</surname> <given-names>S</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Emotion facilitates perception and potentiates the perceptual benefits of attention</article-title><source>Psychological Science</source><volume>17</volume><fpage>292</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2006.01701.x</pub-id><pub-id pub-id-type="pmid">16623685</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichon</surname> <given-names>S</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name><name><surname>Grèzes</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Two different faces of threat. comparing the neural systems for recognizing fear and anger in dynamic body expressions</article-title><source>NeuroImage</source><volume>47</volume><fpage>1873</fpage><lpage>1883</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.03.084</pub-id><pub-id pub-id-type="pmid">19371787</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pourtois</surname> <given-names>G</given-names></name><name><surname>Schettino</surname> <given-names>A</given-names></name><name><surname>Vuilleumier</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain mechanisms for emotional influences on perception and attention: what is magic and what is not</article-title><source>Biological Psychology</source><volume>92</volume><fpage>492</fpage><lpage>512</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2012.02.007</pub-id><pub-id pub-id-type="pmid">22373657</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelofs</surname> <given-names>K</given-names></name><name><surname>Hagenaars</surname> <given-names>MA</given-names></name><name><surname>Stins</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Facing freeze: social threat induces bodily freeze in humans</article-title><source>Psychological Science</source><volume>21</volume><fpage>1575</fpage><lpage>1581</lpage><pub-id pub-id-type="doi">10.1177/0956797610384746</pub-id><pub-id pub-id-type="pmid">20876881</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rothkirch</surname> <given-names>M</given-names></name><name><surname>Stein</surname> <given-names>T</given-names></name><name><surname>Sekutowicz</surname> <given-names>M</given-names></name><name><surname>Sterzer</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A direct oculomotor correlate of unconscious visual processing</article-title><source>Current Biology</source><volume>22</volume><fpage>R514</fpage><lpage>R515</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.04.046</pub-id><pub-id pub-id-type="pmid">22789995</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sandberg</surname> <given-names>K</given-names></name><name><surname>Timmermans</surname> <given-names>B</given-names></name><name><surname>Overgaard</surname> <given-names>M</given-names></name><name><surname>Cleeremans</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Measuring consciousness: is one measure better than the other?</article-title><source>Consciousness and Cognition</source><volume>19</volume><fpage>1069</fpage><lpage>1078</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2009.12.013</pub-id><pub-id pub-id-type="pmid">20133167</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname> <given-names>LJ</given-names></name><name><surname>Belopolsky</surname> <given-names>AV</given-names></name><name><surname>Theeuwes</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The presence of threat affects saccade trajectories</article-title><source>Visual Cognition</source><volume>20</volume><fpage>284</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1080/13506285.2012.658885</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncini</surname> <given-names>C</given-names></name><name><surname>Perrinet</surname> <given-names>LU</given-names></name><name><surname>Montagnini</surname> <given-names>A</given-names></name><name><surname>Mamassian</surname> <given-names>P</given-names></name><name><surname>Masson</surname> <given-names>GS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>More is not always better: adaptive gain control explains dissociation between perception and action</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1596</fpage><lpage>1603</lpage><pub-id pub-id-type="doi">10.1038/nn.3229</pub-id><pub-id pub-id-type="pmid">23023292</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spering</surname> <given-names>M</given-names></name><name><surname>Pomplun</surname> <given-names>M</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Tracking without perceiving: a dissociation between eye movements and motion perception</article-title><source>Psychological Science</source><volume>22</volume><fpage>216</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1177/0956797610394659</pub-id><pub-id pub-id-type="pmid">21189353</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spering</surname> <given-names>M</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Similar effects of feature-based attention on motion perception and pursuit eye movements at different levels of awareness</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>7594</fpage><lpage>7601</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0355-12.2012</pub-id><pub-id pub-id-type="pmid">22649238</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spering</surname> <given-names>M</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Acting without seeing: eye movements reveal visual processing without awareness</article-title><source>Trends in Neurosciences</source><volume>38</volume><fpage>247</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2015.02.002</pub-id><pub-id pub-id-type="pmid">25765322</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Springer</surname> <given-names>US</given-names></name><name><surname>Rosas</surname> <given-names>A</given-names></name><name><surname>McGetrick</surname> <given-names>J</given-names></name><name><surname>Bowers</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Differences in startle reactivity during the perception of angry and fearful faces</article-title><source>Emotion</source><volume>7</volume><fpage>516</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1037/1528-3542.7.3.516</pub-id><pub-id pub-id-type="pmid">17683208</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sterzer</surname> <given-names>P</given-names></name><name><surname>Stein</surname> <given-names>T</given-names></name><name><surname>Ludwig</surname> <given-names>K</given-names></name><name><surname>Rothkirch</surname> <given-names>M</given-names></name><name><surname>Hesselmann</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural processing of visual information under interocular suppression: a critical review</article-title><source>Frontiers in Psychology</source><volume>5</volume><fpage>453</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00453</pub-id><pub-id pub-id-type="pmid">24904469</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Striemer</surname> <given-names>CL</given-names></name><name><surname>Whitwell</surname> <given-names>RL</given-names></name><name><surname>Goodale</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Affective blindsight in the absence of input from face processing regions in occipital-temporal cortex</chapter-title><source>Neuropsychologia </source><publisher-name>Elsevier</publisher-name><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.11.014</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamietto</surname> <given-names>M</given-names></name><name><surname>Castelli</surname> <given-names>L</given-names></name><name><surname>Vighetti</surname> <given-names>S</given-names></name><name><surname>Perozzo</surname> <given-names>P</given-names></name><name><surname>Geminiani</surname> <given-names>G</given-names></name><name><surname>Weiskrantz</surname> <given-names>L</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Unseen facial and bodily expressions trigger fast emotional reactions</article-title><source>PNAS</source><volume>106</volume><fpage>17661</fpage><lpage>17666</lpage><pub-id pub-id-type="doi">10.1073/pnas.0908994106</pub-id><pub-id pub-id-type="pmid">19805044</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamietto</surname> <given-names>M</given-names></name><name><surname>Cauda</surname> <given-names>F</given-names></name><name><surname>Celeghin</surname> <given-names>A</given-names></name><name><surname>Diano</surname> <given-names>M</given-names></name><name><surname>Costa</surname> <given-names>T</given-names></name><name><surname>Cossa</surname> <given-names>FM</given-names></name><name><surname>Sacco</surname> <given-names>K</given-names></name><name><surname>Duca</surname> <given-names>S</given-names></name><name><surname>Geminiani</surname> <given-names>GC</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Once you feel it, you see it: insula and sensory-motor contribution to visual awareness for fearful bodies in parietal neglect</article-title><source>Cortex</source><volume>62</volume><fpage>56</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2014.10.009</pub-id><pub-id pub-id-type="pmid">25465122</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamietto</surname> <given-names>M</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural bases of the non-conscious perception of emotional signals</article-title><source>Nature Reviews Neuroscience</source><volume>11</volume><fpage>697</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1038/nrn2889</pub-id><pub-id pub-id-type="pmid">20811475</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavassoli</surname> <given-names>A</given-names></name><name><surname>Ringach</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>When your eyes see more than you do</article-title><source>Current Biology</source><volume>20</volume><fpage>R93</fpage><lpage>R94</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.11.048</pub-id><pub-id pub-id-type="pmid">20144775</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Terburg</surname> <given-names>D</given-names></name><name><surname>Hooiveld</surname> <given-names>N</given-names></name><name><surname>Aarts</surname> <given-names>H</given-names></name><name><surname>Kenemans</surname> <given-names>JL</given-names></name><name><surname>van Honk</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Eye tracking unconscious face-to-face confrontations: dominance motives prolong gaze to masked angry faces</article-title><source>Psychological Science</source><volume>22</volume><fpage>314</fpage><lpage>319</lpage><pub-id pub-id-type="doi">10.1177/0956797611398492</pub-id><pub-id pub-id-type="pmid">21303993</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Terburg</surname> <given-names>D</given-names></name><name><surname>Aarts</surname> <given-names>H</given-names></name><name><surname>van Honk</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Testosterone affects gaze aversion from angry faces outside of conscious awareness</article-title><source>Psychological Science</source><volume>23</volume><fpage>459</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1177/0956797611433336</pub-id><pub-id pub-id-type="pmid">22477106</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troiani</surname> <given-names>V</given-names></name><name><surname>Price</surname> <given-names>ET</given-names></name><name><surname>Schultz</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Unseen fearful faces promote amygdala guidance of attention</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>9</volume><fpage>133</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1093/scan/nss116</pub-id><pub-id pub-id-type="pmid">23051897</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troiani</surname> <given-names>V</given-names></name><name><surname>Schultz</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Amygdala, pulvinar, and inferior parietal cortex contribute to early processing of faces without awareness</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3389/fnhum.2013.00241</pub-id><pub-id pub-id-type="pmid">23761748</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsuchiya</surname> <given-names>N</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Gilroy</surname> <given-names>LA</given-names></name><name><surname>Blake</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Depth of interocular suppression associated with continuous flash suppression, flash suppression, and binocular rivalry</article-title><source>Journal of Vision</source><volume>6</volume><fpage>6</fpage><lpage>1078</lpage><pub-id pub-id-type="doi">10.1167/6.10.6</pub-id><pub-id pub-id-type="pmid">17132078</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsuchiya</surname> <given-names>N</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Continuous flash suppression reduces negative afterimages</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1096</fpage><lpage>1101</lpage><pub-id pub-id-type="doi">10.1038/nn1500</pub-id><pub-id pub-id-type="pmid">15995700</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valk</surname> <given-names>JMd</given-names></name><name><surname>Wijnen</surname> <given-names>JG</given-names></name><name><surname>Kret</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Anger fosters action. fast responses in a motor task involving approach movements toward angry faces and bodies</article-title><source>Frontiers in Psychology</source><volume>6</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2015.01240</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Vetter</surname> <given-names>P</given-names></name><name><surname>Badde</surname> <given-names>S</given-names></name><name><surname>Phelps</surname> <given-names>EA</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>EyeMovementsSuppressedEmotionalFaces_Repository</data-title><version designator="0201f6e">0201f6e</version><publisher-name>GitHub</publisher-name><ext-link ext-link-type="uri" xlink:href="https://github.com/StephBadde/EyeMovementsSuppressedEmotionalFaces">https://github.com/StephBadde/EyeMovementsSuppressedEmotionalFaces</ext-link></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vuilleumier</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>How brains beware: neural mechanisms of emotional attention</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>585</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.10.011</pub-id><pub-id pub-id-type="pmid">16289871</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whalen</surname> <given-names>PJ</given-names></name><name><surname>Raila</surname> <given-names>H</given-names></name><name><surname>Bennett</surname> <given-names>R</given-names></name><name><surname>Mattek</surname> <given-names>A</given-names></name><name><surname>Brown</surname> <given-names>A</given-names></name><name><surname>Taylor</surname> <given-names>J</given-names></name><name><surname>van Tieghem</surname> <given-names>M</given-names></name><name><surname>Tanner</surname> <given-names>A</given-names></name><name><surname>Miner</surname> <given-names>M</given-names></name><name><surname>Palmer</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neuroscience and facial expressions of emotion: the role of Amygdala–Prefrontal Interactions</article-title><source>Emotion Review</source><volume>5</volume><fpage>78</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1177/1754073912457231</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willenbockel</surname> <given-names>V</given-names></name><name><surname>Sadr</surname> <given-names>J</given-names></name><name><surname>Fiset</surname> <given-names>D</given-names></name><name><surname>Horne</surname> <given-names>GO</given-names></name><name><surname>Gosselin</surname> <given-names>F</given-names></name><name><surname>Tanaka</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Controlling low-level image properties: the SHINE toolbox</article-title><source>Behavior Research Methods</source><volume>42</volume><fpage>671</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.3758/BRM.42.3.671</pub-id><pub-id pub-id-type="pmid">20805589</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname> <given-names>LM</given-names></name><name><surname>Das</surname> <given-names>P</given-names></name><name><surname>Liddell</surname> <given-names>B</given-names></name><name><surname>Olivieri</surname> <given-names>G</given-names></name><name><surname>Peduto</surname> <given-names>A</given-names></name><name><surname>Brammer</surname> <given-names>MJ</given-names></name><name><surname>Gordon</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>BOLD, sweat and fears: fmri and skin conductance distinguish facial fear signals</article-title><source>NeuroReport</source><volume>16</volume><fpage>49</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1097/00001756-200501190-00012</pub-id><pub-id pub-id-type="pmid">15618889</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname> <given-names>LM</given-names></name><name><surname>Das</surname> <given-names>P</given-names></name><name><surname>Liddell</surname> <given-names>BJ</given-names></name><name><surname>Kemp</surname> <given-names>AH</given-names></name><name><surname>Rennie</surname> <given-names>CJ</given-names></name><name><surname>Gordon</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Mode of functional connectivity in amygdala pathways dissociates level of awareness for signals of fear</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>9264</fpage><lpage>9271</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1016-06.2006</pub-id><pub-id pub-id-type="pmid">16957082</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>E</given-names></name><name><surname>Zald</surname> <given-names>DH</given-names></name><name><surname>Blake</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Fearful expressions gain preferential access to awareness during continuous flash suppression</article-title><source>Emotion</source><volume>7</volume><fpage>882</fpage><lpage>886</lpage><pub-id pub-id-type="doi">10.1037/1528-3542.7.4.882</pub-id><pub-id pub-id-type="pmid">18039058</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhan</surname> <given-names>M</given-names></name><name><surname>Hortensius</surname> <given-names>R</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The body as a tool for anger awareness--differential effects of angry facial and bodily expressions on suppression from awareness</article-title><source>Plos One</source><volume>10</volume><elocation-id>e0139768</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0139768</pub-id><pub-id pub-id-type="pmid">26469878</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhan</surname> <given-names>M</given-names></name><name><surname>de Gelder</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Unconscious fearful body perception enhances discrimination of conscious anger expressions under continuous flash suppression</article-title><source>Neuropsychologia</source><volume>23</volume><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.04.019</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.43467.009</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Goodale</surname><given-names>Melvyn</given-names></name><role>Reviewing Editor</role><aff><institution>Western University</institution><country>Canada</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Goodale</surname><given-names>Melvyn</given-names> </name><role>Reviewer</role><aff><institution>Western University</institution><country>Canada</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>de Gelder</surname><given-names>Beatrice</given-names> </name><role>Reviewer</role><aff><institution>Maastricht University</institution><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;The eyes react to emotional faces without perceiving them&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Melvyn Goodale as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Richard Ivry as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Beatrice de Gelder (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This is an interesting paper. Although there is certainly a lot of evidence for unconscious processing of emotional expression, this is the first study is the first to show differences in the direction of eye movements as a function of the nature of masked emotional expressions. Nonconscious emotion perception first in de Gelder 1999 showing that individuals with lesions of primary visual cortex can detect facial expressions in their blind field – and most recently by Striemer et al., 2017, showing that a patient with lesions of V1 – plus the face processing regions in occipito-temporal cortex – can nevertheless discriminate a happy vs. fearful facial expression and a happy vs. angry expression (albeit unconsciously in a 2AFC task) [1]. But the authors have taken this one rather large step forward and shown that the nature of the response (look to or away from) the masked facial expression can take place in quite unconsciously in neurologically intact individuals. This is not mere detection or discrimination – but instead is a specific directional action to an unseen facial expression. The result is similar to that reported by Bannerman et al., 2009, for visible fearful expressions, but importantly the emotional expressions in the current study are masked from awareness by CFS.</p><p>Essential revisions:</p><p>1) The authors report: &quot;When observers are presented with unmasked angry and fearful faces, both attract their attention and eye gaze equally strongly, suggesting that different factors drive eye movements to angry faces under aware and unaware conditions.&quot; This seems puzzling and deserves some explanation. If the eye movements towards and away for fearful and angry faces are thought to be adaptive, as the authors suggest, then it would seem likely that the such behaviour would also occur when participants actually perceive the faces. Is it possible that if participants are aware of the emotional expression on a face presented as a photograph, they could suppress the 'automatic' tendency to look towards or away from the face? In any case, the authors need to discuss (briefly) why this should be the case.</p><p>2) The theoretical contribution and the novelty could be better highlighted. Some directly relevant studies that have used eye movement based measures (Bannerman et al., 2009) or CFS (Zhan, 2015, 2018a, b, c) need to be discussed in brief. Two other points; first, different responses to fearful and angry faces have been reported in the literature and therefore the authors need to signal both in the title and in the Introduction, how the actions they measured differ from others. Second, the explanation for the differences in the direction of the eye movements they observed is highly speculative. A bit more explication for the observed directional differences in warranted.</p><p>3) The authors should also look at the methodological paper by Zhan et al., 2018, getting into details of the psychophysical parameters that yield or do not yield reliable CFS. What is the rationale of using a mask frequency of 28.3 Hz?</p><p>4) In Results section paragraph three, it says that the authors &quot;…only analyzed trials in which subjective visibility was zero, and only data from participants whose overall performance at zero visibility was at chance level…&quot;. Could the authors please explain how this overall performance was calculated? Was this done per trial or for a group of trials? How were the responses to the two objective questions (location and expression) combined to determine chance level?</p><p>5) Figure 2: why do participants seem to be doing so poorly in the conditions of 'almost clear' and 'very clear' visibility. It seems that, even in these conditions of subjective clear visibility, participants still cannot identify the position of the faces (for upright faces), and are quite poor at identifying angry or afraid expressions (again for upright faces). Does this mean that the subjective ratings are a very poor measure of actual visibility? If so, is this a valid measure to use at all? Also, could the authors please add a scale so that the reader can interpret the size of the circles? Finally consider showing performance for all non-suppressed trials in this figure.</p><p>6) For the time series analysis (subsection “Data Analysis”), paired t-tests seem to have been computed at each time point, across all participants – is that correct? Also, what were the criteria to identify clusters of significant t-values? The authors say that &quot;clusters of adjacent time points with significant, equally directed differences between emotions were identified&quot; – did they have to be all adjacent, without a single interruption? What was the minimum number of time points? Finally, what does the p-value correspond to?</p><p>7) For the analysis of dwell times, the authors need again to explain the criteria used to establish a cluster. For this analysis, the authors explain that the p-value corresponds to the percentile of the cluster's summed t-value in the distribution of all summed t-values (for largest clusters) obtained after permutations. I believe that this means that we can make a judgment about the size of the obtained cluster (how unlikely is to find a summed t-value of that size) but not about its spatial distribution. Wouldn't be more informative to test the probability of each segment being included in a cluster?</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.43467.010</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The authors report: &quot;When observers are presented with unmasked angry and fearful faces, both attract their attention and eye gaze equally strongly, suggesting that different factors drive eye movements to angry faces under aware and unaware conditions.&quot; This seems puzzling and deserves some explanation. If the eye movements towards and away for fearful and angry faces are thought to be adaptive, as the authors suggest, then it would seem likely that the such behaviour would also occur when participants actually perceive the faces. Is it possible that if participants are aware of the emotional expression on a face presented as a photograph, they could suppress the 'automatic' tendency to look towards or away from the face?. In any case, the authors need to discuss (briefly) why this should be the case.</p></disp-quote><p>We have clarified the information that had led to the quoted sentence, and we have revised this sentence. We specify that awareness modulates perceptual, attentional, and physiological effects of emotional information, as well as the neuronal activity associated with processing of emotional information. We can only speculate on the role of awareness for the guidance of actions by emotional information, as we are the first to demonstrate such effects in the absence of awareness.</p><disp-quote content-type="editor-comment"><p>2) The theoretical contribution and the novelty could be better highlighted. Some directly relevant studies that have used eye movement based measures (Bannerman et al., 2009) or CFS (Zhan 2015, 2018a, b,c) need to be discussed in brief. Two other points; first, different responses to fearful and angry faces have been reported in the literature and therefore the authors need to signal both in the title and in the Introduction, how the actions they measured differ from others. Second, the explanation for the differences in the direction of the eye movements they observed is highly speculative. A bit more explication for the observed directional differences in warranted.</p></disp-quote><p>We have highlighted the theoretical contribution and the novelty of our study and extended our discussion of studies investigating processing of emotional faces using eye movements or CFS. We extended our discussion of differential effects of angry and fearful faces and relate back to these differences when discussing our finding of differential eye movements to suppressed angry and fearful faces.</p><disp-quote content-type="editor-comment"><p>3) The authors should also look at the methodological paper by Zhan et al., 2018, getting into details of the psychophysical parameters that yield or do not yield reliable CFS. What is the rationale of using a mask frequency of 28.3 Hz?</p></disp-quote><p>We based the frequency mask on the first study testing a wide range of mask frequencies, which found CFS to be most effective at the mask frequency we used (Kaunitz et al., 2014). This is consistent with our observation that CFS was effective in suppressing the face stimuli from awareness. We note that the Zhan and colleagues (2018) paper had not been published when we conducted our experiment. They report that CFS is most effective with lower mask frequencies, especially 4 Hz and its harmonics (differences of 2-3 seen trials on average with 360 trials presented in total). In future studies, we will take this more recent finding into account.</p><disp-quote content-type="editor-comment"><p>4) In Results section paragraph three, it says that the authors &quot;…only analyzed trials in which subjective visibility was zero, and only data from participants whose overall performance at zero visibility was at chance level…&quot;. Could the authors please explain how this overall performance was calculated? Was this done per trial or for a group of trials? How were the responses to the two objective questions (location and expression) combined to determine chance level?</p></disp-quote><p>To determine whether a participant performed above chance level in trials with subjective zero visibility, we calculated the proportion correct across a group of trials and compared it to the proportion correct corresponding to chance responses. There is no possibility to evaluate chance level performance on a single trial level. We analyzed responses to the two questions (location and emotional expression) separately, because participants may have based their subjective visibility ratings on either feature. We calculated the proportion correct for each task across all trials rated ‘not seen’, independent of face position, expression, and orientation. For the position task, chance level was at 25%. The two excluded participants responded correctly in 78% resp. 67% of trials. In the emotion task, several participants exhibited a strong response bias towards ‘neutral’. However, all participants’ percent correct in this task was close to the predicted 33%. We clarified in the main text and added information in the Materials and methods section.</p><disp-quote content-type="editor-comment"><p>5) Figure 2: why do participants seem to be doing so poorly in the conditions of 'almost clear' and 'very clear' visibility. It seems that, even in these conditions of subjective clear visibility, participants still cannot identify the position of the faces (for upright faces), and are quite poor at identifying angry or afraid expressions (again for upright faces). Does this mean that the subjective ratings are a very poor measure of actual visibility? If so, is this a valid measure to use at all? Also, could the authors please add a scale so that the reader can interpret the size of the circles? Finally consider showing performance for all non-suppressed trials in this figure.</p></disp-quote><p>Please note that as indicated by the size of the circles, subjectivity ratings other than ‘not seen’ were very rare; the average accuracies are based on very few trials only and few participants contributed to each cell. Whereas this low frequency of responses indicates that it is hard to evaluate the corresponding accuracy level for visibility ratings other than ‘not seen’, it does confirm that faces were successfully suppressed. Moreover, performance for trials in which the face was presented on top of the mask clearly indicates that participants could identify the position and the emotional expression of the faces, when these were not suppressed. As suggested, we have added results from these trials and a scale indicating the proportion of trials each mark represents to Figure 2.</p><disp-quote content-type="editor-comment"><p>6) For the time series analysis (subsection “Data Analysis”), paired t-tests seem to have been computed at each time point, across all participants – is that correct? Also, what were the criteria to identify clusters of significant t-values? The authors say that &quot;clusters of adjacent time points with significant, equally directed differences between emotions were identified&quot; – did they have to be all adjacent, without a single interruption? What was the minimum number of time points? Finally, what does the p-value correspond to?</p></disp-quote><p>Yes, for the time series analysis paired t-tests have been computed at each time point. Six t-tests for paired samples were conducted at each time point, comparing the average distance between gaze and target within participants for all pairings of the three emotional expressions, separately for upright and upside down presented faces. These t-tests were considered significant, if the absolute t-value was larger than 2 (which is slightly more conservative than using <italic>p</italic> &lt; 0.05). Time points with significant differences were grouped into temporal clusters of adjacent time points. Any interruption, i.e., time point without a significant difference between the two emotions at hand, split the cluster. The minimal number of time points that could form a cluster was 1. The p-value of a cluster corresponds to the probability that a temporal cluster of the same or larger size as the largest cluster in the original data could occur by chance. In other words, the p-value of a cluster indicates how likely it is that two emotions differed as strongly and for as many adjacent time points as in the original data, given the variability in our dependent variable. We extended our description of the analysis by including this information.</p><disp-quote content-type="editor-comment"><p>7) For the analysis of dwell times, the authors need again to explain the criteria used to establish a cluster. For this analysis, the authors explain that the p-value corresponds to the percentile of the cluster's summed t-value in the distribution of all summed t-values (for largest clusters) obtained after permutations. I believe that this means that we can make a judgment about the size of the obtained cluster (how unlikely is to find a summed t-value of that size) but not about its spatial distribution. Wouldn't be more informative to test the probability of each segment being included in a cluster?</p></disp-quote><p>Indeed, cluster permutation tests are not spatially specific, as they only evaluate the strength of the cluster. Thereby, the strength of a cluster depends on the size of the cluster and the size of the effect at all included points. Spatial information is contained in the identity of the cluster, and usually evaluated afterwards (which voxels/sensors belong to the cluster). The same holds for temporal clusters and this non-selectivity is only possible because by default cluster permutation tests evaluate only the strongest cluster (which could be problematic, if two strong clusters concurrently exist). In our data, competing clusters were not a problem. In fact, cluster sizes never exceeded one segment. We still used cluster permutation tests, as they safeguard our results against the risk of false positives associated with the large number of t-tests involved in our analyses. By comparing the strength of the difference in one segment of the original data against differences in (clusters of) segments occurring by chance across the whole stimulus field we are more strict in our test than by applying a spatially selective method. We have added this information in the Materials and methods section.</p></body></sub-article></article>