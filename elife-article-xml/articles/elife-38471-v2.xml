<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">38471</article-id><article-id pub-id-type="doi">10.7554/eLife.38471</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Unsupervised discovery of temporal sequences in high-dimensional datasets, with applications to neuroscience</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-113220"><name><surname>Mackevicius</surname><given-names>Emily L</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6593-4398</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-113824"><name><surname>Bahle</surname><given-names>Andrew H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0567-7195</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-65630"><name><surname>Williams</surname><given-names>Alex H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5853-103X</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-114069"><name><surname>Gu</surname><given-names>Shijie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6257-5756</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-10225"><name><surname>Denisenko</surname><given-names>Natalia I</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-8684"><name><surname>Goldman</surname><given-names>Mark S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8257-2314</contrib-id><email>msgoldman@ucdavis.edu</email><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-10226"><name><surname>Fee</surname><given-names>Michale S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7539-1745</contrib-id><email>fee@mit.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">McGovern Institute for Brain Research, Department of Brain and Cognitive Sciences</institution><institution>Massachusetts Institute of Technology</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Neurosciences Program</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">School of Life Sciences and Technology</institution><institution>ShanghaiTech University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Center for Neuroscience, Department of Neurobiology, Physiology and Behavior</institution><institution>University of California, Davis</institution><addr-line><named-content content-type="city">Davis</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Department of Ophthamology and Vision Science</institution><institution>University of California, Davis</institution><addr-line><named-content content-type="city">Davis</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Colgin</surname><given-names>Laura</given-names></name><role>Reviewing Editor</role><aff><institution>The University of Texas at Austin</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>05</day><month>02</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e38471</elocation-id><history><date date-type="received" iso-8601-date="2018-05-19"><day>19</day><month>05</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2019-01-04"><day>04</day><month>01</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Mackevicius et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Mackevicius et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-38471-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.38471.001</object-id><p>Identifying low-dimensional features that describe large-scale neural recordings is a major challenge in neuroscience. Repeated temporal patterns (sequences) are thought to be a salient feature of neural dynamics, but are not succinctly captured by traditional dimensionality reduction techniques. Here, we describe a software toolbox—called seqNMF—with new methods for extracting informative, non-redundant, sequences from high-dimensional neural data, testing the significance of these extracted patterns, and assessing the prevalence of sequential structure in data. We test these methods on simulated data under multiple noise conditions, and on several real neural and behavioral datas. In hippocampal data, seqNMF identifies neural sequences that match those calculated manually by reference to behavioral events. In songbird data, seqNMF discovers neural sequences in untutored birds that lack stereotyped songs. Thus, by identifying temporal structure directly from neural data, seqNMF enables dissection of complex neural circuits without relying on temporal references from stimuli or behavioral outputs.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Zebra finch</kwd><kwd>sequence</kwd><kwd>matrix factorization</kwd><kwd>unsupervised</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>Simons Collaboration for the Global Brain</award-id><principal-award-recipient><name><surname>Goldman</surname><given-names>Mark S</given-names></name><name><surname>Fee</surname><given-names>Michale S</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000055</institution-id><institution>National Institute on Deafness and Other Communication Disorders</institution></institution-wrap></funding-source><award-id>R01 DC009183</award-id><principal-award-recipient><name><surname>Fee</surname><given-names>Michale S</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001229</institution-id><institution>G Harold and Leila Y. Mathers Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Fee</surname><given-names>Michale S</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000005</institution-id><institution>U.S. Department of Defense</institution></institution-wrap></funding-source><award-id>NDSEG Fellowship program</award-id><principal-award-recipient><name><surname>Mackevicius</surname><given-names>Emily L</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004944</institution-id><institution>Department of Energy, Labor and Economic Growth</institution></institution-wrap></funding-source><award-id>Computational Science Graduate Fellowship (CSGF)</award-id><principal-award-recipient><name><surname>Williams</surname><given-names>Alex H</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000052</institution-id><institution>NIH Office of the Director</institution></institution-wrap></funding-source><award-id>Training Grant 5T32EB019940-03</award-id><principal-award-recipient><name><surname>Bahle</surname><given-names>Andrew H</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>U19 NS104648</award-id><principal-award-recipient><name><surname>Goldman</surname><given-names>Mark S</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R25 MH062204</award-id><principal-award-recipient><name><surname>Goldman</surname><given-names>Mark S</given-names></name><name><surname>Fee</surname><given-names>Michale S</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Building on simple unsupervised matrix factorization techniques, the seqNMF algorithm successfully recovers neural sequences in a wide range of simulated and real datasets.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The ability to detect and analyze temporal sequences embedded in a complex sensory stream is an essential cognitive function, and as such is a necessary capability of neuronal circuits in the brain (<xref ref-type="bibr" rid="bib12">Clegg et al., 1998</xref>; <xref ref-type="bibr" rid="bib26">Janata and Grafton, 2003</xref>; <xref ref-type="bibr" rid="bib2">Bapi et al., 2005</xref>; <xref ref-type="bibr" rid="bib24">Hawkins and Ahmad, 2016</xref>), as well as artificial intelligence systems (<xref ref-type="bibr" rid="bib13">Cui et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Sutskever et al., 2014</xref>). The detection and characterization of temporal structure in signals is also useful for the analysis of many forms of physical and biological data. In neuroscience, recent advances in technology for electrophysiological and optical measurements of neural activity have enabled the simultaneous recording of hundreds or thousands of neurons (<xref ref-type="bibr" rid="bib7">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Kim et al., 2016</xref>; <xref ref-type="bibr" rid="bib55">Scholvin et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Jun et al., 2017</xref>), in which neuronal dynamics are often structured in sparse sequences (<xref ref-type="bibr" rid="bib21">Hahnloser et al., 2002</xref>; <xref ref-type="bibr" rid="bib22">Harvey et al., 2012</xref>; <xref ref-type="bibr" rid="bib38">MacDonald et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Okubo et al., 2015</xref>; <xref ref-type="bibr" rid="bib16">Fujisawa et al., 2008</xref>; <xref ref-type="bibr" rid="bib47">Pastalkova et al., 2008</xref>). Such sequences can be identified by averaging across multiple trials, but only in cases where an animal receives a temporally precise sensory stimulus, or executes a sufficiently stereotyped behavioral task.</p><p>Neural sequences have been hypothesized to play crucial roles over a much broader range of natural settings, including during learning, sleep, or diseased states (<xref ref-type="bibr" rid="bib40">Mackevicius and Fee, 2018</xref>). In these applications, it may not be possible to use external timing references, either because behaviors are not stereotyped or are entirely absent. Thus, sequences must be extracted directly from the neuronal data using unsupervised learning methods. Commonly used methods of this type, such as principal component analysis (PCA) or clustering methods, do not efficiently extract sequences, because they typically only model synchronous patterns of activity, rather than extended spatio-temporal motifs of firing.</p><p>Existing approaches that search for repeating neural patterns require computationally intensive or statistically challenging analyses (<xref ref-type="bibr" rid="bib4">Brody, 1999</xref>; <xref ref-type="bibr" rid="bib43">Mokeichev et al., 2007</xref>; <xref ref-type="bibr" rid="bib52">Quaglio et al., 2018</xref>; <xref ref-type="bibr" rid="bib5">Brunton et al., 2016</xref>). While progress has been made in analyzing non-synchronous sequential patterns using statistical models that capture cross-correlations between pairs of neurons (<xref ref-type="bibr" rid="bib54">Russo and Durstewitz, 2017</xref>; <xref ref-type="bibr" rid="bib18">Gerstein et al., 2012</xref>; <xref ref-type="bibr" rid="bib56">Schrader et al., 2008</xref>; <xref ref-type="bibr" rid="bib62">Torre et al., 2016</xref>; <xref ref-type="bibr" rid="bib20">Grossberger et al., 2018</xref>; <xref ref-type="bibr" rid="bib65">van der Meij and Voytek, 2018</xref>), such methods may not have statistical power to scale to patterns that include many (more than a few dozen) neurons, may require long periods (≥10<sup>5</sup> timebins) of stationary data, and may have challenges in dealing with (non-sequential) background activity. For a review highlighting features and limitations of these methods see (<xref ref-type="bibr" rid="bib52">Quaglio et al., 2018</xref>).</p><p>Here, we explore a complementary approach, which uses matrix factorization to reconstruct neural dynamics using a small set of exemplar sequences. In particular, we build on convolutional non-negative matrix factorization (convNMF) (<xref ref-type="bibr" rid="bib58">Smaragdis, 2004</xref>; <xref ref-type="bibr" rid="bib59">Smaragdis, 2007</xref>) (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), which has been previously applied to identify recurring motifs in audio signals such as speech (<xref ref-type="bibr" rid="bib46">O’Grady and Pearlmutter, 2006</xref>; <xref ref-type="bibr" rid="bib59">Smaragdis, 2007</xref>; <xref ref-type="bibr" rid="bib66">Vaz et al., 2016</xref>), as well as neural signals (<xref ref-type="bibr" rid="bib50">Peter et al., 2017</xref>). ConvNMF identifies exemplar patterns (factors) in conjunction with the times and amplitudes of pattern occurrences. This strategy eliminates the need to average activity aligned to any external behavioral references.</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.002</object-id><label>Figure 1.</label><caption><title>Convolutional NMF factorization.</title><p>(<bold>A</bold>) NMF (non-negative matrix factorization) approximates a data matrix describing the activity of <inline-formula><mml:math id="inf1"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula> neurons at <inline-formula><mml:math id="inf2"><mml:mi mathsize="125%">T</mml:mi></mml:math></inline-formula> timepoints as a sum of <inline-formula><mml:math id="inf3"><mml:mi mathsize="125%">K</mml:mi></mml:math></inline-formula> rank-one matrices. Each matrix is generated as the outer product of two nonnegative vectors: <inline-formula><mml:math id="inf4"><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐰</mml:mi><mml:mi mathsize="125%">k</mml:mi></mml:msub></mml:math></inline-formula> of length <inline-formula><mml:math id="inf5"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula>, which stores a neural ensemble, and <inline-formula><mml:math id="inf6"><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐡</mml:mi><mml:mi mathsize="125%">k</mml:mi></mml:msub></mml:math></inline-formula> of length <inline-formula><mml:math id="inf7"><mml:mi mathsize="125%">T</mml:mi></mml:math></inline-formula>, which holds the times at which the neural ensemble is active, and the relative amplitudes of this activity. (<bold>B</bold>) Convolutional NMF also approximates an <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">×</mml:mo><mml:mi mathsize="125%">T</mml:mi></mml:mrow></mml:math></inline-formula> data matrix as a sum of <inline-formula><mml:math id="inf9"><mml:mi mathsize="125%">K</mml:mi></mml:math></inline-formula> matrices. Each matrix is generated as the convolution of two components: a non-negative matrix <inline-formula><mml:math id="inf10"><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐰</mml:mi><mml:mi mathsize="125%">k</mml:mi></mml:msub></mml:math></inline-formula> of dimension <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">×</mml:mo><mml:mi mathsize="125%">L</mml:mi></mml:mrow></mml:math></inline-formula> that stores a sequential pattern of the <inline-formula><mml:math id="inf12"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula> neurons at <inline-formula><mml:math id="inf13"><mml:mi mathsize="125%">L</mml:mi></mml:math></inline-formula> lags, and a vector of temporal loadings, <inline-formula><mml:math id="inf14"><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐡</mml:mi><mml:mi mathsize="125%">k</mml:mi></mml:msub></mml:math></inline-formula>, which holds the times at which each factor pattern is active in the data, and the relative amplitudes of this activity. (<bold>C</bold>) Three types of inefficiencies present in unregularized convNMF: Type 1, in which two factors are used to reconstruct the same instance of a sequence; Type 2, in which two factors reconstruct a sequence in a piece-wise manner; and Type 3, in which two factors are used to reconstruct different instances of the same sequence. For each case, the factors (<inline-formula><mml:math id="inf15"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>) are shown, as well as the reconstruction (<inline-formula><mml:math id="inf17"><mml:mrow><mml:mover accent="true"><mml:mi mathsize="125%" mathvariant="bold">𝐗</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">~</mml:mo></mml:mover><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊛</mml:mo><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐰</mml:mi><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn></mml:msub><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊛</mml:mo><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐡</mml:mi><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn></mml:msub></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">+</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐰</mml:mi><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:msub><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊛</mml:mo><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐡</mml:mi><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:msub></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">+</mml:mo><mml:mi mathsize="125%" mathvariant="normal">⋯</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.003</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Quantifying the effect of different penalties on convNMF.</title><p>(<bold>A</bold>) Example factorizations of a synthetic dataset for convolutional NMF, and for convolutional NMF with three different penalties designed to eliminate correlations in <underline><underline><underline><underline><inline-formula><mml:math id="inf18"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula></underline></underline></underline></underline> or in both <underline><underline><underline><underline><inline-formula><mml:math id="inf19"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula></underline></underline></underline></underline> and <underline><inline-formula><mml:math id="inf20"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula></underline>. Notice that different penalties lead to different types of redundancies in the corresponding factorizations. (<bold>B</bold>) Quantification of correlations in <underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><inline-formula><mml:math id="inf21"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline> <underline>and</underline> <underline><underline><underline><underline><inline-formula><mml:math id="inf22"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula></underline></underline></underline></underline> for each different penalty. H correlations are measured using <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">∥</mml:mo><mml:msup><mml:mi mathsize="125%" mathvariant="bold">𝐇𝐒𝐇</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊤</mml:mo></mml:msup><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">∥</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathsize="125%">i</mml:mi></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">≠</mml:mo><mml:mi mathsize="125%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and W correlations are measured using <inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">∥</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi mathsize="125%">f</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%">l</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%">a</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%">t</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:msubsup><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi mathsize="125%">f</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%">l</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%">a</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%">t</mml:mi></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊤</mml:mo></mml:msubsup></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">∥</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathsize="125%">i</mml:mi></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">≠</mml:mo><mml:mi mathsize="125%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf25"><mml:mrow><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi mathsize="125%">f</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%">l</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%">a</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%">t</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" mathsize="125%" mathvariant="normal" stretchy="false" symmetric="true">∑</mml:mo><mml:mi mathsize="125%" mathvariant="normal">ℓ</mml:mi></mml:msub><mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">(</mml:mo><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⋅</mml:mo><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁣</mml:mo><mml:mrow><mml:mi/><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⋅</mml:mo><mml:mi mathsize="125%" mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig1-figsupp1-v2.tif"/></fig></fig-group><p>While convNMF may produce excellent reconstructions of the data, it does not automatically produce the minimal number of factors required. Indeed, if the number of factors in the convNMF model is greater than the true number of sequences, the algorithm returns overly complex and redundant factorizations. Moreover, in these cases, the sequences extracted by convNMF will often be inconsistent across optimization runs from different initial conditions, complicating scientific interpretations of the results (<xref ref-type="bibr" rid="bib50">Peter et al., 2017</xref>; <xref ref-type="bibr" rid="bib71">Wu et al., 2016</xref>).</p><p>To address these concerns, we developed a toolbox of methods, called seqNMF, which includes two different strategies to resolve the problem of redundant factorizations described above. In addition, the toolbox includes methods for promoting potentially desirable features such as orthogonality or sparsity of the spatial and temporal structure of extracted factors, and methods for analyzing the statistical significance and prevalence of the identified sequential structure. To assess these tools, we characterize their performance on synthetic data under a variety of noise conditions and also show that they are able to find sequences in neural data collected from two different animal species using different behavioral protocols and recording technologies. Applied to extracellular recordings from rat hippocampus, seqNMF identifies neural sequences that were previously found by trial-averaging. Applied to functional calcium imaging data recorded in vocal/motor cortex of untutored songbirds, seqNMF robustly identifies neural sequences active in a biologically atypical and overlapping fashion. This finding highlights the utility of our approach to extract sequences without reference to external landmarks; untutored bird songs are so variable that aligning neural activity to song syllables would be difficult and highly subjective.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Matrix factorization framework for unsupervised discovery of features in neural data</title><p>Matrix factorization underlies many well-known unsupervised learning algorithms, including PCA (<xref ref-type="bibr" rid="bib49">Pearson, 1901</xref>), non-negative matrix factorization (NMF) (<xref ref-type="bibr" rid="bib32">Lee and Seung, 1999</xref>), dictionary learning, and k-means clustering (see <xref ref-type="bibr" rid="bib64">Udell et al., 2016</xref> for a review). We start with a data matrix, <inline-formula><mml:math id="inf26"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>, containing the activity of <inline-formula><mml:math id="inf27"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons at <inline-formula><mml:math id="inf28"><mml:mi>T</mml:mi></mml:math></inline-formula> timepoints. If the neurons exhibit a single repeated pattern of synchronous activity, the entire data matrix can be reconstructed using a column vector <inline-formula><mml:math id="inf29"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula> representing the neural pattern, and a row vector <inline-formula><mml:math id="inf30"><mml:mi mathvariant="bold">𝐡</mml:mi></mml:math></inline-formula> representing the times and amplitudes at which that pattern occurs (temporal loadings). In this case, the data matrix <inline-formula><mml:math id="inf31"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> is mathematically reconstructed as the outer product of <inline-formula><mml:math id="inf32"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf33"><mml:mi mathvariant="bold">𝐡</mml:mi></mml:math></inline-formula>. If multiple component patterns are present in the data, then each pattern can be reconstructed by a separate outer product, where the reconstructions are summed to approximate the entire data matrix (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) as follows:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐖𝐇</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf34"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf35"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> element of matrix <inline-formula><mml:math id="inf36"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>, that is, the activity of neuron <inline-formula><mml:math id="inf37"><mml:mi>n</mml:mi></mml:math></inline-formula> at time <inline-formula><mml:math id="inf38"><mml:mi>t</mml:mi></mml:math></inline-formula>. Here, in order to store <inline-formula><mml:math id="inf39"><mml:mi>K</mml:mi></mml:math></inline-formula> different patterns, <inline-formula><mml:math id="inf40"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> matrix containing the <inline-formula><mml:math id="inf42"><mml:mi>K</mml:mi></mml:math></inline-formula> exemplar patterns, and <inline-formula><mml:math id="inf43"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> matrix containing the <inline-formula><mml:math id="inf45"><mml:mi>K</mml:mi></mml:math></inline-formula> timecourses:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="center center center center" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd/><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd/><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mo>−</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>−</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>−</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>−</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Given a data matrix with unknown patterns, the goal of matrix factorization is to discover a small set of patterns, <inline-formula><mml:math id="inf46"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>, and a corresponding set of temporal loading vectors, <inline-formula><mml:math id="inf47"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>, that approximate the data. In the case that the number of patterns, <inline-formula><mml:math id="inf48"><mml:mi>K</mml:mi></mml:math></inline-formula>, is sufficiently small (less than <inline-formula><mml:math id="inf49"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf50"><mml:mi>T</mml:mi></mml:math></inline-formula>), this corresponds to a dimensionality reduction, whereby the data is expressed in more compact form. PCA additionally requires that the columns of <inline-formula><mml:math id="inf51"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and the rows of <inline-formula><mml:math id="inf52"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> are orthogonal. NMF instead requires that the elements of <inline-formula><mml:math id="inf53"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf54"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> are nonnegative. The discovery of unknown factors is often accomplished by minimizing the following cost function, which measures the element-by-element sum of all squared errors between a reconstruction <inline-formula><mml:math id="inf55"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐖𝐇</mml:mi></mml:mrow></mml:math></inline-formula> and the original data matrix <inline-formula><mml:math id="inf56"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> using the Frobenius norm, <inline-formula><mml:math id="inf57"><mml:mrow><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mo>∥</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mspace linebreak="newline"/></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>(Note that other loss functions may be substituted if desired, for example to better reflect the noise statistics; see (<xref ref-type="bibr" rid="bib64">Udell et al., 2016</xref>) for a review). The factors <inline-formula><mml:math id="inf58"><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf59"><mml:msup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> that minimize this cost function produce an optimal reconstruction <inline-formula><mml:math id="inf60"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Iterative optimization methods such as gradient descent can be used to search for global minima of the cost function; however, it is often possible for these methods to get caught in local minima. Thus, as described below, it is important to run multiple rounds of optimization to assess the stability/consistency of each model.</p><p>While this general strategy works well for extracting synchronous activity, it is unsuitable for discovering temporally extended patterns—first, because each element in a sequence must be represented by a different factor, and second, because NMF assumes that the columns of the data matrix are independent ‘samples’ of the data, so permutations in time have no effect on the factorization of a given data. It is therefore necessary to adopt a different strategy for temporally extended features.</p></sec><sec id="s2-2"><title>Convolutional matrix factorization</title><p>Convolutional nonnegative matrix factorization (convNMF) (<xref ref-type="bibr" rid="bib58">Smaragdis, 2004</xref>; <xref ref-type="bibr" rid="bib59">Smaragdis, 2007</xref>) extends NMF to provide a framework for extracting temporal patterns, including sequences, from data. While in classical NMF each factor <inline-formula><mml:math id="inf61"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> is represented by a single vector (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), the factors <inline-formula><mml:math id="inf62"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> in convNMF represent patterns of neural activity over a brief period of time. Each pattern is stored as an <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> matrix, <inline-formula><mml:math id="inf64"><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mi mathvariant="bold">𝐤</mml:mi></mml:msub></mml:math></inline-formula>, where each column (indexed by <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf66"><mml:mi>L</mml:mi></mml:math></inline-formula>) indicates the activity of neurons at different timelags within the pattern (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The times at which this pattern/sequence occurs are encoded in the row vector <inline-formula><mml:math id="inf67"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>𝟏</mml:mn></mml:msub></mml:math></inline-formula>, as for NMF. The reconstruction is produced by convolving the <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> pattern with the time series <inline-formula><mml:math id="inf69"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>𝟏</mml:mn></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>If the data contains multiple patterns, each pattern is captured by a different <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> matrix and a different associated time series vector <inline-formula><mml:math id="inf71"><mml:mi mathvariant="bold">𝐡</mml:mi></mml:math></inline-formula>. A collection of <inline-formula><mml:math id="inf72"><mml:mi>K</mml:mi></mml:math></inline-formula> different patterns can be compiled together into an <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> array (also known as a tensor), <inline-formula><mml:math id="inf74"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and a corresponding <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> time series matrix <inline-formula><mml:math id="inf76"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>. Analogous to NMF, convNMF generates a reconstruction of the data as a sum of <inline-formula><mml:math id="inf77"><mml:mi>K</mml:mi></mml:math></inline-formula> convolutions between each neural activity pattern (<inline-formula><mml:math id="inf78"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>), and its corresponding temporal loadings (<inline-formula><mml:math id="inf79"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>):<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>⊛</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The tensor/matrix convolution operator <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⊛</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (notation summary, <xref ref-type="table" rid="table1">Table 1</xref>) reduces to matrix multiplication in the <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> case, which is equivalent to standard NMF. The quality of this reconstruction can be measured using the same cost function shown in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, and <inline-formula><mml:math id="inf82"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> may be found iteratively using similar multiplicative gradient descent updates to standard NMF (<xref ref-type="bibr" rid="bib32">Lee and Seung, 1999</xref>; <xref ref-type="bibr" rid="bib58">Smaragdis, 2004</xref>; <xref ref-type="bibr" rid="bib59">Smaragdis, 2007</xref>).</p><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.004</object-id><label>Table 1.</label><caption><title>Notation for convolutional matrix factorization</title></caption><table frame="hsides" rules="groups"><tbody><tr><td style="author-callout-style-b7">Shift operator</td></tr><tr><td><break/>The operator <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">H</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> shifts a matrix <inline-formula><mml:math id="inf85"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> in the <inline-formula><mml:math id="inf86"><mml:mo>→</mml:mo></mml:math></inline-formula> direction by <inline-formula><mml:math id="inf87"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> timebins:<break/><break/> <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">H</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and likewise <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold">H</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:mover><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula><break/><break/> where <inline-formula><mml:math id="inf90"><mml:mo>⋅</mml:mo></mml:math></inline-formula> indicates all elements along the respective matrix dimension.<break/><break/>The shift operator inserts zeros when <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><break/><break/></td></tr><tr><td style="author-callout-style-b7">Tensor convolution operator</td></tr><tr><td><break/>Convolutive matrix factorization reconstructs a data matrix <inline-formula><mml:math id="inf93"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula><break/><break/> using a <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> tensor <inline-formula><mml:math id="inf95"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and a <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf97"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>:<break/><break/> <inline-formula><mml:math id="inf98"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊛</mml:mo><mml:mpadded width="+5pt"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mpadded></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>→</mml:mo><mml:mi/></mml:mrow></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula><break/><break/>Note that each neuron <inline-formula><mml:math id="inf99"><mml:mi>n</mml:mi></mml:math></inline-formula> is reconstructed as the sum of <inline-formula><mml:math id="inf100"><mml:mi>k</mml:mi></mml:math></inline-formula> convolutions:<break/><break/> <inline-formula><mml:math id="inf101"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊛</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><break/><break/></td></tr><tr><td style="author-callout-style-b7">Transpose tensor convolution operator</td></tr><tr><td><break/>The following quantity is useful in several contexts:<break/><break/> <inline-formula><mml:math id="inf102"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mrow><mml:mi/><mml:mo>←</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula><break/><break/>Note that each element <inline-formula><mml:math id="inf103"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> measures<break/><break/> the overlap (correlation) of factor <inline-formula><mml:math id="inf104"><mml:mi>k</mml:mi></mml:math></inline-formula> with the data at time <inline-formula><mml:math id="inf105"><mml:mi>t</mml:mi></mml:math></inline-formula><break/><break/></td></tr><tr><td style="author-callout-style-b7">convNMF reconstruction</td></tr><tr><td><break/><inline-formula><mml:math id="inf106"><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>≈</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>⁣</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>⊛</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁣</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊛</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula><break/><break/>Note that NMF is a special case of convNMF, where <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula><break/><break/></td></tr><tr><td style="author-callout-style-b7"><inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> entrywise norm excluding diagonal elements</td></tr><tr><td><break/>For any <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf110"><mml:mi mathvariant="bold">𝐂</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf111"><mml:mrow><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula><break/></td></tr><tr><td style="author-callout-style-b7">Special matrices</td></tr><tr><td><break/><inline-formula><mml:math id="inf112"><mml:mn>𝟏</mml:mn></mml:math></inline-formula> is a <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> matrix of ones<break/><break/><inline-formula><mml:math id="inf114"><mml:mi mathvariant="bold">𝐈</mml:mi></mml:math></inline-formula> is the <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> identity matrix<break/><break/><inline-formula><mml:math id="inf116"><mml:mi mathvariant="bold">𝐒</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> smoothing matrix: <inline-formula><mml:math id="inf118"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐒</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and otherwise <inline-formula><mml:math id="inf120"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐒</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula><break/><break/></td></tr></tbody></table></table-wrap><p>While convNMF can perform extremely well at reconstructing sequential structure, it can be challenging to use when the number of sequences in the data is not known (<xref ref-type="bibr" rid="bib50">Peter et al., 2017</xref>). In this case, a reasonable strategy would be to choose <inline-formula><mml:math id="inf121"><mml:mi>K</mml:mi></mml:math></inline-formula> at least as large as the number of sequences that one might expect in the data. However, if <inline-formula><mml:math id="inf122"><mml:mi>K</mml:mi></mml:math></inline-formula> is greater than the actual number of sequences, convNMF often identifies more significant factors than are minimally required. This is because each sequence in the data may be approximated equally well by a single sequential pattern or by a linear combination of multiple partial patterns. A related problem is that running convNMF from different random initial conditions produces inconsistent results, finding different combinations of partial patterns on each run (<xref ref-type="bibr" rid="bib50">Peter et al., 2017</xref>). These inconsistency errors fall into three main categories (<xref ref-type="fig" rid="fig1">Figure 1C</xref>):</p><list list-type="bullet"><list-item><p>Type 1: Two or more factors are used to reconstruct the same instances of a sequence.</p></list-item><list-item><p>Type 2: Two or more factors are used to reconstruct temporally different parts of the same sequence, for instance the first half and the second half.</p></list-item><list-item><p>Type 3: Duplicate factors are used to reconstruct different instances of the same sequence.</p></list-item></list><p>Together, these inconsistency errors manifest as strong correlations between different redundant factors, as seen in the similarity of their temporal loadings (<inline-formula><mml:math id="inf123"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>) and/or their exemplar activity patterns (<inline-formula><mml:math id="inf124"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>).</p><p>We next describe two strategies for overcoming the redundancy errors described above. Both strategies build on previous work that reduces correlations between factors in NMF. The first strategy is based on regularization, a common technique in optimization that allows the incorporation of constraints or additional information with the goal of improving generalization performance or simplifying solutions to resolve degeneracies (<xref ref-type="bibr" rid="bib23">Hastie et al., 2009</xref>). A second strategy directly estimates the number of underlying sequences by minimizing a measure of correlations between factors (stability NMF; <xref ref-type="bibr" rid="bib71">Wu et al., 2016</xref>).</p></sec><sec id="s2-3"><title>Optimization penalties to reduce redundant factors</title><p>To reduce the occurrence of redundant factors (and inconsistent factorizations) in convNMF, we sought a principled way of penalizing the correlations between factors by introducing a penalty term, <inline-formula><mml:math id="inf125"><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:math></inline-formula>, into the convNMF cost function:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Regularization has previously been used in NMF to address the problem of duplicated factors, which, similar to Type 1 errors above, present as correlations between the <inline-formula><mml:math id="inf126"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>’s (<xref ref-type="bibr" rid="bib9">Choi, 2008</xref>; <xref ref-type="bibr" rid="bib8">Chen and Cichocki, 2004</xref>). Such correlations are measured by computing the correlation matrix <inline-formula><mml:math id="inf127"><mml:msup><mml:mi mathvariant="bold">𝐇𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:math></inline-formula>, which contains the correlations between the temporal loadings of every pair of factors. The regularization may be implemented using the penalty term <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi class="ltx_font_mathscript">ℛ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐇𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where the seminorm <inline-formula><mml:math id="inf129"><mml:mrow><mml:mo>∥</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mo>∥</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> sums the absolute value of every matrix entry except those along the diagonal (notation summary, <xref ref-type="table" rid="table1">Table 1</xref>) so that correlations between different factors are penalized, while the correlation of each factor with itself is not. Thus, during the minimization process, similar factors compete, and a larger amplitude factor drives down the temporal loading of a correlated smaller factor. The parameter <inline-formula><mml:math id="inf130"><mml:mi>λ</mml:mi></mml:math></inline-formula> controls the magnitude of the penalty term <inline-formula><mml:math id="inf131"><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:math></inline-formula>.</p><p>In convNMF, a penalty term based on <inline-formula><mml:math id="inf132"><mml:msup><mml:mi mathvariant="bold">𝐇𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:math></inline-formula> yields an effective method to prevent errors of Type 1, because it penalizes the associated zero lag correlations. However, it does not prevent errors of the other types, which exhibit different types of correlations. For example, Type 2 errors result in correlated temporal loadings that have a small temporal offset and thus are not detected by <inline-formula><mml:math id="inf133"><mml:msup><mml:mi mathvariant="bold">𝐇𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:math></inline-formula>. One simple way to address this problem is to smooth the <inline-formula><mml:math id="inf134"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>’s in the penalty term with a square window of length <inline-formula><mml:math id="inf135"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> using the smoothing matrix <inline-formula><mml:math id="inf136"><mml:mi mathvariant="bold">𝐒</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf137"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐒</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf138"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> and otherwise <inline-formula><mml:math id="inf139"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐒</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). The resulting penalty, <inline-formula><mml:math id="inf140"><mml:mrow><mml:mi class="ltx_font_mathscript">ℛ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∥</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐇𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>∥</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, allows factors with small temporal offsets to compete, effectively preventing errors of Types 1 and 2.</p><p>This penalty does not prevent errors of Type 3, in which redundant factors with highly similar patterns in <inline-formula><mml:math id="inf141"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> are used to explain different instances of the same sequence. Such factors have temporal loadings that are segregated in time, and thus have low correlations, to which the cost term <inline-formula><mml:math id="inf142"><mml:mrow><mml:mo>∥</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐇𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>∥</mml:mo></mml:mrow></mml:math></inline-formula> is insensitive. One way to resolve errors of Type 3 might be to include an additional cost term that penalizes the similarity of the factor patterns in <inline-formula><mml:math id="inf143"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>. This has the disadvantage of requiring an extra parameter, namely the <inline-formula><mml:math id="inf144"><mml:mi>λ</mml:mi></mml:math></inline-formula> associated with this cost.</p><p>Instead we chose an alternative approach to resolve errors of Type 3 that simultaneously detects correlations in <inline-formula><mml:math id="inf145"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf146"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> using a single cross-orthogonality cost term. We note that, for Type 3 errors, redundant <inline-formula><mml:math id="inf147"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> patterns have a high degree of overlap with the data at the same times, even though their temporal loadings are segregated at different times. To introduce competition between these factors, we first compute, for each pattern in <inline-formula><mml:math id="inf148"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>, its overlap with the data at time <inline-formula><mml:math id="inf149"><mml:mi>t</mml:mi></mml:math></inline-formula>. This quantity is captured in symbolic form by <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula> (see <xref ref-type="table" rid="table1">Table 1</xref>). We then compute the pairwise correlation between the temporal loading of each factor and the overlap of every other factor with the data. This cross-orthogonality penalty term, which we refer to as 'x-ortho’, sums up these correlations across all pairs of factors, implemented as follows:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi class="ltx_font_mathscript">ℛ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>When incorporated into the update rules, this causes any factor that has a high overlap with the data to suppress the temporal loadings (<inline-formula><mml:math id="inf151"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>) of any other factors that have high overlap with the data at that time (Further analysis, Appendix 2). Thus, factors compete to explain each feature of the data, favoring solutions that use a minimal set of factors to give a good reconstruction. The resulting global cost function is:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The update rules for <inline-formula><mml:math id="inf152"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf153"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> are based on the derivatives of this global cost function, leading to a simple modification of the standard multiplicative update rules used for NMF and convNMF (<xref ref-type="bibr" rid="bib32">Lee and Seung, 1999</xref>; <xref ref-type="bibr" rid="bib58">Smaragdis, 2004</xref>; <xref ref-type="bibr" rid="bib59">Smaragdis, 2007</xref>) (<xref ref-type="table" rid="table2">Table 2</xref>). Note that the addition of this cross-orthogonality term does not formally constitute regularization, because it also includes a contribution from the data matrix <inline-formula><mml:math id="inf154"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>, rather than just the model variables <inline-formula><mml:math id="inf155"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf156"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>. However, at least for the case that the data is well reconstructed by the sum of all factors, the x-ortho penalty can be shown to be approximated by a formal regularization (Appendix 2). This formal regularization contains both a term corresponding to a weighted smoothed orthogonality penalty on <inline-formula><mml:math id="inf157"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and a term corresponding to a weighted smoothed) orthogonality penalty on <inline-formula><mml:math id="inf158"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>, consistent with the observation that the x-ortho penalty simultaneously punishes factor correlations in <inline-formula><mml:math id="inf159"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf160"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>.</p><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.005</object-id><label>Table 2.</label><caption><title>Regularized NMF and convNMF: cost functions and algorithms</title></caption><table frame="hsides" rules="groups"><tbody><tr><td style="author-callout-style-b4">NMF</td><td style="author-callout-style-b4"/></tr><tr><td><inline-formula><mml:math id="inf161"><mml:mrow><mml:mi class="ltx_font_mathscript">ℒ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> <break/><inline-formula><mml:math id="inf162"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐖𝐇</mml:mi></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> <break/><inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td style="author-callout-style-b4">convNMF</td><td style="author-callout-style-b4"/></tr><tr><td><inline-formula><mml:math id="inf165"><mml:mrow><mml:mi class="ltx_font_mathscript">ℒ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> <break/><inline-formula><mml:math id="inf166"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊛</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mtext> </mml:mtext><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> <break/><inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>⊛</mml:mo></mml:mstyle><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>⊛</mml:mo></mml:mstyle><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td style="author-callout-style-b7"><inline-formula><mml:math id="inf169"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> regularization for <inline-formula><mml:math id="inf170"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> ( <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf172"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> is analogous)</td><td style="author-callout-style-b7"/></tr><tr><td><inline-formula><mml:math id="inf173"><mml:mrow><mml:mi class="ltx_font_mathscript">ℛ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf174"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> <break/><inline-formula><mml:math id="inf175"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mn>𝟏</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td style="author-callout-style-b7">Orthogonality cost for <inline-formula><mml:math id="inf176"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula></td><td style="author-callout-style-b7"/></tr><tr><td><inline-formula><mml:math id="inf177"><mml:mrow><mml:mi class="ltx_font_mathscript">ℛ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐇𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf178"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> <break/><inline-formula><mml:math id="inf179"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>𝟏</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐈</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td style="author-callout-style-b7">Smoothed orthogonality cost for <inline-formula><mml:math id="inf180"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> (favors ‘events-based’)</td><td style="author-callout-style-b7"/></tr><tr><td><inline-formula><mml:math id="inf181"><mml:mrow><mml:mi class="ltx_font_mathscript">ℛ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐇𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf182"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> <break/><inline-formula><mml:math id="inf183"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>𝟏</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐈</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐇𝐒</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td style="author-callout-style-b7">Smoothed orthogonality cost for <inline-formula><mml:math id="inf184"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> (favors ‘parts-based’)</td><td style="author-callout-style-b7"/></tr><tr><td><inline-formula><mml:math id="inf185"><mml:mrow><mml:mi class="ltx_font_mathscript">ℛ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> <break/> where <break/><inline-formula><mml:math id="inf186"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf187"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>𝟏</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐈</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> <break/><inline-formula><mml:math id="inf188"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td style="author-callout-style-b7">Smoothed cross-factor orthogonality (x-ortho penalty)</td><td style="author-callout-style-b7"/></tr><tr><td><inline-formula><mml:math id="inf189"><mml:mrow><mml:mi class="ltx_font_mathscript">ℛ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf190"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mrow><mml:mi/><mml:mo>←</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>𝟏</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐈</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> <break/><inline-formula><mml:math id="inf191"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>𝟏</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐈</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗𝐒</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>There is an interesting relation between our method for penalizing correlations and other methods for constraining optimization, namely sparsity. Because of the non-negativity constraint imposed in NMF, correlations can also be reduced by increasing the sparsity of the representation. Previous efforts have been made to minimize redundant factors using sparsity constraints; however, this approach may require penalties on both <inline-formula><mml:math id="inf192"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf193"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>, necessitating the selection of two hyper-parameters (<inline-formula><mml:math id="inf194"><mml:msub><mml:mi>λ</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf195"><mml:msub><mml:mi>λ</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula>) (<xref ref-type="bibr" rid="bib50">Peter et al., 2017</xref>). Since the use of multiple penalty terms increases the complexity of model fitting and selection of parameters, one goal of our work was to design a simple, single penalty function that could regularize both <underline><inline-formula><mml:math id="inf196"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula></underline> and <underline><inline-formula><mml:math id="inf197"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula></underline> simultaneously. The x-ortho penalty described above serves this purpose (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>). As we will describe below, the application of sparsity penalties can be very useful for shaping the factors produced by convNMF, and our code includes options for applying sparsity penalties on both <inline-formula><mml:math id="inf198"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf199"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>.</p><sec id="s2-3-1"><title>Extracting ground-truth sequences with the x-ortho penalty when the number of sequences is not known</title><p>We next examined the effect of the x-ortho penalty on factorizations of sequences in simulated data, with a focus on convergence, consistency of factorizations, the ability of the algorithm to discover the correct number of sequences in the data, and robustness to noise (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). We first assessed the model’s ability to extract three ground-truth sequences lasting 30 timesteps and containing 10 neurons in the absence of noise (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The resulting data matrix had a total duration of 15,000 timesteps and contained on average 60±6 instances of each sequence. Neural activation events were represented with an exponential kernel to simulate calcium imaging data. The algorithm was run with the x-ortho penalty for 1000 iterations andit reliably converged to a root-mean-squared-error (RMSE) close to zero (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). RMSE reached a level within 10% of the asymptotic value in approximately 100 iterations.</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.006</object-id><label>Figure 2.</label><caption><title>Effect of the x-ortho penalty on the factorization of sequences.</title><p>(<bold>A</bold>) A simulated dataset with three sequences. Also shown is a factorization with x-ortho penalty (<inline-formula><mml:math id="inf200"><mml:mrow><mml:mi mathsize="125%">K</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">20</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf201"><mml:mrow><mml:mi mathsize="125%">L</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">50</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf202"><mml:mrow><mml:mi mathsize="125%">λ</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.003</mml:mn></mml:mrow></mml:math></inline-formula>). Each significant factor is shown in a different color. At left are the exemplar patterns (<inline-formula><mml:math id="inf203"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>) and on top are the timecourses (<inline-formula><mml:math id="inf204"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>). (<bold>B</bold>) Reconstruction error as a function of iteration number. Factorizations were run on a simulated dataset with three sequences and 15,000 timebins (<inline-formula><mml:math id="inf205"><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">≈</mml:mo></mml:math></inline-formula> 60 instances of each sequence). Twenty independent runs are shown. Here, the algorithm converges to within 10% of the asymptotic error value within <inline-formula><mml:math id="inf206"><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">≈</mml:mo></mml:math></inline-formula> 100 iterations. (<bold>C</bold>) The x-ortho penalty produces more consistent factorizations than unregularized convNMF across 400 independent fits (<inline-formula><mml:math id="inf207"><mml:mrow><mml:mi mathsize="125%">K</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">20</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf208"><mml:mrow><mml:mi mathsize="125%">L</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">50</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf209"><mml:mrow><mml:mi mathsize="125%">λ</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.003</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>D</bold>) The number of statistically significant factors (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) vs. the number of ground-truth sequences for factorizations with and without the x-ortho penalty. Shown for each condition is a vertical histogram representing the number of significant factors over 20 runs (<inline-formula><mml:math id="inf210"><mml:mrow><mml:mi mathsize="125%">K</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">20</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf211"><mml:mrow><mml:mi mathsize="125%">L</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">50</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf212"><mml:mrow><mml:mi mathsize="125%">λ</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.003</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>E</bold>) Factorization with x-ortho penalty of two simulated neural sequences with shared neurons that participate at the same latency. (<bold>F</bold>) Same as E but for two simulated neural sequences with shared neurons that participate at different latencies.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.007</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Outline of the procedure used to assess factor significance.</title><p>(<bold>A</bold>) Distribution of overlap values between an extracted factor and the held-out data. (<bold>B</bold>) A null factor was constructed by randomly circularly shifting each row of a factor independently. Many null factors were constructed and the distribution of overlap values (<inline-formula><mml:math id="inf213"><mml:mrow><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mover accent="true"><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊛</mml:mo><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊤</mml:mo></mml:mover><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%" mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula>) was measured between each null factor and the held-out data. (<bold>C</bold>) A comparison of the skewness values for each null factor and the skewness of the overlaps of the original extracted factor. A factor is deemed significant if its skewness is significantly greater than the distribution of skewness values for the null factor overlaps.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.008</object-id><label>Figure 2—figure supplement 2.</label><caption><title>Number of significant factors as a function of <inline-formula><mml:math id="inf214"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> for datasets containing between 1 and 10 sequences.</title><p>The number of significant factors obtained by fitting data containing between 1 and 10 ground truth sequences using the x-ortho penalty (<inline-formula><mml:math id="inf215"><mml:mrow><mml:mi mathsize="125%">K</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">20</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf216"><mml:mrow><mml:mi mathsize="125%">L</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">50</mml:mn></mml:mrow></mml:math></inline-formula>) for a large range of values of <inline-formula><mml:math id="inf217"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>. For each value of <inline-formula><mml:math id="inf218"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>, 20 fits are shown and the mean is shown as a solid line. Each color corresponds to a ground-truth dataset containing a different number of sequences and no added noise. Values of <inline-formula><mml:math id="inf219"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> ranging between <inline-formula><mml:math id="inf220"><mml:mn mathsize="125%" mathvariant="normal">0.001</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf221"><mml:mn mathsize="125%" mathvariant="normal">0.1</mml:mn></mml:math></inline-formula> tended to return the correct number of significant sequences at least 90% of the time.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig2-figsupp2-v2.tif"/></fig></fig-group><p>While similar RMSE values were achieved using convNMF with and without the x-ortho penalty; the addition of this penalty allowed three ground-truth sequences to be robustly extracted into three separate factors (<inline-formula><mml:math id="inf222"><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf223"><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf224"><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> in <xref ref-type="fig" rid="fig2">Figure 2A</xref>) so long as <inline-formula><mml:math id="inf225"><mml:mi>K</mml:mi></mml:math></inline-formula> was chosen to be larger than the true number of sequences. In contrast, convNMF with no penalty converged to inconsistent factorizations from different random initializations when <inline-formula><mml:math id="inf226"><mml:mi>K</mml:mi></mml:math></inline-formula> was chosen to be too large, due to the ambiguities described in <xref ref-type="fig" rid="fig1">Figure 1</xref>. We quantified the consistency of each model (see Materials and methods), and found that factorizations using the x-ortho penalty demonstrated near perfect consistency across different optimization runs (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><p>We next evaluated the performance of convNMF with and without the x-ortho penalty on datasets with a larger number of sequences. In particular, we set out to observe the effect of the x-ortho penalty on the number of statistically significant factors extracted. Statistical significance was determined based on the overlap of each extracted factor with held out data (see Materials and methods and code package). With the penalty term, the number of significant sequences closely matched the number of ground-truth sequences. Without the penalty, all 20 extracted sequences were significant by our test (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p><p>We next considered how the x-ortho penalty performs on sequences with more complex structure than the sparse uniform sequences of activity ediscussed above. We further examined the case in which a population of neurons is active in multiple different sequences. Such neurons that are shared across different sequences have been observed in several neuronal datasets (<xref ref-type="bibr" rid="bib45">Okubo et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Pastalkova et al., 2008</xref>; <xref ref-type="bibr" rid="bib22">Harvey et al., 2012</xref>). For one test, we constructed two sequences in which shared neurons were active at a common pattern of latencies in both sequences; in another test, shared neurons were active in a different pattern of latencies in each sequence. In both tests, factorizations using the x-ortho penalty achieved near-perfect reconstruction error, and consistency was similar to the case with no shared neurons (<xref ref-type="fig" rid="fig2">Figure 2E,F</xref>). We also examined other types of complex structure and have found that the x-ortho penalty performs well in data with large gaps between activity or with large overlaps of activity between neurons in the sequence. This approach also worked well in cases in which the duration of the activity or the interval between the activity of neurons varied across the sequence (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>).</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.009</object-id><label>Figure 3.</label><caption><title>Testing factorization performance on sequences contaminated with noise.</title><p>Performance of the x-ortho penalty was tested under four different noise conditions: (<bold>A</bold>) probabilistic participation, (<bold>B</bold>) additive noise, (<bold>C</bold>) temporal jitter, and (<bold>D</bold>) sequence warping. For each noise type, we show: (top) examples of synthetic data at three different noise levels; (middle) similarity of extracted factors to ground-truth patterns across a range of noise levels (20 fits for each level); and (bottom) examples of extracted factors <inline-formula><mml:math id="inf227"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>’s for one of the ground-truth patterns. Examples are shown at the same three noise levels illustrated in the top row. In these examples, the algorithm was run with <inline-formula><mml:math id="inf228"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf229"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf230"><mml:mi>λ</mml:mi></mml:math></inline-formula> = <inline-formula><mml:math id="inf231"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (via the procedure described in <xref ref-type="fig" rid="fig4">Figure 4</xref>). For <bold>C</bold>, jitter displacements were draw from a discrete guassian distribution with the standard deviation in timesteps shown above For <bold>D</bold>, timewarp conditions 1–10 indicate: 0, 66, 133, 200, 266, 333, 400, 466, 533 and 600 max % stretching respectively. For results at different values of <inline-formula><mml:math id="inf232"><mml:mi>λ</mml:mi></mml:math></inline-formula>, see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.010</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Robustness to noise at different values of <inline-formula><mml:math id="inf233"><mml:mi>λ</mml:mi></mml:math></inline-formula>.</title><p>Performance of the x-ortho penalty was tested under four different noise conditions, at different values of <inline-formula><mml:math id="inf234"><mml:mi>λ</mml:mi></mml:math></inline-formula> than in <xref ref-type="fig" rid="fig3">Figure 3</xref> (where <inline-formula><mml:math id="inf235"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>): (<bold>A</bold>) probabilistic participation, <inline-formula><mml:math id="inf236"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, (<bold>B</bold>) additive noise, <inline-formula><mml:math id="inf237"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (<bold>C</bold>) timing jitter, <inline-formula><mml:math id="inf238"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and (<bold>D</bold>) sequence warping, <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. For each noise type, we show: (top) examples of synthetic data at three different noise levels; (middle) similarity of x-ortho factors to ground-truth factors across a range of noise levels (20 fits for each level); and (bottom) example of one of the <inline-formula><mml:math id="inf240"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>’s extracted at three different noise levels (same conditions as data shown above). The algorithm was run with <inline-formula><mml:math id="inf241"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf242"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.011</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Robustness to small dataset size when using the x-ortho penalty.</title><p>(<bold>A</bold>) A short (400 timestep) dataset containing one example each of three ground-truth sequences, as well as additive noise. (<bold>B</bold>) As a function of dataset size, similarity of extracted factors to noiseless, ground-truth factors. At each dataset size, 20 independent fits of penalized convNMF are shown. Median shown in red. Three examples of each sequence were sufficient to acheive similiarty scores within 10% of asymptotic performance. (<bold>C</bold>) Example factors fit on data containing 2, 3, 4 or 20 examples of each sequence. Extracted factors were significant on held-out data compared to null (shuffled) factors even when training and test datasets each contained only 2 examples of each sequence.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.012</object-id><label>Figure 3—figure supplement 3.</label><caption><title>Robustness to different types of sequences.</title><p>Characterization of the x-ortho penalty for additional types of noise. (<bold>A</bold>) An example of a factorization for a sequence with large gaps between members of the sequence. (<bold>B–D</bold>) Example factorizations of sequences with neuronal activations that are highly overlapped in time. (<bold>B</bold>) An example of an x-ortho penalized factorization that reconstructs the data using complex patterns in <underline><inline-formula><mml:math id="inf243"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula></underline> <underline>and</underline> <underline><inline-formula><mml:math id="inf244"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula></underline>. (<bold>C</bold>) An example of an x-ortho penalized factorization with the addition of an L1 penalty on <underline><underline><underline><underline><inline-formula><mml:math id="inf245"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula></underline></underline></underline></underline> models the data as an overlapping pattern with sparse activations. (<bold>D</bold>) An example of an x-ortho penalized factorization with the addition of an L1 penalty on <underline><underline><underline><underline><inline-formula><mml:math id="inf246"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula></underline></underline></underline></underline> models the data as a non-overlapping pattern with dense activations. (<bold>E</bold>) An example of an x-ortho penalized factorization for data in which neurons have varying durations of activation which form two patterns. (<bold>F</bold>) An example of an x-ortho penalized factorization for data in which neurons have varying durations of activation which are random. (<bold>G–I</bold>) Examples factorizations of sequences with statistics that vary systematically. (<bold>G</bold>) An example of an x-ortho penalized factorization for data in which neurons have systematically varying changes in duration of activity. (<bold>H</bold>) An example of an x-ortho penalized factorization for data in which neurons have systematically varying changes in the gaps between members of the sequence. (<bold>I</bold>) An example of an x-ortho penalized factorization for data in which neurons have systematically varying changes in the amount of jitter.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig3-figsupp3-v2.tif"/></fig></fig-group><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.013</object-id><label>Figure 4.</label><caption><title>Procedure for choosing <inline-formula><mml:math id="inf247"><mml:mi>λ</mml:mi></mml:math></inline-formula> for a new dataset based on finding a balance between reconstruction cost and x-ortho cost.</title><p>(<bold>A</bold>) Simulated data containing three sequences in the presence of participation noise (50% participation probability). This noise condition is used for the tests in (<bold>B–F</bold>). (<bold>B</bold>) Normalized reconstruction cost (<inline-formula><mml:math id="inf248"><mml:msubsup><mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathsize="125%" mathvariant="bold">𝐗</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">~</mml:mo></mml:mover><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">-</mml:mo><mml:mi mathsize="125%" mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo></mml:mrow><mml:mi mathsize="125%">F</mml:mi><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:msubsup></mml:math></inline-formula>) and cross-orthogonality cost (<inline-formula><mml:math id="inf249"><mml:msub><mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo><mml:mrow><mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">(</mml:mo><mml:mrow><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mover accent="true"><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊛</mml:mo><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊤</mml:mo></mml:mover><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%" mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">)</mml:mo></mml:mrow><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:msup><mml:mi mathsize="125%" mathvariant="bold">𝐒𝐇</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊤</mml:mo></mml:msup></mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathsize="125%">i</mml:mi></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">≠</mml:mo><mml:mi mathsize="125%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) as a function of <inline-formula><mml:math id="inf250"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> for 20 fits of these data. The cross-over point <inline-formula><mml:math id="inf251"><mml:msub><mml:mi mathsize="125%">λ</mml:mi><mml:mn mathsize="125%" mathvariant="normal">0</mml:mn></mml:msub></mml:math></inline-formula> is marked with a black circle. Note that in this plot the reconstruction cost and cross-orthogonality cost are normalized to vary between 0 and 1. (<bold>C</bold>) The number of significant factors obtained as a function of <inline-formula><mml:math id="inf252"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>; 20 fits, mean plotted in orange. Red arrow at left indicates the correct number of sequences (three). (<bold>D</bold>) Fraction of fits returning the correct number of significant factors as a function of <inline-formula><mml:math id="inf253"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>. (<bold>E</bold>) Similarity of extracted factors to ground-truth sequences as a function of <inline-formula><mml:math id="inf254"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>. (<bold>F</bold>) Composite performance, as the product of the curves in (<bold>D</bold>) and (<bold>E</bold>) (smoothed using a three sample boxcar, plotted in orange with a circle marking the peak). Shaded region indicates the range of <inline-formula><mml:math id="inf255"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> that works well (<inline-formula><mml:math id="inf256"><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">±</mml:mo></mml:math></inline-formula> half height of composite performance). (<bold>G–L</bold>) same as (<bold>A–F</bold>) but for simulated data containing three noiseless sequences. (<bold>M</bold>) Summary plot showing the range of values of <inline-formula><mml:math id="inf257"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> (vertical bars), relative to the cross-over point <inline-formula><mml:math id="inf258"><mml:msub><mml:mi mathsize="125%">λ</mml:mi><mml:mn mathsize="125%" mathvariant="normal">0</mml:mn></mml:msub></mml:math></inline-formula>, that work well for each noise condition (<inline-formula><mml:math id="inf259"><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">±</mml:mo></mml:math></inline-formula> half height points of composite performance). Circles indicate the value of <inline-formula><mml:math id="inf260"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> at the peak of the smoothed composite performance. For each noise type, results for all noise levels from <xref ref-type="fig" rid="fig3">Figure 3</xref> are shown (increasing color saturation at high noise levels; Green, participation: 90, 80, 70, 60, 50, 40, 30, and 20%; Orange, additive noise 0.5, 1, 2, 2.5, 3, 3.5, and 4%; Purple, jitter: SD of the distribution of random jitter: 5, 10, 15, 20, 25, 30, 35, 40, and 45 timesteps; Grey, timewarp: 66, 133, 200, 266, 333, 400, 466, 533, 600, and 666 max % stretching. Asterisk (*) indicates the noise type and level used in panels (<bold>A–F</bold>). Gray band indicates a range between <inline-formula><mml:math id="inf261"><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:msub><mml:mi mathsize="125%">λ</mml:mi><mml:mn mathsize="125%" mathvariant="normal">0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf262"><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">5</mml:mn><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:msub><mml:mi mathsize="125%">λ</mml:mi><mml:mn mathsize="125%" mathvariant="normal">0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, a range that tended to perform well across the different noise conditions. In real data, it may be useful to explore a wider range of <inline-formula><mml:math id="inf263"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.014</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Analysis of the best range of <inline-formula><mml:math id="inf264"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>.</title><p>Here, we quantify the full width at the half maximum for the composite performance scores in different noise conditions. For each condition, a box and whisker plot quantifies the number of orders of magnitude over which a good factorization is returned (median denoted by a white circle). Next to each box plot individual points are shown, corresponding to different noise level. Color saturation reflects noise level as in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.015</object-id><label>Figure 4—figure supplement 2.</label><caption><title>Procedure for choosing <inline-formula><mml:math id="inf265"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> applied to data with shared neurons.</title><p>(<bold>A</bold>) Simulated data containing two patterns which share 50% of their neurons, in the presence of participation noise (70% participation probability). (<bold>B</bold>) Normalized reconstruction cost (<inline-formula><mml:math id="inf266"><mml:msubsup><mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathsize="125%" mathvariant="bold">𝐗</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">~</mml:mo></mml:mover><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">-</mml:mo><mml:mi mathsize="125%" mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo></mml:mrow><mml:mi mathsize="125%">F</mml:mi><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:msubsup></mml:math></inline-formula>) and cross-orthogonality cost (<inline-formula><mml:math id="inf267"><mml:msub><mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo><mml:mrow><mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">(</mml:mo><mml:mrow><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mover accent="true"><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊛</mml:mo><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊤</mml:mo></mml:mover><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%" mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">)</mml:mo></mml:mrow><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:msup><mml:mi mathsize="125%" mathvariant="bold">𝐒𝐇</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊤</mml:mo></mml:msup></mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathsize="125%">i</mml:mi></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">≠</mml:mo><mml:mi mathsize="125%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) as a function of <inline-formula><mml:math id="inf268"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> for these data. The cross-over point <inline-formula><mml:math id="inf269"><mml:msub><mml:mi mathsize="125%">λ</mml:mi><mml:mn mathsize="125%" mathvariant="normal">0</mml:mn></mml:msub></mml:math></inline-formula> is marked with a black circle. (<bold>C</bold>) The number of significant factors obtained from 20 fits of these data as a function of <inline-formula><mml:math id="inf270"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> (mean number plotted in orange). The correct number of factors (two) is marked by a red triangle. (<bold>D</bold>) The fraction of fits returning the correct number of significant factors as a function of <inline-formula><mml:math id="inf271"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>. (<bold>E</bold>) Similarity of the top two factors to ground-truth (noiseless) factors as a function of <inline-formula><mml:math id="inf272"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>. (<bold>F</bold>) Composite performance measured as the product of the curves shown in (<bold>D</bold>) and (<bold>E</bold>), (smoothed curve plotted in orange with a circle marking the peak). Shaded region indicates the range of <inline-formula><mml:math id="inf273"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> that works well (<inline-formula><mml:math id="inf274"><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">±</mml:mo></mml:math></inline-formula> half height of composite performance). For this dataset, the best performance occurs at <inline-formula><mml:math id="inf275"><mml:mrow><mml:mi mathsize="125%">λ</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">5</mml:mn><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:msub><mml:mi mathsize="125%">λ</mml:mi><mml:mn mathsize="125%" mathvariant="normal">0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, while a range of <inline-formula><mml:math id="inf276"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> between 2 <inline-formula><mml:math id="inf277"><mml:msub><mml:mi mathsize="125%">λ</mml:mi><mml:mn mathsize="125%" mathvariant="normal">0</mml:mn></mml:msub></mml:math></inline-formula> and 10 <inline-formula><mml:math id="inf278"><mml:msub><mml:mi mathsize="125%">λ</mml:mi><mml:mn mathsize="125%" mathvariant="normal">0</mml:mn></mml:msub></mml:math></inline-formula> performs well.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.016</object-id><label>Figure 4—figure supplement 3.</label><caption><title>Using cross-validation on held-out (masked) data to choose <inline-formula><mml:math id="inf279"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>.</title><p>A method for choosing a reasonable value of <inline-formula><mml:math id="inf280"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> based on cross validation is shown for five different noise types (each column shows a different noise type; from left to right: (<bold>I</bold>) participation noise, (II) additive noise, (III) jitter, (IV) temporal warping), and (<bold>V</bold>) a lower level of participation noise. The cross-validated test error is calculated by fitting x-ortho penalized factorizations while randomly holding out 10% of the elements in the data matrix as a test set (<xref ref-type="bibr" rid="bib70">Wold, 1978</xref>; <xref ref-type="bibr" rid="bib3">Bro et al., 2008</xref>). In many of our test datasets, there was a minimum or a divergence point in the difference between the test and training error, that agreed with the procedure described in <xref ref-type="fig" rid="fig4">Figure 4</xref>, based on <inline-formula><mml:math id="inf281"><mml:msub><mml:mi mathsize="125%">λ</mml:mi><mml:mn mathsize="125%" mathvariant="normal">0</mml:mn></mml:msub></mml:math></inline-formula>. (<bold>A</bold>) Examples of each dataset. (<bold>B</bold>) Test error (blue) and training error (red) as a function of <inline-formula><mml:math id="inf282"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> for each of the different noise conditions. (<bold>C</bold>) The difference between the test error and training error values shown above. (<bold>D</bold>) Normalized reconstruction cost (<inline-formula><mml:math id="inf283"><mml:msubsup><mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathsize="125%" mathvariant="bold">𝐗</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">~</mml:mo></mml:mover><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">-</mml:mo><mml:mi mathsize="125%" mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo></mml:mrow><mml:mi mathsize="125%">F</mml:mi><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:msubsup></mml:math></inline-formula>) and cross-orthogonality cost (<inline-formula><mml:math id="inf284"><mml:msub><mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo><mml:mrow><mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">(</mml:mo><mml:mrow><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mover accent="true"><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊛</mml:mo><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊤</mml:mo></mml:mover><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%" mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">)</mml:mo></mml:mrow><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:msup><mml:mi mathsize="125%" mathvariant="bold">𝐒𝐇</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">⊤</mml:mo></mml:msup></mml:mrow><mml:mo fence="true" mathvariant="normal" maxsize="125%" minsize="125%">||</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathsize="125%">i</mml:mi></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">≠</mml:mo><mml:mi mathsize="125%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) as a function of <inline-formula><mml:math id="inf285"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> for each of the different noise conditions. (<bold>E</bold>) Composite performance as a function of <inline-formula><mml:math id="inf286"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>. Panels D and E are identical to those in <xref ref-type="fig" rid="fig4">Figure 4</xref>, and are included here for comparison. (<bold>V</bold>) These data have a lower amount of participation noise than (<bold>I</bold>). Note that in low-noise conditions, test error may not exhibit a minima within the range of <inline-formula><mml:math id="inf287"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> that produces the ground truth number of factors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig4-figsupp3-v2.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.017</object-id><label>Figure 4—figure supplement 4.</label><caption><title>Quantifying the effect of L1 sparsity penalties on <underline><underline><underline><underline><inline-formula><mml:math id="inf288"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula></underline></underline></underline></underline> and <underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><inline-formula><mml:math id="inf289"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline>.</title><p>(<bold>A</bold>) An example window of simulated data with three sequences and 40% dropout. (<bold>B</bold>) The fraction of fits to data with the noise level in (<bold>A</bold>) that yielded three significant factors, as a function of two L1 sparsity regularization parameters on <underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><inline-formula><mml:math id="inf290"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline> and <underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><inline-formula><mml:math id="inf291"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline>. Each bin represents 20 fits of sparsity penalized convNMF with K = 20 and L = 50. (<bold>C</bold>) The mean similarity to ground-truth for the same 20 factorizations as in (<bold>B</bold>). (<bold>D–F</bold>) same as panels A-C but with additional noise events in 2.5% of the bins. (<bold>H–J</bold>) same as panels A-C but with a jitter standard deviation of 20 bins. (<bold>K–M</bold>) same as panels A-C but for warping noise with a maximum of 260% warping.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig4-figsupp4-v2.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.018</object-id><label>Figure 4—figure supplement 5.</label><caption><title>Comparing the performance of convNMF with an x-ortho or a sparsity penalty.</title><p>(<bold>A</bold>) The fraction of 20 x-ortho penalized fits which had the same number of significant factors as the ground-truth for all noise conditions shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> at the <underline><inline-formula><mml:math id="inf292"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula></underline> with the best performance. (<bold>B</bold>) The similarity to ground-truth for 20 x-ortho penalized fits to all noise conditions shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> at the <underline><inline-formula><mml:math id="inf293"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula></underline> with the best performance. (<bold>C</bold>) The number of significant factors for 100 fits with an x-ortho penalty (black) and with sparsity penalties on <underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><inline-formula><mml:math id="inf294"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline> and <underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><underline><inline-formula><mml:math id="inf295"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline></underline> (Red) of four different noise conditions at the level indicated in (<bold>A</bold>) and (<bold>B</bold>). Penalty parameters used in (<bold>C–E</bold>) were selected by performing a parameter sweep and selecting the parameters which gave the maximum composite score as described above. (<bold>D</bold>) The fraction of 20 x-ortho or sparsity penalized fits with the ground truth number of significant sequences. Noise conditions are the same as in (<bold>C</bold>). Values for <inline-formula><mml:math id="inf296"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> were selected as those that give the highest composite performance (see <xref ref-type="fig" rid="fig4">Figure 4F</xref>). (<bold>E</bold>) Similarity to ground-truth for the fits shown in (<bold>C–D</bold>). Median is shown with black dot and bottom and top edges of boxes indicate the 25<sup>th</sup> and 75<sup>th</sup> percentiles.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig4-figsupp5-v2.tif"/></fig></fig-group></sec><sec id="s2-3-2"><title>Robustness to noisy data</title><p>The cross-orthogonality penalty performed well in the presence of types of noise commonly found in neural data. In particular, we considered: participation noise, in which individual neurons participate probabilistically in instances of a sequence; additive noise, in which neuronal events occur randomly outside of normal sequence patterns; temporal jitter, in which the timing of individual neurons is shifted relative to their typical time in a sequence; and finally, temporal warping, in which each instance of the sequence occurs at a different randomly selected speed. To test the robustness of the algorithm with the x-ortho penalty to each of these noise conditions, we factorized data containing three neural sequences at a variety of noise levels (<xref ref-type="fig" rid="fig3">Figure 3</xref>, top row). The value of <inline-formula><mml:math id="inf297"><mml:mi>λ</mml:mi></mml:math></inline-formula> was chosen using methods described in the next section. Factorizations with the x-ortho penalty proved relatively robust to all four noise types, with a high probability of returning the correct numbers of significant factors (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>). Furthermore, under low-noise conditions, the algorithm produced factors that were highly similar to ground-truth, and this similarity declined gracefully at higher noise levels (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Visualization of the extracted factors revealed a good qualitative match to ground-truth sequences even in the presence of high noise except for the case of temporal jitter (<xref ref-type="fig" rid="fig3">Figure 3</xref>). We also found that the x-ortho penalty allows reliable extraction of sequences in which the duration of each neuron’s activity exhibits substantial random variation across different renditions of the sequence, and in which the temporal jitter of neural activity exhibits systematic variation at different points in the sequences (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>).</p><p>Finally, we wondered how our approach with the x-ortho penalty performs on datasets with only a small number of instances of each sequence. We generated data containing different numbers of repetitions ranging from 1 to 20, of each underlying ground-truth sequence. For intermediate levels of additive noise, we found that three repetitions of each sequence were sufficient to correctly extract factors with similarity scores close to those obtained with much larger numbers of repetitions (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p></sec><sec id="s2-3-3"><title>Methods for choosing an appropriate value of <inline-formula><mml:math id="inf298"><mml:mi>λ</mml:mi></mml:math></inline-formula></title><p>The x-ortho penalty performs best when the strength of the regularization term (determined by the hyperparameter <inline-formula><mml:math id="inf299"><mml:mi>λ</mml:mi></mml:math></inline-formula>) is chosen appropriately. For <inline-formula><mml:math id="inf300"><mml:mi>λ</mml:mi></mml:math></inline-formula> too small, the behavior of the algorithm approaches that of convNMF, producing a large number of redundant factors with high x-ortho cost. For <inline-formula><mml:math id="inf301"><mml:mi>λ</mml:mi></mml:math></inline-formula> too large, all but one of the factors are suppressed to zero amplitude, resulting in a factorization with near-zero x-ortho cost, but with large reconstruction error if multiple sequences are present in the data. Between these extremes, there exists a region in which increasing <inline-formula><mml:math id="inf302"><mml:mi>λ</mml:mi></mml:math></inline-formula> produces a rapidly increasing reconstruction error and a rapidly decreasing x-ortho cost. Thus, there is a single point, which we term <inline-formula><mml:math id="inf303"><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, at which changes in reconstruction cost and changes in x-ortho cost are balanced (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). We hypothesized that the optimal choice of <inline-formula><mml:math id="inf304"><mml:mi>λ</mml:mi></mml:math></inline-formula> (i.e. the one producing the correct number of ground-truth factors) would lie near this point.</p><p>To test this intuition, we examined the performance of the x-ortho penalty as a function of<underline><inline-formula><mml:math id="inf305"><mml:mi>λ</mml:mi></mml:math></inline-formula></underline> in noisy synthetic data consisting of three non-overlapping sequences (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Our analysis revealed that, overall, values of <inline-formula><mml:math id="inf306"><mml:mi>λ</mml:mi></mml:math></inline-formula> between 2<inline-formula><mml:math id="inf307"><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and 5<inline-formula><mml:math id="inf308"><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> performed well for these data across all noise types and levels (<xref ref-type="fig" rid="fig4">Figure 4B,C</xref>). In general, near-optimal performance was observed over an order of magnitude range of <inline-formula><mml:math id="inf309"><mml:mi>λ</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1</xref>). However, there were systematic variations depending on noise type: for additive noise, performance was better when <inline-formula><mml:math id="inf310"><mml:mi>λ</mml:mi></mml:math></inline-formula> was closer to <inline-formula><mml:math id="inf311"><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, while with other noise types, performance was better at somewhat higher values of <inline-formula><mml:math id="inf312"><mml:mi>λ</mml:mi></mml:math></inline-formula>s (<inline-formula><mml:math id="inf313"><mml:mrow><mml:mi/><mml:mo>≈</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>Similar ranges of <inline-formula><mml:math id="inf314"><mml:mi>λ</mml:mi></mml:math></inline-formula> appeared to work for datasets with different numbers of ground-truth sequences—for the datasets used in <xref ref-type="fig" rid="fig2">Figure 2D</xref>, a range of <inline-formula><mml:math id="inf315"><mml:mi>λ</mml:mi></mml:math></inline-formula> between 0.001 and 0.01 returned the correct number of sequences at least 90% of the time for datasets containing between 1 and 10 sequences (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Furthermore, this method for choosing <inline-formula><mml:math id="inf316"><mml:mi>λ</mml:mi></mml:math></inline-formula> also worked on datasets containing sequences with shared neurons (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p><p>The value of <inline-formula><mml:math id="inf317"><mml:mi>λ</mml:mi></mml:math></inline-formula> may also be determined by cross-validation (see Materials and methods). Indeed, the <inline-formula><mml:math id="inf318"><mml:mi>λ</mml:mi></mml:math></inline-formula> chosen with the heuristic described above coincided with a minimum or distinctive feature in the cross-validated test error for all the cases we examined (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). The seqNMF code package accompanying this paper provides functions to determine <inline-formula><mml:math id="inf319"><mml:mi>λ</mml:mi></mml:math></inline-formula> both by cross-validation or in reference to <inline-formula><mml:math id="inf320"><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>.</p></sec><sec id="s2-3-4"><title>Sparsity constraints to reduce redundant factors</title><p>One of the advantages of the x-ortho penalty is that it includes only a single term to penalize correlations between different factors, and thus requires only a single hyperparameter <inline-formula><mml:math id="inf321"><mml:mi>λ</mml:mi></mml:math></inline-formula>. This contrasts with the approach of incorporating a sparsity constraint on <inline-formula><mml:math id="inf322"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf323"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> of the form <inline-formula><mml:math id="inf324"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>∥</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>∥</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib50">Peter et al., 2017</xref>). We have found that the performance of the sparsity approach depends on the correct choice of both hyperparameters <inline-formula><mml:math id="inf325"><mml:msub><mml:mi>λ</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf326"><mml:msub><mml:mi>λ</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>). Given the optimal choice of these parameters, the L1 sparsity constraint yields an overall performance approximately as good as the x-ortho penalty (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>). However, there are some consistent differences in the performance of the sparsity and x-ortho approaches depending on noise type; an analysis at moderately high noise levels reveals that the x-ortho penalty performs slightly better with warping and participation noise, while the L1 sparsity penalty performs slightly better on data with jitter and additive noise (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>). However, given the added complexity of choosing two hyperparameters for L1 sparsity, we prefer the x-ortho approach.</p></sec></sec><sec id="s2-4"><title>Direct selection of <inline-formula><mml:math id="inf327"><mml:mi>K</mml:mi></mml:math></inline-formula> to reduce redundant factors</title><p>An alternative strategy to minimizing redundant factorizations is to estimate the number of underlying sequences and to select the appropriate value of <inline-formula><mml:math id="inf328"><mml:mi>K</mml:mi></mml:math></inline-formula>. An approach for choosing the number of factors in regular NMF is to run the algorithm many times with different initial conditions, at different values of <inline-formula><mml:math id="inf329"><mml:mi>K</mml:mi></mml:math></inline-formula>, and choose the case with the most consistent and uncorrelated factors. This strategy is called stability NMF (<xref ref-type="bibr" rid="bib71">Wu et al., 2016</xref>) and is similar to other stability-based metrics that have been used in clustering models (<xref ref-type="bibr" rid="bib68">von Luxburg, 2010</xref>). The stability NMF score, <italic>diss</italic>, is measured between two factorizations, <inline-formula><mml:math id="inf330"><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf331"><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, run from different initial conditions:<disp-formula id="equ8"><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">diss</mml:mtext></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf332"><mml:mi mathvariant="bold">𝐂</mml:mi></mml:math></inline-formula> is the cross-correlation matrix between the columns of the matrix <inline-formula><mml:math id="inf333"><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:math></inline-formula> and the the columns of the matrix <inline-formula><mml:math id="inf334"><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>. Note that <italic>diss</italic> is low when there is a one-to-one mapping between factors in <inline-formula><mml:math id="inf335"><mml:msup><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf336"><mml:msup><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, which tends to occur at the correct K in NMF (<xref ref-type="bibr" rid="bib71">Wu et al., 2016</xref>; <xref ref-type="bibr" rid="bib63">Ubaru et al., 2017</xref>). NMF is run many times and the <italic>diss</italic> metric is calculated for all unique pairs. The best value of K is chosen as that which yields the lowest average <italic>diss</italic> metric.</p><p>To use this approach for convNMF, we needed to slightly modify the stability NMF <italic>diss</italic> metric. Unlike in NMF, convNMF factors have a temporal degeneracy; that is, one can shift the elements of <inline-formula><mml:math id="inf337"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> by one time step while shifting the elements of <inline-formula><mml:math id="inf338"><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> by one step in the opposite direction with little change to the model reconstruction. Thus, rather than computing correlations from the factor patterns <inline-formula><mml:math id="inf339"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> or loadings <inline-formula><mml:math id="inf340"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>, we computed the <italic>diss</italic> metric using correlations between factor reconstructions (<inline-formula><mml:math id="inf341"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mi mathvariant="bold">𝐤</mml:mi></mml:msub><mml:mo>⊛</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mi mathvariant="bold">𝐤</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>).<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>∥</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>∥</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf342"><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the trace operator, <inline-formula><mml:math id="inf343"><mml:mrow><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. That is, <inline-formula><mml:math id="inf344"><mml:msub><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> measures the correlation between the reconstruction of factor i in <inline-formula><mml:math id="inf345"><mml:msup><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:math></inline-formula> and the reconstruction of factor j in <inline-formula><mml:math id="inf346"><mml:msup><mml:mi mathvariant="bold">𝐅</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>. Here, as for stability NMF, the approach is to run convNMF many times with different numbers of factors (<inline-formula><mml:math id="inf347"><mml:mi>K</mml:mi></mml:math></inline-formula>) and choose the <inline-formula><mml:math id="inf348"><mml:mi>K</mml:mi></mml:math></inline-formula> which minimizes the <italic>diss</italic> metric.</p><p>We evaluated the robustness of this approach in synthetic data with the four noise conditions examined earlier. Synthetic data were constructed with three ground-truth sequences and 20 convNMF factorizations were carried out for each K ranging from 1 to 10. For each K the average <italic>diss</italic> metric was computed over all 20 factorizations. In many cases, the average <italic>diss</italic> metric exhibited a minimum at the ground-truth <inline-formula><mml:math id="inf349"><mml:mi>K</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). As shown below, this method also appears to be useful for identifying the number of sequences in real neural data.</p><p>Not only does the <italic>diss</italic> metric identify factorizations that are highly similar to the ground truth and have the correct number of underlying factors, it also yields factorizations that minimize reconstruction error in held out data (<xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>), as shown using the same cross-validation procedure described above (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). For simulated datasets with participation noise, additive noise, and temporal jitter, there is a clear minimum in the test error at the K given by <italic>diss</italic> metric. In other cases, there is a distinguishing feature such as a kink or a plateau in the test error at this K (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>).</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.019</object-id><label>Figure 5.</label><caption><title>Direct selection of K using the <italic>diss</italic> metric, a measure of the dissimilarity between different factorizations.</title><p>Panels show the distribution of <italic>diss</italic> as a function of K for several different noise conditions. Lower values of <italic>diss</italic> indicate greater consistency or stability of the factorizations, an indication of low factor redundancy. (<bold>A</bold>) probabilistic participation (60%), (<bold>B</bold>) additive noise (2.5% bins), (<bold>C</bold>) timing jitter (SD = 20 bins), and (<bold>D</bold>) sequence warping (max warping = 266%). For each noise type, we show: (top) examples of synthetic data; (bottom) the <italic>diss</italic> metric for 20 fits of convNMF for K from 1 to 10; the black line shows the median of the <italic>diss</italic> metric and the dotted red line shows the true number of factors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.020</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Direct selection of K using the <italic>diss</italic> metric for all noise conditions.</title><p>(<bold>A</bold>) Participation noise, (<bold>B</bold>) additive noise, (<bold>C</bold>) jitter and (<bold>D</bold>) warping. For each panel, the top shows an example of data with three sequences and each noise type. The bottom panel shows the dissimilarity of factorizations in different levels of noise, as a function of K. A condition with no noise is shown in blue and dark red represents the highest noise condition with the color gradient spanning the levels between. Noise levels are the same as in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref>. Notice that there is often either a minimum or a distinct feature at K = 3, corresponding to the ground-truth number of sequences in the data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.021</object-id><label>Figure 5—figure supplement 2.</label><caption><title>Estimating the number of sequences in a dataset using cross-validation on randomly masked held-out datapoints.</title><p>(<bold>A</bold>) Reconstruction error (RMSE) for test data (red) and training data (blue) plotted as a function of the number of components (<bold>K</bold>) used in convNMF. Twenty independent convNMF fits are shown for each value of K. This panel shows results for 10% participation noise. For synthetic data fits, 10% of the data was held out as the test set. For neural data 5% of the data was held out. Other noise conditions are shown as follows: (<bold>B</bold>) jitter noise (10 timestep SD); (<bold>C</bold>) warping (13%); (<bold>D</bold>) higher additive noise (2.5%); (<bold>E</bold>) higher jitter noise (25 timestep SD); (<bold>F</bold>) higher warping (33%) (<bold>G</bold>) Reconstruction error vs. K for neuronal data collected from premotor cortex (area HVC) of a singing bird (<xref ref-type="fig" rid="fig9">Figure 9</xref>) and (<bold>H</bold>) hippocampus of rat 2 performing a left-right alternation task (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig5-figsupp2-v2.tif"/></fig></fig-group></sec><sec id="s2-5"><title>Strategies for dealing with ambiguous sequence structure</title><p>Some sequences can be interpreted in multiple ways, and these interpretations will correspond to different factorizations. A common example arises when neurons are shared between different sequences, as is shown in <xref ref-type="fig" rid="fig6">Figure 6A and B</xref>. In this case, there are two ensembles of neurons (1 and 2), that participate in two different types of events. In one event type, ensemble one is active alone, while in the other event type, ensemble one is coactive with ensemble 2. There are two different reasonable factorizations of these data. In one factorization, the two different ensembles are separated into two different factors, while in the other factorization the two different event types are separated into two different factors. We refer to these as ’parts-based’ and ’events-based’ respectively. Note that these different factorizations may correspond to different intuitions about underlying mechanisms. ‘Parts-based’ factorizations will be particularly useful for clustering neurons into ensembles, and ‘events-based’ factorizations will be particularly useful for correlating neural events with behavior.</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.022</object-id><label>Figure 6.</label><caption><title>Using penalties to bias toward events-based and parts-based factorizations.</title><p>Datasets that have neurons shared between multiple sequences can be factorized in different ways, emphasizing discrete temporal events (events-based) or component neuronal ensembles (parts-based), by using orthogonality penalties on <inline-formula><mml:math id="inf350"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf351"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> to penalize factor correlations (see <xref ref-type="table" rid="table2">Table 2</xref>). (Left) A dataset with two different ensembles of neurons that participate in two different types of events, with (<bold>A</bold>) events-based factorization obtained using an orthogonality penalty on <inline-formula><mml:math id="inf352"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> and (<bold>B</bold>) parts-based factorizations obtained using an orthogonality penalty on <inline-formula><mml:math id="inf353"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>. (Right) A dataset with six different ensembles of neurons that participate in three different types of events, with (<bold>C</bold>) events-based and (<bold>D</bold>) parts-based factorizations obtained as in (<bold>A</bold>) and (<bold>B</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.023</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Biasing factorizations between sparsity in <inline-formula><mml:math id="inf354"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf355"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>.</title><p>Two different factorizations of the same simulated data, where a sequence is always repeated precisely three times. Both yield perfect reconstructions, and no cross-factor correlations. The factorizations differ in the amount of features placed in <inline-formula><mml:math id="inf356"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> versus <inline-formula><mml:math id="inf357"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>. Both use <inline-formula><mml:math id="inf358"><mml:mrow><mml:mi mathsize="125%">K</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">3</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf359"><mml:mrow><mml:mi mathsize="125%">λ</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.001</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>A</bold>) Factorization achieved using a sparsity penalty on <inline-formula><mml:math id="inf360"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>, with <inline-formula><mml:math id="inf361"><mml:mrow><mml:msub><mml:mi mathsize="125%">λ</mml:mi><mml:mrow><mml:mi mathsize="125%">L</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>B</bold>) Factorization achieved using a sparsity penalty on <inline-formula><mml:math id="inf362"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>, with <inline-formula><mml:math id="inf363"><mml:mrow><mml:msub><mml:mi mathsize="125%">λ</mml:mi><mml:mrow><mml:mi mathsize="125%">L</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig6-figsupp1-v2.tif"/></fig></fig-group><p>Here, we show that the addition of penalties on either <inline-formula><mml:math id="inf364"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf365"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> correlations can be used to shape the factorizations of convNMF, with or without the x-ortho penalty, to produce ‘parts-based’ or ‘events-based’ factorization. Without this additional control, factorizations may be either ‘parts-based’, or ‘events-based’ depending on initial conditions and the structure of shared neurons activities. This approach works because, in ‘events-based’ factorization, the <inline-formula><mml:math id="inf366"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>’s are orthogonal (uncorrelated) while the <inline-formula><mml:math id="inf367"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>’s have high overlap; conversely, in the ‘parts-based’ factorization, the <inline-formula><mml:math id="inf368"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>’s are orthogonal while the <inline-formula><mml:math id="inf369"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>’s are strongly correlated. Note that these correlations in <inline-formula><mml:math id="inf370"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf371"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> are unavoidable in the presence of shared neurons and such correlations do not indicate a redundant factorization. Update rules to implement penalties on correlations in <inline-formula><mml:math id="inf372"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf373"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> are provided in <xref ref-type="table" rid="table2">Table 2</xref> with derivations in Appendix 1. <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref> shows examples of using these penalties on the songbird dataset described in <xref ref-type="fig" rid="fig9">Figure 9</xref>.</p><p><inline-formula><mml:math id="inf374"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> regularization is a widely used strategy for achieving sparse model parameters (<xref ref-type="bibr" rid="bib72">Zhang et al., 2016</xref>), and has been incorporated into convNMF in the past (<xref ref-type="bibr" rid="bib46">O’Grady and Pearlmutter, 2006</xref>; <xref ref-type="bibr" rid="bib53">Ramanarayanan et al., 2013</xref>). In some of our datasets, we found it useful to include <inline-formula><mml:math id="inf375"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> regularization for sparsity. The multiplicative update rules in the presence of <inline-formula><mml:math id="inf376"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> regularization are included in <xref ref-type="table" rid="table2">Table 2</xref>, and as part of our code package. Sparsity on the matrices <inline-formula><mml:math id="inf377"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf378"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> may be particularly useful in cases when sequences are repeated rhythmically (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref>). For example, the addition of a sparsity regularizer on the <inline-formula><mml:math id="inf379"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> update will bias the <inline-formula><mml:math id="inf380"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> exemplars to include only a single repetition of the repeated sequence, while the addition of a sparsity regularizer on <inline-formula><mml:math id="inf381"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> will bias the <inline-formula><mml:math id="inf382"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> exemplars to include multiple repetitions of the repeated sequence. Like the ambiguities described above, these are both valid interpretations of the data, but each may be more useful in different contexts.</p></sec><sec id="s2-6"><title>Quantifying the prevalence of sequential structure in a dataset</title><p>While sequences may be found in a variety of neural datasets, their importance and prevalence is still a matter of debate and investigation. To address this, we developed a metric to assess how much of the explanatory power of a seqNMF factorization was due to synchronous vs. asynchronous neural firing events. Since convNMF can fit both synchronous and sequential events in a dataset, reconstruction error is not, by itself, diagnostic of the ‘sequenciness’ of neural activity. Our approach is guided by the observation that in a data matrix with only synchronous temporal structure (i.e. patterns of rank 1), the columns can be permuted without sacrificing convNMF reconstruction error. In contrast, permuting the columns eliminates the ability of convNMF to model data that contains sparse temporal sequences (i.e. high rank patterns) but no synchronous structure. We thus compute a ‘sequenciness’ metric, ranging from 0 to 1, that compares the performance of convNMF on column-shuffled versus non-shuffled data matrices (see Materials and methods), and quantify the performance of this metric in simulated datasets containing synchronous and sequential events with varying prevalence (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). We found that this metric varies approximately linearly with the degree to which sequences are present in a dataset. Below, we apply this method to real experimental data and obtain high ‘sequenciness’ scores, suggesting that convolutional matrix factorization is a well-suited tool for summarizing neural dynamics in these datasets.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.024</object-id><label>Figure 7.</label><caption><title>Using seqNMF to assess the prevalence of sequences in noisy data.</title><p>(<bold>A</bold>) Example simulated datasets. Each dataset contains 10 neurons, with varying amounts of additive noise, and varying proportions of synchronous events versus asynchronous sequences. For the purposes of this figure, 'sequence' refers to a sequential pattern with no synchrony between different neurons in the pattern. The duration of each dataset used below is 3000 times, and here 300 timebins are shown. (<bold>B</bold>) Median percent power explained by convNMF (L = 12; K = 2; <inline-formula><mml:math id="inf383"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>=0) for each type of dataset (100 examples of each dataset type). Different colors indicate the three different levels of additive noise shown in A. Solid lines and filled circles indicate results on unshuffled datasets. Note that performance is flat for each noise level, regardless of the probability of sequences vs synchronous events. Dotted lines and open circles indicate results on column-shuffled datasets. When no sequences are present, convNMF performs the same on column-shuffled data. However, when sequences are present, convNMF performs worse on column-shuffled data. (<bold>C</bold>) For datasets with patterns ranging from exclusively synchronous events to exclusively asynchronous sequences, convNMF was used to generate a ‘Sequenciness’ score. Colors correspond to different noise levels shown in A. Asterisks denote cases where the power explained exceeds the Bonferroni-corrected significance threshold generated from column-shuffled datasets. Open circles denote cases that do not achieve significance. Note that this significance test is fairly sensitive, detecting even relatively low presence of sequences, and that the ‘Sequenciness’ score distinguishes between cases where more or less of the dataset consists of sequences.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig7-v2.tif"/></fig></sec><sec id="s2-7"><title>Application of seqNMF to hippocampal sequences</title><p>To test the ability of seqNMF to discover patterns in electrophysiological data, we analyzed multielectrode recordings from rat hippocampus (<ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/hc">https://crcns.org/data-sets/hc</ext-link>), which were previously shown to contain sequential patterns of neural firing (<xref ref-type="bibr" rid="bib48">Pastalkova et al., 2015</xref>). Specifically, rats were trained to alternate between left and right turns in a T-maze to earn a water reward. Between alternations, the rats ran on a running wheel during an imposed delay period lasting either 10 or 20 seconds. By averaging spiking activity during the delay period, the authors reported long temporal sequences of neural activity spanning the delay. In some rats, the same sequence occurred on left and right trials, while in other rats, different sequences were active in the delay period during each trial types.</p><p>Without reference to the behavioral landmarks, seqNMF was able to extract sequences in both datasets. In Rat 1, seqNMF extracted a single factor, corresponding to a sequence active throughout the running wheel delay period and immediately after, when the rat ran up the stem of the maze (<xref ref-type="fig" rid="fig8">Figure 8A</xref>); for 10 fits of K ranging from 1 to 10, the average <italic>diss</italic> metric reached a minimum at 1 and with <inline-formula><mml:math id="inf384"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, most runs using the x-ortho penalty extracted a single significant factor (<xref ref-type="fig" rid="fig8">Figure 8C–E</xref>). Factorizations of thes data with one factor captured 40.8% of the power in the dataset on average, and had a ‘sequenciness’ score of 0.49. Some runs using the x-ortho penalty extracted two factors (<xref ref-type="fig" rid="fig8">Figure 8E</xref>), splitting the delay period sequence and the maze stem sequence; this is a reasonable interpretation of the data, and likely results from variability in the relative timing of running wheel and maze stem traversal. At somewhat lower values of <inline-formula><mml:math id="inf385"><mml:mi>λ</mml:mi></mml:math></inline-formula>, factorizations more often split these sequences into two factors. At even lower values of <inline-formula><mml:math id="inf386"><mml:mi>λ</mml:mi></mml:math></inline-formula>, factorizations had even more significant factors. Such higher granularity factorizations may correspond to real variants of the sequences, as they generalize to held-out data or may reflect time warping in the data (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2J</xref>). However, a single sequence may be a better description of the data because the <italic>diss</italic> metric displayed a clear minimum at <inline-formula><mml:math id="inf387"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). In Rat 2, seqNMF typically identified three factors (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). The first two correspond to distinct sequences active for the duration of the delay period on alternating left and right trials. A third sequence was active immediately following each of the alternating sequences, corresponding to the time at which the animal exits the wheel and runs up the stem of the maze. For 10 fits of K ranging from 1 to 10, the average <italic>diss</italic> metric reached a minimum at three and with <inline-formula><mml:math id="inf388"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, most runs with the x-ortho penalty extracted between 2 and 4 factors (<xref ref-type="fig" rid="fig8">Figure 8F–H</xref>). Factorizations of these data with three factors captured 52.6% of the power in the dataset on average, and had a pattern ‘sequenciness’ score of 0.85. Taken together, these results suggest that seqNMF can detect multiple neural sequences without the use of behavioral landmarks.</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.025</object-id><label>Figure 8.</label><caption><title>Application of seqNMF to extract hippocampal sequences from two rats.</title><p>(<bold>A</bold>) Firing rates of 110 neurons recorded in the hippocampus of Rat 1 during an alternating left-right task with a delay period (<xref ref-type="bibr" rid="bib48">Pastalkova et al., 2015</xref>). The single significant extracted x-ortho penalized factor. Both an x-ortho penalized reconstruction of each factor (left) and raw data (right) are shown. Neurons are sorted according to the latency of their peak activation within the factor. The red line shows the onset and offset of the forced delay periods, during which the animal ran on a treadmill. (<bold>B</bold>) Firing rates of 43 hippocampal neurons recorded in Rat 2 during the same task (<xref ref-type="bibr" rid="bib41">Mizuseki et al., 2013</xref>). Neurons are sorted according to the latency of their peak activation within each of the three significant extracted sequences. The first two factors correspond to left and right trials, and the third corresponds to running along the stem of the maze. (<bold>C</bold>) The <italic>diss</italic> metric as a function of K for Rat 1. Black line represents the median of the black points. Notice the minimum at K = 1. (<bold>D</bold>) (Left) Reconstruction (red) and correlation (blue) costs as a function of <inline-formula><mml:math id="inf389"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula> for Rat 1. Arrow indicates <inline-formula><mml:math id="inf390"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, used for the x-ortho penalized factorization shown in (<bold>A</bold>). (<bold>E</bold>) Histogram of the number of significant factors across 30 runs of x-ortho penalized convNMF. (<bold>D</bold>) The <italic>diss</italic> metric as a function of K for Rat 2. Notice the minimum at K = 3. (<bold>G–H</bold>) Same as in (<bold>D–E</bold>) but for Rat 2. Arrow indicates <inline-formula><mml:math id="inf391"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, used for the factorization shown in (<bold>B</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig8-v2.tif"/></fig></sec><sec id="s2-8"><title>Application of seqNMF to abnormal sequence development in avian motor cortex</title><p>We applied seqNMF methods to analyze functional calcium imaging data recorded in the songbird premotor cortical nucleus HVC during singing. Normal adult birds sing a highly stereotyped song, making it possible to detect sequences by averaging neural activity aligned to the song. Using this approach, it has been shown that HVC neurons generate precisely timed sequences that tile each song syllable (<xref ref-type="bibr" rid="bib21">Hahnloser et al., 2002</xref>; <xref ref-type="bibr" rid="bib51">Picardo et al., 2016</xref>; <xref ref-type="bibr" rid="bib36">Lynch et al., 2016</xref>). Songbirds learn their song by imitation and must hear a tutor to develop normal adult vocalizations. Birds isolated from a tutor sing highly variable and abnormal songs as adults (<xref ref-type="bibr" rid="bib15">Fehér et al., 2009</xref>). Such ‘isolate’ birds provide an opportunity to study how the absence of normal auditory experience leads to pathological vocal/motor development. However, the high variability of pathological ‘isolate’ song makes it difficult to identify neural sequences using the standard approach of aligning neural activity to vocal output.</p><p>Using seqNMF, we were able to identify repeating neural sequences in isolate songbirds (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). At the chosen <inline-formula><mml:math id="inf392"><mml:mi>λ</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig9">Figure 9B</xref>), x-ortho penalized factorizations typically extracted three significant sequences (<xref ref-type="fig" rid="fig9">Figure 9C</xref>). Similarly, the <italic>diss</italic> measure has a local minimum at <inline-formula><mml:math id="inf393"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1B</xref>). The three-sequence factorization explained 41% of the total power in the dataset, with a sequenciness score of 0.7 andhe extracted sequences included sequences deployed during syllables of abnormally long and variable durations (<xref ref-type="fig" rid="fig9">Figure 9D–F</xref>, <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1A</xref>).</p><fig-group><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.026</object-id><label>Figure 9.</label><caption><title>SeqNMF applied to calcium imaging data from a singing isolate bird reveals abnormal sequence deployment.</title><p>(<bold>A</bold>) Functional calcium signals recorded from 75 neurons, unsorted, in a singing isolate bird. (<bold>B</bold>) Reconstruction and cross-orthogonality cost as a function of <inline-formula><mml:math id="inf394"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>. The arrow at <inline-formula><mml:math id="inf395"><mml:mrow><mml:mi mathsize="125%">λ</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.005</mml:mn></mml:mrow></mml:math></inline-formula> indicates the value selected for the rest of the analysis. (<bold>C</bold>) Number of significant factors for 100 runs with the x-ortho penalty with <inline-formula><mml:math id="inf396"><mml:mrow><mml:mi mathsize="125%">K</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">10</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf397"><mml:mrow><mml:mi mathsize="125%">λ</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.005</mml:mn></mml:mrow></mml:math></inline-formula>. Arrow indicates three is the most common number of significant factors. (<bold>D</bold>) X-ortho factor exemplars (<inline-formula><mml:math id="inf398"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>’s). Neurons are grouped according to the factor in which they have peak activation, and within each group neurons are sorted by the latency of their peak activation within the factor. (<bold>E</bold>) The same data shown in (<bold>A</bold>), after sorting neurons by their latency within each factor as in (<bold>D</bold>). A spectrogram of the bird’s song is shown at top, with a purple ‘*’ denoting syllable variants correlated with <inline-formula><mml:math id="inf399"><mml:msub><mml:mi mathsize="125%" mathvariant="bold">𝐰</mml:mi><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:msub></mml:math></inline-formula>. (<bold>F</bold>) Same as (<bold>E</bold>), but showing reconstructed data rather than calcium signals. Shown at top are the temporal loadings (<inline-formula><mml:math id="inf400"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>) of each factor.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig9-v2.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.027</object-id><label>Figure 9—figure supplement 1.</label><caption><title>Further analysis of sequences.</title><p>(<bold>A</bold>) For each of the three extracted sequences, examples of song spectrograms triggered at moments where there is a peak in H. Different examples are separated by a red line. Note that each sequence factor corresponds to a particular syllable type. (<bold>B</bold>) As a function of <inline-formula><mml:math id="inf401"><mml:mi mathsize="125%">K</mml:mi></mml:math></inline-formula>, <italic>diss</italic> measure across all combinations of 10 fits of convNMF. Note the local minima at K = 3. (<bold>C</bold>) Percent power explained (for convNMF with K = 3 and <inline-formula><mml:math id="inf402"><mml:mrow><mml:mi mathsize="125%">λ</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0</mml:mn></mml:mrow></mml:math></inline-formula>) as a function of L. Note the bend that truncates at approximately 0.25 s, corresponding to a typical syllable duration.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig9-figsupp1-v2.tif"/></fig><fig id="fig9s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38471.028</object-id><label>Figure 9—figure supplement 2.</label><caption><title>Events-based and parts-based factorizations of songbird data.</title><p>Illustration of a trade-off between parts-based (<inline-formula><mml:math id="inf403"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> is more strictly orthogonal) and events-based (<inline-formula><mml:math id="inf404"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> is more strictly orthogonal) factorizations in a dataset where some neurons are shared between different sequences. The same data as in <xref ref-type="fig" rid="fig9">Figure 9</xref> is factorized with an orthogonality cost just on <inline-formula><mml:math id="inf405"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> (A, events-based), or just on <inline-formula><mml:math id="inf406"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> (B, parts-based). Below each motivating cartoon factorization, we show x-ortho penalized convNMF fits (<inline-formula><mml:math id="inf407"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf408"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> together with the reconstruction) of the data in <xref ref-type="fig" rid="fig9">Figure 9</xref>. The right panels contain the raw data sorted according to these factorizations. Favoring events-based or parts-based factorizations is a matter of preference. Parts-based factorizations are particularly useful for separating neurons into ensembles. Events-based factorizations are particularly useful for identifying what neural events occur when.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig9-figsupp2-v2.tif"/></fig></fig-group><p>In addition, the extracted sequences exhibit properties not observed in normal adult birds. We see an example of two distinct sequences that sometimes, but not always, co-occur (<xref ref-type="fig" rid="fig9">Figure 9</xref>). We observe that a shorter sequence (green) occurs alone on some syllable renditions while a second, longer sequence (purple) occurs simultaneously on other syllable renditions. We found that biasing x-ortho penalized convNMF towards ’parts-based’ or ’events-based’ factorizations gives a useful tool to visualize this feature of the data (<xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>). This probabilistic overlap of different sequences is highly atypical in normal adult birds (<xref ref-type="bibr" rid="bib21">Hahnloser et al., 2002</xref>; <xref ref-type="bibr" rid="bib35">Long et al., 2010</xref>; <xref ref-type="bibr" rid="bib51">Picardo et al., 2016</xref>; <xref ref-type="bibr" rid="bib36">Lynch et al., 2016</xref>) and is associated with abnormal variations in syllable structure—in this case resulting in a longer variant of the syllable when both sequences co-occur. This acoustic variation is a characteristic pathology of isolate song (<xref ref-type="bibr" rid="bib15">Fehér et al., 2009</xref>).</p><p>Thus, even though we observe HVC generating sequences in the absence of a tutor, it appears that these sequences are deployed in a highly abnormal fashion.</p></sec><sec id="s2-9"><title>Application of seqNMF to a behavioral dataset: song spectrograms</title><p>Although we have focused on the application of seqNMF to neural activity data, these methods naturally extend to other types of high-dimensional datasets, including behavioral data with applications to neuroscience. The neural mechanisms underlying song production and learning in songbirds is an area of active research. However, the identification and labeling of song syllables in acoustic recordings is challenging, particularly in young birds in which song syllables are highly variable. Because automatic segmentation and clustering often fail, song syllables are still routinely labelled by hand (<xref ref-type="bibr" rid="bib45">Okubo et al., 2015</xref>). We tested whether seqNMF, applied to a spectrographic representation of zebra finch vocalizations, is able to extract meaningful features in behavioral data. Using the x-ortho penalty, factorizations correctly identified repeated acoustic patterns in juvenile songs, placing each distinct syllable type into a different factor (<xref ref-type="fig" rid="fig10">Figure 10</xref>). The resulting classifications agree with previously published hand-labeled syllable types (<xref ref-type="bibr" rid="bib45">Okubo et al., 2015</xref>). A similar approach could be applied to other behavioral data, for example movement data or human speech, and could facilitate the study of neural mechanisms underlying even earlier and more variable stages of learning. Indeed, convNMF was originally developed for application to spectrograms (<xref ref-type="bibr" rid="bib58">Smaragdis, 2004</xref>); notably it has been suggested that auditory cortex may use similar computations to represent and parse natural statistics (<xref ref-type="bibr" rid="bib42">Młynarski and McDermott, 2018</xref>).</p><fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.029</object-id><label>Figure 10.</label><caption><title>SeqNMF applied to song spectrograms.</title><p>(<bold>A</bold>) Spectrogram of juvenile song, with hand-labeled syllable types (<xref ref-type="bibr" rid="bib45">Okubo et al., 2015</xref>). (<bold>B</bold>) Reconstruction cost and x-ortho cost for these data as a function of <inline-formula><mml:math id="inf409"><mml:mi mathsize="125%">λ</mml:mi></mml:math></inline-formula>. Arrow denotes <inline-formula><mml:math id="inf410"><mml:mrow><mml:mi mathsize="125%">λ</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.0003</mml:mn></mml:mrow></mml:math></inline-formula>, which was used to run convNMF with the x-ortho penalty (<bold>C</bold>) <inline-formula><mml:math id="inf411"><mml:mi mathsize="125%" mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>’s for this song, fit with <inline-formula><mml:math id="inf412"><mml:mrow><mml:mi mathsize="125%">K</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">8</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf413"><mml:mrow><mml:mi mathsize="125%">L</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">200</mml:mn><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%">m</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">⁢</mml:mo><mml:mi mathsize="125%">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf414"><mml:mrow><mml:mi mathsize="125%">λ</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.0003</mml:mn></mml:mrow></mml:math></inline-formula>. Note that there are three non-empty factors, corresponding to the three hand-labeled syllables a, b, and c. (<bold>D</bold>) X-ortho penalized <inline-formula><mml:math id="inf415"><mml:mi mathsize="125%" mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>’s (for the three non-empty factors) and reconstruction of the song shown in (<bold>A</bold>) using these factors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-fig10-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>As neuroscientists strive to record larger datasets, there is a need for rigorous tools to reveal underlying structure in high-dimensional data (<xref ref-type="bibr" rid="bib17">Gao and Ganguli, 2015</xref>; <xref ref-type="bibr" rid="bib57">Sejnowski et al., 2014</xref>; <xref ref-type="bibr" rid="bib10">Churchland and Abbott, 2016</xref>; <xref ref-type="bibr" rid="bib6">Bzdok and Yeo, 2017</xref>). In particular, sequential structure is increasingly regarded as a fundamental property of neuronal circuits (<xref ref-type="bibr" rid="bib21">Hahnloser et al., 2002</xref>; <xref ref-type="bibr" rid="bib22">Harvey et al., 2012</xref>; <xref ref-type="bibr" rid="bib45">Okubo et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Pastalkova et al., 2008</xref>), but standardized statistical approaches for extracting such structure have not been widely adopted or agreed upon. Extracting sequences is particularly challenging when animal behaviors are variable (e.g. during learning) or absent entirely (e.g. during sleep).</p><p>Here, we explored a simple matrix factorization-based approach to identify neural sequences without reference to animal behavior. The convNMF model elegantly captures sequential structure in an unsupervised manner (<xref ref-type="bibr" rid="bib58">Smaragdis, 2004</xref>; <xref ref-type="bibr" rid="bib59">Smaragdis, 2007</xref>; <xref ref-type="bibr" rid="bib50">Peter et al., 2017</xref>). However, in datasets where the number of sequences is not known, convNMF may return inefficient and inconsistent factorizations. To address these challenges, we introduced a new regularization term to penalize correlated factorizations, and developed a new dissimilarity measure to assess model stability. Both proposed methods can be used to infer the number of sequences in neural data and are highly robust to noise. For example, even when (synthetic) neurons participate probabilistically in sequences at a rate of 50%, the model typically identifies factors with greater than 80% similarity to the ground truth (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Additionally, these methods perform well even with very limited amounts of data: for example successfully extracting sequences that only appear a handful of times in a noisy data stream (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p><p>The x-ortho penalty developed in this paper may represent a useful improvement over traditional orthogonality regularizations or suggest how traditional regularization penalties may be usefully modified. First, it simultaneously provides a penalty on correlations in both <inline-formula><mml:math id="inf416"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf417"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>, thus simplifying analyses by having only one penalty term. Second, although the x-ortho penalty does not formally constitute regularization due to its inclusion of the data <inline-formula><mml:math id="inf418"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>, we have described how the penalty can be approximated by a data-free regularization with potentially useful properties (Appendix 2). Specifically, the data-free regularization contains terms corresponding to weighted orthogonality in (smoothed) <inline-formula><mml:math id="inf419"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf420"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>, where the weights focus the orthogonality penalty preferentially on those factors that contribute the most power to the reconstruction. This concept of using power-weighted regularization penalties may be applicable more generally to matrix factorization techniques.</p><p>As in many data analysis scenarios, a variety of statistical approaches may be brought to bear on finding sequences in neural data. A classic method is to construct cross-correlogram plots, showing spike time correlations between pairs of neurons at various time lags. However, other forms of spike rate covariation, such as trial-to-trial gain modulation, can produce spurious peaks in this measure (<xref ref-type="bibr" rid="bib4">Brody, 1999</xref>); recent work has developed statistical corrections for these effects (<xref ref-type="bibr" rid="bib54">Russo and Durstewitz, 2017</xref>). After significant pairwise correlations are identified, one can heuristically piece together pairs of neurons with significant interactions into a sequence. This bottom-up approach may be better than seqNMF at detecting sequences involving small numbers of neurons, since seqNMF specifically targets sequences that explain large amounts of variance in the data. On the other hand, bottom-up approaches to sequence extraction may fail to identify long sequences with high participation noise or jitter in each neuron (<xref ref-type="bibr" rid="bib52">Quaglio et al., 2018</xref>). One can think of seqNMF as a complementary top-down approach, which performs very well in the high-noise regime since it learns a template sequence at the level of the full population that is robust to noise at the level of individual units.</p><p>Statistical models with a dynamical component, such as Hidden Markov Models (HMMs) (<xref ref-type="bibr" rid="bib37">Maboudi et al., 2018</xref>), linear dynamical systems (<xref ref-type="bibr" rid="bib28">Kao et al., 2015</xref>), and models with switching dynamics (<xref ref-type="bibr" rid="bib34">Linderman et al., 2017</xref>), can also capture sequential firing patterns. These methods will typically require many hidden states or latent dimensions to capture sequences, similar to PCA and NMF which require many components to recover sequences. Nevertheless, visualizing the transition matrix of an HMM can provide insight into the order in which hidden states of the model are visited, mapping onto different sequences that manifest in population activity (<xref ref-type="bibr" rid="bib37">Maboudi et al., 2018</xref>). One advantage of this approach is that it can model sequences that occasionally end prematurely, while convNMF will always reconstruct the full sequence. On the other hand, this pattern completion property makes convNMF robust to participation noise and jitter. In contrast, a standard HMM must pass through each hidden state to model a sequence, and therefore may have trouble if many of these states are skipped. Thus, we expect HMMs and related models to exhibit complementary strengths and weaknesses when compared to convNMF.</p><p>Another strength of convNMF is its ability to accommodate sequences with shared neurons, as has been observed during song learning (<xref ref-type="bibr" rid="bib45">Okubo et al., 2015</xref>). Sequences with shared neurons can be interpreted either in terms of ‘parts-based’ or ‘events-based’ factorizations (<xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>). This capacity for a combinatorial description of overlapping sequences distinguishes convNMF from many other methods, which assume that neural patterns/sequences do not co-occur in time. For example, a vanilla HMM can only model each time step with a single hidden state and thus cannot express parts-based representations of neural sequences. Likewise, simple clustering models would assign each time interval to a single cluster label. Adding hierarchical and factorial structure to these models could allow them to test for overlapping neural sequences (see e.g. <xref ref-type="bibr" rid="bib19">Ghahramani and Jordan, 1997</xref>); however, we believe seqNMF provides a simpler and more direct framework to explore this possibility.</p><p>Finally, as demonstrated by our development of new regularization terms and stability measures, convolutional matrix factorization is a flexible and extensible framework for sequence extraction. For example, one can tune the overall sparsity in the model by introducing additional L1 regularization terms. The loss function may also be modified, for example substituting in KL divergence or more general <inline-formula><mml:math id="inf421"><mml:mi>β</mml:mi></mml:math></inline-formula>-divergence (<xref ref-type="bibr" rid="bib67">Villasana et al., 2018</xref>). Both L1 regularization and <inline-formula><mml:math id="inf422"><mml:mi>β</mml:mi></mml:math></inline-formula>-divergence losses are included in the seqNMF code package so that the model can be tuned to the particular needs of future analyses. Future development could incorporate outlier detection into the objective function (<xref ref-type="bibr" rid="bib44">Netrapalli et al., 2014</xref>), or online optimization methods for large datasets (<xref ref-type="bibr" rid="bib69">Wang et al., 2013</xref>). Other extensions to NMF, for example, Union of Intersections NMF Cluster (<xref ref-type="bibr" rid="bib63">Ubaru et al., 2017</xref>), have yielded increased robustness and consistency of NMF factorizations, and could potentially also be modified for application to convNMF. Thus, adding convolutional structure to factorization-based models of neural data represents a rich opportunity for statistical neuroscience.</p><p>Despite limiting ourselves to a relatively simple model for the purposes of this paper, we extracted biological insights that would have been difficult to otherwise achieve. For example, we identified neural sequences in isolated songbirds without aligning to song syllables, enabling new research into songbird learning on two fronts. First, since isolated and juvenile birds sing highly variable songs that are not easily segmented into stereotyped syllables, it is difficult and highly subjective to identify sequences by aligning to human-labeled syllables. SeqNMF enables the discovery and future characterization of neural sequences in these cases. Second, while behaviorally aligned sequences exist in tutored birds, it is that possible many neural sequences—for example, in different brain areas or stages of development—are not closely locked to song syllables. Thus, even in cases where stereotyped song syllables exist, behavioral alignment may overlook relevant sequences and structure in the data. These lessons apply broadly to many neural systems, and demonstrate the importance of general-purpose methods that extract sequences without reference to behavior. Our results show that convolutional matrix factorization models are an attractive framework to meet this need.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th>Reagent type <break/>(species) or resource</th><th>Designation</th><th>Source or <break/>reference</th><th>Identifiers</th><th>Additional <break/>information</th></tr></thead><tbody><tr><td>Software, <break/>algorithm</td><td>seqNMF</td><td>this paper</td><td><ext-link ext-link-type="uri" xlink:href="https://github.com/FeeLab/seqNMF">https://github.com/FeeLab/seqNMF</ext-link></td><td>start with demo.m</td></tr><tr><td>Software, <break/>algorithm</td><td>convNMF</td><td><xref ref-type="bibr" rid="bib58">Smaragdis, 2004</xref>; <break/><xref ref-type="bibr" rid="bib59">Smaragdis, 2007</xref></td><td><ext-link ext-link-type="uri" xlink:href="https://github.com/colinvaz/nmf-toolbox">https://github.com/colinvaz/nmf-toolbox</ext-link></td><td/></tr><tr><td>Software, <break/>algorithm</td><td>sparse convNMF</td><td><xref ref-type="bibr" rid="bib46">O’Grady and Pearlmutter, 2006</xref>; <break/><xref ref-type="bibr" rid="bib53">Ramanarayanan et al., 2013</xref></td><td><ext-link ext-link-type="uri" xlink:href="https://github.com/colinvaz/nmf-toolbox">https://github.com/colinvaz/nmf-toolbox</ext-link></td><td/></tr><tr><td>Software, <break/>algorithm</td><td>NMF orthogonality penalties</td><td><xref ref-type="bibr" rid="bib9">Choi, 2008</xref>; <break/><xref ref-type="bibr" rid="bib8">Chen and Cichocki, 2004</xref></td><td/><td/></tr><tr><td>Software, <break/>algorithm</td><td>other NMF extensions</td><td><xref ref-type="bibr" rid="bib11">Cichocki et al., 2009</xref></td><td/><td/></tr><tr><td>Software, <break/>algorithm</td><td>NMF</td><td><xref ref-type="bibr" rid="bib32">Lee and Seung, 1999</xref></td><td/><td/></tr><tr><td>Software, <break/>algorithm</td><td>CNMF_E (cell extraction)</td><td><xref ref-type="bibr" rid="bib73">Zhou et al., 2018</xref></td><td><ext-link ext-link-type="uri" xlink:href="https://github.com/zhoupc/CNMF_E">https://github.com/zhoupc/CNMF_E</ext-link></td><td/></tr><tr><td>Software, <break/>algorithm</td><td>MATLAB</td><td>MathWorks</td><td><ext-link ext-link-type="uri" xlink:href="http://www.mathworks.com">www.mathworks.com</ext-link>, <break/>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001622">SCR_001622</ext-link></td><td/></tr><tr><td>Strain, strain <break/>background <break/>(<italic>adeno-associated virus</italic>)</td><td>AAV9.CAG.GCaMP6f. <break/>WPRE.SV40</td><td><xref ref-type="bibr" rid="bib7">Chen et al., 2013</xref></td><td>Addgene viral prep # 100836-AAV9, <break/><ext-link ext-link-type="uri" xlink:href="http://n2t.net/addgene:100836">http://n2t.net/addgene:100836</ext-link>, <break/>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/Addgene_100836">Addgene_100836</ext-link></td><td/></tr><tr><td>Commercial <break/>assay or kit</td><td>Miniature <break/>microscope</td><td>Inscopix</td><td><ext-link ext-link-type="uri" xlink:href="https://www.inscopix.com/nvista">https://www.inscopix.com/nvista</ext-link></td><td/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Contact for resource sharing</title><p>Further requests should be directed to Michale Fee (fee@mit.edu).</p></sec><sec id="s4-2"><title>Software and data availability</title><p>The seqNMF MATLAB code is publicly available as a github repository, which also includes our songbird data (<xref ref-type="fig" rid="fig9">Figure 9</xref>) for demonstration (<xref ref-type="bibr" rid="bib39">Mackevicius et al., 2018</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/seqNMF">https://github.com/elifesciences-publications/seqNMF</ext-link>).</p><p>The repository includes the seqNMF function, as well as helper functions for selecting <inline-formula><mml:math id="inf423"><mml:mi>λ</mml:mi></mml:math></inline-formula>, testing the significance of factors, plotting, and other functions. It also includes a demo script with an example of how to select <inline-formula><mml:math id="inf424"><mml:mi>λ</mml:mi></mml:math></inline-formula> for a new dataset, test for significance of factors, plot the seqNMF factorization, switch between parts-based and events-based factorizations, and calculate cross-validated performance on a masked test set.</p></sec><sec id="s4-3"><title>Generating simulated data</title><p>We simulated neural sequences containing between 1 and 10 distinct neural sequences in the presence of various noise conditions. Each neural sequence was made up of 10 consecutively active neurons, each separated by three timebins. The binary activity matrix was convolved with an exponential kernel (<inline-formula><mml:math id="inf425"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> timebins) to resemble neural calcium imaging activity.</p></sec><sec id="s4-4"><title>SeqNMF algorithm details</title><p>The x-ortho penalized convNMF algorithm is a direct extension of the multiplicative update convNMF algorithm (<xref ref-type="bibr" rid="bib58">Smaragdis, 2004</xref>), and draws on previous work regularizing NMF to encourage factor orthogonality (<xref ref-type="bibr" rid="bib8">Chen and Cichocki, 2004</xref>).</p><p>The uniqueness and consistency of traditional NMF has been better studied than convNMF. In special cases, NMF has a unique solution comprised of sparse, ‘parts-based’ features that can be consistently identified by known algorithms (<xref ref-type="bibr" rid="bib14">Donoho and Stodden, 2004</xref>; <xref ref-type="bibr" rid="bib1">Arora et al., 2011</xref>). However, this ideal scenario does not hold in many practical settings. In these cases, NMF is sensitive to initialization, resulting in potentially inconsistent features. This problem can be addressed by introducing additional constraints or regularization terms that encourage the model to extract particular, e.g. sparse or approximately orthogonal features (<xref ref-type="bibr" rid="bib25">Huang et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Kim and Park, 2008</xref>). Both theoretical work and empirical observations suggest that these modifications result in more consistently identified features (<xref ref-type="bibr" rid="bib61">Theis et al., 2005</xref>; <xref ref-type="bibr" rid="bib30">Kim and Park, 2008</xref>).</p><p>For x-ortho penalized seqNMF, we added to the convNMF cost function a term that promotes competition between overlapping factors, resulting in the following cost function:<disp-formula id="equ10"><label>(8)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We derived the following multiplicative update rules for <inline-formula><mml:math id="inf426"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf427"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> (Appendix 1):<disp-formula id="equ11"><label>(9)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo></mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>λ</mml:mi><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:mover><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo mathvariant="bold">−</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ12"><label>(10)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where the division and <inline-formula><mml:math id="inf428"><mml:mo>×</mml:mo></mml:math></inline-formula> are element-wise. The operator <inline-formula><mml:math id="inf429"><mml:mover accent="true"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>→</mml:mo><mml:mi/></mml:mrow></mml:mover></mml:math></inline-formula> shifts a matrix in the <inline-formula><mml:math id="inf430"><mml:mo>→</mml:mo></mml:math></inline-formula> direction by <inline-formula><mml:math id="inf431"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> timebins, that is a delay by <inline-formula><mml:math id="inf432"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> timebins, and <inline-formula><mml:math id="inf433"><mml:mover accent="true"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi/><mml:mo>←</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mover></mml:math></inline-formula> shifts a matrix in the <inline-formula><mml:math id="inf434"><mml:mo>←</mml:mo></mml:math></inline-formula> direction by <inline-formula><mml:math id="inf435"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> timebins (notation summary, <xref ref-type="table" rid="table1">Table 1</xref>). Note that multiplication with the <inline-formula><mml:math id="inf436"><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf437"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>𝟏</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐈</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> effectively implements factor competition because it places in the <inline-formula><mml:math id="inf438"><mml:mi>k</mml:mi></mml:math></inline-formula>th row a sum across all other factors. These update rules are derived in Appendix 1 by taking the derivative of the cost function in <xref ref-type="disp-formula" rid="equ10">Equation 8</xref> and choosing an appropriate learning rate for each element.</p><p>In addition to the multiplicative updates outlined in <xref ref-type="table" rid="table2">Table 2</xref>, we also renormalize so rows of <inline-formula><mml:math id="inf439"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> have unit norm; shift factors to be centered in time such that the center of mass of each <inline-formula><mml:math id="inf440"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> pattern occurs in the middle; and in the final iteration run one additional step of unregularized convNMF to prioritize the cost of reconstruction error over the regularization (Algorithm 1). This final step is done to correct a minor suppression in the amplitude of some peaks in <inline-formula><mml:math id="inf441"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> that may occur within <inline-formula><mml:math id="inf442"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> timebins of neighboring sequences.</p><sec id="s4-4-1"><title>Testing the significance of each factor on held-out data</title><p>In order to test whether a factor is significantly present in held-out data, we measured the distribution across timebins of the overlaps of the factor with the held-out data, and compared the skewness of this distribution to the null case (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Overlap with the data is measured as <inline-formula><mml:math id="inf443"><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula>, a quantity which will be high at timepoints when the sequence occurs, producing a distribution of <inline-formula><mml:math id="inf444"><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:math></inline-formula> with high skew. In contrast, a distribution of overlaps exhibiting low skew indicates a sequence is not present in the data, since there are few timepoints of particularly high overlap. We estimated what skew levels would appear by chance by constructing null factors where temporal relationships between neurons have been eliminated. To create such null factors, we start from the real factors then circularly shift the timecourse of each neuron by a random amount between 0 and <inline-formula><mml:math id="inf445"><mml:mi>L</mml:mi></mml:math></inline-formula>. We measure the skew of the overlap distributions for each null factor, and ask whether the skew we measured for the real factor is significant at p-value <inline-formula><mml:math id="inf446"><mml:mi>α</mml:mi></mml:math></inline-formula>, that is, if it exceeds the Bonferroni corrected <inline-formula><mml:math id="inf447"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mi>α</mml:mi><mml:mi>K</mml:mi></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mn>100</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> percentile of the null skews (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th valign="top"><bold>Algorithm 1</bold>: SeqNMF (x-ortho algorithm)</th></tr></thead><tbody><tr><td valign="top">   <bold> Input</bold>: Data matrix <inline-formula><mml:math id="inf448"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>, number of factors <inline-formula><mml:math id="inf449"><mml:mi>K</mml:mi></mml:math></inline-formula>, factor duration <inline-formula><mml:math id="inf450"><mml:mi>L</mml:mi></mml:math></inline-formula>, regularization <break/>                  strength <inline-formula><mml:math id="inf451"><mml:mi>λ</mml:mi></mml:math></inline-formula><break/>    <bold>Output</bold>: Factor exemplars <inline-formula><mml:math id="inf452"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>, factor timecourses <inline-formula><mml:math id="inf453"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula><break/>1 Initialize <inline-formula><mml:math id="inf454"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf455"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> randomly <break/>2 Iter = 1 <break/>3 <bold>While </bold>(Iter <inline-formula><mml:math id="inf456"><mml:mo>&lt;</mml:mo></mml:math></inline-formula> maxIter) and (<inline-formula><mml:math id="inf457"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula> cost <inline-formula><mml:math id="inf458"><mml:mo>&gt;</mml:mo></mml:math></inline-formula> tolerance) <bold>do</bold><break/>4       Update <inline-formula><mml:math id="inf459"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> using multiplicative update from <xref ref-type="table" rid="table2">Table 2</xref> <break/>5       Shift <inline-formula><mml:math id="inf460"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf461"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> to center <inline-formula><mml:math id="inf462"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>’s in time <break/>6       Renormalize <inline-formula><mml:math id="inf463"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf464"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> so rows of <inline-formula><mml:math id="inf465"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> have unit norm <break/>7       Update <inline-formula><mml:math id="inf466"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> using multiplicative update from <xref ref-type="table" rid="table2">Table 2</xref> <break/>8       Iter = Iter + 1<break/>9 Do one final unregularized convNMF update of <inline-formula><mml:math id="inf467"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf468"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula><break/>10 <bold>return</bold></td></tr></tbody></table></table-wrap><p>Note that if <inline-formula><mml:math id="inf469"><mml:mi>λ</mml:mi></mml:math></inline-formula> is set too small, seqNMF will produce multiple redundant factors to explain one sequence in the data. In this case, each redundant candidate sequence will pass the significance test outlined here. We will address below a procedure for choosing <inline-formula><mml:math id="inf470"><mml:mi>λ</mml:mi></mml:math></inline-formula> and methods for determining the number of sequences.</p></sec><sec id="s4-4-2"><title>Calculating the percent power explained by a factorization</title><p>In assessing the relevance of sequences in a dataset, it can be useful to calculate what percentage of the total power in the dataset is explained by the factorization (<inline-formula><mml:math id="inf471"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula>). The total power in the data is <inline-formula><mml:math id="inf472"><mml:mrow><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> (abbreviating <inline-formula><mml:math id="inf473"><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf474"><mml:mrow><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>). The power unexplained by the factorization is <inline-formula><mml:math id="inf475"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∑</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, the percent of the total power explained by the factorization is:<disp-formula id="equ13"><label>(11)</label><mml:math id="m13"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-4-3"><title>‘Sequenciness’ score</title><p>The ‘sequenciness’ score was developed to distinguish between datasets with exclusively synchronous patterns, and datasets with temporally extended sequential patterns. This score relies on the observation that synchronous patterns are not disrupted by shuffling the columns of the data matrix. The ‘sequenciness’ score is calculated by first computing the difference between the power explained by seqNMF in the actual and column-shuffled data. This quantity is then divided by the power explained in the actual data minus the power explained in data where each neuron is time-shuffled by a different random permutation.</p></sec><sec id="s4-4-4"><title>Choosing appropriate parameters for a new dataset</title><p>The choice of appropriate parameters (<inline-formula><mml:math id="inf476"><mml:mi>λ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf477"><mml:mi>K</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf478"><mml:mi>L</mml:mi></mml:math></inline-formula>) will depend on the data type (sequence length, number, and density; amount of noise; etc.).</p><p>In practice, we found that results were relatively robust to the choice of parameters. When <inline-formula><mml:math id="inf479"><mml:mi>K</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf480"><mml:mi>L</mml:mi></mml:math></inline-formula> is set larger than necessary, seqNMF tends to simply leave the unnecessary factors or times empty. For choosing <inline-formula><mml:math id="inf481"><mml:mi>λ</mml:mi></mml:math></inline-formula>, the goal is to find the ‘sweet spot’ (<xref ref-type="fig" rid="fig4">Figure 4</xref>) to explain as much data as possible while still producing sensible factorizations, that is, minimally correlated factors, with low values of <inline-formula><mml:math id="inf482"><mml:msub><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Our software package includes demo code for determining the best parameters for a new type of data, using the following strategy:</p><list list-type="order"><list-item><p>Start with <inline-formula><mml:math id="inf483"><mml:mi>K</mml:mi></mml:math></inline-formula> slightly larger than the number of sequences anticipated in the data</p></list-item><list-item><p>Start with <inline-formula><mml:math id="inf484"><mml:mi>L</mml:mi></mml:math></inline-formula> slightly longer than the maximum expected factor length</p></list-item><list-item><p>Run seqNMF for a range of <inline-formula><mml:math id="inf485"><mml:mi>λ</mml:mi></mml:math></inline-formula>’s, and for each <inline-formula><mml:math id="inf486"><mml:mi>λ</mml:mi></mml:math></inline-formula> measure the reconstruction error <inline-formula><mml:math id="inf487"><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊛</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and the correlation cost term <inline-formula><mml:math id="inf488"><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Choose a <inline-formula><mml:math id="inf489"><mml:mi>λ</mml:mi></mml:math></inline-formula> slightly above the crossover point <inline-formula><mml:math id="inf490"><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula></p></list-item><list-item><p>Decrease <inline-formula><mml:math id="inf491"><mml:mi>K</mml:mi></mml:math></inline-formula> if desired, as otherwise some factors will be consistently empty</p></list-item><list-item><p>Decrease <inline-formula><mml:math id="inf492"><mml:mi>L</mml:mi></mml:math></inline-formula> if desired, as otherwise some times will consistently be empty</p></list-item></list><p>In some applications, achieving the desired accuracy may depend on choosing a <inline-formula><mml:math id="inf493"><mml:mi>λ</mml:mi></mml:math></inline-formula> that allows some inconsistency. It is possible to deal with this remaining inconsistency by comparing factors produced by different random initializations, and only considering factors that arise from several different initializations, a strategy that has been previously applied to standard convNMF on neural data (<xref ref-type="bibr" rid="bib50">Peter et al., 2017</xref>).</p><p>During validation of our procedure for choosing <inline-formula><mml:math id="inf494"><mml:mi>λ</mml:mi></mml:math></inline-formula>, we compared factorizations to ground truth sequences as shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. To find the optimal value of <inline-formula><mml:math id="inf495"><mml:mi>λ</mml:mi></mml:math></inline-formula>, we used the product of two curves. The first curve was obtained by calculating the fraction of fits in which the true number of sequences was recovered as a function of <inline-formula><mml:math id="inf496"><mml:mi>λ</mml:mi></mml:math></inline-formula>. The second curve was obtained by calculating similarity to ground truth as a function of <inline-formula><mml:math id="inf497"><mml:mi>λ</mml:mi></mml:math></inline-formula> (see Materials and methods section ‘Measuring performance on noisy fits by comparing seqNMF sequence to ground-truth sequences’). The product of these two curves was smoothed using a three-sample boxcar sliding window, and the width was found as the values of <inline-formula><mml:math id="inf498"><mml:mi>λ</mml:mi></mml:math></inline-formula> on either side of the peak value that correspond most closely to the half-maximum points of the curve.</p></sec><sec id="s4-4-5"><title>Preprocessing</title><p>While seqNMF is generally quite robust to noisy data, and different types of sequential patterns, proper preprocessing of the data can be important to obtaining reasonable factorizations on real neural data. A key principle is that, in minimizing the reconstruction error, seqNMF is most strongly influenced by parts of the data that exhibit high variance. This can be problematic if the regions of interest in the data have relatively low amplitude. For example, high firing rate neurons may be prioritized over those with lower firing rate. As an alternative to subtracting the mean firing rate of each neuron, which would introduce negative values, neurons could be normalized divisively or by subtracting off a NMF reconstruction fit using a method that forces a non-negative residual (<xref ref-type="bibr" rid="bib31">Kim and Smaragdis, 2014</xref>). Additionally, variations in behavioral state may lead to seqNMF factorizations that prioritize regions of the data with high variance and neglect other regions. It may be possible to mitigate these effects by normalizing data, or by restricting analysis to particular subsets of the data, either by time or by neuron.</p></sec><sec id="s4-4-6"><title>Measuring performance on noisy data by comparing seqNMF sequences to ground-truth sequences</title><p>We wanted to measure the ability of seqNMF to recover ground-truth sequences even when the sequences are obstructed by noise. Our noisy data consisted of three ground-truth sequences, obstructed by a variety of noise types. For each ground-truth sequence, we found its best match among the seqNMF factors. This was performed in a greedy manner. Specifically, we first computed a reconstruction for one of the ground-truth factors. We then measured the correlation between this reconstruction and reconstructions generated from each of the extracted factors, and chose the best match (highest correlation). Next, we matched a second ground-truth sequence with its best match (highest correlation between reconstructions) among the remaining seqNMF factors, and finally we found the best match for the third ground-truth sequence. The mean of these three correlations was used as a measure of similarity between the seqNMF factorization and the ground-truth (noiseless) sequences.</p></sec><sec id="s4-4-7"><title>Testing generalization of factorization to randomly held-out (masked) data entries</title><p>The data matrix <inline-formula><mml:math id="inf499"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> was divided into training data and test data by randomly selecting 5 or 10% of matrix entries to hold out. Specifically, the objective function (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>, in the Results section) was modified to:<disp-formula id="equ14"><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(12)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:munder></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>⊛</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf500"><mml:mo>×</mml:mo></mml:math></inline-formula> indicates elementwise multiplication (Hadamard product) and <inline-formula><mml:math id="inf501"><mml:mi mathvariant="bold">𝐌</mml:mi></mml:math></inline-formula> is a binary matrix with 5 or 10% of the entries randomly selected to be zero (held-out test set) and the remaining 95 or 90% set to one (training set). To search for a solution, we reformulate this optimization problem as:<disp-formula id="equ15"><label>(13)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>⊛</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where we have introduced a new optimization variable <inline-formula><mml:math id="inf502"><mml:mi mathvariant="bold">𝐙</mml:mi></mml:math></inline-formula>, which can be thought of as a surrogate dataset that is equal to the ground truth data only on the training set. The goal is now to minimize the difference between the model estimate, <inline-formula><mml:math id="inf503"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊛</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and the surrogate, <inline-formula><mml:math id="inf504"><mml:mi mathvariant="bold">𝐙</mml:mi></mml:math></inline-formula>, while constraining <inline-formula><mml:math id="inf505"><mml:mi mathvariant="bold">𝐙</mml:mi></mml:math></inline-formula> to equal <inline-formula><mml:math id="inf506"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> at unmasked elements (where <inline-formula><mml:math id="inf507"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) and allowing <inline-formula><mml:math id="inf508"><mml:mi mathvariant="bold">𝐙</mml:mi></mml:math></inline-formula> to be freely chosen at masked elements (where <inline-formula><mml:math id="inf509"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). Clearly, at masked elements, the best choice is to make <inline-formula><mml:math id="inf510"><mml:mi mathvariant="bold">𝐙</mml:mi></mml:math></inline-formula> equal to the current model estimate <inline-formula><mml:math id="inf511"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> as this minimizes the cost function without violating the constraint. This leads to the following update rules which are applied cyclically to update <inline-formula><mml:math id="inf512"><mml:mi mathvariant="bold">𝐙</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf513"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf514"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>.<disp-formula id="equ16"><label>(14)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>⊛</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ17"><label>(15)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mover><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:mover><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo mathvariant="bold">−</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ18"><label>(16)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The measure used for testing generalization performance was root mean squared error (RMSE). For the testing phase, RMSE was computed from the difference between <inline-formula><mml:math id="inf515"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> and the data matrix <inline-formula><mml:math id="inf516"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> only for held-out entries.</p></sec></sec><sec id="s4-5"><title>Hippocampus data</title><p>The hippocampal data was collected in the Buzsaki lab (<xref ref-type="bibr" rid="bib48">Pastalkova et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Mizuseki et al., 2013</xref>), and is publicly available on the Collaborative Research in Computational Neuroscience (CRCNS) Data sharing website. The dataset we refer to as ‘Rat 1’ is in the <ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/hc/hc-5">hc-5</ext-link> dataset, and the dataset we refer to as ‘Rat 2’ is in the <ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/hc/hc-3">hc-3</ext-link> dataset. Before running seqNMF, we processed the data by convolving the raw spike trains with a gaussian kernel of standard deviation 100 ms.</p></sec><sec id="s4-6"><title>Animal care and use</title><p>We used male zebra finches (<italic>Taeniopygia guttata</italic>) from the MIT zebra finch breeding facility (Cambridge, MA). Animal care and experiments were carried out in accordance with NIH guidelines, and reviewed and approved by the Massachusetts Institute of Technology Committee on Animal Care (protocol 0715-071-18).</p><p>In order to prevent exposure to a tutor song, birds were foster-raised by female birds, which do not sing, starting on or before post-hatch day 15. For experiments, birds were housed singly in custom-made sound isolation chambers.</p></sec><sec id="s4-7"><title>Data acquisition and preprocessing</title><p>The calcium indicator GCaMP6f was expressed in HVC by intracranial injection of the viral vector AAV9.CAG.GCaMP6f.WPRE.SV40 (<xref ref-type="bibr" rid="bib7">Chen et al., 2013</xref>) into HVC. In the same surgery, a cranial window was made using a GRIN (gradient index) lens (1 mm diamenter, 4 mm length, Inscopix). After at least one week, in order to allow for sufficient viral expression, recordings were made using the Inscopix nVista miniature fluorescent microscope.</p><p>Neuronal activity traces were extracted from raw fluorescence movies using the CNMF_E algorithm, a constrained non-negative matrix factorization algorithm specialized for microendoscope data by including a local background model to remove activity from out-of-focus cells (<xref ref-type="bibr" rid="bib73">Zhou et al., 2018</xref>).</p><p>We performed several preprocessing steps before applying seqNMF to functional calcium traces extracted by CNMF_E. First, we estimated burst times from the raw traces by deconvolving the traces using an AR-2 process. The deconvolution parameters (time constants and noise floor) were estimated for each neuron using the CNMF_E code package (<xref ref-type="bibr" rid="bib73">Zhou et al., 2018</xref>). Some neurons exhibited larger peaks than others, likely due to different expression levels of the calcium indicator. Since seqNMF would prioritize the neurons with the most power, we renormalized by dividing the signal from each neuron by the sum of the maximum value of that row and the <inline-formula><mml:math id="inf517"><mml:msup><mml:mn>95</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> percentile of the signal across all neurons. In this way, neurons with larger peaks were given some priority, but not much more than that of neurons with weaker signals.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by a grant from the Simons Collaboration for the Global Brain, the National Institutes of Health (NIH) [grant number R01 DC009183] and the G Harold and Leila Y Mathers Charitable Foundation. ELM received support through the NDSEG Fellowship program. AHB received support through NIH training grant 5T32EB019940-03. MSG received support from the NIH [grant number U19NS104648]. AHW received support from the U.S. Department of Energy Computational Science Graduate Fellowship (CSGF) program. Thanks to Pengcheng Zhou for advice on his CNMF_E calcium data cell extraction algorithm. Thanks to Wiktor Młynarski for helpful convNMF discussions. Thanks to Michael Stetner, Galen Lynch, Nhat Le, Dezhe Jin, Edward Nieh, Adam Charles, Jane Van Velden and Yiheng Wang for comments on the manuscript and on our code package. Thanks to our reviewers for wonderful suggestions, including the use of <italic>diss</italic> to select <inline-formula><mml:math id="inf518"><mml:mi>K</mml:mi></mml:math></inline-formula>, and using seqNMF to measure ‘sequenciness’. Special thanks to the 2017 Methods in Computational Neuroscience course [supported by NIH grant R25 MH062204 and Simons Foundation] at the Woods Hole Marine Biology Lab, where this collaboration was started.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Validation, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation</p></fn><fn fn-type="con" id="con5"><p>Investigation</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Formal analysis, Supervision, Funding acquisition, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Validation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: Animal care and experiments were carried out in accordance with NIH guidelines, and reviewed and approved by the Massachusetts Institute of Technology Committee on Animal Care (protocol 0715-071-18).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.38471.031</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-38471-transrepform-v2.pdf"/></supplementary-material><sec id="s9" sec-type="data-availability"><title>Data availability</title><p>Code and songbird dataset is publicly available on github: <ext-link ext-link-type="uri" xlink:href="https://github.com/FeeLab/seqNMF">https://github.com/FeeLab/seqNMF</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/seqNMF">https://github.com/elifesciences-publications/seqNMF</ext-link>). Rat datasets were collected in the Buzsaki lab, and are publicly available on CRCNS (<ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/hc">https://crcns.org/data-sets/hc</ext-link>); users must first create a free account (<ext-link ext-link-type="uri" xlink:href="https://crcns.org/register">https://crcns.org/register</ext-link>) before they can download the datasets from the site.</p><p>The following previously published datasets were used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Mizuseki</surname><given-names>K</given-names></name><name><surname>Sirota</surname><given-names>A</given-names></name><name><surname>Pastalkova</surname><given-names>E</given-names></name><name><surname>Diba</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><data-title>Multiple single unit recordings from different rat hippocampal and entorhinal regions while the animals were performing multiple behavioral tasks. CRCNS.org.</data-title><source>Collaborative Research in Computational Neuroscience</source><pub-id assigning-authority="other" pub-id-type="doi">10.6080/K09G5JRZ</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Pastalkova</surname><given-names>E</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Simultaneous extracellular recordings from left and right hippocampal areas CA1 and right entorhinal cortex from a rat performing a left / right alternation task and other behaviors. CRCNS.org.</data-title><source>Collaborative Research in Computational Neuroscience</source><pub-id assigning-authority="other" pub-id-type="doi">10.6080/K0KS6PHF</pub-id></element-citation></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Arora</surname> <given-names>S</given-names></name><name><surname>Ge</surname> <given-names>R</given-names></name><name><surname>Kannan</surname> <given-names>R</given-names></name><name><surname>Moitra</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Computing a nonnegative matrix factorization -- provably</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1111.0952">https://arxiv.org/abs/1111.0952</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bapi</surname> <given-names>RS</given-names></name><name><surname>Pammi</surname> <given-names>VSC</given-names></name><name><surname>Miyapuram</surname> <given-names>KP</given-names></name><name><surname>Ahmed</surname></name></person-group><year iso-8601-date="2005">2005</year><article-title>Investigation of sequence processing: a cognitive and computational neuroscience perspective</article-title><source>Current Science</source><volume>89</volume><fpage>1690</fpage><lpage>1698</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bro</surname> <given-names>R</given-names></name><name><surname>Kjeldahl</surname> <given-names>K</given-names></name><name><surname>Smilde</surname> <given-names>AK</given-names></name><name><surname>Kiers</surname> <given-names>HA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cross-validation of component models: a critical look at current methods</article-title><source>Analytical and Bioanalytical Chemistry</source><volume>390</volume><fpage>1241</fpage><lpage>1251</lpage><pub-id pub-id-type="doi">10.1007/s00216-007-1790-1</pub-id><pub-id pub-id-type="pmid">18214448</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Correlations without synchrony</article-title><source>Neural Computation</source><volume>11</volume><fpage>1537</fpage><lpage>1551</lpage><pub-id pub-id-type="doi">10.1162/089976699300016133</pub-id><pub-id pub-id-type="pmid">10490937</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunton</surname> <given-names>BW</given-names></name><name><surname>Johnson</surname> <given-names>LA</given-names></name><name><surname>Ojemann</surname> <given-names>JG</given-names></name><name><surname>Kutz</surname> <given-names>JN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Extracting spatial-temporal coherent patterns in large-scale neural recordings using dynamic mode decomposition</article-title><source>Journal of Neuroscience Methods</source><volume>258</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2015.10.010</pub-id><pub-id pub-id-type="pmid">26529367</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bzdok</surname> <given-names>D</given-names></name><name><surname>Yeo</surname> <given-names>BTT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inference in the age of big data: Future perspectives on neuroscience</article-title><source>NeuroImage</source><volume>155</volume><fpage>549</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.04.061</pub-id><pub-id pub-id-type="pmid">28456584</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>TW</given-names></name><name><surname>Wardill</surname> <given-names>TJ</given-names></name><name><surname>Sun</surname> <given-names>Y</given-names></name><name><surname>Pulver</surname> <given-names>SR</given-names></name><name><surname>Renninger</surname> <given-names>SL</given-names></name><name><surname>Baohan</surname> <given-names>A</given-names></name><name><surname>Schreiter</surname> <given-names>ER</given-names></name><name><surname>Kerr</surname> <given-names>RA</given-names></name><name><surname>Orger</surname> <given-names>MB</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name><name><surname>Looger</surname> <given-names>LL</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Kim</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title><source>Nature</source><volume>499</volume><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1038/nature12354</pub-id><pub-id pub-id-type="pmid">23868258</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Cichocki</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Nonnegative matrix factorization with temporal smoothness and/or spatial decorrelation constraints</article-title><source>Signal Processing</source><volume>11</volume></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Choi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Algorithms for orthogonal nonnegative matrix factorization</article-title><conf-name>IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)</conf-name><fpage>1828</fpage><lpage>1832</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname> <given-names>AK</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Conceptual and technical advances define a key moment for theoretical neuroscience</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>348</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1038/nn.4255</pub-id><pub-id pub-id-type="pmid">26906500</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cichocki</surname> <given-names>A</given-names></name><name><surname>Zdunek</surname> <given-names>R</given-names></name><name><surname>Phan</surname> <given-names>AH</given-names></name><name><surname>Amari S-I</surname></name></person-group><year iso-8601-date="2009">2009</year><source>Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi‐Way Data Analysis and Blind Source Separation</source><publisher-name>Wiley</publisher-name><pub-id pub-id-type="isbn">9780470747278</pub-id><pub-id pub-id-type="doi">10.1002/9780470747278</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clegg</surname> <given-names>BA</given-names></name><name><surname>Digirolamo</surname> <given-names>GJ</given-names></name><name><surname>Keele</surname> <given-names>SW</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Sequence learning</article-title><source>Trends in Cognitive Sciences</source><volume>2</volume><fpage>275</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(98)01202-9</pub-id><pub-id pub-id-type="pmid">21227209</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cui</surname> <given-names>Y</given-names></name><name><surname>Ahmad</surname> <given-names>S</given-names></name><name><surname>Hawkins</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Continuous online sequence learning with an unsupervised neural network model</article-title><source>Neural Computation</source><volume>28</volume><fpage>2474</fpage><lpage>2504</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00893</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Donoho</surname> <given-names>D</given-names></name><name><surname>Stodden</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2004">2004</year><chapter-title>When does non-negative matrix factorization give a correct decomposition into parts?</chapter-title><person-group person-group-type="editor"><name><surname>Thrun</surname> <given-names>S</given-names></name><name><surname>Saul</surname> <given-names>L. K</given-names></name><name><surname>Schölkopf</surname> <given-names>B</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><volume>16</volume><publisher-name>MIT Press</publisher-name><fpage>1141</fpage><lpage>1148</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fehér</surname> <given-names>O</given-names></name><name><surname>Wang</surname> <given-names>H</given-names></name><name><surname>Saar</surname> <given-names>S</given-names></name><name><surname>Mitra</surname> <given-names>PP</given-names></name><name><surname>Tchernichovski</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>De novo establishment of wild-type song culture in the zebra finch</article-title><source>Nature</source><volume>459</volume><fpage>564</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1038/nature07994</pub-id><pub-id pub-id-type="pmid">19412161</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujisawa</surname> <given-names>S</given-names></name><name><surname>Amarasingham</surname> <given-names>A</given-names></name><name><surname>Harrison</surname> <given-names>MT</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Behavior-dependent short-term assembly dynamics in the medial prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>823</fpage><lpage>833</lpage><pub-id pub-id-type="doi">10.1038/nn.2134</pub-id><pub-id pub-id-type="pmid">18516033</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname> <given-names>P</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>On simplicity and complexity in the brave new world of large-scale neuroscience</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>148</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.04.003</pub-id><pub-id pub-id-type="pmid">25932978</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstein</surname> <given-names>GL</given-names></name><name><surname>Williams</surname> <given-names>ER</given-names></name><name><surname>Diesmann</surname> <given-names>M</given-names></name><name><surname>Grün</surname> <given-names>S</given-names></name><name><surname>Trengove</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Detecting synfire chains in parallel spike data</article-title><source>Journal of Neuroscience Methods</source><volume>206</volume><fpage>54</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2012.02.003</pub-id><pub-id pub-id-type="pmid">22361572</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghahramani</surname> <given-names>Z</given-names></name><name><surname>Jordan</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Factorial hidden markov models</article-title><source>Machine Learning</source><volume>29</volume><fpage>245</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1023/A:1007425814087</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossberger</surname> <given-names>L</given-names></name><name><surname>Battaglia</surname> <given-names>FP</given-names></name><name><surname>Vinck</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Unsupervised clustering of temporal patterns in high-dimensional neuronal ensembles using a novel dissimilarity measure</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006283</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006283</pub-id><pub-id pub-id-type="pmid">29979681</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahnloser</surname> <given-names>RH</given-names></name><name><surname>Kozhevnikov</surname> <given-names>AA</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>An ultra-sparse code underlies the generation of neural sequences in a songbird</article-title><source>Nature</source><volume>419</volume><fpage>65</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/nature00974</pub-id><pub-id pub-id-type="pmid">12214232</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Coen</surname> <given-names>P</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title><source>Nature</source><volume>484</volume><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1038/nature10918</pub-id><pub-id pub-id-type="pmid">22419153</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname> <given-names>T</given-names></name><name><surname>Tibshirani</surname> <given-names>R</given-names></name><name><surname>Friedman</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-84858-7</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname> <given-names>J</given-names></name><name><surname>Ahmad</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Why neurons have thousands of synapses, a theory of sequence memory in neocortex</article-title><source>Frontiers in Neural Circuits</source><volume>10</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2016.00023</pub-id><pub-id pub-id-type="pmid">27065813</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>K</given-names></name><name><surname>Sidiropoulos</surname> <given-names>ND</given-names></name><name><surname>Swami</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Non-negative matrix factorization revisited: uniqueness and algorithm for symmetric decomposition</article-title><source>IEEE Transactions on Signal Processing</source><volume>62</volume><fpage>211</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1109/TSP.2013.2285514</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janata</surname> <given-names>P</given-names></name><name><surname>Grafton</surname> <given-names>ST</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Swinging in the brain: shared neural substrates for behaviors related to sequencing and music</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>682</fpage><lpage>687</lpage><pub-id pub-id-type="doi">10.1038/nn1081</pub-id><pub-id pub-id-type="pmid">12830159</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname> <given-names>JJ</given-names></name><name><surname>Steinmetz</surname> <given-names>NA</given-names></name><name><surname>Siegle</surname> <given-names>JH</given-names></name><name><surname>Denman</surname> <given-names>DJ</given-names></name><name><surname>Bauza</surname> <given-names>M</given-names></name><name><surname>Barbarits</surname> <given-names>B</given-names></name><name><surname>Lee</surname> <given-names>AK</given-names></name><name><surname>Anastassiou</surname> <given-names>CA</given-names></name><name><surname>Andrei</surname> <given-names>A</given-names></name><name><surname>Aydın</surname> <given-names>Ç</given-names></name><name><surname>Barbic</surname> <given-names>M</given-names></name><name><surname>Blanche</surname> <given-names>TJ</given-names></name><name><surname>Bonin</surname> <given-names>V</given-names></name><name><surname>Couto</surname> <given-names>J</given-names></name><name><surname>Dutta</surname> <given-names>B</given-names></name><name><surname>Gratiy</surname> <given-names>SL</given-names></name><name><surname>Gutnisky</surname> <given-names>DA</given-names></name><name><surname>Häusser</surname> <given-names>M</given-names></name><name><surname>Karsh</surname> <given-names>B</given-names></name><name><surname>Ledochowitsch</surname> <given-names>P</given-names></name><name><surname>Lopez</surname> <given-names>CM</given-names></name><name><surname>Mitelut</surname> <given-names>C</given-names></name><name><surname>Musa</surname> <given-names>S</given-names></name><name><surname>Okun</surname> <given-names>M</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Putzeys</surname> <given-names>J</given-names></name><name><surname>Rich</surname> <given-names>PD</given-names></name><name><surname>Rossant</surname> <given-names>C</given-names></name><name><surname>Sun</surname> <given-names>WL</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Harris</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kao</surname> <given-names>JC</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Single-trial dynamics of motor cortex and their applications to brain-machine interfaces</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>7759</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms8759</pub-id><pub-id pub-id-type="pmid">26220660</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>TH</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Lecoq</surname> <given-names>J</given-names></name><name><surname>Jung</surname> <given-names>JC</given-names></name><name><surname>Li</surname> <given-names>J</given-names></name><name><surname>Zeng</surname> <given-names>H</given-names></name><name><surname>Niell</surname> <given-names>CM</given-names></name><name><surname>Schnitzer</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Long-term optical access to an estimated one million neurons in the live mouse cortex</article-title><source>Cell Reports</source><volume>17</volume><fpage>3385</fpage><lpage>3394</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2016.12.004</pub-id><pub-id pub-id-type="pmid">28009304</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>J</given-names></name><name><surname>Park</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>Sparse nonnegative matrix factorization for clustering</source><publisher-name>Georgia Institute of Technology. Technical Report GT-CSE</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>M</given-names></name><name><surname>Smaragdis</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Efficient model selection for speech enhancement using a deflation method for nonnegative matrix factorization</article-title><conf-name>2014 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</conf-name><pub-id pub-id-type="doi">10.1109/GlobalSIP.2014.7032175</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>DD</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Learning the parts of objects by non-negative matrix factorization</article-title><source>Nature</source><volume>401</volume><fpage>788</fpage><lpage>791</lpage><pub-id pub-id-type="doi">10.1038/44565</pub-id><pub-id pub-id-type="pmid">10548103</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>DD</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2001">2001</year><chapter-title>Algorithms for non-negative matrix factorization</chapter-title><person-group person-group-type="editor"><name><surname>Leen</surname> <given-names>T. K</given-names></name><name><surname>Dietterich</surname> <given-names>T. G</given-names></name><name><surname>Tresp</surname> <given-names>V</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><volume>13</volume><publisher-name>MIT Press</publisher-name><fpage>556</fpage><lpage>562</lpage></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Linderman</surname> <given-names>S</given-names></name><name><surname>Johnson</surname> <given-names>M</given-names></name><name><surname>Miller</surname> <given-names>A</given-names></name><name><surname>Adams</surname> <given-names>R</given-names></name><name><surname>Blei</surname> <given-names>D</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Bayesian learning and inference in recurrent switching linear dynamical systems</article-title><conf-name>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</conf-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>MA</given-names></name><name><surname>Jin</surname> <given-names>DZ</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Support for a synaptic chain model of neuronal sequence generation</article-title><source>Nature</source><volume>468</volume><fpage>394</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1038/nature09514</pub-id><pub-id pub-id-type="pmid">20972420</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lynch</surname> <given-names>GF</given-names></name><name><surname>Okubo</surname> <given-names>TS</given-names></name><name><surname>Hanuschkin</surname> <given-names>A</given-names></name><name><surname>Hahnloser</surname> <given-names>RH</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rhythmic continuous-time coding in the songbird analog of vocal motor cortex</article-title><source>Neuron</source><volume>90</volume><fpage>877</fpage><lpage>892</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.04.021</pub-id><pub-id pub-id-type="pmid">27196977</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maboudi</surname> <given-names>K</given-names></name><name><surname>Ackermann</surname> <given-names>E</given-names></name><name><surname>de Jong</surname> <given-names>LW</given-names></name><name><surname>Pfeiffer</surname> <given-names>BE</given-names></name><name><surname>Foster</surname> <given-names>D</given-names></name><name><surname>Diba</surname> <given-names>K</given-names></name><name><surname>Kemere</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Uncovering temporal structure in hippocampal output patterns</article-title><source>eLife</source><volume>7</volume><elocation-id>e34467</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34467</pub-id><pub-id pub-id-type="pmid">29869611</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacDonald</surname> <given-names>CJ</given-names></name><name><surname>Lepage</surname> <given-names>KQ</given-names></name><name><surname>Eden</surname> <given-names>UT</given-names></name><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hippocampal &quot;time cells&quot; bridge the gap in memory for discontiguous events</article-title><source>Neuron</source><volume>71</volume><fpage>737</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.07.012</pub-id><pub-id pub-id-type="pmid">21867888</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Mackevicius</surname> <given-names>EL</given-names></name><name><surname>Bahle</surname> <given-names>AH</given-names></name><name><surname>Williams</surname> <given-names>AH</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>seqNMF</data-title><source>GitHub</source><version designator="25df0d6">25df0d6</version><ext-link ext-link-type="uri" xlink:href="https://github.com/FeeLab/seqNMF">https://github.com/FeeLab/seqNMF</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackevicius</surname> <given-names>EL</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Building a state space for song learning</article-title><source>Current Opinion in Neurobiology</source><volume>49</volume><fpage>59</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.12.001</pub-id><pub-id pub-id-type="pmid">29268193</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Mizuseki</surname></name><name><surname>Sirota</surname></name><name><surname>Pastalkova</surname></name><name><surname>Diba</surname></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><data-title>Multiple single unit recordings from different rat hippocampal and entorhinal regions while the animals were performing multiple behavioral tasks</data-title><source>CRCNS</source><ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/hc/hc-3c">https://crcns.org/data-sets/hc/hc-3c</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Młynarski</surname> <given-names>W</given-names></name><name><surname>McDermott</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning midlevel auditory codes from natural sound statistics</article-title><source>Neural Computation</source><volume>30</volume><fpage>631</fpage><lpage>669</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01048</pub-id><pub-id pub-id-type="pmid">29220308</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mokeichev</surname> <given-names>A</given-names></name><name><surname>Okun</surname> <given-names>M</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name><name><surname>Katz</surname> <given-names>Y</given-names></name><name><surname>Ben-Shahar</surname> <given-names>O</given-names></name><name><surname>Lampl</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Stochastic emergence of repeating cortical motifs in spontaneous membrane potential fluctuations in vivo</article-title><source>Neuron</source><volume>53</volume><fpage>413</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.01.017</pub-id><pub-id pub-id-type="pmid">17270737</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Netrapalli</surname> <given-names>P</given-names></name><name><surname>Niranjan</surname> <given-names>UN</given-names></name><name><surname>Sanghavi</surname> <given-names>S</given-names></name><name><surname>Anandkumar</surname> <given-names>A</given-names></name><name><surname>Jain</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Non-convex robust PCA</chapter-title><person-group person-group-type="editor"><name><surname>Ghahramani</surname> <given-names>Z</given-names></name><name><surname>Welling</surname> <given-names>M</given-names></name><name><surname>Cortes</surname> <given-names>C</given-names></name><name><surname>Lawrence</surname> <given-names>ND</given-names></name><name><surname>Weinberger</surname> <given-names>KQ</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><volume>27</volume><publisher-name>Curran Associates, Inc</publisher-name><fpage>1107</fpage><lpage>1115</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okubo</surname> <given-names>TS</given-names></name><name><surname>Mackevicius</surname> <given-names>EL</given-names></name><name><surname>Payne</surname> <given-names>HL</given-names></name><name><surname>Lynch</surname> <given-names>GF</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Growth and splitting of neural sequences in songbird vocal development</article-title><source>Nature</source><volume>528</volume><fpage>352</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1038/nature15741</pub-id><pub-id pub-id-type="pmid">26618871</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>O’Grady</surname> <given-names>PD</given-names></name><name><surname>Pearlmutter</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Convolutive non-negative matrix factorisation with a sparseness constraint</article-title><conf-name>16th IEEE Signal Processing Society Workshop on Machine Learning for Signal Processing</conf-name><pub-id pub-id-type="doi">10.1109/MLSP.2006.275588</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pastalkova</surname> <given-names>E</given-names></name><name><surname>Itskov</surname> <given-names>V</given-names></name><name><surname>Amarasingham</surname> <given-names>A</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Internally generated cell assembly sequences in the rat hippocampus</article-title><source>Science</source><volume>321</volume><fpage>1322</fpage><lpage>1327</lpage><pub-id pub-id-type="doi">10.1126/science.1159775</pub-id><pub-id pub-id-type="pmid">18772431</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Pastalkova</surname> <given-names>W</given-names></name><name><surname>Wang</surname> <given-names>Y</given-names></name><name><surname>Mizuseki</surname> <given-names>K</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Simultaneous extracellular recordings from left and right hippocampal areas CA1 and right entorhinal cortex from a rat performing a left / right alternation task and other behaviors</data-title><source>CRCNS</source><ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/hc/hc-5">https://crcns.org/data-sets/hc/hc-5</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1901">1901</year><article-title>On lines and planes of closest fit to systems of points in space</article-title><source>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</source><volume>2</volume><fpage>559</fpage><lpage>572</lpage><pub-id pub-id-type="doi">10.1080/14786440109462720</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Peter</surname> <given-names>S</given-names></name><name><surname>Kirschbaum</surname> <given-names>E</given-names></name><name><surname>Both</surname> <given-names>M</given-names></name><name><surname>Campbell</surname> <given-names>L</given-names></name><name><surname>Harvey</surname> <given-names>B</given-names></name><name><surname>Heins</surname> <given-names>C</given-names></name><name><surname>Durstewitz</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Sparse convolutional coding for neuronal assembly detection</chapter-title><person-group person-group-type="editor"><name><surname>Guyon</surname> <given-names>I</given-names></name><name><surname>Luxburg</surname> <given-names>U. V</given-names></name><name><surname>Bengio</surname> <given-names>S</given-names></name><name><surname>Wallach</surname> <given-names>H</given-names></name><name><surname>Fergus</surname> <given-names>R</given-names></name><name><surname>Vishwanathan</surname> <given-names>S</given-names></name><name><surname>Garnett</surname> <given-names>R</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><volume>30</volume><publisher-name>Curran Associates, Inc</publisher-name><fpage>3675</fpage><lpage>3685</lpage></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picardo</surname> <given-names>MA</given-names></name><name><surname>Merel</surname> <given-names>J</given-names></name><name><surname>Katlowitz</surname> <given-names>KA</given-names></name><name><surname>Vallentin</surname> <given-names>D</given-names></name><name><surname>Okobi</surname> <given-names>DE</given-names></name><name><surname>Benezra</surname> <given-names>SE</given-names></name><name><surname>Clary</surname> <given-names>RC</given-names></name><name><surname>Pnevmatikakis</surname> <given-names>EA</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Long</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Population-level representation of a temporal sequence underlying song production in the zebra finch</article-title><source>Neuron</source><volume>90</volume><fpage>866</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.016</pub-id><pub-id pub-id-type="pmid">27196976</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quaglio</surname> <given-names>P</given-names></name><name><surname>Rostami</surname> <given-names>V</given-names></name><name><surname>Torre</surname> <given-names>E</given-names></name><name><surname>Grün</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Methods for identification of spike patterns in massively parallel spike trains</article-title><source>Biological Cybernetics</source><volume>112</volume><fpage>57</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1007/s00422-018-0755-0</pub-id><pub-id pub-id-type="pmid">29651582</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramanarayanan</surname> <given-names>V</given-names></name><name><surname>Goldstein</surname> <given-names>L</given-names></name><name><surname>Narayanan</surname> <given-names>SS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Spatio-temporal articulatory movement primitives during speech production: extraction, interpretation, and validation</article-title><source>The Journal of the Acoustical Society of America</source><volume>134</volume><fpage>1378</fpage><lpage>1394</lpage><pub-id pub-id-type="doi">10.1121/1.4812765</pub-id><pub-id pub-id-type="pmid">23927134</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname> <given-names>E</given-names></name><name><surname>Durstewitz</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cell assemblies at multiple time scales with arbitrary lag constellations</article-title><source>eLife</source><volume>6</volume><elocation-id>e19428</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.19428</pub-id><pub-id pub-id-type="pmid">28074777</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholvin</surname> <given-names>J</given-names></name><name><surname>Kinney</surname> <given-names>JP</given-names></name><name><surname>Bernstein</surname> <given-names>JG</given-names></name><name><surname>Moore-Kochlacs</surname> <given-names>C</given-names></name><name><surname>Kopell</surname> <given-names>N</given-names></name><name><surname>Fonstad</surname> <given-names>CG</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Close-packed silicon microelectrodes for scalable spatially oversampled neural recording</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>63</volume><fpage>120</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1109/TBME.2015.2406113</pub-id><pub-id pub-id-type="pmid">26699649</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrader</surname> <given-names>S</given-names></name><name><surname>Grün</surname> <given-names>S</given-names></name><name><surname>Diesmann</surname> <given-names>M</given-names></name><name><surname>Gerstein</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Detecting synfire chain activity using massively parallel spike train recording</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>2165</fpage><lpage>2176</lpage><pub-id pub-id-type="doi">10.1152/jn.01245.2007</pub-id><pub-id pub-id-type="pmid">18632888</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name><name><surname>Churchland</surname> <given-names>PS</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Putting big data to good use in neuroscience</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1440</fpage><lpage>1441</lpage><pub-id pub-id-type="doi">10.1038/nn.3839</pub-id><pub-id pub-id-type="pmid">25349909</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smaragdis</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Non-negative matrix factor deconvolution; extraction of multiple sound sources from monophonic inputs</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-540-30110-3_63</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smaragdis</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Convolutive speech bases and their application to supervised speech separation</article-title><source>IEEE Transactions on Audio, Speech and Language Processing</source><volume>15</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1109/TASL.2006.876726</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Vinyals</surname> <given-names>O</given-names></name><name><surname>Le</surname> <given-names>QV</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Sequence to sequence learning with neural networks</chapter-title><person-group person-group-type="editor"><name><surname>Ghahramani</surname> <given-names>Z</given-names></name><name><surname>Welling</surname> <given-names>M</given-names></name><name><surname>Cortes</surname> <given-names>C</given-names></name><name><surname>Weinberger</surname> <given-names>KQ</given-names></name><name><surname>Lawrence</surname> <given-names>ND</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><volume>27</volume><publisher-name>MIT Press</publisher-name><fpage>3104</fpage><lpage>3112</lpage></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Theis</surname> <given-names>FJ</given-names></name><name><surname>Stadlthanner</surname> <given-names>K</given-names></name><name><surname>Tanaka</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>First results on uniqueness of sparse non-negative matrix factorization</article-title><conf-name>13th European Signal Processing Conference</conf-name></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torre</surname> <given-names>E</given-names></name><name><surname>Canova</surname> <given-names>C</given-names></name><name><surname>Denker</surname> <given-names>M</given-names></name><name><surname>Gerstein</surname> <given-names>G</given-names></name><name><surname>Helias</surname> <given-names>M</given-names></name><name><surname>Grün</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>ASSET: analysis of sequences of synchronous events in massively parallel spike trains</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004939</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004939</pub-id><pub-id pub-id-type="pmid">27420734</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ubaru</surname> <given-names>S</given-names></name><name><surname>Wu</surname> <given-names>K</given-names></name><name><surname>Bouchard</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>UoI-NMF cluster: a robust nonnegative matrix factorization algorithm for improved parts-based decomposition and reconstruction of noisy data</article-title><conf-name>2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)</conf-name></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Udell</surname> <given-names>M</given-names></name><name><surname>Horn</surname> <given-names>C</given-names></name><name><surname>Zadeh</surname> <given-names>R</given-names></name><name><surname>Boyd</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Generalized low rank models</article-title><source>Foundations and Trends in Machine Learning</source><volume>9</volume><fpage>1</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1561/2200000055</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Meij</surname> <given-names>R</given-names></name><name><surname>Voytek</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Uncovering neuronal networks defined by consistent between-neuron spike timing from neuronal spike recordings</article-title><source>Eneuro</source><volume>5</volume><elocation-id>ENEURO.0379-17.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0379-17.2018</pub-id><pub-id pub-id-type="pmid">29789811</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vaz</surname> <given-names>C</given-names></name><name><surname>Toutios</surname> <given-names>A</given-names></name><name><surname>Narayanan</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Convex hull convolutive non-negative matrix factorization for uncovering temporal patterns in multivariate time-series data</article-title><conf-name>Interspeech 2016</conf-name></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villasana</surname> <given-names>TPJ</given-names></name><name><surname>Gorlow</surname> <given-names>S</given-names></name><name><surname>Hariraman</surname> <given-names>AT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multiplicative updates for convolutional NMF under β-Divergence</article-title><source>arVix</source></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Luxburg</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Clustering stability: an overview</article-title><source>Foundations and Trends in Machine Learning</source><volume>2</volume><fpage>235</fpage><lpage>274</lpage></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>D</given-names></name><name><surname>Vipperla</surname> <given-names>R</given-names></name><name><surname>Evans</surname> <given-names>N</given-names></name><name><surname>Zheng</surname> <given-names>TF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Online non-negative convolutive pattern learning for speech signals</article-title><source>IEEE Transactions on Signal Processing</source><volume>61</volume><fpage>44</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1109/TSP.2012.2222381</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wold</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Cross-validatory estimation of the number of components in factor and principal components models</article-title><source>Technometrics</source><volume>20</volume><fpage>397</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1080/00401706.1978.10489693</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname> <given-names>S</given-names></name><name><surname>Joseph</surname> <given-names>A</given-names></name><name><surname>Hammonds</surname> <given-names>AS</given-names></name><name><surname>Celniker</surname> <given-names>SE</given-names></name><name><surname>Yu</surname> <given-names>B</given-names></name><name><surname>Frise</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks</article-title><source>PNAS</source><volume>113</volume><fpage>4290</fpage><lpage>4295</lpage><pub-id pub-id-type="doi">10.1073/pnas.1521171113</pub-id><pub-id pub-id-type="pmid">27071099</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>Z</given-names></name><name><surname>Xu</surname> <given-names>Y</given-names></name><name><surname>Yang</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>X</given-names></name><name><surname>Zhang</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A survey of sparse representation: algorithms and applications</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1602.07017">https://arxiv.org/abs/1602.07017</ext-link></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname> <given-names>P</given-names></name><name><surname>Resendez</surname> <given-names>SL</given-names></name><name><surname>Rodriguez-Romaguera</surname> <given-names>J</given-names></name><name><surname>Jimenez</surname> <given-names>JC</given-names></name><name><surname>Neufeld</surname> <given-names>SQ</given-names></name><name><surname>Giovannucci</surname> <given-names>A</given-names></name><name><surname>Friedrich</surname> <given-names>J</given-names></name><name><surname>Pnevmatikakis</surname> <given-names>EA</given-names></name><name><surname>Stuber</surname> <given-names>GD</given-names></name><name><surname>Hen</surname> <given-names>R</given-names></name><name><surname>Kheirbek</surname> <given-names>MA</given-names></name><name><surname>Sabatini</surname> <given-names>BL</given-names></name><name><surname>Kass</surname> <given-names>RE</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Efficient and accurate extraction of in vivo calcium signals from microendoscopic video data</article-title><source>eLife</source><volume>7</volume><elocation-id>e28728</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.28728</pub-id><pub-id pub-id-type="pmid">29469809</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.38471.032</object-id><sec id="s7" sec-type="appendix"><title>Deriving multiplicative update rules</title><p>Standard gradient descent methods for minimizing a cost function must be adapted when solutions are constrained to be non-negative, since gradient descent steps may result in negative values. Lee and Seung invented an elegant and widely-used algorithm for non-negative gradient descent that avoids negative values by performing multiplicative updates (<xref ref-type="bibr" rid="bib33">Lee and Seung, 2001</xref>; <xref ref-type="bibr" rid="bib32">Lee and Seung, 1999</xref>). They derived these multiplicative updates by choosing an adaptive learning rate that makes additive terms cancel from standard gradient descent on the cost function. We will reproduce their derivation here, and detail how to extend it to the convolutional case (<xref ref-type="bibr" rid="bib58">Smaragdis, 2004</xref>) and apply several forms of regularization (<xref ref-type="bibr" rid="bib46">O’Grady and Pearlmutter, 2006</xref>; <xref ref-type="bibr" rid="bib53">Ramanarayanan et al., 2013</xref>; <xref ref-type="bibr" rid="bib8">Chen and Cichocki, 2004</xref>). See <xref ref-type="table" rid="table2">Table 2</xref> for a compilation of cost functions, derivatives and multiplicative updates for NMF and convNMF under several different regularization conditions.</p><sec id="s7-1"><title>Standard NMF</title><p>NMF performs the factorization <inline-formula><mml:math id="inf519"><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>≈</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐖𝐇</mml:mi></mml:mrow></mml:math></inline-formula>. NMF factorizations seek to solve the following problem:<disp-formula id="equ19"><label>(17)</label><mml:math id="m19"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mi>min</mml:mi><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℒ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ20"><label>(18)</label><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ21"><label>(19)</label><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This problem is convex in <inline-formula><mml:math id="inf520"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf521"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> separately, not together, so a local minimum is found by alternating <inline-formula><mml:math id="inf522"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf523"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> updates. Note that:<disp-formula id="equ22"><label>(20)</label><mml:math id="m22"><mml:mrow><mml:mrow><mml:mfrac><mml:mo>∂</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi class="ltx_font_mathscript">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐗𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ23"><label>(21)</label><mml:math id="m23"><mml:mrow><mml:mrow><mml:mfrac><mml:mo>∂</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi class="ltx_font_mathscript">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Thus, gradient descent steps for <inline-formula><mml:math id="inf524"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf525"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> are:<disp-formula id="equ24"><label>(22)</label><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ25"><label>(23)</label><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To arrive at multiplicative updates, Lee and Seung (<xref ref-type="bibr" rid="bib33">Lee and Seung, 2001</xref>) set:<disp-formula id="equ26"><label>(24)</label><mml:math id="m26"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mi mathvariant="bold">𝐖</mml:mi></mml:msub><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mfrac><mml:mi mathvariant="bold">𝐖</mml:mi><mml:msup><mml:mi mathvariant="bold">𝐖𝐇𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ27"><label>(25)</label><mml:math id="m27"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mi mathvariant="bold">𝐇</mml:mi></mml:msub><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mfrac><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐖𝐇</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Thus, the gradient descent updates become multiplicative:<disp-formula id="equ28"><label>(26)</label><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">←</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ29"><label>(27)</label><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">←</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where the division and <inline-formula><mml:math id="inf526"><mml:mo>×</mml:mo></mml:math></inline-formula> are element-wise.</p></sec><sec id="s7-2"><title>Standard convNMF</title><p>Convolutional NMF factorizes data <inline-formula><mml:math id="inf527"><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>≈</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>→</mml:mo><mml:mi/></mml:mrow></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊛</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. ConvNMF factorizations seek to solve the following problem:<disp-formula id="equ30"><label>(28)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mi>min</mml:mi><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℒ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ31"><label>(29)</label><mml:math id="m31"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathscript">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ32"><label>(30)</label><mml:math id="m32"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p>The derivation above for standard NMF can be applied for each <inline-formula><mml:math id="inf528"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula>, yielding the following update rules for convNMF (<xref ref-type="bibr" rid="bib58">Smaragdis, 2004</xref>):<disp-formula id="equ33"><label>(31)</label><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ34"><label>(32)</label><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mover><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:mover></mml:mrow></mml:mfrac><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where the operator <inline-formula><mml:math id="inf529"><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>→</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> shifts a matrix in the <inline-formula><mml:math id="inf530"><mml:mo>→</mml:mo></mml:math></inline-formula> direction by <inline-formula><mml:math id="inf531"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> timebins, that is a delay by <inline-formula><mml:math id="inf532"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> timebins, and <inline-formula><mml:math id="inf533"><mml:mrow><mml:mi/><mml:mo>←</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:math></inline-formula> shifts a matrix in the <inline-formula><mml:math id="inf534"><mml:mo>←</mml:mo></mml:math></inline-formula> direction by <inline-formula><mml:math id="inf535"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> timebins (<xref ref-type="table" rid="table1">Table 1</xref>). Note that NMF is a special case of convNMF where <inline-formula><mml:math id="inf536"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s7-3"><title>Incorporating regularization terms</title><p>Suppose we want to regularize by adding a new term, <inline-formula><mml:math id="inf537"><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:math></inline-formula> to the cost function:<disp-formula id="equ35"><label>(33)</label><mml:math id="m35"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mi>min</mml:mi><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℒ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ36"><label>(34)</label><mml:math id="m36"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathscript">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ37"><label>(35)</label><mml:math id="m37"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p>Using a similar trick to Lee and Seung, we choose a <inline-formula><mml:math id="inf538"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mi mathvariant="bold">𝐖</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mi mathvariant="bold">𝐇</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to arrive at a simple multiplicative update. Below is the standard NMF case, which generalizes trivially to the convNMF case.</p><p>Note that:<disp-formula id="equ38"><label>(36)</label><mml:math id="m38"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi></mml:mrow></mml:mfrac><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐗𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ39"><label>(37)</label><mml:math id="m39"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mfrac><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We set:<disp-formula id="equ40"><label>(38)</label><mml:math id="m40"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mi mathvariant="bold">𝐖</mml:mi></mml:msub><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mfrac><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐖</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ41"><label>(39)</label><mml:math id="m41"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mi mathvariant="bold">𝐇</mml:mi></mml:msub><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mo>=</mml:mo><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mfrac><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathscript">ℛ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Thus, the gradient descent updates become multiplicative:<disp-formula id="equ42"><label>(40)</label><mml:math id="m42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ43"><label>(41)</label><mml:math id="m43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">ℛ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where the division and <inline-formula><mml:math id="inf539"><mml:mo>×</mml:mo></mml:math></inline-formula> are element-wise.</p><p>The above formulation enables flexible incorporation of different types of regularization or penalty terms into the multiplicative NMF update algorithm. This framework also extends naturally to the convolutional case. See <xref ref-type="table" rid="table2">Table 2</xref> for examples of several regularization terms, including <inline-formula><mml:math id="inf540"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> sparsity (<xref ref-type="bibr" rid="bib46">O’Grady and Pearlmutter, 2006</xref>; <xref ref-type="bibr" rid="bib53">Ramanarayanan et al., 2013</xref>) and spatial decorrelation (<xref ref-type="bibr" rid="bib8">Chen and Cichocki, 2004</xref>), as well as the terms we introduce here to combat the types of inefficiencies and cross correlations we identified in convolutional NMF, namely, smoothed orthogonality for <inline-formula><mml:math id="inf541"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf542"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>, and the x-ortho penalty term. For the x-ortho penalty term, <inline-formula><mml:math id="inf543"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the multiplicative update rules are:<disp-formula id="equ44"><label>(42)</label><mml:math id="m44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>λ</mml:mi><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:mover><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo mathvariant="bold">−</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ45"><label>(43)</label><mml:math id="m45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mover><mml:mo>⊛</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi></mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where the division and <inline-formula><mml:math id="inf544"><mml:mo>×</mml:mo></mml:math></inline-formula> are element-wise. Note that multiplication with the <inline-formula><mml:math id="inf545"><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf546"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>𝟏</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐈</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> effectively implements factor competition because it places in the <inline-formula><mml:math id="inf547"><mml:mi>k</mml:mi></mml:math></inline-formula>th row a sum across all other factors.</p></sec></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.38471.033</object-id><sec id="s8" sec-type="appendix"><title>Relation of the x-ortho penalty to traditional regularizations</title><p>As noted in the main text, the x-ortho penalty term is not formally a regularization because it includes the data <inline-formula><mml:math id="inf548"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>. In this Appendix, we show how this penalty can be approximated by a data-free regularization. The resulting regularization contains three terms corresponding to a weighted orthogonality penalty on pairs of <inline-formula><mml:math id="inf549"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> factors, a weighted orthogonality penalty on pairs of <inline-formula><mml:math id="inf550"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> factors, and a term that penalizes interactions among triplets of factors. We analyze each term in both the time domain (<xref ref-type="disp-formula" rid="equ52">Equation 50</xref>) and in the frequency domain (<xref ref-type="disp-formula" rid="equ52 equ71">Equations 50 and 69)</xref>.</p><sec id="s8-1"><title>Time domain analysis</title><p>We consider the cross-orthogonality penalty term:<disp-formula id="equ46"><label>(44)</label><mml:math id="m46"><mml:mrow><mml:mi class="ltx_font_mathscript">ℛ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>and define, <inline-formula><mml:math id="inf551"><mml:mrow><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mo>⊛</mml:mo><mml:mo>⊤</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐗</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which is a <inline-formula><mml:math id="inf552"><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> matrix. Each element <inline-formula><mml:math id="inf553"><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a positive number describing the overlap or correlation between factor <inline-formula><mml:math id="inf554"><mml:mi>i</mml:mi></mml:math></inline-formula> and factor <inline-formula><mml:math id="inf555"><mml:mi>j</mml:mi></mml:math></inline-formula> in the model. Each element of <inline-formula><mml:math id="inf556"><mml:mi mathvariant="bold">𝐑</mml:mi></mml:math></inline-formula> can be written explicitly as:<disp-formula id="equ47"><label>(45)</label><mml:math id="m47"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>τ</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐒</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Where the index variables <inline-formula><mml:math id="inf557"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf558"><mml:mi>τ</mml:mi></mml:math></inline-formula> range from <inline-formula><mml:math id="inf559"><mml:mn>1</mml:mn></mml:math></inline-formula> to <inline-formula><mml:math id="inf560"><mml:mi>T</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf561"><mml:mi>n</mml:mi></mml:math></inline-formula> ranges from <inline-formula><mml:math id="inf562"><mml:mn>1</mml:mn></mml:math></inline-formula> to <inline-formula><mml:math id="inf563"><mml:mi>N</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf564"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> ranges from <inline-formula><mml:math id="inf565"><mml:mn>1</mml:mn></mml:math></inline-formula> to <inline-formula><mml:math id="inf566"><mml:mi>L</mml:mi></mml:math></inline-formula>.</p><p>Our goal here is to find a close approximation to this penalty term that does not contain the data <inline-formula><mml:math id="inf567"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>. This can readily be done if <inline-formula><mml:math id="inf568"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> is well-approximated by the convNMF decomposition:<disp-formula id="equ48"><label>(46)</label><mml:math id="m48"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊛</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Substituting this expression into <xref ref-type="disp-formula" rid="equ47">Equation 45</xref> and defining the smoothed matrix <inline-formula><mml:math id="inf569"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>τ</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐒</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="inf570"><mml:msubsup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mtext>smooth</mml:mtext></mml:msubsup></mml:math></inline-formula> gives:<disp-formula id="equ49"><label>(47)</label><mml:math id="m49"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mtext>smooth</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Making the substitution <inline-formula><mml:math id="inf571"><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> gives:<disp-formula id="equ50"><label>(48)</label><mml:math id="m50"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mtext>smooth</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where in the above expression we have taken <inline-formula><mml:math id="inf572"><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> to extend over the full range from <inline-formula><mml:math id="inf573"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf574"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> under the implicit assumption that <inline-formula><mml:math id="inf575"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf576"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> are zero padded such that values of <inline-formula><mml:math id="inf577"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> for lag indices outside the range <inline-formula><mml:math id="inf578"><mml:mn>0</mml:mn></mml:math></inline-formula> to <inline-formula><mml:math id="inf579"><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and values of <inline-formula><mml:math id="inf580"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> for time indices outside the range <inline-formula><mml:math id="inf581"><mml:mn>1</mml:mn></mml:math></inline-formula> to <inline-formula><mml:math id="inf582"><mml:mi>T</mml:mi></mml:math></inline-formula> are taken to be zero.</p><p>Relabeling <inline-formula><mml:math id="inf583"><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> as <inline-formula><mml:math id="inf584"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> and gathering terms together yields<disp-formula id="equ51"><label>(49)</label><mml:math id="m51"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mtext>smooth</mml:mtext></mml:msubsup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We note that the above expression contains terms that resemble penalties on orthogonality between two <underline><inline-formula><mml:math id="inf585"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula></underline> factors (first parenthetical) or two <underline><inline-formula><mml:math id="inf586"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula></underline> factors (one of which is smoothed, second parenthetical), but in this case allowing for different time lags <inline-formula><mml:math id="inf587"><mml:mi>u</mml:mi></mml:math></inline-formula> between the factors. To understand this formula better, we decompose the above sum over <underline><inline-formula><mml:math id="inf588"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></underline> into three contributions corresponding to <inline-formula><mml:math id="inf589"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf590"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf591"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula><disp-formula id="equ52"><label>(50)</label><mml:math id="m52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>smooth</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>j</mml:mi><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>smooth</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>smooth</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The first term above contains, for <inline-formula><mml:math id="inf592"><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, a simple extension of the <inline-formula><mml:math id="inf593"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> element of the <inline-formula><mml:math id="inf594"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> orthogonality condition <inline-formula><mml:math id="inf595"><mml:msup><mml:mi mathvariant="bold">𝐇𝐒𝐇</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:math></inline-formula>. The extension is that the orthogonality is weighted by the power, that is the sum of squared elements, in the <inline-formula><mml:math id="inf596"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> factor of <inline-formula><mml:math id="inf597"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> (the apparent lack of symmetry in weighing by the <inline-formula><mml:math id="inf598"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> rather than the <inline-formula><mml:math id="inf599"><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> factor can be removed by simultaneously considering the term <inline-formula><mml:math id="inf600"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, as shown in the Fourier representation of the following section). This weighting has the benefit of applying the penalty on <inline-formula><mml:math id="inf601"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> orthogonality most strongly to those factors whose corresponding <inline-formula><mml:math id="inf602"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> components contain the most power. For <inline-formula><mml:math id="inf603"><mml:mrow><mml:mi>u</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, this orthogonality condition is extended to allow for overlap of time-shifted <inline-formula><mml:math id="inf604"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>’s, with weighting at each time shift by the autocorrelation of the corresponding <inline-formula><mml:math id="inf605"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> factor. Qualitatively, this enforces that (even in the absence of the smoothing matrix <inline-formula><mml:math id="inf606"><mml:mi mathvariant="bold">𝐒</mml:mi></mml:math></inline-formula>), <inline-formula><mml:math id="inf607"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>’s that are offset by less than the width of the autocorrelation of the corresponding <inline-formula><mml:math id="inf608"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>’s will have overlapping convolutions with these <inline-formula><mml:math id="inf609"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>’s due to the temporal smoothing associated with the convolution operation. We note that, for sparse sequences as in the examples of <xref ref-type="fig" rid="fig1">Figure 1</xref>, there is no time-lagged component to the autocorrelation, so this term corresponds simply to a smoothed <inline-formula><mml:math id="inf610"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> orthogonality regularization, weighted by the strength of the corresponding <inline-formula><mml:math id="inf611"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> factors.</p><p>The second term above represents a complementary orthogonality condition on the <inline-formula><mml:math id="inf612"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> components, in which orthogonality in the <inline-formula><mml:math id="inf613"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf614"><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> factors are weighted by the (smoothed) autocorrelation of the <inline-formula><mml:math id="inf615"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> factors. For the case in which the <inline-formula><mml:math id="inf616"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> factors have no time-lagged autocorrelations, this corresponds to a simple weighting of <inline-formula><mml:math id="inf617"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> orthogonality by the strength of the corresponding <inline-formula><mml:math id="inf618"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> factors.</p><p>Finally, we consider the remaining terms of the cost function, for which <inline-formula><mml:math id="inf619"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. We note that these terms are only relevant when the factorization contains at least three factors, and thus their role cannot be visualized from the simple Type 1 to Type 3 examples of <xref ref-type="fig" rid="fig1">Figure 1</xref>. These terms have the form:<disp-formula id="equ53"><label>(51)</label><mml:math id="m53"><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mtext>smooth</mml:mtext></mml:msubsup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To understand how this term contributes, we consider each of the expressions in parentheses. The first expression corresponds, as described above, to the time-lagged cross correlation of the <inline-formula><mml:math id="inf620"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf621"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> components. Likewise, the second expression corresponds to the time-lagged correlation of the (smoothed) <inline-formula><mml:math id="inf622"><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf623"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> components. Thus, this term of <inline-formula><mml:math id="inf624"><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> contributes whenever there is a factor (<inline-formula><mml:math id="inf625"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>⁣</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁣</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) that overlaps, at the same time lags, with the <inline-formula><mml:math id="inf626"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> factor’s <inline-formula><mml:math id="inf627"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> component and the <inline-formula><mml:math id="inf628"><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> factor’s <inline-formula><mml:math id="inf629"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> component. Thus, this term penalizes cases where, rather than (or in addition to) two factors <inline-formula><mml:math id="inf630"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf631"><mml:mi>j</mml:mi></mml:math></inline-formula> directly overlapping one another, they have a common factor <inline-formula><mml:math id="inf632"><mml:mi>k</mml:mi></mml:math></inline-formula> with which they overlap.</p><p>An example of the contribution of a triplet penalty term, as well as of the paired terms of <xref ref-type="disp-formula" rid="equ52">Equation 50</xref>, is shown in <xref ref-type="fig" rid="fig1">Figure 1</xref> of this Appendix. By inspection, there is a penalty <inline-formula><mml:math id="inf633"><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mn>23</mml:mn></mml:msub></mml:math></inline-formula> due to the overlapping values of the pair (<inline-formula><mml:math id="inf634"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>𝟐</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>𝟑</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>). Likewise, there is a penalty <inline-formula><mml:math id="inf635"><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mn>13</mml:mn></mml:msub></mml:math></inline-formula> due to the overlapping values of the pair (<inline-formula><mml:math id="inf636"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>𝟏</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>𝟑</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>). The triplet penalty term contributes to <inline-formula><mml:math id="inf637"><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mn>12</mml:mn></mml:msub></mml:math></inline-formula> and derives from the fact that <inline-formula><mml:math id="inf638"><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>𝟏</mml:mn></mml:msub></mml:math></inline-formula> overlaps with <inline-formula><mml:math id="inf639"><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>𝟑</mml:mn></mml:msub></mml:math></inline-formula> at the same time (and with the same, zero time lag) as <inline-formula><mml:math id="inf640"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>𝟐</mml:mn></mml:msub></mml:math></inline-formula> overlaps with <inline-formula><mml:math id="inf641"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>𝟑</mml:mn></mml:msub></mml:math></inline-formula>.</p><p>In summary, the above analysis shows that for good reconstructions of the data where <inline-formula><mml:math id="inf642"><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊛</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the x-ortho penalty can be well-approximated by the sum of three contributions. The first corresponds to a penalty on time-lagged (smoothed) <inline-formula><mml:math id="inf643"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> orthogonality weighted at each time lag by the autocorrelation of the corresponding <inline-formula><mml:math id="inf644"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> factors. The second similarly corresponds to a penalty on time-lagged <inline-formula><mml:math id="inf645"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> orthogonality weighted at each time lag by the (smoothed) autocorrelation of the corresponding <inline-formula><mml:math id="inf646"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> factors. For simple cases of sparse sequences, these contributions reduce to orthogonality in <inline-formula><mml:math id="inf647"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf648"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> weighted by the power in the corresponding <inline-formula><mml:math id="inf649"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf650"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>, respectively, thus focusing the penalties most heavily on those factors which contribute most heavily to the data reconstruction. The third, triplet contribution corresponds to the case in which a factor in <inline-formula><mml:math id="inf651"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and a different factor in <inline-formula><mml:math id="inf652"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> both overlap (at the same time lag) with a third common factor, and may occur even when the factors <inline-formula><mml:math id="inf653"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf654"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> themselves are orthogonal. Further work is needed to determine whether this third contribution is critical to the x-ortho penalty or is simply a by-product of the x-ortho penalty procedure’s direct use of the data <inline-formula><mml:math id="inf655"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>.</p><fig id="app2fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.38471.034</object-id><label>Appendix 2—figure 1.</label><caption><title>Example of redundancy with three factors.</title><p>In addition to the direct overlap of <inline-formula><mml:math id="inf656"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf657"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>, and of <inline-formula><mml:math id="inf658"><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf659"><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>, there is a ‘triplet’ penalty <inline-formula><mml:math id="inf660"><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mn>12</mml:mn></mml:msub></mml:math></inline-formula> on factors 1 and 2 that occurs because each has an overlap (in either <inline-formula><mml:math id="inf661"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf662"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>) with the 3rd factor (<inline-formula><mml:math id="inf663"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>). This occurs even though neither <inline-formula><mml:math id="inf664"><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf665"><mml:msub><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, nor <inline-formula><mml:math id="inf666"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf667"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, are themselves overlapping.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-38471-app2-fig1-v2.tif"/></fig></sec><sec id="s8-2"><title>Frequency domain analysis</title><p>Additional insight may be obtained by analyzing these three components of <inline-formula><mml:math id="inf668"><mml:mi mathvariant="bold">𝐑</mml:mi></mml:math></inline-formula> in the Fourier domain. Before doing so, we below derive the Fourier domain representation of <inline-formula><mml:math id="inf669"><mml:mi mathvariant="bold">𝐑</mml:mi></mml:math></inline-formula>, and provide insights suggested by this perspective.</p><sec id="s8-2-1"><title>Fourier representation of the x-ortho penalty</title><p>As in the time domain analysis, we start with defining:<disp-formula id="equ54"><label>(52)</label><mml:math id="m54"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>τ</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐒</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Unpacking the notation above, we note that:<disp-formula id="equ55"><label>(53)</label><mml:math id="m55"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf670"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> is the <inline-formula><mml:math id="inf671"><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> row of <inline-formula><mml:math id="inf672"><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>⁣</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf673"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> is the <inline-formula><mml:math id="inf674"><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> row of <inline-formula><mml:math id="inf675"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf676"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> is <inline-formula><mml:math id="inf677"><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁣</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf678"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:math></inline-formula> is a smoothing vector corresponding to the entries of each row of the smoothing matrix <inline-formula><mml:math id="inf679"><mml:mi mathvariant="bold">𝐒</mml:mi></mml:math></inline-formula>, and “<inline-formula><mml:math id="inf680"><mml:mo>⋅</mml:mo></mml:math></inline-formula>” is a dot product. For ease of mathematical presentation, in the following, we work with continuous time rather than the discretely sampled data and extend the <inline-formula><mml:math id="inf681"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> factors, <inline-formula><mml:math id="inf682"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> factors, and data matrix <inline-formula><mml:math id="inf683"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> through zero-padding on both ends so that:<disp-formula id="equ56"><label>(54)</label><mml:math id="m56"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">ℓ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>and<disp-formula id="equ57"><label>(55)</label><mml:math id="m57"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Recall that the Fourier transform is defined as:<disp-formula id="equ58"><label>(56)</label><mml:math id="m58"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>with inverse Fourier transform:<disp-formula id="equ59"><label>(57)</label><mml:math id="m59"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Now recall some basic features of Fourier transforms of correlation and convolution integrals:<disp-formula id="equ60"><label>(58)</label><mml:math id="m60"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ61"><label>(59)</label><mml:math id="m61"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ62"><label>(60)</label><mml:math id="m62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left right" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="script">ℱ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>ω</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>ω</mml:mi></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This final identity, known as Parseval’s theorem, says that the inner product (dot product) between two functions evaluated in the time and frequency domain are equivalent up to a proportionality constant of <inline-formula><mml:math id="inf684"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. With the above identities, we can calculate our quantity of interest:<disp-formula id="equ63"><label>(61)</label><mml:math id="m63"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>First, define:<disp-formula id="equ64"><label>(62)</label><mml:math id="m64"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ65"><label>(63)</label><mml:math id="m65"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>From <xref ref-type="disp-formula" rid="equ62">Equation 60</xref> (Parseval’s theorem):<disp-formula id="equ66"><label>(64)</label><mml:math id="m66"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, from <xref ref-type="disp-formula" rid="equ60 equ61">Equations 58 and 59</xref>, we have:<disp-formula id="equ67"><label>(65)</label><mml:math id="m67"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The above formula shows that:</p><list list-type="order"><list-item><p>Viewed in the frequency domain, the x-ortho penalty reduces to a (sum over neurons and frequencies of a) simple product of Fourier transforms of the four matrices involved in the penalty.</p></list-item><list-item><p>The smoothing can equally well be applied to <inline-formula><mml:math id="inf685"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf686"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf687"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>. (For <inline-formula><mml:math id="inf688"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>, note that for symmetric smoothing function <inline-formula><mml:math id="inf689"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, we also have <inline-formula><mml:math id="inf690"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.)</p></list-item><list-item><p>One can view this operation as either of the below:</p><list list-type="alpha-lower"><list-item><p>First correlate <inline-formula><mml:math id="inf691"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf692"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> by summing correlations of each row, and then calculate the overlap with the smoothed <inline-formula><mml:math id="inf693"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>, as described in the main text:<inline-formula><mml:math id="inf694"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Correlate <inline-formula><mml:math id="inf695"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> with each row of <inline-formula><mml:math id="inf696"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> and then calculate the overlap of this correlation with the corresponding smoothed row of <inline-formula><mml:math id="inf697"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>. Then sum over all rows:<inline-formula><mml:math id="inf698"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list></list-item></list></sec><sec id="s8-2-2"><title>Fourier representation of the traditional regularization approximation of the x-ortho penalty</title><p>We now proceed to show how the x-ortho penalty can be approximated by a traditional (data-free) regularization, expressing the results in the frequency domain. As in the time domain analysis, we consider the approximation in which the data <inline-formula><mml:math id="inf699"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> are nearly perfectly reconstructed by the convNMF decomposition (<inline-formula><mml:math id="inf700"><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊛</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>Noting that this decomposition is a sum over factors of row-by-row ordinary convolutions, we can write the Fourier analog for each row of <inline-formula><mml:math id="inf701"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> as:<disp-formula id="equ68"><label>(66)</label><mml:math id="m68"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Thus, substituting <inline-formula><mml:math id="inf702"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> with the reconstruction, <inline-formula><mml:math id="inf703"><mml:mrow><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo>⊛</mml:mo><mml:mi mathvariant="bold">𝐇</mml:mi></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ67">Equation 65</xref>, we have:<disp-formula id="equ69"><label>(67)</label><mml:math id="m69"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As in the time domain analysis, we separate the sum over <inline-formula><mml:math id="inf704"><mml:mi>k</mml:mi></mml:math></inline-formula> into three cases: <inline-formula><mml:math id="inf705"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf706"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf707"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Recall that for real numbers, <inline-formula><mml:math id="inf708"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf709"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. Thus, separating the sum over <inline-formula><mml:math id="inf710"><mml:mi>k</mml:mi></mml:math></inline-formula> into the three cases, we have:<disp-formula id="equ70"><label>(68)</label><mml:math id="m70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:mi>ω</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold">W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold">W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold">W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>Y</mml:mi></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf711"><mml:mi>Y</mml:mi></mml:math></inline-formula> represents the remaining terms for which <inline-formula><mml:math id="inf712"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>We can obtain a more symmetric form of this equation by summing the contributions of factors <inline-formula><mml:math id="inf713"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf714"><mml:mi>j</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf715"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. For symmetric smoothing functions <inline-formula><mml:math id="inf716"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, for which <inline-formula><mml:math id="inf717"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, we obtain:<disp-formula id="equ71"><label>(69)</label><mml:math id="m71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:mi>ω</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold">W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold">W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold">W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold">W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>Y</mml:mi></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>As in the time domain analysis, the first two terms above have a simple interpretation in comparison to traditional orthogonality regularizations: The first term resembles a traditional regularization of orthogonality in (smoothed) <inline-formula><mml:math id="inf718"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>, but now weighted frequency-by-frequency by the summed power at that frequency in the corresponding <inline-formula><mml:math id="inf719"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> factors. For sparse (delta-function-like) sequences, the power in <inline-formula><mml:math id="inf720"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> at each frequency is a constant and can be taken outside the integral. In this case, the regularization corresponds precisely to orthogonality in (smoothed) H, weighted by the summed power in the corresponding <inline-formula><mml:math id="inf721"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>’s. Likewise, the second term above corresponds to a traditional regularization of orthogonality in (smoothed) <inline-formula><mml:math id="inf722"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>, weighted by the summed power at each component frequency in the corresponding <inline-formula><mml:math id="inf723"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> factors.</p><p>Altogether, we see that these terms represent a Fourier-power weighted extension of (smoothed) traditional orthogonality regularizations in <inline-formula><mml:math id="inf724"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf725"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>. This weighting may be beneficial relative to traditional orthogonality penalties, since it makes the regularization focus most heavily on the factors and frequencies that contribute most to the data reconstruction.</p><p>Finally, we consider the remaining terms in the cost function, for which <inline-formula><mml:math id="inf726"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. As noted previously, these terms are only relevant when the factorization contains at least three terms, so cannot be seen in the simple Type 1, 2 and 3 cases illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>. These terms have the form:<disp-formula id="equ72"><label>(70)</label><mml:math id="m72"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To understand how this term contributes, we consider each of the expressions in parentheses. The first expression contains each frequency component of the correlation of the <inline-formula><mml:math id="inf727"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf728"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> factors’ <inline-formula><mml:math id="inf729"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula>’s. The second expression likewise contains each frequency component of the correlation of the <inline-formula><mml:math id="inf730"><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf731"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> factors’ <inline-formula><mml:math id="inf732"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula>’s. Thus, analogous to the time domain analysis, this term of <inline-formula><mml:math id="inf733"><mml:msub><mml:mi mathvariant="bold">𝐑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> contributes whenever there is a factor (<inline-formula><mml:math id="inf734"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mrow><mml:mi/><mml:mo>⋅</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>⁣</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁣</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) that overlaps at any frequency with the <inline-formula><mml:math id="inf735"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> factor’s <inline-formula><mml:math id="inf736"><mml:mi mathvariant="bold">𝐖</mml:mi></mml:math></inline-formula> component and the <inline-formula><mml:math id="inf737"><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> factor’s <inline-formula><mml:math id="inf738"><mml:mi mathvariant="bold">𝐇</mml:mi></mml:math></inline-formula> component. In this manner, this three-factor interaction term effectively enforces competition between factors <inline-formula><mml:math id="inf739"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf740"><mml:mi>j</mml:mi></mml:math></inline-formula> even if they are not correlated themselves, as demonstrated in <xref ref-type="fig" rid="fig1">Figure 1</xref> of this Appendix.</p></sec></sec></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.38471.040</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Colgin</surname><given-names>Laura</given-names></name><role>Reviewing Editor</role><aff><institution>The University of Texas at Austin</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for sending your article entitled &quot;Unsupervised discovery of temporal sequences in high-dimensional datasets, with applications to neuroscience&quot; for peer review at <italic>eLife</italic>. Your article is being evaluated by Timothy Behrens as the Senior Editor, a Reviewing Editor, and three reviewers.</p><p>While the reviewers were generally enthusiastic about the work, major concerns were brought up that raise the question of whether this work is appropriate for the general readership of <italic>eLife</italic>. All reviewers agreed that these major concerns require essential revisions and thus would like to see an action plan that addresses these concerns before they issue a formal decision. In particular, the reviewers were not convinced that this method constitutes a major advance over current state-of-the-art methods in the field. Also, the selection of hyperparameters was not convincingly justified. The reviews are included below in their entirety.</p><p><italic>Reviewer #1:</italic> </p><p>This paper introduces a matrix factorization-based method, seqNMF, for extracting temporal sequences from high-dimensional data. The authors convincingly demonstrate that seqNMF performs well in artificial and real datasets. I believe seqNMF will be a useful tool for a wide range of applications. After its appearance at COSYNE, the tool has already created considerable excitement in the computational and systems neuroscience community.</p><p>I found the paper to be very well written and the results clearly presented. However, some points need improvement:</p><p>1) While Type 1, 2, and 3 errors are clearly explained, I thought better naming could improve the readability of the paper a lot.</p><p>2) Building into equation 6, the authors discuss two alternative regularizers, and claim that they would fail at preventing Type 1, 2 and 3 errors simultaneously. While the arguments are clear, an experimental demonstration would be more convincing.</p><p>3) In subsection “SeqNMF: A constrained convolutional non-negative matrix factorization”, authors call ||M||<sub>1,i≠j</sub> a norm on M, which technically isn't correct (Under this definition, a diagonal matrix would have a zero norm).</p><p>4) I have trouble with understanding the first method of choosing <italic>λ</italic>. Is this method only applicable to data for which ground-truth is known? Or are the authors suggesting to choose <italic>λ</italic> between 2<italic>λ<sub>0</sub></italic> and 5<italic>λ<sub>0</sub></italic> for any dataset? Please clarify what the suggested method is.</p><p><italic>Reviewer #2:</italic> </p><p>This paper describes a targeted dimensionality reduction approached, called seqNMF, to identify sequences in neural data. The approach is an improvement on previously published methods. The method will be of interest to its target audience. Also, the method is well described in straightforward terms, and the authors do a great job of describing how to use the method. They provide good examples, both from simulated data and multiple types of real data. They also give a good understanding of how the method can be tuned, such as using the <italic>λ</italic> value, to explore sequences with different levels of granularity or to test different ideas about the data. The paper is well written and is presented in a fair manner. I am therefore supportive of this paper and suggest publication.</p><p><italic>Reviewer #3:</italic> </p><p>Overview: This manuscript aims to address the problem of automated discovery of temporal sequences from neural data. The authors report on a modification (seqNMF) of the convolutional non-negative matrix factorization (convNMF) algorithm targeted to address purported issues in the base method. The fundamental issue that seqNMF attempts to address is the redundancy/replication of learned bases by the vanilla convNMF algorithm. The authors use extensive numerical studies to characterize the performance of their algorithm. The target biological datasets are neural activity (spike trains and calcium signals) from hippocampus and nucleus HVC of the zebra finch. This paper is generally well written, though it could be streamlined substantially to accelerate the reader through the manuscript. However, for reasons elaborated below, it is unclear how much of an advance the reported method is over the current state of the art in the field. Most importantly, the authors seem unaware of recent advances in the statistical-machine learning literature that aim at addressing the issue of non-redundant bases learning by NMF algorithms. Furthermore, the heuristic used for selection of the penalty hyperparameter is not adequately justified. As such, the impact/novelty of the proposed algorithm has not been evaluated or demonstrated: quantitative comparisons to the state-of-the-art is required for evaluation of proposed new analytic methods. While these issues could potentially be addressed, in my opinion, this work does not present a clear major improvement over existing methods and thus belongs in a much more specialized venue.</p><p>Major Critiques:</p><p>1) Insufficient comparison to state-of-the-art methods. The central issue that the seqNMF algorithm attempts to solve is &quot;To reduce the occurrence of redundant factors in convNMF […]&quot;. The authors identify three types of bases learning consistency errors. To address these errors, the authors augment an existing penalty on the reconstruction cost function to encourage uncorrelated bases and loadings. This is a reasonable strategy but is by no means the only approach. Indeed, as mentioned by the authors:</p><p>-Direct selection of the rank K vs. hyperparameter strength (<italic>λ</italic>)</p><p>-Use of a sparsity penalty (e.g., L1-regularization)</p><p>While these approaches are mentioned, and sometimes examined, the benefit of the proposed method over these approaches has not been demonstrated in either synthetic or neurobiological data. For example, if one were to attempt to optimize the number of bases (K) and the regularization strength (<italic>λ</italic>) in a sparse convNMF, how do the performance measures on the synthetic data sets with various noise levels stack up? Note that the results of Figure 2C,D in this regard are not at all convincing, as the rank K of the factorization of convNMF is not optimized: indeed, I find this figure to be misleading, as it is not an 'apples-to-apples' comparison (also, I believe there is a mislabeling of convNMF vs. seqNMF in this figure).</p><p>More seriously, the authors seem unaware of recent advances in the statistical-machine learning community which directly address the issue of learning non-redundant bases. For example, the 'stability NMF' algorithm (Wu et al., 2016) was recently successfully applied to extract biologically meaningful patterns from diverse data sets. More recently, the UoI-NMFcluster algorithm (Ubaru et al., 2018) has been shown to extract the precise generative bases, and assign sparse reconstruction weights to those bases, from noisy synthetic and neurobiological data. Both of these methods aim to select bases that are minimally correlated with each other. While these methods have not been applied to sequences per se, there is no conceptual reason why they could not be. For example, if one allows for a sliding cross-correlation, it is likely that both stability NMF and UoI-NMF could deal with temporal jitter. Can these methods be modified to provide even greater robustness for learning non-redundant bases for sequences?</p><p>In summary, the authors have not demonstrated the broad and robust improvement of the proposed algorithm relative to other methods in the field.</p><p>2) Method for selecting regularization parameters. In the seqNMF algorithm, the balance between reconstruction and un-correlated bases is modulated by the <italic>λ</italic> hyperparamter. The authors provide various heuristics for choosing <italic>λ</italic> to find the correct bases. The main heuristic used for the main figure use a heuristic of 2-5x of the <italic>λ</italic> value that gives the cross-over in normalized reconstruction vs. correlation costs (<italic>λ<sub>0</sub></italic>). The authors justify this as giving good results in the synthetic data sets and having a reasonable overlap with cross-validated reconstruction error. However, for many of the synthetic and real data sets, lambda0 also lies at the most sensitive part of the range, and small modulations lead to large changes in the trade-off of reconstruction vs. correlation. That is, the results are likely to be very sensitive to this selection. Furthermore, the selected values often result in very poor data reconstruction (e.g., Figure 6C,D and Figure 7B,F). As quantified, this is a big problem for 'interpretability', as it is very hard to interpret the results of a method that does not have good data prediction quality. In science, ground-truth is not known, and a major metric we have to quantitatively evaluate methods is reconstruction error/parsimony. If the authors could show that, something like cross-validated Bayesian Information Criterion, was optimized for the selected values of <italic>λ</italic>, that would be much more convincing.</p><p>In summary, the heuristic method used to select the regularization strength, which is a major component of the reported work, is not sufficiently justified from a statistical-machine learning perspective.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.38471.041</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors' note: the authors’ plan for revisions was approved and the authors made a formal revised submission.]</p><disp-quote content-type="editor-comment"><p>While the reviewers were generally enthusiastic about the work, major concerns were brought up that raise the question of whether this work is appropriate for the general readership of eLife. All reviewers agreed that these major concerns require essential revisions and thus would like to see an action plan that addresses these concerns before they issue a formal decision. In particular, the reviewers were not convinced that this method constitutes a major advance over current state-of-the-art methods in the field. Also, the selection of hyperparameters was not convincingly justified. The reviews are included below in their entirety.</p></disp-quote><p>We thank the reviewers for their thorough and insightful reading of our manuscript. We believe their criticisms will lead to a substantial improvement in our work. Here we address these criticisms and outline improvements to the paper.</p><p>We would like to emphasize that the aim of this paper is to provide the neuroscience community with a broadly useful and largely automated method for identifying temporal patterns (which we refer to as “sequences”) in neural data. Such methods do not currently exist.</p><p>Thus, we think that, for the neuroscience community, seqNMF is a major advance and fills an important gap. Sequential firing patterns have appeared in a large range of important studies in different brain regions and different behaviors (Hahnloser, 2002; Harvey, 2012; Okubo, 2015; Fujisawa, 2008; Pastalkova, 2008; MacDonald, 2011). These studies are all from top neuroscience labs with strong quantitative expertise, and yet in each of these studies, the sequences were extracted manually by alignment of neural activity with external behavioral or task events. This suggests a profound lack of existing methods in the field for automatic extraction of sequential structure in neuronal data.</p><p>Towards our goal of making our method practical and easy-to-adopt, we have intentionally worked to build upon core existing approaches from the machine learning community. We have attempted to write the paper in an accessible and pedagogical format so that the broader neuroscience community can easily apply the methods described, using the easy-to-use code we have posted publicly.</p><p>Our goal has therefore been to bring together a variety of statistical approaches: Convolutional matrix factorization, regularization strategies to encourage orthogonal factors, cross-validation, and now (thanks to reviewer 3) stability-based measures for determining model hyperparameters. Many of these individual concepts can be found by digging through different pieces of previous literature; however, repurposing and refining models for practical applications has a strong history in machine learning and computational neuroscience and can constitute a significant advance for the field. To make this clearer in our current version of the manuscript we have used seqNMF only to refer to a toolbox containing all of our contributions. The penalty term previously referred to as seqNMF is not referred to as the cross-orthogonality (x-ortho) penalty term. Thus, even if our paper did not represent a technical advance in statistical methodology, our careful analysis of the performance of regularized convNMF on different types of synthetic and real neural (and behavioral) data represents a significant advance of interest to the broad readership of <italic>eLife</italic>.</p><p>That said, we still take very seriously the concerns of Reviewer 3 about the novelty of our method. We thank the reviewer for pointing out the richness of recent advances in the area of preventing bases learning consistency errors, some of which we were unaware of. Thanks to the suggestion of the reviewer we have now incorporated some of these recent advances in our method.</p><p>In short, we would prefer to think of our contribution not as a detailed benchmarking of different methods of regularizing convolutional NMF, but rather as a toolkit for neuroscientists (who may not be statistics experts) to automatically and reliably extract sequences from neural data, to test their statistical significance, and to flexibly fine-tune the structure of the factorization. Finally, we have endeavored to present these ideas within a pedagogical framework so that readers can readily understand, and potentially build on the method.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>This paper introduces a matrix factorization-based method, seqNMF, for extracting temporal sequences from high-dimensional data. The authors convincingly demonstrate that seqNMF performs well in artificial and real datasets. I believe seqNMF will be a useful tool for a wide range of applications. After its appearance at COSYNE, the tool has already created considerable excitement in the computational and systems neuroscience community.</p><p>I found the paper to be very well written and the results clearly presented. However, some points need improvement:</p><p>1) While Type 1, 2, and 3 errors are clearly explained, I thought better naming could improve the readability of the paper a lot.</p></disp-quote><p>We attempted to change the error types to: duplication, fragmentation and duplication with independence but felt that this made the paper more difficult to read. Perhaps there are other better names which would improve readability however we have chosen to keep Type 1, 2 and 3 for the time being.</p><disp-quote content-type="editor-comment"><p>2) Building into equation 6, the authors discuss two alternative regularizers, and claim that they would fail at preventing Type 1, 2 and 3 errors simultaneously. While the arguments are clear, an experimental demonstration would be more convincing.</p></disp-quote><p>We have added a supplementary figure (Figure 1—figure supplement 1) illustrating each type of error on reconstructions of synthetic data. We have also quantified the rates of occurrence of each error type for the different regularizers described in the paper. This analysis illustrates the pattern described in the text.</p><p>In addition, we have added an appendix analyzing how the penalty used in seqNMF relates to traditional regularizers.</p><disp-quote content-type="editor-comment"><p>3) In subsection “SeqNMF: A constrained convolutional non-negative matrix factorization”, authors call ||M||<sub>1,i≠</sub>j a norm on M, which technically isn't correct (Under this definition, a diagonal matrix would have a zero norm).</p></disp-quote><p>We changed the text to refer to this as a seminorm.</p><disp-quote content-type="editor-comment"><p>4) I have trouble with understanding the first method of choosing λ. Is this method only applicable to data for which ground-truth is known? Or are the authors suggesting to choose λ between 2 λ<sub>0</sub> and 5λ<sub>0</sub> for any dataset? Please clarify what the suggested method is.</p></disp-quote><p>The reviewer’s interpretation of our method is correct; we are suggesting that the choice of <italic>λ</italic> between 2<italic>λ<sub>0</sub></italic> and 5<italic>λ<sub>0</sub></italic> is a good starting point for any dataset. This range is based on the performance of the algorithm on synthetic datasets for which the ground truth is known.</p><p>Further clarification: The procedure for choosing <italic>λ</italic> is motivated by the common-sense observation that <italic>λ</italic> provides a way to weight the priority given to reconstruction error vs redundancy (correlation cost). In the datasets we tested, values between 2<italic>λ<sub>0</sub></italic> and 5<italic>λ<sub>0</sub></italic> tended to yield reasonable results, reducing redundancy without sacrificing too much reconstruction error. In certain applications, the desired tradeoff may be different (e.g., when the main priority is reconstruction error, and some redundancy is tolerable), in which case the intuition behind our procedure may be useful for selecting <italic>λ</italic> (e.g., choose lower <italic>λ</italic>). We have made this intuition clearer in the text, so that future users of our algorithm will have a better idea of the tradeoffs being made when choosing different values of <italic>λ</italic>. That being said, based on our synthetic data and examining the results with different values for the real data, we suggest starting with a λ between 2 and 5 for any data set. In the cases we have considered where the ground-truth is known, we have shown that this strategy produces results that agree with the ground-truth (Figure 4). Furthermore, these results were relatively insensitive to the precise choice of <italic>λ</italic>. That is, ground-truth factors could be recovered across a wide range of <italic>λ</italic> (Figures 4 and Figure 2—figure supplement 2; also compare Figure 3 with Figure 3—figure supplement 1).</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>Overview: This manuscript aims to address the problem of automated discovery of temporal sequences from neural data. The authors report on a modification (seqNMF) of the convolutional non-negative matrix factorization (convNMF) algorithm targeted to address purported issues in the base method. The fundamental issue that seqNMF attempts to address is the redundancy/replication of learned bases by the vanilla convNMF algorithm. The authors use extensive numerical studies to characterize the performance of their algorithm. The target biological datasets are neural activity (spike trains and calcium signals) from hippocampus and nucleus HVC of the zebra finch. This paper is generally well written, though it could be streamlined substantially to accelerate the reader through the manuscript. However, for reasons elaborated below, it is unclear how much of an advance the reported method is over the current state of the art in the field. Most importantly, the authors seem unaware of recent advances in the statistical-machine learning literature that aim at addressing the issue of non-redundant bases learning by NMF algorithms. Furthermore, the heuristic used for selection of the penalty hyperparameter is not adequately justified. As such, the impact/novelty of the proposed algorithm has not been evaluated or demonstrated: quantitative comparisons to the state-of-the-art is required for evaluation of proposed new analytic methods. While these issues could potentially be addressed, in my opinion, this work does not present a clear major improvement over existing methods and thus belongs in a much more specialized venue.</p></disp-quote><p>We thank the reviewer for a thorough reading and thoughtful response to our manuscript.</p><p>The central aim of our paper is to present a simple and widely applicable toolset for neuroscientists to detect sequences in their data. Reducing the occurrence of redundant factors in convNMF was an important part of this aim, but not an endpoint in itself. Thus, we looked to the statistical-machine learning literature for inspiration, not for competition. (Indeed, we thank the reviewer for pointing out advances in this field that have turned out to be very effective.) We purposely targeted <italic>eLife</italic>, rather than a computer science or machine learning venue, because the appropriate audience for this work is the neuroscience community, which has a long-standing interest in neural sequences from both experimental and theoretical perspectives (e.g. Hahnloser, 2002; Harvey, 2012; Okubo, 2015; Fujisawa, 2008; Pastalkova, 2008; MacDonald, 2011; Brody, 1999; Mokeichev, 2007; Schrader, 2008; Gerstein, 2012; Torre, 2016; Russo, 2017’ Grossberger, 2018; Quaglio, 2018).</p><p>Thus, our contribution is less about a particular advance in constrained optimization, but rather about adapting and applying a collection of statistical tools to the problem of discovering interpretable sequential structure in neuroscience data. These include: the use of convolutional NMF on a variety of neural and behavioral datasets, constraints to reduce redundant factorizations, methods to select hyperparameters, estimation of statistical significance, quantification of robustness to different common noise types, and regularization to shape the structure of factorizations (i.e. parts vs events).</p><p>Despite the interest in neural sequences, factorization-based approaches have been underutilized in the field. This is likely because the appropriate set of statistical tools had not yet been assembled, adapted and applied to neuroscience data in a convincing manner. We believe that our work represents such an advance.</p><disp-quote content-type="editor-comment"><p>Major Critiques:</p><p>1) Insufficient comparison to state-of-the-art methods. The central issue that the seqNMF algorithm attempts to solve is &quot;To reduce the occurrence of redundant factors in convNMF […]&quot;. The authors identify three types of bases learning consistency errors. To address these errors, the authors augment an existing penalty on the reconstruction cost function to encourage uncorrelated bases and loadings. This is a reasonable strategy but is by no means the only approach. Indeed, as mentioned by the authors:</p><p>-Direct selection of the rank K vs. hyperparameter strength (λ)</p><p>-Use of a sparsity penalty (e.g., L1-regularization)</p><p>While these approaches are mentioned, and sometimes examined, the benefit of the proposed method over these approaches has not been demonstrated in either synthetic or neurobiological data. For example, if one were to attempt to optimize the number of bases (K) and the regularization strength (λ) in a sparse convNMF, how do the performance measures on the synthetic data sets with various noise levels stack up?</p></disp-quote><p>We agree with the reviewer that it would be useful to compare our approach to the others mentioned (i.e. direct selection of rank K, or the use of a sparsity penalty) and we have added additional results exploring these approaches.</p><p>Direct selection of K</p><p>We had previously shown that direct selection of K can sometimes be carried out using cross-validated error performance on held-out data. However, following the reviewer’s suggestion of extending stability NMF (and the ​<italic>diss</italic>​ metric, see below) to the convolutional case has yielded significantly better performance for directly estimating the rank K. We have therefore added a new main figure (Figure 5) and a new supplementary figure (Figure 5—figure supplement 1) to the paper illustrating the stability NMF approach and have modified the text to present this as an additional, complementary method for minimizing bases consistency errors.</p><p>L1-sparsity</p><p>One of the advantages of the cross orthogonality penalty is that it consists of only a single term to penalize correlations between different factors, and thus requires the determination of only a single hyperparameter (<italic>λ</italic>). This contrasts with a different potential approach to minimize redundant factorizations, namely incorporating a sparsity constraint on W and H of the form <italic>λ<sub>w</sub></italic>||W||<sub>1</sub>+ <italic>λ<sub>h</sub></italic>||H||<sub>1</sub>. We first wanted to understand the dependence of the sparsity approach on the hyperparameters (<italic>λ<sub>h</sub></italic> and <italic>λ<sub>w</sub></italic>). Performance was quantified by measuring both the similarity to ground-truth and the probability of obtaining the correct number of significant factors using synthetic data sets with intermediate levels of each of the four noise types. We found that the similarity to ground-truth and probability of obtaining the correct number of significant factors varies in a complex manner as a function of the two <italic>λ</italic> parameters (​Figure 4—figure supplement 4).</p><p>We next wondered how the ‘optimal’ performance of the sparsity approach (at the best choice of <italic>λ<sub>h</sub></italic> and <italic>λ<sub>w</sub></italic>) compares with the ‘optimal’ performance of the x-ortho approach (at the best choice of the cross orthogonality <italic>λ</italic>). [See below for more information on how this was done.] Having chosen the best <italic>λ</italic> for L1-sparsity and cross orthogonality for each noise condition, we ran the factorization for each method and quantified the distribution of performance (again using similarity and the number of significant factors). We found that L1 sparsity constraint yielded an overall performance quite similar to the cross orthogonality regularizer (Figure 4—figure supplement 5). Interestingly though, there are some consistent differences in the performance of these two constraints depending on the noise type: the x-ortho approach performs slightly better with warping and participation noise, while L1 sparsity performs slightly better with jitter and additive noise (Figure 4—figure supplement 5).</p><p>Given these findings, we feel that the x-ortho penalty represents a practical advance over the sparsity approach. The two methods perform similarly once the optimal values of the hyperparameters is determined, but in cases where the ground truth is not known, it will be simpler to find the optimal value of a single parameter (by using the seqNMF method for selecting <italic>λ</italic>), than two parameters for sparsity. Of course, it may be also possible to develop a method to find suitable values of <italic>λ<sub>h</sub></italic> and <italic>λ<sub>w</sub></italic> for the sparsity approach, but given the strong performance of the x-ortho penalty term, there is little motivation to develop such a method.</p><p>More detailed methods for L1 sparsity comparison</p><p>Optimal values of <italic>λ<sub>h</sub></italic> and <italic>λ<sub>w</sub></italic> for the sparsity method were selected by computing the composite performance score at each value of <italic>λ<sub>h</sub></italic> and <italic>λ<sub>w</sub></italic> for each noise type (see arrow in Figure 4—figure supplement 4A). The composite performance is the same as that used in Figure 4F, namely the element-wise product of the similarity to ground-truth and the fraction of fits with the correct number of factors. The peak value of this composite performance score yielded a pair of lambdas at which the sparsity penalties gave the best performance on this dataset. For the comparison, the same procedure was used to select the optimal <italic>λ</italic> for the x-ortho penalty in each noise condition. The performance of sparsity and x-ortho were each evaluated at their optimal lambdas by fitting the data 100 times from random initial conditions.</p><disp-quote content-type="editor-comment"><p>Note that the results of Figure 2C,D in this regard are not at all convincing, as the rank K of the factorization of convNMF is not optimized: indeed, I find this figure to be misleading, as it is not an 'apples-to-apples' comparison (also, I believe there is a mislabeling of convNMF vs. seqNMF in this figure).</p></disp-quote><p>With regard to Figure 2, we apologize that our intention was not clear. Our intention was simply to illustrate the effect of the regularizer, rather than to formally compare to convNMF as a competing method. This is now clarified in the text.</p><disp-quote content-type="editor-comment"><p>More seriously, the authors seem unaware of recent advances in the statistical-machine learning community which directly address the issue of learning non-redundant bases. For example, the 'stability NMF' algorithm (Wu et al., 2016) was recently successfully applied to extract biologically meaningful patterns from diverse data sets. More recently, the UoI-NMFcluster algorithm (Ubaru et al., 2018) has been shown to extract the precise generative bases, and assign sparse reconstruction weights to those bases, from noisy synthetic and neurobiological data. Both of these methods aim to select bases that are minimally correlated with each other. While these methods have not been applied to sequences per se, there is no conceptual reason why they could not be. For example, if one allows for a sliding cross-correlation, it is likely that both stability NMF and UoI-NMF could deal with temporal jitter. Can these methods be modified to provide even greater robustness for learning non-redundant bases for sequences?</p><p>In summary, the authors have not demonstrated the broad and robust improvement of the proposed algorithm relative to other methods in the field.</p></disp-quote><p>We thank the reviewer for these suggestions for alternative possible methods to constrain convNMF factorizations, and to reduce bases learning consistency errors. Indeed, we were not aware of these recent exciting advances in the field and have explored the application of both stability NMF and UoI to convNMF.</p><p>Stability NMF</p><p>We have successfully adapted stability NMF (Wu et al., 2016) using the dissimilarity (diss) metric to account for temporally-shifted correlations. As the reviewer pointed out, different runs of convNMF may produce factors which are shifted in time (in both H and W) and thus have artificially low correlations, confounding the stability measure described in Wu et al. In our initial exploration of this metric, we observed that smoothing the factors worked sometimes but found that a much more robust way of overcoming this misalignment problem is to apply the stability analysis to the reconstructions of each individual factor (that is, w<sub>k</sub>⊛ h<sub>k</sub>). Because this is a more reliable method of optimizing K for convNMF than the cross-validation approach we developed previously, we have added a new main figure (Figure 5) and a new supplementary figure (Figure 5—figure supplement 1) and have incorporated this into our Results section “Strategy for choosing K rather than choosing <italic>λ</italic>?”</p><p>Union of Intersections</p><p>We next wondered if the approach of UoI-NMF could be adapted to improve the performance of seqNMF. To apply this method to the convolutional case we first had to make several adjustments to the UoI algorithm. First, we simplified things by skipping the first step of estimating the appropriate rank using stability NMF and simply chose K to be the ground-truth rank of our synthetic data. Because convolutional NMF relies on structure across observations in time it is not possible to bootstrap our data in the conventional way by taking random samples of observations. Instead we took the approach that we used for cross-validation in Figure 5—figure supplement 2; we randomly held out individual bins from the dataset during fitting and defined the remaining data as our bootstrapped sample. Finally, rather than solving a non-negative least squares problem, we took advantage of the multiplicative update rules to constrain our factorizations. After taking the intersections of our factors from many different bootstrapped fits we defined an intersection mask where an element was defined as 1 if it was a member of the intersection and zero otherwise. We then used this mask as an initialization of our factorization, and the multiplicative nature of the updates ensures that elements which are initialized as 0 remain fixed at zero throughout the optimization thus enforcing the intersection.</p><p>Before applying our implementation of UoI-NMF to convNMF, we first tested that our changes to the UoI algorithm did not negatively impact its ability to robustly identify bases in an NMF model (<xref ref-type="fig" rid="respfig1">Author response image 1</xref>). We generated a ground-truth dataset using synchronous patterns and compared the ability of UoI-NMF and NMF to extract the ground-truth bases. As expected, our adapted implementation of UoI-NMF extracted factors that were much more similar to the ground-truth set than NMF for a range of noise conditions (<xref ref-type="fig" rid="respfig1">Author response image 1</xref>).</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.38471.036</object-id><label>Author response image 1.</label><caption><title>Validation of our adaptations of UoI-NMF.</title><p>Similarity to ground-truth as a function of additive noise level for UoI-NMF factorizations and NMF factorizations using the algorithmic changes detailed above.</p><p>Note that UoI-NMF extracts bases which are closer to the ground-truth patterns than regular NMF.</p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-38471-resp-fig1-v2"/></fig><p>After ensuring that our adaptations of the UoI-NMF algorithm still yielded good results in the NMF case, we tested it on convNMF, but found that it gave mixed results. In the best cases, UoI-convNMF returned factors which were a 5-10% better match to the ground-truth factors than regular convNMF, but in some conditions, UoI-convNMF returned factors which were a 5-10% worse match, and our attempts to further adapt the algorithm or change parameters sometimes yielded even worse results. We believe that this is primarily due to the challenge that convNMF factors on individual runs can be shifted in time relative to one another, as mentioned above. This could, in principle, cause problems during the intersection step of the algorithm, because misaligned bases will tend not to overlap. In summary, the UoI approach is a promising way to address bases correlations in convNMF, but we believe that the additional extensions necessary for good performance in the convNMF model are beyond the scope of our paper. In our Discussion section, we have added a description of the UoI-NMF method, highlighting it as a promising possible extension for discovering robust and sparse sequential bases.</p><disp-quote content-type="editor-comment"><p>2) Method for selecting regularization parameters. In the seqNMF algorithm, the balance between reconstruction and un-correlated bases is modulated by the λ hyperparamter. The authors provide various heuristics for choosing λ to find the correct bases. The main heuristic used for the main figure use a heuristic of 2-5x of the λ value that gives the cross-over in normalized reconstruction vs. correlation costs (λ<sub>0</sub>). The authors justify this as giving good results in the synthetic data sets and having a reasonable overlap with cross-validated reconstruction error. However, for many of the synthetic and real data sets, lambda0 also lies at the most sensitive part of the range, and small modulations lead to large changes in the trade-off of reconstruction vs. correlation. That is, the results are likely to be very sensitive to this selection. Furthermore, the selected values often result in very poor data reconstruction (e.g., Figure 6C,D and Figure 7B,F). As quantified, this is a big problem for 'interpretability', as it is very hard to interpret the results of a method that does not have good data prediction quality. In science, ground-truth is not known, and a major metric we have to quantitatively evaluate methods is reconstruction error/parsimony. If the authors could show that, something like cross-validated Bayesian Information Criterion, was optimized for the selected values of λ, that would be much more convincing.</p><p>In summary, the heuristic method used to select the regularization strength, which is a major component of the reported work, is not sufficiently justified from a statistical-machine learning perspective.</p></disp-quote><p>We agree that our method for choosing <italic>λ</italic> is heuristic and is not well motivated formally from a machine learning perspective. However, we have also confirmed that our heuristic method agrees with the results of a cross-validation procedure, as presented in Figure 5—figure supplement 2. The reason we kept the heuristic in the main text is because it is more robust in low-noise cases (where overfitting is minimal) and is more interpretable for some noise types such as warping. In our action plan we stated that we would explore the use of cross-validated reconstruction error in the selection of <italic>λ</italic>, but we were unable to find any methods that performed better than our heuristic. Furthermore, our analysis of synthetic datasets reveals that the performance on a given dataset is not very sensitive to the choice of <italic>λ</italic> in the range we have described. In fact, in most cases, we recover the correct number of factors over a range of <italic>λ</italic> spanning nearly an order of magnitude. We have quantified this in a new figure (Figure 4—figure supplement 1).</p><p>The reviewer suggests that the optimal value of <italic>λ</italic> often results in very poor data reconstruction. This interpretation of Figure 6C,D and 7B,F, is incorrect. The plots of reconstruction that the reviewer refers to are normalized to go between 0 and 1, and the overall reconstruction error cannot be interpreted from these plots. At low <italic>λ</italic>, reconstruction error reaches a minimum which corresponds to the unregularized convNMF reconstruction error. In contrast, at high <italic>λ</italic>, reconstruction error saturates at a value that is determined by the amount of variance which can be explained by a single factor (often quite a lot). This is because the x-ortho penalty does not affect K=1 factorizations. For the case of single sequential pattern embedded in high noise, the low <italic>λ</italic> region reflects massive over-fitting of noise, whereas the saturated region at high lambdas will represent ideal performance (even though it is the “worst” reconstruction error of any range of <italic>λ</italic> values). For the case of multiple sequences embedded in noise, the leftmost region still represents massive over-fitting while the right-most region represents the error incurred by incorrectly using a 1 factor model. Thus, by definition the best parameter value must lie on the steep region of the reconstruction error as factorizations move from an essentially un-penalized 20 factor factorization to an un-penalized 1 factor factorization. In other words, reconstruction error is large because the data are noisy, and seqNMF is fitting only the repeatable sequences, and not the noise. Furthermore, values of <italic>λ</italic> that produce better reconstruction error do so by overfitting the data, that is, producing factors with lower similarity to ground-truth (Figure 4E and 4K).</p></body></sub-article></article>