<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">51784</article-id><article-id pub-id-type="doi">10.7554/eLife.51784</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-156734"><name><surname>Di Liberto</surname><given-names>Giovanni M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7361-0980</contrib-id><email>diliberg@tcd.ie</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-158782"><name><surname>Pelofi</surname><given-names>Claire</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-158783"><name><surname>Bianco</surname><given-names>Roberta</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-158784"><name><surname>Patel</surname><given-names>Prachi</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-57873"><name><surname>Mehta</surname><given-names>Ashesh D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7293-1101</contrib-id><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-156548"><name><surname>Herrero</surname><given-names>Jose L</given-names></name><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-158785"><name><surname>de Cheveigné</surname><given-names>Alain</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-44371"><name><surname>Shamma</surname><given-names>Shihab</given-names></name><email>sas@isr.umd.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-62020"><name><surname>Mesgarani</surname><given-names>Nima</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2987-759X</contrib-id><email>nima@ee.columbia.edu</email><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Laboratoire des systèmes perceptifs, Département d’études cognitives, École normale supérieure, PSL University, CNRS</institution><addr-line><named-content content-type="city">75005 Paris</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution>Department of Psychology, New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Institut de Neurosciences des Système, UMR S 1106, INSERM, Aix Marseille Université</institution><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff><aff id="aff4"><label>4</label><institution>UCL Ear Institute</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff5"><label>5</label><institution>Department of Electrical Engineering, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution>Mortimer B Zuckerman Mind Brain Behavior Institute, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution>Department of Neurosurgery, Zucker School of Medicine at Hofstra/Northwell</institution><addr-line><named-content content-type="city">Manhasset</named-content></addr-line><country>United States</country></aff><aff id="aff8"><label>8</label><institution>Feinstein Institute of Medical Research, Northwell Health</institution><addr-line><named-content content-type="city">Manhasset</named-content></addr-line><country>United States</country></aff><aff id="aff9"><label>9</label><institution>Institute for Systems Research, Electrical and Computer Engineering, University of Maryland</institution><addr-line><named-content content-type="city">College Park</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelle</surname><given-names>Jonathan Erik</given-names></name><role>Reviewing Editor</role><aff><institution>Washington University in St. Louis</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>03</day><month>03</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e51784</elocation-id><history><date date-type="received" iso-8601-date="2019-09-11"><day>11</day><month>09</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-01-20"><day>20</day><month>01</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Di Liberto et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Di Liberto et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-51784-v1.pdf"/><abstract><p>Humans engagement in music rests on underlying elements such as the listeners’ cultural background and interest in music. These factors modulate how listeners anticipate musical events, a process inducing instantaneous neural responses as the music confronts these expectations. Measuring such neural correlates would represent a direct window into high-level brain processing. Here we recorded cortical signals as participants listened to Bach melodies. We assessed the relative contributions of acoustic <italic>versus</italic> melodic components of the music to the neural signal. Melodic features included information on pitch progressions and their tempo, which were extracted from a predictive model of musical structure based on Markov chains. We related the music to brain activity with temporal response functions demonstrating, for the first time, distinct cortical encoding of pitch and note-onset expectations during naturalistic music listening. This encoding was most pronounced at response latencies up to 350 ms, and in both <italic>planum temporale</italic> and <italic>Heschl’s gyrus</italic>.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cortical signals</kwd><kwd>sensory</kwd><kwd>music</kwd><kwd>expectations</kwd><kwd>pitch</kwd><kwd>markov model</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>787836</award-id><principal-award-recipient><name><surname>Shamma</surname><given-names>Shihab</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010669</institution-id><institution>H2020 LEIT Information and Communication Technologies</institution></institution-wrap></funding-source><award-id>644732</award-id><principal-award-recipient><name><surname>de Cheveigné</surname><given-names>Alain</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NIMH MH114166-01</award-id><principal-award-recipient><name><surname>Mehta</surname><given-names>Ashesh D</given-names></name><name><surname>Mesgarani</surname><given-names>Nima</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Computational models of musical structure reveal cortical encoding of pitch and rhythm expectations during naturalistic music listening.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Experiencing music as a listener, performer, or a composer is an active process that engages perceptual and cognitive faculties, endowing the experience with memories and emotion (<xref ref-type="bibr" rid="bib63">Koelsch, 2014</xref>). Through this active auditory engagement, humans analyze and comprehend complex musical scenes by invoking the cultural norms of music, segregating sound mixtures, and marshaling expectations and anticipation (<xref ref-type="bibr" rid="bib54">Huron, 2006</xref>). However, this process rests on the ‘structural knowledge’ that listeners acquire and encode through frequent exposure to music in their daily lives. Ultimately, this knowledge is thought to shape listeners’ expectations and to determine what constitutes a ‘familiar’ musical style that they are likely to understand and appreciate (<xref ref-type="bibr" rid="bib86">Morrison et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Hannon et al., 2012</xref>; <xref ref-type="bibr" rid="bib102">Pearce, 2018</xref>). There is convincing evidence that musical structures can be learnt through the passive exposure of music in everyday life (<xref ref-type="bibr" rid="bib7">Bigand and Poulin-Charronnat, 2006</xref>; <xref ref-type="bibr" rid="bib111">Rohrmeier et al., 2011</xref>), a phenomenon that was included in several models musical learning. The connectionist model from Tillman and colleagues, which simulated implicit learning of pitch structures occurring in Western music, was shown to successfully predict behavioral and neurophysiological results on music perception (<xref ref-type="bibr" rid="bib133">Tillmann et al., 2000</xref>). Further developments led to models that accurately reflect listeners’ expectations on each upcoming note by considering both the listener’s musical background (long-term model) and proximal knowledge (short-term model) (<xref ref-type="bibr" rid="bib99">Pearce, 2005</xref>; <xref ref-type="bibr" rid="bib54">Huron, 2006</xref>).</p><p>A similar learning process has been demonstrated in many domains of human learning (e.g., language; <xref ref-type="bibr" rid="bib114">Saffran et al., 1997</xref>; <xref ref-type="bibr" rid="bib136">Toro et al., 2005</xref>; <xref ref-type="bibr" rid="bib39">Finn et al., 2014</xref>), as well as in animals that must learn their species-specific songs and vocalizations (<xref ref-type="bibr" rid="bib142">Woolley, 2012</xref>). Theoretically, this is often conceptualized as learning the statistical regularities of the sensory environment (<xref ref-type="bibr" rid="bib113">Romberg and Saffran, 2010</xref>; <xref ref-type="bibr" rid="bib37">Erickson and Thiessen, 2015</xref>; <xref ref-type="bibr" rid="bib9">Bretan et al., 2017</xref>; <xref ref-type="bibr" rid="bib125">Skerritt-Davis and Elhilali, 2018</xref>) so as to predict upcoming events, in a process referred to as ‘statistical learning’. Supporting evidence is that failed predictions due to deviations in the sensory sequence produce measurable brain activations that have been associated with the detection and strength of these irregularities, that is prediction error (<xref ref-type="bibr" rid="bib140">Vuust et al., 2012</xref>; <xref ref-type="bibr" rid="bib19">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib84">Moldwin et al., 2017</xref>; <xref ref-type="bibr" rid="bib94">Omigie et al., 2019</xref>; <xref ref-type="bibr" rid="bib107">Quiroga-Martinez et al., 2019b</xref>; <xref ref-type="bibr" rid="bib106">Quiroga-Martinez et al., 2019a</xref>), and with learning (<xref ref-type="bibr" rid="bib129">Storkel and Rogers, 2000</xref>; <xref ref-type="bibr" rid="bib1">Attaheri et al., 2015</xref>; <xref ref-type="bibr" rid="bib105">Qi et al., 2017</xref>).</p><p>Decades of research have established that listeners have strong and well-defined musical expectations (<xref ref-type="bibr" rid="bib121">Schmuckler, 1989</xref>; <xref ref-type="bibr" rid="bib23">Cuddy and Lunney, 1995</xref>; <xref ref-type="bibr" rid="bib77">Margulis, 2005</xref>; <xref ref-type="bibr" rid="bib85">Morgan et al., 2019</xref>) which depend on a person’s musical culture (<xref ref-type="bibr" rid="bib14">Carlsen, 1981</xref>; <xref ref-type="bibr" rid="bib56">Kessler et al., 1984</xref>; <xref ref-type="bibr" rid="bib66">Krumhansl et al., 2000</xref>; <xref ref-type="bibr" rid="bib36">Eerola et al., 2009</xref>). Neurophysiology studies demonstrated that violation of music expectations elicits consistent event-related potentials (ERPs), such as the mismatch negativity (MMN) or the early right-anterior negativity (ERAN), which typically emerges between 100 and 250 ms after the onset of the violation (<xref ref-type="bibr" rid="bib62">Koelsch, 2009</xref>; <xref ref-type="bibr" rid="bib140">Vuust et al., 2012</xref>). Such brain responses have been measured for a range of violations of auditory regularities, including out-of-key notes embedded in chords (e.g. Neapolitan sixth in <xref ref-type="bibr" rid="bib58">Koelsch et al., 2000</xref>), unlikely chords (e.g. double dominant in <xref ref-type="bibr" rid="bib61">Koelsch et al., 2007</xref>), and single tones (<xref ref-type="bibr" rid="bib82">Miranda and Ullman, 2007</xref>; <xref ref-type="bibr" rid="bib93">Omigie et al., 2013</xref>; <xref ref-type="bibr" rid="bib94">Omigie et al., 2019</xref>). These physiological markers enabled researchers to investigate the encoding of music expectations during development (<xref ref-type="bibr" rid="bib60">Koelsch et al., 2003</xref>) as well as the impact of attention (<xref ref-type="bibr" rid="bib73">Loui et al., 2005</xref>), short-term context (<xref ref-type="bibr" rid="bib64">Koelsch and Jentschke, 2008</xref>), and musical experience (<xref ref-type="bibr" rid="bib59">Koelsch et al., 2002</xref>) on music perception.</p><p>ERP studies (<xref ref-type="bibr" rid="bib4">Besson and Macar, 1987</xref>; <xref ref-type="bibr" rid="bib96">Paller et al., 1992</xref>; <xref ref-type="bibr" rid="bib82">Miranda and Ullman, 2007</xref>; <xref ref-type="bibr" rid="bib101">Pearce et al., 2010b</xref>; <xref ref-type="bibr" rid="bib15">Carrus et al., 2013</xref>) often limit the range of expectations’ violations strength that are tested to severe violations because of the need for repeated stimulus presentations. One issue with this approach is that notes eliciting strong violations could be considered by the listener as production mistakes (e.g., a pianist playing the wrong note), thus the corresponding cortical correlates may characterise only a limited aspect of the neural underpinnings of melodic perception. In fact, music sequences induce a wide range of violation strengths (<xref ref-type="bibr" rid="bib104">Pearce and Wiggins, 2012</xref>) because the (valid) sequential events are not all equally likely (and hence not equally predictable), whether we consider note or chord sequences (<xref ref-type="bibr" rid="bib131">Temperley, 2008</xref>) in both classical music (<xref ref-type="bibr" rid="bib112">Rohrmeier and Cross, 2008</xref>) or more popular music genres (<xref ref-type="bibr" rid="bib132">Temperley and Clercq, 2013</xref>). Here, we study the brain responses to continuous naturalistic musical stimuli, thus allowing us to explore the full range of expectations’ strengths. This study seeks to determine whether the neural response reflects this range of strengths by regressing the musical information with it.</p><p>Music is a complex, multi-layered signal that have structures allowing predictions of a variety of properties. One layer concerns the notes sequences forming melodies, which is a key structural aspect of music across musical styles (<xref ref-type="bibr" rid="bib109">Reck, 1997</xref>) and cultures (<xref ref-type="bibr" rid="bib35">Eerola, 2003</xref>; <xref ref-type="bibr" rid="bib103">Pearce and Wiggins, 2006</xref>). Pearce et al. designed a framework based on variable-order Markov models that learns statistics describing the temporal sequences in melodic sequences at various time-scales (IDyOM; <xref ref-type="bibr" rid="bib99">Pearce, 2005</xref>; <xref ref-type="bibr" rid="bib103">Pearce and Wiggins, 2006</xref>). This framework attempts to optimally predict the next note in a melodic sequence by combining predictions based on 1) long-term statistics learnt from a large corpus of western music and 2) short-term statistics from the previous notes of the current musical stream. In turn, this provides us with likelihood values for each note in a melody that have been shown to tightly mirror listeners’ expectations (<xref ref-type="bibr" rid="bib99">Pearce, 2005</xref>).</p><p>Here, we regressed these estimates with electrophysiological data to investigate the impact of expectations on auditory perception and cortical processing. We recorded scalp electroencephalography (EEG; 20 subjects; about 1 hr and 15 min of data) signals and invasive electrocorticography (ECoG; three patients; about 25 min of data each) signals as participants listened to monophonic piano music from Bach that was generated from MIDI scores (see Materials and methods). According to the predictive coding theory (<xref ref-type="bibr" rid="bib42">Friston and Kiebel, 2009</xref>; <xref ref-type="bibr" rid="bib19">Clark, 2013</xref>), cortical signals partly reflect the mismatch between a participant’s prediction and the actual sensory input. If this is the case, less expected musical notes should produce relatively stronger cortical responses. We expected individuals with musical expertise to internally generate predictions on a next note that more closely relate the ones of a specialized statistical model than non-musicians. We tested this hypothesis on our EEG dataset, which was recorded from both non-musicians and expert pianists, and investigated specific details of the cortical response using our ECoG dataset.</p><p>Our stimuli were regular musical streams, rather than artificially-constructed repeated patterns such as those usually utilized for the induction and detection of mismatched negativity (MMN) responses (<xref ref-type="bibr" rid="bib43">Garrido et al., 2009</xref>; <xref ref-type="bibr" rid="bib19">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib40">Fishman, 2014</xref>; <xref ref-type="bibr" rid="bib71">Lecaignard et al., 2015</xref>; <xref ref-type="bibr" rid="bib127">Southwell and Chait, 2018</xref>). Musical streams are imbued with melodic events that routinely violates listeners’ expectation to some degree (<xref ref-type="bibr" rid="bib104">Pearce and Wiggins, 2012</xref>; <xref ref-type="bibr" rid="bib117">Salimpoor et al., 2015</xref>). Here we sought to test the hypothesis that the listeners’ expectation violations produce cortical responses that change with the degree of the violations and that are measurable with EEG during naturalistic music listening. To establish the contribution of expectations (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) to the neural responses, we used multivariate ridge regression (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) to quantify how well the acoustic (<bold>A</bold>) factors (e.g., signal envelope and its derivative) and melodic expectation or surprise (<bold>M</bold>) factors (e.g., pitch and onset-timing) can predict the EEG and ECoG responses to music (<xref ref-type="bibr" rid="bib22">Crosse et al., 2016</xref>). Since the prediction quality is considered to be an estimate of how strongly a stimulus property is encoded in the EEG data (<xref ref-type="bibr" rid="bib27">Di Liberto et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Di Liberto et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">Brodbeck et al., 2018b</xref>; <xref ref-type="bibr" rid="bib126">Somers et al., 2019</xref>; <xref ref-type="bibr" rid="bib138">Verschueren et al., 2019</xref>), and since cortical signals are assumed to be modulated by the various <bold>A</bold> and <bold>M</bold> factors above, we consequently expected the combination of both the acoustic and surprise features to predict the neural responses better than either set of features alone (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Validating these hypotheses would therefore provide physiological support for the melodic expectations generated according to the statistical learning model.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>System identification framework for isolating neural correlates of melodic expectations.</title><p>(<bold>A</bold>) Music score of a segment of auditory stimulus, with its corresponding features (from bottom to top): acoustic envelope (Env), half-way rectified first derivative of the envelope (Env’), and the four melodic expectation features: entropy of note-onset (H<sub>o</sub>) and pitch (H<sub>p</sub>), surprise of note-onset (S<sub>o</sub>) and pitch (S<sub>p</sub>). (<bold>B</bold>) Regularized linear regression models were fit to optimally describe the mapping from stimulus features (Env in the example) to each EEG and ECoG channel. This approach, called the Temporal Response Function (TRF), allows us to investigate the spatio-temporal dynamics of the linear model by studying the regression weights for different EEG and ECoG channels and time-latencies. (<bold>C</bold>) TRFs were used to predict EEG and ECoG signals on unseen data by using only acoustic features (A) and a combination of acoustic and melodic expectation features (AM). We hypothesised that cortical signals encode melodic expectations, therefore we expected larger EEG and ECoG predictions for the combined feature-set AM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig1-v1.tif"/></fig><p>This work presents novel insights into the precise spatio-temporal dynamics of the integration of melodic expectations and sensory input during naturalistic musical listening. In turn, our results provide evidence of the neurophysiological validity of predictive statistical models of music structure. Crucially, we found distinct encoding of different melodic expectation features, (such as pitch and note onset) to the cortical responses to music.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Neural data were recorded from twenty healthy EEG participants and three ECoG epilepsy patients as they listened to monophonic excerpts of music from Bach sonatas and partitas that were synthesized with piano sound. The melodic expectation of each note was estimated from the musical score of the stimulus with IDyOM, the model for predictive statistical modelling of musical structure. Specifically, given a musical piece at time <italic>t<sub>0</sub>,</italic> the model estimates the likelihood of having a note with a particular pitch at time <italic>t<sub>0</sub></italic> given short-term information for <italic>t</italic> &lt; <italic>t<sub>0</sub></italic> and long-term information from a large corpus of Western music. Based on these estimates we calculated four measures (referred to as melodic features M, see Materials and methods for details) that capture distinct aspects of expectation and surprise at each new note within a melody: entropy of pitch (<italic>H<sub>p</sub></italic>), entropy of onset-time (<italic>H<sub>o</sub></italic>), surprise of pitch (<italic>S<sub>p</sub></italic>), and surprise of onset-time (<italic>S<sub>o</sub></italic>). The first two measures refer to the Shannon entropy at a particular position in a melody, before the musical note is observed. Intuitively, the entropy indicates the amount of uncertainty represented by the distribution of pitch and onset-time for the next note, where the most uncertain scenario is when all possible notes have equal likelihood and the least uncertain scenario is when the next note is known. The latter two measures refer to the <italic>inverse</italic> probability of occurrence of a particular note pitch or onset-time, with smaller values for more predictable transitions (see Materials and methods).</p><sec id="s2-1"><title>Melodic expectation encoding in low-rate cortical signals</title><p>In all of the analyses and results below, we focused on the EEG and ECoG responses in the low-rate bands between 1 and 8 Hz, filtering out the remainder of the bands (see Materials and methods; note that inclusion of rates down to 0.1 Hz and up to 30 Hz did not alter any of the results that follow). Because of potential interactions between the responses to the succession of notes (which would complicate the interpretation of the ERPs time-locked to note onsets), we began by utilizing a linear modelling framework known as the temporal response function (TRF) (<xref ref-type="bibr" rid="bib31">Ding and Simon, 2012a</xref>; <xref ref-type="bibr" rid="bib22">Crosse et al., 2016</xref>) as depicted in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. This approach 1) explicitly dissociates the effects of expectations from those due to changes in the acoustic envelope on the neural responses to music and 2) allows us to investigate neural responses to rapidly presented stimuli by accounting for the dependence among the sequences of input notes. Specifically, TRFs were derived by using ridge regression between suitably parameterized stimuli and their neural responses. These were then used to predict unseen EEG data (with leave-one-out cross-validation) based on either the acoustic properties alone (A predictions) or a combination of acoustics and melodic expectation features (AM predictions). The predictive models included time-lagged versions of the stimulus accounting for delays between the stimulus and corresponding neural response. The time-lag window was limited to [0, 350] ms as longer latencies had little impact on the predictive power of the TRF model (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and Materials and methods).</p><p>In <xref ref-type="fig" rid="fig2">Figure 2</xref> we illustrate the average (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) and individual (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) EEG prediction correlations for all subjects using either the A features alone (envelope and its derivative) or combined with the melodic expectation features, AM. The A correlations were significantly positive (p&lt;0.05, permutation test) for all subjects but one (S20), confirming that neural responses to monophonic music track the stimulus envelopes (henceforth ‘envelope tracking’). Crucially, AM correlations were significantly larger than A, implying that melodic expectations explained EEG variance that was not captured by acoustic information alone. Specifically, the average EEG prediction correlation over all electrodes was significantly larger for AM than for A both at the group level (<italic>r<sub>AM</sub></italic> &gt;<italic>r<sub>A</sub></italic>: permutation test, p&lt;10<sup>−6</sup>, <italic>d</italic> = 1.64; <xref ref-type="fig" rid="fig2">Figure 2A</xref>) and at the individual-subject level (16 out of 20 subjects, permutation test, p&lt;0.05; <xref ref-type="fig" rid="fig2">Figure 2B</xref>), and this difference was even larger when computed from selected single electrodes (e.g., Cz channel: <italic>d</italic> = 1.80). This supports the hypothesis that melodic expectation is directly reflected in the EEG responses to music.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Low-rate (1–8 Hz) cortical signals reflect melodic expectations.</title><p>Scalp EEG data were recorded while participants listened to monophonic music. Forward ridge regression models were fit to assess what features of the stimulus were encoded in the low-rate EEG signal. The link between music features and EEG was assessed by using these models to predict unseen EEG data. (<bold>A</bold>) Prediction correlations were greatest when the stimulus was described by a combination of acoustic information (<bold>A</bold>: envelope Env, and its half-way rectified first derivative Env’) and melodic expectations (<bold>M</bold>: S<sub>P</sub>, S<sub>O</sub>, H<sub>P</sub>, H<sub>O</sub>). This effect of expectations was significant on the average prediction correlation across all 64 EEG electrodes (*p&lt;10<sup>−6</sup>). The error bars indicate the SEM across participants. (<bold>B</bold>) The enhancement due to melodic expectations emerged at the individual subject level. The gray bars indicate the predictive enhancement due to melodic expectation. Error bars indicate the SEM over trials (***p&lt;0.001, **p&lt;0.01, *p&lt;0.05, permutation test). (<bold>C</bold>) Predictive enhancement due to melodic expectations (<bold>AM-A</bold>; <italic>y</italic>-axis) increased with the length of the local context (in bars; <italic>x</italic>-axis) used to estimate the expectation of each note (ANOVA: *p=0.0003). The boxplot shows the 25th and 75th percentiles, with the whiskers extending to the most extreme data-points that were not considered outliers. Circles indicate outliers. (<bold>D</bold>) The effect of melodic expectations (<italic>r<sub>AM</sub>-r<sub>A</sub></italic>) emerged bilaterally on the same scalp areas that showed also envelope tracking. (<italic>E</italic>) Ridge regression weights for TRF<sub>AM</sub>. Red and blue colors indicate positive and negative TRF components respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>The latency of the effect of melodic expectations on the low-rate EEG data (1–8 Hz) was assessed by measuring the loss in EEG prediction correlation when a given time-latency window is removed from the TRF fit.</title><p>This loss was calculated for a sliding 50ms-long latency window and was plotted as a heat-map for each electrode. White colored pixels indicate non-significant results (p&gt;0.05, cluster-mass statistics). Red color indicates a larger loss, thus a stronger importance, of a particular time-latency window. The x-axis indicates the centre of the time-latency window.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig2-figsupp1-v1.tif"/></fig></fig-group><p>While these results indicate that AM is a better descriptor of the signal than A, it should also be noted that TRF<sub>AM</sub> has higher dimensionality than TRF<sub>A</sub>. Correlations are measured on unseen data, and thus should be immune to overfitting, nonetheless to verify that the better predictions for AM are not due simply to the higher degrees of freedom afforded by the addition of more M components, we assessed the performance of our TRF<sub>AM</sub> model using less elaborate M functions that nevertheless maintained the TRF<sub>AM</sub> with the same dimensionality and value distributions. Specifically, we built melodic expectation estimates by relying on progressively smaller amount of local context (or short-term memory) (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), with M based on 1, 2, 4, 8, 16, and 32 musical bars, as opposed to the unbounded (‘∞’) estimates in our predictions of <xref ref-type="fig" rid="fig2">Figure 2A</xref>. In each of these cases, fitted TRFs of the same dimensionality performed better as the memory increased (<xref ref-type="fig" rid="fig2">Figure 2C</xref>; ANOVA: <italic>F</italic>(4.6,101.1) = 4.52, p=0.0003), indicating that the contribution of melodic expectation estimates to the prediction accuracy was not due to increased TRF dimensionality per se.</p><p>Despite the significant positive effects of melodic expectation on prediction correlations, we found no differences between the corresponding EEG topographical distributions for A, AM, or their difference AM-A (DISS<sub>A,AM</sub> = 0.017, p=0.33; DISS<sub>A,AM-A</sub> = 0.214, p=0.61; DISS<sub>AM,AM-A</sub> = 0.197, p=0.57; <xref ref-type="fig" rid="fig2">Figure 2D</xref>). By contrast, melodic expectations induced new <italic>long temporal latencies</italic> in the linear regression weights of the TRF<sub>AM</sub> model (<xref ref-type="fig" rid="fig2">Figure 2E</xref>), which were mostly centered around 200 ms compared to the 50 ms latency of the acoustic TRF<sub>A</sub> Env component (p&lt;0.05, FDR correction; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p></sec><sec id="s2-2"><title>Melodic expectations modulate auditory responses in higher cortical areas</title><p>Since melodic expectations reflect regularities within a musical tone sequence at multiple time-scales that depend on the extent of knowledge and exposure of the subject listening to them, we hypothesized that neural signals correlated with the melodic properties of the music would be generated at higher hierarchical cortical levels than those strictly due to the acoustics (<xref ref-type="bibr" rid="bib118">Sammler et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Bianco et al., 2016</xref>; <xref ref-type="bibr" rid="bib90">Nourski et al., 2018</xref>). EEG lacks the spatial resolution needed to test this hypothesis, but the test was possible in spatially localized ECoG recordings from three patients who had electrodes over the early primary auditory areas in the <italic>anterior transverse temporal gyrus</italic>, also called <italic>Heschl Gyrus</italic> (HG; patients 1 and 3), the belt regions along <italic>planum temporale</italic> (PT) and the <italic>superior temporal gyrus</italic> (STG), as well as the <italic>supra-marginal gyrus</italic> (SMG) in the parietal lobe (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for details on the channel locations and <xref ref-type="video" rid="video1">Videos 1</xref>–<xref ref-type="video" rid="video3">3</xref> and <xref ref-type="supplementary-material" rid="supp2">Supplementary files 2</xref>–<xref ref-type="supplementary-material" rid="supp4">4</xref> for a 3D view of the electrode placement). Although those regions are functionally heterogeneous, our choice of anatomical division was motivated by both previous work indicating HG as the locus responsible for primary auditory processing (<xref ref-type="bibr" rid="bib83">Moerel et al., 2014</xref>; <xref ref-type="bibr" rid="bib89">Nourski, 2017</xref>), PT as an intermediary stage (<xref ref-type="bibr" rid="bib45">Griffiths and Warren, 2002</xref>), and STG as a region involved the processing of high-level speech properties (<xref ref-type="bibr" rid="bib16">Chang et al., 2010</xref>; <xref ref-type="bibr" rid="bib80">Mesgarani et al., 2014</xref>). Both anatomical and functional studies measured a gradient change from the primary auditory processing in HG to the nonprimary areas in the lateral STG, and suggested a nonprimary role of PT (<xref ref-type="bibr" rid="bib45">Griffiths and Warren, 2002</xref>; <xref ref-type="bibr" rid="bib53">Hickok and Saberi, 2012</xref>), which is here considered as a higher cortical area. The inferior frontal gyrus (IFG) was expected to reflect melodic expectations as well, however we only had limited coverage in that cortical area. The subjects listened to the same monophonic music described earlier for the EEG experiments.</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-51784-video1.mp4"><label>Video 1.</label><caption><title>Video showing the ECoG electrode placement in 3D for each of the three participants.</title><p>Dots indicate ECoG channels. Red dots indicate channels that were responsive to the music input. The corresponding interactive Matlab 3D plots were also uploaded.</p></caption></media><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-51784-video2.mp4"><label>Video 2.</label><caption><title>Video showing the ECoG electrode placement in 3D for each of the three participants.</title><p>Dots indicate ECoG channels. Red dots indicate channels that were responsive to the music input. The corresponding interactive Matlab 3D plots were also uploaded.</p></caption></media><media id="video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-51784-video3.mp4"><label>Video 3.</label><caption><title>Video showing the ECoG electrode placement in 3D for each of the three participants.</title><p>Dots indicate ECoG channels. Red dots indicate channels that were responsive to the music input. The corresponding interactive Matlab 3D plots were also uploaded.</p></caption></media><p>We first identified 21/241, 25/200, and 33/285 electrodes in Patients 1, 2, and three respectively that exhibited reliable auditory responses (stronger responses to monophonic music than to silence: Cohen’s <italic>d</italic> &gt; 0.5; <xref ref-type="fig" rid="fig3">Figures 3C</xref> and <xref ref-type="fig" rid="fig4">4C</xref>) in the form of either low-rate (1–8 Hz) local field potentials (similar bands to those in EEG analyses above), or power in the high-γ (70–150 Hz) field potentials which are thought to reflect local neuronal activity (<xref ref-type="bibr" rid="bib81">Miller et al., 2007</xref>; see Materials and methods for details). A similar TRF analysis was conducted to identify responses that were sensitive to melodic expectations (<xref ref-type="fig" rid="fig3">Figure 3A</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Envelope tracking was significant (permutation test on the ECoG prediction correlations over trials, p&lt;0.05) in STG, PT, and HG channels. The predictive enhancement due to melodic expectations was small but significant (p&lt;0.01, FDR-corrected) on several electrodes in PT in all patients, on one electrode in the <italic>transverse temporal sulcus</italic> (TTS), but not on the three HG electrodes in Patient 1, and was also significant on five bilateral HG electrodes in Patient 3. A Wilcoxon rank sum test indicated that the effect of expectations AM-A is larger in PT than HG (p=0.011; all electrodes in the two cortical areas from Patients 1 and 3 were combined, while Patient two was excluded as there was no coverage in HG). Similar effects as in PT were also measured in right parietal cortical areas (SMG and the <italic>postcentral gyrus</italic>) for Patient two but not for Patient 3. In addition, the effect of expectations seen in STG, PT, and HG was right lateralized in Patient 3 (p=0.038), while envelope tracking did not show a hemispheric bias (p=0.85).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Low-rate (1–8 Hz) cortical signals in bilateral temporal cortex reflect melodic expectations.</title><p>Electrocorticography (ECoG) data were recorded from three epilepsy patients undergoing brain surgery. Magnetic resonance imaging was used to localise the ECoG electrodes. Electrodes with stronger low-rate or high-γ (70–150 Hz) responses to monophonic music than to silence were selected (Cohen’s <italic>d</italic> &gt; 0.5). (<bold>A</bold>) ECoG prediction correlations for individual electrodes for <bold>A</bold> and <bold>AM</bold>. Electrodes within each group, as indicated by the gray square brackets, were sorted from lateral to medial cortical sites. The gray bars indicate the predictive enhancement due to melodic expectation (<italic>r<sub>AM</sub>-r<sub>A</sub></italic>). Error bars indicate the SEM over trials (*p&lt;0.01, FDR-corrected permutation test). (<bold>B</bold>) TRF weights for selected electrodes. For Patient 1, PT and TTS electrodes (e1-e5 and e6 respectively) exhibited large effects of musical expectations, while HG electrodes (e7-e9) had strong envelope tracking (<bold>A</bold>) but showed smaller effects of expectations that did not reach statistical significance. Patient two showed also strong envelope tracking and a significant effect of melodic expectations in the right temporal and parietal lobes (for example, the PT electrode e4 and the SMG electrode e10 respectively). (<bold>C</bold>) Low-rate (1–8 Hz) ECoG segments time-locked to note onsets were selected and compared with segments corresponding to silence. Colors in the first brain plot of each patient indicate the effect-size of the note vs. silence comparison (Cohen’s <italic>d</italic> &gt; 0.5). The second brain plot shows the EEG prediction correlations when using acoustic features only (<bold>A</bold>). The third brain plot depicts the increase in EEG predictions when including melodic expectation features (<bold>AM-A</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Bilateral electrocorticography (ECoG) results for Patient 3.</title><p>Electrodes with stronger low-rate (1–8 Hz) or high-<bold>γ</bold> (70–150 Hz) time-locked responses to musical notes than to silence were selected (Cohen’s <italic>d</italic> &gt; 0.5). (<bold>A</bold>) Low-rate ECoG prediction correlations for individual electrodes for A and AM. Electrodes within each group, as indicated by the gray square brackets, were sorted from lateral to medial cortical sites. The gray bars indicate the predictive enhancement due to melodic expectation (<italic>r<sub>AM</sub>-r<sub>A</sub></italic>). Error bars indicate the SEM over trials (*p&lt;0.01, FDR-corrected permutation test). (<bold>B</bold>) ECoG electrodes that showed stronger time-locked responses to notes than to silence. The three plots show the effect-size of the note vs. silence response comparison for low-rate (1–8 Hz) ECoG signals (<italic>left</italic>), the EEG prediction correlations when using acoustic features only (<bold>A</bold>) (<italic>right</italic>), and the contrast between EEG predictions for AM and A (<italic>bottom</italic>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig3-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>High-γ neural signals in bilateral temporal cortex reflect melodic expectations.</title><p>Electrodes with stronger low-rate (1–8 Hz) or high-γ (70–150 Hz) responses to monophonic music than to silence were selected (Cohen’s <italic>d</italic> &gt; 0.5). (<bold>A</bold>) ECoG prediction correlations for individual electrodes for <bold>A</bold> and <bold>AM</bold>. Electrodes within each group, as indicated by the gray square brackets, were sorted from lateral to medial cortical sites. The gray bars indicate the predictive enhancement due to melodic expectation (<italic>r<sub>AM</sub>-r<sub>A</sub></italic>). Error bars indicate the SEM over trials (*p&lt;0.01, FDR-corrected permutation test). (<bold>B</bold>) Normalised TRF weights for selected electrodes (same electrodes as for <xref ref-type="fig" rid="fig3">Figure 3</xref>). For Patient 1, the HG electrode e9 showed the strongest envelope tracking and small effect of melodic expectations, while e6 in TTS exhibited the largest effect of expectations (Δr<sub>6</sub> &gt; Δr<sub>9</sub>, p=1.8e<sup>−4</sup>, <italic>d</italic> = 2.38). For Patient 2, both e4 (PT) and e10 (SMG) electrodes showed strong envelope tracking and a significant effect of melodic expectations. (<bold>C</bold>) High-γ (70–150 Hz) ECoG segments time-locked to note onsets were selected and compared with segments corresponding to silence. Colors in the first brain plot of each patient indicate the effect-size of the note vs. silence comparison (Cohen’s <italic>d</italic> &gt; 0.5). The second brain plot shows the EEG prediction correlations when using acoustic features only (<bold>A</bold>). The third brain plot depicts the increase in EEG predictions when including melodic expectation features (<bold>AM-A</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Bilateral electrocorticography (ECoG) results for Patient 3.</title><p>Electrodes with stronger low-rate (1–8 Hz) or high-<bold>γ</bold> (70–150 Hz) time-locked responses to musical notes than to silence were selected (Cohen’s <italic>d</italic> &gt; 0.5). (<bold>A</bold>) High-<bold>γ</bold> ECoG prediction correlations for individual electrodes for <bold>A</bold> and <bold>AM</bold>. Electrodes within each group, as indicated by the gray square brackets, were sorted from lateral to medial cortical sites. The gray bars indicate the predictive enhancement due to melodic expectation (<italic>r<sub>AM</sub>-r<sub>A</sub></italic>). Error bars indicate the SEM over trials (*p&lt;0.01, FDR-corrected permutation test). (<bold>B</bold>) ECoG electrodes that showed stronger time-locked responses to notes than to silence. The three plots show the effect-size of the note vs. silence response comparison for high-<bold>γ</bold> (70–150 Hz) ECoG signals (<italic>left</italic>), the EEG prediction correlations when using acoustic features only (<bold>A</bold>) (<italic>right</italic>), and the contrast between EEG predictions for <bold><sc>AM</sc></bold> and <bold>A</bold> (<italic>bottom</italic>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig4-figsupp1-v1.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig3">Figure 3B</xref> depicts the TRF<sub>AM</sub> weights for selected electrodes in SMG, PT, TTS, and HG. The TRF<sub>AM</sub> weights for ECoG exhibited low-rate temporal patterns very similar to those measured with EEG. Specifically, strong correlations were found with the TRF<sub>AM</sub> measured with EEG at Cz (<italic>r</italic> = 0.60, 0.50, and 0.45 in left PT, TTS, and HG respectively – e2, e6, and e9 from Patient 1; <italic>r</italic> = 0.61 and 0.80 in right PT and SMG – e4 and e10 from Patient two respectively). Overall, the TRF analysis of the ECoG recordings demonstrates that low-rate cortical responses to music encode melodic expectations. The strong similarities between ECoG responses and the EEG template obtained by averaging data from twenty participants (both musicians and non-musicians) suggests the possibility that the EEG melodic expectation results originate from temporal regions between PT and HG, and or parietal regions such as SMG. However, more direct evidence (with within-subject comparisons or source localization) are required to more confidently pinpoint the cortical origins of the EEG results.</p><p>ECoG recordings also allowed us to investigate more directly the link between local neuronal activity and melodic expectations, since these signals are available in the instantaneous power of the high-γ field potentials (<xref ref-type="bibr" rid="bib21">Crone et al., 2001</xref>; <xref ref-type="bibr" rid="bib34">Edwards et al., 2009</xref>; <xref ref-type="bibr" rid="bib108">Ray et al., 2008</xref>; <xref ref-type="bibr" rid="bib128">Steinschneider et al., 2008</xref>). Again, TRF analysis was used in <xref ref-type="fig" rid="fig4">Figure 4</xref> (and in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) to disentangle the contributions of envelope tracking and melodic expectations. As before, left HG (in Patients 1 and 3), left TTS (Patient 1), bilateral STG (Patients 2 and 3), bilateral PT (Patients 1, 2, and 3), and right SMG (Patient two but not Patient 3) electrodes exhibited substantial envelope tracking. By contrast, the effect of expectations (<italic>Δ</italic>r = <italic>r<sub>AM</sub> rA</italic>) was largest in PT, TTS, and in HG electrodes close to the junction between PT and HG, with a predictive enhancement up to ~50% (e.g., <italic>Δ</italic>r<sub>e6</sub> = 0.09 in Patient 1, which corresponds to a prediction enhancement r<sub>AM,e6</sub>/r<sub>A,e6</sub> of 149%), in contrast to an enhancement of only 6% in the HG electrode with strongest envelope tracking (e9). Similar patterns emerged for Patient 3, with a predictive enhancement up to ~20% in PT and an enhancement of only 5% in the HG electrode with strongest envelope tracking (e9; but with stronger effects in other HG electrodes with weaker envelope tracking). Also, the temporal latencies in the TRF weights (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) were rather different from what was previously seen for low-rate EEG and ECoG signals. In fact, the TRF<sub>A</sub> weights corresponding to the acoustic features exhibited sharp, short-latency dynamics while those of the melodic expectation features (TRF<sub>AM</sub>) pointed to more temporally extended and strong neural responses.</p></sec><sec id="s2-3"><title>Explicit encoding of melodic expectations in the evoked-responses</title><p>So far, melodic effects were extracted in terms of the temporally extended analysis of the TRF, and indirectly validated through assessment of prediction accuracy. A more direct measure of these effects is possible by examining whether event-related potentials (ERPs) time-locked to note onsets are specifically modulated by melodic expectations, that is <italic>beyond</italic> what is expected from the acoustic features of the stimuli. For instance, here we specifically demonstrate that the cortical responses evoked by tones of identical envelope can produce significantly different responses that are modulated proportionately to the melodic values of the tones. To do so, we selected notes with equal acoustic envelopes corresponding to the median peak envelope amplitude (25% of all notes), but had large disparity in their surprise values S<sub>p</sub> according to the IDyOM model, namely the top 20% and bottom 20% (<xref ref-type="fig" rid="fig5">Figure 5</xref>, purple and pink respectively). As illustrated in <xref ref-type="fig" rid="fig5">Figure 5</xref>, the two groups (purple and pink curves) had identical average signal envelopes (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), but displayed significantly disparate EEG responses (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), with significantly larger responses to the notes with the high pitch surprise (purple &gt;pink; p&lt;0.05 at the N1 and P2 peaks, permutation test; p=0.001 on the power of the average ERP across all channels for latencies between 0 and 200 ms). A similar effect emerged for H<sub>p</sub> and H<sub>o</sub> (average power ERP within 0–200 ms, high surprise &gt;low surprise with p=0.0425 and p=0.006 for H<sub>p</sub> and H<sub>o</sub> respectively<italic>; not shown</italic>), while no effect was measured for S<sub>o</sub> (p=0.8764). Note that the ERPs showed large responses at pre-stimulus latencies (before zero latency). This is due to the temporal regularities that are intrinsic in music, which results in large average envelope before the note of interest (see <xref ref-type="fig" rid="fig5">Figure 5A</xref>). In fact, limiting the ERP calculation to musical events with preceding inter-note-interval longer than 200 ms eliminated such pre-stimulus responses from the ERPs (<italic>not shown</italic>). However, this selection procedure reduced the number of EEG epochs, and thus our choice to include short inter-note-interval in the analysis in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Event-related potentials (ERP) analysis.</title><p>(<bold>A</bold>) Notes with equal peak envelope were selected (median envelope amplitude across all notes with a tolerance of ±5%). Together, the selected elements were 25% of all notes. Notes were grouped according to the corresponding pitch surprise values (S<sub>p</sub>). The figure shows the average sound envelope for the 20% notes with lowest and highest surprise values. Shaded areas indicate the 95% confidence interval. (<bold>B</bold>) Low-rate EEG signals time-locked to note-onset were selected for high and low S<sub>p</sub> values. ERPs for channel Cz are shown on the left. Shaded areas indicate the 95% confidence interval (across subjects). Stars indicate significant differences between ERPs for high and low surprise for a given note onset-EEG latency (permutation test, p&lt;0.05, FDR-corrected). The right panel shows the total ERP power for the latencies from 0 to 200 ms (*p&lt;0.001, permutation test). Error-bars indicate the SEM across subjects. (<bold>C</bold>) ERPs for high-γ ECoG data from left TTS and HG (Patient 1). Stars indicate significant differences between ERPs for high and low surprise for a given note onset-EEG latency (permutation test, p&lt;0.05, FDR-corrected). Shaded areas indicate the 95% confidence interval (across individual trials, that is responses to single notes).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Event-related potentials (ERP) analysis.</title><p>(<bold>A</bold>) Musical notes with equal envelope amplitude (within 5% of the median amplitude for all notes) were selected. Together, this corresponds to the 25% of all data. Musical notes were grouped in five bins according to S<sub>p</sub>. ERPs for high-<bold>γ</bold> ECoG data for selected electrodes from patients 1 and 2. Stars indicate significance (p&lt;0.05, unpaired two-samples Wilcoxon test, uncorrected values). (<bold>B</bold>) ERPs for low-rate ECoG data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig5-figsupp1-v1.tif"/></fig></fig-group><p>Similar analyses for both low-rate and high-γ ECoG data revealed that ERP responses in TTS electrodes to musical notes with <italic>equal</italic> envelopes were modulated in proportion to the S<sub>p</sub> (<xref ref-type="fig" rid="fig5">Figure 5C</xref>) stats; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). This effect of melodic surprise was absent in the electrode with strongest envelope tracking e9 in Patient 1 (in the left HG; <xref ref-type="fig" rid="fig5">Figure 5C</xref>). These results are consistent with previous findings on melodic expectations (<xref ref-type="bibr" rid="bib93">Omigie et al., 2013</xref>; <xref ref-type="bibr" rid="bib94">Omigie et al., 2019</xref>) and in line with the hypothesis that higher stimulus expectation can reduce auditory responses (<xref ref-type="bibr" rid="bib134">Todorovic et al., 2011</xref>; <xref ref-type="bibr" rid="bib135">Todorovic and de Lange, 2012</xref>). Furthermore, this result complements the TRF analysis by confirming that the effect of melodic expectations on the cortical responses can be disentangled from changes in the amplitude of the stimulus envelope. It should be emphasized, however, that compared to the TRF approach, this analysis may in many cases suffer from the potential of interactions between the responses to the sequence of notes, for example if the internote interval is shorter than the duration of the neural response of interest. It also cannot isolate among the interactions and modulations due to the various melodic expectation features. Nevertheless, the validity of these results is confirmed by the parallel TRF findings in <xref ref-type="fig" rid="fig2">Figures 2</xref>–<xref ref-type="fig" rid="fig4">4</xref>, that the encoding of melodic expectations in the cortical responses is different from responses due to stimulus acoustics.</p></sec><sec id="s2-4"><title>Pitch and onset-time induce distinct musical expectations</title><p>So far, we have parameterized melodic expectations in terms of surprise and entropy features, each for pitch and note-onsets. Surprise and entropy were expected to interact as they convey complementary information (<xref ref-type="bibr" rid="bib18">Cheung et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Gold et al., 2019</xref>). Entropy provides information on the uncertainty of the prediction of the next note before observing the event, thus it describes the overall probability distribution. Surprise depends on that same distribution but is specific to the observed event. For this reason, we expected the responses to entropy and surprise to be dissociable in their temporal dynamics. This hypothesis was tested in our EEG data by measuring the contrast in the TRF<sub>AM</sub> weights for surprise <italic>versus</italic> entropy (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, top; weights were averaged as follows: (S<sub>p</sub>+S<sub>o</sub>)/2 vs. (H<sub>p</sub>+H<sub>o</sub>)/2). The results showed that responses with latencies up to 350 ms were significantly dominated by both surprise and entropy in alternation (p&lt;0.05, permutation test, FDR-corrected).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Distinct cortical encoding of pitch and note onset-time during naturalistic music listening.</title><p>(<bold>A</bold>) Contrasts at each EEG channel of the TRF weights for surprise vs. entropy (top) and pitch vs. onset-time (bottom) in TRF<sub>AM</sub>. Colors indicate significant differences (p&lt;0.05, permutation test, FDR-corrected) (<bold>B</bold>) Average surprise and entropy of note-onsets (S<sub>o</sub> and H<sub>o</sub>) and of pitch (S<sub>p</sub> and H<sub>p</sub>) for each musical piece. Musical pieces were sorted based on S<sub>o</sub>, where lower average S<sub>o</sub> indicates musical pieces with more predictable tempo. (<bold>C</bold>) Cortical tracking of music changes with overall surprise of note onset-time within a musical piece. Single-trial EEG prediction result (average across all channels) for musicians (N<sub>m</sub> = 10) and non-musicians (N<sub>n</sub> = 10). Trials were sorted as in panel B. (<bold>D</bold>) Single-trial ECoG prediction correlations for the surgery Patient one for two electrodes of interest.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Scatter plots indicating the correlation between EEG prediction correlation using the acoustic regressors A for each musical piece and the average expectation score (Sp, Hp, So, or Ho) for all notes of the corresponding piece.</title><p>Significant correlations were highlighted in red. Results are shown for non-musicians and musicians separately.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig6-figsupp1-v1.tif"/></fig></fig-group><p>A second analysis was conducted to test the relative contribution of pitch and onset-time expectations to the TRF<sub>AM</sub> model. As previous studies suggested a dissociation between pitch and sound onset processing (<xref ref-type="bibr" rid="bib122">Schönwiesner and Zatorre, 2008</xref>; <xref ref-type="bibr" rid="bib20">Coffey et al., 2017</xref>), we expected similar differences in the processing of their expectations in early auditory cortical regions. We tested for such a dissociation in our EEG data by measuring the contrast in the TRF<sub>AM</sub> weights for pitch <italic>versus</italic> onset time (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, bottom; (S<sub>p</sub>+H<sub>p</sub>)/2 vs. (S<sub>o</sub>+H<sub>o</sub>)/2). Note-onset dominant responses emerged only up to 200 ms, while pitch dominant responses persisted for much longer latencies up to 400 ms. The latency differences for pitch and note-onset TRFs suggests a certain level of dissociation between pitch and onset-time expectations.</p><p>Our results indicate that brain responses to music are modulated by melodic expectations, an effect that was explicitly accounted for by including <bold>M</bold> in the TRF mapping, and are in line with the hypothesis that more surprising notes elicit larger auditory responses (<xref ref-type="bibr" rid="bib134">Todorovic et al., 2011</xref>; <xref ref-type="bibr" rid="bib135">Todorovic and de Lange, 2012</xref>; <xref ref-type="bibr" rid="bib17">Chennu et al., 2013</xref>; <xref ref-type="bibr" rid="bib2">Auksztulewicz and Friston, 2016</xref>). Accordingly, musical pieces with higher mean surprise values were expected to elicit EEG and ECoG responses with higher SNR, thus producing larger prediction correlation scores. To test this hypothesis, we calculated the mean scores for each expectation feature (<xref ref-type="fig" rid="fig6">Figure 6B</xref>) and measured their correlation with the envelope tracking (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Significant Spearman correlations were measured between the average S<sub>o</sub> of a piece and the neural signal prediction correlations for EEG (<italic>r</italic> = 0.98, p&lt;0.001 for non-musicians; <italic>r</italic> = 0.96, p&lt;0.001 for musicians; <xref ref-type="fig" rid="fig6">Figure 6C</xref>) and high-γ ECoG data (<italic>r</italic> = 0.88, p=0.002 for e6 in the left TTS of Patient 1; <italic>r</italic> = 0.88, p=0.002 for e9 in the left HG of Patient 1; <xref ref-type="fig" rid="fig6">Figure 6D</xref>). These effects were specific to onset-time surprise. In fact, Spearman correlations of comparable magnitude emerged with -H<sub>o</sub>, while no significant correlations were measured for S<sub>p</sub> and H<sub>p</sub> for these pieces (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). <xref ref-type="fig" rid="fig6">Figure 6C and D</xref> also illustrates the prediction correlations for AM<sub>p</sub>, showing that small (nearly zero) envelope tracking due to small average S<sub>o</sub> does not hamper the encoding of pitch expectations on the same ECoG electrode (see <xref ref-type="fig" rid="fig6">Figure 6D</xref> left), thus further highlighting the dissociation of processes underlying expectation of pitch and onset-time.</p></sec><sec id="s2-5"><title>Effect of musical expertise on the encoding of melodic expectations</title><p>We were also able to shed light on the effect of musical expertise on the encoding of melodic expectations. Specifically, by design, half of the EEG participants had no musical training, while the others were expert pianists that studied for at least ten years (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In <xref ref-type="fig" rid="fig7">Figure 7A</xref> we show a comparison between the two EEG groups. A cluster statistics indicated that melodic expectation was larger for musicians than non-musicians for frontal EEG channels (<xref ref-type="fig" rid="fig7">Figure 7B</xref>; see Di Liberto et al. in press, for comparisons that are specific to music envelope tracking). Note that subjective reporting indicated no significant effect of musical training on the familiarity with the musical pieces (see Materials and methods).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Effect of musical expertise on the low-rate encoding of melodic expectations.</title><p>(<bold>A</bold>) EEG prediction correlations (average across all scalp channels) for musicians and non-musicians (*p&lt;10<sup>−5</sup>). The error bars indicate the SEM across participants. (<bold>B</bold>) EEG prediction correlations for individual scalp electrodes (red color indicates stronger prediction correlations) for <bold>A</bold>, <bold>AM</bold>, and <bold>AM-A</bold>. A cluster statistics found a stronger effect of melodic expectations for musicians.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51784-fig7-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Musical perception is strongly influenced by expectations (<xref ref-type="bibr" rid="bib3">Bar et al., 2006</xref>; <xref ref-type="bibr" rid="bib54">Huron, 2006</xref>; <xref ref-type="bibr" rid="bib65">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib102">Pearce, 2018</xref>; <xref ref-type="bibr" rid="bib52">Henin et al., 2019</xref>). Violation of these expectations elicits distinct neural signatures that may underlie the emotional and intellectual engagement with music (<xref ref-type="bibr" rid="bib143">Zatorre and Salimpoor, 2013</xref>; <xref ref-type="bibr" rid="bib18">Cheung et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Gold et al., 2019</xref>). Here, we exploited such neural signatures of melodic expectations during listening to Bach monophonic pieces to demonstrate that cortical responses encode explicitly subtle changes in note predictability during naturalistic music listening. In doing so, we 1) demonstrated a novel methodology to assess the cortical encoding of melodic expectations that is effective at the individual subject level with non-invasive EEG signals recorded during passive music listening, 2) provided detailed insights into the spatial selectivity and temporal properties of that encoding in ECoG recordings, and 3) physiologically evaluated the statistical learning framework of melodic expectation, exemplified by models such as IDyOM (<xref ref-type="bibr" rid="bib99">Pearce, 2005</xref>).</p><p>Our findings have several implications for current theories and understanding of sensory perception in general, and musical cognition in particular. First, we show that the neural responses are consistent with the statistical learning theoretical frameworks in that response dynamics are proportional to the predictability of several signal attributes, including pitch and onset-timing. For musical sequences, additional attributes, such as timbre, loudness, and complex patterns of harmony are likely to be relevant, although we did not investigate them here. These properties may all contribute in tandem to music perception, or sometimes in a dissociated manner, as we demonstrated for the relative contribution of pitch and onset-timing (see also <xref ref-type="bibr" rid="bib20">Coffey et al., 2017</xref>). Second, melodic expectations attributes are encoded in both low-rate and high-γ responses of higher cortical regions (e.g., bilateral PT in humans). The effect of expectations extended to the junction between the PT with HG (TTS) and to neighbouring electrodes in HG, an early auditory cortical area that favors the accurate representation of the acoustic properties of the music. In addition, neural signals in some SMG electrodes showed similar encoding of melodic entropy and surprise, a result that well aligns with previous findings suggesting a role of the left SMG in short-term pitch memory (<xref ref-type="bibr" rid="bib139">Vines et al., 2006</xref>; <xref ref-type="bibr" rid="bib119">Schaal et al., 2015</xref>; <xref ref-type="bibr" rid="bib120">Schaal et al., 2017</xref>). Third, we have demonstrated a methodology to objectively assess the cortical encoding of melodic expectations using non-invasive neural recordings and passive listening of ecologically valid stimuli. This finding combined with recent evidence on the precise link between pleasure of music listening and the particular combinations of surprise and entropy of the stimuli (<xref ref-type="bibr" rid="bib18">Cheung et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Gold et al., 2019</xref>) opens new avenues for investigation of the neurophysiological bases of music perception and its link with emotions (<xref ref-type="bibr" rid="bib115">Salimpoor et al., 2009</xref>; <xref ref-type="bibr" rid="bib116">Salimpoor et al., 2013</xref>; <xref ref-type="bibr" rid="bib124">Shany et al., 2019</xref>).</p><p>This work builds upon previous ERP findings on tone repetitions (<xref ref-type="bibr" rid="bib134">Todorovic et al., 2011</xref>) and simple note sequences (<xref ref-type="bibr" rid="bib93">Omigie et al., 2013</xref>; <xref ref-type="bibr" rid="bib94">Omigie et al., 2019</xref>), but circumvents one of the main issues tied to the ERP approach, namely that it does not allow in general to isolate responses to continuous or rapidly presented stimuli. While such limitation may be overcome for stimuli with particular statistics, such as speech (<xref ref-type="bibr" rid="bib57">Khalighinejad et al., 2017</xref>), the temporal regularities inherent in music hamper the ability to dissociate the late ERP components in response to a note from the early responses to subsequent ones. Instead, the TRF framework is more resistant to this issue as it assumes that the transformation of a stimulus into the corresponding brain responses can be captured by a linear time-invariant system. Although the human brain is neither linear nor time-invariant, these assumptions can be reasonable in certain cases, and this approach was shown to be effective in previous studies with stimuli that were either discrete and rapidly presented, or continuous (<xref ref-type="bibr" rid="bib69">Lalor et al., 2006</xref>; <xref ref-type="bibr" rid="bib32">Ding and Simon, 2012b</xref>; <xref ref-type="bibr" rid="bib27">Di Liberto et al., 2015</xref>; <xref ref-type="bibr" rid="bib22">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Fiedler et al., 2017</xref>; <xref ref-type="bibr" rid="bib10">Brodbeck et al., 2018a</xref>; <xref ref-type="bibr" rid="bib13">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="bib137">Vanthornhout et al., 2018</xref>; <xref ref-type="bibr" rid="bib141">Wong et al., 2018</xref>). Using this approach, we have demonstrated how information on melodic expectations, as well as levels and fidelity of envelope tracking, can be extracted from both EEG and ECoG recordings. There is ample evidence that melodic expectations of various origins modulate responses to stimulus acoustics, possibly increasing the magnitude of those responses according to the degree of violation of the expectations. Such a modulation was captured by our multivariate TRF analysis in <xref ref-type="fig" rid="fig2">Figure 2E</xref>, which emerged as a positive expectation component at latencies between 150 and 250 ms, the same latencies corresponding to ERP components elicited by musical violations such as the ERAN (<xref ref-type="bibr" rid="bib59">Koelsch et al., 2002</xref>). The present findings go beyond previous work by investigating the encoding of melodic expectations in valid sequences with high temporal (EEG and ECoG) and spatial (ECoG) resolution. However, similar to ERP analyses, the TRF approach is most effective in capturing signals that are precisely time-locked to note onset. Therefore, further studies with different methodologies are required to assess melodic expectation encoding on cortical sites with low or no time-locked responses to musical notes, where the TRF methodology was less effective. Despite this limitation, the TRF approach allowed us to extract objective indices of melodic expectations encoding that were derived by both explicit (<bold>AM-M</bold>; <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref>, <xref ref-type="fig" rid="fig4">4</xref> and <xref ref-type="fig" rid="fig7">7</xref>) and implicit (<bold>A</bold>-only; <xref ref-type="fig" rid="fig6">Figure 6</xref>) analyses.</p><p>Furthermore, since subjects’ expectations modulate musical responses differently depending on their cultural experience and musical exposure, it is expected that musical expertise may significantly enhance these modulations and hence reveal stronger encoding of melodic expectations. The evidence we present in this study (<xref ref-type="fig" rid="fig7">Figure 7</xref>) is in line with this view and, although preliminary, the finding is consistent with previous neuroimaging results showing effects of musical training on the brain responses to music in both children and adults (e.g. <xref ref-type="bibr" rid="bib55">Jentschke and Koelsch, 2009</xref>; <xref ref-type="bibr" rid="bib92">Oechslin et al., 2013</xref>). The results in <xref ref-type="fig" rid="fig7">Figure 7</xref> leave open a key question: Do musicians in general encode melodic expectations better (more strongly or accurately), or is the estimate of the IDyOM model more in tune with that of musicians’ predictions than non-musicians’? Our results also suggest the possibility of a right hemispheric bias in the processing of melodic expectations, and the separate analysis of low-rate and high-γ neural signals seems crucial to investigate such an effect (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Other possibilities need to be controlled for in future studies with larger sample sizes and more types of stimuli, for example whether the experiment may have been more engaging for musically trained participants, which would explain their stronger envelope tracking! It will also be important to assess the effect of musical expertise on the optimal amount of memory for estimating melodic expectations.</p><p>The present study demonstrates that rich predictive models can be combined with neural recordings to disentangle the processing of distinct properties of complex sensory inputs. Previous work demonstrated that this approach is effective in other domains, such as in the study of natural speech perception, where neural responses at the level of acoustics, phonemes, phonotactics, and semantics were measured (<xref ref-type="bibr" rid="bib27">Di Liberto et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Di Liberto et al., 2019</xref>; <xref ref-type="bibr" rid="bib12">Brodbeck et al., 2018c</xref>; <xref ref-type="bibr" rid="bib13">Broderick et al., 2018</xref>). The ability to investigate multiple domains with a same framework may contribute to the search for fundamental shared neural mechanisms (<xref ref-type="bibr" rid="bib98">Patel, 2003</xref>; <xref ref-type="bibr" rid="bib41">Fitch and Martins, 2014</xref>). For instance, the hypothesis of shared resources between language and music has been supported by evidence of overlapping brain responses for processing music and language structures (<xref ref-type="bibr" rid="bib76">Maess et al., 2001</xref>). However, it is unclear whether these effects reflect domain-specific computations or domain-general functions such as working memory or cognitive control (<xref ref-type="bibr" rid="bib110">Rogalsky et al., 2011</xref>; <xref ref-type="bibr" rid="bib41">Fitch and Martins, 2014</xref>), or what properties in the stimuli provide the most parsimonious comparison across these domains (<xref ref-type="bibr" rid="bib51">Heffner and Slevc, 2015</xref>). Indeed, there is a fundamental difference in the use of predictions in the two domains. In speech, expectations are important to successfully understand the meaning of a sentence (e.g., phonemic restoration, priming; <xref ref-type="bibr" rid="bib72">Leonard et al., 2016</xref>; <xref ref-type="bibr" rid="bib88">Norris et al., 2016</xref>), especially in noisy, multi-talker environments (<xref ref-type="bibr" rid="bib78">McGettigan et al., 2012</xref>; <xref ref-type="bibr" rid="bib130">Strauß et al., 2013</xref>). In music, expectations may have a stronger link to emotions and musical engagement (<xref ref-type="bibr" rid="bib33">Dunsby, 2014</xref>; <xref ref-type="bibr" rid="bib117">Salimpoor et al., 2015</xref>).</p><p>Our results on melodic expectations exhibited spatio-temporal patterns that are different from those for the typical ERPs for syntactic and semantic violations in the case of natural speech perception, (N400, P600; for example <xref ref-type="bibr" rid="bib95">Osterhout and Holcomb, 1995</xref>; <xref ref-type="bibr" rid="bib68">Kutas and Federmeier, 2011</xref>; <xref ref-type="bibr" rid="bib8">Borovsky et al., 2012</xref>). Furthermore, previous work on phonotactic- and semantic-level expectations that used system identification methods as in the present study (<xref ref-type="bibr" rid="bib12">Brodbeck et al., 2018c</xref>; <xref ref-type="bibr" rid="bib13">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Di Liberto et al., 2019</xref>) exhibited strong TRF centro-parietal components at latencies around 300–500 ms, which were absent in the responses to melodic surprise and entropy. Instead, our findings align nicely with previous results for syntax surprisal responses based on recurrent neural network models of language structure (<xref ref-type="bibr" rid="bib48">Hale et al., 2018</xref>). This calls for further investigations with explicit within-subject comparisons, with the present findings providing a key starting point to tackle these questions. One factor that may be crucial in this investigation is the ability to exploit different expectation models to disentangle different contributors to expectations, such as statistical learning rule-based processing (<xref ref-type="bibr" rid="bib85">Morgan et al., 2019</xref>). In fact, although statistical learning (IDyOM) was shown to have a prominent role on melodic expectations based on behavioral data, an independent contribution of a rule-like music-theoretically motivated approach was found (Temperley Probabilistic Model of Melody Perception; <xref ref-type="bibr" rid="bib131">Temperley, 2008</xref>). In this sense, it is possible that the melodic expectation signals presented here only partly represent music responses.</p><p>This study presented novel detailed insights on the impact of melodic expectations on the neural processes underlying music perception and informed us on the physiological validity of models of melodic expectations based on Markov chains. In the process, we introduced the first solution to investigate the neural underpinnings of music perception in ecologically-valid listening conditions. As a result, this work constitutes a platform where research in cognitive neuroscience and musicology meet and can inform each other.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>EEG data acquisition and preprocessing</title><p>Twenty healthy subjects (10 male, aged between 23 and 42, M = 29) participated in the EEG experiment. Ten of them were highly trained musicians with a degree in music and at least ten years of experience, while the other participants had no musical background. Each subject reported no history of hearing impairment or neurological disorder, provided written informed consent, and was paid for their participation. The study was undertaken in accordance with the Declaration of Helsinki and was approved by the CERES committee of Paris Descartes University (CERES 2013–11). The experiment was carried out in a single session for each participant. EEG data were recorded from 64 electrode positions, digitized at 512 Hz using a BioSemi Active Two system. Audio stimuli were presented at a sampling rate of 44,100 Hz using Sennheiser HD650 headphones and Presentation software (<ext-link ext-link-type="uri" xlink:href="http://www.neurobs.com">http://www.neurobs.com</ext-link>). Testing was carried out at École Normale Supérieure, in a dark room, and subjects were instructed to maintain visual fixation on a crosshair centered on the screen, and to minimize motor activities while music was presented. A preliminary analysis was conducted on part of this EEG dataset in a separate study that compared EEG tracking in musicians and non-musicians (<xref ref-type="bibr" rid="bib29">Di Liberto et al., 2020</xref>).</p><p>Neural data were analysed offline using MATLAB software (The Mathworks Inc). EEG signals were digitally filtered between 1 and 8 Hz using a Butterworth zero-phase filter (low- and high-pass filters both with order two and implemented with the function <italic>filtfilt</italic>), and down-sampled to 64 Hz. Results were also reproduced with high-pass filters down to 0.1 Hz and low-pass filters up to 30 Hz. EEG channels with a variance exceeding three times that of the surrounding ones were replaced by an estimate calculated using spherical spline interpolation. All channels were then re-referenced to the average of the two mastoid channels with the goal of maximising the EEG responses to the auditory stimuli (<xref ref-type="bibr" rid="bib74">Luck, 2005</xref>).</p></sec><sec id="s4-2"><title>ECoG data acquisition and preprocessing</title><p>We recorded cortical activity from three adult human patients (one male) implanted with stereotactic EEG electrodes at the Northwell Health University Hospital as part of their clinical evaluation for epilepsy surgery. The research protocol was approved and monitored by the institutional review board at the Feinstein Institute for Medical Research (07–125), and written informed consent of the patients was obtained before surgery. The first patient (P1) was a highly trained musician with about twenty years of experience; the second patient (P2) had no musical background; and the third patient (P3) studied clarinet for 8 years while in secondary school and had not played for 25 years. As a part of their clinical diagnosis of epileptic focus, P1 was implanted with a total of 241 electrodes in the left hemisphere, P2 with 200 electrodes in the right hemisphere, and P3 with 285 electrodes in both left and right hemispheres. Patients had self-reported normal hearing. Electrocorticography signals with sampling rate of 3000 Hz were recorded with a multichannel amplifier connected to a digital signal processor (Tucker-Davis Technologies). All data were montaged again to common average reference (<xref ref-type="bibr" rid="bib21">Crone et al., 2001</xref>).</p><p>Channel positions were mapped to brain anatomy using registration of the postimplantation computed tomography (CT) to the preimplantation MRI via the postoperative MRI (<xref ref-type="bibr" rid="bib46">Groppe et al., 2017</xref>). The CT was first coregistered with the postimplantation structural MRI and, subsequently, with the preimplantation MRI. The coregistration was performed by means of the automated procedure FSL’s FLIRT (<xref ref-type="bibr" rid="bib79">Mehta and Klein, 2010</xref>). Channels were assigned to anatomical areas according to the Destrieux atlas (<xref ref-type="bibr" rid="bib26">Destrieux et al., 2010</xref>) and confirmed by expert inspection blinded to the results of this study.</p><p>Neural responses were transformed using Hilbert transform to extract the high-<bold>γ</bold> band (70–150 Hz) for analysis (<xref ref-type="bibr" rid="bib34">Edwards et al., 2009</xref>). This signal is known to correlate with neural spiking activity (<xref ref-type="bibr" rid="bib108">Ray et al., 2008</xref>; <xref ref-type="bibr" rid="bib128">Steinschneider et al., 2008</xref>) and was shown to reliably reflect auditory responses (<xref ref-type="bibr" rid="bib67">Kubanek et al., 2013</xref>; <xref ref-type="bibr" rid="bib80">Mesgarani et al., 2014</xref>). Secondly, low-rate responses were extracted from the raw unfiltered data by digitally filtering between 1 and 8 Hz using a Butterworth zero-phase filter (low- and high-pass filters both with order two and implemented with the function <italic>filtfilt</italic>). Both high-<bold>γ</bold> and low-rate signals were then down-sampled to 100 Hz.</p><p>Music-responsive sites (i.e. with significant electrical potentials time-locked to note onset) were determined by comparing portion of ECoG responses to music with signals recorded during the pre-stimulus silence. 25 chunks of data, each with duration 200 ms, were selected for each of the two conditions and Cohen’s <italic>d</italic> effect-size was calculated to quantify the effect of a monophonic music stimulus on the ECoG data. Electrodes with <italic>d</italic> &gt; 0.5 (medium effect-size) were marked as music-responsive (21, 25, and 34 electrodes for patients 1, 2, and three respectively).</p></sec><sec id="s4-3"><title>Stimuli and procedure</title><p>Monophonic MIDI versions of ten musical pieces from Bach’s monodic instrumental corpus were partitioned into short snippets of approximately 150 s. The selected melodies were originally extracted from violin (partita BWV 1001, presto; BWV 1002, allemande; BWV 1004, allemande and gigue; BWV 1006, loure and gavotte) and flute (partita BWV1013 allemande, corrente, sarabande, and bourrée angloise) scores and were synthesised by using piano sounds with MuseScore two software (MuseScore BVBA), each played with a fixed rate (between 47 and 140 bpm). This was done in order to reduce familiarity for the expert pianist participants while enhancing their neural response by using their preferred instrument timbre (<xref ref-type="bibr" rid="bib97">Pantev et al., 2001</xref>). Each 150 s piece, corresponding to an EEG/ECoG trial, was presented three times throughout the experiment, adding up to 30 trials that were presented in a random order. At the end of each trial, participants were asked to report on their familiarity to the piece (from 1: unknown; to 7: know the piece very well). This rating could take into account both their familiarity with the piece at its first occurrence in the experiment, as well as the build-up of familiarity across repetitions. Behavioural results confirmed that participants reported repeated pieces as more familiar (paired t-test on the average familiarity ratings for all participants across repetitions: rep<sub>2</sub> &gt; rep<sub>1</sub>, p=6.9×10<sup>−6</sup>; rep<sub>3</sub> &gt; rep<sub>2</sub>, p=0.003, Bonferroni correction). No significant difference emerged between musicians and non-musicians on this account (two-sample <italic>t</italic>-test, p=0.07, 0.16, 0.19 for repetitions 1, 2, and three respectively). EEG participants undertook the entire experiment (30 trials: ten stimuli repeated three times), ECoG patients were presented with 10 trials (ten stimuli, played with random order).</p></sec><sec id="s4-4"><title>IDyOM</title><p>The Information Dynamics of Music model (IDyOM; <xref ref-type="bibr" rid="bib99">Pearce, 2005</xref>) is a framework based on variable-order hidden Markov models. Given a note sequence of a melody, the probability distribution over every possible note continuation is estimated for every <italic>n</italic>-gram context up to a given length <italic>k</italic> (model order). The distributions for the various orders were combined according to an entropy-based weighting function (IDyOM; <xref ref-type="bibr" rid="bib99">Pearce, 2005</xref>), Section 6.2). Here, we used an unbounded implementation of IDyOM that builds <italic>n</italic>-grams using contexts up to the size of each musical piece. In addition, predictions were the result of a combination of long- and short-term models (LTM and STM respectively), which yields better estimates than either LTM or STM alone. The LTM was the result of a pre-training on a large corpus of Western music that did not include the stimuli presented during the EEG experiment, thus simulating the statistical knowledge of a listener that was implicitly acquired after a life-time of exposure to music. The STM, on the other hand, is constructed online for each individual musical piece that was used in the EEG experiment.</p><p>Our choice of IDyOM was motivated by the empirical support that Markov model-based frameworks received as a model of human melodic expectation (<xref ref-type="bibr" rid="bib103">Pearce and Wiggins, 2006</xref>; <xref ref-type="bibr" rid="bib100">Pearce et al., 2010a</xref>; <xref ref-type="bibr" rid="bib93">Omigie et al., 2013</xref>; <xref ref-type="bibr" rid="bib106">Quiroga-Martinez et al., 2019a</xref>). Specifically, among other evidence, previous work has indicated that it predicts human ratings of uncertainty during music listening (<xref ref-type="bibr" rid="bib50">Hansen and Pearce, 2014</xref>; <xref ref-type="bibr" rid="bib84">Moldwin et al., 2017</xref>; <xref ref-type="bibr" rid="bib6">Bianco et al., 2020</xref>).</p></sec><sec id="s4-5"><title>Music features</title><p>In the present study, we have assessed the coupling between the EEG data and various properties of the musical stimuli. Of course, this required the extraction of such properties from the stimulus data in the first place. First, we defined a set of descriptors summarizing <italic>low-level acoustic properties</italic> of the music stimuli (<bold>A</bold>). Since the specific set of stimuli was monophonic, broadband envelope and fundamental frequency (f<sub>0</sub>) of each note fully characterize the sound acoustics. However, only the envelope descriptor was used in the present study as the frequency information did not explain additional EEG variance. The broadband amplitude envelope was extracted from the acoustic waveform using the Hilbert transform (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In addition, <bold>A</bold> included the half-way rectified first-derivative of the envelope, which was shown to contribute to the stimulus-EEG mapping when using linear system identification methods (<xref ref-type="bibr" rid="bib25">Daube et al., 2019</xref>).</p><p>In order to investigate the cortical processing of melodic expectations, we estimated <italic>melodic surprise</italic> and <italic>entropy</italic> for each individual note of a given musical piece by using IDyOM. Given a note <italic>e<sub>i</sub></italic>, a note sequence <italic>e<sub>1..n</sub></italic> that immediately precedes that note, and an alphabet <italic>E</italic> describing the possible pitch or note-onset values for the note, <italic>melodic surprise</italic> <inline-formula><mml:math id="inf1"><mml:mi>S</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced> <mml:mi/></mml:math></inline-formula> refers to the inverse probability of occurrence of a particular note at a given position in the melody. In other words, this surprise indicates the degree to which a note appearing in a given context in a melody is unexpected, or information content (<xref ref-type="bibr" rid="bib101">Pearce et al., 2010b</xref>; <xref ref-type="bibr" rid="bib75">MacKay and Mac, 2003</xref>):<disp-formula id="equ1"><mml:math id="m1"><mml:mi>S</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac> <mml:mi/><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>The second feature that was extrapolated from IDyOM is the entropy in a given melodic context. This measure was defined as the Shannon entropy (<xref ref-type="bibr" rid="bib123">Shannon, 1948</xref>) computed by averaging the surprise over all possible continuations of the note sequence, as described by E:<disp-formula id="equ2"><mml:math id="m2"><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>e</mml:mi> <mml:mi/><mml:mo>∈</mml:mo> <mml:mi/><mml:mi>E</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>S</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced> <mml:mi/><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Inverse probability and entropy are complementary in that the first indicates the level of expectedness of a note, while the second clarifies whether an unexpected note occurred in a context that was more or less uncertain, thus corresponding to a weaker or stronger note sequence violation respectively.</p><p>IDyOM simulates implicit melodic learning by estimating the probability distribution of each upcoming note. This model can operate on multiple viewpoints, meaning that it can capture the distributions of various properties of music. Here, we focused on two such properties that are considered the most relevant to describe a melody: the <italic>pitch</italic> and the <italic>onset time</italic> of a note. IDyOM generates predictions of upcoming musical events based on what is learned, allowing the estimation of surprise and entropy values for the properties of interest. This provided us with four features describing the prediction of an upcoming note: surprise of pitch (<bold>S<sub>p</sub></bold>), entropy of pitch (<bold>H<sub>p</sub></bold>), surprise of onset time (<bold>S<sub>o</sub></bold>), and entropy of onset time (<bold>H<sub>o</sub></bold>). Each of these features was encoded into time-series by using their values to modulate the amplitude of a note-onset vector that is vectors of zeros marking with value one all note onsets, with length matching that of the corresponding musical piece and with the same sampling frequency as the EEG (or ECoG) data. The matrix composed of the four resulting vectors is referred to as <italic>melodic expectations</italic> feature-set (<bold>M</bold>).</p><p>In order to assess and quantify the contribution of melodic expectations to the music-EEG mapping, the main analyses were conducted on <bold>A</bold> and the concatenation of <bold>A</bold> and <bold>M</bold> (<bold>AM</bold>). The rationale is that the inclusion of M will improve the fitting score if the EEG responses to music are modulated by melodic expectations that is if <bold>M</bold> describes dynamics of the EEG signal that are not redundant with A (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p></sec><sec id="s4-6"><title>Control analysis</title><p>The concatenation of acoustic and melodic expectation features <bold>AM</bold> constitutes a richer representation of a musical piece than <bold>A</bold> or <bold>M</bold> alone, and we hypothesized that it would be a better descriptor of the neural responses to music. However, it is also true that <bold>AM</bold> has more dimensions than <bold>A</bold>, which could be a confounding factor when comparing their coupling with the neural signal. In order to factor out dimensionality from this comparison, we have built stimulus descriptors with the same dimensionality as <bold>AM</bold> that carry the same acoustic information but less meaningful melodic expectation values, the hypothesis being that such descriptors would be less coupled with the neural signal. Such vectors were obtained by degrading the STM model by imposing memory restrictions on the local musical piece, while leaving untouched the LTM model, which represents the participants’ prior knowledge on Western music. The memory restrictions on the STM model were introduced by subdividing each musical piece in chunks of exponentially longer lengths (1, 2, 4, 8, 16, and 32 musical bars) and then calculating the melodic expectations in each chunk separately. Similar results were obtained by reducing the model order <italic>k</italic>. However, the model order restricts the memory in terms of number of notes, while the first approach works in the musical bar dimension, which we considered more relevant and comparable across musical pieces.</p><p>The same analysis was conducted by using a stimulus descriptor consisting of the concatenation of <bold>A</bold> with the <bold>M</bold> descriptor after randomly shuffling the surprise and entropy values in time (but by preserving the note onset times; <bold>AM<sub>shu</sub></bold>), providing us with a feature-set with the same dimensionality and surprise- and entropy-values distributions of <bold>AM</bold> that contains <bold>A</bold> but not <bold>M</bold> information and that was outperformed by <bold>AM</bold> (<italic>r</italic><sub>AM</sub> &gt; <italic>r</italic><sub>A</sub>: permutation test, p&lt;10<sup>−6</sup>, <italic>d</italic> = 1.31).</p></sec><sec id="s4-7"><title>Computational model and data analysis</title><p>A system identification technique was used to compute the channel-specific music-EEG mapping. This method, here referred to as the temporal response function (TRF; <xref ref-type="bibr" rid="bib70">Lalor et al., 2009</xref>; <xref ref-type="bibr" rid="bib30">Ding et al., 2014</xref>), uses a regularized linear regression (<xref ref-type="bibr" rid="bib22">Crosse et al., 2016</xref>) to estimate a filter that optimally describes how the brain transforms a set of stimulus features into the corresponding neural response (forward model; <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Leave-one-out cross-validation (across trials) was used to assess how well the TRF models could predict unseen data while controlling for overfitting. The quality of a prediction was quantified by calculating Pearson’s correlation between the preprocessed recorded signals and the corresponding predictions at each scalp electrode.</p><p>The interaction between stimulus and recorded brain responses is not instantaneous, in fact a sound stimulus at time <italic>t<sub>0</sub></italic> affects the brain signals for a certain time-window [<italic>t<sub>1</sub></italic>, <italic>t<sub>1</sub>+t</italic><sub>win</sub>], with <italic>t<sub>1</sub></italic> ≥0 and <italic>t<sub>win</sub></italic> &gt;0. The TRF takes this into account by including multiple time-lags between stimulus and neural signal, providing us with model weights that can be interpreted in both space (scalp topographies) and time (music-EEG latencies). First, a time-lag window of −150–750 ms was used to fit the TRF models. The temporal dynamics of the music responses were inferred from the TRF model weights, as shown in <xref ref-type="fig" rid="fig2">Figures 2E</xref>, <xref ref-type="fig" rid="fig3">3B</xref> and <xref ref-type="fig" rid="fig4">4B</xref>. We then performed the EEG prediction analysis by restricting the TRF model fit to the window [0350] ms, thus reducing the dimensionality of the data and the risk for overfitting. This time-lag window was identified by means of a backward elimination procedure that quantified the relevance of the various stimulus-EEG latencies to the TRF mapping (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>Backward elimination is a method to perform feature selection on multivariate data (<xref ref-type="bibr" rid="bib47">Guyon and Elisseeff, 2003</xref>) (only the first iteration of this approach was run for computational reasons). In our context, the relevance of a feature (where feature includes both stimulus properties and time-lags) is quantified as the loss in EEG prediction correlation due to its exclusion from the TRF model. Specifically, TRF were fit for the time-lag window −150 and 750 ms after excluding a 50 ms window of time-lags [<italic>t<sub>i</sub></italic>,<italic>t<sub>i</sub></italic>+50] ms. Then, the loss was calculated as <italic>r<sub>LOSS</sub></italic> = <italic>r</italic><sub>[-150,750]</sub> – <italic>r</italic><sub>[-150,750] \ [ti,ti+50]</sub>. Ultimately, this allowed us also to isolate the temporal dynamics of the effect of melodic surprise AM-A. Note that this procedure is similar to a single-lag analysis (<xref ref-type="bibr" rid="bib91">O'Sullivan et al., 2015</xref>; <xref ref-type="bibr" rid="bib24">Das et al., 2016</xref>), with the difference that latencies capturing information that is redundant with other lags will not produce a large <italic>r<sub>LOSS</sub></italic>, which is a useful property when the goal is to minimise the time-latency window. The results of this analysis are presented in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, where only significant <italic>r<sub>LOSS</sub></italic> values are reported (p&lt;0.05, Bonferroni corrected permutation test with <italic>N</italic> = 10000).</p></sec><sec id="s4-8"><title>ERP analysis</title><p>To obtain time-locked neural responses to each note, the neural data were segmented and aligned according to note onset. Notes were grouped into <italic>high</italic> and <italic>low</italic> surprise by selecting the ones with the highest and lowest 20% S<sub>p</sub> values respectively. Among these epochs, we selected neural segments corresponding to notes with equal acoustic envelopes that is the 25% of notes with peak envelope amplitude closest to the median value (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). ERPs and average acoustic envelopes were calculated by averaging the time-aligned neural data over each surprise group. Significant differences in the ERP traces between the two groups were calculated by means of an FDR-corrected permutation test. <xref ref-type="fig" rid="fig5">Figure 5</xref> shows the EEG result for channel Cz and the ECoG result for two selected electrodes. ERP power was calculated in the responsive latency window from 0 to 200 ms. ERP magnitude, which is often reported in μV, is indicated in arbitrary units (a.u.) here to avoid misleading the readers into comparing the absolute values of data from different recording modalities (EEG and ECoG).</p></sec><sec id="s4-9"><title>Statistical analysis</title><p>Statistical analyses were performed using two-tailed permutation tests for pair-wise comparisons. Correction for multiple comparisons was applied where necessary via the false discovery rate (FDR) approach. One-way ANOVA was used to assess when testing the significance of an effect over multiple (&gt;2) groups (e.g., memory size in <xref ref-type="fig" rid="fig2">Figure 2C</xref>). The values reported use the convention <italic>F</italic>(<italic>df</italic>, <italic>df<sub>error</sub></italic>). Greenhouse-Geisser corrections was applied when the assumption of sphericity was not met (as indicated by a significant Mauchly’s test). Cohen’s <italic>d</italic> was used as a measure of effect size.</p><p>Topographical dissimilarity scores were calculated according to <xref ref-type="bibr" rid="bib87">Murray et al. (2008)</xref>:<disp-formula id="equ3"><mml:math id="m3"><mml:mi>D</mml:mi><mml:mi>I</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo> <mml:mi/><mml:msqrt><mml:mn>2</mml:mn><mml:mi>*</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:msqrt></mml:math></disp-formula>where <italic>r</italic> is the correlation between two topographical distributions of interest. Significance was assessed by means of a one-sided <italic>p</italic>-values based on a randomization test with 100 permutations.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>Marcus Pearce and Jens Hjortkjær for useful discussion. Gaelle Rouvier for her help with the data collection. Part of the data analysis and discussions were conducted at the Telluride Cognitive Neuromorphic Engineering Workshop.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Formal analysis, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Data curation, Formal analysis, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Resources, Data curation, Software, Formal analysis, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Data curation, Funding acquisition, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Methodology, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: Experimental procedures were approved by the CERES committee of Paris Descartes University (CERES 2013-11) and by the Feinstein Institute for Medical Research (07-125). All participants provided written informed consent before the experiment.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Tables indicating the coordinates (MNI) of the intracranial electrodes for each patient.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-51784-supp1-v1.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Matlab interactive 3D plots showing the ECoG electrode placement for the first ECoG patient.</title><p>Dots indicate ECoG channels. Red dots indicate channels that were responsive to the music input.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-51784-supp2-v1.zip"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Matlab interactive 3D plots showing the ECoG electrode placement for the second ECoG patient.</title><p>Dots indicate ECoG channels. Red dots indicate channels that were responsive to the music input.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-51784-supp3-v1.zip"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Matlab interactive 3D plots showing the ECoG electrode placement for the third ECoG patient.</title><p>Dots indicate ECoG channels. Red dots indicate channels that were responsive to the music input.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-51784-supp4-v1.zip"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-51784-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All EEG data and stimuli have been deposited on the Dryad repository. The TRF analysis was carried out using the freely available multivariate temporal response function (mTRF) toolbox, which can be downloaded from <ext-link ext-link-type="uri" xlink:href="https://sourceforge.net/projects/aespa/">https://sourceforge.net/projects/aespa/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Giovanni</surname><given-names>M. Di Liberto</given-names></name><name><surname>Claire</surname><given-names>Pelofi</given-names></name><name><surname>Roberta</surname><given-names>Bianco</given-names></name><name><surname>Prachi</surname><given-names>Patel</given-names></name><name><surname>Ashesh</surname><given-names>D Mehta</given-names></name><name><surname>Jose</surname><given-names>L Herrero</given-names></name><name><surname>Alain</surname><given-names>de Cheveigné</given-names></name><name><surname>Shihab</surname><given-names>Shamma</given-names></name><name><surname>Nima</surname><given-names>Mesgarani</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Cortical encoding of melodic expectations in human temporal cortex</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.g1jwstqmh</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attaheri</surname> <given-names>A</given-names></name><name><surname>Kikuchi</surname> <given-names>Y</given-names></name><name><surname>Milne</surname> <given-names>AE</given-names></name><name><surname>Wilson</surname> <given-names>B</given-names></name><name><surname>Alter</surname> <given-names>K</given-names></name><name><surname>Petkov</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>EEG potentials associated with artificial grammar learning in the primate brain</article-title><source>Brain and Language</source><volume>148</volume><fpage>74</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2014.11.006</pub-id><pub-id pub-id-type="pmid">25529405</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auksztulewicz</surname> <given-names>R</given-names></name><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Repetition suppression and its contextual determinants in predictive coding</article-title><source>Cortex</source><volume>80</volume><fpage>125</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.11.024</pub-id><pub-id pub-id-type="pmid">26861557</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name><name><surname>Kassam</surname> <given-names>KS</given-names></name><name><surname>Ghuman</surname> <given-names>AS</given-names></name><name><surname>Boshyan</surname> <given-names>J</given-names></name><name><surname>Schmid</surname> <given-names>AM</given-names></name><name><surname>Schmidt</surname> <given-names>AM</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Hämäläinen</surname> <given-names>MS</given-names></name><name><surname>Marinkovic</surname> <given-names>K</given-names></name><name><surname>Schacter</surname> <given-names>DL</given-names></name><name><surname>Rosen</surname> <given-names>BR</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Top-down facilitation of visual recognition</article-title><source>PNAS</source><volume>103</volume><fpage>449</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1073/pnas.0507062103</pub-id><pub-id pub-id-type="pmid">16407167</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Besson</surname> <given-names>M</given-names></name><name><surname>Macar</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>An event-related potential analysis of incongruity in music and other non-linguistic contexts</article-title><source>Psychophysiology</source><volume>24</volume><fpage>14</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1987.tb01853.x</pub-id><pub-id pub-id-type="pmid">3575590</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname> <given-names>R</given-names></name><name><surname>Novembre</surname> <given-names>G</given-names></name><name><surname>Keller</surname> <given-names>PE</given-names></name><name><surname>Kim</surname> <given-names>SG</given-names></name><name><surname>Scharf</surname> <given-names>F</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name><name><surname>Villringer</surname> <given-names>A</given-names></name><name><surname>Sammler</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural networks for harmonic structure in music perception and action</article-title><source>NeuroImage</source><volume>142</volume><fpage>454</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.025</pub-id><pub-id pub-id-type="pmid">27542722</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname> <given-names>R</given-names></name><name><surname>Ptasczynski</surname> <given-names>LE</given-names></name><name><surname>Omigie</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Pupil responses to pitch deviants reflect predictability of melodic sequences</article-title><source>Brain and Cognition</source><volume>138</volume><elocation-id>103621</elocation-id><pub-id pub-id-type="doi">10.1016/j.bandc.2019.103621</pub-id><pub-id pub-id-type="pmid">31862512</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bigand</surname> <given-names>E</given-names></name><name><surname>Poulin-Charronnat</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Are we “experienced listeners”? A review of the musical capacities that do not depend on formal musical training</article-title><source>Cognition</source><volume>100</volume><fpage>100</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2005.11.007</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borovsky</surname> <given-names>A</given-names></name><name><surname>Elman</surname> <given-names>JL</given-names></name><name><surname>Kutas</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Once is Enough: N400 Indexes Semantic Integration of Novel Word Meanings from a Single Exposure in Context</article-title><source>Language Learning and Development</source><volume>8</volume><fpage>278</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.1080/15475441.2011.614893</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bretan</surname> <given-names>M</given-names></name><name><surname>Oore</surname> <given-names>S</given-names></name><name><surname>Eck</surname> <given-names>D</given-names></name><name><surname>Heck</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning and evaluating musical features with deep autoencoders</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.04486">https://arxiv.org/abs/1706.04486</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname> <given-names>C</given-names></name><name><surname>Hong</surname> <given-names>LE</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Rapid Transformation from Auditory to Linguistic Representations of Continuous Speech</article-title><source>Current Biology</source><volume>28</volume><fpage>3976</fpage><lpage>3983</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.10.042</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname> <given-names>C</given-names></name><name><surname>Presacco</surname> <given-names>A</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Neural source dynamics of brain responses to continuous stimuli: Speech processing from acoustics to comprehension</article-title><source>NeuroImage</source><volume>172</volume><fpage>162</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.042</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Brodbeck</surname> <given-names>C</given-names></name><name><surname>Hong</surname> <given-names>LE</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2018">2018c</year><article-title>Transformation from auditory to linguistic representations across auditory cortex is rapid and attention dependent for continuous speech</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/326785</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broderick</surname> <given-names>MP</given-names></name><name><surname>Anderson</surname> <given-names>AJ</given-names></name><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>Crosse</surname> <given-names>MJ</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Electrophysiological correlates of semantic dissimilarity reflect the comprehension of natural, narrative speech</article-title><source>Current Biology</source><volume>28</volume><fpage>803</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.01.080</pub-id><pub-id pub-id-type="pmid">29478856</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlsen</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Some factors which influence melodic expectancy</article-title><source>Psychomusicology: A Journal of Research in Music Cognition</source><volume>1</volume><fpage>12</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1037/h0094276</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carrus</surname> <given-names>E</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Bhattacharya</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Melodic pitch expectation interacts with neural responses to syntactic but not semantic violations</article-title><source>Cortex</source><volume>49</volume><fpage>2186</fpage><lpage>2200</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2012.08.024</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>EF</given-names></name><name><surname>Rieger</surname> <given-names>JW</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Berger</surname> <given-names>MS</given-names></name><name><surname>Barbaro</surname> <given-names>NM</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Categorical speech representation in human superior temporal gyrus</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1428</fpage><lpage>1432</lpage><pub-id pub-id-type="doi">10.1038/nn.2641</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chennu</surname> <given-names>S</given-names></name><name><surname>Noreika</surname> <given-names>V</given-names></name><name><surname>Gueorguiev</surname> <given-names>D</given-names></name><name><surname>Blenkmann</surname> <given-names>A</given-names></name><name><surname>Kochen</surname> <given-names>S</given-names></name><name><surname>Ibanez</surname> <given-names>A</given-names></name><name><surname>Owen</surname> <given-names>AM</given-names></name><name><surname>Bekinschtein</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Expectation and Attention in Hierarchical Auditory Prediction</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>11194</fpage><lpage>11205</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0114-13.2013</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname> <given-names>VKM</given-names></name><name><surname>Harrison</surname> <given-names>PMC</given-names></name><name><surname>Meyer</surname> <given-names>L</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Haynes</surname> <given-names>J-D</given-names></name><name><surname>Koelsch</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Uncertainty and surprise jointly predict musical pleasure and Amygdala, Hippocampus, and auditory cortex activity</article-title><source>Current Biology</source><volume>29</volume><fpage>4084</fpage><lpage>4092</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.09.067</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title><source>Behavioral and Brain Sciences</source><volume>36</volume><fpage>181</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1017/S0140525X12000477</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coffey</surname> <given-names>EBJ</given-names></name><name><surname>Musacchia</surname> <given-names>G</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cortical Correlates of the Auditory Frequency-Following and Onset Responses: EEG and fMRI Evidence</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>830</fpage><lpage>838</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1265-16.2016</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crone</surname> <given-names>NE</given-names></name><name><surname>Boatman</surname> <given-names>D</given-names></name><name><surname>Gordon</surname> <given-names>B</given-names></name><name><surname>Hao</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Induced electrocorticographic gamma activity during auditory perception. Brazier Award-winning article, 2001</article-title><source>Clinical Neurophysiology</source><volume>112</volume><fpage>565</fpage><lpage>582</lpage><pub-id pub-id-type="doi">10.1016/s1388-2457(00)00545-9</pub-id><pub-id pub-id-type="pmid">11275528</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname> <given-names>MJ</given-names></name><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>Bednar</surname> <given-names>A</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The multivariate temporal response function (mTRF) Toolbox: a MATLAB toolbox for relating neural signals to continuous stimuli</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>604</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id><pub-id pub-id-type="pmid">27965557</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuddy</surname> <given-names>LL</given-names></name><name><surname>Lunney</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Expectancies generated by melodic intervals: perceptual judgments of melodic continuity</article-title><source>Perception &amp; Psychophysics</source><volume>57</volume><fpage>451</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.3758/BF03213071</pub-id><pub-id pub-id-type="pmid">7596743</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname> <given-names>N</given-names></name><name><surname>Biesmans</surname> <given-names>W</given-names></name><name><surname>Bertrand</surname> <given-names>A</given-names></name><name><surname>Francart</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The effect of head-related filtering and ear-specific decoding Bias on auditory attention detection</article-title><source>Journal of Neural Engineering</source><volume>13</volume><elocation-id>056014</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/13/5/056014</pub-id><pub-id pub-id-type="pmid">27618842</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daube</surname> <given-names>C</given-names></name><name><surname>Ince</surname> <given-names>RAA</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Simple acoustic features can explain Phoneme-Based predictions of cortical responses to speech</article-title><source>Current Biology</source><volume>29</volume><fpage>1924</fpage><lpage>1937</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.04.067</pub-id><pub-id pub-id-type="pmid">31130454</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destrieux</surname> <given-names>C</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Dale</surname> <given-names>A</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</article-title><source>NeuroImage</source><volume>53</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.06.010</pub-id><pub-id pub-id-type="pmid">20547229</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>O'Sullivan</surname> <given-names>JA</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Low-Frequency cortical entrainment to speech reflects Phoneme-Level processing</article-title><source>Current Biology</source><volume>25</volume><fpage>2457</fpage><lpage>2465</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.030</pub-id><pub-id pub-id-type="pmid">26412129</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>Wong</surname> <given-names>D</given-names></name><name><surname>Melnik</surname> <given-names>GA</given-names></name><name><surname>de Cheveigné</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Low-frequency cortical responses to natural speech reflect probabilistic phonotactics</article-title><source>NeuroImage</source><volume>196</volume><fpage>237</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.04.037</pub-id><pub-id pub-id-type="pmid">30991126</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>Pelofi</surname> <given-names>C</given-names></name><name><surname>Shamma</surname> <given-names>S</given-names></name><name><surname>de Cheveigné</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Musical expertise enhances the cortical tracking of the acoustic envelope during naturalistic music listening</article-title><source>Acoustical Science and Technology</source><volume>41</volume><fpage>361</fpage><lpage>364</lpage><pub-id pub-id-type="doi">10.1250/ast.41.361</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Chatterjee</surname> <given-names>M</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure</article-title><source>NeuroImage</source><volume>88</volume><fpage>41</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.054</pub-id><pub-id pub-id-type="pmid">24188816</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Neural coding of continuous speech in auditory cortex during monaural and dichotic listening</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>78</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1152/jn.00297.2011</pub-id><pub-id pub-id-type="pmid">21975452</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title><source>PNAS</source><volume>109</volume><fpage>11854</fpage><lpage>11859</lpage><pub-id pub-id-type="doi">10.1073/pnas.1205381109</pub-id><pub-id pub-id-type="pmid">22753470</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunsby</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>On repeat: how music plays the mind. By Elizabeth Hellmuth Margulis</article-title><source>Music and Letters</source><volume>95</volume><fpage>497</fpage><lpage>499</lpage><pub-id pub-id-type="doi">10.1093/ml/gcu055</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edwards</surname> <given-names>E</given-names></name><name><surname>Soltani</surname> <given-names>M</given-names></name><name><surname>Kim</surname> <given-names>W</given-names></name><name><surname>Dalal</surname> <given-names>SS</given-names></name><name><surname>Nagarajan</surname> <given-names>SS</given-names></name><name><surname>Berger</surname> <given-names>MS</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Comparison of time-frequency responses and the event-related potential to auditory speech stimuli in human cortex</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>377</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1152/jn.90954.2008</pub-id><pub-id pub-id-type="pmid">19439673</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eerola</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>The Dynamics of Musical Expectancy Cross-Cultural and Statistical Approaches to Melodic Expectations</source><publisher-name>University of Jyväskylä</publisher-name></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eerola</surname> <given-names>T</given-names></name><name><surname>Louhivuori</surname> <given-names>J</given-names></name><name><surname>Lebaka</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Expectancy in Sami Yoiks revisited: the role of data-driven and schema-driven knowledge in the formation of melodic expectations</article-title><source>Musicae Scientiae</source><volume>13</volume><fpage>231</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1177/102986490901300203</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erickson</surname> <given-names>LC</given-names></name><name><surname>Thiessen</surname> <given-names>ED</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Statistical learning of language: theory, validity, and predictions of a statistical learning account of language acquisition</article-title><source>Developmental Review</source><volume>37</volume><fpage>66</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.dr.2015.05.002</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiedler</surname> <given-names>L</given-names></name><name><surname>Wöstmann</surname> <given-names>M</given-names></name><name><surname>Graversen</surname> <given-names>C</given-names></name><name><surname>Brandmeyer</surname> <given-names>A</given-names></name><name><surname>Lunner</surname> <given-names>T</given-names></name><name><surname>Obleser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Single-channel in-ear-EEG detects the focus of auditory attention to concurrent tone streams and mixed speech</article-title><source>Journal of Neural Engineering</source><volume>14</volume><elocation-id>036020</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aa66dd</pub-id><pub-id pub-id-type="pmid">28384124</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finn</surname> <given-names>AS</given-names></name><name><surname>Lee</surname> <given-names>T</given-names></name><name><surname>Kraus</surname> <given-names>A</given-names></name><name><surname>Hudson Kam</surname> <given-names>CL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>When It Hurts (and Helps) to Try: The Role of Effort in Language Learning</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e101806</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0101806</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fishman</surname> <given-names>YI</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The mechanisms and meaning of the mismatch negativity</article-title><source>Brain Topography</source><volume>27</volume><fpage>500</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1007/s10548-013-0337-3</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitch</surname> <given-names>WT</given-names></name><name><surname>Martins</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Hierarchical processing in music, language, and action: lashley revisited</article-title><source>Annals of the New York Academy of Sciences</source><volume>1316</volume><fpage>87</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1111/nyas.12406</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name><name><surname>Kiebel</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Predictive coding under the free-energy principle</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>364</volume><fpage>1211</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0300</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrido</surname> <given-names>MI</given-names></name><name><surname>Kilner</surname> <given-names>JM</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The mismatch negativity: a review of underlying mechanisms</article-title><source>Clinical Neurophysiology</source><volume>120</volume><fpage>453</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2008.11.029</pub-id><pub-id pub-id-type="pmid">19181570</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname> <given-names>BP</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Mas-Herrero</surname> <given-names>E</given-names></name><name><surname>Dagher</surname> <given-names>A</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predictability and uncertainty in the pleasure of music: a reward for learning?</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>9397</fpage><lpage>9409</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0428-19.2019</pub-id><pub-id pub-id-type="pmid">31636112</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Warren</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The planum temporale as a computational hub</article-title><source>Trends in Neurosciences</source><volume>25</volume><fpage>348</fpage><lpage>353</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(02)02191-4</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groppe</surname> <given-names>DM</given-names></name><name><surname>Bickel</surname> <given-names>S</given-names></name><name><surname>Dykstra</surname> <given-names>AR</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Mégevand</surname> <given-names>P</given-names></name><name><surname>Mercier</surname> <given-names>MR</given-names></name><name><surname>Lado</surname> <given-names>FA</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Honey</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>iELVis: An open source MATLAB toolbox for localizing and visualizing human intracranial electrode data</article-title><source>Journal of Neuroscience Methods</source><volume>281</volume><fpage>40</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.01.022</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guyon</surname> <given-names>I</given-names></name><name><surname>Elisseeff</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>An introduction to variable and feature selection</article-title><source>Journal of Machine Learning Research</source><volume>3</volume><fpage>1157</fpage><lpage>1182</lpage></element-citation></ref><ref id="bib48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hale</surname> <given-names>J</given-names></name><name><surname>Dyer</surname> <given-names>C</given-names></name><name><surname>Kuncoro</surname> <given-names>A</given-names></name><name><surname>Brennan</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Finding syntax in human encephalography with beam search</article-title><conf-name>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</conf-name><conf-loc>Melbourne, Australia</conf-loc><fpage>2727</fpage><lpage>2736</lpage><pub-id pub-id-type="doi">10.18653/v1/P18-1254</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hannon</surname> <given-names>EE</given-names></name><name><surname>Soley</surname> <given-names>G</given-names></name><name><surname>Ullal</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Familiarity overrides complexity in rhythm perception: a cross-cultural comparison of american and turkish listeners</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>38</volume><fpage>543</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1037/a0027225</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname> <given-names>NC</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Predictive uncertainty in auditory sequence processing</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>1052</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.01052</pub-id><pub-id pub-id-type="pmid">25295018</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffner</surname> <given-names>CC</given-names></name><name><surname>Slevc</surname> <given-names>LR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Prosodic structure as a parallel to musical structure</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>1962</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.01962</pub-id><pub-id pub-id-type="pmid">26733930</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Henin</surname> <given-names>S</given-names></name><name><surname>Turk-Browne</surname> <given-names>N</given-names></name><name><surname>Friedman</surname> <given-names>D</given-names></name><name><surname>Liu</surname> <given-names>A</given-names></name><name><surname>Dugan</surname> <given-names>P</given-names></name><name><surname>Flinker</surname> <given-names>A</given-names></name><name><surname>Doyle</surname> <given-names>W</given-names></name><name><surname>Devinsky</surname> <given-names>O</given-names></name><name><surname>Melloni</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Statistical learning shapes neural sequence representations</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/583856</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hickok</surname> <given-names>G</given-names></name><name><surname>Saberi</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Redefining the Functional Organization of the Planum Temporale Region: Space, Objects and Sensory–Motor Integration</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4614-2314-0_12</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huron</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Sweet Anticipation : Music and the Psychology of Expectation</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jentschke</surname> <given-names>S</given-names></name><name><surname>Koelsch</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Musical training modulates the development of syntax processing in children</article-title><source>NeuroImage</source><volume>47</volume><fpage>735</fpage><lpage>744</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.04.090</pub-id><pub-id pub-id-type="pmid">19427908</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kessler</surname> <given-names>EJ</given-names></name><name><surname>Hansen</surname> <given-names>C</given-names></name><name><surname>Shepard</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Tonal schemata in the perception of music in Bali and in the west</article-title><source>Music Perception: An Interdisciplinary Journal</source><volume>2</volume><fpage>131</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.2307/40285289</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khalighinejad</surname> <given-names>B</given-names></name><name><surname>Cruzatto da Silva</surname> <given-names>G</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic encoding of acoustic features in neural responses to continuous speech</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>2176</fpage><lpage>2185</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2383-16.2017</pub-id><pub-id pub-id-type="pmid">28119400</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname> <given-names>S</given-names></name><name><surname>Gunter</surname> <given-names>T</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name><name><surname>Schröger</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Brain indices of music processing: &quot;nonmusicians&quot; are musical</article-title><source>Journal of Cognitive Neuroscience</source><volume>12</volume><fpage>520</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1162/089892900562183</pub-id><pub-id pub-id-type="pmid">10931776</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname> <given-names>S</given-names></name><name><surname>Schmidt</surname> <given-names>B-helmer</given-names></name><name><surname>Kansok</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Effects of musical expertise on the early right anterior negativity: an event-related brain potential study</article-title><source>Psychophysiology</source><volume>39</volume><fpage>657</fpage><lpage>663</lpage><pub-id pub-id-type="doi">10.1111/1469-8986.3950657</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname> <given-names>S</given-names></name><name><surname>Grossmann</surname> <given-names>T</given-names></name><name><surname>Gunter</surname> <given-names>TC</given-names></name><name><surname>Hahne</surname> <given-names>A</given-names></name><name><surname>Schröger</surname> <given-names>E</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Children processing music: electric brain responses reveal musical competence and gender differences</article-title><source>Journal of Cognitive Neuroscience</source><volume>15</volume><fpage>683</fpage><lpage>693</lpage><pub-id pub-id-type="doi">10.1162/jocn.2003.15.5.683</pub-id><pub-id pub-id-type="pmid">12965042</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname> <given-names>S</given-names></name><name><surname>Jentschke</surname> <given-names>S</given-names></name><name><surname>Sammler</surname> <given-names>D</given-names></name><name><surname>Mietchen</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Untangling syntactic and sensory processing: an ERP study of music perception</article-title><source>Psychophysiology</source><volume>44</volume><fpage>476</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2007.00517.x</pub-id><pub-id pub-id-type="pmid">17433099</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Music-syntactic processing and auditory memory: similarities and differences between ERAN and MMN</article-title><source>Psychophysiology</source><volume>46</volume><fpage>179</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2008.00752.x</pub-id><pub-id pub-id-type="pmid">19055508</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Brain correlates of music-evoked emotions</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>170</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1038/nrn3666</pub-id><pub-id pub-id-type="pmid">24552785</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname> <given-names>S</given-names></name><name><surname>Jentschke</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Short-term effects of processing musical syntax: an ERP study</article-title><source>Brain Research</source><volume>1212</volume><fpage>55</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2007.10.078</pub-id><pub-id pub-id-type="pmid">18439987</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>265</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id><pub-id pub-id-type="pmid">22841311</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krumhansl</surname> <given-names>CL</given-names></name><name><surname>Toivanen</surname> <given-names>P</given-names></name><name><surname>Eerola</surname> <given-names>T</given-names></name><name><surname>Toiviainen</surname> <given-names>P</given-names></name><name><surname>Järvinen</surname> <given-names>T</given-names></name><name><surname>Louhivuori</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Cross-cultural music cognition: cognitive methodology applied to north sami yoiks</article-title><source>Cognition</source><volume>76</volume><fpage>13</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/S0010-0277(00)00068-8</pub-id><pub-id pub-id-type="pmid">10822042</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubanek</surname> <given-names>J</given-names></name><name><surname>Brunner</surname> <given-names>P</given-names></name><name><surname>Gunduz</surname> <given-names>A</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Schalk</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The tracking of speech envelope in the human cortex</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e53398</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0053398</pub-id><pub-id pub-id-type="pmid">23408924</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname> <given-names>M</given-names></name><name><surname>Federmeier</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP)</article-title><source>Annual Review of Psychology</source><volume>62</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id><pub-id pub-id-type="pmid">20809790</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalor</surname> <given-names>EC</given-names></name><name><surname>Pearlmutter</surname> <given-names>BA</given-names></name><name><surname>Reilly</surname> <given-names>RB</given-names></name><name><surname>McDarby</surname> <given-names>G</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The VESPA: a method for the rapid estimation of a visual evoked potential</article-title><source>NeuroImage</source><volume>32</volume><fpage>1549</fpage><lpage>1561</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.05.054</pub-id><pub-id pub-id-type="pmid">16875844</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalor</surname> <given-names>EC</given-names></name><name><surname>Power</surname> <given-names>AJ</given-names></name><name><surname>Reilly</surname> <given-names>RB</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Resolving precise temporal processing properties of the auditory system using continuous stimuli</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>349</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1152/jn.90896.2008</pub-id><pub-id pub-id-type="pmid">19439675</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lecaignard</surname> <given-names>F</given-names></name><name><surname>Bertrand</surname> <given-names>O</given-names></name><name><surname>Gimenez</surname> <given-names>G</given-names></name><name><surname>Mattout</surname> <given-names>J</given-names></name><name><surname>Caclin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Implicit learning of predictable sound sequences modulates human brain responses at different levels of the auditory hierarchy</article-title><source>Frontiers in Human Neuroscience</source><volume>9</volume><elocation-id>505</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2015.00505</pub-id><pub-id pub-id-type="pmid">26441602</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonard</surname> <given-names>MK</given-names></name><name><surname>Baud</surname> <given-names>MO</given-names></name><name><surname>Sjerps</surname> <given-names>MJ</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual restoration of masked speech in human cortex</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13619</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13619</pub-id><pub-id pub-id-type="pmid">27996973</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loui</surname> <given-names>P</given-names></name><name><surname>Grent-'t-Jong</surname> <given-names>T</given-names></name><name><surname>Torpey</surname> <given-names>D</given-names></name><name><surname>Woldorff</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Effects of attention on the neural processing of harmonic syntax in western music</article-title><source>Cognitive Brain Research</source><volume>25</volume><fpage>678</fpage><lpage>687</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2005.08.019</pub-id><pub-id pub-id-type="pmid">16257518</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Luck</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>An Introduction to the Event-Related Potential Technique</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib75"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>MacKay</surname> <given-names>D</given-names></name><name><surname>Mac</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Information Theory Inference And Learning Algorithms</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maess</surname> <given-names>B</given-names></name><name><surname>Koelsch</surname> <given-names>S</given-names></name><name><surname>Gunter</surname> <given-names>TC</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Musical syntax is processed in broca's area: an MEG study</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>540</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1038/87502</pub-id><pub-id pub-id-type="pmid">11319564</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margulis</surname> <given-names>EH</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A model of melodic expectation</article-title><source>Music Perception: An Interdisciplinary Journal</source><volume>22</volume><fpage>663</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1525/mp.2005.22.4.663</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGettigan</surname> <given-names>C</given-names></name><name><surname>Faulkner</surname> <given-names>A</given-names></name><name><surname>Altarelli</surname> <given-names>I</given-names></name><name><surname>Obleser</surname> <given-names>J</given-names></name><name><surname>Baverstock</surname> <given-names>H</given-names></name><name><surname>Scott</surname> <given-names>SK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Speech comprehension aided by multiple modalities: behavioural and neural interactions</article-title><source>Neuropsychologia</source><volume>50</volume><fpage>762</fpage><lpage>776</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.01.010</pub-id><pub-id pub-id-type="pmid">22266262</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Klein</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Clinical utility of functional magnetic resonance imaging for brain mapping in epilepsy surgery</article-title><source>Epilepsy Research</source><volume>89</volume><fpage>126</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1016/j.eplepsyres.2009.12.001</pub-id><pub-id pub-id-type="pmid">20211545</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Cheung</surname> <given-names>C</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Phonetic feature encoding in human superior temporal gyrus</article-title><source>Science</source><volume>343</volume><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id><pub-id pub-id-type="pmid">24482117</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>KJ</given-names></name><name><surname>Leuthardt</surname> <given-names>EC</given-names></name><name><surname>Schalk</surname> <given-names>G</given-names></name><name><surname>Rao</surname> <given-names>RP</given-names></name><name><surname>Anderson</surname> <given-names>NR</given-names></name><name><surname>Moran</surname> <given-names>DW</given-names></name><name><surname>Miller</surname> <given-names>JW</given-names></name><name><surname>Ojemann</surname> <given-names>JG</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spectral changes in cortical surface potentials during motor movement</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>2424</fpage><lpage>2432</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3886-06.2007</pub-id><pub-id pub-id-type="pmid">17329441</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miranda</surname> <given-names>RA</given-names></name><name><surname>Ullman</surname> <given-names>MT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Double dissociation between rules and memory in music: an event-related potential study</article-title><source>NeuroImage</source><volume>38</volume><fpage>331</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.07.034</pub-id><pub-id pub-id-type="pmid">17855126</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moerel</surname> <given-names>M</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An anatomical and functional topography of human auditory cortical Areas</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>225</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00225</pub-id><pub-id pub-id-type="pmid">25120426</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moldwin</surname> <given-names>T</given-names></name><name><surname>Schwartz</surname> <given-names>O</given-names></name><name><surname>Sussman</surname> <given-names>ES</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Statistical learning of melodic patterns influences the brain's Response to Wrong Notes</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>2114</fpage><lpage>2122</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01181</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgan</surname> <given-names>E</given-names></name><name><surname>Fogel</surname> <given-names>A</given-names></name><name><surname>Nair</surname> <given-names>A</given-names></name><name><surname>Patel</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Statistical learning and Gestalt-like principles predict melodic expectations</article-title><source>Cognition</source><volume>189</volume><fpage>23</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2018.12.015</pub-id><pub-id pub-id-type="pmid">30913527</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrison</surname> <given-names>SJ</given-names></name><name><surname>Demorest</surname> <given-names>SM</given-names></name><name><surname>Stambaugh</surname> <given-names>LA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Enculturation effects in music cognition</article-title><source>Journal of Research in Music Education</source><volume>56</volume><fpage>118</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1177/0022429408322854</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>MM</given-names></name><name><surname>Brunet</surname> <given-names>D</given-names></name><name><surname>Michel</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Topographic ERP analyses: a step-by-step tutorial review</article-title><source>Brain Topography</source><volume>20</volume><fpage>249</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.1007/s10548-008-0054-5</pub-id><pub-id pub-id-type="pmid">18347966</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname> <given-names>D</given-names></name><name><surname>McQueen</surname> <given-names>JM</given-names></name><name><surname>Cutler</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prediction, bayesian inference and feedback in speech recognition</article-title><source>Language, Cognition and Neuroscience</source><volume>31</volume><fpage>4</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1080/23273798.2015.1081703</pub-id><pub-id pub-id-type="pmid">26740960</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nourski</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Auditory processing in the human cortex: an intracranial electrophysiology perspective</article-title><source>Laryngoscope Investigative Otolaryngology</source><volume> 2</volume><fpage>147</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1002/lio2.73</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nourski</surname> <given-names>KV</given-names></name><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Rhone</surname> <given-names>AE</given-names></name><name><surname>Kawasaki</surname> <given-names>H</given-names></name><name><surname>Howard</surname> <given-names>MA</given-names></name><name><surname>Banks</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Processing of auditory novelty across the cortical hierarchy: an intracranial electrophysiology study</article-title><source>NeuroImage</source><volume>183</volume><fpage>412</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.08.027</pub-id><pub-id pub-id-type="pmid">30114466</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Sullivan</surname> <given-names>JA</given-names></name><name><surname>Power</surname> <given-names>AJ</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Rajaram</surname> <given-names>S</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name><name><surname>Slaney</surname> <given-names>M</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attentional selection in a cocktail party environment can be decoded from Single-Trial EEG</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>1697</fpage><lpage>1706</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht355</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oechslin</surname> <given-names>MS</given-names></name><name><surname>Van De Ville</surname> <given-names>D</given-names></name><name><surname>Lazeyras</surname> <given-names>F</given-names></name><name><surname>Hauert</surname> <given-names>C-A</given-names></name><name><surname>James</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Degree of Musical Expertise Modulates Higher Order Brain Functioning</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>2213</fpage><lpage>2224</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs206</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Omigie</surname> <given-names>D</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Williamson</surname> <given-names>VJ</given-names></name><name><surname>Stewart</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Electrophysiological correlates of melodic processing in congenital amusia</article-title><source>Neuropsychologia</source><volume>51</volume><fpage>1749</fpage><lpage>1762</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2013.05.010</pub-id><pub-id pub-id-type="pmid">23707539</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Omigie</surname> <given-names>D</given-names></name><name><surname>Pearce</surname> <given-names>M</given-names></name><name><surname>Lehongre</surname> <given-names>K</given-names></name><name><surname>Hasboun</surname> <given-names>D</given-names></name><name><surname>Navarro</surname> <given-names>V</given-names></name><name><surname>Adam</surname> <given-names>C</given-names></name><name><surname>Samson</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Intracranial recordings and computational modeling of music reveal the time course of prediction error signaling in frontal and temporal cortices</article-title><source>Journal of Cognitive Neuroscience</source><volume>31</volume><fpage>855</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01388</pub-id><pub-id pub-id-type="pmid">30883293</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Osterhout</surname> <given-names>L</given-names></name><name><surname>Holcomb</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>Event - Related Potentials and Language</chapter-title><source>Electrophysiology of the Mind: Event - Related Brain Potentials and Cognition</source><publisher-name>Oxford University Press</publisher-name><fpage>171</fpage><lpage>187</lpage></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paller</surname> <given-names>KA</given-names></name><name><surname>McCarthy</surname> <given-names>G</given-names></name><name><surname>Wood</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Event-related potentials elicited by deviant endings to melodies</article-title><source>Psychophysiology</source><volume>29</volume><fpage>202</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1992.tb01686.x</pub-id><pub-id pub-id-type="pmid">1635962</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pantev</surname> <given-names>C</given-names></name><name><surname>Roberts</surname> <given-names>LE</given-names></name><name><surname>Schulz</surname> <given-names>M</given-names></name><name><surname>Engelien</surname> <given-names>A</given-names></name><name><surname>Ross</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Timbre-specific enhancement of auditory cortical representations in musicians</article-title><source>Neuroreport</source><volume>12</volume><fpage>169</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1097/00001756-200101220-00041</pub-id><pub-id pub-id-type="pmid">11201080</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Language, music, syntax and the brain</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>674</fpage><lpage>681</lpage><pub-id pub-id-type="doi">10.1038/nn1082</pub-id><pub-id pub-id-type="pmid">12830158</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>MT</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The construction and evaluation of statistical models of melodic structure in music perception and composition (unpublished doctoral thesis)</article-title><publisher-name>City University London</publisher-name></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Müllensiefen</surname> <given-names>D</given-names></name><name><surname>Wiggins</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2010">2010a</year><article-title>The role of expectation and probabilistic learning in auditory boundary perception: a model comparison</article-title><source>Perception</source><volume>39</volume><fpage>1367</fpage><lpage>1391</lpage><pub-id pub-id-type="doi">10.1068/p6507</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Ruiz</surname> <given-names>MH</given-names></name><name><surname>Kapasi</surname> <given-names>S</given-names></name><name><surname>Wiggins</surname> <given-names>GA</given-names></name><name><surname>Bhattacharya</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010b</year><article-title>Unsupervised statistical learning underpins computational, behavioural, and neural manifestations of musical expectation</article-title><source>NeuroImage</source><volume>50</volume><fpage>302</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.12.019</pub-id><pub-id pub-id-type="pmid">20005297</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>MT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Statistical learning and probabilistic prediction in music cognition: mechanisms of stylistic enculturation</article-title><source>Annals of the New York Academy of Sciences</source><volume>1423</volume><fpage>378</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1111/nyas.13654</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Wiggins</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Expectation in melody: the influence of context and learning</article-title><source>Music Perception</source><volume>23</volume><fpage>377</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1525/mp.2006.23.5.377</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Wiggins</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Auditory expectation: the information dynamics of music perception and cognition</article-title><source>Topics in Cognitive Science</source><volume>4</volume><fpage>625</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1111/j.1756-8765.2012.01214.x</pub-id><pub-id pub-id-type="pmid">22847872</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qi</surname> <given-names>Z</given-names></name><name><surname>Beach</surname> <given-names>SD</given-names></name><name><surname>Finn</surname> <given-names>AS</given-names></name><name><surname>Minas</surname> <given-names>J</given-names></name><name><surname>Goetz</surname> <given-names>C</given-names></name><name><surname>Chan</surname> <given-names>B</given-names></name><name><surname>Gabrieli</surname> <given-names>JDE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Native-language N400 and P600 predict dissociable language-learning abilities in adults</article-title><source>Neuropsychologia</source><volume>98</volume><fpage>177</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2016.10.005</pub-id><pub-id pub-id-type="pmid">27737775</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga-Martinez</surname> <given-names>DR</given-names></name><name><surname>Hansen</surname> <given-names>NC</given-names></name><name><surname>Højlund</surname> <given-names>A</given-names></name><name><surname>Pearce</surname> <given-names>MT</given-names></name><name><surname>Brattico</surname> <given-names>E</given-names></name><name><surname>Vuust</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Reduced prediction error responses in high-as compared to low-uncertainty musical contexts</article-title><source>Cortex</source><volume>120</volume><fpage>181</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2019.06.010</pub-id><pub-id pub-id-type="pmid">31323458</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Quiroga-Martinez</surname> <given-names>DR</given-names></name><name><surname>Hansen</surname> <given-names>NC</given-names></name><name><surname>Højlund</surname> <given-names>A</given-names></name><name><surname>Pearce</surname> <given-names>M</given-names></name><name><surname>Brattico</surname> <given-names>E</given-names></name><name><surname>Vuust</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Decomposing neural responses to melodic surprise in musicians and non-musicians: evidence for a hierarchy of predictions in the auditory system</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/786574</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ray</surname> <given-names>S</given-names></name><name><surname>Crone</surname> <given-names>NE</given-names></name><name><surname>Niebur</surname> <given-names>E</given-names></name><name><surname>Franaszczuk</surname> <given-names>PJ</given-names></name><name><surname>Hsiao</surname> <given-names>SS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural correlates of high-gamma oscillations (60-200 hz) in macaque local field potentials and their potential implications in electrocorticography</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>11526</fpage><lpage>11536</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2848-08.2008</pub-id><pub-id pub-id-type="pmid">18987189</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Reck</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>Music of the Whole Earth</source><publisher-name>Da Capo Press</publisher-name></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogalsky</surname> <given-names>C</given-names></name><name><surname>Rong</surname> <given-names>F</given-names></name><name><surname>Saberi</surname> <given-names>K</given-names></name><name><surname>Hickok</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional anatomy of language and music perception: temporal and structural factors investigated using functional magnetic resonance imaging</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>3843</fpage><lpage>3852</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4515-10.2011</pub-id><pub-id pub-id-type="pmid">21389239</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohrmeier</surname> <given-names>M</given-names></name><name><surname>Rebuschat</surname> <given-names>P</given-names></name><name><surname>Cross</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Incidental and online learning of melodic structure</article-title><source>Consciousness and Cognition</source><volume>20</volume><fpage>214</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2010.07.004</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rohrmeier</surname> <given-names>M</given-names></name><name><surname>Cross</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Statistical properties of harmony in bach’s Chorales</article-title><conf-name>Proceedings of the 10th International Conference on Music Perception and Cognition</conf-name><fpage>619</fpage><lpage>627</lpage></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romberg</surname> <given-names>AR</given-names></name><name><surname>Saffran</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Statistical learning and language acquisition</article-title><source>Wiley Interdisciplinary Reviews: Cognitive Science</source><volume>1</volume><fpage>906</fpage><lpage>914</lpage><pub-id pub-id-type="doi">10.1002/wcs.78</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saffran</surname> <given-names>JR</given-names></name><name><surname>Newport</surname> <given-names>EL</given-names></name><name><surname>Aslin</surname> <given-names>RN</given-names></name><name><surname>Tunick</surname> <given-names>RA</given-names></name><name><surname>Barrueco</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Incidental Language Learning: Listening (and Learning) Out of the Corner of Your Ear</article-title><source>Psychological Science</source><volume>8</volume><fpage>101</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.1997.tb00690.x</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salimpoor</surname> <given-names>VN</given-names></name><name><surname>Benovoy</surname> <given-names>M</given-names></name><name><surname>Longo</surname> <given-names>G</given-names></name><name><surname>Cooperstock</surname> <given-names>JR</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The rewarding aspects of music listening are related to degree of emotional arousal</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e7487</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0007487</pub-id><pub-id pub-id-type="pmid">19834599</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salimpoor</surname> <given-names>VN</given-names></name><name><surname>van den Bosch</surname> <given-names>I</given-names></name><name><surname>Kovacevic</surname> <given-names>N</given-names></name><name><surname>McIntosh</surname> <given-names>AR</given-names></name><name><surname>Dagher</surname> <given-names>A</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Interactions between the nucleus accumbens and auditory cortices predict music reward value</article-title><source>Science</source><volume>340</volume><fpage>216</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1126/science.1231059</pub-id><pub-id pub-id-type="pmid">23580531</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salimpoor</surname> <given-names>VN</given-names></name><name><surname>Zald</surname> <given-names>DH</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name><name><surname>Dagher</surname> <given-names>A</given-names></name><name><surname>McIntosh</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predictions and the brain: how musical sounds become rewarding</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>86</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.12.001</pub-id><pub-id pub-id-type="pmid">25534332</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sammler</surname> <given-names>D</given-names></name><name><surname>Koelsch</surname> <given-names>S</given-names></name><name><surname>Ball</surname> <given-names>T</given-names></name><name><surname>Brandt</surname> <given-names>A</given-names></name><name><surname>Grigutsch</surname> <given-names>M</given-names></name><name><surname>Huppertz</surname> <given-names>HJ</given-names></name><name><surname>Knösche</surname> <given-names>TR</given-names></name><name><surname>Wellmer</surname> <given-names>J</given-names></name><name><surname>Widman</surname> <given-names>G</given-names></name><name><surname>Elger</surname> <given-names>CE</given-names></name><name><surname>Friederici</surname> <given-names>AD</given-names></name><name><surname>Schulze-Bonhage</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Co-localizing linguistic and musical syntax with intracranial EEG</article-title><source>NeuroImage</source><volume>64</volume><fpage>134</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.09.035</pub-id><pub-id pub-id-type="pmid">23000255</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaal</surname> <given-names>NK</given-names></name><name><surname>Williamson</surname> <given-names>VJ</given-names></name><name><surname>Kelly</surname> <given-names>M</given-names></name><name><surname>Muggleton</surname> <given-names>NG</given-names></name><name><surname>Pollok</surname> <given-names>B</given-names></name><name><surname>Krause</surname> <given-names>V</given-names></name><name><surname>Banissy</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A causal involvement of the left supramarginal gyrus during the retention of musical pitches</article-title><source>Cortex</source><volume>64</volume><fpage>310</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2014.11.011</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaal</surname> <given-names>NK</given-names></name><name><surname>Pollok</surname> <given-names>B</given-names></name><name><surname>Banissy</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hemispheric differences between left and right supramarginal gyrus for pitch and rhythm memory</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>42456</elocation-id><pub-id pub-id-type="doi">10.1038/srep42456</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmuckler</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Expectation in Music: Investigation of Melodic and Harmonic Processes</article-title><source>Music Perception: An Interdisciplinary Journal</source><volume>7</volume><fpage>109</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.2307/40285454</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schönwiesner</surname> <given-names>M</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Depth electrode recordings show double dissociation between pitch processing in lateral Heschl’s gyrus and sound onset processing in medial Heschl’s gyrus</article-title><source>Experimental Brain Research</source><volume>187</volume><fpage>97</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1007/s00221-008-1286-z</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>A Mathematical Theory of Communication</article-title><source>Bell System Technical Journal</source><volume>27</volume><fpage>379</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shany</surname> <given-names>O</given-names></name><name><surname>Singer</surname> <given-names>N</given-names></name><name><surname>Gold</surname> <given-names>BP</given-names></name><name><surname>Jacoby</surname> <given-names>N</given-names></name><name><surname>Tarrasch</surname> <given-names>R</given-names></name><name><surname>Hendler</surname> <given-names>T</given-names></name><name><surname>Granot</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Surprise-related activation in the nucleus accumbens interacts with music-induced pleasantness</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>14</volume><fpage>459</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1093/scan/nsz019</pub-id><pub-id pub-id-type="pmid">30892654</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skerritt-Davis</surname> <given-names>B</given-names></name><name><surname>Elhilali</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Detecting change in stochastic sound sequences</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006162</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006162</pub-id><pub-id pub-id-type="pmid">29813049</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Somers</surname> <given-names>B</given-names></name><name><surname>Verschueren</surname> <given-names>E</given-names></name><name><surname>Francart</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural tracking of the speech envelope in cochlear implant users</article-title><source>Journal of Neural Engineering</source><volume>16</volume><elocation-id>16003</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aae6b9</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Southwell</surname> <given-names>R</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Enhanced deviant responses in patterned relative to random sound sequences</article-title><source>Cortex</source><volume>109</volume><fpage>92</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2018.08.032</pub-id><pub-id pub-id-type="pmid">30312781</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinschneider</surname> <given-names>M</given-names></name><name><surname>Fishman</surname> <given-names>YI</given-names></name><name><surname>Arezzo</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spectrotemporal analysis of evoked and induced electroencephalographic responses in primary auditory cortex (A1) of the awake monkey</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>610</fpage><lpage>625</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm094</pub-id><pub-id pub-id-type="pmid">17586604</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storkel</surname> <given-names>HL</given-names></name><name><surname>Rogers</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The effect of probabilistic phonotactics on lexical acquistion</article-title><source>Clinical Linguistics &amp; Phonetics</source><volume>14</volume><fpage>407</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1080/026992000415859</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strauß</surname> <given-names>A</given-names></name><name><surname>Kotz</surname> <given-names>SA</given-names></name><name><surname>Obleser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Narrowed Expectancies under Degraded Speech: Revisiting the N400</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1383</fpage><lpage>1395</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00389</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Temperley</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A probabilistic model of melody perception</article-title><source>Cognitive Science: A Multidisciplinary Journal</source><volume>32</volume><fpage>418</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1080/03640210701864089</pub-id><pub-id pub-id-type="pmid">21635341</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Temperley</surname> <given-names>D</given-names></name><name><surname>Clercq</surname> <given-names>Tde</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Statistical analysis of harmony and melody in rock music</article-title><source>Journal of New Music Research</source><volume>42</volume><fpage>187</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1080/09298215.2013.788039</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tillmann</surname> <given-names>B</given-names></name><name><surname>Bharucha</surname> <given-names>JJ</given-names></name><name><surname>Bigand</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Implicit learning of tonality: a self-organizing approach</article-title><source>Psychological Review</source><volume>107</volume><fpage>885</fpage><lpage>913</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.107.4.885</pub-id><pub-id pub-id-type="pmid">11089410</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorovic</surname> <given-names>A</given-names></name><name><surname>van Ede</surname> <given-names>F</given-names></name><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prior expectation mediates neural adaptation to repeated sounds in the auditory cortex: an MEG study</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>9118</fpage><lpage>9123</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1425-11.2011</pub-id><pub-id pub-id-type="pmid">21697363</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorovic</surname> <given-names>A</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Repetition suppression and expectation suppression are dissociable in time in early auditory evoked fields</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>13389</fpage><lpage>13395</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2227-12.2012</pub-id><pub-id pub-id-type="pmid">23015429</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toro</surname> <given-names>JM</given-names></name><name><surname>Sinnett</surname> <given-names>S</given-names></name><name><surname>Soto-Faraco</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Speech segmentation by statistical learning depends on attention</article-title><source>Cognition</source><volume>97</volume><fpage>B25</fpage><lpage>B34</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2005.01.006</pub-id></element-citation></ref><ref id="bib137"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanthornhout</surname> <given-names>J</given-names></name><name><surname>Decruy</surname> <given-names>L</given-names></name><name><surname>Wouters</surname> <given-names>J</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name><name><surname>Francart</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Speech Intelligibility Predicted from Neural Entrainment of the Speech Envelope</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>19</volume><fpage>181</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1007/s10162-018-0654-z</pub-id></element-citation></ref><ref id="bib138"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verschueren</surname> <given-names>E</given-names></name><name><surname>Somers</surname> <given-names>B</given-names></name><name><surname>Francart</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural envelope tracking as a measure of speech understanding in cochlear implant users</article-title><source>Hearing Research</source><volume>373</volume><fpage>23</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2018.12.004</pub-id><pub-id pub-id-type="pmid">30580236</pub-id></element-citation></ref><ref id="bib139"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vines</surname> <given-names>BW</given-names></name><name><surname>Schnider</surname> <given-names>NM</given-names></name><name><surname>Schlaug</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Testing for causality with transcranial direct current stimulation: pitch memory and the left supramarginal gyrus</article-title><source>NeuroReport</source><volume>17</volume><fpage>1047</fpage><lpage>1050</lpage><pub-id pub-id-type="doi">10.1097/01.wnr.0000223396.05070.a2</pub-id><pub-id pub-id-type="pmid">16791101</pub-id></element-citation></ref><ref id="bib140"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vuust</surname> <given-names>P</given-names></name><name><surname>Brattico</surname> <given-names>E</given-names></name><name><surname>Seppänen</surname> <given-names>M</given-names></name><name><surname>Näätänen</surname> <given-names>R</given-names></name><name><surname>Tervaniemi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The sound of music: differentiating musicians using a fast, musical multi-feature mismatch negativity paradigm</article-title><source>Neuropsychologia</source><volume>50</volume><fpage>1432</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.02.028</pub-id><pub-id pub-id-type="pmid">22414595</pub-id></element-citation></ref><ref id="bib141"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname> <given-names>DDE</given-names></name><name><surname>Fuglsang</surname> <given-names>SA</given-names></name><name><surname>Hjortkjær</surname> <given-names>J</given-names></name><name><surname>Ceolini</surname> <given-names>E</given-names></name><name><surname>Slaney</surname> <given-names>M</given-names></name><name><surname>de Cheveigné</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A comparison of regularization methods in forward and backward models for auditory attention decoding</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>531</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00531</pub-id><pub-id pub-id-type="pmid">30131670</pub-id></element-citation></ref><ref id="bib142"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolley</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Early experience shapes vocal neural coding and perception in songbirds</article-title><source>Developmental Psychobiology</source><volume>54</volume><fpage>612</fpage><lpage>631</lpage><pub-id pub-id-type="doi">10.1002/dev.21014</pub-id><pub-id pub-id-type="pmid">22711657</pub-id></element-citation></ref><ref id="bib143"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname> <given-names>RJ</given-names></name><name><surname>Salimpoor</surname> <given-names>VN</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>From perception to pleasure: music and its neural substrates</article-title><source>PNAS</source><volume>110</volume><fpage>10430</fpage><lpage>10437</lpage><pub-id pub-id-type="doi">10.1073/pnas.1301228110</pub-id><pub-id pub-id-type="pmid">23754373</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.51784.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Peelle</surname><given-names>Jonathan Erik</given-names></name><role>Reviewing Editor</role><aff><institution>Washington University in St. Louis</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Predicting upcoming events is central to sensory experience, and expectation plays a fundamental role in processing music. Here the authors investigated neural responses to predictability in the context of natural music listening by presenting monophonic music to one group of listeners recorded with scalp EEG, and a second group with implanted electrodes. A model of melodic prediction based on Markov chains was used to estimate musical predictability. Auditory regions showed increased responses to expectations. Demonstrating cortical responses to expectation during is a useful extension to prior work, particularly in the context of natural listening. The work is of interest for its elegant investigation of expectation in music, but also has broader implications for prediction and expectation in sensory processing.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Cortical encoding of melodic expectations in human temporal cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Barbara Shinn-Cunningham as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary</p><p>The authors investigated neural responses to predictability in the context of musical rhythm by presenting monophonic music to one group of listeners recorded with scalp EEG, and a second group with implanted electrodes (ECoG). A model of melodic prediction based on Markov chains was used to estimate musical predictability. Auditory regions (auditory cortex and planum temporale) showed increased responses to expectations. Demonstrating cortical responses to expectation during is a useful extension to prior work, particularly in the context of natural listening.</p><p>Essential revisions</p><p>1) The manuscript does not make adequate contact with existing literatures on music processing and the theoretical advances achieved by the current study. The Introduction explains the methods but does not situate the current results within a broader theoretical framework (for example, a few prior music studies are cited but not explained). Although the methods are generally strong, the theoretical framing must be strengthened substantially in both the Introduction and Discussion.</p><p>2) We do have substantial knowledge on which regions process musical structure – and those go beyond temporal areas (e.g., including IFG). The way the study is constructed and presented neglects possible contributions of these regions. ECoG analyses were limited to electrodes that showed auditory responses which biases results to the temporal lobe (as said at the end of the third paragraph of the Discussion section). Why this limitation? And/or why not fitting data to M only?</p><p>3) The paper is substantially lacking in the clarity of anatomical detail, particularly regarding iEEG data. Figure 3 and Figure 4 would benefit from anatomy panels depicting the location of recording sites in the subjects or some other way to visualize where the recordings were made from. It is important to have information about overall electrode coverage to estimate which parts of the perisylvian network were researchable.</p><p>4) The Discussion proposes &quot;fundamental differences between music and speech perception&quot; based on discrepancies between the present finding on musical melody (syntax) and previous findings on phonotactic and semantic processing in language. However, there are no theoretical grounds to compare these findings. At least linguists draw more or less clear borders between phonology, syntax and semantics and different neural networks are being discussed for these processes (plausible reason for different scalp topographies for example, Discussion paragraph five). A comparison of the present music data to data on syntax in language may seem more suitable (preferably as within-subject comparison).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.51784.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions</p><p>1) The manuscript does not make adequate contact with existing literatures on music processing and the theoretical advances achieved by the current study. The Introduction explains the methods but does not situate the current results within a broader theoretical framework (for example, a few prior music studies are cited but not explained). Although the methods are generally strong, the theoretical framing must be strengthened substantially in both the Introduction and Discussion.</p></disp-quote><p>We agree with the reviewer that the previous version of the manuscript did not sufficiently link our study to the rich literature in music research. A significant part of the Introduction and Discussion sections have been rewritten to more clearly relate our work with that previous literature. The main changes include:</p><p>– An introduction on the theoretical views and computational models attempting to explain how our brains learn musical structures (Introduction paragraph one);</p><p>– A more comprehensive introduction of previous neurophysiological work, especially with a focus on ERP results (Introduction paragraph three);</p><p>– A more in-depth explanation of the limitations of previous studies that our experiment overcomes (Introduction paragraph four). We also detail the limitations of our approach and discuss how future work could further advance our understanding of the neural underpinnings of music perception (Discussion paragraph three).</p><p>– A more comprehensive and careful discussion on the similarities and differences between our findings on melodic expectations encoding and previous work on language processing (as the reviewer pointed out in the fourth essential point) (Discussion paragraph five).</p><p>The link between our study and previous research has also been generally reinforced throughout the paper with additional references and considerations.</p><disp-quote content-type="editor-comment"><p>2) We do have substantial knowledge on which regions process musical structure – and those go beyond temporal areas (e.g., including IFG). The way the study is constructed and presented neglects possible contributions of these regions. ECoG analyses were limited to electrodes that showed auditory responses which biases results to the temporal lobe (as said at the end of the third paragraph of the Discussion section). Why this limitation? And/or why not fitting data to M only?</p></disp-quote><p>We thank the reviewers for raising this issue. This question made us realise that our explanation of the adopted methodology was unclear, thus leading us to apply major changes to the manuscript.</p><p>ECoG analyses were conducted on electrodes which showed stronger responses to note-onsets than to silence. While this approach certainly selected auditory responsive electrodes in temporal cortex, any other response that was time-locked to note-onset would have emerged as well, thus including melodic expectation responses that were time-locked to note onset. Note that this is the same assumption of our TRF analysis which, in fact, requires time-locking between the stimulus (note sequences) and the neural responses. In other words, our electrode selection approach identified electrodes capturing time-locked responses, which are also the same ones where the TRF analysis can be effective. As the reviewer suggested, another option could be to perform the channel selection directly on the TRF results. However, this comes at the risk of introducing a bias towards the particular set of features used for the model fit (A, M, AM, AM-A).</p><p>Regarding the possible contribution of areas beyond temporal cortex, our coverage of relevant areas such as IFG was not sufficient to claim neither a positive nor a null result on this (we found one responsive electrode in IFG). For this reason, the revised manuscript focuses on our findings in the temporal cortex but also reports the results and anatomical locations of other relevant electrodes (see Figures 3C, 4C, Figure 3—figure supplement 1 and Figure 4—figure supplement 1, Supplementary file 1). In addition, we also report on the full coverage for each ECoG patient with interactive 3D brain plots (Supplementary files 2-4).</p><p>We have clarified these points in the text, both in the Materials and methods, Results, and Discussion sections.</p><disp-quote content-type="editor-comment"><p>3) The paper is substantially lacking in the clarity of anatomical detail, particularly regarding iEEG data. Figure 3 and Figure 4 would benefit from anatomy panels depicting the location of recording sites in the subjects or some other way to visualize where the recordings were made from. It is important to have information about overall electrode coverage to estimate which parts of the perisylvian network were researchable.</p></disp-quote><p>Anatomy panels indicating the electrodes selected for the analysis have been added to Figures 3, 4, Figure 3—figure supplement 1 and Figure 4—figure supplement 1 (note that the exact location of those electrodes is specified in the supplementary files). Furthermore, we have added supplementary interactive 3D plots with the full coverage of the recordings that 1) give a more precise graphical indication on where the selected electrodes were positioned and 2) provide information also on the electrodes that were not sensitive to our analyses.</p><disp-quote content-type="editor-comment"><p>4) The Discussion proposes &quot;fundamental differences between music and speech perception&quot; based on discrepancies between the present finding on musical melody (syntax) and previous findings on phonotactic and semantic processing in language. However, there are no theoretical grounds to compare these findings. At least linguists draw more or less clear borders between phonology, syntax and semantics and different neural networks are being discussed for these processes (plausible reason for different scalp topographies for example, Discussion paragraph five). A comparison of the present music data to data on syntax in language may seem more suitable (preferably as within-subject comparison).</p></disp-quote><p>Syntax is certainly a critical part of language processing that has often been the focus of studies that looked for shared mechanisms between speech and music processing. In this context, it is also important to investigate various other levels of speech processing (e.g. prosody) (Heffner and Slevs, 2015). The revised version of the paper expands this discussion by pointing to similarities between our results and previous work on language perception at various processing levels. This part of the manuscript has been largely rewritten, especially by including a discussion on previous language research on syntax processing. We also clarified that similarities between our results and previous research should be taken only as speculations for now, and that further work should be conducted to explicitly compare the cortical underpinnings of music and language perception.</p></body></sub-article></article>