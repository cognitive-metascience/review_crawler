<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">47314</article-id><article-id pub-id-type="doi">10.7554/eLife.47314</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Brian 2, an intuitive and efficient neural simulator</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-82268"><name><surname>Stimberg</surname><given-names>Marcel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2648-4790</contrib-id><email>marcel.stimberg@inserm.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-23180"><name><surname>Brette</surname><given-names>Romain</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0110-1623</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-6781"><name><surname>Goodman</surname><given-names>Dan FM</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1007-6474</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Sorbonne Université, INSERM, CNRS, Institut de la Vision</institution><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Electrical and Electronic Engineering</institution><institution>Imperial College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Skinner</surname><given-names>Frances K</given-names></name><role>Reviewing Editor</role><aff><institution>Krembil Research Institute, University Health Network</institution><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>20</day><month>08</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e47314</elocation-id><history><date date-type="received" iso-8601-date="2019-04-01"><day>01</day><month>04</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-08-19"><day>19</day><month>08</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Stimberg et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Stimberg et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-47314-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.47314.001</object-id><p>Brian 2 allows scientists to simply and efficiently simulate spiking neural network models. These models can feature novel dynamical equations, their interactions with the environment, and experimental protocols. To preserve high performance when defining new models, most simulators offer two options: low-level programming or description languages. The first option requires expertise, is prone to errors, and is problematic for reproducibility. The second option cannot describe all aspects of a computational experiment, such as the potentially complex logic of a stimulation protocol. Brian addresses these issues using runtime code generation. Scientists write code with simple and concise high-level descriptions, and Brian transforms them into efficient low-level code that can run interleaved with their code. We illustrate this with several challenging examples: a plastic model of the pyloric network, a closed-loop sensorimotor model, a programmatic exploration of a neuron model, and an auditory model with real-time input.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.47314.002</object-id><title>eLife digest</title><p>Simulating the brain starts with understanding the activity of a single neuron. From there, it quickly gets very complicated. To reconstruct the brain with computers, neuroscientists have to first understand how one brain cell communicates with another using electrical and chemical signals, and then describe these events using code. At this point, neuroscientists can begin to build digital copies of complex neural networks to learn more about how those networks interpret and process information.</p><p>To do this, computational neuroscientists have developed simulators that take models for how the brain works to simulate neural networks. These simulators need to be able to express many different models, simulate these models accurately, and be relatively easy to use. Unfortunately, simulators that can express a wide range of models tend to require technical expertise from users, or perform poorly; while those capable of simulating models efficiently can only do so for a limited number of models. An approach to increase the range of models simulators can express is to use so-called ‘model description languages’. These languages describe each element within a model and the relationships between them, but only among a limited set of possibilities, which does not include the environment. This is a problem when attempting to simulate the brain, because a brain is precisely supposed to interact with the outside world.</p><p>Stimberg et al. set out to develop a simulator that allows neuroscientists to express several neural models in a simple way, while preserving high performance, without using model description languages. Instead of describing each element within a specific model, the simulator generates code derived from equations provided in the model. This code is then inserted into the computational experiments. This means that the simulator generates code specific to each model, allowing it to perform well across a range of models. The result, Brian 2, is a neural simulator designed to overcome the rigidity of other simulators while maintaining performance.</p><p>Stimberg et al. illustrate the performance of Brian 2 with a series of computational experiments, showing how Brian 2 can test unconventional models, and demonstrating how users can extend the code to use Brian 2 beyond its built-in capabilities.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>computational neuroscience</kwd><kwd>simulation</kwd><kwd>software</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>Axode ANR-14-CE13-0003</award-id><principal-award-recipient><name><surname>Brette</surname><given-names>Romain</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><award-id>RG170298</award-id><principal-award-recipient><name><surname>Goodman</surname><given-names>Dan FM</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Brian 2 is a software package for neural simulations that makes it both easy and computationally efficient to define original models for computational experiment.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neural simulators are increasingly used to develop models of the nervous system, at different scales and in a variety of contexts (<xref ref-type="bibr" rid="bib7">Brette et al., 2007</xref>). These simulators generally have to find a trade-off between performance and the flexibility to easily define new models and computational experiments. Brian 2 is a complete rewrite of the Brian simulator designed to solve this apparent dichotomy using the technique of code generation. The design is based around two fundamental ideas. Firstly, it is equation based: defining new neural models should be no more difficult than writing down their equations. Secondly, the computational experiment is fundamental: the interactions between neurons, environment and experimental protocols are as important as the neural model itself. We cover these points in more detail in the following paragraphs.</p><p>Popular tools for simulating spiking neurons and networks of such neurons are NEURON (<xref ref-type="bibr" rid="bib13">Carnevale and Hines, 2006</xref>), GENESIS (<xref ref-type="bibr" rid="bib6">Bower and Beeman, 1998</xref>), NEST (<xref ref-type="bibr" rid="bib28">Gewaltig and Diesmann, 2007</xref>), and Brian (<xref ref-type="bibr" rid="bib32">Goodman and Brette, 2008</xref>; <xref ref-type="bibr" rid="bib33">Goodman and Brette, 2009</xref>; <xref ref-type="bibr" rid="bib34">Goodman and Brette, 2013</xref>). Most of these simulators come with a library of standard models that the user can choose from. However, we argue that to be maximally useful for research, a simulator should also be designed to facilitate work that goes beyond what is known at the time that the tool is created, and therefore enable the user to investigate new mechanisms. Simulators take widely different approaches to this issue. For some simulators, adding new mechanisms requires specifying them in a low-level programming language such as C++, and integrating them with the simulator code (e.g. NEST). Amongst these, some provide domain-specific languages, for example NMODL (<xref ref-type="bibr" rid="bib42">Hines and Carnevale, 2000</xref>, for NEURON) or NESTML (<xref ref-type="bibr" rid="bib61">Plotnikov et al., 2016</xref>, for NEST), and tools to transform these descriptions into compiled modules that can then be used in simulation scripts. Finally, the Brian simulator has been built around mathematical model descriptions that are part of the simulation script itself.</p><p>Another approach to model definitions has been established by the development of simulator-independent markup languages, for example NeuroML/LEMS (<xref ref-type="bibr" rid="bib29">Gleeson et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Cannon et al., 2014</xref>) and NineML (<xref ref-type="bibr" rid="bib65">Raikov et al., 2011</xref>). However, markup languages address only part of the problem. A computational experiment is not fully specified by a neural model: it also includes a particular experimental protocol (set of rules defining the experiment), for example a sequence of visual stimuli. Capturing the full range of potential protocols cannot be done with a purely declarative markup language, but is straightforward in a general purpose programming language. For this reason, the Brian simulator combines the model descriptions with a procedural, computational experiment approach: a simulation is a user script written in Python, with models described in their mathematical form, without any reference to predefined models. This script may implement arbitrary protocols by loading data, defining models, running simulations and analysing results. Due to Python’s expressiveness, there is no limit on the structure of the computational experiment: stimuli can be changed in a loop, or presented conditionally based on the results of the simulation, etc. This flexibility can only be obtained with a general-purpose programming language and is necessary to specify the full range of computational experiments that scientists are interested in.</p><p>While the procedural, equation-oriented approach addresses the issue of flexibility for both the modelling and the computational experiment, it comes at the cost of reduced performance, especially for small-scale models that do not benefit much from vectorisation techniques (<xref ref-type="bibr" rid="bib9">Brette and Goodman, 2011</xref>). The reduced performance results from the use of an interpreted language to implement arbitrary models, instead of the use of pre-compiled code for a set of previously defined models. Thus, simulators generally have to find a trade-off between flexibility and performance, and previous approaches have often chosen one over the other. In practice, this makes computational experiments that are based on non-standard models either difficult to implement or slow to perform. We will describe four case studies in this article: exploring unconventional plasticity rules for a small neural circuit (case study 1, <xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig2">Figure 2</xref>); running a model of a sensorimotor loop (case study 2, <xref ref-type="fig" rid="fig3">Figure 3</xref>); determining the spiking threshold of a complex model by bisection (case study 3, <xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig5">Figure 5</xref>); and running an auditory model with real-time input from a microphone (case study 4, <xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.003</object-id><label>Figure 1.</label><caption><title>Case study 1: A model of the pyloric network of the crustacean stomatogastric ganglion, inspired by several modelling papers on this subject (<xref ref-type="bibr" rid="bib30">Golowasch et al., 1999</xref>; <xref ref-type="bibr" rid="bib63">Prinz et al., 2004</xref>; <xref ref-type="bibr" rid="bib64">Prinz, 2006</xref>; <xref ref-type="bibr" rid="bib57">O'Leary et al., 2014</xref>).</title><p>(<bold>a</bold>) Schematic of the modelled circuit (after <xref ref-type="bibr" rid="bib63">Prinz et al., 2004</xref>). The pacemaker kernel is modelled by a single neuron representing both anterior burster and pyloric dilator neurons (AB/PD, blue). There are two types of follower neurons, lateral pyloric (LP, orange), and pyloric (PY, green). Neurons are connected via slow cholinergic (thick lines) and fast glutamatergic (thin lines) synapses. (<bold>b</bold>) Activity of the simulated neurons. Membrane potential is plotted over time for the neurons in (<bold>a</bold>), using the same colour code. The bottom row shows their spiking activity in a raster plot, with spikes defined as excursions of the membrane potential over −20 mV. In the left column (‘initial’), activity is shown for 4 s after an initial settling time of 2.5 s. The right column (‘adapted’) shows the activity with fully adapted conductances (see text for details) after an additional time of 49 s. (<bold>c</bold>) Activity of the simulated neurons of a biologically detailed version of the circuit shown in (<bold>a</bold>), following (<xref ref-type="bibr" rid="bib30">Golowasch et al., 1999</xref>). All conventions as in (<bold>b</bold>), except for showing 3 s of activity after a settling time of 0.5 s (‘initial’), and after an additional time of 24 s (‘adapted’). Also note that the biologically detailed model consists of two coupled compartments, but only the membrane potential of the somatic compartment (<inline-formula><mml:math id="inf1"><mml:msub><mml:mi>V</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>) is shown here.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-fig1-v2.tif"/></fig><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.004</object-id><label>Figure 2.</label><caption><title>Case study 1: A model of the pyloric network of the crustacean stomatogastric ganglion.</title><p>Simulation code for the model shown in <xref ref-type="fig" rid="fig1">Figure 1a</xref>, producing the circuit activity shown in <xref ref-type="fig" rid="fig1">Figure 1b</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.47314.005</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Simulation code for the more biologically detailed model of the circuit shown in <xref ref-type="fig" rid="fig1">Figure 1a</xref>, producing the circuit activity shown in <xref ref-type="fig" rid="fig1">Figure 1c</xref>.</title><p>Simulation code for the more biologically detailed model of the circuit shown in <xref ref-type="fig" rid="fig1">Figure 1a</xref> (based on <xref ref-type="bibr" rid="bib30">Golowasch et al., 1999</xref>). The code for the synaptic model and connections is identical to the code shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, except for acting on <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>V</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf3"><mml:mi>v</mml:mi></mml:math></inline-formula> in the target cell.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-fig2-figsupp1-v2.tif"/></fig></fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.006</object-id><label>Figure 3.</label><caption><title>Case study 2: Smooth pursuit eye movements.</title><p>(<bold>a</bold>) Schematics of the model. An object (green) moves along a line and activates retinal neurons (bottom row; black) that are sensitive to the relative position of the object to the eye. Retinal neurons activate two motor neurons with weights depending on the eccentricity of their preferred position in space. Motor neurons activate the ocular muscles responsible for turning the eye. (<bold>b</bold>) Top: Simulated activity of the sensory neurons (black), and the left (blue) and right (orange) motor neurons. Bottom: Position of the eye (black) and the stimulus (green). (<bold>c</bold>) Simulation code.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-fig3-v2.tif"/></fig><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.007</object-id><label>Figure 4.</label><caption><title>Case study 3: Using bisection to find a neuron’s voltage threshold.</title><p>(<bold>a</bold>) Schematic of the bisection algorithm for finding a neuron’s voltage threshold. The algorithm is applied in parallel for different values of sodium density. (<bold>b</bold>) Top: Refinement of the voltage threshold estimate over iterations for three sodium densities (blue: 23.5 mS cm<sup><bold>−</bold>2</sup>, orange: 57.5 mS cm<sup><bold>−</bold>2</sup>, green: 91.5 mS cm<sup><bold>−</bold>2</sup>); Bottom: Voltage threshold estimation as a function of sodium density.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-fig4-v2.tif"/></fig><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.008</object-id><label>Figure 5.</label><caption><title>Case study 3: Simulation code to find a neuron’s voltage threshold, implementing the bisection algorithm detailed in <xref ref-type="fig" rid="fig4">Figure 4a</xref>.</title><p>The code simulates 100 unconnected neurons with sodium densities between 15 mS cm<sup>−2</sup> and 100 mS cm<sup>−2</sup>, following the model of <xref ref-type="bibr" rid="bib43">Hodgkin and Huxley (1952)</xref>. Results from these simulations are shown in <xref ref-type="fig" rid="fig4">Figure 4b</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-fig5-v2.tif"/></fig><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.009</object-id><label>Figure 6.</label><caption><title>Case study 4: Neural pitch processing with real-time input.</title><p>(<bold>a</bold>) Model schematic: Audio input is converted into spikes and fed into a population of coincidence-detection neurons via two pathways, one instantaneous, that is without any delay (top), and one with incremental delays (bottom). Each neuron therefore receives the spikes resulting from the audio signal twice, with different temporal shifts between the two. The inverse of this shift determines the preferred frequency of the neuron. (<bold>b</bold>) Simulation results for a sample run of the simulation code in <xref ref-type="fig" rid="fig7">Figure 7</xref>. Top: Raw sound input (a rising sequence of tones – C, E, G, C – played on a synthesised flute). Middle: Spectrogram of the sound input. Bottom: Raster plot of the spiking response of receiving neurons (group <monospace>neurons</monospace> in the code), ordered by their preferred frequency.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-fig6-v2.tif"/></fig><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.010</object-id><label>Figure 7.</label><caption><title>Case study 4: Simulation code for the model shown in <xref ref-type="fig" rid="fig6">Figure 6a</xref>.</title><p>The sound input is acquired in real time from a microphone, using user-provided low-level code written in C that makes use of an Open Source library for audio input (<xref ref-type="bibr" rid="bib4">Bencina and Burk, 1999</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-fig7-v2.tif"/></fig><p>Brian 2 solves the performance-flexibility trade-off using the technique of code generation (<xref ref-type="bibr" rid="bib31">Goodman, 2010</xref>; <xref ref-type="bibr" rid="bib71">Stimberg et al., 2014</xref>; <xref ref-type="bibr" rid="bib5">Blundell et al., 2018</xref>). The term code generation here refers to the process of automatically transforming a high-level user-defined model into executable code in a computationally efficient low-level language, compiling it in the background and running it without requiring any actions from the user. This generated code is inserted within the flow of the simulation script, which makes it compatible with the procedural approach. Code generation is not only used to run the models but also to build them, and therefore also accelerates stages such as synapse creation. The code generation framework has been designed to be extensible on several levels. On a general level, code generation targets can be added to generate code for other architectures, for example graphical processing units, from the same simulation description. On a more specific level, new functionality can be added by providing a small amount of code written in the target language, for example to connect the simulation to an input device. Implementing this solution in a way that is transparent to the user requires solving important design and computational problems, which we will describe in the following.</p></sec><sec id="s2" sec-type="materials|methods"><title>Materials and methods</title><sec id="s2-1"><title>Design and implementation</title><p>We will explain the key design decisions by starting from the requirements that motivated them. Note that from now on we will use the term ‘Brian’ as referring to its latest version, that is Brian 2, and only use ‘Brian 1’ and ‘Brian 2’ when discussing differences between them.</p><p>Before discussing the requirements, we start by motivating the choice of programming language. Python is a high-level language, that is, it is abstracted from machine level details and highly readable (indeed, it is often described as ‘executable pseudocode’). In this sense, it is higher level than C++, for example, which in this article we will refer to as a low-level language (since we will not need to refer to even lower level languages such as assembly language). The use of a high-level language is important for scientific software because the majority of scientists are not trained programmers, and high-level languages are generally easier to learn and use, and lead to shorter code that is easier to debug. This last point, and the fact that Python is a very popular general purpose programming language with excellent built-in and third party tools, is also important for reducing development time, enabling the developers to be more efficient. It is now widely recognised that Python is well suited to scientific software, and it is commonly used in computational neuroscience (<xref ref-type="bibr" rid="bib17">Davison et al., 2009</xref>; <xref ref-type="bibr" rid="bib55">Muller et al., 2015</xref>). Note that expert level Python knowledge is not necessary for using Brian or the Python interfaces for other simulators.</p><p>We now move on to the major design requirements.</p><list list-type="order"><list-item><p>Users should be able to easily define non-standard models, which may include models of neurons and synapses but also of other aspects such as muscles and environment. This is made possible by an equation-oriented approach, that is models are described by mathematical equations. We first focus on the design at the <italic>mathematical level</italic>, and we illustrate with two unconventional models: a model of intrinsic plasticity in the pyloric network of the crustacean stomatogastric ganglion (case study 1, <xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig2">Figure 2</xref>), and a closed-loop sensorimotor model of ocular movements (case study 2, <xref ref-type="fig" rid="fig3">Figure 3</xref>).</p></list-item><list-item><p>Users should be able to easily implement a complete computational experiment in Brian. Models must interact with a general control flow, which may include stimulus generation and various operations. This is made possible by taking a procedural approach to defining a complete computational experiment, rather than a declarative model definition, allowing users to make full use of the generality of the Python language. In the section on the <italic>computational experiment level</italic>, we demonstrate the interaction between a general control flow expressed in Python and the simulation run in a case study that uses a bisection algorithm to determine a neuron's firing threshold as a function of sodium channel density (case study 3, <xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig5">Figure 5</xref>).</p></list-item><list-item><p>Computational efficiency. Often, computational neuroscience research is limited more by the scientist’s time spent designing and implementing models, and analysing results, rather than the simulation time. However, there are occasions where high computational efficiency is necessary. To achieve high performance while preserving maximum flexibility, Brian generates code from user-defined equations and integrates it into the simulation flow.</p></list-item><list-item><p>Extensibility: no simulator can implement everything that any user might conceivably want, but users shouldn’t have to discard the simulator entirely if they want to go beyond its built-in capabilities. We therefore provide the possibility for users to extend the code either at a high or low level. We illustrate these last two requirements at the <italic>implementation level</italic> with a case study of a model of pitch perception using real-time audio input (case study 4, <xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig7">Figure 7</xref>).</p></list-item></list><p>In this section, we give a high level overview of the major decisions. A detailed analysis of the case studies and the features of Brian they use can be found in Appendix 1. Source code for the case studies has been deposited in a repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/brian-team/brian2_paper_examples">https://github.com/brian-team/brian2_paper_examples</ext-link> (<xref ref-type="bibr" rid="bib73">Stimberg et al., 2019a</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/brian2_paper_examples">https://github.com/elifesciences-publications/brian2_paper_examples</ext-link>).</p><sec id="s2-1-1"><title>Mathematical level</title><sec id="s2-1-1-1"><title>Case study 1: Pyloric network</title><p>We start with a case study of a model of the pyloric network of the crustacean stomatogastric ganglion (<xref ref-type="fig" rid="fig1">Figure 1a</xref>), adapted and simplified from earlier studies (<xref ref-type="bibr" rid="bib30">Golowasch et al., 1999</xref>; <xref ref-type="bibr" rid="bib63">Prinz et al., 2004</xref>; <xref ref-type="bibr" rid="bib64">Prinz, 2006</xref>; <xref ref-type="bibr" rid="bib57">O'Leary et al., 2014</xref>). This network has a small number of well-characterised neuron types – anterior burster (AB), pyloric dilator (PD), lateral pyloric (LP), and pyloric (PY) neurons – and is known to generate a stereotypical triphasic motor pattern (<xref ref-type="fig" rid="fig1">Figure 1b–c</xref>). Following previous studies, we lump AB and PD neurons into a single neuron type (AB/PD) and consider a circuit with one neuron of each type. The neurons in this circuit have rebound and bursting properties. We model this using a variant of the model proposed by <xref ref-type="bibr" rid="bib41">Hindmarsh and Rose (1984)</xref>, a three-variable model exhibiting such properties. We make this choice only for simplicity: the biophysical equations originally used in <xref ref-type="bibr" rid="bib30">Golowasch et al. (1999)</xref> can be used instead (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>Although this model is based on widely used neuron models, it has the unusual feature that some of the conductances are regulated by activity as monitored by a calcium trace. One of the first design requirements of Brian, then, is that non-standard aspects of models such as this should be as easy to implement in code as they are to describe in terms of their mathematical equations. We briefly summarise how it applies to this model (see Appendix 1 and <xref ref-type="bibr" rid="bib71">Stimberg et al., 2014</xref> for more detail). The three-variable underlying neuron model is implemented by writing its differential equations directly in standard mathematical form (<xref ref-type="fig" rid="fig2">Figure 2</xref>, l. 8–10). The calcium trace increases at each spike (l. 21; defined by a discrete event triggered after a spike, <monospace>reset=’Ca + = 0.1’</monospace>) and then decays (l. 13; again defined by a differential equation). A slow variable <inline-formula><mml:math id="inf4"><mml:mi>z</mml:mi></mml:math></inline-formula> tracks the difference of this calcium trace to a neuron-type-specific target value (l. 14) which then regulates the conductances <inline-formula><mml:math id="inf5"><mml:mi>s</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:mi>g</mml:mi></mml:math></inline-formula> (l. 11–12).</p><p>Not only the neuron model but also their connections are non-standard. Neurons are connected together by nonlinear graded synapses of two different types, slow and fast (l. 29–54). These are unconventional synapses in that the synaptic current has a graded dependence on the pre-synaptic action potential and a continuous effect rather than only being triggered by pre-synaptic action potentials (<xref ref-type="bibr" rid="bib1">Abbott and Marder, 1998</xref>). A key design requirement of Brian was to allow for the same expressivity for synaptic models as for neuron models, which led us to a number of features that allow for a particularly flexible specification of synapses in Brian. Firstly, we allow synapses to have dynamics defined by differential equations in precisely the same way as neurons. In addition to the usual role of triggering instantaneous changes in response to discrete neuronal events such as spikes, synapses can directly and continuously modify neuronal variables allowing for a very wide range of synapse types. To illustrate this, for the slow synapse, we have a synaptic variable (<monospace>m_slow</monospace>) that evolves according to a differential equation (l. 47) that depends on the pre-synaptic membrane potential (<monospace>v_pre</monospace>). The effect of this synapse is defined by setting the value of a post-synaptic neuron current (<monospace>I_slow</monospace>) in the definition of the synapse model (l. 46; referred to there as <monospace>I_slow_post</monospace>). The keyword (<monospace>summed</monospace>) in the equation specifies that the post-synaptic neuron variable is set using the summed value of the expression across all the synapses connected to it. Note that this mechanism also allows Brian to be used to specify abstract rate-based neuron models in addition to biophysical graded synapse models.</p><p>The model is defined not only by its dynamics, but also the values of parameters and the connectivity pattern of synapses. The next design requirement of Brian was that these essential elements of specifying a model should be equally flexible and readable as the dynamics. In this case, we have added a label variable to the model that can take values ABPD, LP or PY (l. 18, 20, 23) and used this label to set up the initial values (l. 36–40, 51–54) and connectivity patterns (l. 35, 50). Human readability of scripts is a key aspect of Brian code, and important for reproducibility (which we will come back to in the Discussion). We highlight line 35 to illustrate this. We wish to have synapses between all neurons of different types but not of the same type, except that we do not wish to have synapses from PY neurons to AB/PD neurons. Having set up the labels, we can now express this connectivity pattern with the expression <monospace>’label_pre!=label_post and not (label_pre == PY and label_post == ABPD)’</monospace>. This example illustrates one of the many possibilities offered by the equation-oriented approach to concisely express connectivity patterns (for more details see Appendix 1 and <xref ref-type="bibr" rid="bib71">Stimberg et al., 2014</xref>).</p></sec></sec><sec id="s2-1-2"><title>Comparison to other approaches</title><p>In this case study, we have shown how a non-standard neural network, with graded synapses and adapting conductances, can be described in the Brian simulator. How could such a model be implemented with one of the other approaches described previously? We will briefly discuss this by focussing on implementations of the graded synapse model. One approach is to directly write an implementation of the model in a programming language like C++, without the use of any simulation software. While this requires significant technical skill, it allows for complete freedom in the definition of the model itself. This was the approach taken for a study that ran 20 million parametrised instances of the same pyloric network model (<xref ref-type="bibr" rid="bib36">Günay and Prinz, 2010</xref>). The increased effort of writing the simulation was offset by reusing the same model an extremely large number of times. An excerpt of this code is shown in <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1c</xref>. Note that unless great care is taken, this approach may lead to a very specific implementation of the model that is not straightforward to adapt for other purposes. With such long source code (3510 lines in this case) it is also difficult to check that there are no errors in the code, or implicit assumptions that deviate from the description (as in, for example <xref ref-type="bibr" rid="bib39">Hathway and Goodman, 2018</xref>; <xref ref-type="bibr" rid="bib59">Pauli et al., 2018</xref>).</p><p>Another approach for describing model components such as graded synapses is to use a description language such as LEMS/NeuroML2. If the specific model has already been added as a ‘core type’, then it can be readily referenced in the description of the model (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2a</xref>). If not, then the LEMS description can be used to describe it (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2a</xref>). Such a description is on a similar level of abstraction as the Brian description, but somewhat more verbose (although this may be reduced by using a library such as PyLEMS to create the description; <xref ref-type="bibr" rid="bib79">Vella et al., 2014</xref>).</p><p>If the user chooses to use the NEURON simulator to simulate the model, then a new synaptic mechanism can be added using the NMODL language (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2b</xref>). However, for the user this requires learning a new, idiosyncratic language, and detailed knowledge about simulator internals, for example the numerical solution of equations. Other simulators, such as NEST, are focussed on discrete spike-based interactions and currently do not come with models of graded synapses, and such models are not yet supported by its description language NESTML. Leveraging the infrastructure added for gap-junctions (<xref ref-type="bibr" rid="bib37">Hahne et al., 2015</xref>) and rate models (<xref ref-type="bibr" rid="bib38">Hahne et al., 2017</xref>), the NEST simulator could certainly integrate such models in principle but in practice this may not be feasible without direct support from the NEST team.</p><sec id="s2-1-2-1"><title>Case study 2: Ocular model</title><p>The second example is a closed-loop sensorimotor model of ocular movements (used for illustration and not intended to be a realistic description of the system), where the eye tracks an object (<xref ref-type="fig" rid="fig3">Figure 3a,b</xref>). Thus, in addition to neurons, the model also describes the activity of ocular muscles and the dynamics of the stimulus. Each of the two antagonistic muscles is modelled mechanically as an elastic spring with some friction, which moves the eye laterally.</p><p>The next design requirement of Brian was that it should be both possible and straightforward to define non-neuronal elements of a model, as these are just as essential to the model as a whole, and the importance of connecting with these elements is often neglected in neural simulators. We will come back to this requirement in various forms over the next few case studies, but here we emphasise how the mechanisms for specifying arbitrary differential equations can be re-used for non-neuronal elements of a simulation.</p><p>The position of the eye follows a second-order differential equation, with resting position <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, the difference in resting positions of the two muscles (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, l. 4–5). The stimulus is an object that moves in front of the eye according to a stochastic process (l. 7–8). Muscles are controlled by two motoneurons (l. 11–13), for which each spike triggers a muscular ‘twitch’. This corresponds to a transient change in the resting position <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> of the eye in either direction, which then decays back to zero (l. 6, 15).</p><p>Retinal neurons receive a visual input, modelled as a Gaussian function of the difference between the neuron’s preferred position and the actual position of the object, measured in retinal coordinates (l. 21). Thus, the input to the neurons depends on dynamical variables external to the neuron model. This is a further illustration of the design requirement above that we need to include non-neuronal elements in our model specifications. In this case, to achieve this we link the variables in the eye model with the variables in the retina model using the <monospace>linked_var</monospace> function (l. 4, 7, 23–24, 28–29).</p><p>Finally, we implement a simple feedback mechanism by having retinal neurons project onto the motoneuron controlling the contralateral muscle (l. 33), with a strength proportional to their eccentricity (l. 36): thus, if the object appears on the edge of the retina, the eye is strongly pulled towards the object; if the object appears in the centre, muscles are not activated. This simple mechanism allows the eye to follow the object (<xref ref-type="fig" rid="fig3">Figure 3b</xref>), and the code illustrates the previous design requirement that the code should reflect the mathematical description of the model.</p></sec></sec><sec id="s2-1-3"><title>Comparison to other approaches</title><p>The remarks we made earlier regarding the graded synapse in case study one mostly apply here as well. For LEMS/NeuroML2, both motor neurons and the environment could be modelled with a LEMS description. Similarly, a simulation with NEURON would require NMODL specifications of both models, using its <monospace>POINTER</monospace> mechanism (see <xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2</xref>) to link them together. Since NEST’s modelling language NESTML does not allow for the necessary continuous interaction between a single environment and multiple neurons, implementing this model would be a major effort and require writing code in C++ and detailed knowledge of NEST’s internal architecture.</p></sec><sec id="s2-1-4"><title>Computational experiment level</title><p>The mathematical model descriptions discussed in the previous section provide only a partial description of what we might call a ‘computational experiment’. Let us consider the analogy to an electrophysiological experiment: for a full description, we would not only state the model animal, the cell type and the preparation that was investigated, but also the stimulation and analysis protocol. In the same way, a full description of a computational experiment requires not only a description of the neuron and synapse models, but also information such as how input stimuli are generated, or what sequence of simulations is run. Some examples of computational experimental protocols would include: threshold finding (discussed in detail below) where the stimulus on the next trial depends on the outcome of the current trial; generalisations of this to potentially very complex closed-loop experiments designed to determine the optimal stimuli for a neuron (e.g. <xref ref-type="bibr" rid="bib23">Edin et al., 2004</xref>); models including a complex simulated environment defined in an external package (e.g. <xref ref-type="bibr" rid="bib81">Voegtlin, 2011</xref>); or models with plasticity based on an error signal that depends on the global behaviour of the network (e.g. <xref ref-type="bibr" rid="bib76">Stroud et al., 2018</xref>; <xref ref-type="bibr" rid="bib85">Zenke and Ganguli, 2018</xref>). Capturing all these potential protocols in a purely descriptive format (one that is not Turing complete) is impossible by definition, but it can be easily expressed in a programming language with control structures such as loops and conditionals. The Brian simulator allows the user to write complete computational experimental protocols that include both the model description and the simulation protocol in a single, readable Python script.</p><sec id="s2-1-4-1"><title>Case study 3: Threshold finding</title><p>In this case study, we want to determine the voltage firing threshold of a neuron (<xref ref-type="fig" rid="fig4">Figure 4</xref>), modelled with three conductances, a passive leak conductance and voltage-dependent sodium and potassium conductances (<xref ref-type="fig" rid="fig5">Figure 5</xref>, l. 4–24).</p><p>To get an accurate estimate of the threshold, we use a bisection algorithm (<xref ref-type="fig" rid="fig4">Figure 4a</xref>): starting from an initial estimate and with an initial step width (<xref ref-type="fig" rid="fig5">Figure 5</xref>, l. 30–31), we set the neuron’s membrane potential to the estimate (l. 35) and simulate its dynamics for 20 ms (l. 36). If the neuron spikes, that is if the estimate was above the neuron’s threshold, we decrease our estimate (l. 38); if the neuron does not spike, we increase it (l. 37). We then halve the step width (l. 39) and perform the same process again until we have performed a certain number of iterations (l. 33) and converged to a precise estimate (<xref ref-type="fig" rid="fig4">Figure 4b</xref> top). Note that the order of operations is important here. When we modify the variable <inline-formula><mml:math id="inf9"><mml:mi>v</mml:mi></mml:math></inline-formula> in lines 37–38, we use the output of the simulation run on line 36, and this determines the parameters for the next iteration. A purely declarative definition could not represent this essential feature of the computational experiment.</p><p>For each iteration of this loop, we restore the network state (<monospace>restore()</monospace>; l. 34) to what it was at the beginning of the simulation (<monospace>store();</monospace> l. 27). This <monospace>store()/restore()</monospace> mechanism is a key part of Brian’s design for allowing computational experiments to be easily and flexibly expressed in Python, as it gives a very effective way of representing common computational experimental protocols. Examples that can easily be implemented with this mechanism include a training/testing/validation cycle in a synaptic plasticity setting; repeating simulations with some aspect of the model changed but the rest held constant (e.g. parameter sweeps, responses to different stimuli); or simply repeatedly running an identical stochastic simulation to evaluate its statistical properties.</p><p>At the end of the script, by performing this estimation loop in parallel for many neurons, each having a different maximal sodium conductance, we arrive at an estimate of the dependence of the voltage threshold on the sodium conductance (<xref ref-type="fig" rid="fig4">Figure 4b</xref> bottom).</p></sec></sec><sec id="s2-1-5"><title>Comparison to other approaches</title><p>Such a simulation protocol could be implemented in other simulators as well, since they use a general programming language to control the simulation flow (e.g. SLI or Python for NEST; HOC or Python for NEURON) in similar ways to Brian. However, general simulation workflows are not part of description languages like NeuroML2/LEMS. While a LEMS model description can include a <monospace>&lt;Simulation&gt;</monospace> element, this is only meant to specify the duration and step size of one or several simulation runs, together with information about what variables should be recorded and/or displayed. General workflows, for example deciding whether to run another simulation based on the results of a previous simulation, are beyond its scope. These could be implemented in a separate script in a different programming language.</p></sec><sec id="s2-1-6"><title>Implementation level</title><sec id="s2-1-6-1"><title>Case study 4: Real-time audio</title><p>The case studies so far were described by equations and algorithms on a level that is independent of the programming language and hardware that will eventually perform the computation. However, in some cases this lower level cannot be ignored. To demonstrate this, we will consider the example presented in <xref ref-type="fig" rid="fig6">Figure 6</xref>. We want to record an audio signal with a microphone and feed this signal—in real-time—into a neural network performing a crude ‘pitch detection’ based on the autocorrelation of the signal (<xref ref-type="bibr" rid="bib48">Licklider, 1962</xref>). This model first transforms the continuous stimulus into a sequence of spikes by feeding the stimulus into an integrate-and-fire model with an adaptive threshold (<xref ref-type="fig" rid="fig7">Figure 7</xref>, l. 36–41). It then detects periodicity in this spike train by feeding it into an array of coincidence detector neurons (<xref ref-type="fig" rid="fig6">Figure 6a</xref>; <xref ref-type="fig" rid="fig7">Figure 7</xref>, l. 44–47). Each of these neurons receives the input spike train via two pathways with different delays (l. 49–51). This arrangement allows the network to detect periodicity in the input stimulus; a periodic stimulus will most strongly excite the neuron where the difference in delays matches the stimulus’ period. Depending on the periodicity present in the stimulus, for example for tones of different pitch (<xref ref-type="fig" rid="fig6">Figure 6b</xref> middle), different sub-populations of neurons respond (<xref ref-type="fig" rid="fig6">Figure 6b</xref> bottom).</p><p>To perform such a study, our simulator has to meet two new requirements: firstly, the simulation has to run fast enough to be able to process the audio input in real-time. Secondly, we need a way to connect the running simulation to an audio signal via low-level code.</p><p>The challenge is to make the computational efficiency requirement compatible with the requirement of flexibility. With version 1 of Brian, we made the choice to sacrifice computational efficiency, because we reasoned that frequently in computational modelling, considerably more time was spent developing the model and writing the code than was spent on running it (often weeks versus minutes or hours; cf. <xref ref-type="bibr" rid="bib18">De Schutter, 1992</xref>). However, there are obviously cases where simulation time is a bottleneck. To increase computational efficiency without sacrificing flexibility, We decided to make code generation the fundamental mode of operation for Brian 2 (<xref ref-type="bibr" rid="bib71">Stimberg et al., 2014</xref>). Code generation was used previously in Brian 1 (<xref ref-type="bibr" rid="bib31">Goodman, 2010</xref>), but only in parts of the simulation. This technique is now being increasingly widely used in other simulators, see <xref ref-type="bibr" rid="bib5">Blundell et al. (2018)</xref> for a review.</p><p>In brief, from the high level abstract description of the model, we generate independent blocks of code (in C++ or other languages). We run these blocks in sequence to carry out the simulation. Typically, we first carry out numerical integration in one code block, check for threshold crossings in a second block, propagate synaptic activity in a third block, and finally run post-spike reset code in a fourth block. To generate this code, we make use of a combination of various techniques from symbolic mathematics and compilers that are available in third party Python libraries, as well as some domain-specific optimisations to further improve performance (see Appendix 1 for more details, or <xref ref-type="bibr" rid="bib71">Stimberg et al., 2014</xref>; <xref ref-type="bibr" rid="bib5">Blundell et al., 2018</xref>). We can then run the complete simulation in one of two modes, as follows.</p><p>In <italic>runtime</italic> mode, the overall simulation is controlled by Python code, which calls out to the compiled code objects to do the heavy lifting. This method of running the simulation is the default, because despite some computational overhead associated with repeatedly switching from Python to another language, it allows for a great deal of flexibility in how the simulation is run: whenever Brian’s model description formalism is not expressive enough for a task at hand, the researcher can interleave the execution of generated code with a hand-written function that can potentially access and modify any aspect of the model. This facility is widely used in computational models using Brian.</p><p>In <italic>standalone</italic> mode, additional low-level code is generated that controls the overall simulation, meaning that during the main run of the simulation it is not necessary to switch back to Python. This gives an improvement to performance, but at the cost of reduced flexibility since we cannot translate arbitrary Python code into low level code. The standalone mode can also be used to generate code to run on a platform where Python is not available or not practical (such as a GPU; <xref ref-type="bibr" rid="bib72">Stimberg et al., 2018</xref>).</p><p>The choice of which mode to use is left to the user, and will depend on details of the simulation and how much additional flexibility is required. The performance that can be gained from using the standalone mode also depends strongly on the details of the model; we will come back to this point in the discussion.</p><p>The second issue we needed to address for this case study was how to connect the running simulation to an audio signal via low-level code. The general issue here is how to extend the functionality of Brian. While Brian’s syntax allows a researcher to define a wide range of models within its general framework, inevitably it will not be sufficient for all computational research projects. Taking this into account, Brian has been built with extensibility in mind. Importantly, it should be possible to extend Brian’s functionality and still include the full description of the model in the main Python script, that is without requiring the user to edit the source code of the simulator itself or to add and compile separate modules.</p><p>As discussed previously, the runtime mode offers researchers the possibility to combine their simulation code with arbitrary Python code. However, in some cases, such as a model that requires real-time access to hardware (<xref ref-type="fig" rid="fig6">Figure 6</xref>), it may be necessary to add functionality at the target-language level itself. To this end, simulations can use a general extension mechanism: model code can refer not only to predefined mathematical functions, but also to functions defined in the target language by the user (<xref ref-type="fig" rid="fig7">Figure 7</xref>, l. 9–34). This can refer to code external to Brian, for example to third-party libraries (as is necessary in this case to get access to the microphone). In order to establish the link, Brian allows the user to specify additional libraries, header files or macro definitions (l. 29–31) that will be taken into account during the compilation of the code. With this mechanism the Brian simulator offers researchers the possibility to add functionality to their model at the lowest possible level, without abandoning the use of a convenient simulator and forcing them to write their model ‘from scratch’ in a low-level language. We think it is important to acknowledge that a simulator will never have every possible feature to cover all possible models, and we therefore provide researchers with the means to adapt the simulator’s behaviour to their needs at every level of the simulation.</p></sec></sec><sec id="s2-1-7"><title>Comparison to other approaches</title><p>The NEURON simulator can include user-written C code in <monospace>VERBATIM</monospace> blocks of an NMODL description, but there is no documented mechanism to link to external libraries. Another approach to interface a simulation with external input or output is to do this on the script level. For example, a recent study (<xref ref-type="bibr" rid="bib22">Dura-Bernal et al., 2017</xref>) linked a NEURON simulation of the motor cortex to a virtual musculoskeletal arm, by running a single simulation step at a time, and then exchanging values between the two systems. The NEST simulator provides a general mechanism to couple a simulation to another system (e.g. another simulator) via the MUSIC interface (<xref ref-type="bibr" rid="bib20">Djurfeldt et al., 2010</xref>). This framework has been successfully used to connect the NEST simulators to robotic simulators (<xref ref-type="bibr" rid="bib83">Weidel et al., 2016</xref>). The MUSIC framework does support both spike-based and continuous interactions, but NEST cannot currently apply continuous-valued inputs as used here. Finally, model description languages such as NeuroML2/LEMS are not designed to capture this kind of interaction.</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Brian 2 was designed to overcome some of the major challenges we saw for neural simulators (including Brian 1). Notably: the flexibility/performance dichotomy, and the need to integrate complex computational experiments that go beyond their neuronal and network components. As a result of this work, Brian can address a wide range of modelling problems faced by neuroscientists, as well as giving more robust and reproducible results and therefore contributing to increasing reproducibility in computational science. We now discuss these challenges in more detail.</p><p>Brian’s code generation framework allows for a solution to the dichotomy between flexibility and performance. Brian 2 improves on Brian 1 both in terms of flexibility (particularly the new, very general synapse model; for more details see Appendix 5) and performance, where it performs similarly to simulators written in low-level languages which do not have the same flexibility (<xref ref-type="bibr" rid="bib77">Tikidji-Hamburyan et al., 2017</xref>; also see section <italic>Performance</italic> below). Flexibility is essential to be useful for fundamental research in neuroscience, where basic concepts and models are still being actively investigated and have not settled to the point where they can be standardised. Performance is increasingly important, for example as researchers begin to model larger scale experimental data such as that provided by the Neuropixels probe (<xref ref-type="bibr" rid="bib46">Jun et al., 2017</xref>), or when doing comprehensive parameter sweeps to establish robustness of models (<xref ref-type="bibr" rid="bib58">O'Leary et al., 2015</xref>).</p><p>It is possible to write plugins for Brian to generate code for other platforms without modifying the core code, and there are several ongoing projects to do so. These include Brian2GeNN (<xref ref-type="bibr" rid="bib72">Stimberg et al., 2018</xref>) which uses the GPU-enhanced Neural Network simulator (GeNN; <xref ref-type="bibr" rid="bib84">Yavuz et al., 2016</xref>) to accelerate simulations in some cases by tens to hundreds of times, and Brian2CUDA (<ext-link ext-link-type="uri" xlink:href="https://github.com/brian-team/brian2cuda">https://github.com/brian-team/brian2cuda</ext-link>). The modular structure of the code generation framework was designed for this in order to be ready for future trends in both high-performance computing and computational neuroscience research. Increasingly, high-performance scientific computing relies on the use of heterogeneous computing architectures such as GPUs, FPGAs, and even more specialised hardware (<xref ref-type="bibr" rid="bib25">Fidjeland et al., 2009</xref>; <xref ref-type="bibr" rid="bib66">Richert et al., 2011</xref>; <xref ref-type="bibr" rid="bib10">Brette and Goodman, 2012</xref>; <xref ref-type="bibr" rid="bib54">Moore et al., 2012</xref>; <xref ref-type="bibr" rid="bib26">Furber et al., 2014</xref>; <xref ref-type="bibr" rid="bib14">Cheung et al., 2015</xref>), as well as techniques such as approximate computing (<xref ref-type="bibr" rid="bib53">Mittal, 2016</xref>). In addition to basic research, spiking neural networks may increasingly be used in applications thanks to their low power consumption (<xref ref-type="bibr" rid="bib51">Merolla et al., 2014</xref>), and the standalone mode of Brian is designed to facilitate the process of converting research code into production code.</p><p>A neural computational model is more than just its components (neurons, synapses, etc.) and network structure. In designing Brian, we put a strong emphasis on the complete computational experiment, including specification of the stimulus, interaction with non-neuronal components, etc. This is important both to minimise the time and expertise required to develop computational models, but also to reduce the chance of errors (see below). Part of our approach here was to ensure that features in Brian are as general and flexible as possible. For example, the equations system intended for defining neuron models can easily be repurposed for defining non-neuronal elements of a computational experiment (case study 2, <xref ref-type="fig" rid="fig3">Figure 3</xref>). However, ultimately we recognise that any way of specifying all elements of a computational experiment would be at least as complex as a fully featured programming language. We therefore simply allow users to define these aspects in Python, the same language used for defining the neural components, as this is already highly capable and readable. We made great efforts to ensure that the detailed work in designing and implementing new features should not interfere with the goal that the user script should be a readable description of the complete computational experiment, as we consider this to be an essential element of what makes a computational model valuable.</p><p>Brian’s approach to defining models leads to particularly concise code (<xref ref-type="bibr" rid="bib77">Tikidji-Hamburyan et al., 2017</xref>), as well as code whose syntax matches closely descriptions of models in papers. This is important not only because it saves scientists time if they have to write less code, but also because such code is easier to verify and reproduce. It is difficult for anyone, the authors of a model included, to verify that thousands of lines of model simulation code match the description they have given of it. An additional advantage of the clean syntax is that Brian is an excellent tool for teaching, for example in the computational neuroscience textbook of <xref ref-type="bibr" rid="bib27">Gerstner et al. (2013)</xref>. Expanding on this point, a major issue in computational science generally, and computational neuroscience in particular, is the reproducibility of computational models (<xref ref-type="bibr" rid="bib47">LeVeque et al., 2012</xref>; <xref ref-type="bibr" rid="bib24">Eglen et al., 2017</xref>; <xref ref-type="bibr" rid="bib62">Podlaski et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Manninen et al., 2018</xref>). A frequent complaint of students and researchers at all levels, is that when they try to implement published models using their own code, they get different results. A fascinating and detailed description of one such attempt is given in <xref ref-type="bibr" rid="bib59">Pauli et al. (2018)</xref>. These sorts of problems led to the creation of the ReScience journal, dedicated to publishing replications of previous models or describing when those replication attempts failed (<xref ref-type="bibr" rid="bib68">Rougier et al., 2017</xref>). A number of issues contribute to this problem, and we designed Brian with these in mind. So, for example, users are required to write equations that are dimensionally consistent, a common source of problems. In addition, by requiring users to write equations explicitly rather than using pre-defined neuron types such as ‘integrate-and-fire’ and ‘Hodgkin-Huxley’, as in other simulators, we reduce the chance that the implementation expected by the user is different to the one provided by the simulator. We discuss this point further below, but we should note the opposing view that standardisation and common implementation are advantages has also been put forward (<xref ref-type="bibr" rid="bib16">Davison et al., 2008</xref>; <xref ref-type="bibr" rid="bib29">Gleeson et al., 2010</xref>; <xref ref-type="bibr" rid="bib65">Raikov et al., 2011</xref>). Perhaps more importantly, by making user-written code simpler and more readable, we increase the chance that the implementation faithfully represents the description of a model. Allowing for more flexibility and targeting the complete computational experiment increases the chances that the entire simulation script can be compactly represented in a single file or programming language, further reducing the chances of such errors.</p><sec id="s3-1"><title>Comparison to other approaches</title><p>We have described some of the key design choices we made for version 2 of the Brian simulator. These represent a particular balance between the conflicting demands of flexibility, ease-of-use, features and performance, and we now compare the results of these choices to other available options for simulations.</p><p>There are two main differences of approach between Brian and other simulators. Firstly, we require model definitions to be explicit. Users are required to give the full set of equations and parameters that define the model, rather than using ‘standard’ model names and default parameters (cf. <xref ref-type="bibr" rid="bib8">Brette, 2012</xref>). This approach requires a slightly higher initial investment of effort from the user, but ensures that users know precisely what their model is doing and reduces the risk of a difference between the implementation of the model and the description of it in a paper (see discussion above). One limitation of this approach is that it makes it more difficult to design tools to programmatically inspect a model, for example to identify and shut down all inhibitory currents (although note that this issue remains for languages such as NeuroML and NineML that are primarily based on standard models as they include the ability to define arbitrary equations).</p><p>The second main difference is that we consider the complete computational experiment to be fundamental, and so everything is tightly integrated to the extent that an entire model can be specified in a single, readable file, including equations, protocols, data analysis, etc. In Neuron and NEST, model definitions are separate from the computational experiment script, and indeed written in an entirely different language (see below). This adds complexity and increases the chance of errors. In NeuroML and NineML, there is no way of specifying arbitrary computational experiments. One counter-argument to this approach is that clearly separating model definitions may reduce the effort in re-using models or programmatically comparing them (as in <xref ref-type="bibr" rid="bib62">Podlaski et al., 2017</xref>).</p><p>A consequence of the requirement to make model definitions explicit, and an important feature for doing novel research, is that the simulator must support arbitrary user-specified equations. This is available in Neuron via the NMODL description format (<xref ref-type="bibr" rid="bib42">Hines and Carnevale, 2000</xref>), and in a limited form in NEST using NESTML (<xref ref-type="bibr" rid="bib61">Plotnikov et al., 2016</xref>). NeuroML and NineML now both include the option for specifying arbitrary equations, although the level of simulator support for these aspects of the standards is unclear. While some level of support for arbitrary model equations is now fairly widespread in simulators, Brian was the first to make this a fundamental, core concept that is applied universally. Some simulators that have since followed this approach include DynaSim (<xref ref-type="bibr" rid="bib70">Sherfey et al., 2018</xref>), which is based on MATLAB, and ANNarchy (<xref ref-type="bibr" rid="bib80">Vitay et al., 2015</xref>). Other new simulators have taken an alternative approach, such as Xolotl (<xref ref-type="bibr" rid="bib35">Gorur-Shandilya et al., 2018</xref>) which is based on building hierarchical representations of neurons from a library of basic components. One aspect of the equation-based approach that is missing from other simulators is the specification of additional defining network features, such as synaptic connectivity patterns, in an equally flexible, equation-oriented way. Neuron is focused on single neuron modelling rather than networks, and only supports directly setting the connectivity synapse-by-synapse. NEST, PyNN (<xref ref-type="bibr" rid="bib16">Davison et al., 2008</xref>), NeuroML, and NineML support this too, and also include some predefined general connectivity patterns such as one-to-one and all-to-all. NEST further includes a system for specifying connectivity via a ‘connection set algebra’ (<xref ref-type="bibr" rid="bib21">Djurfeldt, 2012</xref>) allowing for combinations of a few core types of connectivity. However, none have yet followed Brian in allowing the user to specify connectivity patterns via equations, as is commonly done in research papers.</p><sec id="s3-1-1"><title>Performance</title><p>Running compiled code for arbitrary equations means that code generation must be used. This requirement leads to a problem: a simulator that makes use of a fixed set of models can provide hand-optimised implementations of them, whereas a fully flexible simulator must rely on automated techniques. By contrast, an advantage of automated techniques is that they can generate optimisations for specialisations of models. For example, using the CUBA benchmark (<xref ref-type="bibr" rid="bib82">Vogels and Abbott, 2005</xref>; <xref ref-type="bibr" rid="bib7">Brette et al., 2007</xref>) in which all neurons have identical time constants, Brian 2 is dramatically faster than Brian 1, NEURON and NEST (<xref ref-type="fig" rid="fig8">Figure 8</xref>, left). This happens because Brian 2 can generate a specialised optimisation of the code since the model definition states that time constants are the same. If instead we modify the benchmark to feature heterogeneous time constants (<xref ref-type="fig" rid="fig8">Figure 8</xref>, right), then Brian 2 has to do much more work since it can no longer use these optimisations, while the run times for NEST and NEURON do not change.</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.011</object-id><label>Figure 8.</label><caption><title>Benchmark of the simulation time for the CUBA network (<xref ref-type="bibr" rid="bib82">Vogels and Abbott, 2005</xref>; <xref ref-type="bibr" rid="bib7">Brette et al., 2007</xref>), a sparsely connected network of leaky-integrate and fire network with synapses modelled as exponentially decaying currents.</title><p>Synaptic connections are random, with each neuron receiving on average 80 synaptic inputs and weights set to ensure ongoing asynchronous activity in the network. The simulations use exact integration, but spike spike times are aligned to the simulation grid of 0.1 ms. Simulations are shown for a homogeneous population (left), where the membrane time constant, as well as the excitatory and inhibitory time constant, are the same for all neurons. In the heterogeneous population (right), these constants are different for each neuron, randomly set between 90% and 110% of the constant values used in the homogeneous population. Simulations were performed with NEST 2.16 (blue, <xref ref-type="bibr" rid="bib49">Linssen et al., 2018</xref>, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002963">SCR_002963</ext-link>), NEURON 7.6 (orange; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_005393">SCR_005393</ext-link>), Brian 1.4.4 (red), and Brian 2.2.2.1 (shades of green, <xref ref-type="bibr" rid="bib74">Stimberg et al., 2019b</xref>, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002998">SCR_002998</ext-link>). Benchmarks were run under Python 2.7.16 on an Intel Core i9-7920X machine with 12 processor cores. For NEST and one of the Brian 2 simulations (light green), simulations made use of all processor cores by using 12 threads via the OpenMP framework. Brian 2 ‘runtime’ simulations execute C++ code via the weave library, while ‘standalone’ code executes an independent binary file compiled from C++ code (see Appendix 1 for details). Simulation times do not include the one-off times to prepare the simulation and generate synaptic connections as these will become a vanishing fraction of the total time for runs with longer simulated times. Simulations were run for a biological time of 10 s for small networks (8000 neurons or fewer) and for 1 s for large networks. The times plotted here are the best out of three repetitions. Note that Brian 1.4.4 does not support exact integration for a heterogeneous population and has therefore not been included for that benchmark.</p><p><supplementary-material id="fig8sdata1"><object-id pub-id-type="doi">10.7554/eLife.47314.012</object-id><label>Figure 8—source data 1.</label><caption><title>Benchmark results for the homogeneous network.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-47314-fig8-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig8sdata2"><object-id pub-id-type="doi">10.7554/eLife.47314.013</object-id><label>Figure 8—source data 2.</label><caption><title>Benchmark results for the heterogeneous network.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-47314-fig8-data2-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-fig8-v2.tif"/></fig><p>We can make two additional observations based on this benchmark. Firstly, the benefits of parallelisation via multi-threading depend heavily on the model being simulated. For a large homogeneous population, the single threaded and multi-threaded standalone runs of Brian 2 take approximately the same time, and the single threaded run is actually faster at smaller network sizes. For the heterogeneous population, the opposite result holds: multi-threaded is always faster at all network sizes.</p><p>The second observation is that the advantage of running a Brian 2 simulation in standalone mode is most significant for smaller networks, at least for the single threaded case (for the moment, multi-threaded code is only available for standalone mode).</p><p>It should be noted, however, that despite the fact that Brian 2 is the fastest simulator at large network sizes for this benchmark, this does not mean that Brian 2 is faster than other simulators such as NEURON or NEST in general. The NEURON simulator can be used to simulate the point neuron models used in this benchmark, but with its strong focus on the simulation of biologically detailed, multi-compartment neuron models, it is not well adapted to this task. NEST, on the other hand, has been optimised to simulate very large networks, with many synapses impinging on each neuron. Most importantly, Brian’s performance here strongly benefits from its focus on running simulations on individual machines where all simulation elements are kept in a single, shared memory space. In contrast, NEST and NEURON use more sophisticated communication between model elements which may cost performance in benchmarks like the one shown here, but can scale up to bigger simulations spread out over multiple machines. For a fairly recent and more detailed comparison of simulators, see <xref ref-type="bibr" rid="bib77">Tikidji-Hamburyan et al. (2017)</xref>, although note that they did not test the standalone mode of Brian 2.</p></sec><sec id="s3-1-2"><title>Limitations of brian</title><p>The main limitation of Brian compared to other simulators is the lack of support for running large networks over multiple machines, and scaling up to specialised, high-performance clusters as well as supercomputers. While this puts a limit on the maximum feasible size of simulations, the majority of neuroscientists do not have direct access to such equipment, and few computational neuroscience studies require such large scale simulations (tens of millions of neurons). More common is to run smaller networks but multiple times over a large range of different parameters. This ‘embarrassingly parallel’ case can be easily and straightforwardly carried out with Brian at any scale, from individual machines to cloud computing platforms or the non-specialised clusters routinely available as part of university computing services. An example for such a parameter exploration is shown in <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2</xref>. This simulation strongly benefits from parallelisation even on a single machine, with the simulation time reduced by about a factor of about 45 when run on a GPU.</p><p>Finally, let us note that this manuscript has focused exclusively on single-compartment point neuron models, where an entire neuron is represented without any spatial properties or compartmentalisation into dendrites, soma, and axon. Such models have been extensively used for the study of network properties, but are not sufficiently detailed for studying other questions, for example dendritic integration. For such studies, researchers typically investigate multi-compartment models, that is neurons modelled as a set of interconnected compartments. Currents across the membrane in each compartment are modelled in the same way as for point neurons, but there are additional axial currents with neighbouring compartments. Such models are the primary focus of simulators such as NEURON and GENESIS, but only have very limited support in simulators such as NEST. While Brian is used mostly for point neurons, it does offer support for multi-compartmental models, using the same equation-based approach (see <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>). This feature is not yet as mature as those of specialised simulators such as NEURON and GENESIS, and is an important area for future development in Brian.</p></sec></sec><sec id="s3-2"><title>Development and availability</title><p>Brian is released under the free and open CeCILL 2 license. Development takes place in a public code repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/brian-team/brian2">https://github.com/brian-team/brian2</ext-link> (<xref ref-type="bibr" rid="bib11">Brian contributors, 2019</xref>). All examples in this article have been simulated with Brian 2 version 2.2.2.1 (<xref ref-type="bibr" rid="bib74">Stimberg et al., 2019b</xref>). Brian has a permanent core team of three developers (the authors of this paper), and regularly receives substantial contributions from a number of students, postdocs and users (see Acknowledgements). Code is continuously and automatically checked against a comprehensive test suite run on all platforms, with almost complete coverage. Extensive documentation, including installation instructions, is hosted at <ext-link ext-link-type="uri" xlink:href="http://brian2.readthedocs.org">http://brian2.readthedocs.org</ext-link>. Brian is available for Python 2 and 3, and for the operating systems Windows, OS X and Linux; our download statistics show that all these versions are in active use. More information can be found at <ext-link ext-link-type="uri" xlink:href="http://briansimulator.org/">http://briansimulator.org/</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank the following contributors for having made contributions, big or small, to the Brian 2 code or documentation: Moritz Augustin, Victor Benichoux, Werner Beroux, Edward Betts, Daniel Bliss, Jacopo Bono, Paul Brodersen, Romain Cazé, Meng Dong, Guillaume Dumas, Ben Evans, Charlee Fletterman, Dominik Krzemiński, Kapil Kumar, Thomas McColgan, Matthieu Recugnat, Dylan Richard, Cyrille Rossant, Jan-Hendrik Schleimer, Alex Seeholzer, Martino Sorbaro, Daan Sprenkels, Teo Stocco, Mihir Vaidya, Adrien F Vincent, Konrad Wartke, Pierre Yger, Friedemann Zenke. Three of these contributors (CF, DK, KK) contributed while participating in Google’s Summer of Code program.</p></ack><sec id="s4" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Supervision, Funding acquisition, Investigation, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Supervision, Investigation, Methodology, Writing—original draft, Writing—review and editing</p></fn></fn-group></sec><sec id="s5" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><object-id pub-id-type="doi">10.7554/eLife.47314.014</object-id><label>Source code 1.</label><caption><title>Benchmarking code.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-47314-code1-v2.zip"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.47314.015</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-47314-transrepform-v2.docx"/></supplementary-material></sec><sec id="s6" sec-type="data-availability"><title>Data availability</title><p>Source code to replicate Figures 1-7, as well as the simulations shown in Appendix 4, are provided in a github repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/brian-team/brian2_paper_examples">https://github.com/brian-team/brian2_paper_examples</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/brian2_paper_examples">https://github.com/elifesciences-publications/brian2_paper_examples</ext-link>). Source code to run benchmarks as presented in Figure 8 is provided as a supplementary file together with this submission (Source code 1).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1998">1998</year><chapter-title>Modeling small networks</chapter-title><person-group person-group-type="editor"><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Segev</surname> <given-names>I</given-names></name></person-group><source>Methods in Neuronal Modeling</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>361</fpage><lpage>410</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ascoli</surname> <given-names>GA</given-names></name><name><surname>Donohue</surname> <given-names>DE</given-names></name><name><surname>Halavi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>NeuroMorpho.Org: a central resource for neuronal morphologies</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>9247</fpage><lpage>9251</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2055-07.2007</pub-id><pub-id pub-id-type="pmid">17728438</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behnel</surname> <given-names>S</given-names></name><name><surname>Bradshaw</surname> <given-names>R</given-names></name><name><surname>Citro</surname> <given-names>C</given-names></name><name><surname>Dalcin</surname> <given-names>L</given-names></name><name><surname>Seljebotn</surname> <given-names>DS</given-names></name><name><surname>Smith</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cython: the best of both worlds</article-title><source>Computing in Science &amp; Engineering</source><volume>13</volume><fpage>31</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2010.118</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bencina</surname> <given-names>R</given-names></name><name><surname>Burk</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><data-title>PortAudio: Portable real-time audio library</data-title><ext-link ext-link-type="uri" xlink:href="http://www.portaudio.com">http://www.portaudio.com</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blundell</surname> <given-names>I</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name><name><surname>Cleland</surname> <given-names>TA</given-names></name><name><surname>Close</surname> <given-names>TG</given-names></name><name><surname>Coca</surname> <given-names>D</given-names></name><name><surname>Davison</surname> <given-names>AP</given-names></name><name><surname>Diaz-Pier</surname> <given-names>S</given-names></name><name><surname>Fernandez Musoles</surname> <given-names>C</given-names></name><name><surname>Gleeson</surname> <given-names>P</given-names></name><name><surname>Goodman</surname> <given-names>DFM</given-names></name><name><surname>Hines</surname> <given-names>M</given-names></name><name><surname>Hopkins</surname> <given-names>MW</given-names></name><name><surname>Kumbhar</surname> <given-names>P</given-names></name><name><surname>Lester</surname> <given-names>DR</given-names></name><name><surname>Marin</surname> <given-names>B</given-names></name><name><surname>Morrison</surname> <given-names>A</given-names></name><name><surname>Müller</surname> <given-names>E</given-names></name><name><surname>Nowotny</surname> <given-names>T</given-names></name><name><surname>Peyser</surname> <given-names>A</given-names></name><name><surname>Plotnikov</surname> <given-names>D</given-names></name><name><surname>Richmond</surname> <given-names>P</given-names></name><name><surname>Rowley</surname> <given-names>A</given-names></name><name><surname>Rumpe</surname> <given-names>B</given-names></name><name><surname>Stimberg</surname> <given-names>M</given-names></name><name><surname>Stokes</surname> <given-names>AB</given-names></name><name><surname>Tomkins</surname> <given-names>A</given-names></name><name><surname>Trensch</surname> <given-names>G</given-names></name><name><surname>Woodman</surname> <given-names>M</given-names></name><name><surname>Eppler</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Code generation in computational neuroscience: a review of tools and techniques</article-title><source>Frontiers in Neuroinformatics</source><volume>12</volume><elocation-id>68</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2018.00068</pub-id><pub-id pub-id-type="pmid">30455637</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bower</surname> <given-names>JM</given-names></name><name><surname>Beeman</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>The Book of GENESIS: Exploring Realistic Neural Models with the GEneral NEural SImulation System</source><edition>Second Edition</edition><publisher-name>Springer-Verlag</publisher-name></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname> <given-names>R</given-names></name><name><surname>Rudolph</surname> <given-names>M</given-names></name><name><surname>Carnevale</surname> <given-names>T</given-names></name><name><surname>Hines</surname> <given-names>M</given-names></name><name><surname>Beeman</surname> <given-names>D</given-names></name><name><surname>Bower</surname> <given-names>JM</given-names></name><name><surname>Diesmann</surname> <given-names>M</given-names></name><name><surname>Morrison</surname> <given-names>A</given-names></name><name><surname>Goodman</surname> <given-names>PH</given-names></name><name><surname>Harris</surname> <given-names>FC</given-names></name><name><surname>Zirpe</surname> <given-names>M</given-names></name><name><surname>Natschläger</surname> <given-names>T</given-names></name><name><surname>Pecevski</surname> <given-names>D</given-names></name><name><surname>Ermentrout</surname> <given-names>B</given-names></name><name><surname>Djurfeldt</surname> <given-names>M</given-names></name><name><surname>Lansner</surname> <given-names>A</given-names></name><name><surname>Rochel</surname> <given-names>O</given-names></name><name><surname>Vieville</surname> <given-names>T</given-names></name><name><surname>Muller</surname> <given-names>E</given-names></name><name><surname>Davison</surname> <given-names>AP</given-names></name><name><surname>El Boustani</surname> <given-names>S</given-names></name><name><surname>Destexhe</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Simulation of networks of spiking neurons: a review of tools and strategies</article-title><source>Journal of Computational Neuroscience</source><volume>23</volume><fpage>349</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1007/s10827-007-0038-6</pub-id><pub-id pub-id-type="pmid">17629781</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>On the design of script languages for neural simulation</article-title><source>Network: Computation in Neural Systems</source><volume>23</volume><fpage>150</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.3109/0954898X.2012.716902</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname> <given-names>R</given-names></name><name><surname>Goodman</surname> <given-names>DF</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Vectorized algorithms for spiking neural network simulation</article-title><source>Neural Computation</source><volume>23</volume><fpage>1503</fpage><lpage>1535</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00123</pub-id><pub-id pub-id-type="pmid">21395437</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname> <given-names>R</given-names></name><name><surname>Goodman</surname> <given-names>DFM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Simulating spiking neural networks on GPU</article-title><source>Network: Computation in Neural Systems</source><volume>23</volume><fpage>167</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.3109/0954898X.2012.730170</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Brian contributors</collab></person-group><year iso-8601-date="2019">2019</year><data-title><italic>brian2</italic></data-title><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/brian-team/brian2">https://github.com/brian-team/brian2</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cannon</surname> <given-names>RC</given-names></name><name><surname>Gleeson</surname> <given-names>P</given-names></name><name><surname>Crook</surname> <given-names>S</given-names></name><name><surname>Ganapathy</surname> <given-names>G</given-names></name><name><surname>Marin</surname> <given-names>B</given-names></name><name><surname>Piasini</surname> <given-names>E</given-names></name><name><surname>Silver</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>LEMS: a language for expressing complex biological models in concise and hierarchical form and its use in underpinning NeuroML 2</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>79</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00079</pub-id><pub-id pub-id-type="pmid">25309419</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Carnevale</surname> <given-names>NT</given-names></name><name><surname>Hines</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>The NEURON Book</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname> <given-names>K</given-names></name><name><surname>Schultz</surname> <given-names>SR</given-names></name><name><surname>Luk</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>NeuroFlow: a general purpose spiking neural network simulation platform using customizable processors</article-title><source>Frontiers in Neuroscience</source><volume>9</volume><elocation-id>516</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2015.00516</pub-id><pub-id pub-id-type="pmid">26834542</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crook</surname> <given-names>SM</given-names></name><name><surname>Bednar</surname> <given-names>JA</given-names></name><name><surname>Berger</surname> <given-names>S</given-names></name><name><surname>Cannon</surname> <given-names>R</given-names></name><name><surname>Davison</surname> <given-names>AP</given-names></name><name><surname>Djurfeldt</surname> <given-names>M</given-names></name><name><surname>Eppler</surname> <given-names>J</given-names></name><name><surname>Kriener</surname> <given-names>B</given-names></name><name><surname>Furber</surname> <given-names>S</given-names></name><name><surname>Graham</surname> <given-names>B</given-names></name><name><surname>Plesser</surname> <given-names>HE</given-names></name><name><surname>Schwabe</surname> <given-names>L</given-names></name><name><surname>Smith</surname> <given-names>L</given-names></name><name><surname>Steuber</surname> <given-names>V</given-names></name><name><surname>van Albada</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Creating, documenting and sharing network models</article-title><source>Network: Computation in Neural Systems</source><volume>23</volume><fpage>131</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.3109/0954898X.2012.722743</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davison</surname> <given-names>AP</given-names></name><name><surname>Brüderle</surname> <given-names>D</given-names></name><name><surname>Eppler</surname> <given-names>J</given-names></name><name><surname>Kremkow</surname> <given-names>J</given-names></name><name><surname>Muller</surname> <given-names>E</given-names></name><name><surname>Pecevski</surname> <given-names>D</given-names></name><name><surname>Perrinet</surname> <given-names>L</given-names></name><name><surname>Yger</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>PyNN: a common interface for neuronal network simulators</article-title><source>Frontiers in Neuroinformatics</source><volume>2</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.011.2008</pub-id><pub-id pub-id-type="pmid">19194529</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davison</surname> <given-names>AP</given-names></name><name><surname>Hines</surname> <given-names>ML</given-names></name><name><surname>Muller</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Trends in programming languages for neuroscience simulations</article-title><source>Frontiers in Neuroscience</source><volume>3</volume><fpage>374</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.3389/neuro.01.036.2009</pub-id><pub-id pub-id-type="pmid">20198154</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Schutter</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>A consumer guide to neuronal modeling software</article-title><source>Trends in Neurosciences</source><volume>15</volume><fpage>462</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(92)90011-V</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destexhe</surname> <given-names>A</given-names></name><name><surname>Neubig</surname> <given-names>M</given-names></name><name><surname>Ulrich</surname> <given-names>D</given-names></name><name><surname>Huguenard</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Dendritic low-threshold calcium currents in thalamic relay cells</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>3574</fpage><lpage>3588</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-10-03574.1998</pub-id><pub-id pub-id-type="pmid">9570789</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Djurfeldt</surname> <given-names>M</given-names></name><name><surname>Hjorth</surname> <given-names>J</given-names></name><name><surname>Eppler</surname> <given-names>JM</given-names></name><name><surname>Dudani</surname> <given-names>N</given-names></name><name><surname>Helias</surname> <given-names>M</given-names></name><name><surname>Potjans</surname> <given-names>TC</given-names></name><name><surname>Bhalla</surname> <given-names>US</given-names></name><name><surname>Diesmann</surname> <given-names>M</given-names></name><name><surname>Hellgren Kotaleski</surname> <given-names>J</given-names></name><name><surname>Ekeberg</surname> <given-names>Örjan</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Run-Time interoperability between neuronal network simulators based on the MUSIC framework</article-title><source>Neuroinformatics</source><volume>8</volume><fpage>43</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1007/s12021-010-9064-z</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Djurfeldt</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The connection-set algebra--a novel formalism for the representation of connectivity structure in neuronal network models</article-title><source>Neuroinformatics</source><volume>10</volume><fpage>287</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1007/s12021-012-9146-1</pub-id><pub-id pub-id-type="pmid">22437992</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dura-Bernal</surname> <given-names>S</given-names></name><name><surname>Neymotin</surname> <given-names>SA</given-names></name><name><surname>Kerr</surname> <given-names>CC</given-names></name><name><surname>Sivagnanam</surname> <given-names>S</given-names></name><name><surname>Majumdar</surname> <given-names>A</given-names></name><name><surname>Francis</surname> <given-names>JT</given-names></name><name><surname>Lytton</surname> <given-names>WW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evolutionary algorithm optimization of biological learning parameters in a biomimetic neuroprosthesis</article-title><source>IBM Journal of Research and Development</source><volume>61</volume><fpage>6:1</fpage><lpage>6:6</lpage><pub-id pub-id-type="doi">10.1147/JRD.2017.2656758</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edin</surname> <given-names>F</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name><name><surname>Schütze</surname> <given-names>H</given-names></name><name><surname>Herz</surname> <given-names>AV</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Searching for optimal sensory signals: iterative stimulus reconstruction in closed-loop experiments</article-title><source>Journal of Computational Neuroscience</source><volume>17</volume><fpage>47</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1023/B:JCNS.0000023868.18446.a2</pub-id><pub-id pub-id-type="pmid">15218353</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eglen</surname> <given-names>SJ</given-names></name><name><surname>Marwick</surname> <given-names>B</given-names></name><name><surname>Halchenko</surname> <given-names>YO</given-names></name><name><surname>Hanke</surname> <given-names>M</given-names></name><name><surname>Sufi</surname> <given-names>S</given-names></name><name><surname>Gleeson</surname> <given-names>P</given-names></name><name><surname>Silver</surname> <given-names>RA</given-names></name><name><surname>Davison</surname> <given-names>AP</given-names></name><name><surname>Lanyon</surname> <given-names>L</given-names></name><name><surname>Abrams</surname> <given-names>M</given-names></name><name><surname>Wachtler</surname> <given-names>T</given-names></name><name><surname>Willshaw</surname> <given-names>DJ</given-names></name><name><surname>Pouzat</surname> <given-names>C</given-names></name><name><surname>Poline</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Toward standard practices for sharing computer code and programs in neuroscience</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>770</fpage><lpage>773</lpage><pub-id pub-id-type="doi">10.1038/nn.4550</pub-id><pub-id pub-id-type="pmid">28542156</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fidjeland</surname> <given-names>AK</given-names></name><name><surname>Roesch</surname> <given-names>EB</given-names></name><name><surname>Shanahan</surname> <given-names>MP</given-names></name><name><surname>Luk</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>NeMo: a platform for neural modelling of spiking neurons using GPUs</article-title><conf-name>Proceedings of the International Conference on Application-Specific Systems, Architectures and Processors</conf-name><fpage>137</fpage><lpage>144</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Furber</surname> <given-names>SB</given-names></name><name><surname>Galluppi</surname> <given-names>F</given-names></name><name><surname>Temple</surname> <given-names>S</given-names></name><name><surname>Plana</surname> <given-names>LA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The SpiNNaker project</article-title><conf-name>Proceedings of the IEEE</conf-name><fpage>652</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2014.2304638</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gerstner</surname> <given-names>W</given-names></name><name><surname>Kistler</surname> <given-names>WM</given-names></name><name><surname>Naud</surname> <given-names>R</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Neuronal Dynamics</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gewaltig</surname> <given-names>M-O</given-names></name><name><surname>Diesmann</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>NEST (NEural simulation tool)</article-title><source>Scholarpedia</source><volume>2</volume><elocation-id>1430</elocation-id><pub-id pub-id-type="doi">10.4249/scholarpedia.1430</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gleeson</surname> <given-names>P</given-names></name><name><surname>Crook</surname> <given-names>S</given-names></name><name><surname>Cannon</surname> <given-names>RC</given-names></name><name><surname>Hines</surname> <given-names>ML</given-names></name><name><surname>Billings</surname> <given-names>GO</given-names></name><name><surname>Farinella</surname> <given-names>M</given-names></name><name><surname>Morse</surname> <given-names>TM</given-names></name><name><surname>Davison</surname> <given-names>AP</given-names></name><name><surname>Ray</surname> <given-names>S</given-names></name><name><surname>Bhalla</surname> <given-names>US</given-names></name><name><surname>Barnes</surname> <given-names>SR</given-names></name><name><surname>Dimitrova</surname> <given-names>YD</given-names></name><name><surname>Silver</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>NeuroML: a language for describing data driven models of neurons and networks with a high degree of biological detail</article-title><source>PLOS Computational Biology</source><volume>6</volume><fpage>e1000815</fpage><lpage>e1000819</lpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000815</pub-id><pub-id pub-id-type="pmid">20585541</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golowasch</surname> <given-names>J</given-names></name><name><surname>Casey</surname> <given-names>M</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Network stability from activity-dependent regulation of neuronal conductances</article-title><source>Neural Computation</source><volume>11</volume><fpage>1079</fpage><lpage>1096</lpage><pub-id pub-id-type="doi">10.1162/089976699300016359</pub-id><pub-id pub-id-type="pmid">10418158</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname> <given-names>DF</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Code generation: a strategy for neural network simulators</article-title><source>Neuroinformatics</source><volume>8</volume><fpage>183</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1007/s12021-010-9082-x</pub-id><pub-id pub-id-type="pmid">20857234</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname> <given-names>D</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Brian: a simulator for spiking neural networks in Python</article-title><source>Frontiers in Neuroinformatics</source><volume>2</volume><elocation-id>5</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.005.2008</pub-id><pub-id pub-id-type="pmid">19115011</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname> <given-names>DF</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The Brian simulator</article-title><source>Frontiers in Neuroscience</source><volume>3</volume><fpage>192</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.3389/neuro.01.026.2009</pub-id><pub-id pub-id-type="pmid">20011141</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname> <given-names>D</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brian simulator</article-title><source>Scholarpedia</source><volume>8</volume><elocation-id>10883</elocation-id><pub-id pub-id-type="doi">10.4249/scholarpedia.10883</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorur-Shandilya</surname> <given-names>S</given-names></name><name><surname>Hoyland</surname> <given-names>A</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Xolotl: an intuitive and approachable neuron and network simulator for research and teaching</article-title><source>Frontiers in Neuroinformatics</source><volume>12</volume><elocation-id>87</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2018.00087</pub-id><pub-id pub-id-type="pmid">30534067</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Günay</surname> <given-names>C</given-names></name><name><surname>Prinz</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Model calcium sensors for network homeostasis: sensor and readout parameter analysis from a database of model neuronal networks</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>1686</fpage><lpage>1698</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3098-09.2010</pub-id><pub-id pub-id-type="pmid">20130178</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahne</surname> <given-names>J</given-names></name><name><surname>Helias</surname> <given-names>M</given-names></name><name><surname>Kunkel</surname> <given-names>S</given-names></name><name><surname>Igarashi</surname> <given-names>J</given-names></name><name><surname>Bolten</surname> <given-names>M</given-names></name><name><surname>Frommer</surname> <given-names>A</given-names></name><name><surname>Diesmann</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A unified framework for spiking and gap-junction interactions in distributed neuronal network simulations</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>22</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00022</pub-id><pub-id pub-id-type="pmid">26441628</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahne</surname> <given-names>J</given-names></name><name><surname>Dahmen</surname> <given-names>D</given-names></name><name><surname>Schuecker</surname> <given-names>J</given-names></name><name><surname>Frommer</surname> <given-names>A</given-names></name><name><surname>Bolten</surname> <given-names>M</given-names></name><name><surname>Helias</surname> <given-names>M</given-names></name><name><surname>Diesmann</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Integration of Continuous-Time dynamics in a spiking neural network simulator</article-title><source>Frontiers in Neuroinformatics</source><volume>11</volume><elocation-id>34</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2017.00034</pub-id><pub-id pub-id-type="pmid">28596730</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hathway</surname> <given-names>P</given-names></name><name><surname>Goodman</surname> <given-names>DFM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>[Re] Spike timing dependent plasticity finds the start of repeating patterns in continuous spike trains</article-title><source>ReScience</source><volume>4</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.5281/zenodo.1327348.</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hettinger</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><data-title>PEP 289 – Generator Expressions</data-title><ext-link ext-link-type="uri" xlink:href="https://www.python.org/dev/peps/pep-0289">https://www.python.org/dev/peps/pep-0289</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hindmarsh</surname> <given-names>JL</given-names></name><name><surname>Rose</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>A model of neuronal bursting using three coupled first order differential equations,</article-title><source>Proceedings of the Royal Society of London. Series B, Biological Sciences</source><volume>221</volume><fpage>87</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1098/rspb.1984.0024</pub-id><pub-id pub-id-type="pmid">6144106</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname> <given-names>ML</given-names></name><name><surname>Carnevale</surname> <given-names>NT</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Expanding NEURON's repertoire of mechanisms with NMODL</article-title><source>Neural Computation</source><volume>12</volume><fpage>995</fpage><lpage>1007</lpage><pub-id pub-id-type="doi">10.1162/089976600300015475</pub-id><pub-id pub-id-type="pmid">10905805</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hodgkin</surname> <given-names>AL</given-names></name><name><surname>Huxley</surname> <given-names>AF</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>A quantitative description of membrane current and its application to conduction and excitation in nerve</article-title><source>The Journal of Physiology</source><volume>117</volume><fpage>500</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1952.sp004764</pub-id><pub-id pub-id-type="pmid">12991237</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huguenard</surname> <given-names>JR</given-names></name><name><surname>Prince</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>A novel T-type current underlies prolonged Ca(2+)-dependent burst firing in GABAergic neurons of rat thalamic reticular nucleus</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>3804</fpage><lpage>3817</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.12-10-03804.1992</pub-id><pub-id pub-id-type="pmid">1403085</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>E</given-names></name><name><surname>Oliphant</surname> <given-names>T</given-names></name><name><surname>Peterson</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><data-title>SciPy: Open source scientific tools for Python</data-title><ext-link ext-link-type="uri" xlink:href="http://www.scipy.org">http://www.scipy.org</ext-link></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname> <given-names>JJ</given-names></name><name><surname>Steinmetz</surname> <given-names>NA</given-names></name><name><surname>Siegle</surname> <given-names>JH</given-names></name><name><surname>Denman</surname> <given-names>DJ</given-names></name><name><surname>Bauza</surname> <given-names>M</given-names></name><name><surname>Barbarits</surname> <given-names>B</given-names></name><name><surname>Lee</surname> <given-names>AK</given-names></name><name><surname>Anastassiou</surname> <given-names>CA</given-names></name><name><surname>Andrei</surname> <given-names>A</given-names></name><name><surname>Aydın</surname> <given-names>Ç</given-names></name><name><surname>Barbic</surname> <given-names>M</given-names></name><name><surname>Blanche</surname> <given-names>TJ</given-names></name><name><surname>Bonin</surname> <given-names>V</given-names></name><name><surname>Couto</surname> <given-names>J</given-names></name><name><surname>Dutta</surname> <given-names>B</given-names></name><name><surname>Gratiy</surname> <given-names>SL</given-names></name><name><surname>Gutnisky</surname> <given-names>DA</given-names></name><name><surname>Häusser</surname> <given-names>M</given-names></name><name><surname>Karsh</surname> <given-names>B</given-names></name><name><surname>Ledochowitsch</surname> <given-names>P</given-names></name><name><surname>Lopez</surname> <given-names>CM</given-names></name><name><surname>Mitelut</surname> <given-names>C</given-names></name><name><surname>Musa</surname> <given-names>S</given-names></name><name><surname>Okun</surname> <given-names>M</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Putzeys</surname> <given-names>J</given-names></name><name><surname>Rich</surname> <given-names>PD</given-names></name><name><surname>Rossant</surname> <given-names>C</given-names></name><name><surname>Sun</surname> <given-names>WL</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Harris</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeVeque</surname> <given-names>RJ</given-names></name><name><surname>Mitchell</surname> <given-names>IM</given-names></name><name><surname>Stodden</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reproducible research for scientific computing: tools and strategies for changing the culture</article-title><source>Computing in Science &amp; Engineering</source><volume>14</volume><fpage>13</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2012.38</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Licklider</surname> <given-names>JCR</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Periodicity pitch and related auditory process models</article-title><source>International Audiology</source><volume>1</volume><fpage>11</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.3109/05384916209074592</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Linssen</surname> <given-names>C</given-names></name><name><surname>Lepperød ME</surname></name><name><surname>Mitchell</surname> <given-names>J</given-names></name><name><surname>Pronold</surname> <given-names>J</given-names></name><name><surname>Eppler</surname> <given-names>JM</given-names></name><name><surname>Keup</surname> <given-names>C</given-names></name><name><surname>Peyser</surname> <given-names>A</given-names></name><name><surname>Kunkel</surname> <given-names>S</given-names></name><name><surname>Weidel</surname> <given-names>P</given-names></name><name><surname>Nodem</surname> <given-names>Y</given-names></name><name><surname>Terhorst</surname> <given-names>D</given-names></name><name><surname>Deepu</surname> <given-names>R</given-names></name><name><surname>Deger</surname> <given-names>M</given-names></name><name><surname>Hahne</surname> <given-names>J</given-names></name><name><surname>Sinha</surname> <given-names>A</given-names></name><name><surname>Antonietti</surname> <given-names>A</given-names></name><name><surname>Schmidt</surname> <given-names>M</given-names></name><name><surname>Paz</surname> <given-names>L</given-names></name><name><surname>Garrido</surname> <given-names>J</given-names></name><name><surname>Ippen</surname> <given-names>T</given-names></name><name><surname>Riquelme</surname> <given-names>L</given-names></name><name><surname>Serenko</surname> <given-names>A</given-names></name><name><surname>Kühn</surname> <given-names>T</given-names></name><name><surname>Kitayama</surname> <given-names>I</given-names></name><name><surname>Mørk</surname> <given-names>H</given-names></name><name><surname>Spreizer</surname> <given-names>S</given-names></name><name><surname>Jordan</surname> <given-names>J</given-names></name><name><surname>Krishnan</surname> <given-names>J</given-names></name><name><surname>Senden</surname> <given-names>M</given-names></name><name><surname>Hagen</surname> <given-names>E</given-names></name><name><surname>Shusharin</surname> <given-names>A</given-names></name><name><surname>Vennemo</surname> <given-names>SB</given-names></name><name><surname>Rodarie</surname> <given-names>D</given-names></name><name><surname>Morrison</surname> <given-names>A</given-names></name><name><surname>Graber</surname> <given-names>S</given-names></name><name><surname>Schuecker</surname> <given-names>J</given-names></name><name><surname>Diaz</surname> <given-names>S</given-names></name><name><surname>Zajzon</surname> <given-names>B</given-names></name><name><surname>Plesser</surname> <given-names>HE</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Nest 2.16.0</data-title><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1400175">https://doi.org/10.5281/zenodo.1400175</ext-link></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manninen</surname> <given-names>T</given-names></name><name><surname>Aćimović</surname> <given-names>J</given-names></name><name><surname>Havela</surname> <given-names>R</given-names></name><name><surname>Teppola</surname> <given-names>H</given-names></name><name><surname>Linne</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Challenges in reproducibility, replicability, and comparability of computational models and tools for neuronal and glial networks, cells, and subcellular structures</article-title><source>Frontiers in Neuroinformatics</source><volume>12</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2018.00020</pub-id><pub-id pub-id-type="pmid">29765315</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merolla</surname> <given-names>PA</given-names></name><name><surname>Arthur</surname> <given-names>JV</given-names></name><name><surname>Alvarez-Icaza</surname> <given-names>R</given-names></name><name><surname>Cassidy</surname> <given-names>AS</given-names></name><name><surname>Sawada</surname> <given-names>J</given-names></name><name><surname>Akopyan</surname> <given-names>F</given-names></name><name><surname>Jackson</surname> <given-names>BL</given-names></name><name><surname>Imam</surname> <given-names>N</given-names></name><name><surname>Guo</surname> <given-names>C</given-names></name><name><surname>Nakamura</surname> <given-names>Y</given-names></name><name><surname>Brezzo</surname> <given-names>B</given-names></name><name><surname>Vo</surname> <given-names>I</given-names></name><name><surname>Esser</surname> <given-names>SK</given-names></name><name><surname>Appuswamy</surname> <given-names>R</given-names></name><name><surname>Taba</surname> <given-names>B</given-names></name><name><surname>Amir</surname> <given-names>A</given-names></name><name><surname>Flickner</surname> <given-names>MD</given-names></name><name><surname>Risk</surname> <given-names>WP</given-names></name><name><surname>Manohar</surname> <given-names>R</given-names></name><name><surname>Modha</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Artificial brains. A million spiking-neuron integrated circuit with a scalable communication network and interface</article-title><source>Science</source><volume>345</volume><fpage>668</fpage><lpage>673</lpage><pub-id pub-id-type="doi">10.1126/science.1254642</pub-id><pub-id pub-id-type="pmid">25104385</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meurer</surname> <given-names>A</given-names></name><name><surname>Smith</surname> <given-names>CP</given-names></name><name><surname>Paprocki</surname> <given-names>M</given-names></name><name><surname>Čertík</surname> <given-names>O</given-names></name><name><surname>Kirpichev</surname> <given-names>SB</given-names></name><name><surname>Rocklin</surname> <given-names>M</given-names></name><name><surname>Kumar</surname> <given-names>AMiT</given-names></name><name><surname>Ivanov</surname> <given-names>S</given-names></name><name><surname>Moore</surname> <given-names>JK</given-names></name><name><surname>Singh</surname> <given-names>S</given-names></name><name><surname>Rathnayake</surname> <given-names>T</given-names></name><name><surname>Vig</surname> <given-names>S</given-names></name><name><surname>Granger</surname> <given-names>BE</given-names></name><name><surname>Muller</surname> <given-names>RP</given-names></name><name><surname>Bonazzi</surname> <given-names>F</given-names></name><name><surname>Gupta</surname> <given-names>H</given-names></name><name><surname>Vats</surname> <given-names>S</given-names></name><name><surname>Johansson</surname> <given-names>F</given-names></name><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>Curry</surname> <given-names>MJ</given-names></name><name><surname>Terrel</surname> <given-names>AR</given-names></name><name><surname>Roučka</surname> <given-names>Štěpán</given-names></name><name><surname>Saboo</surname> <given-names>A</given-names></name><name><surname>Fernando</surname> <given-names>I</given-names></name><name><surname>Kulal</surname> <given-names>S</given-names></name><name><surname>Cimrman</surname> <given-names>R</given-names></name><name><surname>Scopatz</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>SymPy: symbolic computing in Python</article-title><source>PeerJ Computer Science</source><volume>3</volume><elocation-id>e103</elocation-id><pub-id pub-id-type="doi">10.7717/peerj-cs.103</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mittal</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A survey of techniques for approximate computing</article-title><source>ACM Computing Surveys</source><volume>48</volume><fpage>1</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1145/2893356</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>SW</given-names></name><name><surname>Fox</surname> <given-names>PJ</given-names></name><name><surname>Marsh</surname> <given-names>SJ</given-names></name><name><surname>Markettos</surname> <given-names>AT</given-names></name><name><surname>Mujumdar</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Bluehive - A field-programable custom computing machine for extreme-scale real-time neural network simulation</article-title><conf-name>Proceedings of the 2012 IEEE 20th International Symposium on Field-Programmable Custom Computing Machines</conf-name><fpage>133</fpage><lpage>140</lpage></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname> <given-names>E</given-names></name><name><surname>Bednar</surname> <given-names>JA</given-names></name><name><surname>Diesmann</surname> <given-names>M</given-names></name><name><surname>Gewaltig</surname> <given-names>M-O</given-names></name><name><surname>Hines</surname> <given-names>M</given-names></name><name><surname>Davison</surname> <given-names>AP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Python in neuroscience</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><pub-id pub-id-type="doi">10.3389/fninf.2015.00011</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadim</surname> <given-names>F</given-names></name><name><surname>Manor</surname> <given-names>Y</given-names></name><name><surname>Nusbaum</surname> <given-names>MP</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Frequency regulation of a slow rhythm by a fast periodic input</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>5053</fpage><lpage>5067</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-13-05053.1998</pub-id><pub-id pub-id-type="pmid">9634571</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Williams</surname> <given-names>AH</given-names></name><name><surname>Franci</surname> <given-names>A</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cell types, network homeostasis, and pathological compensation from a biologically plausible ion channel expression model</article-title><source>Neuron</source><volume>82</volume><fpage>809</fpage><lpage>821</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.002</pub-id><pub-id pub-id-type="pmid">24853940</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Sutton</surname> <given-names>AC</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Computational models in the age of large datasets</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>87</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.01.006</pub-id><pub-id pub-id-type="pmid">25637959</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pauli</surname> <given-names>R</given-names></name><name><surname>Weidel</surname> <given-names>P</given-names></name><name><surname>Kunkel</surname> <given-names>S</given-names></name><name><surname>Morrison</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reproducing polychronization: a guide to maximizing the reproducibility of spiking network models</article-title><source>Frontiers in Neuroinformatics</source><volume>12</volume><elocation-id>46</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2018.00046</pub-id><pub-id pub-id-type="pmid">30123121</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Platkiewicz</surname> <given-names>J</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Impact of fast sodium channel inactivation on spike threshold dynamics and synaptic integration</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1001129</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1001129</pub-id><pub-id pub-id-type="pmid">21573200</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Plotnikov</surname> <given-names>D</given-names></name><name><surname>Blundell</surname> <given-names>I</given-names></name><name><surname>Ippen</surname> <given-names>T</given-names></name><name><surname>Eppler</surname> <given-names>JM</given-names></name><name><surname>Morrison</surname> <given-names>A</given-names></name><name><surname>Rumpe</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><person-group person-group-type="editor"><name><surname>Oberweis</surname> <given-names>A</given-names></name><name><surname>Reussner</surname> <given-names>R</given-names></name></person-group><source>NESTML: A Modeling Language for Spiking Neurons</source><publisher-loc>Bonn</publisher-loc><publisher-name>Gesellschaft für Informatik</publisher-name></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Podlaski</surname> <given-names>WF</given-names></name><name><surname>Seeholzer</surname> <given-names>A</given-names></name><name><surname>Groschner</surname> <given-names>LN</given-names></name><name><surname>Miesenböck</surname> <given-names>G</given-names></name><name><surname>Ranjan</surname> <given-names>R</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mapping the function of neuronal ion channels in model and experiment</article-title><source>eLife</source><volume>6</volume><elocation-id>e22152</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22152</pub-id><pub-id pub-id-type="pmid">28267430</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prinz</surname> <given-names>AA</given-names></name><name><surname>Bucher</surname> <given-names>D</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Similar network activity from disparate circuit parameters</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>1345</fpage><lpage>1352</lpage><pub-id pub-id-type="doi">10.1038/nn1352</pub-id><pub-id pub-id-type="pmid">15558066</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prinz</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Insights from models of rhythmic motor systems</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>615</fpage><lpage>620</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.10.001</pub-id><pub-id pub-id-type="pmid">17056249</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raikov</surname> <given-names>I</given-names></name><name><surname>Cannon</surname> <given-names>R</given-names></name><name><surname>Clewley</surname> <given-names>R</given-names></name><name><surname>Cornelis</surname> <given-names>H</given-names></name><name><surname>Davison</surname> <given-names>A</given-names></name><name><surname>De Schutter</surname> <given-names>E</given-names></name><name><surname>Djurfeldt</surname> <given-names>M</given-names></name><name><surname>Gleeson</surname> <given-names>P</given-names></name><name><surname>Gorchetchnikov</surname> <given-names>A</given-names></name><name><surname>Plesser</surname> <given-names>HE</given-names></name><name><surname>Hill</surname> <given-names>S</given-names></name><name><surname>Hines</surname> <given-names>M</given-names></name><name><surname>Kriener</surname> <given-names>B</given-names></name><name><surname>Le Franc</surname> <given-names>Y</given-names></name><name><surname>Lo</surname> <given-names>C-C</given-names></name><name><surname>Morrison</surname> <given-names>A</given-names></name><name><surname>Muller</surname> <given-names>E</given-names></name><name><surname>Ray</surname> <given-names>S</given-names></name><name><surname>Schwabe</surname> <given-names>L</given-names></name><name><surname>Szatmary</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>NineML: the network interchange for neuroscience modeling language</article-title><source>BMC Neuroscience</source><volume>12</volume><elocation-id>P330</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-12-S1-P330</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richert</surname> <given-names>M</given-names></name><name><surname>Nageswaran</surname> <given-names>JM</given-names></name><name><surname>Dutt</surname> <given-names>N</given-names></name><name><surname>Krichmar</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>An efficient simulation environment for modeling large-scale cortical processing</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00019</pub-id><pub-id pub-id-type="pmid">22007166</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossant</surname> <given-names>C</given-names></name><name><surname>Goodman</surname> <given-names>DF</given-names></name><name><surname>Platkiewicz</surname> <given-names>J</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Automatic fitting of spiking neuron models to electrophysiological recordings</article-title><source>Frontiers in Neuroinformatics</source><volume>4</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.002.2010</pub-id><pub-id pub-id-type="pmid">20224819</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rougier</surname> <given-names>NP</given-names></name><name><surname>Hinsen</surname> <given-names>K</given-names></name><name><surname>Alexandre</surname> <given-names>F</given-names></name><name><surname>Arildsen</surname> <given-names>T</given-names></name><name><surname>Barba</surname> <given-names>LA</given-names></name><name><surname>Benureau</surname> <given-names>FCY</given-names></name><name><surname>Brown</surname> <given-names>CT</given-names></name><name><surname>de Buyl</surname> <given-names>P</given-names></name><name><surname>Caglayan</surname> <given-names>O</given-names></name><name><surname>Davison</surname> <given-names>AP</given-names></name><name><surname>Delsuc</surname> <given-names>M-A</given-names></name><name><surname>Detorakis</surname> <given-names>G</given-names></name><name><surname>Diem</surname> <given-names>AK</given-names></name><name><surname>Drix</surname> <given-names>D</given-names></name><name><surname>Enel</surname> <given-names>P</given-names></name><name><surname>Girard</surname> <given-names>B</given-names></name><name><surname>Guest</surname> <given-names>O</given-names></name><name><surname>Hall</surname> <given-names>MG</given-names></name><name><surname>Henriques</surname> <given-names>RN</given-names></name><name><surname>Hinaut</surname> <given-names>X</given-names></name><name><surname>Jaron</surname> <given-names>KS</given-names></name><name><surname>Khamassi</surname> <given-names>M</given-names></name><name><surname>Klein</surname> <given-names>A</given-names></name><name><surname>Manninen</surname> <given-names>T</given-names></name><name><surname>Marchesi</surname> <given-names>P</given-names></name><name><surname>McGlinn</surname> <given-names>D</given-names></name><name><surname>Metzner</surname> <given-names>C</given-names></name><name><surname>Petchey</surname> <given-names>O</given-names></name><name><surname>Plesser</surname> <given-names>HE</given-names></name><name><surname>Poisot</surname> <given-names>T</given-names></name><name><surname>Ram</surname> <given-names>K</given-names></name><name><surname>Ram</surname> <given-names>Y</given-names></name><name><surname>Roesch</surname> <given-names>E</given-names></name><name><surname>Rossant</surname> <given-names>C</given-names></name><name><surname>Rostami</surname> <given-names>V</given-names></name><name><surname>Shifman</surname> <given-names>A</given-names></name><name><surname>Stachelek</surname> <given-names>J</given-names></name><name><surname>Stimberg</surname> <given-names>M</given-names></name><name><surname>Stollmeier</surname> <given-names>F</given-names></name><name><surname>Vaggi</surname> <given-names>F</given-names></name><name><surname>Viejo</surname> <given-names>G</given-names></name><name><surname>Vitay</surname> <given-names>J</given-names></name><name><surname>Vostinar</surname> <given-names>AE</given-names></name><name><surname>Yurchak</surname> <given-names>R</given-names></name><name><surname>Zito</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sustainable computational science: the ReScience initiative</article-title><source>PeerJ Computer Science</source><volume>3</volume><elocation-id>e142</elocation-id><pub-id pub-id-type="doi">10.7717/peerj-cs.142</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudolph</surname> <given-names>M</given-names></name><name><surname>Destexhe</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>How much can we trust neural simulation strategies?</article-title><source>Neurocomputing</source><volume>70</volume><fpage>1966</fpage><lpage>1969</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2006.10.138</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherfey</surname> <given-names>JS</given-names></name><name><surname>Soplata</surname> <given-names>AE</given-names></name><name><surname>Ardid</surname> <given-names>S</given-names></name><name><surname>Roberts</surname> <given-names>EA</given-names></name><name><surname>Stanley</surname> <given-names>DA</given-names></name><name><surname>Pittman-Polletta</surname> <given-names>BR</given-names></name><name><surname>Kopell</surname> <given-names>NJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DynaSim: a MATLAB toolbox for neural modeling and simulation</article-title><source>Frontiers in Neuroinformatics</source><volume>12</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2018.00010</pub-id><pub-id pub-id-type="pmid">29599715</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stimberg</surname> <given-names>M</given-names></name><name><surname>Goodman</surname> <given-names>DF</given-names></name><name><surname>Benichoux</surname> <given-names>V</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Equation-oriented specification of neural models for simulations</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00006</pub-id><pub-id pub-id-type="pmid">24550820</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Stimberg</surname> <given-names>M</given-names></name><name><surname>Goodman</surname> <given-names>DFM</given-names></name><name><surname>Nowotny</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Brian2GeNN: a system for accelerating a large variety of spiking neural networks with graphics hardware</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/448050</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Stimberg</surname> <given-names>M</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name><name><surname>Goodman</surname> <given-names>DF</given-names></name></person-group><year iso-8601-date="2019">2019a</year><data-title>brian2_paper_examples</data-title><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/brian-team/brian2_paper_examples">https://github.com/brian-team/brian2_paper_examples</ext-link></element-citation></ref><ref id="bib74"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Stimberg</surname> <given-names>M</given-names></name><name><surname>Goodman</surname> <given-names>DF</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019b</year><data-title>Brian 2</data-title><version designator="2.2.2.1">2.2.2.1</version><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/2619969">https://zenodo.org/record/2619969</ext-link></element-citation></ref><ref id="bib75"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stimberg</surname> <given-names>M</given-names></name><name><surname>Goodman</surname> <given-names>DF</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name><name><surname>De Pittà</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019c</year><chapter-title>Modeling neuron–glia interactions with the brian 2 simulator</chapter-title><source>Computational Glioscience</source><publisher-name>Springer</publisher-name><fpage>471</fpage><lpage>505</lpage></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stroud</surname> <given-names>JP</given-names></name><name><surname>Porter</surname> <given-names>MA</given-names></name><name><surname>Hennequin</surname> <given-names>G</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Motor primitives in space and time via targeted gain modulation in cortical networks</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1774</fpage><lpage>1783</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0276-0</pub-id><pub-id pub-id-type="pmid">30482949</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tikidji-Hamburyan</surname> <given-names>RA</given-names></name><name><surname>Narayana</surname> <given-names>V</given-names></name><name><surname>Bozkus</surname> <given-names>Z</given-names></name><name><surname>El-Ghazawi</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Software for brain network simulations: a comparative study</article-title><source>Frontiers in Neuroinformatics</source><volume>11</volume><elocation-id>46</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2017.00046</pub-id><pub-id pub-id-type="pmid">28775687</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Traub</surname> <given-names>RD</given-names></name><name><surname>Miles</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1991">1991</year><source>Neuronal Networks of the Hippocampus</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vella</surname> <given-names>M</given-names></name><name><surname>Cannon</surname> <given-names>RC</given-names></name><name><surname>Crook</surname> <given-names>S</given-names></name><name><surname>Davison</surname> <given-names>AP</given-names></name><name><surname>Ganapathy</surname> <given-names>G</given-names></name><name><surname>Robinson</surname> <given-names>HP</given-names></name><name><surname>Silver</surname> <given-names>RA</given-names></name><name><surname>Gleeson</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>libNeuroML and PyLEMS: using Python to combine procedural and declarative modeling approaches in computational neuroscience</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>38</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00038</pub-id><pub-id pub-id-type="pmid">24795618</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vitay</surname> <given-names>J</given-names></name><name><surname>Dinkelbach</surname> <given-names>HÜ</given-names></name><name><surname>Hamker</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ANNarchy: a code generation approach to neural simulations on parallel hardware</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00019</pub-id><pub-id pub-id-type="pmid">26283957</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voegtlin</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>CLONES : a closed-loop simulation framework for body, muscles and neurons</article-title><source>BMC Neuroscience</source><volume>12</volume><elocation-id>P363</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-12-S1-P363</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Signal propagation and logic gating in networks of integrate-and-fire neurons</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>10786</fpage><lpage>10795</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3508-05.2005</pub-id><pub-id pub-id-type="pmid">16291952</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weidel</surname> <given-names>P</given-names></name><name><surname>Djurfeldt</surname> <given-names>M</given-names></name><name><surname>Duarte</surname> <given-names>RC</given-names></name><name><surname>Morrison</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Closed loop interactions between spiking neural network and robotic simulators based on MUSIC and ROS</article-title><source>Frontiers in Neuroinformatics</source><volume>10</volume><elocation-id>31</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00031</pub-id><pub-id pub-id-type="pmid">27536234</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yavuz</surname> <given-names>E</given-names></name><name><surname>Turner</surname> <given-names>J</given-names></name><name><surname>Nowotny</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>GeNN: a code generation framework for accelerated brain simulations</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>18854</elocation-id><pub-id pub-id-type="doi">10.1038/srep18854</pub-id><pub-id pub-id-type="pmid">26740369</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname> <given-names>F</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>SuperSpike: supervised learning in multilayer spiking neural networks</article-title><source>Neural Computation</source><volume>30</volume><fpage>1514</fpage><lpage>1541</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01086</pub-id><pub-id pub-id-type="pmid">29652587</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.47314.016</object-id><sec id="s7" sec-type="appendix"><title>Design details</title><p>In this appendix, we provide further details about technical design decisions behind the Brian simulator. We also more exhaustively comment on the simulation code of the four case studies. Note that the example code provided as Jupyter notebooks (<ext-link ext-link-type="uri" xlink:href="https://github.com/brian-team/brian2_paper_examples">https://github.com/brian-team/brian2_paper_examples</ext-link>; <xref ref-type="bibr" rid="bib73">Stimberg et al., 2019a</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/brian2_paper_examples">https://github.com/elifesciences-publications/brian2_paper_examples</ext-link>) has extensive additional annotations as well.</p></sec><sec id="s8" sec-type="appendix"><title>Mathematical level</title><sec id="s8-1"><title>Physical units</title><p>Neural models are models of a physical system, and therefore variables have physical dimensions such as voltage or time. Accordingly, the Brian simulator requires quantities provided by the user, such as parameters or initial values of dynamical variables, to be specified in consistent physical units such as mV or s. This is in contrast to the approach of most other simulators, which simply define expected units for all model components, for example units of mV for the membrane potential. This is a common source of error because conventions are not always obvious and can be inconsistent. For example, while membrane surface area is often stated in units of μm<sup>2</sup>, channel densities are often given in mS cm<sup>−2</sup>. To remove this potential source of error, the Brian simulator enforces explicit use of units. It automatically takes care of conversions—multiplying a resistance (dimensions of <inline-formula><mml:math id="inf10"><mml:mi mathvariant="normal">Ω</mml:mi></mml:math></inline-formula>) with a current (dimensions of A) will result in a voltage (dimensions of V)—and raises an error when physical dimensions are incompatible, for example when adding a current to a resistance. Unit consistency is also checked within textual model descriptions (e.g. <xref ref-type="fig" rid="fig2">Figure 2</xref>, l. 8–18) and variable assignments (e.g. l. 23–27). To make this possible, a dimension in SI units has to be assigned to each dimensional model variable in the model description (l. 8–18).</p></sec><sec id="s8-2"><title>Model dynamics</title><p>Neuron and synapse models are generally hybrid systems consisting of continuous dynamics described by differential equations and discrete events (<xref ref-type="bibr" rid="bib7">Brette et al., 2007</xref>).</p><p>In the Brian simulator, differential equations are specified in strings using mathematical notation (<xref ref-type="fig" rid="fig2">Figure 2</xref>, l. 8–18). Differential equations can also be stochastic by using the symbol xi representing the noise term <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3c</xref>,l. 8). The numerical integration method can be specified explicitly, for example the pyloric circuit model chooses a second-order Runge-Kutta method (<xref ref-type="fig" rid="fig2">Figure 2</xref>, l. 22); without specification, an appropriate method is automatically chosen and reported. To this end, the user-provided equations are analysed symbolically using the Python package SymPy (<xref ref-type="bibr" rid="bib52">Meurer et al., 2017</xref>), and transformed into a sequence of operations to advance the system’s state by a single time step (for more details, see <xref ref-type="bibr" rid="bib71">Stimberg et al., 2014</xref>).</p><p>This approach applies both to neuron models and to synaptic models. In many models, synaptic conductances do not need to be calculated for each synapse individually, instead they can be lumped into a single post-synaptic variable that is part of the neuronal model description. In contrast, non-linear synaptic dynamics as in the pyloric network example need to be calculated for each synapse individually. Using the same formalism as for neurons, the synaptic model equations can describe dynamics with differential equations (e.g. <xref ref-type="fig" rid="fig2">Figure 2</xref>, l. 31–32/44–47). Post-synaptic conductances or currents can then be calculated individually and summed up for each post-synaptic neuron as indicated by the (<monospace>summed</monospace>) annotation (l. 32 and 46).</p><p>Neuron- or synapse-specific values which are not updated by differential equations are also included in the string description. This can be used to define values that are updated by external mechanisms, for example the synaptic currents in each neuron (l. 15–16) are updated by the respective synapses (l. 32 and l. 46). The same mechanism can also be used for neuron-specific parameters such as the calcium target value (l. 17), or the label identifying the neuron type (l. 18). For optimisation, the flag (<monospace>constant</monospace>) can be added to indicate that the value will not change during a simulation.</p><p>Neural simulations typically refer to two types of discrete events: production of a spike, and reception of a spike. A spike is produced by a neuron when a certain condition on its variables is met. A typical case is the integrate-and-fire model, where a spike is produced when the potential reaches a threshold of a fixed value. But there are other cases when the condition is more complex, for example when the threshold is adaptive (<xref ref-type="bibr" rid="bib60">Platkiewicz and Brette, 2011</xref>). To support conditions of all kind, Brian expects the user to define a mathematical expression as the <monospace>threshold</monospace>. In the case study 1, a spike is triggered whenever <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>v</mml:mi><mml:mo>&gt;</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2</xref>, l. 21). No explicit resetting takes place, since the model dynamics describe the membrane potential trajectory during an action potential. For a simpler integrate-and-fire model as the one used in case study 2, the membrane potential is reset to a fixed value after the threshold crossing (<xref ref-type="fig" rid="fig3">Figure 3</xref>, l. 12). Such spike-triggered actions are most generally specified by providing one or more assignments and operations (reset) that should take place if the threshold condition is fulfilled; in the case study 1, this is mechanism is used to update the calcium trace (<xref ref-type="fig" rid="fig2">Figure 2</xref>, l. 21).</p><p>Once a spike is produced, it may affect variables of synapses and post-synaptic neurons (possibly after a delay). Again, this is specified generally as a series of assignments and operations. In the pyloric circuit example, this does not apply because the synaptic effect is continuous and not triggered by discrete spikes. In case study 2 (<xref ref-type="fig" rid="fig3">Figure 3</xref>) however, each spike has an instantaneous effect. For example, when a motoneuron spikes, the eye resting position is increased or decreased by a fixed amount. This is specified by <monospace>on_pre=’x0_post + = w’</monospace> (l. 15), where <monospace>on_pre</monospace> is a keyword for stating what operations should be executed when a pre-synaptic spike is received. These operations can refer to both local synaptic variables (here <inline-formula><mml:math id="inf13"><mml:mi>w</mml:mi></mml:math></inline-formula>, defined in the synaptic model) and variables of the pre- and postsynaptic neuron (here <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, a variable of the post-synaptic neuron). In the same way, the <monospace>on_post</monospace> keyword can be used to specify operations executed when a postsynaptic spike is received, which allows defining various types of spike-timing-dependent models.</p><p>This general definition scheme applies to neurons and synapses, but as case study 2 (<xref ref-type="fig" rid="fig3">Figure 3</xref>) illustrates, it can also be used to define dynamical models of muscles and the environment. It also naturally extends to the modelling of non-neuronal elements of the brain such as glial cells (<xref ref-type="bibr" rid="bib75">Stimberg et al., 2019c</xref>).</p></sec><sec id="s8-3"><title>Links between model components</title><p>The equations defining the dynamics of variables can only refer to other variables within the same model component, for example within the same group of neurons or synapses. Connections to other components have to be explicitly modelled using synaptic connections as explained above. However, we may sometimes also need to directly refer to the state of variables in other model component. For example, in case study 2 (<xref ref-type="fig" rid="fig3">Figure 3</xref>), the input to retinal neurons depends on eye and object positions, which are updated in a group separate from the group representing the retinal neurons (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, l. 3–9). This can be expressed by defining a ‘linked variable’, which refers to a variable defined in a different model component. In the group modelling the retinal neurons, the variables <monospace>x_object</monospace> and <monospace>x_eye</monospace> are annotated with the (<monospace>linked</monospace>) flag to state that they are references to variables defined elsewhere (l. 23–24). This link is then made explicit by stating the group and variable they refer to via the <monospace>linked_var</monospace> function (l. 28–29).</p></sec><sec id="s8-4"><title>Initialisation</title><p>The description of its dynamics does not yet completely define a model, we also need to define its initial state. For some variables, this initial state can simply be a fixed value, for example in the pyloric network model, the neurons’ membrane potential is initialised to the resting potential <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2</xref> l. 24). In the general case, however, we might want to calculate the initial state; Brian therefore accepts arbitrary mathematical expressions for setting the initial value of state variables. These expressions can refer to model variables, as well as to pre-defined constant such as the index of a neuron within its group (i), or the total number of neurons within a group (<monospace>N</monospace>), as well as to pre-defined functions such as <monospace>rand()</monospace> (providing uniformly distributed random numbers between 0 and 1). In case study 1, we use this mechanism to initialise variables <inline-formula><mml:math id="inf16"><mml:mi>w</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf17"><mml:mi>z</mml:mi></mml:math></inline-formula> randomly (<xref ref-type="fig" rid="fig2">Figure 2</xref>, l. 25–26); in case study 2, we assign individual preferred positions to each retinal neuron, covering the space from −1 to 1 in a regular fashion (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, l. 30).</p><p>Mathematical expressions can also be used to select a subset of neurons and synapses and make conditional assignments. In case study 1, we assign a specific value to the conductance of synapses between ABPD and LP neurons by using the selection criterion <monospace>’label_pre == ABPD and label_post == LP’</monospace> (<xref ref-type="fig" rid="fig2">Figure 2</xref>, l. 36), referring to the custom label identifier of the pre- and post-synaptic neuron that has been introduced as part of the neuron model definition (l. 18). In this example there is only a single neuron per type, but the syntax generalises to groups of neurons of arbitrary size and is therefore preferable to the explicit use of numerical indices.</p></sec><sec id="s8-5"><title>Synaptic connections</title><p>The second main aspect of model construction is the creation of synaptic connections. For maximal expressivity, we again allow the use of mathematical expressions to define rules of connectivity. For example, in case study 1, following the schematic shown in <xref ref-type="fig" rid="fig1">Figure 1a</xref>, we would like to connect neurons with fast glutamatergic synapses according to two rules: 1) connections should occur between all groups, but not within groups of the same neuron type; 2) there should not be any connections from PY neurons to AB/PD neurons. We can express this with a string condition following the same syntax that we used to set initial values for synaptic conductances earlier (<xref ref-type="fig" rid="fig2">Figure 2</xref>, l. 35):</p><p><monospace>fast.connect(’label_pre!=label_post and not (label_pre == PY and label_post == ABPD)’)</monospace></p><p>For more complex examples, in particular connection specifications based on the spatial location of neurons, see <xref ref-type="bibr" rid="bib71">Stimberg et al. (2014)</xref>.</p><p>For larger networks, it can be wasteful to check a condition for each possible connection. Brian therefore also offers the possibility to use a mathematical expression to directly specify the projections of each neuron. In the eye movement example, each retinal neuron on the left hemifield (i.e. <inline-formula><mml:math id="inf18"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>neuron</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) should connect to the first motoneuron (index 0), while neurons on the right hemifield (i.e. <inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>neuron</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) should connect to the second motoneuron (index 1). We can express this connection scheme by defining <inline-formula><mml:math id="inf20"><mml:mi>j</mml:mi></mml:math></inline-formula>, the postsynaptic target index, for each presynaptic neuron accordingly (with the <monospace>int</monospace> function converting a truth value into 0 or 1):</p><p><monospace>sensorimotor_synapses.connect(j=’int(x_neuron_pre&gt;0)’)</monospace></p><p>This syntax can also be extended to generate more than one post-synaptic target per pre-synaptic neuron, using a syntax borrowed from Python’s generator syntax (<xref ref-type="bibr" rid="bib40">Hettinger, 2002</xref>, see the Brian 2 documentation at <ext-link ext-link-type="uri" xlink:href="http://brian2.readthedocs.io">http://brian2.readthedocs.io</ext-link> for more details) These mechanisms can also be used to define stochastic connectivity schemes, either by specifying a fixed connection probability that will be evaluated in addition to the given conditions, or by specifying a connection probability as a function of pre- and post-synaptic properties.</p><p>Specifying synaptic connections in the way presented here has several advantages over alternative approaches. In contrast to explicitly enumerating the connections by referring to pre- and post-synaptic neuron indices, the use of mathematical expressions transparently conveys the logic behind the connection pattern and automatically scales with the size of the connected groups of neurons. These advantages are shared with simulators that provide pre-defined connectivity patterns such as ‘one-to-one’ or ‘all-to-all’. However, such approaches are not as general—for example they could not concisely define the connectivity pattern shown in <xref ref-type="fig" rid="fig1">Figure 1a</xref>—and can additionally suffer from ambiguity. For example, should a group of neurons that is ‘all-to-all’ connected to itself form autapses or not (cf. <xref ref-type="bibr" rid="bib15">Crook et al., 2012</xref>)?</p></sec></sec><sec id="s9" sec-type="appendix"><title>Computational experiment level</title><p>The Brian simulator allows the user to write complete experiment descriptions that include both the model description and the simulation protocol in a single Python script as exemplified by the case studies in this article. In this section, we will discuss how the Brian simulator interacts with the statements and programming logic expressed in the surrounding script code.</p><sec id="s9-1"><title>Simulation flow</title><p>In the case study 3, we use a specific simulation workflow, an iterative approach to finding a parameter value (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). Many other simulation protocols are regularly used. For example, a simulation might consist of several consecutive runs, where some model aspect such as the external stimulation changes between runs. Alternatively, several different types of models might be tested in a single script where each is run independently. Or, a non-deterministic simulation might be run repeatedly to sample its behaviour. Capturing all these potential protocols in a single descriptive framework is hopeless, we therefore need the flexibility of a programming language with its control structures such as loops and conditionals.</p><p>Brian offers two main facilities to assist in implementing arbitrary simulation protocols. Simulations can be continued at their last state, potentially after activating/deactivating model elements, or changing global or group-specific constants and variables as shown above. Additionally, simulations can revert back to a previous state using the functions <monospace>store</monospace> and <monospace>restore</monospace> provided by Brian. In the example script shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>, this mechanism is used to reset the network to an initial state after each iteration. The same mechanism allows for more complex protocols by referring to multiple states, for example to implement a train/test/validate protocol in a synaptic plasticity setting.</p><p>Providing explicit support for this functionality is not only a question of convenience; while the user could approximate this functionality by storing and resetting the systems state variables (membrane potentials, gating variables, etc.) manually, some model aspects such as action potentials that have not yet triggered synaptic effects (due to synaptic delays) are not easily accessible to the user.</p></sec><sec id="s9-2"><title>Model component scheduling</title><p>During each time step of a simulation run, several operations have to be performed. These include the numerical integration of the state variables, the propagation of synaptic activity, or the application of reset statements for neurons that emitted an action potential. All these operations have to be executed in a certain order. The Brian simulator approaches this issue in a flexible and transparent way: each operation has an associated clock with a certain time granularity dt, as well as a ‘scheduling slot’ and a priority value within that slot. Together, these elements determine the order of all operations across and within time steps.</p><p>By default, all objects are associated with the same clock, which simplifies setting a global simulation timestep for all objects (<xref ref-type="fig" rid="fig5">Figure 5</xref>, l. 2). However, individual objects may chose a different timestep, for example to record synaptic weights only sporadically during a long-running simulation run. In the same way, Brian offers a default ordering of all operations during a time step, but allows to change the schedule that is used, or to reschedule individual objects to other scheduling slots.</p><p>This amount of flexibility might appear to be unnecessary at a first glance and indeed details of the scheduling are rarely reported when describing models in a publication. Still, subtle differences in scheduling can have significant impact on simulation results (see <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref> for an illustration). This is most obvious when investigating paradigms such as spike-timing-dependent-plasticity with a high sensitivity to small temporal differences (<xref ref-type="bibr" rid="bib69">Rudolph and Destexhe, 2007</xref>).</p></sec><sec id="s9-3"><title>Name resolution</title><p>Model descriptions refer to various ‘names’, such as variables, constants, or functions. Some of these references, such as function names or global constants, will have the same meaning everywhere. Others, such as state variables or neuron indices, will depend on the context. This context is defined by the model component, that is the group of neurons or the set of synapses, to which the description is attached. For example, consider the assignment to <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>g</mml:mi><mml:mi>Na</mml:mi></mml:msub></mml:math></inline-formula> (the maximum conductance of the sodium channel) in <xref ref-type="fig" rid="fig5">Figure 5</xref> (l. 20). Here, <monospace>gNa_min</monospace> and <monospace>gNa_max</monospace> refer to global constants (defined in l. 6; Brian also offers an alternative system where global constants and functions are explicitly provided via a Python dictionary instead of being deduced from values defined in the execution environment, but this system will not be further discussed here), whereas <monospace>i</monospace>, the neuron index, is a vector of values with one value for each neuron, and <monospace>N</monospace> refers to the total number of elements in the respective group.</p><p>It is important to note that the context is also given by its position in the program flow. For example, if we want to set the initial value for the gating variable <inline-formula><mml:math id="inf22"><mml:mi>m</mml:mi></mml:math></inline-formula> to its steady value, then this value will depend on the membrane potential <inline-formula><mml:math id="inf23"><mml:mi>v</mml:mi></mml:math></inline-formula> via the expressions for <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>β</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>. The order in which we set the values for <inline-formula><mml:math id="inf26"><mml:mi>v</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf27"><mml:mi>m</mml:mi></mml:math></inline-formula> does therefore matter:</p><p><monospace>neuron.v=0*mV</monospace></p><p><monospace>neuron.m=’1/(1 + betam/alpham)’</monospace></p><p>While this might appear trivial, it shows how the procedural aspect of models, that is the order of operations, can be important. A purely descriptive approach, for example stating initial values for all variables as part of the model equations, would not always be sufficient (however, in this specific case, setting <inline-formula><mml:math id="inf28"><mml:mi>v</mml:mi></mml:math></inline-formula> to 0 mV is unnecessary, since Brian automatically assigns the value to all uninitialised variables).</p><p>Some Python statements are translated into code that is run immediately, for example initialising a variable or creating synapses. Others are translated into code that is run at a later time. For example, the code to numerically integrate differential equations is not run at the point where those equations are defined, but rather at the point when the simulation is run via a call to the <monospace>run()</monospace> function. In this case, any named constants referred to in the equations will use their value at the time that the <monospace>run()</monospace> function is called, and not the value at the time the equations are defined. This allows for that value to change between multiple calls to <monospace>run()</monospace>, which may be useful to switch between global behaviours. For example, a typical use case is running with no external input current for a certain time to allow a neuron to settle into its stationary state, and then running with the current switched on by just changing the value of a constant from zero to some nonzero value between two consecutive <monospace>run()</monospace> calls.</p></sec></sec><sec id="s10" sec-type="appendix"><title>Implementation level</title><sec id="s10-1"><title>Code generation</title><p>In order to combine the flexibility and ease-of-use of high-level descriptions with the execution speed of low-level programming languages such as C, we employ a code generation approach (<xref ref-type="bibr" rid="bib31">Goodman, 2010</xref>). This code generation consists of three steps. The textual model description will first be transformed into a ‘code snippet’. The generation of such a code snippet requires various transformations of the provided model description: some syntax elements have to be translated (e.g. the use of the <monospace>**</monospace> operator to denote the power operation to a call to the <monospace>pow</monospace> function for C/C++), variables that are specific to certain neurons or synapses have to be properly indexed (e.g. a reset statement <monospace>v = -70*mV</monospace> has to be translated into a statement along the lines of <monospace>v[neuron_index] = -70*mV)</monospace>, and finally sequences of statements have to be expressed according to the target language syntax (e.g. by adding a semicolon to the end of each statement for C/C++). In a second step, these code snippets will then be embedded into a predefined target-code template, specific to the respective computation performed by the code. For example, the user-provided description of an integrate-and-fire neuron’s reset would be embedded into a loop that iterates over all the neurons that emitted an action potential during the current time step. Finally, the code has to be compiled and executed, giving it access to the memory location that the code has to read and modify. For further details on this approach, see <xref ref-type="bibr" rid="bib31">Goodman (2010)</xref>; <xref ref-type="bibr" rid="bib71">Stimberg et al., 2014</xref>.</p></sec><sec id="s10-2"><title>Code optimisation</title><p>Code resulting from the procedure described above will not necessarily perform computations in the most efficient way. Brian therefore uses additional techniques to further optimise the code for performance. Consider for example the <inline-formula><mml:math id="inf29"><mml:mi>x</mml:mi></mml:math></inline-formula> variable—representing the receptor activity—in <xref ref-type="fig" rid="fig7">Figure 7</xref>, described by the differential equation in l. 36. This equation can be integrated analytically, and the above described code generation process would therefore generate code like the following (here presented as ‘pseudo-code’):</p><p><monospace>for each neuron:</monospace></p><p><monospace>x_new = sound + exp(-dt/tau_ear) * (sound - x_old)</monospace></p><p>However, the expression that is calculated for every neuron contains <monospace>exp(-dt /tau _ear)</monospace> which is not only identical for all neurons but also relatively costly to evaluate. Brian will identify such constant expressions, and calculate them only once outside of the loop:</p><p><monospace>c = exp(-dt/tau_ear)</monospace></p><p><monospace>for each neuron:</monospace></p><p><monospace>x_new = sound + c * (sound - x_old)</monospace></p><p>In addition to this type of optimisation, the Brian simulator will also simplify arithmetic expressions, such as replacing <monospace><monospace>0</monospace>*x</monospace> by <monospace>0</monospace>, or <monospace>x/x</monospace> by <monospace>1</monospace>. While all these optimizations could in principle also be performed by the programming-language compiler (e.g. gcc), we have found that performing these changes before handing over the code to the compiler led to bigger and more reliable performance benefits.</p></sec><sec id="s10-3"><title>Code execution: runtime mode</title><p>After the code generation process, each model component has been transformed into one or more ‘code objects’, each performing a specific computational task. For example, a group of integrate-and-fire neurons would typically result in three code objects. The first would be responsible for integrating the state variables over a single timestep, the second for checking the threshold condition to determine which neurons emit an action potential, and the third for applying the reset statements to those neurons. By default, these code objects will be executed in Brian’s ‘runtime mode’, meaning that the simulation loop will be executed in Python and then call each of the code objects to perform the actual computation (in the order defined by the scheduling as described in the previous section). Note that while the code objects will typically be based on generated C++ code, they can be compiled and executed from within Python using binding libraries such as <italic>weave</italic> (formerly part of S<italic>ciPy</italic>; <xref ref-type="bibr" rid="bib45">Jones et al., 2001</xref>) or <italic>Cython</italic> (<xref ref-type="bibr" rid="bib3">Behnel et al., 2011</xref>).</p><p>This ‘mixed’ approach to model execution leaves the simulation control to the main Python process while the actual computations are performed in compiled code, operating on shared memory structures. This results in a considerable amount of flexibility: whenever Brian’s model description formalism is not expressive enough for a task at hand, the researcher can interleave the execution of generated code with a hand-written function that can potentially access and modify any aspect of the model. In particular, such a function could intervene in the simulation process itself, for example by interrupting the simulation if certain criteria are met. The Jupyter notebook at <ext-link ext-link-type="uri" xlink:href="https://github.com/brian-team/brian2_paper_examples">https://github.com/brian-team/brian2_paper_examples</ext-link> contains an interactive version of case study 2 (<xref ref-type="fig" rid="fig3">Figure 3</xref>). In this example, the aforementioned mechanism is used to allow the user to interactively control a running Brian simulation, as well as for providing a graphical representation of the results that updates continuously.</p><p>While having all these advantages, the back-and-forth between the main loop in Python and the code objects also entails a performance overhead. This performance overhead takes a constant amount of time per code object and time step and does therefore matter less if the individual components perform long-running computations, such as for large networks (see also <xref ref-type="fig" rid="fig8">Figure 8</xref>). On the other hand, for simulations of small or medium-sized networks, such as the network presented in case study 4 (<xref ref-type="fig" rid="fig6">Figure 6</xref>), this overhead can be considerable and the alternative execution mode presented in the following section might provide a better alternative.</p></sec><sec id="s10-4"><title>Code execution: standalone mode</title><p>As an alternative to the mode of execution presented in the previous section, the Brian simulator offers the so-called ‘standalone mode’, currently implemented for the C++ programming language. In this mode, Brian generates code that performs the simulation loop itself and executes the operations according to the schedule. Additionally, it creates code to manages the memory for all state variables and other data structures such as the queuing mechanism used for applying synaptic effects with delays. This code, along with the code of the individual code objects, establishes a complete ‘standalone’ version of the simulation run. When the resulting binary file is executed, it will perform the simulation and write all the results to disk. Since the generated code does not depend on any non-standard libraries, it can be easily transferred to other machines or architectures (e.g. for robotics applications). The generated code is free from any overhead related to Python or complex data structures and therefore executes with high performance.</p><p>For many models, the use of this mode only requires the researcher to add a single line to the simulation script (declaring <monospace>set_device(’cpp_standalone’)</monospace>), all aspects of the model descriptions, including assignments to state variables and the order of operations will be faithfully conserved in the generated code. The Python script will transparently compile and execute the standalone code, and then read the results back from disk so that the researcher does not have to adapt their analysis routines.</p><p>However, in contrast to the runtime execution mode presented earlier, it is not possible to interact with the simulation during its execution from within the Python script. In addition, certain programming logic is no longer possible, since all actions such as synapse generation or variable assignments are not executed when they are stated, but only as part of the simulation run.</p><p>In this execution mode, simulations of moderate size and complexity can be run in real-time (<xref ref-type="fig" rid="fig8">Figure 8</xref>), enabling studies such as the one presented in case study 4 (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Importantly, this mode does not require the researcher to be actively involved in any details of the compilation, execution of the simulation or the retrieval of the results.</p></sec></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.47314.017</object-id><sec id="s11" sec-type="appendix"><title>Simulation scheduling</title><fig id="app2fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.018</object-id><label>Appendix 2—figure 1.</label><caption><title>Demonstration of the effect of scheduling simulation elements.</title><p>(<bold>a</bold>) Timing of synaptic effects on the post-synaptic cell for the two simulation schedules defined in (<bold>c</bold>). (<bold>b</bold>) Basic simulation code for the simulation results shown in (<bold>a</bold>). (<bold>c</bold>) Definition of a simulation schedule where threshold crossings trigger spikes and – assuming the absence of synaptic delays – their effect is applied directly within the same simulation time step (left; see blue line in (<bold>a</bold>)), and a schedule where synaptic effects are applied in the time step following a threshold crossing (right; see orange line in (<bold>a</bold>)). (<bold>d</bold>) Summary of the scheduling of the simulation elements following the default schedule (left code in (<bold>c</bold>)), as provided by Brian’s <monospace>scheduling_summary</monospace> function. Note that for increased readibility, the objects from (<bold>b</bold>) have been explicitly named to match the variable names. Without this change, the code in (<bold>b</bold>) leads to the use of standard names for the objects (<monospace>spikegeneratorgroup, neurongroup, synapses,</monospace> and <monospace>statemonitor</monospace>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-app2-fig1-v2.tif"/></fig></sec></boxed-text></app><app id="appendix-3"><title>Appendix 3</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.47314.019</object-id><sec id="s12" sec-type="appendix"><title>Model definitions in other simulation software</title><fig id="app3fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.020</object-id><label>Appendix 3—figure 1.</label><caption><title>Graded synapse model.</title><p>(<bold>a</bold>) Demonstration of the effect of the graded synapse model used in case study 1 (<xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig2">Figure 2</xref>). On the left, the membrane potential excursion of a pre-synaptic neuron is modelled by a squared sinusoidal function of time with varying amplitudes from 5 mV to 20 mV. The plot on the right shows the post-synaptic membrane potential of a cell receiving graded synaptic input from the pre-synaptic cell via the graded synapse model from case study 1 (slow cholinergic synapse, cf. <xref ref-type="bibr" rid="bib30">Golowasch et al., 1999</xref>). The post-synaptic cell is modelled here as a simple leaky integrator with a single synaptic input current. (<bold>b</bold>) Code excerpt showing the Brian 2 definition of the graded synapse model used in (<bold>a</bold>), taken from the code used in case study 1 (<xref ref-type="fig" rid="fig2">Figure 2</xref>). (<bold>c</bold>) Code excerpt defining a graded synapse model in C++ as part of 'The Pyloric Network Model Simulator' (<ext-link ext-link-type="uri" xlink:href="http://www.biology.emory.edu/research/Prinz/database-sensors/">http://www.biology.emory.edu/research/Prinz/database-sensors/</ext-link>; <xref ref-type="bibr" rid="bib36">Günay and Prinz, 2010</xref>). The complete code is 3510 lines.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-app3-fig1-v2.tif"/></fig><fig id="app3fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.021</object-id><label>Appendix 3—figure 2.</label><caption><title>Graded synapse model (cont.).</title><p>(<bold>a</bold>) Definition of a graded synapse model in NeuroML2/LEMS. The graded synapse model as described in <xref ref-type="bibr" rid="bib63">Prinz et al. (2004)</xref> has been added as a “core type” to the (not yet finalized) NeuroML2 standard and can therefore be accessed under the name gradedSynapse (top). It is fully defined via the LEMS definition partially reproduced here. (<bold>b</bold>) A definition of a graded synapse in the stomatogastric system, written in the NMODL language for the NEURON simulator. This model has been implemented in <xref ref-type="bibr" rid="bib56">Nadim et al. (1998)</xref>, see <ext-link ext-link-type="uri" xlink:href="https://senselab.med.yale.edu/ModelDB/showmodel.cshtml?model=3511">https://senselab.med.yale.edu/ModelDB/showmodel.cshtml?model=3511</ext-link>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-app3-fig2-v2.tif"/></fig></sec></boxed-text></app><app id="appendix-4"><title>Appendix 4</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.47314.022</object-id><sec id="s13" sec-type="appendix"><title>Additional Brian examples</title><sec id="s13-1"><title>Multi-compartmental models</title><fig id="app4fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.023</object-id><label>Appendix 4—figure 1.</label><caption><title>A multi-compartment model of a thalamic relay cell.</title><p>(<bold>a</bold>) Simulation of a thamalic relay cell with increased T-current in distal dendrites (partially reproducing Figure 9C from <xref ref-type="bibr" rid="bib19">Destexhe et al. (1998)</xref>. The plot shows the somatic membrane potential for a current injection of 75 pA during the period marked by the black line. The model consists of a total of 1291 compartments and is based on the morphology available on <ext-link ext-link-type="uri" xlink:href="http://neuromorpho.org">NeuroMorpho.Org</ext-link> (<xref ref-type="bibr" rid="bib2">Ascoli et al., 2007</xref>) under the ID NMO_00881, displayed on the right. This morphology is a reconstruction of a cell in the rat’s ventrobasal complex, originally described in <xref ref-type="bibr" rid="bib44">Huguenard and Prince (1992)</xref>. (<bold>b</bold>) Selected lines from the simulation code implementing the model shown in (<bold>a</bold>), focussing on the differences to single-compartmental models (as shown in case studies 1–4).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-app4-fig1-v2.tif"/></fig></sec><sec id="s13-2"><title>Parameter exploration</title><fig id="app4fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.47314.024</object-id><label>Appendix 4—figure 2.</label><caption><title>Parameter exploration over two parameters.</title><p>(<bold>a</bold>) In a single-compartment neuron model of the Hodgkin-Huxley type (following <xref ref-type="bibr" rid="bib78">Traub and Miles, 1991</xref>), we record the number of spikes over 1 s while varying the strength of a constant input current <inline-formula><mml:math id="inf30"><mml:mi>I</mml:mi></mml:math></inline-formula>, and the density of the sodium conductance <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for a total of 300 × 300 values. The bars on the right show the simulation time on the same machine used for <xref ref-type="fig" rid="fig8">Figure 8</xref> when using Brian 2’s C++ standalone mode with a single thread (top), with 12 threads (middle), or when simulating it on a NVIDIA GeForce RTX 2080 Ti graphics card via the Brian2GeNN (<xref ref-type="bibr" rid="bib72">Stimberg et al., 2018</xref>) interface to the GeNN (<xref ref-type="bibr" rid="bib84">Yavuz et al., 2016</xref>) simulator (bottom). (<bold>b</bold>) Code for the simulation shown in (<bold>a</bold>), here configured to run on the GPU via the Brian2GeNN interface (l.2-3).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47314-app4-fig2-v2.tif"/></fig></sec></sec></boxed-text></app><app id="appendix-5"><title>Appendix 5</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.47314.025</object-id><sec id="s14" sec-type="appendix"><title>Comparison of Brian 1 and Brian 2</title><p>Brian 2 was rewritten from scratch, however it was designed to match the syntax of Brian 1 as closely as possible, breaking compatibility only when essential. Upgrading scripts from Brian 1 to Brian 2 is therefore usually straightforward. A detailed guide is available in the online documentation at <ext-link ext-link-type="uri" xlink:href="https://brian2.readthedocs.io/en/stable/introduction/brian1_to_2/index.html">https://brian2.readthedocs.io/en/stable/introduction/brian1_to_2/index.html</ext-link>.</p><sec id="s14-1"><title>New features</title><sec id="s14-1-1"><title>Code generation</title><p>The major change from Brian 1 to Brian 2 is that all simulation objects are now based around code generation with behaviour determined by user-specified strings in standard mathematical notation. From these strings, C++ code is generated, compiled and run automatically. Brian 2 can be used in runtime mode (similar to Brian 1 but with individual objects accelerated using code generation), or standalone mode (in which a complete C++ source tree is generated which can be used independently of Brian and Python). Third party packages can extend this support to generate code for different devices, such as GPUs (e.g. <xref ref-type="bibr" rid="bib72">Stimberg et al., 2018</xref>). New features have been added to make it easier to write code that can make use of and extend this code generation system, including extending functions by providing their definitions in a target language, and the <monospace>run_regularly()</monospace> method that covers much of what was previously done with the (still existing) <monospace>@network_operation</monospace> but allowing for code generation.</p></sec><sec id="s14-1-2"><title>Equations</title><p>Brian always allowed users to write equations and differential equations in standard mathematical notation. Stochastic differential equations are now handled in a general way. In Brian 1, only additive noise was allowed and integrated with an Euler scheme. Brian 2 additionally supports multiplicative noise with the Heun and Milstein integration schemes. Numerical integration schemes can be added by the user using a general syntax. Variable time step integration using the GNU Scientific Library was added. Flags can now be added to equations to modify their behaviour (e.g. deactivating specific equations while the neuron is refractory, declaring values to be constant over a time step or run to enable optimisations).</p></sec><sec id="s14-1-3"><title>Neurons</title><p>In addition to the new equations features above, the threshold, reset and refractoriness properties of neurons have now been greatly expanded. In Brian 1, these were handled by custom Python classes and could not easily be combined in complex ways. In Brian 2, each is defined by a string written in standard mathematical notation, determining a condition to evaluate (for threshold or refractoriness) or series of operations to be executed (for reset). Whether or not a neuron is refractory is stored in the (user accessible) <monospace>not_refractory</monospace> variable that is used alongside the <monospace>unless refractory</monospace> flag of the differential equations to switch off dynamics for user selected variables of refractory neurons. This structure allows for much greater flexibility and can be used with code generation.</p></sec><sec id="s14-1-4"><title>Multi-compartmental modelling</title><p>Brian 1 had very basic support for modelling of neurons with a small number of compartments. Brian 2 adds support for detailed morphologies and specific integration schemes, see Appendix 4.</p></sec><sec id="s14-1-5"><title>Synapses</title><p>In the first release of Brian, synaptic connectivity was defined by the Connection class, which only allowed a single weight variable which was added to a target neuron variable when a pre-synaptic neuron fired. Later, a more general <monospace>Synapses</monospace> class was added which greatly expanded the flexibility, but was inefficient due to the lack of comprehensive support for code generation in Brian 1. This <monospace>Synapses</monospace> class is now the only mechanism in Brian 2, generalises the version from Brian 1 and adds code generation support. Synapses allows for a user-specified set of differential equations and parameters (exactly the same as for neurons) along with a specification of what operations should be calculated on the event of a pre- or post-synaptic spike. Multiple pathways with different delays are supported. Synapses can modify pre- or post-synaptic neurons in a discrete or continuous manner (to allow for more complex synapse models or rate-based models). Synapses can also target other synapses (for models of astrocytes for example, <xref ref-type="bibr" rid="bib75">Stimberg et al., 2019c</xref>). Multiple synapses per neuron pair are now supported. Brian 1’s Connection supported defining connectivity by an explicit array, or by specifying full, random, or one-to-one connectivity. Brian 2’s <monospace>Synapses</monospace> generalises these with string-based arguments, and adds support for conditional connectivity (a string based expression determining which pairs to connect) and a generator-based syntax that allows you to write code similar to a for loop but that gets converted into efficient low-level code.</p></sec><sec id="s14-1-6"><title>Events</title><p>In Brian 1, there was only one type of event. A neuron created a spike event if a variable crossed a threshold, and this spike event triggered a reset on the source neuron, as well as synaptic activity. In Brian 2, these events remain but the user can also specify arbitrary events and triggered operations.</p></sec><sec id="s14-1-7"><title>Monitors</title><p>Brian 1 had a large collection of monitors to record different types of activity. These have been replaced by just three generalised versions that record discrete, continuous or population activity.</p></sec><sec id="s14-1-8"><title>Store/restore</title><p>Brian 2 has a new <monospace>store()</monospace> and <monospace>restore()</monospace> mechanism that saves and loads the entire simulation state.</p></sec><sec id="s14-1-9"><title>String-based indexing and evaluation</title><p>Brian 2 allows variables to be indexed by strings as well as numerical indices. For example, writing <monospace>G.z['x &gt; 0’] = ’sin(y)’</monospace> would set the value of variable <inline-formula><mml:math id="inf32"><mml:mi>z</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf34"><mml:mi>y</mml:mi></mml:math></inline-formula> can be a single value or neuron variable), but only for those neurons where the variable <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s14-1-10"><title>Units</title><p>In Brian 1, only scalar values could have physical dimensions. Now arrays can also have units. In addition, consistency of dimensions is now used everywhere.</p></sec><sec id="s14-1-11"><title>Safety</title><p>A number of changes were made to minimise the chance that the user would write code that behaved differently from what was expected. This includes raising an error or warning whenever there is any ambiguity.</p></sec><sec id="s14-1-12"><title>Python 3</title><p>Brian 1 was written only for Python 2. Brian 2 is available for Python 2 and 3.</p></sec><sec id="s14-1-13"><title>Continuous integration</title><p>Brian 2 has a large test suite that is automatically run on multiple versions of Python, operating systems (Linux, Max, Windows), and architectures (32/64 bit). Installation has been improved to make it easier to install, and to ensure that C++ compiler tools are installed to make sure that the most high performance generated code can be used.</p></sec></sec><sec id="s14-2"><title>Removed or replaced features</title><sec id="s14-2-1"><title>Packages</title><p>The aim of Brian 2 was to have a simpler, more flexible core package, and allow separate packages to provide extra functionality. The <monospace>brian.tools</monospace> package which provided some general purpose tools was therefore removed. The <monospace>brian.hears</monospace> package has been updated to <monospace>brian2hears</monospace> provided separately. An updated and generalised version of the <monospace>brian.modelfitting</monospace> (<xref ref-type="bibr" rid="bib67">Rossant et al., 2010</xref>) package is in progress.</p></sec><sec id="s14-2-2"><title>Library</title><p>Brian 1 featured a ‘library’ of models that could be used instead of writing equations explicitly. In line with the design philosophy of Brian 2 described in this paper, this feature was removed. All the equations are listed in the documentation and so Brian 1 models using these features can easily be updated.</p></sec><sec id="s14-2-3"><title>STDP</title><p>Brian 1 had specific classes for STDP models. These are now obsolete as the <monospace>Synapses</monospace> class in Brian 2 covers everything they could do and more. Examples are given in the documentation of how to update code.</p></sec><sec id="s14-2-4"><title>Connection class</title><p>The <monospace>Connection</monospace> class of Brian 1 has been removed in favour of the new <monospace>Synapses</monospace> class (see above).</p></sec></sec></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47314.027</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Skinner</surname><given-names>Frances K</given-names></name><role>Reviewing Editor</role><aff><institution>Krembil Research Institute, University Health Network</institution><country>Canada</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Skinner</surname><given-names>Frances K</given-names></name><role>Reviewer</role><aff><institution>Krembil Research Institute, University Health Network</institution><country>Canada</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Zeldenrust</surname><given-names>Fleur</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib><contrib contrib-type="reviewer"><name><surname>Gerkin</surname><given-names>Richard C</given-names></name><role>Reviewer</role><aff><institution>Arizona State University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Brian 2: an intuitive and efficient neural simulator&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Ronald Calabrese as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Scott Rich – Skinner Lab (Reviewer #1); Fleur Zeldenrust (Reviewer #2); Richard C Gerkin (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The authors present Brian2, a flexible, extensible, simulator for neuroscience (or perhaps any dynamical system). Several features of Brian2 are presented by description, code samples, and case studies, and contrasted with alleged limitations of other approaches to simulation neuroscience. A foremost reason to use it is that it is intuitive and readable. Equations look like equations. This puts Brian in the tradition of tools like XPP that mathematical neuroscientists have relied upon but with the benefits of a modern approach to software development, user experience, etc.</p><p>While all reviewers appreciated this as a valuable, useful and powerful simulator tool for the computational neuroscience community, there were several concerns with the paper in its current form. Three essential revisions are detailed below.</p><p>It is noted that the reviewers encompassed those who are Brian users, casual Brian users, and non-Brian users. This was important in considering this work for <italic>eLife</italic> which is a more general and non-specialized journal.</p><p>Essential revisions:</p><p>1) The authors need to be explicit about the use and advantages of Brian2 over other simulators, so that its advantages (and differences) from other simulators are appreciated and realized. This needs to be presented without assuming that the user is already a Brian user and that it is already obvious that Brian2 may be superior to NEURON, NEST etc. Providing explicit examples and comparisons with other simulators would be useful. That is, explicitly state and show how and why Brian2 makes computational modeling available to a much wider group of researchers than before. For example, starting from some model equations, how would one proceed to use Brian2/python, DynaSim/MATLAB, XPP, NEST, NEURON. GENESIS etc. In this way, the &quot;intuitive and efficient&quot; aspects can be understood by the reader.</p><p>Further, by explicit comparisons and examples, the authors could bring forth why and how Brian2 is easy to use and helps make the researcher focus on model description rather than the intricate choices made in for instance writing ode solvers, which makes it also very useful for student projects. One can write one's equations and run them, so the math underlying the model is pretty clear and easy to work with. Show us why Brian2 is the better than other simulators.</p><p>Further specifics along these lines:</p><p>i) The authors have a tendency to &quot;bury the lead&quot; with regards to the key aspects of Brian 2 that make this simulation environment particularly useful. While this problem is present throughout the text, it is particularly evident in the Introduction. The Introduction reads more like a chronological explanation of the thought process behind Brian 2, rather than an argument for its importance and usefulness: indeed, key points are often lost in the middle of long paragraphs (for instance, the sentence in the Introduction section beginning &quot;Thus, simulators generally have to find a trade-off…&quot; is a key point that is not highlighted by the authors). I would suggest the authors consider reorganizing their Introduction, and to some degree the Discussion, in order to better highlight the key &quot;thesis statements&quot; of their manuscript.</p><p>ii) Throughout the &quot;Design and Implementation&quot; section, in particular with regards to the case studies, the authors make a variety of assertions regarding the ways in which their code improves upon Brian 1 and other simulation environments (NEURON, GENESIS, and NEST are highlighted in the Introduction). However, direct comparisons between Brian 2 and these existing environments are never made (and the areas in which Brian 2 improves over Brian 1 are highlighted sparsely). If, for example, the authors want to make the argument that Brian 2 is superior to its competition in defining &quot;unconventional synapses&quot;, some discussion is necessary regarding why NEURON, GENESIS, NEST, etc. either cannot do what is presented in this paper or would do so in an inferior fashion. This issue remains the case for the multiple times in which the authors assert that Brian 2 is inherently more &quot;flexible&quot; or &quot;readable&quot; when compared to other simulators. For another key example, consider that the authors assert that &quot;capturing all these potential protocols in a single declarative framework is impossible, but it can be easily expressed in a programming language with control structures such as loops and conditionals.&quot; This is a major assertion (as would any assertion that something is &quot;impossible&quot;), and at minimum requires some supporting evidence. Finally, while the presentation of direct code from Brian is very useful, these presentations are used in the manuscript to draw conclusions about this code's superiority to similar code in Brian 1 or other languages. As a reader, I cannot simply take the author's word that this is the case; instead, illustrative example code from the other simulators that perform same or similar tasks should be presented alongside the Brian code so that the reader can draw this conclusion for themselves.</p><p>iii) My main comment about the article is about the comparison with other simulators. This is only done in Figure 8, and this is a bit meager. As a computational (neuro)scientist, every time I want to make a model or simulation, I am faced with a choice of platforms. Which one I choose depends on how difficult the implementation is in the platform for this particular problem (i.e. NEST is good for large simple networks, but not for multi-compartment models, NEURON the other way around), but also on the performance on this platform (not only simulation time but also possibly memory problems). I do not expect a full benchmark of Brian 2 for simulation time and memory use for all possible platforms, but Figure 8 is a bit meager. Especially since for most computational neuroscientists the choice will not be between C++ and Brian, but between Brian and Nest, DynaSim/Matlab, Neuron, etc. So I think it would be nice if the article included some comparisons between these. Moreover, DynaSim (Sherfey et al., 2018) is not mentioned at all, even though the authors profile it as 'the Brian in Matlab', more or less. So I believe it should at least be discussed.</p><p>iv) There are several claims about the limitations of other approaches to modeling, especially a) the inability for these approaches to describe and run a simulation experiment protocol on a model and b) the inability to make equations explicit. This was especially noted as a weakness of declarative approaches to modeling (e.g. NeuroML/NineML). However, in NeuroML2, which has been in use for a number of years, both of these are possible. Perhaps some would find the experience of doing these through Brian 2 to be easier, but it is misleading to say that they cannot be done with declarative modeling approaches.</p><p>2) The authors need to be explicit about the intended audience and underlying assumptions regarding Brian 2. That is, in making the claim about Brian 2 being intuitive, generalizable etc., there would seem to be an assumption that the user is already a python expert? Similar to the point above, the authors need to provide guidance to a potential starting user.</p><p>Explicit comparison to other simulators, from ground zero, would be helpful.</p><p>For example, starting from some model equations, one could learn C/C++ code, or learn python to use Brian 2, or hoc to use NEURON, or MATLAB to use DynaSim etc. In doing this, the authors could bring about where Brian 2 could/should be used in comparison to other simulators. For example, NEURON is presumably superior to Brian 2 for multi-compartment models? Etc.</p><p>Again, the authors need to be explicit and clear about underlying and starting assumptions, so that the general reader/user can appreciate and understand Brian 2 to consider its usage.</p><p>Further specifics:</p><p>i) The intended audience for this article, and the corresponding style of writing, is often unclear or varies from section to section. Given the interdisciplinary audience of <italic>eLife</italic>, and the desire for this software to be used by a wide range of computational neuroscientists, I would imagine it is most desirable for this article to be accessible to readers that may not be true &quot;experts&quot; in computer science, or individuals for whom this wasn't their primary training. For this to be the case, more care needs to be taken in clearly defining some of the terminology that is used. One key example is the consistent use of the terminology &quot;high-level&quot; versus &quot;low-level&quot; with no clear definition as to what this means in the context of the paper. More importantly, one of the key features of Brian 2 appears to be the &quot;technique of code generation&quot;, but there is no clear point at which this terminology is explicitly defined in a manner accessible to people not familiar with the concept.</p><p>ii) The interaction between Python and Brian 2 appears to be paramount to the argument made by the authors that Brian 2 represents a clear &quot;step forward&quot; for these type of simulation environments. However, in making this argument the authors make two major assumptions: A) anyone who will make use of Brian 2 will be inherently familiar with Python; B) the integration of Brian 2 and Python is what makes this simulation environment a major &quot;step forward&quot;. However, assumption A is not necessarily the case (I, for example, have not used Python in my computational neuroscience research up to this point), and for those who fall under this category it is not at all clear that Brian 2 is more &quot;readable&quot; or &quot;usable&quot; than other environments (thus making assumption B questionable as well). These issues came forth in a couple of different fashions in my explorations of the Brian 2 code. A major pitfall was quite literally immediately apparent, as the online documentation regarding both Brian 2 and the &quot;Jupyter Notebooks&quot; used to demonstrate it was very much lacking; for example, nowhere in the documentation is there an explicit explanation of how to run a piece of Brian 2 code, or open a Jupyter Notebook. While this might be trivial to someone with an extensive background with Python, it is most certainly not for someone without this background. If the authors wish to assert that Brian 2 is inherently more &quot;useable&quot; than other simulation environments, these are things that can't be assumed, but must be explicitly stated, in the documentation; indeed, improved &quot;usability&quot; implies that this will be the case for all qualified computational neuroscientists, not just those with a particular background or skillset. As I went further with my exploration with Brian 2, I found many additional instances of terminology, code syntax, and other aspects of the programs that were not clear to me, and not explicitly explained or documented anywhere either in the Jupyter Notebooks or in the online documentation. All of this led me to the following conclusion: in order to make full use of Brian 2, I'd have to spend a significant period of time learning the underlying syntax and code structure, and it isn't clear to me that the time I would take to do this would be any less arduous or more streamlined than when I did similar things to use C code or NEURON.</p><p>3) Emphasis is placed on the performance of Brian 2 and its code-generation capabilities. While true high-performance computing simulations are beyond the scope of Brian 2, the embarrassingly parallel case should be trivial to handle (as noted by the authors), and GPU support is noted. However, I could find no examples of using GPUs in the online Brian 2 documentation. The authors should include examples in the documentation (and in supplementary materials) where hundreds of simulations of the same model (e.g. Izhikevich) are executed with different sets of parameters in parallel on the GPU (and dumps out the resulting waveforms).</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47314.028</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The authors need to be explicit about the use and advantages of Brian2 over other simulators, so that its advantages (and differences) from other simulators are appreciated and realized. This needs to be presented without assuming that the user is already a Brian user and that it is already obvious that Brian2 may be superior to NEURON, NEST etc. Providing explicit examples and comparisons with other simulators would be useful. That is, explicitly state and show how and why Brian2 makes computational modeling available to a much wider group of researchers than before. For example, starting from some model equations, how would one proceed to use Brian2/python, DynaSim/MATLAB, XPP, NEST, NEURON. GENESIS etc. In this way, the &quot;intuitive and efficient&quot; aspects can be understood by the reader.</p><p>Further, by explicit comparisons and examples, the authors could bring forth why and how Brian2 is easy to use and helps make the researcher focus on model description rather than the intricate choices made in for instance writing ode solvers, which makes it also very useful for student projects. One can write one's equations and run them, so the math underlying the model is pretty clear and easy to work with. Show us why Brian2 is the better than other simulators.</p></disp-quote><p>We agree with the reviewers that some comparison of the modelling process with Brian compared to other simulators would be helpful for readers, and we have therefore added comparisons to other simulators. For the first case study (pyloric network), we have provided excerpts of equivalent code for the graded synapse model (in Appendix 3), in C++, NeuroML2/LEMS, and NEURON/NMODL, which we have taken from the literature. It cannot be implemented with NESTML – it could certainly be integrated in NEST, but not without detailed knowledge of the simulator codebase (i.e., it could be done by a NEST developer). In order to keep the paper to a reasonable length we restricted our comparisons to these simulators and languages, as a comprehensive approach for all simulators and case studies would make a detailed review paper in itself, and would be difficult without involving the authors of simulators. In addition to this detailed code comparison, we added a discussion of other simulators at the end of the text for each case study (focusing on the same set of simulators).</p><disp-quote content-type="editor-comment"><p>Further specifics along these lines:</p><p>i) The authors have a tendency to &quot;bury the lead&quot; with regards to the key aspects of Brian 2 that make this simulation environment particularly useful. While this problem is present throughout the text, it is particularly evident in the Introduction. The Introduction reads more like a chronological explanation of the thought process behind Brian 2, rather than an argument for its importance and usefulness: indeed, key points are often lost in the middle of long paragraphs (for instance, the sentence in the Introduction section beginning &quot;Thus, simulators generally have to find a trade-off…&quot; is a key point that is not highlighted by the authors). I would suggest the authors consider reorganizing their Introduction, and to some degree the Discussion, in order to better highlight the key &quot;thesis statements&quot; of their manuscript.</p></disp-quote><p>We have rewritten parts of the Abstract, Introduction, design and implementation, and Discussion to address this point.</p><disp-quote content-type="editor-comment"><p>ii) Throughout the &quot;Design and Implementation&quot; section, in particular with regards to the case studies, the authors make a variety of assertions regarding the ways in which their code improves upon Brian 1 and other simulation environments (NEURON, GENESIS, and NEST are highlighted in the Introduction). However, direct comparisons between Brian 2 and these existing environments are never made (and the areas in which Brian 2 improves over Brian 1 are highlighted sparsely).</p></disp-quote><p>We have added these direct comparisons as discussed above. We also added a new appendix listing the key improvements of Brian 2 over Brian 1 (Appendix 5).</p><disp-quote content-type="editor-comment"><p>If, for example, the authors want to make the argument that Brian 2 is superior to its competition in defining &quot;unconventional synapses&quot;, some discussion is necessary regarding why NEURON, GENESIS, NEST, etc. either cannot do what is presented in this paper or would do so in an inferior fashion.</p></disp-quote><p>As discussed above, we have added explicit comparisons of code for different simulators in Appendix 3. We also discuss it at the end of the presentation of case study 1.</p><disp-quote content-type="editor-comment"><p>This issue remains the case for the multiple times in which the authors assert that Brian 2 is inherently more &quot;flexible&quot; or &quot;readable&quot; when compared to other simulators. For another key example, consider that the authors assert that &quot;capturing all these potential protocols in a single declarative framework is impossible, but it can be easily expressed in a programming language with control structures such as loops and conditionals.&quot; This is a major assertion (as would any assertion that something is &quot;impossible&quot;), and at minimum requires some supporting evidence.</p></disp-quote><p>This is rather a theoretical argument. If the language is powerful enough to express anything computable, then by definition it is a general-purpose programming language, and not a domain-specific declarative language. Therefore, to the extent that a declarative language (such as NeuroML) is not fully general (i.e. not Turing complete), there are protocols that it cannot express (a protocol being any kind of procedure for manipulating the model than can be algorithmically described). We have reworded this claim to make this argument clearer.</p><disp-quote content-type="editor-comment"><p>Finally, while the presentation of direct code from Brian is very useful, these presentations are used in the manuscript to draw conclusions about this code's superiority to similar code in Brian 1 or other languages. As a reader, I cannot simply take the author's word that this is the case; instead, illustrative example code from the other simulators that perform same or similar tasks should be presented alongside the Brian code so that the reader can draw this conclusion for themselves.</p></disp-quote><p>As discussed previously, we have added some explicit examples for the first case study in Appendix 3. Note that one of the motivations to show the Brian 2 code for each case study was to demonstrate that it is possible to fit the complete code in a single page, which is impossible for any other simulator that we have looked at.</p><disp-quote content-type="editor-comment"><p>iii) My main comment about the article is about the comparison with other simulators. This is only done in Figure 8, and this is a bit meager. As a computational (neuro)scientist, every time I want to make a model or simulation, I am faced with a choice of platforms. Which one I choose depends on how difficult the implementation is in the platform for this particular problem (i.e. NEST is good for large simple networks, but not for multi-compartment models, NEURON the other way around), but also on the performance on this platform (not only simulation time but also possibly memory problems). I do not expect a full benchmark of Brian 2 for simulation time and memory use for all possible platforms, but Figure 8 is a bit meager. Especially since for most computational neuroscientists the choice will not be between C++ and Brian, but between Brian and Nest, DynaSim/Matlab, Neuron, etc. So I think it would be nice if the article included some comparisons between these.</p></disp-quote><p>We have added comparisons to other simulators in terms of syntax (see points above), and added performance comparisons with Brian 1, NEST, and NEURON to Figure 8. This is a complex issue because it is very hard to do a fair performance comparison of different simulators without 1) testing a wide range of different models and 2) involving the original authors from each simulator being compared, to make sure that the comparison is fair. To do this properly would have to be the subject of its own detailed review paper. From our perspective, we have seen that a number of published papers that compare simulators made choices in their Brian implementations that dramatically affect performance. For example, in Tikidji-Hamburyan et al., 2017, which is mostly very well done, they did not use the standalone mode of Brian 2, which for their examples would have made the code run several times faster. We assume that similar issues are present for other simulators and that these results should therefore be interpreted with caution. While we are able to write our own examples to make sure they run efficiently in Brian, we could not guarantee that code we would write for other simulators was as efficient as it could possibly be. For this reason, we decided in our initial submission to include no performance comparisons with other simulators.</p><p>However, we do agree with the reviewers that readers are likely to be interested in such comparisons, and we have therefore done our best to add some to Figure 8 (for Neuron and NEST), with detailed explanations in the discussion, while also noting the limitations of these benchmarks. For Neuron and NEST we based our code on the reference scripts provided by the Neuron and NEST teams for Brette et al., 2007, as well as on examples from their documentation. We note that for these examples, Brian running in standalone mode compares very favourably to both Neuron and NEST, but as this is only one example, we do not wish to make a strong claim that Brian is more efficient.</p><p>We have also added some text about the limitations of Brian 2 relative to other simulators in the discussion. Specifically, Brian 2 is not designed to run over multiple machines and therefore is not adapted to the simulation of very large models, unlike NEST. There is some support for multicompartmental modelling in Brian 2 (see below), but clearly this is a more specific strength of NEURON and GENESIS.</p><disp-quote content-type="editor-comment"><p>Moreover, DynaSim (Sherfey et al., 2018) is not mentioned at all, even though the authors profile it as 'the Brian in Matlab', more or less. So I believe it should at least be discussed.</p></disp-quote><p>We have added this to our Discussion.</p><disp-quote content-type="editor-comment"><p>iv) There are several claims about the limitations of other approaches to modeling, especially a) the inability for these approaches to describe and run a simulation experiment protocol on a model and b) the inability to make equations explicit. This was especially noted as a weakness of declarative approaches to modeling (e.g. NeuroML/NineML). However, in NeuroML2, which has been in use for a number of years, both of these are possible. Perhaps some would find the experience of doing these through Brian 2 to be easier, but it is misleading to say that they cannot be done with declarative modeling approaches.</p></disp-quote><p>We do not claim that NeuroML and NineML are unable to make equations explicit, only that it cannot represent arbitrary simulation experiment protocols (as discussed above). As far as we can tell, neither NeuroML nor NineML include a mechanism for changing the parameters for subsequent simulation runs based on the outcome of the initial runs, for example.</p><disp-quote content-type="editor-comment"><p>2) The authors need to be explicit about the intended audience and underlying assumptions regarding Brian 2. That is, in making the claim about Brian 2 being intuitive, generalizable etc., there would seem to be an assumption that the user is already a python expert? Similar to the point above, the authors need to provide guidance to a potential starting user.</p><p>Explicit comparison to other simulators, from ground zero, would be helpful.</p><p>For example, starting from some model equations, one could learn C/C++ code, or learn python to use Brian 2, or hoc to use NEURON, or MATLAB to use DynaSim etc. In doing this, the authors could bring about where Brian 2 could/should be used in comparison to other simulators. For example, NEURON is presumably superior to Brian 2 for multi-compartment models? Etc.</p><p>Again, the authors need to be explicit and clear about underlying and starting assumptions, so that the general reader/user can appreciate and understand Brian 2 to consider its usage.</p></disp-quote><p>We have added comparisons to other approaches and multi-compartmental modelling throughout the article and described these changes in our other responses to the reviewers.</p><p>On the specific issue of programming language choice, it is correct that some basic knowledge of Python is required to use Brian 2 (not expert; it is not necessary, for example, to know about object-oriented programming). All simulators require the user to know a language, at least for scripting. In fact, all the major simulators have now chosen to provide Python bindings, because it has become a standard language for scientific computing (MATLAB is indeed another major scientific language).</p><p>Therefore, the use of Python is not what distinguishes Brian 2 or makes it particularly intuitive. Our point is rather that the user is not required to know much more than this general language. Indeed, all other major simulators use an additional language to describe models; for example, to use NEURON one needs to master the scripting language (either hoc or Python) and in addition the domain language NMODL to describe ionic channel properties. In Brian 2, all models are defined within the script by their mathematical equations. Another very important aspect is that the user is not required to know how the models must be simulated (e.g. numerical integration), unlike when writing code in C++.</p><p>We have tried to clarify this point at the beginning of Design and Implementation.</p><disp-quote content-type="editor-comment"><p>Further specifics:</p><p>i) The intended audience for this article, and the corresponding style of writing, is often unclear or varies from section to section. Given the interdisciplinary audience of eLife, and the desire for this software to be used by a wide range of computational neuroscientists, I would imagine it is most desirable for this article to be accessible to readers that may not be true &quot;experts&quot; in computer science, or individuals for whom this wasn't their primary training. For this to be the case, more care needs to be taken in clearly defining some of the terminology that is used. One key example is the consistent use of the terminology &quot;high-level&quot; versus &quot;low-level&quot; with no clear definition as to what this means in the context of the paper. More importantly, one of the key features of Brian 2 appears to be the &quot;technique of code generation&quot;, but there is no clear point at which this terminology is explicitly defined in a manner accessible to people not familiar with the concept.</p></disp-quote><p>We have added a new paragraph defining high- and low-level at the beginning of Design and Implementation, and expanded the definition of code generation in the Introduction.</p><disp-quote content-type="editor-comment"><p>ii) The interaction between Python and Brian 2 appears to be paramount to the argument made by the authors that Brian 2 represents a clear &quot;step forward&quot; for these type of simulation environments. However, in making this argument the authors make two major assumptions: A) anyone who will make use of Brian 2 will be inherently familiar with Python; B) the integration of Brian 2 and Python is what makes this simulation environment a major &quot;step forward&quot;. However, assumption A is not necessarily the case (I, for example, have not used Python in my computational neuroscience research up to this point), and for those who fall under this category it is not at all clear that Brian 2 is more &quot;readable&quot; or &quot;usable&quot; than other environments (thus making assumption B questionable as well). These issues came forth in a couple of different fashions in my explorations of the Brian 2 code. A major pitfall was quite literally immediately apparent, as the online documentation regarding both Brian 2 and the &quot;Jupyter Notebooks&quot; used to demonstrate it was very much lacking; for example, nowhere in the documentation is there an explicit explanation of how to run a piece of Brian 2 code, or open a Jupyter Notebook. While this might be trivial to someone with an extensive background with Python, it is most certainly not for someone without this background. If the authors wish to assert that Brian 2 is inherently more &quot;useable&quot; than other simulation environments, these are things that can't be assumed, but must be explicitly stated, in the documentation; indeed, improved &quot;usability&quot; implies that this will be the case for all qualified computational neuroscientists, not just those with a particular background or skillset. As I went further with my exploration with Brian 2, I found many additional instances of terminology, code syntax, and other aspects of the programs that were not clear to me, and not explicitly explained or documented anywhere either in the Jupyter Notebooks or in the online documentation. All of this led me to the following conclusion: in order to make full use of Brian 2, I'd have to spend a significant period of time learning the underlying syntax and code structure, and it isn't clear to me that the time I would take to do this would be any less arduous or more streamlined than when I did similar things to use C code or NEURON.</p></disp-quote><p>As explained above, the fact that Brian 2 uses Python rather than another language is not the main reason why Brian 2 is, in our view, more intuitive than other simulators. One of the main reasons is that the user is essentially required to know <italic>only</italic> Python. More specifically: 1) there is no difference between scripting language and model description language, unlike NEURON and NEST, and 2) the user is not required to specify the detailed implementation of the models, unlike with C/C++.</p><p>Regarding the first point, both NEURON and NEST now recommend using their Python scripting interfaces for new users rather than their own domain specific languages (HOC and SLI), because (a) it makes no sense for a new user to use a domain specific language if the same functionality is available in a general purpose language with good library support, development environments and debugging tools, and also (b) following a general trend in computational science towards the use of Python. However, models cannot be defined at the level of the Python script with either NEURON (which requires NMODL) or NEST (which requires NESTML, or C++). For the case of NEURON code, it seems unarguable that from the point of view of either a naïve or highly experienced user in all the languages used, a single Python file with model definitions (in standard mathematical notation), simulation control and analysis in one file and one language will be more readable than one written in two or three different languages (NMODL for model definition, HOC or Python for simulation control and perhaps Matlab for analysis).</p><p>Regarding the second point, to write your own custom C code for simulations in not only tedious, but it is also a dangerous practice in terms of reliability and reproducibility (see e.g. (Pauli et al., 2018) for a recent case study).</p><p>Finally on this point, we want to thank the reviewer for highlighting the lack of a good “quick start” guide for non-Python users in the Brian documentation, which we have now corrected (<ext-link ext-link-type="uri" xlink:href="https://brian2.readthedocs.io/en/latest/introduction/scripts.html">https://brian2.readthedocs.io/en/latest/introduction/scripts.html</ext-link>). However, to say that the statement “the online documentation … was very much lacking” based solely on the fact that we didn’t have a quick-start guide for non-Python users seems unreasonable. Firstly, in all of the thousands of messages to our email support group and conversations with users in person, this is the first time anyone has mentioned this as a problem, and good quick-start guides to Python and Jupyter exist online. Secondly, the documentation has been an enormous effort and is often mentioned to us by users as one of the main reasons that they learned Brian or switched to it from other simulators. It is extremely comprehensive, with tutorials, examples, a user guide, advanced guide and detailed reference documentation on every single feature, all cross-referenced, amounting to over 700 pages in printed form. In addition, in the online documentation, every single tutorial and example has a link allowing you to launch it, edit and run it in your browser without having to install anything locally, using the Binder service.</p><p>We have added a new paragraph at the beginning of Design and Implementation discussing the choice of programming language.</p><disp-quote content-type="editor-comment"><p>3) Emphasis is placed on the performance of Brian 2 and its code-generation capabilities. While true high-performance computing simulations are beyond the scope of Brian 2, the embarrassingly parallel case should be trivial to handle (as noted by the authors), and GPU support is noted. However, I could find no examples of using GPUs in the online Brian 2 documentation. The authors should include examples in the documentation (and in supplementary materials) where hundreds of simulations of the same model (e.g. Izhikevich) are executed with different sets of parameters in parallel on the GPU (and dumps out the resulting waveforms).</p></disp-quote><p>We have added an additional simulation to the Appendix (Appendix 4—figure 2), where we show code for the implementation of a parameter exploration. The example runs on GPU using Brian2GeNN, requiring only a single additional line of code (set_device(‘genn’)). GPU support for Brian is provided by external libraries and is not in the core of Brian 2, and we feel that a detailed discussion would be out of scope for this manuscript. We have therefore only added a brief comment on this in the Discussion.</p></body></sub-article></article>