<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">48810</article-id><article-id pub-id-type="doi">10.7554/eLife.48810</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Balancing model-based and memory-free action selection under competitive pressure</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-114062"><name><surname>Kikumoto</surname><given-names>Atsushi</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2179-2700</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-101331"><name><surname>Mayr</surname><given-names>Ulrich</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7512-4556</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Psychology</institution>, <institution>University of Oregon</institution>, <addr-line><named-content content-type="city">Eugene</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-10628"><name><surname>Lee</surname><given-names>Daeyeol</given-names></name><role>Reviewing editor</role><aff><institution>Johns Hopkins University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>mayr@uoregon.edu</email> (UM);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>02</day><month>10</month><year>2019</year></pub-date><volume>8</volume><elocation-id>e48810</elocation-id><history><date date-type="received"><day>26</day><month>05</month><year>2019</year></date><date date-type="accepted"><day>01</day><month>10</month><year>2019</year></date></history><permissions><copyright-statement>Â© 2019, Kikumoto &amp; Mayr</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Kikumoto &amp; Mayr</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-48810-v1.pdf"/><abstract><p>In competitive situations, winning depends on selecting actions that surprise the opponent. Such unpredictable action can be generated based on representations of the opponent's strategy and choice history (model-based counter-prediction) or by choosing actions in a memory-free, stochastic manner. Across five different experiments using a variant of a matching-pennies game with simulated and human opponents we found that people toggle between these two strategies, using model-based selection when recent wins signal the appropriateness of the current model, but reverting to stochastic selection following losses. Also, after wins, feedback-related, mid-frontal EEG activity reflected information about the opponent's global and local strategy, and predicted upcoming choices. After losses, this activity was nearly absent-indicating that the internal model is suppressed after negative feedback. We suggest that the mixed-strategy approach allows negotiating two conflicting goals: (1) exploiting the opponent's deviations from randomness while (2) remaining unpredictable for the opponent.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 AG037564- 01A1</award-id><principal-award-recipient><name><surname>Mayr</surname><given-names>Ulrich</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The entire study protocol and consent forms were approved by the University of Oregon's Human Subjects Review Board (Protocol 10272010.016).</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Data and analyses are available through OSF (https://osf.io/j6beq/).  Specifically, the repository contains for each of the five experiments, all trial-by-trial data files, as well as R codes to conduct the reported analyses.  For Experiment 5, we also include all relevant EEG data and analyses codes.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Kikumoto A</collab><collab>Mayr U</collab></person-group><year iso-8601-date="2019">2019</year><source>Balancing model-based and memory-free action selection under competitive pressure</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/j6beq/">https://osf.io/j6beq/</ext-link><comment>Open Science Foundation, https://osf.io</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-48810-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>