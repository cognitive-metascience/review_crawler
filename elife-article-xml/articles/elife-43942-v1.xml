<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">43942</article-id><article-id pub-id-type="doi">10.7554/eLife.43942</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Unimodal statistical learning produces multimodal object-like representations</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-126362"><name><surname>Lengyel</surname><given-names>Gábor</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1535-3250</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-127314"><name><surname>Žalalytė</surname><given-names>Goda</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0012-9950</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-127315"><name><surname>Pantelides</surname><given-names>Alexandros</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6234-6061</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-48694"><name><surname>Ingram</surname><given-names>James Neilson</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-112596"><name><surname>Fiser</surname><given-names>József</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-31486"><name><surname>Lengyel</surname><given-names>Máté</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor3">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-3698"><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2011-2790</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor4">*</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Cognitive Science</institution>, <institution>Central European University</institution>, <addr-line><named-content content-type="city">Budapest</named-content></addr-line>, <country>Hungary</country></aff><aff id="aff2"><institution content-type="dept">Computational and Biological Learning Lab, Department of Engineering</institution>, <institution>University of Cambridge</institution>, <addr-line><named-content content-type="city">Cambridge</named-content></addr-line>, <country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-4470"><name><surname>Diedrichsen</surname><given-names>Jörn</given-names></name><role>Reviewing editor</role><aff><institution>University of Western Ontario</institution>, <country>Canada</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>lengyel.gaabor@gmail.com</email> (GL);</corresp><corresp id="cor2"><label>*</label>For correspondence: <email>fiserj@ceu.edu</email> (JF);</corresp><corresp id="cor3"><label>*</label>For correspondence: <email>m.lengyel@eng.cam.ac.uk</email> (ML);</corresp><corresp id="cor4"><label>*</label>For correspondence: <email>wolpert@eng.cam.ac.uk</email> (DW);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>01</day><month>05</month><year>2019</year></pub-date><volume>8</volume><elocation-id>e43942</elocation-id><history><date date-type="received"><day>27</day><month>11</month><year>2018</year></date><date date-type="accepted"><day>30</day><month>04</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Lengyel et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Lengyel et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-43942-v1.pdf"/><abstract><p>The concept of objects is fundamental to cognition and is defined by a consistent set of sensory properties and physical affordances. Although it is unknown how the abstract concept of an object emerges, most accounts assume that visual or haptic boundaries are crucial in this process. Here, we tested an alternative hypothesis that boundaries are not essential but simply reflect a more fundamental principle: consistent visual or haptic statistical properties. Using a novel visuo-haptic statistical learning paradigm, we familiarised participants with objects defined solely by across-scene statistics provided either visually or through physical interactions. We then tested them on both a visual familiarity and a haptic pulling task, thus measuring both within-modality learning and across-modality generalisation. Participants showed strong within-modality learning and 'zero-shot' across-modality generalisation which were highly correlated. Our results demonstrate that humans can segment scenes into objects, without any explicit boundary cues, using purely statistical information.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>ERC Consolidator Grant</institution></institution-wrap></funding-source><award-id>ERC-2016-COG/726090</award-id><principal-award-recipient><name><surname>Lengyel</surname><given-names>Máté</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>Royal Society Noreen Murray Professorship in Neurobiolog</institution></institution-wrap></funding-source><award-id>RP120142</award-id><principal-award-recipient><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>EU-FP7 Marie Curie CIG</institution></institution-wrap></funding-source><award-id>CIG 618918</award-id><principal-award-recipient><name><surname>Fiser</surname><given-names>József</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution>Wellcome Trust: New Investigator Award</institution></institution-wrap></funding-source><award-id>095621/Z/11/Z</award-id><principal-award-recipient><name><surname>Lengyel</surname><given-names>Máté</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NIH R21 HD088731</award-id><principal-award-recipient><name><surname>Fiser</surname><given-names>József</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution>Wellcome Trust: Senior Investigator Award</institution></institution-wrap></funding-source><award-id>097803/Z/11/Z</award-id><principal-award-recipient><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants gave informed consent. All experimental protocols were approved by the University of Cambridge Psychology Ethics Committee.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>The scripts for all of the analysis reported in the manuscript can be found here https://github.com/GaborLengyel/Visual-Haptic-Statistical-Learning. There is a README file that explains both where the data can be found (Open Science Framework https://osf.io/456qb/) and how to run the analysis.</p></sec><supplementary-material><ext-link xlink:href="elife-43942-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>