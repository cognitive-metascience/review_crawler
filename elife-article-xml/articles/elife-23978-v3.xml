<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">23978</article-id><article-id pub-id-type="doi">10.7554/eLife.23978</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Attentional modulation of neuronal variability in circuit models of cortex</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-77059"><name><surname>Kanashiro</surname><given-names>Tatjana</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-77060"><name><surname>Ocker</surname><given-names>Gabriel Koch</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-12185"><name><surname>Cohen</surname><given-names>Marlene R</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8583-4300</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-22448"><name><surname>Doiron</surname><given-names>Brent</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6916-5511</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Program for Neural Computation</institution>, <institution>Carnegie Mellon University and University of Pittsburgh</institution>, <addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Mathematics</institution>, <institution>University of Pittsburgh</institution>, <addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><label>3</label><institution>Center for the Neural Basis of Cognition</institution>, <addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><label>4</label><institution>Allen Institute for Brain Science</institution>, <addr-line><named-content content-type="city">Seattle</named-content></addr-line>, <country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Department of Neuroscience</institution>, <institution>University of Pittsburgh</institution>, <addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing editor</role><aff><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>bdoiron@pitt.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>07</day><month>06</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e23978</elocation-id><history><date date-type="received"><day>08</day><month>12</month><year>2016</year></date><date date-type="accepted"><day>20</day><month>05</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Kanashiro et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Kanashiro et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-23978-v3.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.23978.001</object-id><p>The circuit mechanisms behind shared neural variability (noise correlation) and its dependence on neural state are poorly understood. Visual attention is well-suited to constrain cortical models of response variability because attention both increases firing rates and their stimulus sensitivity, as well as decreases noise correlations. We provide a novel analysis of population recordings in rhesus primate visual area V4 showing that a single biophysical mechanism may underlie these diverse neural correlates of attention. We explore model cortical networks where top-down mediated increases in excitability, distributed across excitatory and inhibitory targets, capture the key neuronal correlates of attention. Our models predict that top-down signals primarily affect inhibitory neurons, whereas excitatory neurons are more sensitive to stimulus specific bottom-up inputs. Accounting for trial variability in models of state dependent modulation of neuronal activity is a critical step in building a mechanistic theory of neuronal cognition.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.001">http://dx.doi.org/10.7554/eLife.23978.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.23978.002</object-id><title>eLife digest</title><p>The world around us is complex and our brains need to navigate this complexity. We must focus on relevant inputs from our senses – such as the bus we need to catch – while ignoring distractions – such as the eye-catching displays in the shop windows we pass on the same street. Selective attention is a tool that enables us to filter complex sensory scenes and focus on whatever is most important at the time. But how does selective attention work?</p><p>Our sense of vision results from the activity of cells in a region of the brain called visual cortex. Paying attention to an object affects the activity of visual cortex in two ways. First, it causes the average activity of the brain cells in the visual cortex that respond to that object to increase. Second, it reduces spontaneous moment-to-moment fluctuations in the activity of those brain cells, known as noise. Both of these effects make it easier for the brain to process the object in question.</p><p>Kanashiro et al. set out to build a mathematical model of visual cortex that captures these two components of selective attention. The cortex contains two types of brain cells: excitatory neurons, which activate other cells, and inhibitory neurons, which suppress other cells. Experiments suggest that excitatory neurons contribute to the flow of activity within the cortex, whereas inhibitory neurons help cancel out noise. The new mathematical model predicts that paying attention affects inhibitory neurons far more than excitatory ones. According to the model, selective attention works mainly by reducing the noise that would otherwise distort the activity of visual cortex.</p><p>The next step is to test this prediction directly. This will require measuring the activity of the inhibitory neurons in an animal performing a selective attention task. Such experiments, which should be achievable using existing technologies, will allow scientists to confirm or disprove the current model, and to dissect the mechanisms that underlie visual attention.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.002">http://dx.doi.org/10.7554/eLife.23978.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>noise correlations</kwd><kwd>inhibitory feedback</kwd><kwd>neural correlates of attention</kwd><kwd>mean field model</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DMS-1313225</award-id><principal-award-recipient><name><surname>Kanashiro</surname><given-names>Tatjana</given-names></name><name><surname>Ocker</surname><given-names>Gabriel Koch</given-names></name><name><surname>Doiron</surname><given-names>Brent</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DMS-1517082</award-id><principal-award-recipient><name><surname>Ocker</surname><given-names>Gabriel Koch</given-names></name><name><surname>Doiron</surname><given-names>Brent</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>Simons Collaboration on the Global Brain</award-id><principal-award-recipient><name><surname>Cohen</surname><given-names>Marlene R</given-names></name><name><surname>Doiron</surname><given-names>Brent</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 EY022930</award-id><principal-award-recipient><name><surname>Cohen</surname><given-names>Marlene R</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>CRCNS-R01DC015139</award-id><principal-award-recipient><name><surname>Doiron</surname><given-names>Brent</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Attention reduces noise correlations through enhancing inhibitory feedback in cortical networks.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The behavioral state of the brain exerts a powerful influence on the cortical responses. For example, electrophysiological recordings from both rodents and primates show that the level of wakefulness (<xref ref-type="bibr" rid="bib67">Steriade et al., 1993</xref>), active sensory exploration (<xref ref-type="bibr" rid="bib10">Crochet et al., 2011</xref>), and attentional focus (<xref ref-type="bibr" rid="bib71">Treue, 2001</xref>; <xref ref-type="bibr" rid="bib58">Reynolds and Chelazzi, 2004</xref>; <xref ref-type="bibr" rid="bib23">Gilbert and Sigman, 2007</xref>; <xref ref-type="bibr" rid="bib45">Moore and Zirnsak, 2017</xref>) all modulate synaptic and spiking activity. Despite the diversity of behavioral contexts, in all of these cases an overall elevation and desynchronization of cortical activity accompanies heightened states of processing (<xref ref-type="bibr" rid="bib25">Harris and Thiele, 2011</xref>). Exploration of the neuronal mechanisms that underly such state changes has primarily centered around how various neuromodulators shift the cellular and synaptic properties of cortical circuits (<xref ref-type="bibr" rid="bib26">Hasselmo, 1995</xref>; <xref ref-type="bibr" rid="bib37">Lee and Dan, 2012</xref>; <xref ref-type="bibr" rid="bib48">Noudoost and Moore, 2011</xref>; <xref ref-type="bibr" rid="bib45">Moore and Zirnsak, 2017</xref>) However, a coherent theory linking the modulation of cortical circuits to an active desynchronization of population activity is lacking. In this study we provide a circuit-based theory for the known attention-guided modulations of neuronal activity in the visual cortex of primates performing a stimulus change detection task.</p><p>The investigation of the neuronal correlates of attention has a rich history. Attention increases the firing rates of neurons engaged in feature- and spatial-based processing tasks (<xref ref-type="bibr" rid="bib41">McAdams and Maunsell, 2000</xref>; <xref ref-type="bibr" rid="bib57">Reynolds et al., 1999</xref>). Attentional modulation of the stimulus-response sensitivity (gain) of firing rates is more complicated, often depending on stimulus specifics such as the size and contrast of a visual image (<xref ref-type="bibr" rid="bib75">Williford and Maunsell, 2006</xref>; <xref ref-type="bibr" rid="bib59">Reynolds and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib64">Sanayei et al., 2015</xref>). In recent years there has been increased focus on how brain states affect trial-to-trial spiking variability (<xref ref-type="bibr" rid="bib10">Crochet et al., 2011</xref>; <xref ref-type="bibr" rid="bib38">Lin et al., 2015</xref>; <xref ref-type="bibr" rid="bib15">Doiron et al., 2016</xref>; <xref ref-type="bibr" rid="bib68">Stringer et al., 2016</xref>). In particular, attention decreases the shared variability (noise correlations) of the firing rates from pairs of neurons (<xref ref-type="bibr" rid="bib8">Cohen and Maunsell, 2009</xref>; <xref ref-type="bibr" rid="bib43">Mitchell et al., 2009</xref>; <xref ref-type="bibr" rid="bib9">Cohen and Maunsell, 2011</xref>; <xref ref-type="bibr" rid="bib28">Herrero et al., 2013</xref>; <xref ref-type="bibr" rid="bib63">Ruff and Cohen, 2014</xref>; <xref ref-type="bibr" rid="bib19">Engel et al., 2016</xref>). The combination of a reduction in noise correlations and an increase in response gain has potentially important functional consequences through an improved population code (<xref ref-type="bibr" rid="bib8">Cohen and Maunsell, 2009</xref>; <xref ref-type="bibr" rid="bib55">Rabinowitz et al., 2015</xref>). In total, there is an emerging picture of the impact of attention on the trial-averaged and trial-variable spiking dynamics of cortical populations.</p><p>Phenomenological models of attentional modulation have been popular (<xref ref-type="bibr" rid="bib59">Reynolds and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib47">Navalpakkam and Itti, 2005</xref>; <xref ref-type="bibr" rid="bib23">Gilbert and Sigman, 2007</xref>; <xref ref-type="bibr" rid="bib17">Ecker et al., 2016</xref>); however, such analyses cannot provide insight into the circuit mechanics of attentional modulation. Biophysical models of attention circuits are difficult to constrain, due in large part to the diversity of mechanisms which control the firing rate and response gain of neurons (<xref ref-type="bibr" rid="bib65">Silver, 2010</xref>; <xref ref-type="bibr" rid="bib69">Sutherland et al., 2009</xref>). Nonetheless, several circuit models for attentional modulation have been proposed (<xref ref-type="bibr" rid="bib2">Ardid et al., 2007</xref>; <xref ref-type="bibr" rid="bib11">Deco and Thiele, 2011</xref>; <xref ref-type="bibr" rid="bib6">Buia and Tiesinga, 2008</xref>), but analysis has been mostly confined to trial-averaged responses. Taking inspiration from these studies, mechanistic models of attentional modulation can be broadly grouped along two hypotheses. First, the circuit mechanisms that control trial-averaged responses may be distinct from those that modulate neuronal variability. This hypothesis has support from experiments in primate V1 showing that N-methyl-D-aspartate receptors have no impact on top-down attentional modulation of firing rates, yet have a strong influence of attentional control of noise correlations (<xref ref-type="bibr" rid="bib28">Herrero et al., 2013</xref>). A second hypothesis is that the modulations of firing rates and noise correlations are reflections of a single biophysical mechanism. Support for this comes from pairs of V4 neurons that each show strong attentional modulation of firing rates, also show a strong attention mediated reductions in noise correlation (<xref ref-type="bibr" rid="bib9">Cohen and Maunsell, 2011</xref>). In this study we provide novel analysis of the covariability of V4 population activity engaged in an attention-guided detection task (<xref ref-type="bibr" rid="bib8">Cohen and Maunsell, 2009</xref>) that is consistent with the second hypothesis. Specifically, the modulation of spike count covariance between unattended and attended states has the same dimensionality as the firing rate modulation.</p><p>We use the results from our dimensionality analysis to show that an excitatory-inhibitory recurrent circuit model subject to global fluctuations is sufficient to capture both the increase in firing rate and response gain as well as population-wide decrease of noise correlations. Our model makes two predictions regarding neuronal modulation: (1) that attentional modulation favors inhibitory neurons, and (2) that stimulus drive favors excitatory neurons. Finally, we show that our model predicts increased informational content in the excitatory population, which would result in improved readout by potential downstream targets. In total, our study provides a simple, parsimonious, and biologically motivated model of attentional modulation in cortical networks.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Attention decreases noise correlations primarily by decreasing covariance</title><p>Two rhesus monkeys (<italic>Macaca mulatta</italic>) with microelectrode arrays implanted bilaterally in V4 were trained in an orientation change detection task (<xref ref-type="fig" rid="fig1">Figure 1a</xref>; see Materials and methods: Data preparation). A display with oriented Gabor gratings on the left and right flashed on and off. The monkey was cued to attend to either the left or right grating before each block of trials, while keeping fixation on a point between the two gratings. After a random number of presentations, one of the gratings changed orientation. The monkey then had to saccade to that side to obtain a reward. The behavioral task and data collection have been previously reported (<xref ref-type="bibr" rid="bib8">Cohen and Maunsell, 2009</xref>).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.003</object-id><label>Figure 1.</label><caption><title>Attention increases firing rates and decreases trial-to-trial covariability of population responses.</title><p>(<bold>a</bold>) Overview of orientation-change detection task; see (<xref ref-type="bibr" rid="bib8">Cohen and Maunsell, 2009</xref>) for a full description. (<bold>b</bold>) Firing rates of neurons in the unattended (turquoise) and attended (orange) states, averaged over 3170 units. The slight oscillation in the firing rate was due to the monitor refresh rate. (<bold>c</bold>) Attention significantly decreased the spike count correlation and covariance and slightly increased variance. Error bars provide the SEM. (<bold>d</bold>) Histograms of changes in covariance for each unit pair (black) and variance for each unit (gray). In each case we consider the relative change <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf2"><mml:mi>X</mml:mi></mml:math></inline-formula> is either <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Data was collected from two monkeys over 21 and 16 recording sessions respectively. Signals were analyzed over a <inline-formula><mml:math id="inf5"><mml:mn>200</mml:mn></mml:math></inline-formula> ms interval, starting <inline-formula><mml:math id="inf6"><mml:mn>60</mml:mn></mml:math></inline-formula> ms after stimulus onset.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.003">http://dx.doi.org/10.7554/eLife.23978.003</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-fig1-v3"/></fig></p><p>A neuron is considered to be in an 'attended state' when the attended stimulus is in the hemifield containing that neuron’s receptive field (contralateral hemifield), and in an 'unattended state' when it is in the other (ipsilateral) hemifield. The trial-averaged firing rates from both attended and unattended neurons displayed a brief transient rise (<inline-formula><mml:math id="inf7"><mml:mo>∼</mml:mo></mml:math></inline-formula>100 ms after stimulus onset), and eventually settled to an elevated sustained rate before the trial concluded (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). During the sustained period the mean firing rate of attended neurons (<inline-formula><mml:math id="inf8"><mml:mn>22.0</mml:mn></mml:math></inline-formula> sp/s) was greater than that of unattended neurons (<inline-formula><mml:math id="inf9"><mml:mn>20.6</mml:mn></mml:math></inline-formula> sp/s) (<inline-formula><mml:math id="inf10"><mml:mi>t</mml:mi></mml:math></inline-formula> test, <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>A major finding of <xref ref-type="bibr" rid="bib8">Cohen and Maunsell (2009)</xref> was that the pairwise trial-to-trial noise correlations of the neuronal responses decreased with attention (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, left, mean unattended 0.065, mean attended 0.045, <inline-formula><mml:math id="inf12"><mml:mi>t</mml:mi></mml:math></inline-formula> test, <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). The noise correlation between neurons <inline-formula><mml:math id="inf14"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf15"><mml:mi>j</mml:mi></mml:math></inline-formula> is a normalized measure, <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>, where Cov and Var denote spike count covariance and variance respectively. Both spike count variance and covariance significantly change with attention (<inline-formula><mml:math id="inf17"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msup><mml:mtext>Var</mml:mtext><mml:mi>U</mml:mi></mml:msup><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>trials</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>5.02</mml:mn><mml:mo>⁢</mml:mo><mml:msup> <mml:mtext>spikes</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf18"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msup><mml:mtext>Var</mml:mtext><mml:mi>A</mml:mi></mml:msup><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>trials</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>5.10</mml:mn><mml:mo>⁢</mml:mo><mml:msup> <mml:mtext>spikes</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf19"><mml:mi>t</mml:mi></mml:math></inline-formula> test, <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf21"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msup><mml:mtext>Cov</mml:mtext><mml:mi>A</mml:mi></mml:msup><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mtext>trials</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.252</mml:mn></mml:mrow><mml:msup> <mml:mtext>spikes</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf22"><mml:mi>t</mml:mi></mml:math></inline-formula> test, <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>), but the decrease in covariance (<inline-formula><mml:math id="inf24"><mml:mrow><mml:mn>34.0</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:math></inline-formula>) is much more pronounced than the increase in variance (<inline-formula><mml:math id="inf25"><mml:mrow><mml:mn>1.61</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:math></inline-formula>; <xref ref-type="fig" rid="fig1">Figure 1c</xref>, middle and right). We therefore conclude that the attention mediated decrease in noise correlation is primarily due to decreased covariance.</p><p>To further validate this observation, we consider the distributions of pairwise changes in covariance (black) and variance (gray) with attention over the entire data set (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). Covariance and variance are normalized by their respective maximal unattended or attended values (see Methods: Comparing change in covariance to change in variance). The change in covariance with attention is concentrated below zero with a large spread, whereas the change in variance is centered on zero with a narrower spread. Taken together these results suggest that to understand the mechanism by which noise correlations decrease it is necessary and sufficient to understand how spike count covariance decreases with attention.</p></sec><sec id="s2-2"><title>Attention is a low-rank modulation of noise covariance</title><p>A reasonable simplification of V4 neurons is that they receive a bottom-up stimulus alongside an attention-mediated top-down modulatory input. However, to properly model top-down attention we need to first understand the dimension of attentional modulation on the V4 circuit as a whole. Let <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mo stretchy="false">↦</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the attentional modulation of measure <inline-formula><mml:math id="inf27"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> from its value in the unattended state, <inline-formula><mml:math id="inf28"><mml:msup><mml:mi>ϕ</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:math></inline-formula>, to its value in the attended state, <inline-formula><mml:math id="inf29"><mml:msup><mml:mi>ϕ</mml:mi><mml:mi>A</mml:mi></mml:msup></mml:math></inline-formula>. For example, the firing rate modulation <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> can be written as <inline-formula><mml:math id="inf31"><mml:mrow><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>A</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">𝐔</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf32"><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>A</mml:mi></mml:msup></mml:math></inline-formula> is an <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> vector of neural firing rates in the attended state, <inline-formula><mml:math id="inf34"><mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:math></inline-formula> denotes the firing rate vector in the unattended state, <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> is a vector the same size as <inline-formula><mml:math id="inf36"><mml:mi mathvariant="bold">r</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf37"><mml:mo>∘</mml:mo></mml:math></inline-formula> denotes elementwise multiplication. In this case, the entries <inline-formula><mml:math id="inf38"><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> are the ratios of the firing rates: <inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>A</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2a</xref>).<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.004</object-id><label>Figure 2.</label><caption><title>Rank one structure of attentional modulation of spike count covariance.</title><p>(<bold>a</bold>) Attentional modulation of firing rate. Firing rates of neurons <inline-formula><mml:math id="inf41"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf42"><mml:mi>j</mml:mi></mml:math></inline-formula> (black circles are modulated by bottom-up stimulus and top-down attention. (<bold>b</bold>) Two possible models of attentional modulation of covariance. Left: High-rank covariance modulation, in which attention modulates the shared variability of each pair of neurons. Right: Low-rank covariance modulation, in which attention modulates each neuron individually rather than in a pairwise manner. (<bold>c–e</bold>) The measured covariance values plotted against those predicted by the rank-1 model for data collected in one recording session, for c, the actual data (<inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.77</mml:mn></mml:mrow></mml:math></inline-formula>), d, shuffled data (<inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.22</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 100 shuffles), and (<bold>e</bold>) artificial upper-bound data (<inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 10 realizations of the upper bound model). (<bold>f</bold>) Synthesis of <bold>c-e</bold> in a bar plot. The orange area represents the loss of model performance compared to the upper bound model, and the blue area represents the increase in model performance compared to model applied to shuffled data. (<bold>g</bold>) Rank-1 model performance reported for <inline-formula><mml:math id="inf46"><mml:mn>21</mml:mn></mml:math></inline-formula> recording sessions from one monkey. Each bar represents one recording session. Recordings from a mean of <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>53.5</mml:mn></mml:mrow></mml:math></inline-formula> units in the right-hemisphere were analyzed, with maximum and minimum <inline-formula><mml:math id="inf48"><mml:mi>N</mml:mi></mml:math></inline-formula> of <inline-formula><mml:math id="inf49"><mml:mn>80</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf50"><mml:mn>35</mml:mn></mml:math></inline-formula>, respectively. Error bars denote standard error of the mean. (<bold>h</bold>) Mean normalized performance (relative to <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) for both hemispheres of two monkeys (M1 and M2). (<bold>i</bold>), Analysis as in (<bold>g</bold>), using leave-one-out cross-validation to test the predictive power of the model. (<bold>j</bold>) Mean normalized performance of the cross-validated data.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.004">http://dx.doi.org/10.7554/eLife.23978.004</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-fig2-v3"/></fig></p><p>A less trivial aspect of attentional modulation is the modulation of covariance matrices:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>A</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf52"><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi mathvariant="bold">𝐀</mml:mi></mml:msup></mml:math></inline-formula> is the attended spike count covariance matrix, <inline-formula><mml:math id="inf53"><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi mathvariant="bold">𝐔</mml:mi></mml:msup></mml:math></inline-formula> the unattended spike count covariance matrix, and <inline-formula><mml:math id="inf54"><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> is a matrix the same size as <inline-formula><mml:math id="inf55"><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi mathvariant="bold">𝐔</mml:mi></mml:msup></mml:math></inline-formula>, consisting of entries <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which we will call <italic>covariance gains</italic>. Unlike firing rates, the transformation matrix <inline-formula><mml:math id="inf57"><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> can be of varying rank. On the one hand <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> could be constructed from the ratios of the individual elements: <inline-formula><mml:math id="inf59"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, with each pair of neurons <inline-formula><mml:math id="inf60"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> receiving an individualized attentional modulation <inline-formula><mml:math id="inf61"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of their shared variability (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, left). Under this modulation <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> is a rank <inline-formula><mml:math id="inf63"><mml:mi>N</mml:mi></mml:math></inline-formula> matrix. A rank <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> will always perfectly (and trivially) capture the matrix mapping in <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref>. However, it is difficult to conceive of a top-down circuit mechanism that would allow attention to modulate each pair individually. On the other hand, <inline-formula><mml:math id="inf65"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> could depend not on the specific pair <inline-formula><mml:math id="inf66"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, but on the individual neurons of the pairing: <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, right). In this case, only <inline-formula><mml:math id="inf68"><mml:mi>N</mml:mi></mml:math></inline-formula> values are needed to characterize <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">g</mml:mi><mml:mi mathvariant="bold">g</mml:mi></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf70"><mml:mi mathvariant="bold">𝐠</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> column vector, meaning <inline-formula><mml:math id="inf72"><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> has rank of <inline-formula><mml:math id="inf73"><mml:mn>1</mml:mn></mml:math></inline-formula>. This is a more parsimonious and biophysically plausible scenario for attentional modulation, since in this case the covariance gain <inline-formula><mml:math id="inf74"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of neurons <inline-formula><mml:math id="inf75"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf76"><mml:mi>j</mml:mi></mml:math></inline-formula> is simply emergent from the attentional modulation of the individual neurons. To test whether <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> is low rank we analyzed the V4 population recordings during the visual attention task (<xref ref-type="fig" rid="fig1">Figure 1</xref>), specifically measuring <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> under the assumption that <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> is rank 1:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>A</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐠𝐠</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>∘</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ2">Equation (2)</xref> is a system of <inline-formula><mml:math id="inf80"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> equations of the form <inline-formula><mml:math id="inf81"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> in <inline-formula><mml:math id="inf82"><mml:mi>N</mml:mi></mml:math></inline-formula> unknowns <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> (we only consider <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> to exclude variance modulation from our analysis). For <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> this is an overdetermined system, and we solve for <inline-formula><mml:math id="inf86"><mml:mi mathvariant="bold">𝐠</mml:mi></mml:math></inline-formula> using a nonlinear equation solver. Let <inline-formula><mml:math id="inf87"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> be the optimal solution obtained by the solver (measured as a minimization of the <inline-formula><mml:math id="inf88"><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>-norm of the error; see Methods: objfxn). Then <inline-formula><mml:math id="inf89"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msup><mml:mo>:=</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>∘</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> provides an approximation to the attended covariance matrix. In an example data set from a single recording session with <inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>39</mml:mn></mml:mrow></mml:math></inline-formula> units, the correlation coefficient <inline-formula><mml:math id="inf91"><mml:mi>ρ</mml:mi></mml:math></inline-formula> of the actual attended covariance values from <inline-formula><mml:math id="inf92"><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi mathvariant="bold">𝐀</mml:mi></mml:msup></mml:math></inline-formula> versus the approximated attended covariance values from <inline-formula><mml:math id="inf93"><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msup></mml:math></inline-formula> was <inline-formula><mml:math id="inf94"><mml:mn>0.77</mml:mn></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). A shuffled <inline-formula><mml:math id="inf95"><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi mathvariant="bold">𝐀</mml:mi></mml:msup></mml:math></inline-formula> matrix provides a reasonable null model, and the example data set produces the lower bound correlation <inline-formula><mml:math id="inf96"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>shuf</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.22</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2d</xref>; see Materials and methods: Shuffled covariance matrices). Finally, a Poisson model that perfectly decomposes as <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref>, yet sampled with the same number of trials as in the experiment, gives an upper bound for the rank one structure, the example data yields <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2e</xref>; see Materials and methods: Upper bound covariance matrices). In total, the combination of <inline-formula><mml:math id="inf98"><mml:mi>ρ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2f</xref>) suggests that the rank one model of attention modulation of covariance <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> is well justified.</p><p>We applied this analysis to 21 recording sessions from the right hemisphere of one monkey (<xref ref-type="fig" rid="fig2">Figure 2g</xref>). For most of the recording sessions <inline-formula><mml:math id="inf102"><mml:mi>ρ</mml:mi></mml:math></inline-formula> is closer to <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> than <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The averaged performance of all sessions for both hemispheres of two monkeys generally agreed with this trend (<xref ref-type="fig" rid="fig2">Figure 2h</xref>). We normalized <inline-formula><mml:math id="inf105"><mml:mi>ρ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> by <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for each session to better compare different sessions that were subject to day-to-day variations outside of the experimenter’s control, such as the task performance or the internal state of the monkey. To further validate our model we show the distribution of <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>s computed from the entire data set (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). The majority of <inline-formula><mml:math id="inf109"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> values are less than one, consistent with <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>Cov</mml:mtext></mml:mstyle><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>trials</mml:mtext></mml:mstyle></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>Cov</mml:mtext></mml:mstyle><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>trials</mml:mtext></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). Further, there was little relation between the attentional modulation of firing rates, measured by <inline-formula><mml:math id="inf111"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>A</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, and the attentional modulation of covariance through <inline-formula><mml:math id="inf112"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). This indicates that the circuit modulation of firing rates and covariance are not trivially related to one another (<xref ref-type="bibr" rid="bib15">Doiron et al., 2016</xref>).<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.005</object-id><label>Figure 3.</label><caption><title>Covariance gain shows the attenuation of population-wide fluctuations with attention.</title><p>(<bold>a</bold>) Distribution of covariance gains <inline-formula><mml:math id="inf113"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> computed from the entire data set. (<bold>b</bold>) The relation between covariance <inline-formula><mml:math id="inf114"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and the attention mediated modulation of firing rates <inline-formula><mml:math id="inf115"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>A</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. The correlation coefficients between the data sets were <inline-formula><mml:math id="inf116"><mml:mn>0.036</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf117"><mml:mn>0.051</mml:mn></mml:math></inline-formula> for the right and left hemispheres, respectively.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.005">http://dx.doi.org/10.7554/eLife.23978.005</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-fig3-v3"/></fig></p><p>We additionally tested the validity of our model in <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref> with a leave-one-out cross-validation analysis (see Materials and methods: Leave-one-out cross-validation). We accurately predicted an omitted covariance <inline-formula><mml:math id="inf118"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2i and j</xref>), consistent with our original analysis (<xref ref-type="fig" rid="fig2">Figure 2g and h</xref>). The individual session-by-session performance values for both the standard and leave-one-out setups are provided (Appendix: Model performance for all monkeys and hemispheres).</p><p>Finally, we investigated to what extent the actual value of the covariance gain <inline-formula><mml:math id="inf119"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of neuron <inline-formula><mml:math id="inf120"><mml:mi>i</mml:mi></mml:math></inline-formula> depends on the population of neurons in which it was computed. We solved the system of equations <inline-formula><mml:math id="inf121"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> using covariance matrices computed from recordings from distinct sets of neurons, overlapping only by neuron <inline-formula><mml:math id="inf122"><mml:mi>i</mml:mi></mml:math></inline-formula>. This gives two estimates of <inline-formula><mml:math id="inf123"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, that nevertheless agreed largely with one another (Appendix: Low-dimensional modulation is intrinsic to neurons). This supported the hypothesis that covariance gain <inline-formula><mml:math id="inf124"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is an intrinsic property of neuron <inline-formula><mml:math id="inf125"><mml:mi>i</mml:mi></mml:math></inline-formula>.</p><p>The standard and cross-validation tests verify that the low-rank model of attentional modulation defined in <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref> explains between <inline-formula><mml:math id="inf126"><mml:mn>66</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:mrow><mml:mn>82</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:math></inline-formula> (standard), or <inline-formula><mml:math id="inf128"><mml:mn>56</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf129"><mml:mrow><mml:mn>77</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:math></inline-formula> (cross-validation) of the data. Taking this to be a positive result, we conclude that the covariance gain modulation depends largely on the modulation of individual neurons.</p></sec><sec id="s2-3"><title>Network requirements for attentional modulation</title><p>Having described attentional modulation statistically our next goal is to develop a circuit model to understand the process mechanistically. Consider a network of <inline-formula><mml:math id="inf130"><mml:mi>N</mml:mi></mml:math></inline-formula> coupled neurons, and let the spike count from neuron <inline-formula><mml:math id="inf131"><mml:mi>i</mml:mi></mml:math></inline-formula> on a given trial be <inline-formula><mml:math id="inf132"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. The network output has the covariance matrix <inline-formula><mml:math id="inf133"><mml:mi mathvariant="bold">𝐂</mml:mi></mml:math></inline-formula> with elements <inline-formula><mml:math id="inf134"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext>Cov</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. In this section we identify the minimal circuit elements so that the attentional mapping <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mo stretchy="false">↦</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> satisfies the following two conditions (on average):</p><p><bold>C1:</bold> <inline-formula><mml:math id="inf136"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> ; attentional modulation of covariance is rank one (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><p><bold>C2:</bold> <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> ; spike count covariance decreases with attention (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>What follows is only a sketch of our derivation (a complete treatment is given in Appendix: Network requirements for attentional modulation).</p><p>If inputs are weak then <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> can be described by a linear perturbation about a background state (<xref ref-type="bibr" rid="bib24">Ginzburg and Sompolinsky, 1994</xref>; <xref ref-type="bibr" rid="bib14">Doiron et al., 2004</xref>; <xref ref-type="bibr" rid="bib72">Trousdale et al., 2012</xref>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf139"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the background activity of neuron <inline-formula><mml:math id="inf140"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf141"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the coupling strength from neuron <inline-formula><mml:math id="inf142"><mml:mi>k</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf143"><mml:mi>i</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf144"><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the input-to-output gain of neuron <inline-formula><mml:math id="inf145"><mml:mi>i</mml:mi></mml:math></inline-formula>. In addition to internal coupling we assume a source of external fluctuations <inline-formula><mml:math id="inf146"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> to neuron <inline-formula><mml:math id="inf147"><mml:mi>i</mml:mi></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf148"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf149"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf150"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are random variables that vary across trials. The trial-averaged firing rate of neuron <inline-formula><mml:math id="inf151"><mml:mi>i</mml:mi></mml:math></inline-formula> is <inline-formula><mml:math id="inf152"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf153"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> denotes averaging over trials of length <inline-formula><mml:math id="inf154"><mml:mi>T</mml:mi></mml:math></inline-formula>). The background state has variability <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> which we assume to be independent across neurons, meaning the background network covariance is <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Finally, the external fluctuations have covariance matrix <inline-formula><mml:math id="inf157"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> with element <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Motivated by our analysis of population recordings (<xref ref-type="fig" rid="fig2">Figure 2</xref>) we study attentional modulations that target individual neurons. This amounts to considering only <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo stretchy="false">↦</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo stretchy="false">↦</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Additionally, we assume that any model of attentional modulation must result in <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). A widespread property of both cortical pyramidal cells and interneurons is that an increase of firing rate <inline-formula><mml:math id="inf162"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> causes an increase of input-output gain <inline-formula><mml:math id="inf163"><mml:mi>L</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib7">Cardin et al., 2007</xref>), thus we will also require <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Spiking covariability in recurrent networks can be due to internal interactions (through <inline-formula><mml:math id="inf165"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) or external fluctuations (through <inline-formula><mml:math id="inf166"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>), or both (<xref ref-type="bibr" rid="bib49">Ocker et al., 2017</xref>). Networks with unstructured connectivity have internally generated covariability that vanishes as <inline-formula><mml:math id="inf167"><mml:mi>N</mml:mi></mml:math></inline-formula> grows. This is true if the connectivity is sparse (<xref ref-type="bibr" rid="bib73">van Vreeswijk and Sompolinsky, 1998</xref>), or dense having weak synapses where <inline-formula><mml:math id="inf168"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib72">Trousdale et al., 2012</xref>) or strong synapses where <inline-formula><mml:math id="inf169"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> combined with a balance between excitation and inhibition (<xref ref-type="bibr" rid="bib56">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib60">Rosenbaum et al., 2017</xref>). In these cases spiking covariability requires external fluctuations to be applied and subsequently filtered by the network. We follow this second scenario and choose <inline-formula><mml:math id="inf170"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> so as to provide external covariability to our network.</p><p>Recent analysis of cortical population recordings show that the shared spiking variability across the population can be well approximated by a rank one model of covariability (<xref ref-type="bibr" rid="bib30">Kelly et al., 2010</xref>; <xref ref-type="bibr" rid="bib16">Ecker et al., 2014</xref>; <xref ref-type="bibr" rid="bib38">Lin et al., 2015</xref>; <xref ref-type="bibr" rid="bib17">Ecker et al., 2016</xref>; <xref ref-type="bibr" rid="bib55">Rabinowitz et al., 2015</xref>; <xref ref-type="bibr" rid="bib74">Whiteway and Butts, 2017</xref>) (we remark that <xref ref-type="bibr" rid="bib55">Rabinowitz et al., 2015</xref> analyzed the same data set that we have in <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>). Thus motivated we take the external fluctuations <inline-formula><mml:math id="inf171"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> to be rank one with <inline-formula><mml:math id="inf172"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, reflecting a single source of global external variability <inline-formula><mml:math id="inf173"><mml:mi>ξ</mml:mi></mml:math></inline-formula> with unit variance (neuron <inline-formula><mml:math id="inf174"><mml:mi>i</mml:mi></mml:math></inline-formula> receives <inline-formula><mml:math id="inf175"><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). Combining this assumption with the linear ansatz in <xref ref-type="disp-formula" rid="equ3">Equation (3)</xref> yields:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐊</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐋𝐱</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐊</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐋𝐱</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐜𝐜</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where matrix <inline-formula><mml:math id="inf176"><mml:mi mathvariant="bold">𝐊</mml:mi></mml:math></inline-formula> has element <inline-formula><mml:math id="inf177"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We have also defined the vectors <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf181"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐊</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐋𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In total, the output covariability <inline-formula><mml:math id="inf182"><mml:mi mathvariant="bold">𝐂</mml:mi></mml:math></inline-formula> will simply inherit the rank of the input covariability <inline-formula><mml:math id="inf183"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>. Attentional modulation affects <inline-formula><mml:math id="inf184"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> through <inline-formula><mml:math id="inf185"><mml:mi mathvariant="bold">𝐊</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf186"><mml:mi mathvariant="bold">𝐋</mml:mi></mml:math></inline-formula> and we easily satisfy condition <inline-formula><mml:math id="inf187"><mml:mi mathvariant="bold">𝐂𝟏</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf188"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>A</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>What remains is to find constraints on <inline-formula><mml:math id="inf189"><mml:mi mathvariant="bold">𝐉</mml:mi></mml:math></inline-formula> and the attentional modulation of <inline-formula><mml:math id="inf190"><mml:mi mathvariant="bold">𝐋</mml:mi></mml:math></inline-formula> that satisfy condition <inline-formula><mml:math id="inf191"><mml:mi mathvariant="bold">𝐂𝟐</mml:mi></mml:math></inline-formula>. Let us consider the case where <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> so that condition <inline-formula><mml:math id="inf193"><mml:mi mathvariant="bold">𝐂𝟐</mml:mi></mml:math></inline-formula> is satisfied when <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the sake of mathematical simplicity let us separate the population into <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> excitatory neurons and <inline-formula><mml:math id="inf196"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> inhibitory neurons (<inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mi>q</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Let all excitatory (inhibitory) neurons project with synaptic strength <inline-formula><mml:math id="inf198"><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf199"><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), have gain <inline-formula><mml:math id="inf200"><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf201"><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula>), and receive the external inputs of strength <inline-formula><mml:math id="inf202"><mml:msub><mml:mi>x</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf203"><mml:msub><mml:mi>x</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula>). Finally, let the probability for all connections be <inline-formula><mml:math id="inf204"><mml:mi>p</mml:mi></mml:math></inline-formula>, and consider only weak connections (<inline-formula><mml:math id="inf205"><mml:mrow><mml:mi>J</mml:mi><mml:mo>∝</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf206"><mml:mi>N</mml:mi></mml:math></inline-formula> large) so that we can ignore the influence of polysynaptic paths in the network (<xref ref-type="bibr" rid="bib51">Pernice et al., 2011</xref>; <xref ref-type="bibr" rid="bib72">Trousdale et al., 2012</xref>). Then the attentional modulation of an excitatory neuron decomposes into:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>q</mml:mi><mml:mi>p</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mfrac linethickness="0"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="1"><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="1"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:munder><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable columnspacing="0em" displaystyle="false" rowspacing="0.1em"><mml:mtr><mml:mtd><mml:mfrac linethickness="0"><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="1"><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="1"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mstyle></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:munder><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The first term is the direct transfer of the external fluctuations, and the second and third terms are indirect transfer of external fluctuations via the excitatory and inhibitory populations, respectively. Recall that <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, meaning that for <inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to be satisfied we require the third term to outweigh the combination of the first and second terms. In other words, the inhibitory population must experience a sizable attentional modulation. A similar cancelation of correlations by recurrent inhibition has been recently studied in a variety of cortical models (<xref ref-type="bibr" rid="bib56">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib70">Tetzlaff et al., 2012</xref>; <xref ref-type="bibr" rid="bib39">Ly et al., 2012</xref>; <xref ref-type="bibr" rid="bib15">Doiron et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Rosenbaum et al., 2017</xref>).</p><p>In the above we considered weak synaptic connections where <inline-formula><mml:math id="inf209"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Rather, if we scale <inline-formula><mml:math id="inf210"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>, as would be the case for classical balanced networks (<xref ref-type="bibr" rid="bib73">van Vreeswijk and Sompolinsky, 1998</xref>), then for very large <inline-formula><mml:math id="inf211"><mml:mi>N</mml:mi></mml:math></inline-formula> the solution no longer depends upon the gain <inline-formula><mml:math id="inf212"><mml:mi>L</mml:mi></mml:math></inline-formula>. Finite <inline-formula><mml:math id="inf213"><mml:mi>N</mml:mi></mml:math></inline-formula> or the inclusion of synaptic nonlinearities through short term plasticity (<xref ref-type="bibr" rid="bib44">Mongillo et al., 2012</xref>) may be necessary to satisfy condition <inline-formula><mml:math id="inf214"><mml:mi mathvariant="bold">𝐂𝟐</mml:mi></mml:math></inline-formula> with large synapses. Furthermore, the large synaptic weights associated with <inline-formula><mml:math id="inf215"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> do not allows us to neglect polysynaptic paths, as is needed for <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref>. Extending our analysis to networks with balanced scaling will be the focus of future work.</p><p>In summary our analysis has identified two circuit features that allow recurrent networks to capture conditions <inline-formula><mml:math id="inf216"><mml:mi mathvariant="bold">𝐂𝟏</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf217"><mml:mi mathvariant="bold">𝐂𝟐</mml:mi></mml:math></inline-formula> for attentional modulation. First, the network must be subject to a global source of external fluctuations that dominates network covariability (<inline-formula><mml:math id="inf218"><mml:mi mathvariant="bold">𝐂𝟏</mml:mi></mml:math></inline-formula>). Second, the network must have recurrent inhibitory connections that are subject to a large attentional modulation (<inline-formula><mml:math id="inf219"><mml:mi mathvariant="bold">𝐂𝟐</mml:mi></mml:math></inline-formula>).</p></sec><sec id="s2-4"><title>Mean field model of attention</title><p>We next apply the intuition gained in the preceding section to propose a cortical model that captures key neural correlates of attentional modulation. We model V4 as a recurrently coupled network of excitatory and inhibitory leaky integrate-and-fire model neurons (<xref ref-type="bibr" rid="bib70">Tetzlaff et al., 2012</xref>; <xref ref-type="bibr" rid="bib35">Ledoux and Brunel, 2011</xref>; <xref ref-type="bibr" rid="bib72">Trousdale et al., 2012</xref>; <xref ref-type="bibr" rid="bib14">Doiron et al., 2004</xref>) (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). In addition to recurrent synaptic inputs, each neuron receives private and global sources of external fluctuating input (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). The global noise is an attention-independent source of input correlation that the network filters and transforms into network-wide output spiking correlations (<xref ref-type="fig" rid="fig4">Figure 4c</xref>).<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.006</object-id><label>Figure 4.</label><caption><title>Excitatory-inhibitory network model.</title><p>(<bold>a</bold>) Recurrent excitatory-inhibitory network subject to private and shared fluctuations as well as top-down attentional modulation. (<bold>b</bold>) Example voltage trace from a LIF model neuron in the network. Top tick marks denote spike times. (<bold>c</bold>) Spike time raster plot of the spiking activity from the model network. (<bold>d</bold>) Population-averaged firing rate <inline-formula><mml:math id="inf220"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the excitatory population. Left: frequency distribution of population-averaged firing rate. (<bold>e</bold>) Transfer function <inline-formula><mml:math id="inf221"><mml:msub><mml:mi>f</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> between the effective input and the firing rate for a model excitatory neuron. The red segment represents the attentional shift in effective input and hence firing rate. (<bold>f</bold>), Same as <bold>e</bold>, but for the inhibitory population. (<bold>g</bold>) Attention as a path through (<inline-formula><mml:math id="inf222"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>,<inline-formula><mml:math id="inf223"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula>) space, and equivalently through (<inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> space.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.006">http://dx.doi.org/10.7554/eLife.23978.006</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-fig4-v3"/></fig></p><p>While the linear response theory introduced in <xref ref-type="disp-formula" rid="equ3">Equation (3)</xref> is well suited to study large networks of integrate-and-fire neurons driven by weakly correlated inputs (<xref ref-type="bibr" rid="bib70">Tetzlaff et al., 2012</xref>; <xref ref-type="bibr" rid="bib35">Ledoux and Brunel, 2011</xref>; <xref ref-type="bibr" rid="bib72">Trousdale et al., 2012</xref>; <xref ref-type="bibr" rid="bib14">Doiron et al., 2004</xref>), the analysis offers little analytic insight. Instead, we consider the instantaneous activity across population <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf227"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the spike train from neuron <inline-formula><mml:math id="inf228"><mml:mi>i</mml:mi></mml:math></inline-formula> of population <inline-formula><mml:math id="inf229"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf230"><mml:msub><mml:mi>N</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> is the population size (<inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf232"><mml:mi>I</mml:mi></mml:math></inline-formula>). This approach reduces the model to just the two dynamic variables, the excitatory population rate <inline-formula><mml:math id="inf233"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the inhibitory population rate <inline-formula><mml:math id="inf234"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf235"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is shown in <xref ref-type="fig" rid="fig4">Figure 4d</xref>). Despite this severe reduction the model retains the key ingredients for attentional modulation identified in the previous section – recurrent excitation and inhibition combined with a source of global fluctuations.</p><p>We take the population sizes to be large and consider a phenomenological dynamic mean field (<xref ref-type="bibr" rid="bib70">Tetzlaff et al., 2012</xref>; <xref ref-type="bibr" rid="bib35">Ledoux and Brunel, 2011</xref>) of the cortical network (see Materials and methods: Mean field model):<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mi>ξ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mi>ξ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The function <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>f</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> is the input-output transfer of population <inline-formula><mml:math id="inf237"><mml:mi>α</mml:mi></mml:math></inline-formula>, taken to be the mean firing rate for a fixed input (<xref ref-type="fig" rid="fig4">Figure 4e</xref> for the <inline-formula><mml:math id="inf238"><mml:mi>E</mml:mi></mml:math></inline-formula> population and <xref ref-type="fig" rid="fig4">Figure 4f</xref> for the <inline-formula><mml:math id="inf239"><mml:mi>I</mml:mi></mml:math></inline-formula> population). The parameter <inline-formula><mml:math id="inf240"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the coupling strength from population <inline-formula><mml:math id="inf241"><mml:mi>β</mml:mi></mml:math></inline-formula> to population <inline-formula><mml:math id="inf242"><mml:mi>α</mml:mi></mml:math></inline-formula>. Finally, <inline-formula><mml:math id="inf243"><mml:msub><mml:mi>μ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf244"><mml:msub><mml:mi>σ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> are the respective strengths of the mean input and the global fluctuation <inline-formula><mml:math id="inf245"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to population <inline-formula><mml:math id="inf246"><mml:mi>α</mml:mi></mml:math></inline-formula> (throughout <inline-formula><mml:math id="inf247"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has a zero mean). To simplify our exposition we take symmetric coupling <inline-formula><mml:math id="inf248"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf249"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and symmetric timescales <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We set the recurrent coupling so that the model has a stationary mean firing rate (<inline-formula><mml:math id="inf251"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), about which <inline-formula><mml:math id="inf252"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> induces fluctuations in <inline-formula><mml:math id="inf253"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf254"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Attention is modeled as a top-down influence on the static input: <inline-formula><mml:math id="inf255"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf256"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a background input, the parameter <inline-formula><mml:math id="inf257"><mml:mi>A</mml:mi></mml:math></inline-formula> models attention with <inline-formula><mml:math id="inf258"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> denoting the unattended state and <inline-formula><mml:math id="inf259"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> the fully attended state, and <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is the increase in <inline-formula><mml:math id="inf261"><mml:msub><mml:mi>μ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> due to attention. We note that the choice of representing the unattended state by <inline-formula><mml:math id="inf262"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and the attended state by <inline-formula><mml:math id="inf263"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> is only due to convenience, and is not meant to make any statement about particular bounds on these states. In this model attention simply increases the excitability of all of the neurons in the network (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). This modulation is consistent with the rank one structure of attentional modulation in the data (<xref ref-type="fig" rid="fig2">Figure 2</xref>), since <inline-formula><mml:math id="inf264"><mml:msub><mml:mi>μ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> is a single neuron property. The attention-induced increase in <inline-formula><mml:math id="inf265"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> causes an increase in the mean firing rates <inline-formula><mml:math id="inf266"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (red paths in <xref ref-type="fig" rid="fig4">Figure 4e,f</xref>), consistent with recordings from putative excitatory (<xref ref-type="bibr" rid="bib41">McAdams and Maunsell, 2000</xref>; <xref ref-type="bibr" rid="bib57">Reynolds et al., 1999</xref>) and inhibitory neurons (<xref ref-type="bibr" rid="bib42">Mitchell et al., 2007</xref>) in visual area V4. Since <inline-formula><mml:math id="inf267"><mml:msub><mml:mi>f</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> is a simple rising function then there is a unique mapping of an attentional path in <inline-formula><mml:math id="inf268"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> space to a path in <inline-formula><mml:math id="inf269"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> space (<xref ref-type="fig" rid="fig4">Figure 4g</xref>).</p><p>In total, our population model has the core features required to satisfy Conditions <bold>C1</bold> and <bold>C2</bold> of the previous section. We next use our mean field model to investigate how attentional paths in <inline-formula><mml:math id="inf270"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> space affect population spiking variability.</p></sec><sec id="s2-5"><title>Attention modulates population variability</title><p>The global input <inline-formula><mml:math id="inf271"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> causes fluctuations about the network stationary state: <inline-formula><mml:math id="inf272"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>α</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The fluctuations <inline-formula><mml:math id="inf273"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are directly related to coordinated spiking activity in population <inline-formula><mml:math id="inf274"><mml:mi>α</mml:mi></mml:math></inline-formula>. In particular, in the limit of large <inline-formula><mml:math id="inf275"><mml:msub><mml:mi>N</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> we have that <inline-formula><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the expectation is over <inline-formula><mml:math id="inf277"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> pairs in the spiking network. Thus, in our mean field network we require attentional modulation to decrease population variance <inline-formula><mml:math id="inf278"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>For sufficiently small <inline-formula><mml:math id="inf279"><mml:msub><mml:mi>σ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> the fluctuations <inline-formula><mml:math id="inf280"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf281"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> obey linearized mean field equations (see Materials and methods: Mean field model, <xref ref-type="disp-formula" rid="equ19">Equation (17)</xref>). The linear system is readily analyzed and we obtain the population variance <inline-formula><mml:math id="inf282"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> computed over long time windows (see Materials and methods: Computing <inline-formula><mml:math id="inf283"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf284"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>≡</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>α</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is the response gain of neurons in population <inline-formula><mml:math id="inf285"><mml:mi>α</mml:mi></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ7">Equation (7)</xref> shows that <inline-formula><mml:math id="inf286"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> depends directly on <inline-formula><mml:math id="inf287"><mml:msub><mml:mi>L</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula>, and we recall that <inline-formula><mml:math id="inf288"><mml:msub><mml:mi>L</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> changes with attention (the slope of <inline-formula><mml:math id="inf289"><mml:msub><mml:mi>f</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> in <xref ref-type="fig" rid="fig4">Figure 4e,f</xref>). Thus, while the derivation of <inline-formula><mml:math id="inf290"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> requires linear fluctuations about a steady state, attentional modulation samples the nonlinearity in the transfer <inline-formula><mml:math id="inf291"><mml:msub><mml:mi>f</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> by changing the state about which we linearize. Any attention-mediated change in <inline-formula><mml:math id="inf292"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> is not obvious since both <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, meaning that both the numerator and denominator in <xref ref-type="disp-formula" rid="equ7">Equation (7)</xref> will change with attention.</p><p>We explore <inline-formula><mml:math id="inf295"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> by sweeping over (<inline-formula><mml:math id="inf296"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf297"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula>) space (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). When the network has high <inline-formula><mml:math id="inf298"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> and low <inline-formula><mml:math id="inf299"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> then <inline-formula><mml:math id="inf300"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> is large, while <inline-formula><mml:math id="inf301"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> is low for the opposite case of high <inline-formula><mml:math id="inf302"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> and low <inline-formula><mml:math id="inf303"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>. Along our attention path <inline-formula><mml:math id="inf304"><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> increases while <inline-formula><mml:math id="inf305"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> decreases (<xref ref-type="fig" rid="fig5">Figure 5b</xref>), satisfying our requirements for attentional modulation. The attention path that we highlight is just one potential path that reduces population variability, however all paths which reduce <inline-formula><mml:math id="inf306"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> share a large attention-mediated recruitment of inhibition. If we start with the unattended state (turquoise dot in <xref ref-type="fig" rid="fig5">Figure 5c</xref>) we can label all (<inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) points that have a smaller population variance than the unattended point (light green region in <xref ref-type="fig" rid="fig5">Figure 5c</xref>). These modulations all share that <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5c</xref>, green region is below the <inline-formula><mml:math id="inf309"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> line). While the absolute comparison between <inline-formula><mml:math id="inf310"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf311"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> may depend on model parameters, a robust necessary feature of top-down attentional modulation is that it must significantly recruit the inhibitory population. This observation is a major circuit prediction of our model.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.007</object-id><label>Figure 5.</label><caption><title>Mean field model shows an attention mediated decrease in population variance.</title><p>(<bold>a</bold>) An attentional path in excitatory-inhibitory firing rate space for which the population variance decreases. Colored contours define iso-lines of population variance in increments of <inline-formula><mml:math id="inf312"><mml:mn>10</mml:mn></mml:math></inline-formula> (sp/s)<inline-formula><mml:math id="inf313"><mml:msup><mml:mi/><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>. The attentional path links the unattended state (<inline-formula><mml:math id="inf314"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>; turquoise point) to the attended state (<inline-formula><mml:math id="inf315"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, orange point). (<bold>b</bold>) Variance values as a function of the attentional path defined in <bold>a</bold>. (<bold>c</bold>) The modulation from an unattended state (origin) to an attended state over the input space (<inline-formula><mml:math id="inf316"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>). Solid black line marks where <inline-formula><mml:math id="inf317"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> remains unchanged, and the green region where <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is less than zero. (<bold>d</bold>) The eigenvalue <inline-formula><mml:math id="inf319"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> along the attentional path. With increased attention it becomes more negative, indicating that the state <inline-formula><mml:math id="inf320"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf321"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is more stable. e, Autocovariance function of the excitatory population rate <inline-formula><mml:math id="inf322"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the attended and unattended state (computed using <xref ref-type="disp-formula" rid="equ21">Equation (19)</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.007">http://dx.doi.org/10.7554/eLife.23978.007</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-fig5-v3"/></fig></p><p>An intuitive way to understand inhibition’s role in the decrease in population variance is through the stability analysis of the mean field equations. The eigenvalues of the linearized system are <inline-formula><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf324"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (see Materials and methods: Mean field model, <xref ref-type="disp-formula" rid="equ20">Equation (18)</xref>). Note that the denominator of the population variance (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>) equals the square of the eigenvalue product <inline-formula><mml:math id="inf325"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The stability of the network activity is determined by <inline-formula><mml:math id="inf326"><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>; the more negative <inline-formula><mml:math id="inf327"><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, the more stable the point <inline-formula><mml:math id="inf328"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and the better the network dampens the perturbations about the point due to input fluctuations <inline-formula><mml:math id="inf329"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The decrease of <inline-formula><mml:math id="inf330"><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> along the example attention path is clear (<xref ref-type="fig" rid="fig5">Figure 5d</xref>), and overcomes the increase in the numerator of <inline-formula><mml:math id="inf331"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> due to increases in <inline-formula><mml:math id="inf332"><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf333"><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula>. The enhanced damping is why <inline-formula><mml:math id="inf334"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> decreases, explicitly seen in the steeper decline of the excitatory population autocovariance function in the attended compared to the unattended state (<xref ref-type="fig" rid="fig5">Figure 5e</xref>).</p><p>This enhanced stability due to recurrent inhibition is a reflection of inhibition canceling population variability provided by external fluctuations and recurrent excitation (<xref ref-type="bibr" rid="bib56">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib70">Tetzlaff et al., 2012</xref>; <xref ref-type="bibr" rid="bib50">Ozeki et al., 2009</xref>). Indeed, taking the coupling <inline-formula><mml:math id="inf335"><mml:mi>J</mml:mi></mml:math></inline-formula> to be weak allows the expansion <inline-formula><mml:math id="inf336"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ7">Equation (7)</xref>, so that the attention mediated increase in <inline-formula><mml:math id="inf337"><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> reduces population variance through cancellation, as in <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref>. However, this expansion is not formally required to compute the eigenvalues <inline-formula><mml:math id="inf338"><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf339"><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, and these measure the stability of the firing rate dynamics. We mention the expansion only to compare to the original motivation for inhibition.</p><p>The expression for <inline-formula><mml:math id="inf340"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> given above (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>) assumes a symmetry in the network coupling, namely that <inline-formula><mml:math id="inf341"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf342"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This allowed <inline-formula><mml:math id="inf343"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> to be compactly written, facilitating the analysis of how attention affects both the numerator and denominator of <xref ref-type="disp-formula" rid="equ7">Equation (7)</xref>. However, the linearization of the mean field equations and the subsequent analysis of population variability do not require this assumption (see Materials and methods: Mean field model <xref ref-type="disp-formula" rid="equ20 equ21 equ22">Equations (18–20)</xref>). To explore the robustness of our main result we let <inline-formula><mml:math id="inf344"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf345"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, thereby breaking the coupling symmetry for <inline-formula><mml:math id="inf346"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The reduction in <inline-formula><mml:math id="inf347"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> with attention is robust over a large region of (<inline-formula><mml:math id="inf348"><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:math></inline-formula>) (<xref ref-type="fig" rid="fig6">Figure 6a</xref>, green region). Focusing on selected <inline-formula><mml:math id="inf349"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> pairings within the region where <inline-formula><mml:math id="inf350"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> decreases shows that the attentional path identified for the network with coupling symmetry produces qualitatively similar behavior in the more general network (compare <xref ref-type="fig" rid="fig5">Figure 5c</xref> to <xref ref-type="fig" rid="fig6">Figure 6b–e</xref>). In total, the inhibitory mechanism for attention mediated reduction in population variability is robust to changes in the recurrent coupling with the network.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.008</object-id><label>Figure 6.</label><caption><title>The attention mediated reduction in population variance is robust to changes in strength of recurrent connectivity.</title><p>(<bold>a</bold>) Sweep over <inline-formula><mml:math id="inf351"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf352"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> space (with <inline-formula><mml:math id="inf353"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf354"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> fixed) labeling the region where <inline-formula><mml:math id="inf355"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>E</mml:mi><mml:mi>U</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>E</mml:mi><mml:mi>A</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is positive (grey) and negative (green). (<bold>b–e</bold>) Attentional path in excitatory-inhibitory firing rate space. The colored contours are as in <xref ref-type="fig" rid="fig5">Figure 5a</xref>. All calculations are done using <xref ref-type="disp-formula" rid="equ20 equ21 equ22">Equations (18–20)</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.008">http://dx.doi.org/10.7554/eLife.23978.008</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-fig6-v3"/></fig></p><p>While the reduced mean field equations are straightforward to analyze, a similar attenuation of pairwise covariance <inline-formula><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> along the same attentional path occurs in the LIF model network (Appendix: Spiking network). Using linear response analysis for the spiking network we can relate the effect of inhibition to previous work in spiking networks (<xref ref-type="bibr" rid="bib56">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib70">Tetzlaff et al., 2012</xref>; <xref ref-type="bibr" rid="bib39">Ly et al., 2012</xref>; <xref ref-type="bibr" rid="bib15">Doiron et al., 2016</xref>). In particular, the attention-mediated decrease of <inline-formula><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> occurs for a wide range of timescale, ranging as low as 20 ms. However, for short timescales that match the higher gamma frequency range (approximately 60–70 Hz) this attentional modulation increases <inline-formula><mml:math id="inf358"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig14">Appendix 1—figure 6</xref>). This finding is consistent with reports of attention-mediated increases of neuronal synchrony on gamma frequency timescales(<xref ref-type="bibr" rid="bib20">Fries et al., 2001</xref>; <xref ref-type="bibr" rid="bib6">Buia and Tiesinga, 2008</xref>), particularly when inhibitory circuits are engaged (<xref ref-type="bibr" rid="bib32">Kim et al., 2016</xref>).</p></sec><sec id="s2-6"><title>Attention can simultaneously increase stimulus gain and decrease noise covariance</title><p>An important neural correlate of attention is enhanced stimulus response gain (<xref ref-type="bibr" rid="bib41">McAdams and Maunsell, 2000</xref>). The previous section outlines how the recruitment of recurrent inhibitory feedback by attention reduces response variability. However, inhibitory feedback is also a common gain control mechanism, and increased inhibition reduces response gain through the same mechanism that dampens population variability (<xref ref-type="bibr" rid="bib69">Sutherland et al., 2009</xref>). Thus it is possible that the decorrelating effect of attention in our model may also reduce stimulus response gain as well, which would make the model inconsistent with experimental data.</p><p>To insert a bottom-up stimulus <inline-formula><mml:math id="inf359"><mml:mi>s</mml:mi></mml:math></inline-formula> in our model we let the attention-independent background input have a stimulus term: <inline-formula><mml:math id="inf360"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf361"><mml:msub><mml:mi>k</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> is the feedforward stimulus gain to population <inline-formula><mml:math id="inf362"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf363"><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the background input that is both attention and stimulus independent. Our model captures a bulk firing rate <inline-formula><mml:math id="inf364"><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> rather than a population model with distributed tuning. Because of this the stimulus <inline-formula><mml:math id="inf365"><mml:mi>s</mml:mi></mml:math></inline-formula> should either be conceived as the contrast of an input, or the population conceived as a collection of identically-tuned neurons (i.e a single cortical column).</p><p>Straightforward analysis shows that the stimulus response gain of the excitatory population can be written as (Materials and methods: Computing stimulus response gain):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>≡</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msqrt><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:msqrt></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>If <inline-formula><mml:math id="inf366"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> then <inline-formula><mml:math id="inf367"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>∝</mml:mo><mml:msqrt><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:math></inline-formula>, and thus any attentional modulation that reduces population variability will necessarily reduce population stimulus sensitivity. However, for <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> the second term in <xref ref-type="disp-formula" rid="equ8">Equation (8)</xref> can counteract this effect and decouple stimulus sensitivity and variability modulations.</p><p>Consider the example attentional path (<xref ref-type="fig" rid="fig4">Figure 4g</xref>) with the extreme choice of <inline-formula><mml:math id="inf369"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf370"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. In this case attention causes an increase in <inline-formula><mml:math id="inf371"><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7a,b</xref>), while simultaneously causing a decrease in <inline-formula><mml:math id="inf372"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5a,b</xref>). This is a robust effect, as seen by the region in (<inline-formula><mml:math id="inf373"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) space for which the change in <inline-formula><mml:math id="inf374"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> from the unattended state is negative, and the change in <inline-formula><mml:math id="inf375"><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> is positive (green region, <xref ref-type="fig" rid="fig7">Figure 7c</xref>). Further, for fixed <inline-formula><mml:math id="inf376"><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> the proportion of the gray rectangle that the green region occupies increases with <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7d</xref>). Thus, the decoupling of attentional effects on population variability and stimulus sensitivity is robust to both attentional path (<inline-formula><mml:math id="inf378"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and feedforward gain (<inline-formula><mml:math id="inf379"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) choices. The condition that <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> implies that feedforward stimuli must directly target excitatory neurons to a larger degree than inhibitory neurons (or at least the inhibitory neurons subject to attentional modulation). This gives us a complementary prediction to the one from the previous section: while top-down attention favors inhibitory neurons, the bottom-up stimulus favors excitatory neurons.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.009</object-id><label>Figure 7.</label><caption><title>Attention model can capture increase in stimulus response gain <inline-formula><mml:math id="inf381"><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> despite decrease in population variance <inline-formula><mml:math id="inf382"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>.</title><p>(<bold>a</bold>) Attentional path through (<inline-formula><mml:math id="inf383"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) space shows an increase in stimulus response gain. The shown path is the same path as in <xref ref-type="fig" rid="fig5">Figure 5</xref>. (<bold>b</bold>) Values of <inline-formula><mml:math id="inf384"><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> along the path depicted in a. (<bold>c</bold>) The green region in (<inline-formula><mml:math id="inf385"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) space denotes where <inline-formula><mml:math id="inf386"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf387"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Black lines are iso-lines of covariance and gain, along which those quantities do not change. (<bold>d</bold>) Percent area of the green region in c out of a constant rectangle, as the feedforward stimulus gain <inline-formula><mml:math id="inf388"><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> increases, with <inline-formula><mml:math id="inf389"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> held constant.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.009">http://dx.doi.org/10.7554/eLife.23978.009</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-fig7-v3"/></fig></p><p>In total, our model of attentional modulation in recurrently coupled excitatory and inhibitory cortical networks subject to global fluctuations satisfies three main neural correlates of attention: (1) increase in excitatory firing rates and in (2) stimulus-response gain, with a (3) decrease in pairwise excitatory neuron co-variability.</p></sec><sec id="s2-7"><title>Impact of attentional modulation on neural coding</title><p>Attention serves to enhance cognitive performance, especially on discrimination tasks that are difficult (<xref ref-type="bibr" rid="bib45">Moore and Zirnsak, 2017</xref>). Thus, it is expected that the attention-mediated reduction in population variability and increase in stimulus response gain subserve an enhanced stimulus estimation (<xref ref-type="bibr" rid="bib8">Cohen and Maunsell, 2009</xref>; <xref ref-type="bibr" rid="bib63">Ruff and Cohen, 2014</xref>). In this section we investigate how the attentional modulation outlined in the previous sections affects stimulus coding by the population.</p><p>As mentioned above our simplified mean field model (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>) considers only a bulk response, where any individual neuron tuning is lost. As such a proper analysis of population coding is not possible. Nonetheless, our model has two basic features often associated with enhanced coding, decreased population variability (<xref ref-type="fig" rid="fig5">Figure 5</xref>) and increased stimulus-response gain (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><p>Fisher information (<xref ref-type="bibr" rid="bib3">Averbeck et al., 2006</xref>; <xref ref-type="bibr" rid="bib4">Beck et al., 2011</xref>) gives a lower bound on the variance of a stimulus estimate constructed from noisy population responses, and is an often used metric for population coding. The linear Fisher information (<xref ref-type="bibr" rid="bib4">Beck et al., 2011</xref>) <inline-formula><mml:math id="inf390"><mml:msub><mml:mtext>FI</mml:mtext><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> computed from our two-dimensional recurrent network is:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf391"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf392"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>α</mml:mi></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf393"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The important result is that <inline-formula><mml:math id="inf394"><mml:msub><mml:mtext>FI</mml:mtext><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is invariant with attention, meaning that attention does not increase the network’s capacity to estimate the stimulus <inline-formula><mml:math id="inf395"><mml:mi>s</mml:mi></mml:math></inline-formula>.</p><p>While the proof of <xref ref-type="disp-formula" rid="equ9">Equation (9)</xref> is straightforward and applies to our recurrent excitatory-inhibitory population (see Materials and methods: Fisher information), the invariance of the total information <inline-formula><mml:math id="inf396"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with attention is most easily understood by analogy with an uncoupled, one-dimensional excitatory population (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). Without coupling, the input to the population is simply <inline-formula><mml:math id="inf397"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which is then passed through the firing rate nonlinearity <inline-formula><mml:math id="inf398"><mml:msub><mml:mi>f</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>. In this case the gain is <inline-formula><mml:math id="inf399"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and assuming a linear transfer the population variance is <inline-formula><mml:math id="inf400"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. In total the linear Fisher information from the uncoupled population is then:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>E</mml:mi><mml:mtext>uc</mml:mtext></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>k</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.010</object-id><label>Figure 8.</label><caption><title>Attention improves stimulus estimation by the excitatory population embedded within excitatory (<inline-formula><mml:math id="inf401"><mml:mi>E</mml:mi></mml:math></inline-formula>)-inhibitory (<inline-formula><mml:math id="inf402"><mml:mi>I</mml:mi></mml:math></inline-formula>) network.</title><p>(<bold>a</bold>) Top: For a uncoupled excitatory population, the stimulus response gain <inline-formula><mml:math id="inf403"><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> increases with attention. Turquoise: unattended state; orange: attended state. Bottom: Population variance <inline-formula><mml:math id="inf404"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> increases with attention. Stimulus-response curves same as above. Input variance is computed from all input to a population, including external noise and recurrent coupling. The Fisher information for the uncoupled <inline-formula><mml:math id="inf405"><mml:mi>E</mml:mi></mml:math></inline-formula> population is constant with attention because the squared gain <inline-formula><mml:math id="inf406"><mml:msubsup><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf407"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> increase proportionally (<bold>b</bold>) Same as (a) but for the <inline-formula><mml:math id="inf408"><mml:mi>E</mml:mi></mml:math></inline-formula> population within the <inline-formula><mml:math id="inf409"><mml:mrow><mml:mi>E</mml:mi><mml:mo>-</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> network. Top: <inline-formula><mml:math id="inf410"><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> increases with attention. Bottom: <inline-formula><mml:math id="inf411"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> decreases with attention, because the net input variance of the <inline-formula><mml:math id="inf412"><mml:mi>E</mml:mi></mml:math></inline-formula> population decreases with attention. (<bold>c</bold>) Total Fisher information for coupled E-I populations is constant with attention. By contrast, the Fisher information of the excitatory component <inline-formula><mml:math id="inf413"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> increases with attention.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.010">http://dx.doi.org/10.7554/eLife.23978.010</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-fig8-v3"/></fig></p><p>The proportion <inline-formula><mml:math id="inf414"><mml:msubsup><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> by which attention increases the squared gain (<xref ref-type="fig" rid="fig8">Figure 8a</xref>, top) is exactly matched by the attention related increase in population variance (<xref ref-type="fig" rid="fig8">Figure 8a</xref>, bottom), resulting in cancellation of any attention-dependent terms in <inline-formula><mml:math id="inf415"><mml:msub><mml:mtext>FI</mml:mtext><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>The majority of projection neurons in the neocortex are excitatory, so we now consider the stimulus estimation from a readout of only the excitatory population. Combining our previous results we obtain:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:msub><mml:mtext>FI</mml:mtext><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>-</mml:mo><mml:mrow><mml:msubsup><mml:mi>J</mml:mi><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Restricting the readout to be from only the excitatory population drastically reduces the total information (compare <inline-formula><mml:math id="inf416"><mml:msub><mml:mtext>FI</mml:mtext><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf417"><mml:msub><mml:mtext>FI</mml:mtext><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> in <xref ref-type="fig" rid="fig8">Figure 8c</xref>). As with the uncoupled population the response gain <inline-formula><mml:math id="inf418"><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> of the excitatory neurons in the coupled population increases with attention (<xref ref-type="fig" rid="fig8">Figure 8b</xref>, top). Yet unlike the uncoupled population the net input variability to the <inline-formula><mml:math id="inf419"><mml:mi>E</mml:mi></mml:math></inline-formula> population is reduced by attention through a cancelation of the external variability <inline-formula><mml:math id="inf420"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> via inhibition (<xref ref-type="fig" rid="fig8">Figure 8b</xref>, bottom). These two components combine so that despite <inline-formula><mml:math id="inf421"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, we have that <inline-formula><mml:math id="inf422"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> <italic>does</italic> increase with attention (<xref ref-type="fig" rid="fig8">Figure 8c</xref>). In sum, even though the total stimulus information in the network does not change with attention, the amount of information extractable from the excitatory population increases, which could lead to improved downstream stimulus estimation in the attended state.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Using population recordings from visual area V4 we identified rank one structure in the mapping of population spike count covariability between unattended and attended states. We used this finding to motivate an excitatory-inhibitory cortical circuit model that captures both the attention-mediated increases in the firing rate and stimulus response gain, as well as decreases in noise correlations. Our model accomplishes this with only an attention dependent shift in the overall excitability of the cortical population, in contrast to a scheme where distinct biophysical mechanisms would be responsible for respective firing rate and noise correlations modulations. The model makes two key predictions about how stimulus and modulatory inputs are distributed over the excitatory-inhibitory cortical circuit. First, top-down attentional signals must affect inhibitory neurons more than excitatory neurons to allow a better damping of global fluctuations in the attended state. Second, bottom-up stimulus information must be biased towards excitatory cells to permit higher gain in the attended state. In total, the increased response gain and decreased correlations enhance the flow of information when the readout is confined to the excitatory population.</p><sec id="s3-1"><title>Candidate physiological mechanisms for attentional modulation</title><p>Our model does not consider a specific type of inhibitory neuron, and rather models a generic recurrent excitatory-inhibitory circuit. However, inhibitory circuits in cortex are complex, with at least three distinct interneuron types being prominent in many areas: parvalbumin- (PV), somatostatin- (SOM), and vasointestinal peptide-expressing (VIP) interneurons (<xref ref-type="bibr" rid="bib62">Rudy et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Pfeffer et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Kepecs and Fishell, 2014</xref>). In mouse visual cortex, both SOM and PV cells form recurrent circuits with pyramidal cells, with PV cells having stronger inhibitory projections to pyramidal cells than those of SOM cells (<xref ref-type="bibr" rid="bib52">Pfeffer et al., 2013</xref>). Furthermore, PV and SOM neurons directly inhibit one another, with the SOM to PV connection being stronger than the PV to SOM connection (<xref ref-type="bibr" rid="bib52">Pfeffer et al., 2013</xref>). Finally, VIP cells project strongly to SOM cells (<xref ref-type="bibr" rid="bib52">Pfeffer et al., 2013</xref>) and are activated from inputs outside of the circuit (<xref ref-type="bibr" rid="bib36">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="bib21">Fu et al., 2014</xref>), making them an attractive target for modulation. Recent studies in visual, auditory, and somatosensory cortical circuits show that VIP cell activation provides an active disinhibition of pyramidal cells via a suppression of SOM cells (<xref ref-type="bibr" rid="bib31">Kepecs and Fishell, 2014</xref>). Basal forebrain (BF) stimulation modulates both muscarinic and nicotinic ACh receptors (mAChRs and nAChRs respectively) in a fashion that mimics attentional modulation (<xref ref-type="bibr" rid="bib1">Alitto and Dan, 2012</xref>). In particular, the recruitment of VIP cell activity in vivo through BF stimulation is strongly dependent on both the muscarinic and nicotinic cholinergic pathways (<xref ref-type="bibr" rid="bib1">Alitto and Dan, 2012</xref>; <xref ref-type="bibr" rid="bib34">Kuchibhotla et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Fu et al., 2014</xref>), and it has thus been hypothesized VIP cells activation could be an important component of attentional modulation (<xref ref-type="bibr" rid="bib1">Alitto and Dan, 2012</xref>; <xref ref-type="bibr" rid="bib53">Poorthuis et al., 2014</xref>).</p><p>If we consider the inhibitory population in our model to be PV interneurons then the recruitment of VIP cell activity via top-down cholinergic pathways is consistent with our attentional model in two ways. First, activation of the VIP <inline-formula><mml:math id="inf423"><mml:mo>→</mml:mo></mml:math></inline-formula> SOM <inline-formula><mml:math id="inf424"><mml:mo>→</mml:mo></mml:math></inline-formula> pyramidal cell pathway provides a disinhibition to pyramidal cells, modeled simply as an overall depolarization to pyramidal cells in the attended state (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Second, the activation of the VIP <inline-formula><mml:math id="inf425"><mml:mo>→</mml:mo></mml:math></inline-formula> SOM <inline-formula><mml:math id="inf426"><mml:mo>→</mml:mo></mml:math></inline-formula> PV cell pathway disinhibits PV cells, and the strong SOM <inline-formula><mml:math id="inf427"><mml:mo>→</mml:mo></mml:math></inline-formula> PV projection would suggest that the disinhibition is sizable as required by our model (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). Finally, a recent study in mouse medial prefrontal cortex reports that identified PV interneurons show an attention related increase in activity, and that optogenetic silencing of PV neurons impairs attentional processing (<xref ref-type="bibr" rid="bib32">Kim et al., 2016</xref>).</p><p>However, our logic is perhaps overly simplistic and neglects the direct modulation of SOM cells via muscarinic and nicotinic cholinergic pathways (<xref ref-type="bibr" rid="bib1">Alitto and Dan, 2012</xref>; <xref ref-type="bibr" rid="bib34">Kuchibhotla et al., 2017</xref>) that could compromise the disinhibitory pathways. Further, there is evidence of a direct ACh modulation of PV cells (<xref ref-type="bibr" rid="bib12">Disney et al., 2014</xref>) as opposed to through a disinhibitory pathway. Finally, there may be important differences across both species (mouse vs. primate) and visual area (V1 vs. V4) that fundamentally change the pyramidal, PV, SOM, and VIP circuit that is understood from mouse V1 (<xref ref-type="bibr" rid="bib52">Pfeffer et al., 2013</xref>). Future studies in the inhibitory to excitatory circuitry of primate visual cortex, and its attentional modulation via neuromodulation, are required to navigate these issues.</p><p>Finally, the simultaneous increase in response gain and decrease in noise correlations with attention requires excitatory neurons to be more sensitive to bottom-up visual stimulus than inhibitory neurons (<inline-formula><mml:math id="inf428"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <xref ref-type="fig" rid="fig7">Figure 7</xref>). In mouse visual cortex, GABAergic interneurons show overall less stimulus selectivity than pyramidal neurons (<xref ref-type="bibr" rid="bib66">Sohya et al., 2007</xref>), however this involves both direct feedforward and recurrent contributions to stimulus tuning. While our model simplified the feedforward stimulus gain <inline-formula><mml:math id="inf429"><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf430"><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> to be constant with attention, it is known that attention also modulates feedforward gain through presynaptic nACh receptors (<xref ref-type="bibr" rid="bib13">Disney et al., 2007</xref>). Notably, nAChRs are found at thalamocortical synapses onto layer 4 excitatory cells and not onto inhibitory neurons, suggesting that <inline-formula><mml:math id="inf431"><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> would increase with attention while <inline-formula><mml:math id="inf432"><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> would not. Thus, <inline-formula><mml:math id="inf433"><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> should also increase with attention while <inline-formula><mml:math id="inf434"><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> should not, further supporting that <inline-formula><mml:math id="inf435"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s3-2"><title>Modeling global network fluctuations and their modulation</title><p>Our model considered the source of global fluctuations as external to the network. This choice was due in part to difficulties in producing global, long timescale fluctuations through strictly internal coupling (<xref ref-type="bibr" rid="bib56">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib60">Rosenbaum et al., 2017</xref>). Our model assumed that the intensity of these external input fluctuation were independent of attention. Rather, attention shifted the operating point of the network such that the transfer of input variability to population-wide output activity was attenuated in the attended state.</p><p>Recent analysis of population recordings show that generative models of spike trains that consider gain fluctuations in conjunction with standard spike emission variability capture much of the variability of cortical dynamics (<xref ref-type="bibr" rid="bib55">Rabinowitz et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Lin et al., 2015</xref>). Further, these gain fluctuations are well approximated by a one-dimensional, global stochastic process affecting all neurons in the population (<xref ref-type="bibr" rid="bib16">Ecker et al., 2014</xref>; <xref ref-type="bibr" rid="bib55">Rabinowitz et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Lin et al., 2015</xref>; <xref ref-type="bibr" rid="bib17">Ecker et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Engel et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Whiteway and Butts, 2017</xref>). When these techniques are applied to population recordings subject to attentional modulation, the global gain fluctuations are considerably reduced in the attended state (<xref ref-type="bibr" rid="bib55">Rabinowitz et al., 2015</xref>; <xref ref-type="bibr" rid="bib17">Ecker et al., 2016</xref>). Our assumption that external input fluctuations to our network are attention-invariant is consistent with this statistical analysis since it is necessarily constructed from only output activity. Nevertheless, another potential model is that the reduction in population variability is simply inherited from an attention-mediated suppression of the global input fluctuations. Unfortunately, it is difficult to distinguish between these two mechanisms when restricted to only output spiking activity.</p><p>However, a model where output variability reductions are simply inherited from external inputs suffers from two criticisms. First, it begs the question: what is the mechanism behind the shift in input variability? Second, our model requires only an increase in the external depolarization to excitatory and inhibitory populations to account for all attentional correlates. An inheritance model would necessarily decouple the attentional mechanisms behind increases in network firing rate (still requiring a depolarization) and the decrease in global input variability. Thus, our model offers a parsimonious and biologically motivated explanation of these neural correlates of attention. Further work dissecting the various external and internal sources of variability to cortical networks, and their attentional modulation, is needed to properly validate or refute these different models.</p></sec><sec id="s3-3"><title>Attentional modulation of neural coding through inhibition</title><p>Our network model assumed attention-invariant external fluctuations and weak recurrent inputs, permitting a linear analysis of network activity. As a consequence the linear information transfer by the entire population was attention-invariant (<xref ref-type="fig" rid="fig8">Figure 8</xref>), because attention modulated the network’s transfer of signal and noise equivalently. However, this invariance was only apparent if the decoder had access to both the excitatory and inhibitory populations. However, most of the neurons in cortex that project between areas are excitatory. When the decoder was restricted to only the activity of the excitatory population then our analysis uncovered two main results. First, the excitatory population carried less information than the combined excitatory-inhibitory activity, suggesting an inherently suboptimal coding scheme used by the cortex. Second, the attention-mediated modulation of the inhibitory neurons increased the information carried by the excitatory population. This agrees with the wealth of studies that show that attention improves behavioral performance on stimulus discrimination tasks.</p><p>Determining the impact of population-wide spiking variability on neural coding is complicated (<xref ref-type="bibr" rid="bib3">Averbeck et al., 2006</xref>; <xref ref-type="bibr" rid="bib33">Kohn et al., 2016</xref>). A recent theoretical study has shown that noise correlations that limit stimulus information must be parallel to the direction in which population activity encodes the stimulus (<xref ref-type="bibr" rid="bib46">Moreno-Bote et al., 2014</xref>). The fluctuations in our network satisfy this criteria, albeit trivially since all neurons share the same stimulus input. Indeed, in our network the external inputs appear to the network as <inline-formula><mml:math id="inf436"><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, meaning that fluctuations from the noise source <inline-formula><mml:math id="inf437"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are indistinguishable from fluctuations in the stimulus <inline-formula><mml:math id="inf438"><mml:mi>s</mml:mi></mml:math></inline-formula>. This is an oversimplified view and assumes that the decoder treats the neurons as indistinguishable from one another, at odds with classic work in population coding (<xref ref-type="bibr" rid="bib54">Pouget et al., 2000</xref>). Extending our network to include distributed tuning and feature-based recurrent connectivity is a natural next step (<xref ref-type="bibr" rid="bib5">Ben-Yishai et al., 1995</xref>; <xref ref-type="bibr" rid="bib61">Rubin et al., 2015</xref>). To do this the spatial scales of feedforward tuning, recurrent projections, external fluctuations, as well as attention modulation must all be specified. It is not clear how noise correlations will depend on these choices yet work in spatially distributed balanced networks shows that solutions can be complex (<xref ref-type="bibr" rid="bib60">Rosenbaum et al., 2017</xref>).</p><p>The role of inhibition in shaping cortical function is a longstanding topic of study (<xref ref-type="bibr" rid="bib29">Isaacson and Scanziani, 2011</xref>), including recent work showing inhibition can actively decorrelate cortical responses (<xref ref-type="bibr" rid="bib56">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib70">Tetzlaff et al., 2012</xref>; <xref ref-type="bibr" rid="bib39">Ly et al., 2012</xref>). Our work gives a concrete example of how this decorrelation can be gated and used to control the flow of information. Of interest are tasks that probe a distributed population where attention again decreases noise correlations between neurons with similar stimulus preference, yet <italic>increases</italic> noise correlations between cells with dissimilar stimulus preference (<xref ref-type="bibr" rid="bib63">Ruff and Cohen, 2014</xref>). The circuit mechanisms underlying this neural correlate of attention are unclear. However, there is ample work in understanding how recurrent inhibition shapes cortical activity in distributed populations (<xref ref-type="bibr" rid="bib29">Isaacson and Scanziani, 2011</xref>), including in models of attentional circuits (<xref ref-type="bibr" rid="bib2">Ardid et al., 2007</xref>; <xref ref-type="bibr" rid="bib6">Buia and Tiesinga, 2008</xref>). Adapting our model to include distributed tuning is an important next step and will be a better framework to discuss the coding consequences of the attentional modulation circuits proposed in our study.</p></sec></sec><sec id="s4"><title>Methods and materials</title><sec id="s4-1"><title>Data preparation</title><p>Data was collected by from two rhesus monkeys with microelectrode arrays implanted bilaterally in V4 as they performed an orientation-change detection task (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) (<xref ref-type="bibr" rid="bib8">Cohen and Maunsell, 2009</xref>). All animal procedures were in accordance with the Institutional Animal Care and Use Committee of Harvard Medical School. Two oriented Gabor stimuli flashed on and off several times, until one of them changed orientation. The task of the monkey was to then saccade to the stimulus that changed. Each recording session consisted of at least four blocks of trials in which the monkey’s attention was cued to the left or right. We excluded from the analysis instruction trials which occurred at the start of each block to cue the monkey to one side to attend to, catch trials in which the monkey was rewarded just for fixating, and trials in which the monkey did not perform the task correctly. Moreover, the first and last stimulus presentations in each trial were not analyzed, to prevent transients due to stimulus appearance or change from affecting the results. The total number of trials included in the analysis from all the recording sessions was <inline-formula><mml:math id="inf439"><mml:mrow><mml:mn>42</mml:mn><mml:mo>,</mml:mo><mml:mn>496</mml:mn></mml:mrow></mml:math></inline-formula>. Each trial consisted of between <inline-formula><mml:math id="inf440"><mml:mn>3</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf441"><mml:mn>12</mml:mn></mml:math></inline-formula> stimulus presentations, of which all but the first and last were analyzed.</p><p>Recordings from the left and right hemispheres of each monkey were analyzed separately because the activities of the neurons in opposite hemispheres had near-zero correlations (<xref ref-type="bibr" rid="bib8">Cohen and Maunsell, 2009</xref>). Neurons in the right hemisphere were considered to be in the attended state when the attentional cue was on the left, and vice-versa. We note that because our criteria for choosing which trials and units to analyze were based on different needs for data analysis compared to the original study (<xref ref-type="bibr" rid="bib8">Cohen and Maunsell, 2009</xref>) the specific firing rates and covariances differ quantitatively from those previously reported.</p><p>In monkey 1, an average of <inline-formula><mml:math id="inf442"><mml:mn>51.1</mml:mn></mml:math></inline-formula> (min <inline-formula><mml:math id="inf443"><mml:mn>35</mml:mn></mml:math></inline-formula>, max <inline-formula><mml:math id="inf444"><mml:mn>80</mml:mn></mml:math></inline-formula>) units were analyzed from the right hemisphere, and an average of <inline-formula><mml:math id="inf445"><mml:mn>27.5</mml:mn></mml:math></inline-formula> (min <inline-formula><mml:math id="inf446"><mml:mn>14</mml:mn></mml:math></inline-formula>, max <inline-formula><mml:math id="inf447"><mml:mn>56</mml:mn></mml:math></inline-formula>) units were analyzed from the left hemisphere. From monkey 2, an average of <inline-formula><mml:math id="inf448"><mml:mn>56.6</mml:mn></mml:math></inline-formula> (min <inline-formula><mml:math id="inf449"><mml:mn>43</mml:mn></mml:math></inline-formula>, max <inline-formula><mml:math id="inf450"><mml:mn>71</mml:mn></mml:math></inline-formula>) units from the right hemisphere, and an average of <inline-formula><mml:math id="inf451"><mml:mn>37.7</mml:mn></mml:math></inline-formula> (min <inline-formula><mml:math id="inf452"><mml:mn>32</mml:mn></mml:math></inline-formula>, max <inline-formula><mml:math id="inf453"><mml:mn>46</mml:mn></mml:math></inline-formula>) units from the left hemisphere were analyzed. From each recording, spikes falling between <inline-formula><mml:math id="inf454"><mml:mn>60</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf455"><mml:mn>260</mml:mn></mml:math></inline-formula> ms from stimulus onset were considered for the firing rate analysis, to account for the latency of neuronal responses in V4.</p></sec><sec id="s4-2"><title>Comparing change in covariance to change in variance</title><p>Let <inline-formula><mml:math id="inf456"><mml:msup><mml:mi>S</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:math></inline-formula> be the matrix containing spike counts of the neurons on trials in which they are in the unattended state, and <inline-formula><mml:math id="inf457"><mml:msup><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:msup></mml:math></inline-formula> the matrix containing spike counts of the neurons on trials in which they are in the attended state. Denote the unattended spike count covariance matrix by <inline-formula><mml:math id="inf458"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the attended one by <inline-formula><mml:math id="inf459"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Attentional changes in covariance and variance were measured both on average (<xref ref-type="fig" rid="fig1">Figure 1c</xref>) and as distributions (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). The distributions of the normalized differences<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext> and </mml:mtext></mml:mstyle><mml:mspace width="1em"/><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>reveal a concentration of negative covariance changes, and a distribution of variance changes symmetric about zero. Here, <inline-formula><mml:math id="inf460"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf461"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf462"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf463"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) are vectors containing covariance (variance) values of the entire data set. Note that the distributions are bounded between <inline-formula><mml:math id="inf464"><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf465"><mml:mn>2</mml:mn></mml:math></inline-formula> by construction.</p></sec><sec id="s4-3"><title>Solving systems of equations by error minimization</title><p>When solving systems of the form of <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref> in order to quantify the fit of the model, a nonlinear equation solver (fminunc) in MATLAB was used. The solver found minima of an objective function which we defined as the Euclidean norm of the difference of the approximation of the attended covariance matrix and the original attended covariance matrix, in other words, the error of the approximation:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-4"><title>Shuffled covariance matrices</title><p>For finite population sizes (<inline-formula><mml:math id="inf466"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) we expect our algorithm to extract some low-rank structure between arbitrary covariance matrices. Let <inline-formula><mml:math id="inf467"><mml:msqrt><mml:msup><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:msup></mml:msqrt></mml:math></inline-formula> be the principal square root of the attended covariance matrix, the unique positive-semidefinite square root of a positive-semidefinite matrix. Consider the symmetric matrix <inline-formula><mml:math id="inf468"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> computed from the a random permutation of the upper-triangular entries of <inline-formula><mml:math id="inf469"><mml:msqrt><mml:msup><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:msup></mml:msqrt></mml:math></inline-formula>. Finally, let <inline-formula><mml:math id="inf470"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The square root-permutation-squaring procedure guarantees a positive-semidefinite matrix, as the square of any matrix is positive-semidefinite. Shuffling removes any relation between <inline-formula><mml:math id="inf471"><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi mathvariant="bold">𝐔</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf472"><mml:msubsup><mml:mi>C</mml:mi><mml:mtext>shuf</mml:mtext><mml:mi>A</mml:mi></mml:msubsup></mml:math></inline-formula>, and any remaining detected structure would be due to finite sampling. The shuffled covariance gain <inline-formula><mml:math id="inf473"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> provides the prediction <inline-formula><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>:=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>∘</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf475"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> measures the relation between <inline-formula><mml:math id="inf476"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf477"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Synthetic data shows that as population size <inline-formula><mml:math id="inf478"><mml:mi>N</mml:mi></mml:math></inline-formula> becomes large the coefficient <inline-formula><mml:math id="inf479"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> approaches 0 (Appendix: Detected structure in random covariance matrices is a finite-size effect).</p></sec><sec id="s4-5"><title>Upper bound covariance matrices</title><p>The covariance matrices <inline-formula><mml:math id="inf480"><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi mathvariant="bold">𝐔</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf481"><mml:msup><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mi mathvariant="bold">𝐀</mml:mi></mml:msup></mml:math></inline-formula> are estimates obtained from a finite number of trials, and any estimation error will compromise the ability to detect rank one structure of <inline-formula><mml:math id="inf482"><mml:msub><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula>. Here we outline an upper bound for the model performance based on a finite number of trials over which the covariance matrices were originally estimated. Let <inline-formula><mml:math id="inf483"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msup><mml:mo>:=</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>∘</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf484"><mml:mover accent="true"><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> minimizing the <inline-formula><mml:math id="inf485"><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> norm of <inline-formula><mml:math id="inf486"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:msup><mml:mo>:=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐠𝐠</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>∘</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. We remark that <inline-formula><mml:math id="inf487"><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msup></mml:math></inline-formula> perfectly decomposes according to the statistical model in <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref>. We used <inline-formula><mml:math id="inf488"><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msup></mml:math></inline-formula> to generate an artificial set of <inline-formula><mml:math id="inf489"><mml:mi>N</mml:mi></mml:math></inline-formula> correlated Poisson spike counts, using an algorithm based on a latent multivariate gaussian model (<xref ref-type="bibr" rid="bib40">Macke et al., 2009</xref>). We sampled these population spike counts with a fixed number of trials (<inline-formula><mml:math id="inf490"><mml:mi>M</mml:mi></mml:math></inline-formula>) with <inline-formula><mml:math id="inf491"><mml:mi>D</mml:mi></mml:math></inline-formula> be the resulting <inline-formula><mml:math id="inf492"><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix of Poisson samples for each process. Let <inline-formula><mml:math id="inf493"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> be the 'upper bound' covariance matrix: a finite trial sampling approximation to the perfectly decomposable matrix <inline-formula><mml:math id="inf494"><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msup></mml:math></inline-formula>. Finally, we employ our algorithm to give <inline-formula><mml:math id="inf495"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>:=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the vector <inline-formula><mml:math id="inf496"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> minimizes the <inline-formula><mml:math id="inf497"><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> norm of the error.</p><p>Since <inline-formula><mml:math id="inf498"><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msup></mml:math></inline-formula> is perfectly decomposable then for <inline-formula><mml:math id="inf499"><mml:mrow><mml:mi>M</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> we have <inline-formula><mml:math id="inf500"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus in the large <inline-formula><mml:math id="inf501"><mml:mi>M</mml:mi></mml:math></inline-formula> limit the coefficient <inline-formula><mml:math id="inf502"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> between elements of <inline-formula><mml:math id="inf503"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf504"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> converges to 1 (Appendix: Performance limited by available number of trials). However, for finite <inline-formula><mml:math id="inf505"><mml:mi>M</mml:mi></mml:math></inline-formula> we have that <inline-formula><mml:math id="inf506"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, solely due to inaccuracies in estimating <inline-formula><mml:math id="inf507"><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msup></mml:math></inline-formula> with <inline-formula><mml:math id="inf508"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. To account for the possibility of particular strings of realizations <inline-formula><mml:math id="inf509"><mml:mi>D</mml:mi></mml:math></inline-formula> introducing random biases into <inline-formula><mml:math id="inf510"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, we performed the following analysis on <inline-formula><mml:math id="inf511"><mml:mn>10</mml:mn></mml:math></inline-formula> independently generated upper-bound covariance matrices <inline-formula><mml:math id="inf512"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-6"><title>Leave-one-out cross-validation</title><p>Instead of solving the system consisting of all <xref ref-type="disp-formula" rid="equ2">Equations (2)</xref>, we remove one of them. Denote the complete set of equations by <inline-formula><mml:math id="inf513"><mml:mi>S</mml:mi></mml:math></inline-formula>, an individual equation as <inline-formula><mml:math id="inf514"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the set of equations with one of them removed as <inline-formula><mml:math id="inf515"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. We then solve the system <inline-formula><mml:math id="inf516"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Denote the solution by <inline-formula><mml:math id="inf517"><mml:msub><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We can then compare <inline-formula><mml:math id="inf518"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf519"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. We do this for <inline-formula><mml:math id="inf520"><mml:mrow><mml:mtext>max</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1000</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> possible systems <inline-formula><mml:math id="inf521"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The <inline-formula><mml:math id="inf522"><mml:mi>ρ</mml:mi></mml:math></inline-formula> of the vector of resulting <inline-formula><mml:math id="inf523"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup></mml:math></inline-formula> vs <inline-formula><mml:math id="inf524"><mml:msubsup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup></mml:math></inline-formula> values is a measure of how well the system can predict one of its elements, or in other words, how well the structure holds together when one element is taken out. This leave-one-out cross-validation was performed for the shuffled and the upper-bound cases as well.</p></sec><sec id="s4-7"><title>Mean field model</title><p>The mean spiking activity over the population <inline-formula><mml:math id="inf525"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf526"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:msubsup><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the spike train of excitatory neuron <inline-formula><mml:math id="inf527"><mml:mi>i</mml:mi></mml:math></inline-formula> of population <inline-formula><mml:math id="inf528"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf529"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of spikes from that neuron, and <inline-formula><mml:math id="inf530"><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:math></inline-formula> is the time of spike <inline-formula><mml:math id="inf531"><mml:mi>j</mml:mi></mml:math></inline-formula>. We follow previous studies (<xref ref-type="bibr" rid="bib70">Tetzlaff et al., 2012</xref>; <xref ref-type="bibr" rid="bib50">Ozeki et al., 2009</xref>; <xref ref-type="bibr" rid="bib35">Ledoux and Brunel, 2011</xref>) and consider the firing rate dynamics of the <inline-formula><mml:math id="inf532"><mml:mi>E</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf533"><mml:mi>I</mml:mi></mml:math></inline-formula> populations given by the system in <xref ref-type="disp-formula" rid="equ6">Equations (6)</xref>:<disp-formula id="equ15"><mml:math id="m15"><mml:mtable columnalign=" left right left right left right left right"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msqrt><mml:mi>χ</mml:mi></mml:msqrt><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msqrt><mml:mi>χ</mml:mi></mml:msqrt><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf534"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the attention independent drive to population <inline-formula><mml:math id="inf535"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf536"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the attention variable, and <inline-formula><mml:math id="inf537"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the maximal drive to population <inline-formula><mml:math id="inf538"><mml:mi>α</mml:mi></mml:math></inline-formula> due to attention. The parameter <inline-formula><mml:math id="inf539"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the coupling from population <inline-formula><mml:math id="inf540"><mml:mi>β</mml:mi></mml:math></inline-formula> to populations <inline-formula><mml:math id="inf541"><mml:mi>α</mml:mi></mml:math></inline-formula>. The stochastic processes <inline-formula><mml:math id="inf542"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf543"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf544"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the global fluctuations applied to the network. The excitatory and inhibitory populations have private fluctuations <inline-formula><mml:math id="inf545"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and also common fluctuations <inline-formula><mml:math id="inf546"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> given to both populations; the parameter <inline-formula><mml:math id="inf547"><mml:mi>χ</mml:mi></mml:math></inline-formula> scales the degree of private versus common fluctuations. We perform calculations for arbitrary <inline-formula><mml:math id="inf548"><mml:mi>χ</mml:mi></mml:math></inline-formula> and then take <inline-formula><mml:math id="inf549"><mml:mrow><mml:mi>χ</mml:mi><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to match the system given in <xref ref-type="disp-formula" rid="equ6">Equations (6)</xref>. The total intensity of fluctuations to population <inline-formula><mml:math id="inf550"><mml:mi>α</mml:mi></mml:math></inline-formula> is set by <inline-formula><mml:math id="inf551"><mml:msub><mml:mi>σ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula>. These simplified rate equations give an accurate picture of the long-timescale dynamics of networks of coupled spiking neuron models that are in the fluctuation driven regime (<xref ref-type="bibr" rid="bib35">Ledoux and Brunel, 2011</xref>). The operative timescale reflects a combination of synaptic and membrane integration; since we are interested in spiking covariance over time windows that are much longer than these, we take them to be unity for simplicity.</p><p>To give a quantitative match between the equilibrium statistics of the rate equations and the leaky integrate-and-fire (LIF) network simulations we take the transfer function <inline-formula><mml:math id="inf552"><mml:mi>f</mml:mi></mml:math></inline-formula> to be the inverse first passage time of an LIF neuron driven by white noise (<xref ref-type="bibr" rid="bib35">Ledoux and Brunel, 2011</xref>):<disp-formula id="equ16"><label>(15)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The parameter <inline-formula><mml:math id="inf553"><mml:msub><mml:mi>η</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> is the intensity of the external fluctuations given to the LIF neurons (Appendix: Spiking model). The membrane timescale <inline-formula><mml:math id="inf554"><mml:mi>τ</mml:mi></mml:math></inline-formula> gives the dimensions of 1/s to the firing rate <inline-formula><mml:math id="inf555"><mml:msub><mml:mi>r</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula>. The parameter <inline-formula><mml:math id="inf556"><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> denotes spike threshold while <inline-formula><mml:math id="inf557"><mml:msub><mml:mi>V</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula> is the reset potential. Model parameters are given in <xref ref-type="table" rid="tbl1">Table 1</xref>.<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.011</object-id><label>Table 1.</label><caption><p>Model Parameters.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.011">http://dx.doi.org/10.7554/eLife.23978.011</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Parameter</th><th>Description</th><th>Value</th></tr></thead><tbody><tr><td><inline-formula><mml:math id="inf558"><mml:mi>τ</mml:mi></mml:math></inline-formula></td><td>Time constants for membrane dynamics</td><td>0.01 s</td></tr><tr><td><inline-formula><mml:math id="inf559"><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula></td><td>Spike Threshold</td><td>1</td></tr><tr><td><inline-formula><mml:math id="inf560"><mml:msub><mml:mi>V</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula></td><td>Spike Reset</td><td>0</td></tr><tr><td><inline-formula><mml:math id="inf561"><mml:msub><mml:mi>μ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula></td><td>Excitatory baseline bias</td><td>0.6089</td></tr><tr><td><inline-formula><mml:math id="inf562"><mml:msub><mml:mi>μ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula></td><td>Inhibitory baseline bias</td><td>0.5388</td></tr><tr><td><inline-formula><mml:math id="inf563"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td>Attentional modulation of excitatory bias</td><td>0.2624</td></tr><tr><td><inline-formula><mml:math id="inf564"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td>Attentional modulation of inhibitory bias</td><td>0.3608</td></tr><tr><td><inline-formula><mml:math id="inf565"><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula></td><td>Excitatory coupling constant</td><td>1.5</td></tr><tr><td><inline-formula><mml:math id="inf566"><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula></td><td>Inhibitory coupling constant</td><td>3</td></tr><tr><td><inline-formula><mml:math id="inf567"><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula></td><td>Amplitude of external noise to E population</td><td>0.3</td></tr><tr><td><inline-formula><mml:math id="inf568"><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula></td><td>Amplitude of external noise to I population</td><td>0.35</td></tr><tr><td><inline-formula><mml:math id="inf569"><mml:mi>c</mml:mi></mml:math></inline-formula></td><td>Proportion of common noise to E and I populations</td><td>1</td></tr><tr><td><inline-formula><mml:math id="inf570"><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula></td><td>Sensitivity of E population to stimulus input</td><td>1</td></tr><tr><td><inline-formula><mml:math id="inf571"><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula></td><td>Sensitivity of I population to stimulus input</td><td>0</td></tr></tbody></table></table-wrap></p><p>If the input fluctuations, <inline-formula><mml:math id="inf572"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf573"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf574"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are white noise processes then the nonlinearity in <inline-formula><mml:math id="inf575"><mml:mi>f</mml:mi></mml:math></inline-formula> makes the stochastic dynamics of <inline-formula><mml:math id="inf576"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf577"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> complicated (non-diffusive). To simply the analysis we consider <inline-formula><mml:math id="inf578"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the limiting process from:<disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msqrt><mml:msub><mml:mi>τ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>for <inline-formula><mml:math id="inf579"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf580"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf581"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This makes <inline-formula><mml:math id="inf582"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> sufficiently smooth in time (the same is true for <inline-formula><mml:math id="inf583"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf584"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>We restrict the coupling <inline-formula><mml:math id="inf585"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> such that for <inline-formula><mml:math id="inf586"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> the equilibrium point <inline-formula><mml:math id="inf587"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is stable and given by:<disp-formula id="equ18"><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mtr><mml:mlabeledtr><mml:mtd><mml:mtext>(16)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>For sufficiently small <inline-formula><mml:math id="inf588"><mml:msub><mml:mi>σ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> the fluctuations in population activity about the equilibrium firing rate, <inline-formula><mml:math id="inf589"><mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>α</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, obey the linearized stochastic system:<disp-formula id="equ19"><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mi>δ</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>χ</mml:mi></mml:msqrt><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msqrt><mml:mi>χ</mml:mi></mml:msqrt><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mtr><mml:mlabeledtr><mml:mtd><mml:mtext>(17)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mi>δ</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>χ</mml:mi></mml:msqrt><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msqrt><mml:mi>χ</mml:mi></mml:msqrt><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf590"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the slope of the transfer function <inline-formula><mml:math id="inf591"><mml:msub><mml:mi>f</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> evaluated at the equilibrium point <inline-formula><mml:math id="inf592"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ19">Equation (17)</xref> is a two dimensional Ornstein-Uhlenbeck process (<xref ref-type="bibr" rid="bib22">Gardiner, 2004</xref>) that is readily amenable to analysis.</p><sec id="s4-7-1"><title>Computing <inline-formula><mml:math id="inf593"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula></title><p>In matrix form the system <xref ref-type="disp-formula" rid="equ19">Equation(17)</xref> is written as:<disp-formula id="equ20"><label>(18)</label><mml:math id="m20"><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf594"><mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf595"><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and</p><p><inline-formula><mml:math id="inf596"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf597"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>χ</mml:mi></mml:msqrt></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>χ</mml:mi></mml:msqrt></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The stationary autocovariance function is computed as:<disp-formula id="equ21"><label>(19)</label><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="center center" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi></mml:mtd><mml:mtd><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext> if </mml:mtext></mml:mstyle><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext> if </mml:mtext></mml:mstyle><mml:mi>s</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf598"><mml:mi>s</mml:mi></mml:math></inline-formula> is a time lag and <inline-formula><mml:math id="inf599"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>D</mml:mi><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mi>D</mml:mi><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> is the variance matrix (Det and Tr denote the determinant and trace operations, respectively). Here, <inline-formula><mml:math id="inf600"><mml:mn>1</mml:mn></mml:math></inline-formula> is the <inline-formula><mml:math id="inf601"><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> identity matrix.</p><p>The covariance between populations <inline-formula><mml:math id="inf602"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf603"><mml:mi>β</mml:mi></mml:math></inline-formula> over long time scales is given by<disp-formula id="equ22"><label>(20)</label><mml:math id="m22"><mml:mrow><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the integration is performed over the appropriate element of the matrix <inline-formula><mml:math id="inf604"><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In particular, the long timescale variance of the excitatory population is given by (after some algebra):<disp-formula id="equ23"><label>(21)</label><mml:math id="m23"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msubsup><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We remark that the long timescale covariance matrix can alternatively be computed from <inline-formula><mml:math id="inf605"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib22">Gardiner, 2004</xref>). To obtain the compact expression for <inline-formula><mml:math id="inf606"><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> we have assumed symmetric coupling: <inline-formula><mml:math id="inf607"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>:=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf608"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>:=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf609"><mml:mrow><mml:mi>χ</mml:mi><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. These are not required for the main results of our study and merely ease the analysis of equations.</p></sec><sec id="s4-7-2"><title>Computing stimulus response gain</title><p>We decompose <inline-formula><mml:math id="inf610"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and define the gain of population <inline-formula><mml:math id="inf611"><mml:mi>α</mml:mi></mml:math></inline-formula> to stimulus <inline-formula><mml:math id="inf612"><mml:mi>s</mml:mi></mml:math></inline-formula> as <inline-formula><mml:math id="inf613"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>α</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>. The term <inline-formula><mml:math id="inf614"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> is obtained by differentiating <xref ref-type="disp-formula" rid="equ18">Equations (16)</xref>) with respect to <inline-formula><mml:math id="inf615"><mml:mi>s</mml:mi></mml:math></inline-formula>:<disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Solving the system of two equations for <inline-formula><mml:math id="inf616"><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> yields:<disp-formula id="equ25"><label>(22)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the sake of compactness we set <inline-formula><mml:math id="inf617"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to obtain the result in <xref ref-type="disp-formula" rid="equ8">Equation (8)</xref>.</p></sec></sec><sec id="s4-8"><title>Fisher information</title><p>Linear Fisher Information depends on the stimulus response gains and covariance matrix of the excitatory and inhibitory populations:<disp-formula id="equ26"><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd/></mml:mtr><mml:mlabeledtr><mml:mtd><mml:mtext>(23)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>When the input correlation <inline-formula><mml:math id="inf618"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>χ</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> we have:<disp-formula id="equ27"><label>(24)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>J</mml:mi><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ28"><label>(25)</label><mml:math id="m28"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>J</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ29"><label>(26)</label><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mi>χ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mi>χ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mi>χ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Inserting these expressions and those for <inline-formula><mml:math id="inf619"><mml:msub><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf620"><mml:msub><mml:mi>G</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> into <xref ref-type="disp-formula" rid="equ26">Equation (23)</xref> and simplifying yields:<disp-formula id="equ30"><label>(27)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:msub><mml:mtext>FI</mml:mtext><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>χ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We remark that <inline-formula><mml:math id="inf621"><mml:msub><mml:mtext>FI</mml:mtext><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is independent of <inline-formula><mml:math id="inf622"><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf623"><mml:msub><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> and thus independent of attentional modulation.</p><p>Notice that we have re-introduced the correlation constant <inline-formula><mml:math id="inf624"><mml:mi>χ</mml:mi></mml:math></inline-formula> into the equations, rather than only considering the limit <inline-formula><mml:math id="inf625"><mml:mrow><mml:mi>χ</mml:mi><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. If <inline-formula><mml:math id="inf626"><mml:mrow><mml:mi>χ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the excitatory and inhibitory populations are receiving completely identical noise. If this is the case, the correlation cancellation would be perfect, leading to infinite informational content, as can be seen in <xref ref-type="disp-formula" rid="equ30">Equation (27)</xref>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The research was supported by National Science Foundation grants NSF-DMS-1313225 (BD), NSF DMS-1517082 (BD), a grant from the Simons Foundation collaboration on the global brain (SCGB #325293MC;BD), NIH grants 4R00EY020844-03 and R01 EY022930 (MRC), a Whitehall Foundation Grant (MRC), Klingenstein-Simons Fellowship (MRC), a Sloan Research Fellowship (MRC), and a McKnight Scholar Award (MRC). We thank John Maunsell for the generous use of the data, and Kenneth Miller, Ashok Litwin-Kumar, Douglas Ruff, and Robert Rosenbaum for useful discussions.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>TK, Formal analysis, Investigation, Writing—original draft</p></fn><fn fn-type="con" id="con2"><p>GKO, Formal analysis, Investigation</p></fn><fn fn-type="con" id="con3"><p>MRC, Conceptualization, Funding acquisition, Investigation, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>BD, Conceptualization, Formal analysis, Supervision, Funding acquisition, Investigation, Writing—original draft, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All animal procedures were in accordance with the Institutional Animal Care and Use Committee of Harvard Medical School (Harvard IACUC protocol number: 04214).</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alitto</surname><given-names>HJ</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cell-type-specific modulation of neocortical activity by basal forebrain input</article-title><source>Frontiers in Systems Neuroscience</source><volume>6</volume><elocation-id>79</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2012.00079</pub-id><pub-id pub-id-type="pmid">23316142</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ardid</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>An integrated microcircuit model of attentional processing in the neocortex</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>8486</fpage><lpage>8495</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1145-07.2007</pub-id><pub-id pub-id-type="pmid">17687026</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural correlations, population coding and computation</article-title><source>Nature Reviews Neuroscience</source><volume>7</volume><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn1888</pub-id><pub-id pub-id-type="pmid">16760916</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>J</given-names></name><name><surname>Bejjanki</surname><given-names>VR</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Insights from a simple expression for linear fisher information in a recurrently connected population of spiking neurons</article-title><source>Neural Computation</source><volume>23</volume><fpage>1484</fpage><lpage>1502</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00125</pub-id><pub-id pub-id-type="pmid">21395435</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Yishai</surname><given-names>R</given-names></name><name><surname>Bar-Or</surname><given-names>RL</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Theory of orientation tuning in visual cortex</article-title><source>PNAS</source><volume>92</volume><fpage>3844</fpage><lpage>3848</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.9.3844</pub-id><pub-id pub-id-type="pmid">7731993</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buia</surname><given-names>CI</given-names></name><name><surname>Tiesinga</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Role of interneuron diversity in the cortical microcircuit for attention</article-title><source>Journal of Neurophysiology</source><volume>99</volume><fpage>2158</fpage><lpage>2182</lpage><pub-id pub-id-type="doi">10.1152/jn.01004.2007</pub-id><pub-id pub-id-type="pmid">18287553</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cardin</surname><given-names>JA</given-names></name><name><surname>Palmer</surname><given-names>LA</given-names></name><name><surname>Contreras</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Stimulus feature selectivity in excitatory and inhibitory neurons in primary visual cortex</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>10333</fpage><lpage>10344</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1692-07.2007</pub-id><pub-id pub-id-type="pmid">17898205</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attention improves performance primarily by reducing interneuronal correlations</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1594</fpage><lpage>1600</lpage><pub-id pub-id-type="doi">10.1038/nn.2439</pub-id><pub-id pub-id-type="pmid">19915566</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Using neuronal populations to study the mechanisms underlying spatial and feature attention</article-title><source>Neuron</source><volume>70</volume><fpage>1192</fpage><lpage>1204</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.04.029</pub-id><pub-id pub-id-type="pmid">21689604</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crochet</surname><given-names>S</given-names></name><name><surname>Poulet</surname><given-names>JF</given-names></name><name><surname>Kremer</surname><given-names>Y</given-names></name><name><surname>Petersen</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Synaptic mechanisms underlying sparse coding of active touch</article-title><source>Neuron</source><volume>69</volume><fpage>1160</fpage><lpage>1175</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.022</pub-id><pub-id pub-id-type="pmid">21435560</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cholinergic control of cortical network interactions enables feedback-mediated attentional modulation</article-title><source>European Journal of Neuroscience</source><volume>34</volume><fpage>146</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07749.x</pub-id><pub-id pub-id-type="pmid">21692884</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Disney</surname><given-names>AA</given-names></name><name><surname>Alasady</surname><given-names>HA</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Muscarinic acetylcholine receptors are expressed by most parvalbumin-immunoreactive neurons in area MT of the macaque</article-title><source>Brain and Behavior</source><volume>4</volume><fpage>431</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1002/brb3.225</pub-id><pub-id pub-id-type="pmid">24944872</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Disney</surname><given-names>AA</given-names></name><name><surname>Aoki</surname><given-names>C</given-names></name><name><surname>Hawken</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Gain modulation by nicotine in macaque v1</article-title><source>Neuron</source><volume>56</volume><fpage>701</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.09.034</pub-id><pub-id pub-id-type="pmid">18031686</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doiron</surname><given-names>B</given-names></name><name><surname>Lindner</surname><given-names>B</given-names></name><name><surname>Longtin</surname><given-names>A</given-names></name><name><surname>Maler</surname><given-names>L</given-names></name><name><surname>Bastian</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Oscillatory activity in electrosensory neurons increases with the spatial correlation of the stochastic input stimulus</article-title><source>Physical Review Letters</source><volume>93</volume><elocation-id>048101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.93.048101</pub-id><pub-id pub-id-type="pmid">15323795</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doiron</surname><given-names>B</given-names></name><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Rosenbaum</surname><given-names>R</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Josić</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The mechanics of state-dependent neural correlations</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>383</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1038/nn.4242</pub-id><pub-id pub-id-type="pmid">26906505</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Cotton</surname><given-names>RJ</given-names></name><name><surname>Subramaniyan</surname><given-names>M</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Cadwell</surname><given-names>CR</given-names></name><name><surname>Smirnakis</surname><given-names>SM</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>State dependence of noise correlations in macaque primary visual cortex</article-title><source>Neuron</source><volume>82</volume><fpage>235</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.02.006</pub-id><pub-id pub-id-type="pmid">24698278</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>On the structure of neuronal population activity under fluctuations in attentional state</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>1775</fpage><lpage>1789</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2044-15.2016</pub-id><pub-id pub-id-type="pmid">26843656</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>On the structure of population activity under fluctuations inattentional state</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/018226</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>TA</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Gieselmann</surname><given-names>MA</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Boahen</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Selective modulation of cortical state during spatial attention</article-title><source>Science</source><volume>354</volume><fpage>1140</fpage><lpage>1144</lpage><pub-id pub-id-type="doi">10.1126/science.aag1420</pub-id><pub-id pub-id-type="pmid">27934763</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Rorie</surname><given-names>AE</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Modulation of oscillatory neuronal synchronization by selective visual attention</article-title><source>Science</source><volume>291</volume><fpage>1560</fpage><lpage>1563</lpage><pub-id pub-id-type="doi">10.1126/science.1055465</pub-id><pub-id pub-id-type="pmid">11222864</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>Y</given-names></name><name><surname>Tucciarone</surname><given-names>JM</given-names></name><name><surname>Espinosa</surname><given-names>JS</given-names></name><name><surname>Sheng</surname><given-names>N</given-names></name><name><surname>Darcy</surname><given-names>DP</given-names></name><name><surname>Nicoll</surname><given-names>RA</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A cortical circuit for gain control by behavioral state</article-title><source>Cell</source><volume>156</volume><fpage>1139</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2014.01.050</pub-id><pub-id pub-id-type="pmid">24630718</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gardiner</surname><given-names>CW</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Handbook of Stochastic Methods for Physics, Chemistry and the Natural Sciences</source><edition>3rd edn</edition><publisher-name>Springer-Verlag</publisher-name></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>CD</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Brain states: top-down influences in sensory processing</article-title><source>Neuron</source><volume>54</volume><fpage>677</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.05.019</pub-id><pub-id pub-id-type="pmid">17553419</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ginzburg</surname><given-names>I</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Theory of correlations in stochastic neural networks</article-title><source>Physical Review E</source><volume>50</volume><fpage>3171</fpage><lpage>3191</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.50.3171</pub-id><pub-id pub-id-type="pmid">9962363</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cortical state and attention</article-title><source>Nature Reviews Neuroscience</source><volume>12</volume><fpage>509</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1038/nrn3084</pub-id><pub-id pub-id-type="pmid">21829219</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Neuromodulation and cortical function: modeling the physiological basis of behavior</article-title><source>Behavioural Brain Research</source><volume>67</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1016/0166-4328(94)00113-T</pub-id><pub-id pub-id-type="pmid">7748496</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helias</surname><given-names>M</given-names></name><name><surname>Tetzlaff</surname><given-names>T</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The correlation structure of local neuronal networks intrinsically results from recurrent dynamics</article-title><source>PLoS Computational Biology</source><volume>10</volume><fpage>e1003428</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003428</pub-id><pub-id pub-id-type="pmid">24453955</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrero</surname><given-names>JL</given-names></name><name><surname>Gieselmann</surname><given-names>MA</given-names></name><name><surname>Sanayei</surname><given-names>M</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attention-induced variance and noise correlation reduction in macaque V1 is mediated by NMDA receptors</article-title><source>Neuron</source><volume>78</volume><fpage>729</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.03.029</pub-id><pub-id pub-id-type="pmid">23719166</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isaacson</surname><given-names>JS</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How inhibition shapes cortical activity</article-title><source>Neuron</source><volume>72</volume><fpage>231</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.09.027</pub-id><pub-id pub-id-type="pmid">22017986</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>RC</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Kass</surname><given-names>RE</given-names></name><name><surname>Lee</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Local field potentials indicate network state and account for neuronal response variability</article-title><source>Journal of Computational Neuroscience</source><volume>29</volume><fpage>567</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1007/s10827-009-0208-9</pub-id><pub-id pub-id-type="pmid">20094906</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Fishell</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Interneuron cell types are fit to function</article-title><source>Nature</source><volume>505</volume><fpage>318</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1038/nature12983</pub-id><pub-id pub-id-type="pmid">24429630</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Ährlund-Richter</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Carlén</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prefrontal Parvalbumin neurons in control of attention</article-title><source>Cell</source><volume>164</volume><fpage>208</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.11.038</pub-id><pub-id pub-id-type="pmid">26771492</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Coen-Cagli</surname><given-names>R</given-names></name><name><surname>Kanitscheider</surname><given-names>I</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Correlations and Neuronal Population Information</article-title><source>Annual Review of Neuroscience</source><volume>39</volume><fpage>237</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-013851</pub-id><pub-id pub-id-type="pmid">27145916</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuchibhotla</surname><given-names>KV</given-names></name><name><surname>Gill</surname><given-names>JV</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Papadoyannis</surname><given-names>ES</given-names></name><name><surname>Field</surname><given-names>RE</given-names></name><name><surname>Sten</surname><given-names>TA</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Froemke</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parallel processing by cortical inhibition enables context-dependent behavior</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>62</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1038/nn.4436</pub-id><pub-id pub-id-type="pmid">27798631</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ledoux</surname><given-names>E</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Dynamics of networks of excitatory and inhibitory neurons in response to time-dependent inputs</article-title><source>Frontiers in Computational Neuroscience</source><volume>5</volume><elocation-id>25</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2011.00025</pub-id><pub-id pub-id-type="pmid">21647353</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>S</given-names></name><name><surname>Kruglikov</surname><given-names>I</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Fishell</surname><given-names>G</given-names></name><name><surname>Rudy</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A disinhibitory circuit mediates motor integration in the somatosensory cortex</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1662</fpage><lpage>1670</lpage><pub-id pub-id-type="doi">10.1038/nn.3544</pub-id><pub-id pub-id-type="pmid">24097044</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SH</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neuromodulation of brain states</article-title><source>Neuron</source><volume>76</volume><fpage>209</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.09.012</pub-id><pub-id pub-id-type="pmid">23040816</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>IC</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The nature of shared cortical variability</article-title><source>Neuron</source><volume>87</volume><fpage>644</fpage><lpage>656</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.035</pub-id><pub-id pub-id-type="pmid">26212710</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ly</surname><given-names>C</given-names></name><name><surname>Middleton</surname><given-names>JW</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cellular and circuit mechanisms maintain low spike co-variability and enhance population coding in somatosensory cortex</article-title><source>Frontiers in Computational Neuroscience</source><volume>6</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2012.00007</pub-id><pub-id pub-id-type="pmid">22408615</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating spike trains with specified correlation coefficients</article-title><source>Neural Computation</source><volume>21</volume><fpage>397</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.02-08-713</pub-id><pub-id pub-id-type="pmid">19196233</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAdams</surname><given-names>CJ</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Attention to both space and feature modulates neuronal responses in macaque area V4</article-title><source>Journal of Neurophysiology</source><volume>83</volume><fpage>1751</fpage><lpage>1755</lpage><pub-id pub-id-type="pmid">10712494</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitchell</surname><given-names>JF</given-names></name><name><surname>Sundberg</surname><given-names>KA</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Differential attention-dependent response modulation across cell classes in macaque visual area V4</article-title><source>Neuron</source><volume>55</volume><fpage>131</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.018</pub-id><pub-id pub-id-type="pmid">17610822</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitchell</surname><given-names>JF</given-names></name><name><surname>Sundberg</surname><given-names>KA</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Spatial attention decorrelates intrinsic activity fluctuations in macaque area V4</article-title><source>Neuron</source><volume>63</volume><fpage>879</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.013</pub-id><pub-id pub-id-type="pmid">19778515</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname><given-names>G</given-names></name><name><surname>Hansel</surname><given-names>D</given-names></name><name><surname>van Vreeswijk</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Bistability and spatiotemporal irregularity in neuronal networks with nonlinear synaptic transmission</article-title><source>Physical Review Letters</source><volume>108</volume><elocation-id>158101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.108.158101</pub-id><pub-id pub-id-type="pmid">22587287</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Zirnsak</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural mechanisms of selective visual attention</article-title><source>Annual Review of Psychology</source><volume>68</volume><fpage>47</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-122414-033400</pub-id><pub-id pub-id-type="pmid">28051934</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Beck</surname><given-names>J</given-names></name><name><surname>Kanitscheider</surname><given-names>I</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Information-limiting correlations</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1410</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1038/nn.3807</pub-id><pub-id pub-id-type="pmid">25195105</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navalpakkam</surname><given-names>V</given-names></name><name><surname>Itti</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Modeling the influence of task on attention</article-title><source>Vision Research</source><volume>45</volume><fpage>205</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.07.042</pub-id><pub-id pub-id-type="pmid">15581921</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noudoost</surname><given-names>B</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The role of neuromodulators in selective attention</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>585</fpage><lpage>591</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.10.006</pub-id><pub-id pub-id-type="pmid">22074811</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name><name><surname>Josić</surname><given-names>K</given-names></name><name><surname>Rosenbaum</surname><given-names>R</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>From the statistics of connectivity to the statistics of spike times in neuronal networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1703.03132">1703.03132</ext-link></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozeki</surname><given-names>H</given-names></name><name><surname>Finn</surname><given-names>IM</given-names></name><name><surname>Schaffer</surname><given-names>ES</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Ferster</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Inhibitory stabilization of the cortical network underlies visual surround suppression</article-title><source>Neuron</source><volume>62</volume><fpage>578</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.03.028</pub-id><pub-id pub-id-type="pmid">19477158</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pernice</surname><given-names>V</given-names></name><name><surname>Staude</surname><given-names>B</given-names></name><name><surname>Cardanobile</surname><given-names>S</given-names></name><name><surname>Rotter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How structure determines correlations in neuronal networks</article-title><source>PLoS Computational Biology</source><volume>7</volume><fpage>e1002059</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002059</pub-id><pub-id pub-id-type="pmid">21625580</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeffer</surname><given-names>CK</given-names></name><name><surname>Xue</surname><given-names>M</given-names></name><name><surname>He</surname><given-names>M</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inhibition of inhibition in visual cortex: the logic of connections between molecularly distinct interneurons</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1068</fpage><lpage>1076</lpage><pub-id pub-id-type="doi">10.1038/nn.3446</pub-id><pub-id pub-id-type="pmid">23817549</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poorthuis</surname><given-names>RB</given-names></name><name><surname>Enke</surname><given-names>L</given-names></name><name><surname>Letzkus</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cholinergic circuit modulation through differential recruitment of neocortical interneuron types during behaviour</article-title><source>The Journal of Physiology</source><volume>592</volume><fpage>4155</fpage><lpage>4164</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2014.273862</pub-id><pub-id pub-id-type="pmid">24879871</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Zemel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Information processing with population codes</article-title><source>Nature Reviews Neuroscience</source><volume>1</volume><fpage>125</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1038/35039062</pub-id><pub-id pub-id-type="pmid">11252775</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinowitz</surname><given-names>NC</given-names></name><name><surname>Goris</surname><given-names>RL</given-names></name><name><surname>Cohen</surname><given-names>M</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention stabilizes the shared gain of V4 populations</article-title><source>eLife</source><volume>4</volume><elocation-id>e08998</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.08998</pub-id><pub-id pub-id-type="pmid">26523390</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>de la Rocha</surname><given-names>J</given-names></name><name><surname>Bartho</surname><given-names>P</given-names></name><name><surname>Hollender</surname><given-names>L</given-names></name><name><surname>Parga</surname><given-names>N</given-names></name><name><surname>Reyes</surname><given-names>A</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The asynchronous state in cortical circuits</article-title><source>Science</source><volume>327</volume><fpage>587</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1126/science.1179850</pub-id><pub-id pub-id-type="pmid">20110507</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Competitive mechanisms subserve attention in macaque areas V2 and V4</article-title><source>Journal of Neuroscience</source><volume>19</volume><fpage>1736</fpage><lpage>1753</lpage><pub-id pub-id-type="pmid">10024360</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Attentional modulation of visual processing</article-title><source>Annual Review of Neuroscience</source><volume>27</volume><fpage>611</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.26.041002.131039</pub-id><pub-id pub-id-type="pmid">15217345</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The normalization model of attention</article-title><source>Neuron</source><volume>61</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id><pub-id pub-id-type="pmid">19186161</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenbaum</surname><given-names>R</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>JE</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The spatial structure of correlated neuronal variability</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>107</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1038/nn.4433</pub-id><pub-id pub-id-type="pmid">27798630</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>DB</given-names></name><name><surname>Van Hooser</surname><given-names>SD</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The stabilized supralinear network: a unifying circuit motif underlying multi-input integration in sensory cortex</article-title><source>Neuron</source><volume>85</volume><fpage>402</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.026</pub-id><pub-id pub-id-type="pmid">25611511</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudy</surname><given-names>B</given-names></name><name><surname>Fishell</surname><given-names>G</given-names></name><name><surname>Lee</surname><given-names>S</given-names></name><name><surname>Hjerling-Leffler</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Three groups of interneurons account for nearly 100% of neocortical GABAergic neurons</article-title><source>Developmental Neurobiology</source><volume>71</volume><fpage>45</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1002/dneu.20853</pub-id><pub-id pub-id-type="pmid">21154909</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Cohen</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attention can either increase or decrease spike count correlations in visual cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1591</fpage><lpage>1597</lpage><pub-id pub-id-type="doi">10.1038/nn.3835</pub-id><pub-id pub-id-type="pmid">25306550</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanayei</surname><given-names>M</given-names></name><name><surname>Herrero</surname><given-names>JL</given-names></name><name><surname>Distler</surname><given-names>C</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention and normalization circuits in macaque V1</article-title><source>European Journal of Neuroscience</source><volume>41</volume><fpage>949</fpage><lpage>964</lpage><pub-id pub-id-type="doi">10.1111/ejn.12857</pub-id><pub-id pub-id-type="pmid">25757941</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neuronal arithmetic</article-title><source>Nature Reviews Neuroscience</source><volume>11</volume><fpage>474</fpage><lpage>489</lpage><pub-id pub-id-type="doi">10.1038/nrn2864</pub-id><pub-id pub-id-type="pmid">20531421</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohya</surname><given-names>K</given-names></name><name><surname>Kameyama</surname><given-names>K</given-names></name><name><surname>Yanagawa</surname><given-names>Y</given-names></name><name><surname>Obata</surname><given-names>K</given-names></name><name><surname>Tsumoto</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>GABAergic neurons are less selective to stimulus orientation than excitatory neurons in layer II/III of visual cortex, as revealed by in vivo functional Ca2+ imaging in transgenic mice</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>2145</fpage><lpage>2149</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4641-06.2007</pub-id><pub-id pub-id-type="pmid">17314309</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steriade</surname><given-names>M</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Thalamocortical oscillations in the sleeping and aroused brain</article-title><source>Science</source><volume>262</volume><fpage>679</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1126/science.8235588</pub-id><pub-id pub-id-type="pmid">8235588</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Bartho</surname><given-names>P</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Lesica</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inhibitory control of correlated intrinsic variability in cortical networks</article-title><source>eLife</source><volume>5</volume><elocation-id>e19695</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.19695</pub-id><pub-id pub-id-type="pmid">27926356</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutherland</surname><given-names>C</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name><name><surname>Longtin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Feedback-induced gain control in stochastic spiking networks</article-title><source>Biological Cybernetics</source><volume>100</volume><fpage>475</fpage><lpage>489</lpage><pub-id pub-id-type="doi">10.1007/s00422-009-0298-5</pub-id><pub-id pub-id-type="pmid">19259695</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tetzlaff</surname><given-names>T</given-names></name><name><surname>Helias</surname><given-names>M</given-names></name><name><surname>Einevoll</surname><given-names>GT</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decorrelation of neural-network activity by inhibitory feedback</article-title><source>PLoS Computational Biology</source><volume>8</volume><elocation-id>e1002596</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002596</pub-id><pub-id pub-id-type="pmid">23133368</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural correlates of attention in primate visual cortex</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01814-2</pub-id><pub-id pub-id-type="pmid">11311383</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trousdale</surname><given-names>J</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Josić</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Impact of network structure and cellular response on spike time correlations</article-title><source>PLoS Computational Biology</source><volume>8</volume><elocation-id>e1002408</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002408</pub-id><pub-id pub-id-type="pmid">22457608</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vreeswijk</surname><given-names>C</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Chaotic balanced state in a model of cortical circuits</article-title><source>Neural Computation</source><volume>10</volume><fpage>1321</fpage><lpage>1371</lpage><pub-id pub-id-type="doi">10.1162/089976698300017214</pub-id><pub-id pub-id-type="pmid">9698348</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whiteway</surname><given-names>MR</given-names></name><name><surname>Butts</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Revealing unobserved factors underlying cortical activity with a rectified latent variable model applied to neural population recordings</article-title><source>Journal of Neurophysiology</source><volume>117</volume><fpage>919</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1152/jn.00698.2016</pub-id><pub-id pub-id-type="pmid">27927786</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williford</surname><given-names>T</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Effects of spatial attention on contrast response functions in macaque area V4</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>40</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1152/jn.01207.2005</pub-id><pub-id pub-id-type="pmid">16772516</pub-id></element-citation></ref></ref-list><app-group><app id="app1"><title>Appendix 1</title><boxed-text><sec id="s26" sec-type="appendix"><title>Detected structure in random covariance matrices is a finite-size effect</title><p>Here we show that any prediction of rank one structure in our shuffled covariance matrix (non-zero <inline-formula><mml:math id="inf627"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>shuf</mml:mi></mml:msub></mml:math></inline-formula> in <xref ref-type="fig" rid="fig2">Figure 2</xref> of the main text) is a finite-data effect. The trial-by-trial covariance matrices of the experimental data are computed from the spike counts recorded from a set number of units. To explore the effect of population size on the detected structure in the shuffled covariance matrices we must rely on synthetic data.</p><p>We construct the synthetic covariance matrices by generating Gaussian random numbers with the same mean and standard deviation as the actual covariance matrices from the data. This construction serves as a substitute for the shuffled covariance matrices, and allows for arbitrarily large populations. As we increase the number of units from near <inline-formula><mml:math id="inf628"><mml:mn>10</mml:mn></mml:math></inline-formula> to <inline-formula><mml:math id="inf629"><mml:mn>500</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf630"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>shuf</mml:mi></mml:msub></mml:math></inline-formula> decreases accordingly, indicating that any positive <inline-formula><mml:math id="inf631"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>shuf</mml:mi></mml:msub></mml:math></inline-formula> is due to the finite population size, rather than any inherent structure in the data (<xref ref-type="fig" rid="fig9">Appendix 1—figure 1</xref>).<fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.012</object-id><label>Appendix 1—figure 1.</label><caption><title>Detected structure in randomly generated covariance matrices is a finite-size effect.</title><p>The model performance (<inline-formula><mml:math id="inf632"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>shuf</mml:mi></mml:msub></mml:math></inline-formula>) decreases with increasing system size (black curves). The <inline-formula><mml:math id="inf633"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>shuf</mml:mi></mml:msub></mml:math></inline-formula> computed from the shuffled neural data (red dots) falls in the same area as the synthetic data performance, suggesting that the synthetic data is a reasonable stand-in.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.012">http://dx.doi.org/10.7554/eLife.23978.012</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-app1-fig1-v3"/></fig></p></sec><sec id="s27" sec-type="appendix"><title>Model performance is limited by number of trials in data</title><p>The upper bound for our model <inline-formula><mml:math id="inf634"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>ub</mml:mi></mml:msub></mml:math></inline-formula> did not saturate 1 (see <xref ref-type="fig" rid="fig2">Figure 2</xref> of the main text). Here, we show that this is also due the finite data available. If infinitely many trials were available to compute the spike count covariance matrices from the data, and the data obeyed by the low-rank statistical model, the performance of the model (<inline-formula><mml:math id="inf635"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>ub</mml:mi></mml:msub></mml:math></inline-formula>) should tend to one. To test this, we generate synthetic data from correlated Poisson processes as in the upper bound computation of the main text but do not limit the number of samples to the number of trials in the original data. As the number of samples increases we find that <inline-formula><mml:math id="inf636"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>ub</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig10">Appendix 1—figure 2</xref>).<fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.013</object-id><label>Appendix 1—figure 2.</label><caption><title>The performance of the model <inline-formula><mml:math id="inf637"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>ub</mml:mi></mml:msub></mml:math></inline-formula> (black curves) on synthetic data using increasing numbers of Poisson realizations approaches <inline-formula><mml:math id="inf638"><mml:mn>1</mml:mn></mml:math></inline-formula>.</title><p>The Poisson model computed with the same number of trials as the data is shown for comparison (red dots).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.013">http://dx.doi.org/10.7554/eLife.23978.013</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-app1-fig2-v3"/></fig></p></sec><sec id="s28" sec-type="appendix"><title>Model performance for all monkeys and hemispheres</title><p>The model performance for individual recording sessions are given here for transparency (<xref ref-type="fig" rid="fig11">Appendix 1—figure 3</xref> for the full data and <xref ref-type="fig" rid="fig12">Appendix 1—figure 4</xref> for the leave-one-out cross validation).<fig id="fig11" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.014</object-id><label>Appendix 1—figure 3.</label><caption><title>Performance of basic analysis on model on individual recording sessions from left hemisphere of monkey 1, and both hemispheres of monkey 2.</title><p>The format and colors match that described in <xref ref-type="fig" rid="fig2">Figure 2</xref> of the main text.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.014">http://dx.doi.org/10.7554/eLife.23978.014</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-app1-fig3-v3"/></fig></p></sec><sec id="s29" sec-type="appendix"><title>Low-dimensional modulation is intrinsic to neurons</title><p>In order to further test our model, we asked to what extent the actual value of the covariance gain <inline-formula><mml:math id="inf639"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of neuron <inline-formula><mml:math id="inf640"><mml:mi>i</mml:mi></mml:math></inline-formula> depends on the neural population whose covariance matrix <inline-formula><mml:math id="inf641"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> was estimated from. If we had solved the system <inline-formula><mml:math id="inf642"><mml:mi>S</mml:mi></mml:math></inline-formula> of equations <inline-formula><mml:math id="inf643"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> using covariance matrices computed from recordings from a different set of neurons (including neuron <inline-formula><mml:math id="inf644"><mml:mi>i</mml:mi></mml:math></inline-formula>), would the value of <inline-formula><mml:math id="inf645"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> be different? If not, this would be further indication of the independence of the attentional modulation of neuron <inline-formula><mml:math id="inf646"><mml:mi>i</mml:mi></mml:math></inline-formula> from the particular set of other neurons it is analyzed with.</p><p>We tackle this question by dividing a set of <inline-formula><mml:math id="inf647"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons into <inline-formula><mml:math id="inf648"><mml:mi>k</mml:mi></mml:math></inline-formula> sets <inline-formula><mml:math id="inf649"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="inf650"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≡</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> neurons each that all contain the neuron <inline-formula><mml:math id="inf651"><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf652"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≡</mml:mo><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> if <inline-formula><mml:math id="inf653"><mml:mi>N</mml:mi></mml:math></inline-formula> is originally even). As an example take <inline-formula><mml:math id="inf654"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and consider the set of neurons <inline-formula><mml:math id="inf655"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> partitioned into two subsets <inline-formula><mml:math id="inf656"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf657"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig13">Appendix 1—figure 5a</xref>). We solve <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> using the systems of equations obtained from <inline-formula><mml:math id="inf658"><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf659"><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, and obtain two solutions <inline-formula><mml:math id="inf660"><mml:msubsup><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf661"><mml:msubsup><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. We take the variance of the <inline-formula><mml:math id="inf662"><mml:mi>g</mml:mi></mml:math></inline-formula>-estimations as a metric for how closely the different subsets can estimate an intrinsic value of <inline-formula><mml:math id="inf663"><mml:mi>g</mml:mi></mml:math></inline-formula>. A higher variance would indicate a poorer convergence, and therefore a lower degree of independence from other neurons. <xref ref-type="fig" rid="fig13">Appendix 1—figure 5b</xref> shows the spread of <inline-formula><mml:math id="inf664"><mml:mi>g</mml:mi></mml:math></inline-formula>-estimates from one dataset for the data, as well as the upper (UB) and lower (shuf) bounds. This spread includes estimates for all <inline-formula><mml:math id="inf665"><mml:mi>g</mml:mi></mml:math></inline-formula>-values for all neurons. The spread in the shuffled case (SEM<inline-formula><mml:math id="inf666"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mn>7.42</mml:mn></mml:mrow></mml:math></inline-formula>) is largest by two orders of magnitude, and the spread of the upper bound (SEM<inline-formula><mml:math id="inf667"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mn>2.60</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) is only one order of magnitude tighter than that of the data (SEM<inline-formula><mml:math id="inf668"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.03</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>), so this case is close to ideal.<fig id="fig12" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.015</object-id><label>Appendix 1—figure 4.</label><caption><title>Performance of leave-one-out cross-validation on model on data from individual recording sessions from the left hemisphere of Monkey 1, and both hemispheres of Monkey 2.</title><p>The format and colors match that described in <xref ref-type="fig" rid="fig2">Figure 2</xref> of the main text.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.015">http://dx.doi.org/10.7554/eLife.23978.015</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-app1-fig4-v3"/></fig><fig id="fig13" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.016</object-id><label>Appendix 1—figure 5.</label><caption><title>Overlap analysis of gain parameters.</title><p>(<bold>a</bold>) Schematic of overlap analysis. A set of <inline-formula><mml:math id="inf669"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> neurons is divided into two sets <inline-formula><mml:math id="inf670"><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf671"><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> of <inline-formula><mml:math id="inf672"><mml:mi>i</mml:mi></mml:math></inline-formula>, which overlap by exactly one neuron, indexed without loss of generality as neuron <inline-formula><mml:math id="inf673"><mml:mi>i</mml:mi></mml:math></inline-formula>. Parameter <inline-formula><mml:math id="inf674"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is computed using <inline-formula><mml:math id="inf675"><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf676"><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, resulting in two estimates <inline-formula><mml:math id="inf677"><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf678"><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. (<bold>b</bold>) Spread of <inline-formula><mml:math id="inf679"><mml:mi>g</mml:mi></mml:math></inline-formula> estimates for the data (black), as well as the upper (blue) and lower (green) bounds, from one day of recordings in one monkey. (<bold>c</bold>) Mean variance of the <inline-formula><mml:math id="inf680"><mml:mi>g</mml:mi></mml:math></inline-formula> estimates computed from the data (abscissa) vs from the upper bound (ordinate), normalized by the mean shuffled variance. Each color denotes one of the monkeys, circles denote the right hemisphere recordings, and plusses denote the left hemisphere recordings. The gray regions consist of those points that are beyond <inline-formula><mml:math id="inf681"><mml:mn>0.5</mml:mn></mml:math></inline-formula>, and therefore closer to the lower bound than the upper bound.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.016">http://dx.doi.org/10.7554/eLife.23978.016</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-app1-fig5-v3"/></fig></p><p>For each data set, the analysis is done for each neuron for <inline-formula><mml:math id="inf682"><mml:mn>100</mml:mn></mml:math></inline-formula> different permutations of the neurons to generate <inline-formula><mml:math id="inf683"><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf684"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. For shuffled and upper-bound analysis, <inline-formula><mml:math id="inf685"><mml:mn>10</mml:mn></mml:math></inline-formula> shuffles or Poisson realizations, and <inline-formula><mml:math id="inf686"><mml:mn>10</mml:mn></mml:math></inline-formula> permutations were used. In all cases there was a total of <inline-formula><mml:math id="inf687"><mml:mrow><mml:mrow><mml:mn>100</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">#</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mtext>neurons</mml:mtext></mml:mrow></mml:math></inline-formula> points. <xref ref-type="fig" rid="fig13">Appendix 1—figure 5c</xref> shows an overview of the performance for all datasets. The abscissa is the mean variance of the <inline-formula><mml:math id="inf688"><mml:mi>g</mml:mi></mml:math></inline-formula>-estimates computed from the data, normalized by the mean variance computed from the shuffled data: <inline-formula><mml:math id="inf689"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. The 'shuf' subscript denotes averaging over each shuffle. The ordinate is the mean variance of the <inline-formula><mml:math id="inf690"><mml:mi>g</mml:mi></mml:math></inline-formula>-estimates computed from the upper bound, with the same normalization: <inline-formula><mml:math id="inf691"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>U</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. The 'poiss' subscript denotes averaging over each Poisson realization of the upper bound covariance matrix. We chose to normalize the mean data and upper-bound variances by the mean shuffled variance so that a value of <inline-formula><mml:math id="inf692"><mml:mn>1</mml:mn></mml:math></inline-formula> would mean equality to the lower bound, meaning the only detected structure comes from finite-size effects, and a value of <inline-formula><mml:math id="inf693"><mml:mn>0</mml:mn></mml:math></inline-formula> would mean perfect convergence of the <inline-formula><mml:math id="inf694"><mml:mi>g</mml:mi></mml:math></inline-formula>-estimates. The gray regions are a visualization for the points which are closer to <inline-formula><mml:math id="inf695"><mml:mn>1</mml:mn></mml:math></inline-formula> than <inline-formula><mml:math id="inf696"><mml:mn>0</mml:mn></mml:math></inline-formula> (values above <inline-formula><mml:math id="inf697"><mml:mn>0.5</mml:mn></mml:math></inline-formula>) on the log-axes. Most of the data unsurprisingly falls below the diagonal, so the variance is greater for the data than the upper bound. Less trivially, most of the data falls outside of the gray regions, and are much closer to <inline-formula><mml:math id="inf698"><mml:mn>0</mml:mn></mml:math></inline-formula> than <inline-formula><mml:math id="inf699"><mml:mn>1</mml:mn></mml:math></inline-formula>, indicating excellent performance. This implies a structure in the modulation of the (unshuffled) covariance matrices that is preserved over analysis in the contexts of different groups of other neurons. In other words, attention modulates the individual neurons to a large extent independently, in a low-dimensional manner.</p></sec><sec id="s30" sec-type="appendix"><title>Network requirements for attentional modulation</title><p>In this section we study a network of <inline-formula><mml:math id="inf700"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons with the spike train output from neuron <inline-formula><mml:math id="inf701"><mml:mi>i</mml:mi></mml:math></inline-formula> being <inline-formula><mml:math id="inf702"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf703"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf704"><mml:msup><mml:mi>k</mml:mi><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula> spike time from neuron <inline-formula><mml:math id="inf705"><mml:mi>i</mml:mi></mml:math></inline-formula>. We consider multiple trials of the discrimination experiment and model the spike train only over a time period <inline-formula><mml:math id="inf706"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where we assume that the spike trains to have have reached equilibrium statistics. We abuse notation and take the spike count from neuron <inline-formula><mml:math id="inf707"><mml:mi>i</mml:mi></mml:math></inline-formula> over a trial as <inline-formula><mml:math id="inf708"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The trial-to-trial covariance matrix of the network response is <inline-formula><mml:math id="inf709"><mml:mi mathvariant="bold">𝐂</mml:mi></mml:math></inline-formula> with element <inline-formula><mml:math id="inf710"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>To analyze the network activity we first assume that each spike train is simply perturbed about a background state and employ the linear response ansatz (<xref ref-type="bibr" rid="bib24">Ginzburg and Sompolinsky, 1994</xref>; <xref ref-type="bibr" rid="bib14">Doiron et al., 2004</xref>; <xref ref-type="bibr" rid="bib72">Trousdale et al., 2012</xref>) :<disp-formula id="equ31"><label>(28)</label><mml:math id="m31"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf711"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the synaptic coupling from neuron <inline-formula><mml:math id="inf712"><mml:mi>k</mml:mi></mml:math></inline-formula> to neuron <inline-formula><mml:math id="inf713"><mml:mi>i</mml:mi></mml:math></inline-formula> (proportional to the synaptic weight), and <inline-formula><mml:math id="inf714"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is a fluctuating external input given to neuron <inline-formula><mml:math id="inf715"><mml:mi>i</mml:mi></mml:math></inline-formula>. The background state of neuron <inline-formula><mml:math id="inf716"><mml:mi>i</mml:mi></mml:math></inline-formula> is <inline-formula><mml:math id="inf717"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and it represents the stochastic output of a neuron that is not due to the recurrence from the network <inline-formula><mml:math id="inf718"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> or the external input (<inline-formula><mml:math id="inf719"><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). Finally, <inline-formula><mml:math id="inf720"><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the input to output gain of a neuron <inline-formula><mml:math id="inf721"><mml:mi>i</mml:mi></mml:math></inline-formula>. In this framework, <inline-formula><mml:math id="inf722"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf723"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf724"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are random variables, while <inline-formula><mml:math id="inf725"><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf726"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are parameters that describe the intrinsic and network properties of the system. Without loss of generality we take <inline-formula><mml:math id="inf727"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf728"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, making <inline-formula><mml:math id="inf729"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> a solution for the mean activity. We remark that formally <xref ref-type="disp-formula" rid="equ31">Equation (28)</xref> is incorrect as written; <inline-formula><mml:math id="inf730"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is a random integer while, for instance, <inline-formula><mml:math id="inf731"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> need not be an integer. <xref ref-type="disp-formula" rid="equ31">Equation (28)</xref> is only correct upon taking an expectation (over trials) of <inline-formula><mml:math id="inf732"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>Here we derive the requirements for external fluctuations and internal coupling for network covariability <inline-formula><mml:math id="inf733"><mml:mi mathvariant="bold">𝐂</mml:mi></mml:math></inline-formula> to satisfy the following two conditions (on average):</p><p><bold>C1:</bold> <inline-formula><mml:math id="inf734"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> ; attentional modulation of covariance is rank one.</p><p><bold>C2:</bold> <inline-formula><mml:math id="inf735"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> ; spike count covariance decreases with attention.</p><p>It is convenient to write <xref ref-type="disp-formula" rid="equ31">Equation (28)</xref> in matrix form and isolate for the population response:<disp-formula id="equ32"><label>(29)</label><mml:math id="m32"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐊</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi mathvariant="bold">𝐁</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐋</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>ξ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf736"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> with similar notation for <inline-formula><mml:math id="inf737"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi mathvariant="bold">𝐁</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf738"><mml:mover accent="true"><mml:mi>ξ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>. The matrix <inline-formula><mml:math id="inf739"><mml:mi mathvariant="bold">𝐊</mml:mi></mml:math></inline-formula> has element <inline-formula><mml:math id="inf740"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐊</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, while <inline-formula><mml:math id="inf741"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf742"><mml:mi mathvariant="bold">𝐈</mml:mi></mml:math></inline-formula> is the identity matrix. Using <xref ref-type="disp-formula" rid="equ32">Equation (29)</xref> we can express the covariance matrix <inline-formula><mml:math id="inf743"><mml:mrow><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as:<disp-formula id="equ33"><label>(30)</label><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">L</mml:mi><mml:mi mathvariant="bold">X</mml:mi><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:munder><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf744"><mml:msup><mml:mi/><mml:mi>T</mml:mi></mml:msup></mml:math></inline-formula> denotes the transpose operation. Here <inline-formula><mml:math id="inf745"><mml:mrow><mml:mi mathvariant="bold">𝐁</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi mathvariant="bold">𝐁</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi mathvariant="bold">𝐁</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the background covariance, which we take to be simply <inline-formula><mml:math id="inf746"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The input covariance matrix is <inline-formula><mml:math id="inf747"><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>ξ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>ξ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with elements <inline-formula><mml:math id="inf748"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. In the above we assumed that <inline-formula><mml:math id="inf749"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐲</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi mathvariant="bold">𝐁</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>ξ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>𝟎</mml:mn></mml:mrow></mml:math></inline-formula>, meaning that the background state is uncorrelated with the external noisy input.</p><p>It is clear that <inline-formula><mml:math id="inf750"><mml:mi mathvariant="bold">𝐂</mml:mi></mml:math></inline-formula> naturally decomposes into two terms. The first term represents the correlations that are internally generated within the network, via the direct synaptic coupling <inline-formula><mml:math id="inf751"><mml:mi mathvariant="bold">𝐊</mml:mi></mml:math></inline-formula> acting upon the background state <inline-formula><mml:math id="inf752"><mml:mi mathvariant="bold">𝐁</mml:mi></mml:math></inline-formula>. The second term is how the direct synaptic coupling <inline-formula><mml:math id="inf753"><mml:mi mathvariant="bold">𝐊</mml:mi></mml:math></inline-formula> filters the externally applied correlations <inline-formula><mml:math id="inf754"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula>.</p><sec id="s31"><title>Satisfying C1</title><p>The background matrix <inline-formula><mml:math id="inf755"><mml:mi mathvariant="bold">𝐁</mml:mi></mml:math></inline-formula> is a diagonal matrix and is hence rank <inline-formula><mml:math id="inf756"><mml:mi>N</mml:mi></mml:math></inline-formula>. The high rank <inline-formula><mml:math id="inf757"><mml:mi mathvariant="bold">𝐁</mml:mi></mml:math></inline-formula> combined with attentional modulations of both <inline-formula><mml:math id="inf758"><mml:mi mathvariant="bold">𝐁</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf759"><mml:mi mathvariant="bold">𝐊</mml:mi></mml:math></inline-formula> make it impossible to satisfy condition <inline-formula><mml:math id="inf760"><mml:mi mathvariant="bold">𝐂𝟏</mml:mi></mml:math></inline-formula>. If the spectral radius of <inline-formula><mml:math id="inf761"><mml:mi mathvariant="bold">𝐊</mml:mi></mml:math></inline-formula> is less than 1, then we can expand <inline-formula><mml:math id="inf762"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐊</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:msup><mml:mi mathvariant="bold">𝐊</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib51">Pernice et al., 2011</xref>; <xref ref-type="bibr" rid="bib72">Trousdale et al., 2012</xref>). Inserting this expansion into the expression for the internally generated covariability yields:<disp-formula id="equ34"><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Extracting the covariance between neuron <inline-formula><mml:math id="inf763"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf764"><mml:mi>j</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf765"><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>) due to internal coupling within the network gives:<disp-formula id="equ35"><mml:math id="m35"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">⋯</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>If we take <inline-formula><mml:math id="inf766"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and the network connectivity to be dense (meaning the connection probability is <inline-formula><mml:math id="inf767"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒪</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) then each term is <inline-formula><mml:math id="inf768"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒪</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. So long as the spectral radius of <inline-formula><mml:math id="inf769"><mml:mi mathvariant="bold">𝐊</mml:mi></mml:math></inline-formula> is less than one then the series converges and as <inline-formula><mml:math id="inf770"><mml:mrow><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> we have that <inline-formula><mml:math id="inf771"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> vanishes (<xref ref-type="bibr" rid="bib51">Pernice et al., 2011</xref>; <xref ref-type="bibr" rid="bib72">Trousdale et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Helias et al., 2014</xref>).</p><p>This argument can be extended to networks with <inline-formula><mml:math id="inf772"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> when combined with a balance condition between recurrent excitation and inhibition. Such networks also produce an asynchronous state where <inline-formula><mml:math id="inf773"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, vanishing in the large <inline-formula><mml:math id="inf774"><mml:mi>N</mml:mi></mml:math></inline-formula> limit (<xref ref-type="bibr" rid="bib56">Renart et al., 2010</xref>). However, formally balanced networks in the asynchronous state with <inline-formula><mml:math id="inf775"><mml:mrow><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> have solutions that do not depend on the firing rate transfer <inline-formula><mml:math id="inf776"><mml:mi>L</mml:mi></mml:math></inline-formula>. The attention dependent modulation <inline-formula><mml:math id="inf777"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mi>A</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is a critical component of our model and care must be taken in ensuring that</p><p>In contrast, the external covariance <inline-formula><mml:math id="inf778"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> is not a diagonal matrix, so that the contributions from external fluctuations to <inline-formula><mml:math id="inf779"><mml:mi mathvariant="bold">𝐂</mml:mi></mml:math></inline-formula> scale as <inline-formula><mml:math id="inf780"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>J</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. This is <inline-formula><mml:math id="inf781"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒪</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf782"><mml:mrow><mml:mi>J</mml:mi><mml:mo>∝</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, while the terms in <inline-formula><mml:math id="inf783"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> must be weak for the linear approximation in <xref ref-type="disp-formula" rid="equ31">Equation (28)</xref> to hold, they need not vanish for large <inline-formula><mml:math id="inf784"><mml:mi>N</mml:mi></mml:math></inline-formula>. Indeed, for moderate <inline-formula><mml:math id="inf785"><mml:mi mathvariant="bold">𝐗</mml:mi></mml:math></inline-formula> and large network size it is reasonable to ignore the contribution of internally generated fluctuations to <inline-formula><mml:math id="inf786"><mml:mi mathvariant="bold">𝐂</mml:mi></mml:math></inline-formula>. Recent analysis of cortical population recordings show that the shared spiking variability across the population can be well approximated by a rank one model of covariability (<xref ref-type="bibr" rid="bib16">Ecker et al., 2014</xref>; <xref ref-type="bibr" rid="bib38">Lin et al., 2015</xref>; <xref ref-type="bibr" rid="bib18">Ecker et al., 2015</xref>; <xref ref-type="bibr" rid="bib55">Rabinowitz et al., 2015</xref>). Thus motivated, we take the external fluctuations <inline-formula><mml:math id="inf787"><mml:mrow><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐱𝐱</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf788"><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. In total, we have for large <inline-formula><mml:math id="inf789"><mml:mi>N</mml:mi></mml:math></inline-formula> the approximation:<disp-formula id="equ36"><label>(31)</label><mml:math id="m36"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐂</mml:mi><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐊</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐋𝐱</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐊</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐋𝐱</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐜𝐜</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Hence <inline-formula><mml:math id="inf790"><mml:mi mathvariant="bold">𝐂</mml:mi></mml:math></inline-formula> is rank one matrix with <inline-formula><mml:math id="inf791"><mml:mrow><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐊</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐋𝐱</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. It is trivial to satisfy condition <inline-formula><mml:math id="inf792"><mml:mi mathvariant="bold">𝐂𝟏</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf793"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>A</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>U</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s32"><title>Satisfying C2</title><p>We again use the expansion <inline-formula><mml:math id="inf794"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐊</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:msup><mml:mi mathvariant="bold">𝐊</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Truncating this expansion at <inline-formula><mml:math id="inf795"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> yields an approximation considering only synaptic paths of length one in the network, and neglecting higher order paths. This is appropriate for <inline-formula><mml:math id="inf796"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> sufficiently small. Truncating after inserting the expansion into <xref ref-type="disp-formula" rid="equ33">Equation (30)</xref> yields the following approximation for <inline-formula><mml:math id="inf797"><mml:mi mathvariant="bold">𝐜</mml:mi></mml:math></inline-formula>:<disp-formula id="equ37"><label>(32)</label><mml:math id="m37"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold">𝐊</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐋𝐱</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The analysis in the main text begins with this approximation to derive <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref> of the main text.</p></sec></sec><sec id="s33" sec-type="appendix"><title>Spiking network</title><sec id="s34"><title>Spiking network description</title><p>We implement a network of leaky integrate-and-fire neurons (LIF) with <inline-formula><mml:math id="inf798"><mml:mn>1000</mml:mn></mml:math></inline-formula> excitatory neurons and <inline-formula><mml:math id="inf799"><mml:mn>200</mml:mn></mml:math></inline-formula> inhibitory neurons. Individual neurons were modeled as integrate-and-fire units whose voltages obeyed<disp-formula id="equ38"><label>(33)</label><mml:math id="m38"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mtext>syn</mml:mtext></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mtext>ext</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>for neuron <inline-formula><mml:math id="inf800"><mml:mi>i</mml:mi></mml:math></inline-formula>. When the voltage reached a threshold <inline-formula><mml:math id="inf801"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mtext>th</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, a spike was recorded and the voltage reset to <inline-formula><mml:math id="inf802"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mtext>re</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Time was measured in units of the membrane time constant, <inline-formula><mml:math id="inf803"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for all neurons. The bias <inline-formula><mml:math id="inf804"><mml:mi>μ</mml:mi></mml:math></inline-formula> depended on neuron type and attentional state. In the unattended state, the bias for excitatory neurons was <inline-formula><mml:math id="inf805"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mtext>un</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.6089</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf806"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>un</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.5388</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. In the attended state, <inline-formula><mml:math id="inf807"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mtext>att</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.8713</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf808"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>att</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.8996</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The recurrent input to neuron <inline-formula><mml:math id="inf809"><mml:mi>i</mml:mi></mml:math></inline-formula> was<disp-formula id="equ39"><label>(34)</label><mml:math id="m39"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mtext>syn</mml:mtext></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐉</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf810"><mml:msub><mml:mi mathvariant="bold">𝐖</mml:mi><mml:mi mathvariant="bold">𝐢𝐣</mml:mi></mml:msub></mml:math></inline-formula> is the strength of the connection from neuron <inline-formula><mml:math id="inf811"><mml:mi>j</mml:mi></mml:math></inline-formula> to neuron <inline-formula><mml:math id="inf812"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf813"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the synaptic filter for the projection from neuron <inline-formula><mml:math id="inf814"><mml:mi>j</mml:mi></mml:math></inline-formula> to neuron <inline-formula><mml:math id="inf815"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf816"><mml:mo>*</mml:mo></mml:math></inline-formula> denotes convolution and <inline-formula><mml:math id="inf817"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is neuron <inline-formula><mml:math id="inf818"><mml:mi>j</mml:mi></mml:math></inline-formula>’s spike train – a series of <inline-formula><mml:math id="inf819"><mml:mi>δ</mml:mi></mml:math></inline-formula>-functions centered at spike times. The synaptic filters were taken to be alpha functions,<disp-formula id="equ40"><label>(35)</label><mml:math id="m40"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf820"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> of the passive membrane time constant for all synapses. The connection probability from neurons in population <inline-formula><mml:math id="inf821"><mml:mi>A</mml:mi></mml:math></inline-formula> to population <inline-formula><mml:math id="inf822"><mml:mi>B</mml:mi></mml:math></inline-formula> was <inline-formula><mml:math id="inf823"><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, with <inline-formula><mml:math id="inf824"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf825"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Synaptic weights for connections between excitatory neurons were <inline-formula><mml:math id="inf826"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.0075</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf827"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.0037</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf828"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.0375</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf829"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.0375</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. These parameters, and the bias voltages <inline-formula><mml:math id="inf830"><mml:mi>μ</mml:mi></mml:math></inline-formula>, were chosen so that the mean field theory derived above was valid for the spiking network’s firing rates.</p><p>The excitatory neurons were divided into four clusters, each excitatory neuron receiving half of its inputs from neurons in the same cluster and half from others. Projections to and from inhibitory neurons were unclustered.</p><p>External input from outside the network was contained in <inline-formula><mml:math id="inf831"><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mtext>ext</mml:mtext></mml:msubsup></mml:math></inline-formula>. We modeled this as a partially correlated Gaussian white noise process: <inline-formula><mml:math id="inf832"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>c</mml:mi></mml:msqrt><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msqrt><mml:mi>c</mml:mi></mml:msqrt><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf833"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> was Gaussian white noise private to neuron <inline-formula><mml:math id="inf834"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf835"><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was shared between all neurons. <inline-formula><mml:math id="inf836"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> denoted the fraction of common input and the noise intensity for excitatory neurons was <inline-formula><mml:math id="inf837"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and for inhibitory neurons <inline-formula><mml:math id="inf838"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.35</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The firing rate of neuron <inline-formula><mml:math id="inf839"><mml:mi>i</mml:mi></mml:math></inline-formula> in a trial of length <inline-formula><mml:math id="inf840"><mml:mi>L</mml:mi></mml:math></inline-formula> is given by its spike count in that trial <inline-formula><mml:math id="inf841"><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>L</mml:mi></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf842"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>L</mml:mi></mml:msubsup><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf843"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> denotes averaging over trials. The spike train covariance between neurons <inline-formula><mml:math id="inf844"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf845"><mml:mi>j</mml:mi></mml:math></inline-formula> describes the above-change likelihood that action potentials occur in each spike train separated by a time lag <inline-formula><mml:math id="inf846"><mml:mi>s</mml:mi></mml:math></inline-formula>:<disp-formula id="equ41"><label>(36)</label><mml:math id="m41"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>L</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>L</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For simulations, we measure the population-averaged spike train cross-covariance function <inline-formula><mml:math id="inf847"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> by average a randomly chosen subsample of 100 spike train cross-covariances from pairs of neurons in the same cluster.</p><p>In order to calculate the covariance of neuron <inline-formula><mml:math id="inf848"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf849"><mml:mi>j</mml:mi></mml:math></inline-formula>’s spike counts in windows of length <inline-formula><mml:math id="inf850"><mml:mi>T</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf851"><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf852"><mml:msubsup><mml:mi>n</mml:mi><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:math></inline-formula>, we use the relation<disp-formula id="equ42"><label>(37)</label><mml:math id="m42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≡</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The cross-correlation of input currents was averaged over the same random subsample of the network as the spike train covariances. Current cross-correlations were normalized so that each current’s autocorrelation at zero lag was 1.</p></sec><sec id="s35"><title>Spiking network analysis</title><p>The LIF model simulates voltages and produces spike trains, from which we can compute firing rates and covariances. <xref ref-type="fig" rid="fig14">Appendix 1—figure 6a,b</xref> show example voltage traces of individual excitatory neurons, with the spikes they produce shown above. Note that in the attended state, more spikes are produced, corresponding to a higher firing rate. <xref ref-type="fig" rid="fig14">Appendix 1—figure 6c,d</xref> show rasters for all the neurons in the unattended (c) and attended (d) states. Higher firing rates can be observed, especially for the inhibitory neurons. Averaging the spike trains over the excitatory population gives us the PSTH of the excitatory neurons. <xref ref-type="fig" rid="fig14">Appendix 1—figure 6e</xref>, left shows the unattended (turquoise) and attended (orange) PSTH smoothed with a sliding Gaussian window with width (std dev) <inline-formula><mml:math id="inf853"><mml:mn>10</mml:mn></mml:math></inline-formula> ms. The histograms on the right demonstrate the decrease in population variance with attention.<fig id="fig14" position="float"><object-id pub-id-type="doi">10.7554/eLife.23978.017</object-id><label>Appendix 1—figure 6.</label><caption><title>Spiking model and simulations.</title><p>(<bold>a,b</bold>) Example voltage trace from an excitatory model neuron in the unattended (a) and attended (b) states. Top tick marks denote spike times. (<bold>c</bold>) Raster plot of neurons in the unattended state. Neurons 1 to 1000 are excitatory, and 1001 to 1200 are inhibitory. (<bold>d</bold>) Raster plot of neurons in the attended state. (<bold>e</bold>) Excitatory population-averaged firing rates for the unattended (turquoise) and attended (orange) states. Right: frequency distributions of population-averaged firing rates. (<bold>f</bold>) Mean pairwise spike count covariance for different counting windows. Other than an increase in synchrony on very small timescales due to gamma oscillations, the spike count covariance decreases with attention regardless of counting window. (<bold>g</bold>) Ratio <inline-formula><mml:math id="inf854"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of attended and unattended spike count covariance, as a function of counting window. (<bold>h, i</bold>) Derived (solid turquoise) and simulated (muted turquoise) spike train cross-covariance functions of excitatory neurons in the unattended (h) and attended (i) states, averaged over pairs. (<bold>j</bold>) Spike train cross-covariance functions of excitatory neurons in the unattended and attended states, normalized to peak at <inline-formula><mml:math id="inf855"><mml:mn>1</mml:mn></mml:math></inline-formula>. (<bold>k, l</bold>) Normalized input current cross-correlation functions of excitatory inputs to pairs of neurons (dashed green), inhibitory inputs to pairs of neurons (dashed red), excitatory and inhibitory inputs to pairs of neurons (blue), and summed excitatory and inhibitory recurrent inputs to pairs of neurons (black), in the unattended (k) and attended (l) states. (<bold>m</bold>) Attended (orange) vs unattended (turquoise) recurrent input cross-correlation functions. The excitatory cross-correlation function is narrower, just as for the output cross-covariance function, so the effects are happening on the level of inputs.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.23978.017">http://dx.doi.org/10.7554/eLife.23978.017</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-23978-app1-fig6-v3"/></fig></p><p>The spiking model provides the opportunity to directly compute the pairwise spiking covariance, in addition to the population variance. <xref ref-type="fig" rid="fig14">Appendix 1—figure 6f</xref> shows the pairwise spike count covariance computed over counting windows from <inline-formula><mml:math id="inf856"><mml:mn>0</mml:mn></mml:math></inline-formula> to <inline-formula><mml:math id="inf857"><mml:mn>200</mml:mn></mml:math></inline-formula> ms. For small counting windows, corresponding to high-frequency correlations, neurons in the attended state have slightly higher spike count covariance. This is consistent with the slightly higher peak in the attended autocovariance function from the mean-field theory (<xref ref-type="fig" rid="fig4">Figure 4e</xref>, main text), as well as experimental results (<xref ref-type="bibr" rid="bib20">Fries et al., 2001</xref>). For counting windows greater than <inline-formula><mml:math id="inf858"><mml:mn>30</mml:mn></mml:math></inline-formula> ms, the spike count covariance notably decreases with attention. The experiments we are modeling (<xref ref-type="bibr" rid="bib8">Cohen and Maunsell, 2009</xref>) measure spike count correlations over <inline-formula><mml:math id="inf859"><mml:mn>200</mml:mn></mml:math></inline-formula> ms counting windows, corresponding to the right-most points in <xref ref-type="fig" rid="fig14">Appendix 1—figure 6f</xref>. The proportional changes in the spike count covariance are expressed in the covariance ratio <inline-formula><mml:math id="inf860"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, shown in <xref ref-type="fig" rid="fig14">Appendix 1—figure 6g</xref>. Values of <inline-formula><mml:math id="inf861"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> greater than one indicate increased spike count covariance with attention, and values of <inline-formula><mml:math id="inf862"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> less than one indicate decreased spike count covariance with attention. The crossing of the <inline-formula><mml:math id="inf863"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> line is apparent at counting windows of approximately <inline-formula><mml:math id="inf864"><mml:mn>30</mml:mn></mml:math></inline-formula> ms. The theoretical values were computed using linear response theory (<xref ref-type="bibr" rid="bib72">Trousdale et al., 2012</xref>).</p><p>To dissect the spike count covariance by different time lags, we consider the spike train covariance function (<xref ref-type="disp-formula" rid="equ41">Equation 36</xref>), which is the pairwise-neuron analogue of the autocovariance function of the population-averaged activity (<xref ref-type="fig" rid="fig5">Figure 5e</xref>, main text). <xref ref-type="fig" rid="fig14">Appendix 1—figure 6h,i</xref> show the spike train covariance functions of excitatory neurons in the unattended and attended states. To compare the two, <xref ref-type="fig" rid="fig14">Appendix 1—figure 6j</xref> shows them normalized so that their maximum values are <inline-formula><mml:math id="inf865"><mml:mn>1</mml:mn></mml:math></inline-formula>. In accordance with our mean-field results, the attended spike train covariance decays faster than the unattended spike train covariance, indicating increased stability in the attended state.</p><p>The spiking model also provides the opportunity to investigate the inputs to individual neurons, something that is difficult to do experimentally, and does not apply to mean-field models. <xref ref-type="fig" rid="fig14">Appendix 1—figure 6k,l</xref> shows the correlation functions of different types of inputs to a pair of excitatory neurons, averaged over pairs of excitatory neurons, in the unattended (k) and attended (l) states. Computing the correlation functions of the total recurrent input (black curves) reveals that correlations between excitatory inputs (EPSC-EPSC, dashed green), and correlations between inhibitory inputs (IPSC-IPSC, dashed red), are canceled by anti-correlations between excitatory and inhibitory inputs (EPSC-IPSC, blue). This is consistent with the idea of correlation cancellation by inhibitory tracking of excitatory activity (<xref ref-type="bibr" rid="bib56">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib70">Tetzlaff et al., 2012</xref>; <xref ref-type="bibr" rid="bib39">Ly et al., 2012</xref>). Attention, by shifting the system into a more stable state, allows this cancellation to occur more efficiently, thereby reducing the pairwise covariance. <xref ref-type="fig" rid="fig14">Appendix 1—figure 6m</xref> shows the input current correlation functions of the total recurrent inputs to pairs of excitatory neurons, normalized to peak at <inline-formula><mml:math id="inf866"><mml:mn>1</mml:mn></mml:math></inline-formula>. We conclude that the correlation cancellation brought about by recurrent inhibitory feedback suppresses correlations of the total recurrent input, which in turn decreases the output correlations.</p></sec></sec></boxed-text></app></app-group></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.23978.018</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing editor</role><aff><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Attentional modulation of neuronal variability in circuit models of cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors and the evaluation has been overseen by Timothy Behrens as the Senior Editor. The following individual involved in review of your submission has agreed to reveal his identity: Ruben Coen-Cagli (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>All three reviewers found the paper interesting and potentially important. Of particular note is the demonstration that a single source of attentional modulation in an E-I network can explain both increases in firing rate and decreases in noise covariance, without the need to postulate separate effects. In addition, the modelling is solid and convincing, and the result is an important advance in our understanding of attentional effects.</p><p>However, there are a couple if issues that need to be addressed:</p><p>1) Robustness to network parameters and assumptions needs to be explored.</p><p>2) The rank-1 covariance needs to be better quantified.</p><p>3) Information was about contrast, whereas the experiment was about orientation change detection. This is important because the rank 1 covariance matrix is unlikely to have much effect on information about orientation.</p><p>It won't be completely trivial to address these issues, but we believe they are addressable.</p><p>It is <italic>eLife</italic>'s policy to provide a summary of essential revisions. That's hard to do for these reviews, as they were all relatively extensive. I am, though, going to give it a shot. The exposition will be a little uneven, as I combined the reviews, without trying to edit for uniformity.</p><p>1) A big issue is robustness to parameters. Essentially what we want to know is: what are the constraints on parameter space for the results to hold qualitatively? It's probably hard to fully answer this, but it should be possible to provide answers for some of the more important parameters. Following are some more specific points.</p><p>First is a long comment about two of the modelling assumptions:</p><p>- The weights scale as 1/<italic>N</italic>.</p><p>- Perfect balance (J<italic><sub>EE</sub></italic> = J<italic><sub>IE</sub></italic> = J<italic><sub>E</sub></italic> and J<italic><sub>II</sub> </italic>= J<italic><sub>EI</sub></italic> = J<italic><sub>I</sub></italic>).</p><p>This 1/<italic>N</italic> scaling is different from the usual one, which is <inline-formula><mml:math id="inf867"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>and in that regime perfect balance is problematic. Granted, the <inline-formula><mml:math id="inf868"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>scaling is probably too large, but 1/<italic>N</italic> is probably too small. At the very least, the authors need to comment on this scaling – after all, the last author just published a paper on correlations which was based on <inline-formula><mml:math id="inf869"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>scaling. We're not suggesting that the analysis be redone, but it would be good to know whether the analysis really does apply to biologically plausible connectivity.</p><p>Now, a couple of technical comments relating to these points.</p><p>The first one is mainly a suggestion. The authors use the 1/<italic>N</italic> scaling to argue that the first term in Equation 3 of SM (the full-rank component of the covariance matrix) is O(1/<italic>N</italic>). But this may not be necessary. An instructive case is completely homogeneous coupling, in which J<italic><sub>ij</sub> </italic>depends only on the type (<italic>E</italic> versus <italic>I</italic>) of neurons <italic><sub>i</sub> </italic>and <italic><sub>j</sub></italic>, the probability of connection is 1. In this case, if the scaling is <inline-formula><mml:math id="inf870"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>a back of the envelope calculations indicates that the first term in Equation 3 of SM scales as 1/<italic>N</italic>. I believe that if iid noise is added to the weight matrix (while retaining the <inline-formula><mml:math id="inf871"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>scaling), the first term in Equation 3 of SM would still scale as 1/<italic>N</italic>, but I'm not sure. This should probably be checked: if it one turns out to be correct, it would go a long way toward dispelling doubts about the 1/<italic>N</italic> scaling.</p><p>Second, in Equation 7 the authors derive an expression for the variance over long time windows. This was derived under the perfect balance assumption. If that assumption is dropped, there's an additional term in the denominator that scales as L<italic><sub>E</sub></italic> L<italic><sub>I</sub> -Det(J)</italic> (where <italic>J</italic> is the 2x2 matrix of weights). If the components of <italic>J</italic> are large – as they probably are in realistic networks -- this can have a large effect. Is it possible to estimate its effect for realistic networks? How would that change the results?</p><p>A couple semi-minor points on robustness:</p><p>a) Equation 7 depends on <inline-formula><mml:math id="inf872"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><italic><sub>E</sub></italic> – σ<italic><sub>I</sub></italic>. That's taken to be negative (<xref ref-type="table" rid="tbl1">Table 1</xref>). How much do the results change if it's positive?</p><p>b) In real networks, an increase in drive (which is how top down attention is modelled) would probably lead to an increase in noise (because variance scales with spike count). I think it would be important to estimate how large an increase in noise could be tolerated without an increase in covariance with attention.</p><p>2) The rank-1 covariance</p><p>a) The authors tell us that the modulation matrix is close to being rank one, but they tell us nothing about the vector of modulation gains defining this rank one matrix. I would like to see answers to basic questions such as: What is the distribution of <italic>gs</italic>? Are they correlated with as (firing rate modulations) and/or with baseline firing rates? Or with the vectors obtained using low-rank approximation of the covariance matrix itself? The model must make specific predictions about the answers to all these questions, and it would be nice to see these predictions tested.</p><p>b) It is unclear why the authors used their method for low rank approximation, as opposed to more standard methods based on SVD (that naturally provides a quantification of the quality of a general low rank approximation based on singular values). I think it would be useful to check what they get using alternate methods, to check the robustness of their results.</p><p>c) The data in <xref ref-type="fig" rid="fig1">Figure 1D</xref> show a broad range of effects of attention on noise covariance, but the model addresses only the overall reduction in the mean, not any other property of the distribution (including the fact that there are a substantial minority of cases with increased covariance under attention). Isn't is possible to study the distribution across the network model, at least in simulations? And again, the structure of the covariance (and tuning) is important to determine information.</p><p>d) A separate, smaller issue is the assumption that attention acts as a low-rank modulation of noise covariance. The opening statement in the Results, subsection “Attention as a low-rank modulation of noise covariance<bold>”</bold> is that &quot;we need to first understand the dimension of attentional modulation&quot;, as if a model-comparison of some sort was going to be performed between low-rank and full-rank modulations. Instead, there is only a quantification that the low-rank assumption works reasonably well, but no comparison to a higher-rank description. Also, why is the assumption of a multiplicative effect better than e.g. additive modulation? This could be quantified.</p><p>3) Fisher information: contrast versus orientation.</p><p>My main concern is that I see a disconnect between the modeling and the data/experimental paradigm that motivate the modeling.</p><p>I am not convinced about the generality of the conclusions on the effects of attentional modulation on population coding in the model. The experiment is about orientation-change detection, but in the modeling the stimulus dependence is more like contrast (all neurons are identically modulated by the stimulus intensity) than like orientation (where a change in stimulus value would drive some neurons up, and others down). This is acknowledged in the closing paragraphs, and suggested as future work, but I wonder if it should instead be done as part of this paper. I am no expert in EI networks, I don't know how long it would take, but here is concretely what I would like to see and why. Add the stimulus drive in the actual network, not just in the mean field solutions. And while doing that, assume heterogeneity (and possibly nonlinearity) of tuning. If the stimulus acts like contrast, then doing the information analysis on the mean field is fine; but otherwise the mean field solution is effectively a suboptimal decoder (weight all neurons equally), and the conclusions about information may be only valid for that decoder, not for the optimal decoder. The rank-one external noise by itself does not limit information for orientation-like stimulus dimensions (unless you modify it to exactly align it with the signal) (e.g. Moreno-Bote et al., 2014), so some other source of differential correlations needs to be considered if you want the attentional modulation to have any chance of improving information.</p><p>We'll admit that this may be a hard one to address rigorously. But the authors should provide an extended discussion of this issue. And an attempt should be made to provide approximate calculations and/or estimates.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.23978.019</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>All three reviewers found the paper interesting and potentially important. Of particular note is the demonstration that a single source of attentional modulation in an E-I network can explain both increases in firing rate and decreases in noise covariance, without the need to postulate separate effects. In addition, the modelling is solid and convincing, and the result is an important advance in our understanding of attentional effects.</italic></p><p> <italic>However, there are a couple if issues that need to be addressed:</italic></p><p>We thank the reviewers for their very careful read of our manuscript and for their insightful comments concerning our work. We have made changes to the paper in response to these comments, and as a result we feel our manuscript has significantly improved. In particular, we have added two new figures (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig6">6</xref>). We have also incorporated the previous supplementary section as an appendix, in response to both reviewer requests as well as those from the editorial staff. Below, we list our responses to each detailed comment from the reviewers and summarize the changes we have made.</p><p> <italic>1) Robustness to network parameters and assumptions needs to be explored.</italic></p><p> <italic>2) The rank-1 covariance needs to be better quantified.</italic></p><p> <italic>3) Information was about contrast, whereas the experiment was about orientation change detection. This is important because the rank 1 covariance matrix is unlikely to have much effect on information about orientation.</italic></p><p> <italic>It won't be completely trivial to address these issues, but we believe they are addressable.</italic></p><p> <italic>It is eLife's policy to provide a summary of essential revisions. That's hard to do for these reviews, as they were all relatively extensive. I am, though, going to give it a shot. The exposition will be a little uneven, as I combined the reviews, without trying to edit for uniformity.</italic></p><p> <italic>1) A big issue is robustness to parameters. Essentially what we want to know is: what are the constraints on parameter space for the results to hold qualitatively? It's probably hard to fully answer this, but it should be possible to provide answers for some of the more important parameters. Following are some more specific points.</italic></p><p> <italic>First is a long comment about two of the modelling assumptions:</italic></p><p> <italic>- The weights scale as 1/N.</italic></p><p> <italic>- Perfect balance (J<sub>EE</sub> = J<sub>IE</sub> = J<sub>E</sub> and J<sub>II</sub> = J<sub>EI</sub> = J<sub>I</sub>).</italic></p><p>The reviewers raise two excellent points and we outline our response to both of them below.</p><p> <italic>This 1/N scaling is different from the usual one, which is</italic> <inline-formula><mml:math id="inf873"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> <italic>and in that regime perfect balance is problematic. Granted, the</italic> <inline-formula><mml:math id="inf874"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula><italic>scaling is probably too large, but 1/N is probably too small. At the very least, the authors need to comment on this scaling – after all, the last author just published a paper on correlations which was based on</italic> <inline-formula><mml:math id="inf875"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula><italic>scaling. We're not suggesting that the analysis be redone, but it would be good to know whether the analysis really does apply to biologically plausible connectivity.</italic></p><p>We understand the reviewer’s suggestion and also think that we should discuss the case where weights scale as <inline-formula><mml:math id="inf876"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> Our analysis aims to establish the network requirements for:</p><p>C1:<inline-formula><mml:math id="inf877"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msub><mml:mi>c</mml:mi><mml:mfrac><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>; attentional modulation of covariance is rank one (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><p>C2: g<sub>i</sub> &lt; 1: spike count covariance decreases with attention (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>We will argue below that <inline-formula><mml:math id="inf878"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>coupling will not affect how C1 is satisfied, but will technically prevent C2 (in the large N limit).</p><p>To establish C1 we first observe that the covariance C decomposes as:</p><p>Here the matrix K has element K<sub>ij</sub> = <italic>L<sub>i</sub>J<sub>ij</sub></italic> where <italic>L<sub>i</sub></italic> is the linear response and <italic>J<sub>ij</sub></italic> is the synaptic strength from neuron <italic>j</italic> to <italic>i</italic>. The issue at hand is that we require that the internally generated covariability vanish for large <italic>N</italic>. If the spectral radius of K is less than 1, then we can expand <inline-formula><mml:math id="inf879"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>(Pernice et al., 2011; Trousdale et al., 2012). Inserting this expansion into the expression for the internally generated covariability yields:</p><p>(I − K)<sup>−1</sup>B(I − K<sup>T)−1</sup> = B + BK<sup>T</sup> + KB + KBK<sup>T</sup> + · · ·.</p><p>Extracting the covariance between neuron <italic>i</italic> and <italic>j (i</italic> ≠ <italic>j</italic>) due to internal coupling within the network gives:</p><p><italic>c<sub>ijB</sub> = b<sub>i</sub>L<sub>j</sub>J<sub>ji</sub> + bjL<sub>i</sub>J<sub>ij</sub> +</italic> <inline-formula><mml:math id="inf880"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>K</mml:mi></mml:munder><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><italic><sub>i</sub>L<sub>j</sub>b<sub>k</sub>J<sub>ik</sub>J<sub>jk</sub> + · · ·.</italic></p><p>If we take <italic>J<sub>ij</sub></italic> ∿ 1/<italic>N</italic> and the network connectivity to be dense (meaning the connection probability is ∿ O(1)) then each term is O (1/N). So long as the spectral radius of K is less than 1 then the series converges and as <italic>N</italic> → ∞ we have that <italic>c<sub>ijB</sub></italic> vanishes (Pernice et al., 2011; Trousdale et al., 2012; Helias et al., 2014). As the reviewers intuit this argument can be extended to networks with <italic>J<sub>ij</sub></italic>∼ <inline-formula><mml:math id="inf881"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>, however the analysis is more complicated. When J ∼ <inline-formula><mml:math id="inf882"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>we require a <italic>balance condition</italic> where large excitation and inhibition effectively cancel. Renart et al., 2010 showed that even densely coupled balanced networks produce an asynchronous state where <italic>c<sub>ijB</sub></italic> ∿1/<italic>N</italic>, vanishing in the large <italic>N</italic> limit (Renart et al., 2010). Showing this requires that we consider the excitatory/inhibitory subnetworks and extend the balance condition so that <italic>c<sub>ijB</sub> </italic>is the sum of O(1) terms that nonetheless combine to be <italic>O</italic>(1/<italic>N</italic>).</p><p>While we appreciate the reviewers comment we feel that this is a somewhat tagential point and will derail the flow of the manuscript. The above arguments feature in our Supplementary Materials and in the main text we now write:</p><p>“Spiking covariability in recurrent networks can be due to internal interactions (through J<italic><sub>ik</sub></italic>) or external fluctuations (through ξ<italic><sub>i</sub></italic>), or both (Ocker, 2017). […] In these cases spiking covariability requires external fluctuations to be applied and subsequently filtered by the network.”</p><p>Condition <bold>C2</bold> involves the attentional modulation itself. In the main text we derived:</p><p>In the above each term involves the gain modulation <italic>LA − LU;</italic> recall that <italic>L</italic> = <italic>dr/dµ</italic> (slope of the firing rate curve). In networks with <italic>J ∼</italic> <inline-formula><mml:math id="inf883"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>then for <italic>N → ∞</italic> the balance condition takes precedence and the firing rate solution does not depend upon the firing rate function <italic>f</italic> and consequently neither upon <italic>L</italic> (vanVreeswijk and Sompolinsky, 1988). As such a modulation of <italic>L</italic> will not change the ability of a balanced network to track an external input. Of course, finite size balanced networks may rescue us from this, or including short term plasticity mechanisms will also impart nonlinearities in the transfer (Mongillo et al., 2012). But we again feel that these issues are best left for another paper. We have addressed this in our resubmitted manuscript by including the sentences:</p><p>“In the above we considered weak synaptic connections where <italic>J<sub>ij</sub> ∼</italic> 1<italic>/N.</italic> […] synaptic nonlinearities through short term plasticity (Mongillo et al., 2012) may be necessary to satisfy condition <bold>C2</bold> with large synapses.”</p><p><italic>Now, a couple of technical comments relating to these points.</italic></p><p> <italic>The first one is mainly a suggestion. The authors use the 1/N scaling to argue that the first term in Equation 3 of SM (the full-rank component of the covariance matrix) is O(1/N). But this may not be necessary. An instructive case is completely homogeneous coupling, in which J<sub>ij</sub> depends only on the type (E versus I) of neurons <sub>i</sub> and <sub>j</sub>, the probability of connection is 1. In this case, if the scaling is</italic> <inline-formula><mml:math id="inf884"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula><italic>), a back of the envelope calculations indicates that the first term in Equation 3 of SM scales as 1/N. I believe that if iid noise is added to the weight matrix (while retaining the</italic> <inline-formula><mml:math id="inf885"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> <italic>scaling), the first term in Equation 3 of SM would still scale as 1/N, but I'm not sure. This should probably be checked: if it one turns out to be correct, it would go a long way toward dispelling doubts about the 1/N scaling.</italic></p><p>We hope that the above discussion satisfies this point.</p><p> <italic>Second, in Equation 7 the authors derive an expression for the variance over long time windows. This was derived under the perfect balance assumption. If that assumption is dropped, there's an additional term in the denominator that scales as L<sub>E</sub> L<sub>I</sub> Det(J) (where J is the 2x2 matrix of weights). If the components of J are large – as they probably are in realistic networks – this can have a large effect. Is it possible to estimate its effect for realistic networks? How would that change the results?</italic></p><p>This is an excellent point. The assumption of perfect balance was made so that the formula for population variance was compact (our old Equation 7). However, the reduction of population variance with our attentional modulation is not critically dependent on this assumption. To demonstrate this we have added the following new text to the manuscript as well as a new figure exploring the robustness of our result.</p><p>“The expression for <italic>VE</italic> given above (Equation 7) assumes a symmetry in the network coupling, namely that <italic>J<sub>EE</sub> </italic>= <italic>J<sub>IE</sub><sup>≡</sup> J<sub>E</sub></italic>and <italic>J<sub>EI</sub></italic>= <italic>J<sub>II</sub><sup>≡</sup> J<sub>I</sub>.</italic> […] In total, the inhibitory mechanism for attention mediated reduction in population variability is robust to changes in the recurrent coupling with the network.”</p><p> <italic>A couple semi-minor points on robustness:</italic></p><p> <italic>a) Equation 7 depends on σ<sub>E</sub> – σ<sub>I</sub>. That's taken to be negative (<xref ref-type="table" rid="tbl1">Table 1</xref>). How much do the results change if it's positive?</italic></p><p> <italic>b) In real networks, an increase in drive (which is how top down attention is modelled) would probably lead to an increase in noise (because variance scales with spike count). I think it would be important to estimate how large an increase in noise could be tolerated without an increase in covariance with attention.</italic></p><p>There are several parameters that we could vary, as well as the transfer functions <italic>fE</italic>and <italic>fI</italic> themselves. In most cases the parameters like <italic>σ</italic> are scaled by <italic>J</italic>. We hope that by changing the synaptic coupling <italic>J</italic> (see new <xref ref-type="fig" rid="fig6">Figure 6</xref>) that we have addressed these concerns somewhat. A full analysis remains to be done.</p><p> <italic>2) The rank-1 covariance</italic></p><p> <italic>a) The authors tell us that the modulation matrix is close to being rank one, but they tell us nothing about the vector of modulation gains defining this rank one matrix. I would like to see answers to basic questions such as: What is the distribution of gs? Are they correlated with as (firing rate modulations) and/or with baseline firing rates? Or with the vectors obtained using low-rank approximation of the covariance matrix itself? The model must make specific predictions about the answers to all these questions, and it would be nice to see these predictions tested.</italic></p><p>The reviewers raise some important points. To address them we have added some new analysis and a new figure to the manuscript.</p><p>“To further validate our model we show the distribution of <italic>g<sub>i</sub></italic>s computed from the entire data set (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). […] This indicates that the circuit modulation of firing rates and covariance may not be trivially related to one another (Doiron et al., 2016).”</p><p> <italic>b) It is unclear why the authors used their method for low rank approximation, as opposed to more standard methods based on SVD (that naturally provides a quantification of the quality of a general low rank approximation based on singular values). I think it would be useful to check what they get using alternate methods, to check the robustness of their results.</italic></p><p>The low rank approximation we require is that [<bold>gg</bold><italic>T</italic>]<italic>ij</italic> = [<bold>C</bold><italic>U</italic>]<italic>ij /</italic>[<bold>C</bold><italic>A</italic>]<italic>ij</italic> (here [<bold>A</bold>]<italic>ij</italic> denotes the <italic>ij</italic>th element of matrix <bold>A</bold>). The number of trials are limited in our data and while we are confident that <bold>C</bold> is well estimated the ratio matrix [<bold>C</bold><italic>U</italic>]<italic>ij /</italic>[<bold>C</bold><italic>A</italic>]<italic>ij</italic> unfortunately is not. The justification for the low rank approximation of <italic>C</italic> is given in Rabinowitz et al., 2016 as they analyze the exact same data that we have analyzed.</p><p> <italic>c) The data in <xref ref-type="fig" rid="fig1">Figure 1D</xref> show a broad range of effects of attention on noise covariance, but the model addresses only the overall reduction in the mean, not any other property of the distribution (including the fact that there are a substantial minority of cases with increased covariance under attention). Isn't is possible to study the distribution across the network model, at least in simulations? And again, the structure of the covariance (and tuning) is important to determine information.</italic></p><p>Our study focuses on a mean field theory of population dynamics. Necessarily, this theory cannot address the heterogeneity of correlations in the data. Our spiking simulations do have heterogeneity, owing to the random coupling within the network. However, the simplified binary coupling makes this heterogeneity quite weak. A full accounting of the spread in noise correlations shown in <xref ref-type="fig" rid="fig1">Figure 1</xref> would require a better understanding of the mechanistic source of the heterogeneity in the network firing rates and variability itself. We feel that this is beyond the scope of this study and hope that our field theory provides sufficient insight into the main mechanisms underlying attentional modulation.</p><p> <italic>d) A separate, smaller issue is the assumption that attention acts as a low-rank modulation of noise covariance. The opening statement in the Results, subsection “Attention as a low-rank modulation of noise covariance<bold>”</bold> is that &quot;we need to first understand the dimension of attentional modulation&quot;, as if a model-comparison of some sort was going to be performed between low-rank and full-rank modulations. Instead, there is only a quantification that the low-rank assumption works reasonably well, but no comparison to a higher-rank description. Also, why is the assumption of a multiplicative effect better than e.g. additive modulation? This could be quantified.</italic></p><p>A high rank model (rank <italic>N</italic>) of attentional modulation would always work perfectly since we would simply set <italic>gij</italic>= <inline-formula><mml:math id="inf886"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The problem is that each pair of neurons would require a modulation specific to them. From a biological standpoint this would be very complicated, while a rank 1 modulation is much simpler since only individual neurons need to be targeted. To express this thought we have reworded the text in the Results, subsection “Attention as a low-rank modulation of noise covariance” as follows (for reference Equation 1 is <bold>C</bold><italic>A</italic> = <italic>AC ◦</italic><bold>C</bold><italic>U</italic>):</p><p>“On the one hand <italic>A<sub>C</sub></italic>could be constructed from the ratios of the individual elements: <italic>g<sub>ij</sub></italic>=<inline-formula><mml:math id="inf887"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, with each pair of neurons (<italic>i, j</italic>) receiving an individualized attentional modulation <italic>gij</italic>of their shared variability (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, left). […] To test whether <italic>AC</italic> is low rank we analyzed the V4 population recordings during the visual attention task (<xref ref-type="fig" rid="fig1">Figure 1</xref>), specifically measuring <italic>AC</italic> under the assumption that <italic>AC</italic> is rank 1:</p><p><bold>C</bold><italic>A</italic> = <bold>gg</bold><italic>T ◦</italic> <bold>C</bold><italic>U.”</italic></p><p> <italic>3) Fisher information: contrast versus orientation.</italic></p><p><italic>My main concern is that I see a disconnect between the modeling and the data/experimental paradigm that motivate the modeling.</italic></p><p> <italic>I am not convinced about the generality of the conclusions on the effects of attentional modulation on population coding in the model. The experiment is about orientation-change detection, but in the modeling the stimulus dependence is more like contrast (all neurons are identically modulated by the stimulus intensity) than like orientation (where a change in stimulus value would drive some neurons up, and others down). This is acknowledged in the closing paragraphs, and suggested as future work, but I wonder if it should instead be done as part of this paper. I am no expert in EI networks, I don't know how long it would take, but here is concretely what I would like to see and why. Add the stimulus drive in the actual network, not just in the mean field solutions. And while doing that, assume heterogeneity (and possibly nonlinearity) of tuning. If the stimulus acts like contrast, then doing the information analysis on the mean field is fine; but otherwise the mean field solution is effectively a suboptimal decoder (weight all neurons equally), and the conclusions about information may be only valid for that decoder, not for the optimal decoder. The rank-one external noise by itself does not limit information for orientation-like stimulus dimensions (unless you modify it to exactly align it with the signal) (e.g. Moreno-Bote et al., 2014), so some other source of differential correlations needs to be considered if you want the attentional modulation to have any chance of improving information.</italic></p><p> <italic>We'll admit that this may be a hard one to address rigorously. But the authors should provide an extended discussion of this issue. And an attempt should be made to provide approximate calculations and/or estimates.</italic></p><p>We agree with the reviewers that we only grazed the surface of the implications of our attentional modulation mechanism for neural coding. Extending our model to include distributed tuning would be a natural extension, yet one that would be overly cumbersome for this study. In particular, this would involve putting the network on a ring (at minimum) where orientation preference is coded. We would need to choose (and explore) how noise correlations depends on the spatial scale of recurrent interactions, the spatial scale of feedforward stimulus tuning, and even allow attentional modulation to be localized on the ring (to model feature attention). These aspects make a full study worthy of its own report, and we (Doiron and Cohen) currently have a student working on this project.</p><p>Rather, we chose to highlight only the simplest consequences of the attentional modulation mechanism we present. If we use a linear framework to study the input-output transfer of both signal and noise then in our simplified model the attentional modulation has no affect on information transfer as decoded from the whole network. However, if the decoder has access to only the excitatory population then 1) the code is suboptimal, and 2) attention can now improve the code.</p><p>By virtue of the scalar nature of the model the signal and noise are ‘aligned’ in a trivial sense (<italic>µ</italic> = <italic>ks</italic> + <italic>σξ</italic>(<italic>t</italic>)). The reviewers are correct to point out that should we consider a true population model with distributed tuning where these fluctuations are orthogonal (meaning ‘not parallel’ when speaking about high dimensions) to the dimension over which stimulus is coded then our treatment is overly simplistic. Since extending the model to include a feature dimension is the focus of a later study we will rather interpret our stimulus or population in simpler terms. When first setting up the stimulus we now write:</p><p>“Our model captures a bulk firing rate <italic>rE</italic> rather than a population model with distributed tuning. Because of this the stimulus <italic>s</italic> should either be conceived as the contrast of an input, or a population conceived as a collection of identically-tuned neurons (i.e a single cortical column).”</p><p>In the section where we analyze the stimulus estimation by our model we now write:</p><p>“As mentioned above our simplified mean field model (Equation 6) considers only a bulk response, where any individual neuron tuning is lost. As such a proper analysis of population coding is not possible. Nonetheless, our model has two basic features often associated with enhanced coding, decreased population variability (<xref ref-type="fig" rid="fig5">Figure 5</xref>) and increased stimulus-response gain (<xref ref-type="fig" rid="fig7">Figure 7</xref>).”</p><p>Finally, in the Discussion section we now write:</p><p>“Determining the impact of population-wide spiking variability on neural coding is complicated (Averbeck et al., 2006, Kohn et al., 2016). […] It is not clear how noise correlations will depend on these choices yet work in spatially distributed balanced networks shows that solutions can be complex (Rosenbaum et al., 2017).”</p></body></sub-article></article>