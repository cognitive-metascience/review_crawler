<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">06121</article-id><article-id pub-id-type="doi">10.7554/eLife.06121</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Cortical network architecture for context processing in primate brain</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-23043"><name><surname>Chao</surname><given-names>Zenas C</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-23380"><name><surname>Nagasaka</surname><given-names>Yasuo</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-23381"><name><surname>Fujii</surname><given-names>Naotaka</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Laboratory for Adaptive Intelligence</institution>, <institution>RIKEN Brain Science Institute</institution>, <addr-line><named-content content-type="city">Wako-shi</named-content></addr-line>, <country>Japan</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-1044"><name><surname>Behrens</surname><given-names>Timothy</given-names></name><role>Reviewing editor</role><aff><institution>Oxford University</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>zenas.c.chao@gmail.com</email> (ZCC);</corresp><corresp id="cor2"><email>na@brain.riken.jp</email> (NF)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>29</day><month>09</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>4</volume><elocation-id>e06121</elocation-id><history><date date-type="received"><day>16</day><month>12</month><year>2014</year></date><date date-type="accepted"><day>30</day><month>08</month><year>2015</year></date></history><permissions><copyright-statement>© 2015, Chao et al</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Chao et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-06121-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.06121.001</object-id><p>Context is information linked to a situation that can guide behavior. In the brain, context is encoded by sensory processing and can later be retrieved from memory. How context is communicated within the cortical network in sensory and mnemonic forms is unknown due to the lack of methods for high-resolution, brain-wide neuronal recording and analysis. Here, we report the comprehensive architecture of a cortical network for context processing. Using hemisphere-wide, high-density electrocorticography, we measured large-scale neuronal activity from monkeys observing videos of agents interacting in situations with different contexts. We extracted five context-related network structures including a bottom-up network during encoding and, seconds later, cue-dependent retrieval of the same network with the opposite top-down connectivity. These findings show that context is represented in the cortical network as distributed communication structures with dynamic information flows. This study provides a general methodology for recording and analyzing cortical network neuronal communication during cognition.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.001">http://dx.doi.org/10.7554/eLife.06121.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.06121.002</object-id><title>eLife digest</title><p>If we see someone looking frightened, the way we respond to the situation is influenced by other information, referred to as the ‘context’. For example, if the person is frightened because another individual is shouting at them, we might try to intervene. However, if the person is watching a horror video we may decide that they don't need our help and leave them to it. Nevertheless, it is not clear how the brain processes the context of a situation to inform our response.</p><p>Here, Chao et al. developed a new method to study electrical activity across the whole of the brain and used it to study how monkeys process context in response to several different social situations. In the experiments, monkeys were shown video clips in which one monkey—known as the ‘video monkey’—was threatened by a human or another monkey, or in which the video monkey is facing an empty wall (i.e., in three different contexts). Afterwards, the video monkey either displays a frightened expression or a neutral one. Chao et al. found that if the video monkey looked frightened by the context, the monkeys watching the video clip shifted their gaze to observe the apparent threat. How these monkeys shifted their gaze depended on the context, but this behavior was absent when the video monkey gave a neutral expression.</p><p>The experiments used an array of electrodes that covered a wide area of the monkeys' brains to record electrical activity of nerve cells as the monkeys watched the videos. Chao et al. investigated how brain regions communicated with each other in response to different contexts, and found that the information of contexts was presented in the interactions between distant brain regions. The monkeys' brains sent information from a region called the temporal cortex (which is involved in processing sensory and social information), to another region called the prefrontal cortex (which is involved in functions such as reasoning, attention, and memory). Seconds later, the flow of information was reversed as the monkeys utilized information about the context to guide their behavior.</p><p>Chao et al.'s findings reveal how information about the context of a situation is transmitted around the brain to inform a response. The next challenge is to experimentally manipulate the identified brain circuits to investigate if problems in context processing could lead to the inappropriate responses that contribute to schizophrenia, post-traumatic stress disorder and other psychiatric disorders.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.002">http://dx.doi.org/10.7554/eLife.06121.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>macaque</kwd><kwd>electrocorticography</kwd><kwd>connectivity</kwd><kwd>cognitive context</kwd><kwd>neuronal interaction</kwd><kwd>mesoscopic network</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001700</institution-id><institution>Ministry of Education, Culture, Sports, Science, and Technology</institution></institution-wrap></funding-source><award-id>Grant-in-Aid for Scientific Research on Innovative Areas, 23118002</award-id><principal-award-recipient><name><surname>Fujii</surname><given-names>Naotaka</given-names></name></principal-award-recipient></award-group><funding-statement>The funder had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.3</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Large-scale electrocorticography and big data analysis of brain-wide neuronal interactions reveal the architecture of network information flow for context processing in primate brain.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Context is the contingent sensory or cognitive background for a given situation. Different contexts can dramatically alter perception, cognition, or emotional reactions and decision-making, and in the brain network context can be represented during sensory encoding or mnemonic retrieval. The study of context is important for understanding the link between perception and cognition, in terms of both behavioral and neural processing, and the neural mechanisms underlying contextual information processing have been studied in a variety of domains including visual perception (<xref ref-type="bibr" rid="bib4">Bar, 2004</xref>; <xref ref-type="bibr" rid="bib52">Schwartz et al., 2007</xref>), emotion (<xref ref-type="bibr" rid="bib21">De Gelder, 2006</xref>; <xref ref-type="bibr" rid="bib7">Barrett et al., 2007</xref>; <xref ref-type="bibr" rid="bib6">Barrett and Kensinger, 2010</xref>), language (<xref ref-type="bibr" rid="bib34">Hagoort, 2005</xref>; <xref ref-type="bibr" rid="bib3">Aravena et al., 2010</xref>), and social cognition (<xref ref-type="bibr" rid="bib39">Ibañez and Manes, 2012</xref>). In the brain, context is proposed to require an interplay between bottom-up and top-down information processing in distributed neural networks (<xref ref-type="bibr" rid="bib58">Tononi and Edelman, 1997</xref>; <xref ref-type="bibr" rid="bib29">Friston, 2005</xref>). However, a comprehensive functional view of the brain circuits that mediate contextual processing remains unknown because bottom-up and top-down processes are often concurrent and interdependent, making the temporal and spatial resolution of their neural network organization difficult to separate.</p><p>To understand contextual information processing, we developed a fundamentally new approach to study high-resolution brain network architecture. The approach combines broadband neural recording of brain activity at high spatial and temporal resolution with big data analytical techniques to enable the computational extraction of latent structure in functional network dynamics. We employed this novel methodological pipeline to identify functional network structures underlying fast, internal, concurrent, and interdependent cognitive processes during context processing in monkeys watching video clips with sequentially staged contextual scenarios. Each scenario contained a conspecific showing emotional responses preceded by different situational contexts. With specific combinations of context and response stimuli, this paradigm allowed an examination of context-dependent brain activity and behavior by isolating context processing as a single variable in the task.</p><p>To measure large-scale brain network dynamics with sufficient resolution, we used a 128-channel hemisphere-wide high-density electrocorticography (HD-ECoG) array to quantify neuronal interactions with high spatial, spectral, and temporal resolution. This ECoG system has wider spatial coverage than conventional ECoG and LFP (<xref ref-type="bibr" rid="bib17">Buschman and Miller, 2007</xref>; <xref ref-type="bibr" rid="bib48">Pesaran et al., 2008</xref>; <xref ref-type="bibr" rid="bib33">Haegens et al., 2011</xref>) and single–unit activity (<xref ref-type="bibr" rid="bib31">Gregoriou et al., 2009</xref>), higher spatial resolution than MEG (<xref ref-type="bibr" rid="bib32">Gross et al., 2004</xref>; <xref ref-type="bibr" rid="bib56">Siegel et al., 2008</xref>), broader bandwidth than EEG (<xref ref-type="bibr" rid="bib37">Hipp et al., 2011</xref>), and superior temporal resolution to fMRI (<xref ref-type="bibr" rid="bib50">Rees and House, 2005</xref>; <xref ref-type="bibr" rid="bib27">Freeman et al., 2011</xref>). After recording, we interrogated large-scale functional network dynamics using a multivariate effective connectivity analysis to quantify information content and directional flow within the brain network (<xref ref-type="bibr" rid="bib9">Blinowska, 2011</xref>; <xref ref-type="bibr" rid="bib18">Chao and Fujii, 2013</xref>) followed by big data analytical approaches to search the database of broadband neuronal connectivity for a latent organization of network communication structures.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Large-scale recording of brain activity during video presentations</title><p>Monkeys watched video clips of another monkey (video monkey, or vM) engaging with a second agent (<xref ref-type="fig" rid="fig1">Figure 1</xref>) while cortical activity was recorded with a 128-channel ECoG array covering nearly an entire cerebral hemisphere. Three monkeys participated, one with a right hemisphere array (Subject 1), and two in the right (Subjects 2 and 3) (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The data are fully accessible online and can be downloaded from the website <ext-link ext-link-type="uri" xlink:href="http://Neurotycho.org">Neurotycho.org</ext-link>.<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.06121.003</object-id><label>Figure 1.</label><caption><title>Subjects observe situational contexts with high-density electrocorticography (HD-ECoG) recording.</title><p>We recorded 128-channel HD-ECoG signals from monkeys viewing video clips of a conspecific under three different situational contexts and two responses. The subject (lower-left, green circles represent ECoG electrodes) was seated in front of a TV monitor showing video clips consisting of a <italic>Waiting</italic> period of 2.5 s followed by a <italic>Context</italic> period of 1.5 s with one of three interactions between a video monkey (vM) on the left and a second agent on the right: vM threatened by a human (<italic>C</italic><sub><italic>h</italic></sub>), threatened by another monkey (<italic>C</italic><sub><italic>m</italic></sub>), or an empty wall (<italic>C</italic><sub><italic>w</italic></sub>). Next, a curtain closed to conceal the second agent followed by a <italic>Response</italic> period of 3 s with the vM showing either a frightened (<italic>R</italic><sub><italic>f</italic></sub>), or neutral expression (<italic>R</italic><sub><italic>n</italic></sub>). Pairwise combination of the contexts and responses produced six different video clips.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.003">http://dx.doi.org/10.7554/eLife.06121.003</ext-link></p></caption><graphic xlink:href="elife-06121-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06121.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Electrode locations in 3 subjects.</title><p>In Subject 1, electrodes (green dots) were placed to cover most of the lateral surface of the right hemisphere, also the medial parts of the frontal and occipital lobes. In Subject 2, a similar layout was used, but in the left hemisphere. In Subjects 3, all electrodes were placed on the lateral surface of the left hemisphere, and no medial parts were covered. For brain map registration, the electrode locations and the brain outlines from Subjects 1 and 3 were manually registered to those from Subject 2 based on 13 markers (red circles) in the lateral hemisphere and five markers in the medial hemisphere.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.004">http://dx.doi.org/10.7554/eLife.06121.004</ext-link></p></caption><graphic xlink:href="elife-06121-fig1-figsupp1-v1.tif"/></fig></fig-group></p><p>The video clips started with a context between the two agents (<italic>Context</italic> period) followed by a response to the context (<italic>Response</italic> period). Six different video clips were created from three contexts, vM threatened by a human (<italic>C</italic><sub><italic>h</italic></sub>), threatened by another monkey (<italic>C</italic><sub><italic>m</italic></sub>), or facing an empty wall (<italic>C</italic><sub><italic>w</italic></sub>), combined with two responses, vM showing a frightened expression (<italic>R</italic><sub><italic>f</italic></sub>), or neutral expression (<italic>R</italic><sub><italic>n</italic></sub>), which were termed <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, and <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub> (see <xref ref-type="other" rid="media1 media2 media3 media4 media5 media6">Videos 1–6</xref>). Each video contained audio associated with the event, for example, sounds of a threatening human (<italic>C</italic><sub><italic>h</italic></sub>) and a frightened monkey (<italic>R</italic><sub><italic>f</italic></sub>). Each video represents a unique social context-response scenario, For example, <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> shows a human threatening a monkey (vM) followed by the monkey's frightened response. These staged presentations were designed to examine whether different contexts (<italic>C</italic><sub><italic>h</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub>, or <italic>C</italic><sub><italic>w</italic></sub>) would give rise to context-dependent brain activity even with the same responses (<italic>R</italic><sub><italic>f</italic></sub> or <italic>R</italic><sub><italic>n</italic></sub>).</p></sec><sec id="s2-2"><title>Eye movements demonstrate context- and response-dependent behaviors</title><p>During the task, subjects freely moved their eyes to observe the video interactions. We monitored eye movements to examine these spontaneous behavioral reactions and the associated zones in the video. We divided the trials into two conditions based on whether the context stimulus was visually perceived: <italic>C</italic><sup><italic>+</italic></sup> where the subject was looking at the screen during the <italic>Context</italic> period, and <italic>C</italic><sup><italic>−</italic></sup> where the subject was either closing its eyes or looking outside of the screen. Example eye movements are shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p><p>We first investigated which side of the video monitor the monkey attended. When the context was perceived (<italic>C</italic><sup><italic>+</italic></sup>) and the response stimulus was <italic>R</italic><sub><italic>f</italic></sub>, subjects focused more on the right section during the <italic>Response</italic> period, indicating interest in the curtain, or the threat behind the curtain, than the frightened vM (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). This preference was absent when the response stimulus was <italic>R</italic><sub><italic>n</italic></sub> or the context was not visually perceived (<italic>C</italic><sup><italic>−</italic></sup>). This is behavioral evidence that gaze direction preference required not only the vM response, but also perception of the preceding context, which demonstrated a cognitive association between the perception of the context and response stimuli.<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.06121.011</object-id><label>Figure 2.</label><caption><title>Context- and response-dependent eye movements.</title><p>(<bold>A</bold>) Measurements of gaze shifting revealed a behavioral association between the context and response phases of the task. The gaze positions averaged from three subjects are shown for each trial type (<italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, and <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>) and condition (<italic>C</italic><sup><italic>+</italic></sup> and <italic>C</italic><sup><italic>−</italic></sup>). Gaze shifting was quantified by gaze positions significantly different from baseline values (α<sub>Bonf</sub> = 0.05, baseline: gray bar), and was found only in <italic>R</italic><sub><italic>f</italic></sub> trials under the <italic>C</italic><sup><italic>+</italic></sup> condition (upper-left panel, the timing of gaze shifts are indicated on top, where the color represents the trial type indicated on the right). Black vertical lines represent the following events (see labels on the x-axis): (a) onset of the <italic>Context</italic> period, (b) the curtain starts closing, (c) the curtain is fully closed, (d) onset of the <italic>Response</italic> period, and (e) end of the <italic>Response</italic> period (onset of the next trial). (<bold>B</bold>) Context and response dependence in gazing behavior. Gaze positions between different trial types were compared, separately in <italic>C</italic><sup><italic>+</italic></sup> and <italic>C</italic><sup><italic>−</italic></sup>. For each comparison (y-axis), the timing of significant differences are shown as circles (α<sub>Bonf</sub> = 0.05), where blue, green, and red circles represent context dependence in <italic>R</italic><sub><italic>f</italic></sub>, context dependence in <italic>R</italic><sub><italic>n</italic></sub>, and response dependence, respectively. Gazing behavior showed both response dependence and context dependence, but only in <italic>C</italic><sup><italic>+</italic></sup>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.011">http://dx.doi.org/10.7554/eLife.06121.011</ext-link></p></caption><graphic xlink:href="elife-06121-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06121.012</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Examples of eye movement.</title><p>Examples of eye position during <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> scenario under <italic>C</italic><sup><italic>+</italic></sup> condition from Subject 2. Sampled eye positions (circles) in the three periods (<italic>Waiting</italic>, <italic>Context</italic>, and <italic>Response</italic>) are shown, where the corresponding timings are indicated by the colorbar. Snapshots of the videos during the corresponding periods are also shown.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.012">http://dx.doi.org/10.7554/eLife.06121.012</ext-link></p></caption><graphic xlink:href="elife-06121-fig2-figsupp1-v1.tif"/></fig></fig-group></p><p>We then compared gazing behaviors from different trial types to identify behaviors selective to different scenarios (<italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, or <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>) and conditions (<italic>C</italic><sup><italic>+</italic></sup> or <italic>C</italic><sup><italic>−</italic></sup>). We performed nine pairwise comparisons on gaze positions from different scenarios, separating <italic>C</italic><sup><italic>+</italic></sup> and <italic>C</italic><sup><italic>−</italic></sup> conditions, to examine their context and response dependence. For context dependence, we compared behaviors from trials with different context stimuli but the same response stimulus (6 comparisons: <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, and <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> for context dependence in <italic>R</italic><sub><italic>f</italic></sub>; <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub> vs <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub> vs <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, and <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub> vs <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub> for context dependence in <italic>R</italic><sub><italic>n</italic></sub>). For response dependence, we compared behaviors from trials with the same context stimulus but with different response stimuli (3 comparisons: <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, and <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>).</p><p>A context and response dependence was found in gazing behavior (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). In <italic>C</italic><sup><italic>+</italic></sup>, significant differences in gaze position were found during the <italic>Response</italic> period between <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> and <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, and <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub> and <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, but not between <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> and <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> (blue circles in left panel). This indicated that gaze shifting in <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> and <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> was comparable and stronger than in <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>. This context dependence was absent when the response stimuli were <italic>R</italic><sub><italic>n</italic></sub> (green circles in left panel). Furthermore, a significant response dependence was found during the <italic>Response</italic> period for all contexts (<italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, and <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>) (red circles in left panel) consistent with the results described in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. In <italic>C</italic><sup><italic>−</italic></sup>, the context and response dependence found in <italic>C</italic><sup><italic>+</italic></sup> was absent (right panel). These results indicated that the subjects' gaze shift during the <italic>Response</italic> period showed both response dependence (<italic>R</italic><sub><italic>f</italic></sub> &gt; <italic>R</italic><sub><italic>n</italic></sub>) and context dependence (<italic>C</italic><sub><italic>m</italic></sub> ≈ <italic>C</italic><sub><italic>h</italic></sub> &gt; <italic>C</italic><sub><italic>w</italic></sub>), but only when context was perceived (<italic>C</italic><sup><italic>+</italic></sup> &gt; <italic>C</italic><sup><italic>−</italic></sup>).</p></sec><sec id="s2-3"><title>Mining of large-scale ECoG data for cortical network interactions</title><p>To analyze the large-scale ECoG dataset, we identified cortical areas over the 128 electrodes in the array by independent component analysis (ICA). Each independent component (IC) represented a cortical area with statistically independent source signals (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, and experimental parameters in <xref ref-type="table" rid="tbl1">Table 1</xref>).<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.06121.013</object-id><label>Table 1.</label><caption><p>Experimental parameters</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.013">http://dx.doi.org/10.7554/eLife.06121.013</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th>Subject 1</th><th>Subject 2</th><th>Subject 3</th></tr></thead><tbody><tr><td rowspan="6">Experiment</td><td>Hemisphere implanted</td><td>Right</td><td>Left</td><td>Left</td></tr><tr><td># of electrodes</td><td>128</td><td>128</td><td>128</td></tr><tr><td># of trials per class</td><td>150</td><td>150</td><td>150</td></tr><tr><td># of trials preserved per class (mean ± std) (see trial screening in ‘Materials and methods’)</td><td>117.7 ± 3.5</td><td>122.2 ± 3.1</td><td>109.5 ± 3.6</td></tr><tr><td># of <italic>C</italic><sup><italic>+</italic></sup> trials per class (mean ± std)</td><td>64.8 ± 5.2</td><td>60.3 ± 6.7</td><td>57.3 ± 5.6</td></tr><tr><td># of <italic>C</italic><sup><italic>−</italic></sup> trials per class (mean ± std)</td><td>52.8 ± 1.9</td><td>61.8 ± 8.1</td><td>52.2 ± 3.5</td></tr><tr><td rowspan="2">ICA (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>)</td><td># of ICs for 90% variance explained</td><td>58</td><td>38</td><td>39</td></tr><tr><td># of ICs preserved (see IC screening in ‘Materials and methods’)</td><td>49 (removed ICs 1, 2, 3, 4, 5, 11, 44, 46, and 47)</td><td>33 (removed ICs 1, 2, 7, 8, and 29)</td><td>36 (removed ICs 2, 10, and 27)</td></tr></tbody></table></table-wrap></p><p>We then measured the causality of a connection from one cortical area (source area) to another (sink area) with a multivariate effective connectivity measure based on Granger causality: direct directed transfer function (dDTF) (<xref ref-type="bibr" rid="bib41">Korzeniewska et al., 2003</xref>), which can represent phase differences between the two source signals to provide a time-frequency representation of their asymmetric causal dependence. We acquired dDTFs from all connections for each trial type (12 types: six scenarios and two conditions), and measured event-related causality (ERC), by normalizing the dDTF of each time point and each frequency bin to the median of the corresponding baseline control values. Thus, ERCs represent the spectro-temporal dynamics of network interactions evoked by different scenarios and conditions. Examples of ERCs are shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.06121.014</object-id><label>Figure 3.</label><caption><title>Identification of latent structures in context- and response-dependent cortical network interactions.</title><p>(<bold>A</bold>) Event-related causalities (ERCs) between cortical areas. Example ERCs for a connection (IC 8 to IC 14, the corresponding cortical areas shown on the top) in two scenarios (<italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> and <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub> in <italic>C</italic><sup><italic>+</italic></sup>) from Subject 1 are shown. Each ERC represents the spectro-temporal dynamics of causality evoked by a scenario, calculated as the logarithmic ratio between the direct directed transfer function (dDTF) and corresponding baseline values (baseline: gray bar), and measured in decibel (dB). Black vertical lines represent task events explained in <xref ref-type="fig" rid="fig2">Figure 2</xref>. (<bold>B</bold>) ∆ERCs, or the significant differences in ERCs between the two trial types (<italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>) (α<sub>FDR</sub> = 0.05, false discovery rate correction) are shown. The results were either 0 (no significant difference), +1 (significantly greater), or −1 (significantly weaker). (<bold>C</bold>) 3D tensor of ∆ERCs. The data for the entire study were organized in three dimensions: dynamics (top), function (middle), and anatomy (bottom). <italic>Top</italic>: ∆ERCs shown in <bold>B</bold> describe the dynamics of difference in causality of a connection between two trial types, presented as a vector in 3D space (illustrated as a bar, where each segment represents a ∆ERC value). <italic>Middle</italic>: For the same connection, ∆ERCs from other comparisons were pooled to describe the functional dynamics of the connection (illustrated as a plate). <italic>Bottom</italic>: Functional dynamics from all connections were pooled to summarize the functional network dynamics in a subject (illustrated as a block). The data from all subjects were further combined to assess common functional network dynamics across subjects. (<bold>D</bold>) Parallel factor analysis (PARAFAC) extracted five dominant structures from the 3D tensor with consistency (&gt;80%, also see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Each structure represented a unique pattern of network function, dynamics, and anatomy (e.g., Func. 1, Dyn. 1, and Anat. 1 for Structure 1).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.014">http://dx.doi.org/10.7554/eLife.06121.014</ext-link></p></caption><graphic xlink:href="elife-06121-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06121.015</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Independent component analysis (ICA) results from 3 subjects.</title><p>The spatial distribution of each IC and its time course are shown for each subject. For each IC, the size of each circle represents the relative contribution of the activity from the electrode to the IC, where red and blue colors represent the positive and negative contributions, respectively. For clarity, the time courses shown were obtained by averaging source signals of the IC over one trial type (<italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> trials under <italic>C</italic><sup><italic>+</italic></sup> condition), and the y-axis is not shown. For each time course, three red vertical lines represent the events (a), (d) and (e) described in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The ICs that were removed from analysis are labeled (see <xref ref-type="table" rid="tbl1">Table 1</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.015">http://dx.doi.org/10.7554/eLife.06121.015</ext-link></p></caption><graphic xlink:href="elife-06121-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06121.016</object-id><label>Figure 3—figure supplement 2.</label><caption><title>PARAFAC revealed five dominant structures in the 3D tensor.</title><p>A core consistency diagnostic was used to evaluate how well the tensor can be represented by different numbers of structures. During deconvolving the tensor into different numbers of structures, core consistencies were measured by two methods: DTLD/GRAM and random values (see the ‘Materials and methods’). For the random values method, 100 core consistency values were measured and their means and standard deviations are shown. In both methods, a sharp decrease in consistency was found when the number of structures increased from 5 to 6, indicating that five structures yielded the optimal fit.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.016">http://dx.doi.org/10.7554/eLife.06121.016</ext-link></p></caption><graphic xlink:href="elife-06121-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06121.017</object-id><label>Figure 3—figure supplement 3.</label><caption><title>Five latent network structures were robust against ICA model order selection.</title><p>(<bold>A</bold>) PARAFAC results (with DTLD/GRAM method) from data obtained from ICA with 90% (ICA<sub>90%</sub>, as in <xref ref-type="fig" rid="fig2">Figure 2D</xref>), 80% (ICA<sub>80%</sub>), and 75% (ICA<sub>75%</sub>) of total variance preserved. In all cases, five latent network structures yelled the optimal fits. (<bold>B</bold>) Similarities among structures obtained from different ICA results. Top row: We first compared Structure <italic>i</italic> to Structure <italic>j</italic> obtained from ICA<sub>90%</sub> (90% vs 90%). Correlations were evaluated in four different domains: Comparison (the first tensor dimension), Time and Frequency (the second tensor dimension), and Causal outflow (the third tensor dimension). The significant correlations (α = 0.05) are indicated as asterisks, and the correlations with high correlation coefficients (Pearson, <italic>ρ</italic> &gt; 0.8) are indicated as circles. Second row: Correlations between Structure <italic>i</italic> obtained from ICA<sub>80%</sub> and Structure <italic>j</italic> obtained from ICA<sub>90%</sub> (80% vs 90%). The structures obtained from ICA<sub>80%</sub> were reordered so that the correlations in the diagonals were maximized. Bottom row: Correlations between Structure <italic>i</italic> obtained from ICA<sub>75%</sub> and Structure <italic>j</italic> obtained from ICA<sub>90%</sub> (75% vs 90%). High correlations were found in the diagonals in 80% vs 90% and 75% vs 90%, and high similarities were found among 90% vs 90%, 80% vs 90%, and 75% vs 90%. These results indicate that the five latent network structures were similar under different selections of ICA model order.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.017">http://dx.doi.org/10.7554/eLife.06121.017</ext-link></p></caption><graphic xlink:href="elife-06121-fig3-figsupp3-v1.tif"/></fig></fig-group></p><p>We compared ERCs from different trial types to identify networks selectively activated in different scenarios (<italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, or <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>) and conditions (<italic>C</italic><sup><italic>+</italic></sup> or <italic>C</italic><sup><italic>−</italic></sup>). We performed nine pairwise comparisons on ERCs from different scenarios, separating <italic>C</italic><sup><italic>+</italic></sup> and <italic>C</italic><sup><italic>−</italic></sup> trials, to examine their context and response dependence. To examine context dependence, we compared ERCs from trials with different contexts but the same response (6 comparisons). In contrast, to examine response dependence, we compared ERCs from trials with the same context but with different responses (3 comparisons). This approach is similar to the eye movement analysis (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The comparisons were performed with a subtractive approach to derive a significant difference in ERCs (∆ERCs, <xref ref-type="fig" rid="fig3">Figure 3B</xref>). Hence, ∆ERCs revealed network connections, with corresponding time and frequency, where ERCs were significantly stronger or weaker in one scenario compared to another.</p><p>We pooled ∆ERCs from all comparisons, conditions, connections, and subjects, to create a comprehensive broadband library of network dynamics for the entire study. To organize and visualize the dataset, we created a tensor with three dimensions: <italic>Comparison-Condition, Time-Frequency</italic>, and <italic>Connection-Subject</italic>, for the functional, dynamic, and anatomical aspects of the data, respectively (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The dimensionality of the tensor was 18 (nine comparisons under two conditions) by 3040 (160 time windows and 19 frequency bins) by 4668 (49 × 48 connections for Subject 1, 33 × 32 for Subject 2, and 36 × 35 for Subject 3).</p><p>To extract structured information from this high-volume dataset, we deconvolved the 3D tensor into multiple components by performing parallel factor analysis (PARAFAC), a generalization of principal component analysis (PCA) to higher order arrays (<xref ref-type="bibr" rid="bib35">Harshman and Lundy, 1994</xref>) and measured the consistency of deconvolution under different iterations of PARAFAC (<xref ref-type="bibr" rid="bib15">Bro and Kiers, 2003</xref>). Remarkably, we observed five dominant structures from the pooled ∆ERCs that represented functional network dynamics, where each structure contained a comprehensive fingerprint of network function, dynamics, and anatomy (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). These five structures were robust against model order selection for ICA (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>).</p></sec><sec id="s2-4"><title>Discrete structured representations of functional network dynamics</title><p>The five structures are shown in <xref ref-type="fig" rid="fig4">Figure 4</xref> (Structures 1 and 2) and <xref ref-type="fig" rid="fig5">Figure 5</xref> (Structures 3, 4, and 5). Each structure represented a unique functional network dynamics, described by its compositions in the three tensor dimensions. The first tensor dimension (panel A) represented the differences across comparisons for each structure. We identified the significant differences and reconstructed the activation levels to show how each structure was activated under different scenarios and conditions (see the ‘Materials and methods’). The second tensor dimension (panel B) represented spectro-temporal dynamics for each structure. The third tensor dimension (panel C) represented the anatomical connectivity pattern for each structure. We measured three connectivity statistics: (1) <italic>causal density</italic> is the sum of all outgoing and incoming causality for each area, showing areas with busy interactions; (2) <italic>causal outflow</italic> is the net outgoing causality of each area, indicating the sources and sinks of interactions; and (3) <italic>maximum flow between areas</italic> is the maximal causality of all connections between cortical areas (7 areas found with busy interactions were chosen) (see results for individual subjects in <xref ref-type="fig" rid="fig4s1 fig4s2 fig4s3">Figure 4—figure supplements 1–3</xref>). The extracted statistics were robust across all subjects with different electrode placements suggesting that the structures were bilaterally symmetric across hemispheres (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>).<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.06121.018</object-id><label>Figure 4.</label><caption><title>Network structures for perception of context and response.</title><p>Each structure was defined by three dimensions: function, dynamics, and anatomy. (<bold>A</bold>) <italic>Function</italic>: The function dimension showed each structure's context and response dependence. <italic>Top</italic>: For each structure, the first tensor dimension contained 18 differences for nine pairwise comparisons in the <italic>C</italic><sup><italic>+</italic></sup> or <italic>C</italic><sup><italic>−</italic></sup> condition. Significant differences are highlighted (*, α = 0.05, see the ‘Materials and methods’). <italic>Bottom</italic>: The comparisons with significant differences were used to reconstruct how each structure was selectively activated. Each oval and its vertical position represent the trial type and its activation level, respectively. Blue, green, or red arrows indicate significant context dependence under <italic>R</italic><sub><italic>f</italic></sub>, significant context dependence under <italic>R</italic><sub><italic>n</italic></sub>, and significant response dependence, respectively (each corresponds to a significance highlighted in the top panel). (<bold>B</bold>) <italic>Dynamics</italic>: The dynamics dimension indexed each structure's activation in different times and frequencies. Black vertical lines represent events, as explained in <xref ref-type="fig" rid="fig2">Figure 2</xref>. (<bold>C</bold>) <italic>Anatomy</italic>: The anatomy dimension showed each structure's activation in different connections. Three connectivity statistics, averaged across subjects after brain map registration, are shown on the lateral and medial cortices. <italic>Top</italic>: Cortical areas with greater causal density represent areas with busier interactions. <italic>Middle</italic>: Cortical areas with positive (red) and negative (blue) causal outflows represent the sources and sinks of interactions, respectively. <italic>Bottom</italic>: The direction and strength of each maximum flow between areas are indicated by the direction and size (and color) of an arrow, respectively. Seven cortical areas were determined for visualization: the visual (V), parietal (P), prefrontal (PF), medial prefrontal (mPF), motor (M), anterior temporal (aT), and posterior temporal (pT) cortices.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.018">http://dx.doi.org/10.7554/eLife.06121.018</ext-link></p></caption><graphic xlink:href="elife-06121-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06121.019</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Causal density in individual subjects.</title><p>Cortical areas with high causal density (busy traffic) are indicated as red. Results from individual subjects and the summarized results after brain map registration are shown for each latent network structure.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.019">http://dx.doi.org/10.7554/eLife.06121.019</ext-link></p></caption><graphic xlink:href="elife-06121-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06121.020</object-id><label>Figure 4—figure supplement 2.</label><caption><title>Causal outflow in individual subjects.</title><p>Cortical areas with overall positive causal outflow (source areas) are indicated shown in red, and those with overall negative causal outflow (sink areas) are shown in blue. Results from individual subjects and the summarized results after brain map registration are shown for each latent network structure.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.020">http://dx.doi.org/10.7554/eLife.06121.020</ext-link></p></caption><graphic xlink:href="elife-06121-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06121.021</object-id><label>Figure 4—figure supplement 3.</label><caption><title>Maximum flow between areas in individual subjects.</title><p>(<bold>A</bold>) The maximum information flows between cortical areas in all subjects. For each subject (column) and each structure (row), the maximal loadings of connections from each of the seven cortical areas (source, y-axis) to the others (sink, x-axis) are shown. Seven cortical areas were manually identified: the visual cortex (V), the parietal cortex (P), the posterior temporal cortex (pT), the anterior temporal cortex (aT), the motor cortex (M), the prefrontal cortex (PFC), and the medial PFC (mPF). The seven areas identified in all subjects are shown on the registered brain map on the right. (<bold>B</bold>) The average maximum information flow between areas across subjects presented in two different formats: matrices (left) and arrows (right).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.021">http://dx.doi.org/10.7554/eLife.06121.021</ext-link></p></caption><graphic xlink:href="elife-06121-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06121.022</object-id><label>Figure 4—figure supplement 4.</label><caption><title>Robust connectivity across subjects.</title><p>For each connectivity statistics (causal density, causal outflow, or maximum flow between areas) from each structure, we compared its values (after brain map registration) between each subject pair. As results, 15 correlation coefficients were acquired for each connectivity statistics to show how similar the connectivity statistics across subjects in all structures. The average correlation coefficients for causal density, causal outflow, and maximum flow between areas were 0.66 ± 0.08, 0.60 ± 0.05, and 0.69 ± 0.26, respectively (mean ± std, n = 15: 3 pairs, 5 structures). High correlations in connectivity statistics among subjects indicate that connectivity in each structure is robust across subjects.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.022">http://dx.doi.org/10.7554/eLife.06121.022</ext-link></p></caption><graphic xlink:href="elife-06121-fig4-figsupp4-v1.tif"/></fig></fig-group><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.06121.023</object-id><label>Figure 5.</label><caption><title>Network structures for context representation and modulation.</title><p>The function (<bold>A</bold>), dynamics (<bold>B</bold>), and anatomy (<bold>C</bold>) dimensions of Structures 3, 4, and 5. Structures 3 and 4 represent the initial formation/encoding and later reactivation/retrieval of abstract context information, respectively, and Structure 5 represents context-dependent top-down feedback that modulates eye gaze or visual attention. Same presentation details as in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.023">http://dx.doi.org/10.7554/eLife.06121.023</ext-link></p></caption><graphic xlink:href="elife-06121-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.06121.024</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Spatial and spectral characteristics of network structures.</title><p>(<bold>A</bold>) The correlations between the causal outflows between structures. For each structure, causal outflows were measured for all ICs in all subjects, which resulted in a 1 by 118 vector (= 49 + 33 + 36, see the numbers of ICs in <xref ref-type="table" rid="tbl1">Table 1</xref>). The first two principal components (PC 1 and PC 2) of the causal outflows of the five structures are shown in the inset. The correlations between the causal outflows were also measured. The significant correlations (α = 0.05) are indicated as asterisks, and the correlations with high correlation coefficients (Pearson, <italic>ρ</italic> &gt; 0.8) are indicated as circles. (<bold>B</bold>) The correlations between the frequency profiles of different structures. The frequency profile of each structure, shown in the inset, was quantified by averaging the corresponding loadings in the second tensor dimension (Time-Frequency) across time points, which resulted in a 1 by 19 vector. The correlations between the frequency profiles were then measured and shown.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.024">http://dx.doi.org/10.7554/eLife.06121.024</ext-link></p></caption><graphic xlink:href="elife-06121-fig5-figsupp1-v1.tif"/></fig></fig-group></p><p>Structure 1 was activated first, with context dependence only in the <italic>Context</italic> period (<italic>C</italic><sub><italic>m</italic></sub> &gt; <italic>C</italic><sub><italic>h</italic></sub> &gt; <italic>C</italic><sub><italic>w</italic></sub>) suggesting sensory processing that can discriminate between contextual stimuli, that is, context perception. The context dependence was weaker but remained in <italic>C</italic><sup><italic>−</italic></sup> (<italic>C</italic><sup><italic>+</italic></sup> &gt; <italic>C</italic><sup><italic>−</italic></sup>), suggesting that auditory information in context stimuli was processed. The spectral dynamics of Structure 1 emerged primarily in the high-γ band (&gt;70 Hz), and contained mostly bottom-up connections from the posterior to anterior parts of temporal cortex.</p><p>Structure 2 was the earliest activated in the <italic>Response</italic> period, with only response dependence (<italic>R</italic><sub><italic>f</italic></sub> &gt; <italic>R</italic><sub><italic>n</italic></sub>), and was independent of whether context stimuli were visually perceived (<italic>C</italic><sup><italic>+</italic></sup> ≈ <italic>C</italic><sup><italic>−</italic></sup>). Thus, Structure 2 corresponds to sensory processing that can discriminate between response stimuli, that is, response perception. Spectrally, Structure 2 emerged in both high-γ and β bands, and contained connections similar to those in Structure 1, with an additional communication channel from anterior temporal cortex to the prefrontal cortex (PFC). Therefore, Structures 1 and 2 represent the multisensory processing of audiovisual stimuli, and Structure 2 could underlie the additional evaluation of emotional valence associated with the stimuli.</p><p>Structure 3 was activated the second earliest in the <italic>Context</italic> period showing a generalized context dependence (<italic>C</italic><sub><italic>m</italic></sub> ≈ <italic>C</italic><sub><italic>h</italic></sub> &gt; <italic>C</italic><sub><italic>w</italic></sub>) representing the abstract categorization of the context (‘an indeterminate agent is threatening vM’). Similar to Structure 1, the dependence in Structure 3 was weaker in <italic>C</italic><sup><italic>−</italic></sup> (<italic>C</italic><sup><italic>+</italic></sup> &gt; <italic>C</italic><sup><italic>−</italic></sup>), which suggested that the creation of abstract contextual information depended on the initial perception of context stimuli. Structure 3 appeared mainly in the β band (10–30 Hz), and contained primarily bottom-up connections from the posterior temporal cortex (mainly the area TEO) to the anterior temporal cortex (mainly the temporal pole) and the lateral and medial PFC.</p><p>Structure 4 showed the same generalized context dependence as Structure 3, but during the <italic>Response</italic> period when context stimuli were absent and only in <italic>C</italic><sup><italic>+</italic></sup><italic>R</italic><sub><italic>f</italic></sub> (not in <italic>C</italic><sup><italic>+</italic></sup><italic>R</italic><sub><italic>n</italic></sub> and <italic>C</italic><sup><italic>−</italic></sup>). The absence of context dependence in <italic>C</italic><sup><italic>+</italic></sup><italic>R</italic><sub><italic>n</italic></sub> and <italic>C</italic><sup><italic>−</italic></sup> suggested that Structure 4 required both vM responses with high emotional valence and its context. Moreover, Structure 4 exhibited spatial and spectral characteristics similar to Structure 3 (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We conclude that Structures 3 and 4 represent the same or very similar neural substrate, differing only in when and how they were activated. Structure 3 corresponds to the initial formation/encoding of the contextual information, while Structure 4 represents the <italic>R</italic><sub><italic>f</italic></sub> -triggered reactivation/retrieval of the contextual information. Therefore, Structures 3 and 4 represent the generalized, abstract perceptual and cognitive content of the context.</p><p>Structure 5 showed context dependence (<italic>C</italic><sub><italic>m</italic></sub> ≈ <italic>C</italic><sub><italic>h</italic></sub> &gt; <italic>C</italic><sub><italic>w</italic></sub>) in <italic>C</italic><sup><italic>+</italic></sup><italic>R</italic><sub><italic>f</italic></sub> (not in <italic>C</italic><sup><italic>+</italic></sup><italic>R</italic><sub><italic>n</italic></sub> and <italic>C</italic><sup><italic>−</italic></sup>), and response dependence (<italic>R</italic><sub><italic>f</italic></sub> &gt; <italic>R</italic><sub><italic>n</italic></sub>) in <italic>C</italic><sup><italic>+</italic></sup> (not in <italic>C</italic><sup><italic>−</italic></sup>) during the <italic>Response</italic> period, and appeared mainly in α and low-β bands (5–20 Hz). Anatomically, the structure showed primarily top-down connections between posterior temporal cortex, the anterior temporal cortex, and the lateral and medial PFC. Remarkably, Structure 5 is the only one demonstrating clear top-down connections, with the same context and response dependence as the gaze behavior (see <xref ref-type="fig" rid="fig2">Figure 2B</xref>). These results suggest that Structure 5 corresponds to a network for the context-dependent feedback modulation of eye gaze or visual attention during the task, and the other four structures index internal processes that lead to this behavioral modulation.</p></sec><sec id="s2-5"><title>Functional, dynamical, and anatomical correlations between network structures</title><p>We investigated how the structures coordinated with each other during the task by examining how they correlated with each other in the functional, dynamical, and anatomical domains.</p><p>To study function, we evaluated how each structure's context and response dependence correlated with others', by measuring correlation coefficients of structures' differences in comparisons across contexts in <italic>R</italic><sub><italic>f</italic></sub>, across contexts in <italic>R</italic><sub><italic>n</italic></sub>, and across responses (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) (detailed in the ‘Materials and methods’). Significant correlations between two structures indicated that one structure's activation affected another's, and vice versa, demonstrating a causal interdependence or a common external driver. Across contexts in <italic>R</italic><sub><italic>f</italic></sub> (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, left), Structure 1 significantly correlated to Structures 3 and 4, which were themselves significantly correlated to Structure 5. However, across contexts in <italic>R</italic><sub><italic>n</italic></sub> (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, middle), a significant correlation was found only between Structures 1 and 3. These results confirmed that sensory perception of the context stimuli could be significantly correlated to the formation of an abstract context, and, in turn this abstract context could be significantly correlated to its reactivation and top-down modulation when a response had high emotional valence. Across responses (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, right), Structure 2 significantly correlated to Structure 4, which was itself significantly correlated to Structure 5. This indicated that that top-down modulation is the integration of response information and abstract context information.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.06121.025</object-id><label>Figure 6.</label><caption><title>Coordination and co-activation of network structures.</title><p>(<bold>A</bold>) <italic>Functional coordination</italic>: The coordination between structures was evaluated by the correlation coefficients between structures' context and response dependence (the differences shown in <xref ref-type="fig" rid="fig4 fig5">Figures 4A, 5A</xref>). Each panel illustrates how Structure <italic>i</italic> (y-axis) correlated with Structure <italic>j</italic> (x-axis) in context dependence in <italic>R</italic><sub><italic>f</italic></sub> (left), context dependence in <italic>R</italic><sub><italic>n</italic></sub> (middle), and response dependence (right). Significant correlations are indicated as asterisks (α = 0.05) (see ‘Materials and methods’). (<bold>B</bold>) <italic>Dynamic co-activation</italic>: The dynamics correlation was shown by correlation coefficients between structures' temporal and spectral activation. Each panel shows how Structure <italic>i</italic> correlated with Structure <italic>j</italic> in temporal dynamics (left) and frequency profile (right). Significant correlations are indicated as asterisks (α = 0.05). (<bold>C</bold>) <italic>Anatomical overlap</italic>: The anatomical similarity was indexed by the ratio of shared anatomical connections between structures. Each panel illustrates the ratio of the number of shared connections between Structures <italic>i</italic> and <italic>j</italic> and the total number of connections in Structure <italic>i</italic>. Results obtained from three subjects are shown separately. (<bold>D</bold>) Undirected pathways of connections shared by all structures for each subject (top), and those appearing in at least one structure for each subject (bottom). The lateral cortical surface is shown on the left for Subject 1, and on the right for Subjects 2 and 3. Shared pathways (lines) between two cortical areas (circles) of the top 1, 5, 10, and 25% connections are shown. Pathways with greater strengths are overlaid on those with weaker strengths.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.025">http://dx.doi.org/10.7554/eLife.06121.025</ext-link></p></caption><graphic xlink:href="elife-06121-fig6-v1.tif"/></fig></p><p>To examine dynamics, we tested whether network structures had mutually exclusive or overlapping spectro-temporal dynamics. We measured the temporal dynamics of each structure by summing up the activation in the second tensor dimension across frequencies. Significant correlations in temporal dynamics were found between structures activated in the <italic>Context</italic> period (Structures 1 and 3) and the <italic>Response</italic> period (Structures 2, 4, and 5) (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, left). We then measured the spectral profile of each structure by summing up the activation in the second tensor dimension over time. Significant correlations in spectral profiles were found among structures with β band activation (Structures 2, 3, 4, and 5) (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, right).</p><p>To investigate anatomy, we identified directed connections with the top 10% strengths in the third tensor dimension, and examined the shared top 10% connections between structures for each subject (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). The numbers of shared connections between all structures were particularly high (&gt;70% shared) between Structures 3 and 4 (abstract contextual information), and Structures 1 and 2 (perception). We examined the undirected pathways that exclude the directionality of connections and found pathways shared by all structures and subjects in and from the temporal cortex to PFC (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, top). Pathways appearing in at least one structure were widespread across cortex (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, bottom).</p><p>These results demonstrate the functional coordination and spatio-spectro-temporal co-activation of the five identified network structures, and reveal the multiplexing property of large-scale neuronal interactions in brain: simultaneous information transfer in similar frequency bands along similar anatomical pathways could be functionally reconstituted into distinct cognitive operations depending on other networks' ongoing status. This type of information would be difficult to extract from traditional EEG/MEG/fMRI analyses.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we demonstrate that context can be represented by dynamic communication structures involving distributed brain areas and coordinated within large-scale neuronal networks, or neurocognitive networks (<xref ref-type="bibr" rid="bib60">Varela et al., 2001</xref>; <xref ref-type="bibr" rid="bib28">Fries, 2005</xref>; <xref ref-type="bibr" rid="bib13">Bressler and Menon, 2010</xref>; <xref ref-type="bibr" rid="bib55">Siegel et al., 2012</xref>). Our analysis combines three critical properties of neurophysiology—function, dynamics, and anatomy—to provide a high-resolution large-scale description of brain network dynamics for context. The five network structures we identified reveal how contextual information can be encoded and retrieved to modulate behavior with different bottom-up or top-down configurations. The coordination of distributed brain areas explains how context can regulate diverse neurocognitive operations for behavioral flexibility.</p><sec id="s3-1"><title>Context is encoded by interactions of large-scale network structures</title><p>These findings show that context can be encoded in large-scale bottom-up interactions from the posterior temporal cortex to the anterior temporal cortex and the lateral and medial PFC. The PFC is an important node in the ‘context’ network (<xref ref-type="bibr" rid="bib46">Miller and Cohen, 2001</xref>; <xref ref-type="bibr" rid="bib4">Bar, 2004</xref>), where the lateral PFC is believed to be critical for establishing contingencies between contextually related events (<xref ref-type="bibr" rid="bib30">Fuster et al., 2000</xref>; <xref ref-type="bibr" rid="bib40">Koechlin et al., 2003</xref>), and the medial PFC is involved in context-dependent cognition (<xref ref-type="bibr" rid="bib54">Shidara and Richmond, 2002</xref>) and conditioning (<xref ref-type="bibr" rid="bib30">Fuster et al., 2000</xref>; <xref ref-type="bibr" rid="bib40">Koechlin et al., 2003</xref>; <xref ref-type="bibr" rid="bib26">Frankland et al., 2004</xref>; <xref ref-type="bibr" rid="bib49">Quinn et al., 2008</xref>; <xref ref-type="bibr" rid="bib43">Maren et al., 2013</xref>). Our results indicate that abstract contextual information can be encoded not only within the PFC, but in PFC interactions with lower-level perceptual areas in the temporal cortex. These dynamic interactions between unimodal sensory and multimodal association areas could explain the neuronal basis of why context networks can affect a wide range of cognitive processes, from lower-level perception to higher-level executive functions.</p><p>Apart from the bottom-up network structure that encodes abstract context, we discovered other network structures that process either lower-level sensory inputs for context encoding or integrate contextual information for behavioral modulation. Evidently, brain contextual processing, from initial perception to subsequent retrieval, is represented not by sequential activation but rather sequential modular communication among participating brain areas. Thus, we believe that the network structures we observed represent a module of modules, or ‘meta-module’ for brain communication connectivity. Further investigation of this meta-structure organization for brain network communication could help determine how deficits in context processing in psychiatric disorders such as schizophrenia (<xref ref-type="bibr" rid="bib5">Barch et al., 2003</xref>) and post-traumatic stress disorder (<xref ref-type="bibr" rid="bib45">Milad et al., 2009</xref>) could contribute to their etiology.</p></sec><sec id="s3-2"><title>Cognition as a modular organization consisting of network structures</title><p>These results suggest a basic structural organization of large-scale communication within brain networks that coordinate context processing, and provide insight into how apparently seamless cognition is constructed from these network communication modules. In contrast to previous studies where brain modularity is defined as a ‘community’ of spatial connections (<xref ref-type="bibr" rid="bib16">Bullmore and Sporns, 2009</xref>; <xref ref-type="bibr" rid="bib57">Sporns, 2011</xref>), or coherent oscillations among neuronal populations in overlapping frequency bands (<xref ref-type="bibr" rid="bib55">Siegel et al., 2012</xref>), our findings provide an even more general yet finer grained definition of modularity based on not only anatomical and spectral properties, but also temporal, functional, and directional connectivity data. The relationships among network properties in the functional, temporal, spectral, and anatomical domains revealed network structures whose activity coordinated with each other in a deterministic manner (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), despite being highly overlapping in time, frequency, and space (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Such multiplexed, yet large-scale, neuronal network structures could represent a novel meta-structure organization for brain network communication. Further studies will be needed to show whether these structures are components of cognition.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.06121.026</object-id><label>Figure 7.</label><caption><title>Context as a sequence of interactions between network structures.</title><p>(<bold>A</bold>) Coordination between network structures (S1 to S5, circles), under <italic>R</italic><sub><italic>n</italic></sub> (top) or <italic>R</italic><sub><italic>f</italic></sub> (bottom) responses. In both response contingencies, context perception (S1) encoded contextual information (S3). However, when the response stimulus contained high emotional valence (<italic>R</italic><sub><italic>f</italic></sub>, bottom), response perception (S2) reactivates the contextual information (S4), resulting in top-down modulation feedback (S5) that shares the same context and response dependence as the gazing behavior (black arrow and rounded rectangles). Green, blue, and red arrows represent correlations in context dependence in <italic>R</italic><sub><italic>n</italic></sub>, context dependence in <italic>R</italic><sub><italic>f</italic></sub>, and in response dependence, respectively (see <xref ref-type="fig" rid="fig6">Figure 6A</xref>). (<bold>B</bold>) Temporal, spectral, and spatial profiles and overlap in defined network structures. Network structures can be characterized by frequency range (labeled on the left) and connectivity pattern (shown on the right). Their temporal activations are plotted over trial time, with a ‘sound-like’ presentation, where a higher volume represents stronger activation. Black vertical lines represent the events as indicated in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.026">http://dx.doi.org/10.7554/eLife.06121.026</ext-link></p></caption><graphic xlink:href="elife-06121-fig7-v1.tif"/></fig></p></sec><sec id="s3-3"><title>Applications for large-scale functional brain network mapping</title><p>We developed an analytical approach using an unbiased deconvolution of comprehensive network activity under well-controlled and staged behavioral task conditions. This workflow enabled us to identify novel network structures and their dynamic evolution during ongoing behavior. In principle, this approach can be generally useful to investigate how network structures link neural activity and behavior. However, we caution that the latent network structures we identified were extracted computationally, and therefore will require further confirmatory experiments to verify their biological significance, particularly the causality of the connectivity patterns within each structure and the functional links bridging different structures. The biological meaning of the identified network structures could be achieved by selective manipulation of neuronal pathways by electrical or optogenetic stimulation linked to the ECoG array by neurofeedback, or neuropharmacological manipulations.</p><p>The general class of network structures we identified are not necessarily unique to context, By recording with a hemisphere-wide ECoG array and applying our analytical methodologies to other cognitive behaviors and tasks in non-human primates, we fully expect to observe similar network structures. Our approach of pooling large-scale data across subjects may be useful to extract network structures that are generalizable, because neural processes specific to individual subjects or trials will cancel. Indeed, the stable and consistent trial responses across subjects in our chronic ECoG recordings suggest that the network structures we isolated may be candidate innate, elemental units of brain organization. Conversely, future identification of unique differences in network structures between subjects could offer insight into structures related to individual trait and state variability, and the network-level etiology of brain diseases (<xref ref-type="bibr" rid="bib8">Belmonte et al., 2004</xref>; <xref ref-type="bibr" rid="bib59">Uhlhaas and Singer, 2006</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects and materials</title><p>Customized 128-channel ECoG electrode arrays (Unique Medical, Japan) containing 2.1 mm diameter platinum electrodes (1 mm diameter exposed from a silicone sheet) with an inter-electrode distances of 5 mm were chronically implanted in the subdural space in three Japanese macaques (Subjects 1, 2, and 3). The details of surgical methods can be found on <ext-link ext-link-type="uri" xlink:href="http://Neurotycho.org">Neurotycho.org</ext-link>. In Subject 1, electrodes were placed to cover most of the lateral surface of the right hemisphere, also the medial parts of the frontal and occipital lobes. In Subject 2, a similar layout was used, but in the left hemisphere. In Subjects 3, all electrodes were placed on the lateral surface of the left hemisphere, and no medial parts were covered. The reference electrode was also placed in the subdural space, and the ground electrode was placed in the epidural space. Electrical cables leading from the ECoG electrodes were connected to Omnetics connectors (Unique Medical) affixed to the skull with an adaptor and titanium screws. The locations of the electrodes were identified by overlaying magnetic resonance imaging scans and x-ray images. For brain map registration, the electrode locations and the brain outlines from Subjects 1 and 3 were manually registered to those from Subject 2 based on 13 markers in the lateral hemisphere and 5 markers in the medial hemisphere (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>All experimental and surgical procedures were performed in accordance with the experimental protocols (No. H24-2-203(4)) approved by the RIKEN ethics committee and the recommendations of the Weatherall report, ‘The use of non-human primates in research’. Implantation surgery was performed under sodium pentobarbital anesthesia, and all efforts were made to minimize suffering. No animal was sacrificed in this study. Overall care was managed by the Division of Research Resource Center at RIKEN Brain Science Institute. The animal was housed in a large individual enclosure with other animals visible in the room, and maintained on a 12:12-hr light:dark cycle. The animal was given food (PS-A; Oriental Yeast Co., Ltd., Tokyo, Japan) and water ad libitum, and also daily fruit/dry treats as a means of enrichment and novelty. The animal was occasionally provided toys in the cage. The in-house veterinary doctor checked the animal and updated daily feedings in order to maintain weight. We have attempted to offer as humane treatment of our subject as possible.</p></sec><sec id="s4-2"><title>Task design</title><p>During the task, each monkey was seated in a primate-chair with its arms and head gently restrained, while a series of video clips was presented on a monitor (<xref ref-type="other" rid="media1">Videos 1–6</xref>). In one recording session, each of six video clips was presented 50 times, and all 300 stimuli were presented in a pseudorandom order in which the same stimulus would not be successively presented. In order to keep the monkey's attention to the videos, food items were given after every 100 stimuli. Each monkey participated three recording sessions within a week. Each stimulus consisted of three periods: <italic>Waiting</italic>, <italic>Context</italic>, and <italic>Response</italic> periods. During the <italic>Waiting</italic> period, a still picture created by pixel-based averaging and randomizing the all frames of stimuli was presented without sound for 2. During the first 0.5 s of the <italic>Context</italic> period, a still image of an actor (a monkey) and an opponent (a monkey, a human, or wall) was presented with the sound associated with the opponents. The actor was always positioned on the left side of the image. Then a curtain in the video started to close from the right side toward the center to cover the opponent. The curtain closing animation took 0.5 s, and the curtain stayed closed for another 0.5 s. During the <italic>Response</italic> period, one of two emotional expressions of the actor (frightening or neutral) was presented with sound for 3 s, followed by the <italic>Waiting</italic> period of the next trial.</p></sec><sec id="s4-3"><title>ECoG and behavior recordings</title><p>An iMac personal computer (Apple, USA) was used to present the stimuli on a 24-in LCD monitor (IOData, Japan) located 60 cm away from the subject. The sound was presented through one MA-8BK monitor speaker (Roland, Japan) attached to the PC. The experiments were run by a program developed in MATLAB (MathWorks, USA) with Psychtoolbox-3 extensions (<xref ref-type="bibr" rid="bib12">Brainard, 1997</xref>). The same PC was used to control the experiments and the devices for recording monkey's gaze and neural signals via USB-1208LS data acquisition device (Measurement Computing Co., USA). A custom-made eye-track system was used for monitoring and recording the monkey's left (Subject 1) or right (Subjects 2 and 3) eye at 30 Hz sampling (<xref ref-type="bibr" rid="bib47">Nagasaka et al., 2011</xref>). Cerebus data acquisition systems (Blackrock Microsystems, USA) were used to record ECoG signals with a sampling rate of 1 kHz.</p></sec><sec id="s4-4"><title>Trial screening</title><p>Trials during which the subject's eye position was within the screen area more than 80% of the time during the first 0.5 s of the <italic>Context</italic> period were classified as <italic>C</italic><sup><italic>+</italic></sup> trials. The rest of the trials were identified as <italic>C</italic><sup><italic>−</italic></sup> trials, where the subject either closed its eyes or the eye position was outside the screen or outside the recording range (±30°).</p></sec><sec id="s4-5"><title>Data analysis</title><sec id="s4-5-1"><title>ICA</title><p>ICA was performed on the data combined <italic>C</italic><sup><italic>+</italic></sup> and <italic>C</italic><sup><italic>−</italic></sup> trials to acquire a common basis for easier interpretation of the results. On the other hand, dDTF and the following analyses (ERC and SD-ERC) were calculated from <italic>C</italic><sup><italic>+</italic></sup> and <italic>C</italic><sup><italic>−</italic></sup> trials separately, and later combined in PARAFAC analysis.</p><sec id="s4-5-1-1"><title>Preprocessing</title><p>The 50 Hz line noise was removed from raw ECoG data by using the Chronux toolbox (<xref ref-type="bibr" rid="bib11">Bokil et al., 2010</xref>). The data was then downsampled four times, resulted in a sampling rate of 250 Hz. Trials with abnormal spectra were rejected by using an automated algorithm from the EEGLAB library (<xref ref-type="bibr" rid="bib23">Delorme et al., 2011</xref>), which has been suggested as the most effective method for artifact rejection (<xref ref-type="bibr" rid="bib24">Delorme et al., 2007</xref>). The numbers of trials preserved are shown in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p></sec><sec id="s4-5-1-2"><title>Model order selection</title><p>The model order, that is, the number of components, for ICA was determined by the PCA of the data covariance matrix, where the number of eigenvalues accounted for 90% of the total observed variance. The resulted model order is shown in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p></sec><sec id="s4-5-1-3"><title>ICA algorithm</title><p>ICA was performed in multiple runs using different initial values and different bootstrapped data sets by using the ICASSO package (<xref ref-type="bibr" rid="bib36">Himberg et al., 2004</xref>) with the FastICA algorithm (<xref ref-type="bibr" rid="bib38">Hyvärinen and Oja, 1997</xref>), which could significantly improve the reliability of the results (<xref ref-type="bibr" rid="bib44">Meinecke et al., 2002</xref>). In the end, artifactual components with extreme values and abnormal spectra were discarded by using an automated algorithm from EEGLAB (<xref ref-type="bibr" rid="bib22">Delorme and Makeig, 2004</xref>). The number of components preserved after this screening process is shown in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p></sec></sec><sec id="s4-5-2"><title>Multivariate spectral causality</title><p>Spectral connectivity measures for multitrial multichannel data, which can be derived from the coefficients of the multivariate autoregressive model, require that each time series be covariance stationary, that is, its mean and variance remain unchanged over time. However, ECoG signals are usually highly nonstationary, exhibiting dramatic and transient fluctuations. A sliding-window method was implemented to segment the signals into sufficiently small windows, and connectivity was calculated within each window, where the signal is <italic>locally</italic> stationary.</p><sec id="s4-5-2-1"><title>Preprocessing</title><p>Three preprocessing steps were performed to achieve local stationarity: (1) detrending, (2) temporal normalization, and (3) ensemble normalization (<xref ref-type="bibr" rid="bib25">Ding et al., 2000</xref>). Detrending, which is the subtraction of the best-fitting line from each time series, removes the linear drift in the data. Temporal normalization, which is the subtraction of the mean of each time series and division by the standard deviation, ensures that all variables have equal weights across the trial. These processes were performed on each trial for each channel. Ensemble normalization, which is the pointwise subtraction of the ensemble mean and division by the ensemble standard deviation, targets rich task-relevant information that cannot be inferred from the event-related potential (<xref ref-type="bibr" rid="bib25">Ding et al., 2000</xref>; <xref ref-type="bibr" rid="bib14">Bressler and Seth, 2011</xref>).</p></sec><sec id="s4-5-2-2"><title>Window length selection</title><p>The length and the step size of the sliding-window for segmentation were set as 250 ms and 50 ms, respectively. The window length selection satisfied the general rule that the number of parameters should be &lt;10% of the data samples: to fit a VAR model with model order <italic>p</italic> on data of <italic>k</italic> dimensions (<italic>k</italic> ICs selected from ICA), the following relation needs to be satisfied: <italic>w</italic> ≥ 10 × (<italic>k</italic>^2 × <italic>p</italic>/<italic>n</italic>), where <italic>w</italic> and <italic>n</italic> represent the window length and the number of trials, respectively.</p></sec><sec id="s4-5-2-3"><title>Model order selection</title><p>Model order, which is related to the length of the signal in the past that is relevant to the current observation, was determined by the Akaike information criterion (AIC) (<xref ref-type="bibr" rid="bib1">Akaike, 1974</xref>). In all subjects, a model order of nine samples (equivalent to 9 × 4 = 36 ms of history) resulted in minimal AIC and was selected. The selected model order also passed the Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test, thus maintained local stationarity. Furthermore, the VAR model was validated by the whiteness test and the consistency test.</p></sec><sec id="s4-5-2-4"><title>Spectral connectivity</title><p>dDTF (<xref ref-type="bibr" rid="bib41">Korzeniewska et al., 2003</xref>), a VAR-based spectral connectivity measure, was calculated by using the Source Information Flow Toolbox (SIFT) (<xref ref-type="bibr" rid="bib23">Delorme et al., 2011</xref>) together with other libraries, such as Granger Causal Connectivity Analysis (<xref ref-type="bibr" rid="bib53">Seth, 2010</xref>) and Brain-System for Multivariate AutoRegressive Timeseries (<xref ref-type="bibr" rid="bib20">Cui et al., 2008</xref>). A detailed tutorial of VAR-based connectivity measures can be found in the SIFT handbook (<ext-link ext-link-type="uri" xlink:href="http://sccn.ucsd.edu/wiki/SIFT">http://sccn.ucsd.edu/wiki/SIFT</ext-link>).</p></sec></sec><sec id="s4-5-3"><title>ERC</title><p>To calculate ERC at time <italic>t</italic> and frequency <italic>f</italic>, or ERC(<italic>t, f</italic>), dDTF at time <italic>t</italic> and frequency <italic>f</italic>, or dDTF(<italic>t, f</italic>), was normalized by the median value during the baseline period at frequency <italic>f</italic>, or dDTF<sub>baseline</sub> (<italic>f</italic>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⊂</mml:mo><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>⋅</mml:mo><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></sec><sec id="s4-5-4"><title>Comparisons for context and response dependencies</title><p>Nine comparisons were performed on the ERCs obtained from different social scenarios in <italic>C</italic><sup><italic>+</italic></sup> and <italic>C</italic><sup><italic>−</italic></sup> trials separately. To examine context dependency, three comparisons were performed between scenarios with different contexts followed by the frightened response (<italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, and <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub>), and another three comparisons were performed between scenarios with different contexts followed by the neutral response (<italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub> − <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub> − <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, and <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub> − <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>). To examine response dependency, three comparisons were performed between scenarios with different responses under the same contexts (<italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, and <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>). False discovery rate (FDR) control was used to correct for multiple comparisons in multiple hypothesis testing, and a threshold of α<sub>FDR</sub> = 0.05 was used.</p></sec><sec id="s4-5-5"><title>PARAFAC</title><p>PARAFAC was performed by using the N-way toolbox (<xref ref-type="bibr" rid="bib2">Andersson and Bro, 2000</xref>), with the following constraints: no constraint on the first tensor dimension, and non-negativity on the second and the third tensor dimensions. The non-negativity constraint was introduced mainly for a more simple visualization of the results. The convergence criterion, that is, the relative change in fit for which the algorithm stops, was set to be 1e-6. The initialization method was set to be DTLD (direct trilinear decomposition) or GRAM (generalized rank annihilation method), which was considered the most accurate method (<xref ref-type="bibr" rid="bib19">Cichocki et al., 2009</xref>). Initialization with random orthogonalized values (repeated 100 times, each time with different random values) was also shown for comparison.</p></sec><sec id="s4-5-6"><title>Connectivity statistics</title><p>The connectivity statistics used in this study were calculated from the connectivity matrix (a weighted directed relational matrix) from each latent network structure and each subject, by using the Brain Connectivity Toolbox (<xref ref-type="bibr" rid="bib51">Rubinov and Sporns, 2010</xref>).</p><sec id="s4-5-6-1"><title>Causal density and outflow</title><p>Node strength was measured as the sum of weights of links connected to the node (IC). The causal density of each node was measured as the sum of outward and inward link weights (out-strength + in-strength), and the causal outflow of each node was measured as the difference between outward and inward link weights (out-strength—in-strength). For visualization, each measure was spatially weighted by the absolute normalized spatial weights of the corresponding IC. For example, assume the causal density and causal outflow of IC <italic>ic</italic> is <italic>Density</italic><sub><italic>ic</italic></sub>, and <italic>Outflow</italic><sub><italic>ic</italic></sub>, respectively, and the spatial weights of IC <italic>ic</italic> on channel <italic>ch</italic> is <italic>W</italic><sub><italic>ic,ch</italic></sub>, then the spatial distributions of causal density and causal outflow on each channel will be:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></sec><sec id="s4-5-6-2"><title>Maximum flow between areas</title><p>Seven cortical areas were first manually determined: the visual (V), parietal (P), prefrontal (PF), medial prefrontal (mPF), motor (M), anterior temporal (aT), and posterior temporal (pT) cortices. The maximum link among all links connected two areas was selected to represent the maximum flow between the two areas.</p></sec></sec><sec id="s4-5-7"><title>Activation levels from comparison loadings</title><p>For each experimental condition (<italic>C</italic><sup><italic>+</italic></sup> or <italic>C</italic><sup><italic>−</italic></sup> trials), we determined the significant comparisons for each latent network structure by performing trial shuffling. For each shuffle, dDTF and SR-ERC were recalculated after trial type was randomly shuffled. A new tensor was formed and we estimated how the five spectrotemporal connectivity structures identified from the original data contributed in the shuffled data, by performing PARAFAC on the tensor from the shuffled data with the last two tensor dimensions (Time-Frequency and Connection-Subject) fixed with the values acquired from the original data. Trial shuffling was performed 50 times. The loading from the original data that was significantly different than the loadings from the shuffled data is identified as the significant loading (α = 0.05). The comparisons with significant loadings showed the context and response dependencies of each structure, and further revealed the activation levels of each structure in different scenarios.</p></sec><sec id="s4-5-8"><title>Interdependencies among latent network structures</title><p>We determined the interdependencies among latent network structures by examining how the activation of one structure could affect the activation of the others. To achieve this, we examined the comparison loadings in the first tensor dimension. For the correlations of structures' activation differences across contexts under <italic>R</italic><sub><italic>f</italic></sub>, we focused on the six comparison loadings representing activation differences across contexts under <italic>R</italic><sub><italic>f</italic></sub> (<italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, and <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub>) from <italic>C</italic><sup><italic>+</italic></sup> and <italic>C</italic><sup><italic>−</italic></sup> trials, and evaluated the correlations of these comparison loadings from different structures. The p-values for testing the hypothesis of no correlation were then computed. For the correlations of structures' activation differences across contexts under <italic>R</italic><sub><italic>n</italic></sub>, we used the same approach to evaluate the correlations of structures' activation differences across contexts under <italic>R</italic><sub><italic>n</italic></sub> (<italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub> − <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub> − <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, and <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub> − <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>). For the correlations of structures' activation differences across responses, we evaluated the correlations of structures' activation differences across responses (<italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, and <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub> − <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>).<media content-type="glencoe play-in-place height-250 width-310" id="media1" mime-subtype="mov" mimetype="video" xlink:href="elife-06121-media1.mov"><object-id pub-id-type="doi">10.7554/eLife.06121.005</object-id><label>Video 1.</label><caption><title>Video clip for <italic>C<sub>m</sub>R<sub>f</sub></italic> trials.</title><p>The clip contains the <italic>Context</italic> and <italic>Response</italic> periods.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.005">http://dx.doi.org/10.7554/eLife.06121.005</ext-link></p></caption></media><media content-type="glencoe play-in-place height-250 width-310" id="media2" mime-subtype="mov" mimetype="video" xlink:href="elife-06121-media2.mov"><object-id pub-id-type="doi">10.7554/eLife.06121.006</object-id><label>Video 2.</label><caption><title>Video clip for <italic>C<sub>h</sub>R<sub>f</sub></italic> trials.</title><p>The clip contains the <italic>Context</italic> and <italic>Response</italic> periods.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.006">http://dx.doi.org/10.7554/eLife.06121.006</ext-link></p></caption></media><media content-type="glencoe play-in-place height-250 width-310" id="media3" mime-subtype="mov" mimetype="video" xlink:href="elife-06121-media3.mov"><object-id pub-id-type="doi">10.7554/eLife.06121.007</object-id><label>Video 3.</label><caption><title>Video clip for <italic>C<sub>w</sub>R<sub>f</sub></italic> trials.</title><p>The clip contains the <italic>Context</italic> and <italic>Response</italic> periods.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.007">http://dx.doi.org/10.7554/eLife.06121.007</ext-link></p></caption></media><media content-type="glencoe play-in-place height-250 width-310" id="media4" mime-subtype="mov" mimetype="video" xlink:href="elife-06121-media4.mov"><object-id pub-id-type="doi">10.7554/eLife.06121.008</object-id><label>Video 4.</label><caption><title>Video clip for <italic>C<sub>m</sub>R<sub>n</sub></italic> trials.</title><p>The clip contains the <italic>Context</italic> and <italic>Response</italic> periods.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.008">http://dx.doi.org/10.7554/eLife.06121.008</ext-link></p></caption></media><media content-type="glencoe play-in-place height-250 width-310" id="media5" mime-subtype="mov" mimetype="video" xlink:href="elife-06121-media5.mov"><object-id pub-id-type="doi">10.7554/eLife.06121.009</object-id><label>Video 5.</label><caption><title>Video clip for <italic>C<sub>h</sub>R<sub>n</sub></italic> trials.</title><p>The clip contains the <italic>Context</italic> and <italic>Response</italic> periods.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.009">http://dx.doi.org/10.7554/eLife.06121.009</ext-link></p></caption></media><media content-type="glencoe play-in-place height-250 width-310" id="media6" mime-subtype="mov" mimetype="video" xlink:href="elife-06121-media6.mov"><object-id pub-id-type="doi">10.7554/eLife.06121.010</object-id><label>Video 6.</label><caption><title>Video clip for <italic>C<sub>w</sub>R<sub>n</sub></italic> trials.</title><p>The clip contains the <italic>Context</italic> and <italic>Response</italic> periods.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.06121.010">http://dx.doi.org/10.7554/eLife.06121.010</ext-link></p></caption></media></p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Charles Yokoyama for valuable discussion and paper editing, and Naomi Hasegawa and Tomonori Notoya for medical and technical assistance. We also thank Jun Tani and Douglas Bakkum for their critical comments.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>ZCC, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>YN, Conception and design, Acquisition of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>NF, Conception and design, Drafting or revising the article, Contributed unpublished essential data or reagents</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All experimental and surgical procedures were performed in accordance with the experimental protocols (No. H24-2-203(4)) approved by the RIKEN ethics committee and the recommendations of the Weatherall report, ‘The use of non-human primates in research’. Implantation surgery was performed under sodium pentobarbital anesthesia, and all efforts were made to minimize suffering. No animal was sacrificed in this study. Overall care was managed by the Division of Research Resource Center at RIKEN Brain Science Institute. The animal was housed in a large individual enclosure with other animals visible in the room, and maintained on a 12:12-hr light:dark cycle. The animal was given food (PS-A; Oriental Yeast Co., Ltd., Tokyo, Japan) and water ad libitum, and also daily fruit/dry treats as a means of enrichment and novelty. The animal was occasionally provided toys in the cage. The in-house veterinary doctor checked the animal and updated daily feedings in order to maintain weight. We have attempted to offer as humane treatment of our subject as possible.</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akaike</surname><given-names>H</given-names></name></person-group><year>1974</year><article-title>A new look at the statistical model identification</article-title><source>IEEE Transactions on Automatic Control</source><volume>19</volume><fpage>716</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1109/TAC.1974.1100705</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>CA</given-names></name><name><surname>Bro</surname><given-names>R</given-names></name></person-group><year>2000</year><article-title>The N-way Toolbox for MATLAB</article-title><source>Chemometrics and Intelligent Laboratory Systems</source><volume>52</volume><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1016/S0169-7439(00)00071-X</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aravena</surname><given-names>P</given-names></name><name><surname>Hurtado</surname><given-names>E</given-names></name><name><surname>Riveros</surname><given-names>R</given-names></name><name><surname>Cardona</surname><given-names>JF</given-names></name><name><surname>Manes</surname><given-names>F</given-names></name><name><surname>Ibáñez</surname><given-names>A</given-names></name></person-group><year>2010</year><article-title>Applauding with closed hands: neural signature of action-sentence compatibility effects</article-title><source>PLOS ONE</source><volume>5</volume><fpage>e11751</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0011751</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year>2004</year><article-title>Visual objects in context</article-title><source>Nature Reviews Neuroscience</source><volume>5</volume><fpage>617</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1038/nrn1476</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barch</surname><given-names>DM</given-names></name><name><surname>Carter</surname><given-names>CS</given-names></name><name><surname>Macdonald</surname><given-names>AW</given-names><suffix>III</suffix></name><name><surname>Braver</surname><given-names>TS</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year>2003</year><article-title>Context-processing deficits in schizophrenia: diagnostic specificity, 4-week course, and relationships to clinical symptoms</article-title><source>Journal of Abnormal Psychology</source><volume>112</volume><fpage>132</fpage><pub-id pub-id-type="doi">10.1037/0021–843X.112.1.132</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>LF</given-names></name><name><surname>Kensinger</surname><given-names>EA</given-names></name></person-group><year>2010</year><article-title>Context is routinely encoded during emotion perception</article-title><source>Psychological Science</source><volume>21</volume><fpage>595</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1177/0956797610363547</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>LF</given-names></name><name><surname>Lindquist</surname><given-names>KA</given-names></name><name><surname>Gendron</surname><given-names>M</given-names></name></person-group><year>2007</year><article-title>Language as context for the perception of emotion</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>327</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.06.003</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belmonte</surname><given-names>MK</given-names></name><name><surname>Allen</surname><given-names>G</given-names></name><name><surname>Beckel-Mitchener</surname><given-names>A</given-names></name><name><surname>Boulanger</surname><given-names>LM</given-names></name><name><surname>Carper</surname><given-names>RA</given-names></name><name><surname>Webb</surname><given-names>SJ</given-names></name></person-group><year>2004</year><article-title>Autism and abnormal development of brain connectivity</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>9228</fpage><lpage>9231</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3340-04.2004</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blinowska</surname><given-names>KJ</given-names></name></person-group><year>2011</year><article-title>Review of the methods of determination of directed connectivity from multichannel data</article-title><source>Medical &amp; Biological Engineering &amp; Computing</source><volume>49</volume><fpage>521</fpage><lpage>529</lpage><pub-id pub-id-type="doi">10.1007/s11517-011-0739-x</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bokil</surname><given-names>H</given-names></name><name><surname>Andrews</surname><given-names>P</given-names></name><name><surname>Kulkarni</surname><given-names>JE</given-names></name><name><surname>Mehta</surname><given-names>S</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name></person-group><year>2010</year><article-title>Chronux: a platform for analyzing neural signals</article-title><source>Journal of Neuroscience Methods</source><volume>192</volume><fpage>146</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2010.06.020</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year>1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bressler</surname><given-names>SL</given-names></name><name><surname>Menon</surname><given-names>V</given-names></name></person-group><year>2010</year><article-title>Large-scale brain networks in cognition: emerging methods and principles</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>277</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.04.004</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bressler</surname><given-names>SL</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><year>2011</year><article-title>Wiener-Granger Causality: a well established methodology</article-title><source>Neuroimage</source><volume>58</volume><fpage>323</fpage><lpage>329</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.02.059</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bro</surname><given-names>R</given-names></name><name><surname>Kiers</surname><given-names>HA</given-names></name></person-group><year>2003</year><article-title>A new efficient method for determining the number of components in PARAFAC models</article-title><source>Journal of Chemometrics</source><volume>17</volume><fpage>274</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1002/cem.801</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bullmore</surname><given-names>E</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year>2009</year><article-title>Complex brain networks: graph theoretical analysis of structural and functional systems</article-title><source>Nature Reviews Neuroscience</source><volume>10</volume><fpage>186</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1038/nrn2575</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buschman</surname><given-names>TJ</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year>2007</year><article-title>Top-down versus bottom-up control of attention in the prefrontal and posterior parietal cortices</article-title><source>Science</source><volume>315</volume><fpage>1860</fpage><lpage>1862</lpage><pub-id pub-id-type="doi">10.1126/science.1138071</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>ZC</given-names></name><name><surname>Fujii</surname><given-names>N</given-names></name></person-group><year>2013</year><article-title>Mining spatio-spectro-temporal cortical dynamics: a guideline for offline and online electrocorticographic analyses</article-title><person-group person-group-type="editor"><name><surname>Ogawa</surname><given-names>H</given-names></name><name><surname>Oka</surname><given-names>K</given-names></name></person-group><source>Advanced Methods in Neuroethological Research</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cichocki</surname><given-names>A</given-names></name><name><surname>Zdunek</surname><given-names>R</given-names></name><name><surname>Phan</surname><given-names>AH</given-names></name><name><surname>Amari</surname><given-names>S-I</given-names></name></person-group><year>2009</year><source>Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cui</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>L</given-names></name><name><surname>Bressler</surname><given-names>SL</given-names></name><name><surname>Ding</surname><given-names>M</given-names></name><name><surname>Liang</surname><given-names>H</given-names></name></person-group><year>2008</year><article-title>BSMART: a Matlab/C toolbox for analysis of multichannel neural time series</article-title><source>Neural Networks</source><volume>21</volume><fpage>1094</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2008.05.007</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Gelder</surname><given-names>B</given-names></name></person-group><year>2006</year><article-title>Towards the neurobiology of emotional body language</article-title><source>Nature Reviews Neuroscience</source><volume>7</volume><fpage>242</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1038/nrn1872</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year>2004</year><article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>Journal of Neuroscience Methods</source><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Mullen</surname><given-names>T</given-names></name><name><surname>Kothe</surname><given-names>C</given-names></name><name><surname>Acar</surname><given-names>ZA</given-names></name><name><surname>Bigdely-Shamlo</surname><given-names>N</given-names></name><name><surname>Vankov</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year>2011</year><article-title>EEGLAB, SIFT, NFT, BCILAB, and ERICA: new tools for advanced EEG processing</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>130714</fpage><pub-id pub-id-type="doi">10.1155/2011/130714</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Sejnowski</surname><given-names>T</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year>2007</year><article-title>Enhanced detection of artifacts in EEG data using higher-order statistics and independent component analysis</article-title><source>Neuroimage</source><volume>34</volume><fpage>1443</fpage><lpage>1449</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.11.004</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>M</given-names></name><name><surname>Bressler</surname><given-names>SL</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Liang</surname><given-names>H</given-names></name></person-group><year>2000</year><article-title>Short-window spectral analysis of cortical event-related potentials by adaptive multivariate autoregressive modeling: data preprocessing, model validation, and variability assessment</article-title><source>Biological Cybernetics</source><volume>83</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1007/s004229900137</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frankland</surname><given-names>PW</given-names></name><name><surname>Bontempi</surname><given-names>B</given-names></name><name><surname>Talton</surname><given-names>LE</given-names></name><name><surname>Kaczmarek</surname><given-names>L</given-names></name><name><surname>Silva</surname><given-names>AJ</given-names></name></person-group><year>2004</year><article-title>The involvement of the anterior cingulate cortex in remote contextual fear memory</article-title><source>Science</source><volume>304</volume><fpage>881</fpage><lpage>883</lpage><pub-id pub-id-type="doi">10.1126/science.1094804</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year>2011</year><article-title>Inter-area correlations in the ventral visual pathway reflect feature integration</article-title><source>Journal of Vision</source><volume>11</volume><pub-id pub-id-type="doi">10.1167/11.4.15</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year>2005</year><article-title>A mechanism for cognitive dynamics: neuronal communication through neuronal coherence</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>474</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.08.011</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year>2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuster</surname><given-names>JM</given-names></name><name><surname>Bodner</surname><given-names>M</given-names></name><name><surname>Kroger</surname><given-names>JK</given-names></name></person-group><year>2000</year><article-title>Cross-modal and cross-temporal association in neurons of frontal cortex</article-title><source>Nature</source><volume>405</volume><fpage>347</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1038/35012613</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gregoriou</surname><given-names>GG</given-names></name><name><surname>Gotts</surname><given-names>SJ</given-names></name><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year>2009</year><article-title>High-frequency, long-range coupling between prefrontal and visual cortex during attention</article-title><source>Science</source><volume>324</volume><fpage>1207</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.1126/science.1171402</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Schmitz</surname><given-names>F</given-names></name><name><surname>Schnitzler</surname><given-names>I</given-names></name><name><surname>Kessler</surname><given-names>K</given-names></name><name><surname>Shapiro</surname><given-names>K</given-names></name><name><surname>Hommel</surname><given-names>B</given-names></name><name><surname>Schnitzler</surname><given-names>A</given-names></name></person-group><year>2004</year><article-title>Modulation of long-range neural synchrony reflects temporal limitations of visual attention in humans</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>101</volume><fpage>13050</fpage><lpage>13055</lpage><pub-id pub-id-type="doi">10.1073/pnas.0404944101</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haegens</surname><given-names>S</given-names></name><name><surname>Nácher</surname><given-names>V</given-names></name><name><surname>Hernández</surname><given-names>A</given-names></name><name><surname>Luna</surname><given-names>R</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name></person-group><year>2011</year><article-title>Beta oscillations in the monkey sensorimotor network reflect somatosensory decision making</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>108</volume><fpage>10708</fpage><lpage>10713</lpage><pub-id pub-id-type="doi">10.1073/pnas.1107297108</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><year>2005</year><article-title>On Broca, brain, and binding: a new framework</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>416</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.07.004</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harshman</surname><given-names>RA</given-names></name><name><surname>Lundy</surname><given-names>ME</given-names></name></person-group><year>1994</year><article-title>PARAFAC: parallel factor analysis</article-title><source>Computational Statistics &amp; Data Analysis</source><volume>18</volume><fpage>39</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/0167-9473(94)90132-5</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Himberg</surname><given-names>J</given-names></name><name><surname>Hyvarinen</surname><given-names>A</given-names></name><name><surname>Esposito</surname><given-names>F</given-names></name></person-group><year>2004</year><article-title>Validating the independent components of neuroimaging time series via clustering and visualization</article-title><source>Neuroimage</source><volume>22</volume><fpage>1214</fpage><lpage>1222</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.03.027</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hipp</surname><given-names>JF</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Siegel</surname><given-names>M</given-names></name></person-group><year>2011</year><article-title>Oscillatory synchronization in large-scale cortical networks predicts perception</article-title><source>Neuron</source><volume>69</volume><fpage>387</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.12.027</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Oja</surname><given-names>E</given-names></name></person-group><year>1997</year><article-title>A fast fixed-point algorithm for independent component analysis</article-title><source>Neural Computation</source><volume>9</volume><fpage>1483</fpage><lpage>1492</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.7.1483</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ibañez</surname><given-names>A</given-names></name><name><surname>Manes</surname><given-names>F</given-names></name></person-group><year>2012</year><article-title>Contextual social cognition and the behavioral variant of frontotemporal dementia</article-title><source>Neurology</source><volume>78</volume><fpage>1354</fpage><lpage>1362</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koechlin</surname><given-names>E</given-names></name><name><surname>Ody</surname><given-names>C</given-names></name><name><surname>Kouneiher</surname><given-names>F</given-names></name></person-group><year>2003</year><article-title>The architecture of cognitive control in the human prefrontal cortex</article-title><source>Science</source><volume>302</volume><fpage>1181</fpage><lpage>1185</lpage><pub-id pub-id-type="doi">10.1126/science.1088545</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korzeniewska</surname><given-names>A</given-names></name><name><surname>Manczak</surname><given-names>M</given-names></name><name><surname>Kaminski</surname><given-names>M</given-names></name><name><surname>Blinowska</surname><given-names>KJ</given-names></name><name><surname>Kasicki</surname><given-names>S</given-names></name></person-group><year>2003</year><article-title>Determination of information flow direction among brain structures by a modified directed transfer function (dDTF) method</article-title><source>Journal of Neuroscience Methods</source><volume>125</volume><fpage>195</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1016/S0165-0270(03)00052-9</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maren</surname><given-names>S</given-names></name><name><surname>Phan</surname><given-names>KL</given-names></name><name><surname>Liberzon</surname><given-names>I</given-names></name></person-group><year>2013</year><article-title>The contextual brain: implications for fear conditioning, extinction and psychopathology</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>417</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1038/nrn3492</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meinecke</surname><given-names>F</given-names></name><name><surname>Ziehe</surname><given-names>A</given-names></name><name><surname>Kawanabe</surname><given-names>M</given-names></name><name><surname>Muller</surname><given-names>KR</given-names></name></person-group><year>2002</year><article-title>A resampling approach to estimate the stability of one-dimensional or multidimensional independent components</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>49</volume><fpage>1514</fpage><lpage>1525</lpage><pub-id pub-id-type="doi">10.1109/TBME.2002.805480</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milad</surname><given-names>MR</given-names></name><name><surname>Pitman</surname><given-names>RK</given-names></name><name><surname>Ellis</surname><given-names>CB</given-names></name><name><surname>Gold</surname><given-names>AL</given-names></name><name><surname>Shin</surname><given-names>LM</given-names></name><name><surname>Lasko</surname><given-names>NB</given-names></name><name><surname>Zeidan</surname><given-names>MA</given-names></name><name><surname>Handwerger</surname><given-names>K</given-names></name><name><surname>Orr</surname><given-names>SP</given-names></name><name><surname>Rauch</surname><given-names>SL</given-names></name></person-group><year>2009</year><article-title>Neurobiological basis of failure to recall extinction memory in posttraumatic stress disorder</article-title><source>Biological Psychiatry</source><volume>66</volume><fpage>1075</fpage><lpage>1082</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2009.06.026</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year>2001</year><article-title>An integrative theory of prefrontal cortex function</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>167</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagasaka</surname><given-names>Y</given-names></name><name><surname>Shimoda</surname><given-names>K</given-names></name><name><surname>Fujii</surname><given-names>N</given-names></name></person-group><year>2011</year><article-title>Multidimensional recording (MDR) and data sharing: an ecological open research and educational platform for neuroscience</article-title><source>PLOS ONE</source><volume>6</volume><fpage>e22561</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0022561</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pesaran</surname><given-names>B</given-names></name><name><surname>Nelson</surname><given-names>MJ</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year>2008</year><article-title>Free choice activates a decision circuit between frontal and parietal cortex</article-title><source>Nature</source><volume>453</volume><fpage>406</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1038/nature06849</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>JJ</given-names></name><name><surname>Ma</surname><given-names>QD</given-names></name><name><surname>Tinsley</surname><given-names>MR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Fanselow</surname><given-names>MS</given-names></name></person-group><year>2008</year><article-title>Inverse temporal contributions of the dorsal hippocampus and medial prefrontal cortex to the expression of long-term fear memories</article-title><source>Learning &amp; Memory</source><volume>15</volume><fpage>368</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1101/lm.813608</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>House</surname><given-names>A</given-names></name></person-group><year>2005</year><article-title>Visibility reflects dynamic changes of effective connectivity between V1 and fusiform cortex</article-title><source>Neuron</source><volume>46</volume><fpage>811</fpage><lpage>821</lpage></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubinov</surname><given-names>M</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year>2010</year><article-title>Complex network measures of brain connectivity: uses and interpretations</article-title><source>Neuroimage</source><volume>52</volume><fpage>1059</fpage><lpage>1069</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.10.003</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year>2007</year><article-title>Space and time in visual context</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>522</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1038/nrn2155</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><year>2010</year><article-title>A MATLAB toolbox for Granger causal connectivity analysis</article-title><source>Journal of Neuroscience Methods</source><volume>186</volume><fpage>262</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2009.11.020</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shidara</surname><given-names>M</given-names></name><name><surname>Richmond</surname><given-names>BJ</given-names></name></person-group><year>2002</year><article-title>Anterior cingulate: single neuronal signals related to degree of reward expectancy</article-title><source>Science</source><volume>296</volume><fpage>1709</fpage><lpage>1711</lpage><pub-id pub-id-type="doi">10.1126/science.1069504</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name></person-group><year>2012</year><article-title>Spectral fingerprints of large-scale neuronal interactions</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>121</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1038/nrn3137</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name></person-group><year>2008</year><article-title>Neuronal synchronization along the dorsal visual pathway reflects the focus of spatial attention</article-title><source>Neuron</source><volume>60</volume><fpage>709</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.010</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year>2011</year><article-title>The human connectome: a complex network</article-title><source>Annals of the New York Academy of Sciences</source><volume>1224</volume><fpage>109</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2010.05888.x</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tononi</surname><given-names>G</given-names></name><name><surname>Edelman</surname><given-names>GM</given-names></name></person-group><year>1997</year><article-title>Information: In the stimulus or in the context?</article-title><source>Behavioral and Brain Sciences</source><volume>20</volume><fpage>698</fpage><lpage>700</lpage><pub-id pub-id-type="doi">10.1017/S0140525X97401607</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uhlhaas</surname><given-names>PJ</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name></person-group><year>2006</year><article-title>Neural synchrony in brain disorders: relevance for cognitive dysfunctions and pathophysiology</article-title><source>Neuron</source><volume>52</volume><fpage>155</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.09.020</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varela</surname><given-names>F</given-names></name><name><surname>Lachaux</surname><given-names>J-P</given-names></name><name><surname>Rodriguez</surname><given-names>E</given-names></name><name><surname>Martinerie</surname><given-names>J</given-names></name></person-group><year>2001</year><article-title>The brainweb: phase synchronization and large-scale integration</article-title><source>Nature Reviews Neuroscience</source><volume>2</volume><fpage>229</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1038/35067550</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.06121.027</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Behrens</surname><given-names>Timothy</given-names></name><role>Reviewing editor</role><aff><institution>Oxford University</institution>, <country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for sending your work entitled “Mesoscopic brain networks regulate cognitive enchainment in social monitoring” for consideration at <italic>eLife</italic>. Your article has been favorably evaluated by three reviewers, one of whom, Timothy Behrens (Senior Editor and Reviewing Editor) is a member of our board.</p><p>The editor and the other reviewers discussed their comments before we reached this decision, and the editor has assembled the following comments to help you prepare a revised submission.</p><p>All three reviewers were very impressed by the unusual nature of the data, by the sophisticated and revealing analysis that was able to simplify complex data to the dynamics of network activity that underlies social observations in the task, and all reviewers were excited by the ability to study this particular brain network in macaque monkeys, given its importance in social cognition in many human studies.</p><p>Example praise for the manuscript in review was as follows:</p><p><italic>Reviewer 1:</italic></p><p>The neural mechanisms mediating social behavior appear to be distributed across the brain, including areas thought to be specialized for social information processing (such as those in the temporal lobe and medial prefrontal cortex) and others that serve more general purpose functions (such as those involved in reward, decision-making, executive control, and attention). One impediment to understanding how these circuits interact to translate sensory perception into behavior is that the methods typically applied suffer from either poor temporal resolution (fMRI in humans), poor spatial resolution (EEG), or limited coverage (single unit recording in animals). The current paper uses wide-scale recordings from intracranial EEG (ECoG) system to simultaneously assess interactions amongst cortical areas during social information processing. Importantly, the authors apply sophisticated, data-driven analytical tools to derive the information flow between and amongst these areas. This is an important advance.</p><p><italic>Reviewer 2:</italic></p><p>This paper describes a novel approach to extract information about the functional connectivity of distributed brain networks, and how these connections evolve through time as a function of carrying out social observation. The paper is unique in two respects. First, the dataset contains whole-hemisphere ECoG is collected whilst three non-human primates perform a social task (although the authors have published several papers with similar ECoG data previously). Second, the analysis approach is novel and innovative. dDTF is used to identify connections between regions, and then factor analysis is used to isolate how these vary as a function of different task conditions. Using this approach, the authors identify five separate networks (‘structures’). These superficially have similarities in terms of their connectional structure (e.g. structures 1/2, and structures 3/4), but differ in terms of their temporal dynamics and their activation across conditions. Intriguingly, some of the networks contain classic ‘social’ regions, such as the superior temporal sulcus. By examining how these networks evolve through time, the authors attempt to reveal the chain of events underlying different forms of social observation.</p><p><italic>Reviewer 3:</italic></p><p>This paper reports extremely unusual data from ECoG recordings of macaque monkeys viewing other monkeys engaged in socially threatening situations. It also reports a novel and potentially powerful set of analysis tools for analysing functional networks acquired at high temporal resolution in ECoG data.</p><p>There are several key strengths and novelties about the paper.</p><p>(1) Intriguing patterns of brain activity and functional connections are reported that have perhaps never been recorded from outside and fMRI scanner, and certainly not in social situations. These patterns are reminiscent of human brain areas that respond to complex social tasks.</p><p>(2) The broad coverage of the ECoG data by comparison to most other macaque monkey recordings allows the analysis of information flow between brain areas. Because of the temporal resolution, these analyses can also begin to make directional inferences.</p><p>(3) The complex nature of ECoG data requires sophisticated data compression techniques. The authors are extremely inventive in how they analyse their data – they develop tools which compress the data into its digestible patterns, but which maintain the key comparisons between conditions, between brain areas, and between task times. I find this very impressive.</p><p>However, there were several features of the data that limited the reviewers' enthusiasm. In brief, these were broadly to do with the task and the over-interpretation of the data. We believe that with a thorough rewrite of the manuscript, to focus on describing the data clearly rather than making interpretations of the data, both in terms of its relevance to particular social behaviours, and in terms of causal mechanisms that are not supported directly by the data, it will be possible to improve the manuscript to remove these concerns.</p><p>Specifically:</p><p>Comments about the task:</p><p><italic>Reviewer #1:</italic></p><p>1) The task, which the authors refer to as monitoring of a social context, involves only passive viewing without any differential response expected (or found) on the part of the observer monkey in reaction to different ‘social scenarios’. The only statistically significant difference in behavior is the reported difference in left vs. right gaze positions during the response phase of the trial when monkeys view a frightened vM vs. a neutral vM. Since the authors did not find this result to vary under different ‘social’ contexts, it remains unclear whether/what the observer monkeys made out of the different social situations examined in the study.</p><p>2) In the Abstract, it might be more precise to talk about “mapping the network structure” as monkeys viewed scenes leading up to examining the valence on a conspecific's face rather than calling it a “social cognitive behavior”/“social context monitoring” since the behavior per se doesn't inform us in this regard.</p><p>3) In the Methods, the task lacks a non-social control, which will in turn depend on what the authors, for the purpose of the expt., define as ‘social’. For instance, is the context of two monkeys looking at each other ‘social’ in which case, the non-social control could be another monkey looking away from the monkey on the left of the screen. Is monkey-monkey looking at each other more ‘social’ than monkey-human looking at each other? These are of course tough questions to answer from a single experiment but it will be still useful to discuss the authors' view on these issues as their basis for designing the expt.</p><p>On the other hand, if the element of affect/threatening the monkey on the left is what constitutes social in this case, a non-social control could be a non-threatening/neutral monkey on the right or perhaps an inanimate monkey/human with a threatening expression on it?</p><p>Furthermore, the monkey in the video facing an empty wall does not control for the presence of an object or an individual. There are clear sensory/perceptual differences between a monkey face, a human face, and a wall.</p><p><italic>Reviewer #3:</italic></p><p>It is not clear from the monkey's behaviour that the social nature of the task, rather than the perceptual differences between stimuli, is what is important. In my view this slightly confounds the clear interpretation of these signals as social signals.</p><p>Comments about the interpretation of the data:</p><p><italic>Reviewer #1:</italic></p><p>4) In the Results, the authors say that “subjects tended to focus on the right section during the Response period when the response stimuli were <italic>R</italic><sub><italic>f</italic></sub> (<italic>C+R</italic><sub><italic>f</italic></sub> trials)” vs <italic>C+R</italic><sub><italic>n</italic></sub> trials as well as <italic>C</italic><sup><italic>–</italic></sup> trials. They conclude that this “indicated that gazing behavior required not only vM responses with high emotional valence, but also the context of vM's response. This suggested that the gazing behavior by the subject represented an automatic or intuitive reaction to socially relatable scenarios (e.g. ‘vM was frightened after being threatened’).”</p><p>I think the comparison that the authors analyzed suggest that monkeys are interested in knowing what is behind the curtain when vM is frightened vs. when it is not. This comparison doesn't take into account the nature of context preceding the response since what frightened the monkey – monkey vs. human vs. empty wall – was not compared against each other here. In fact, when the context was indeed taken into account, the authors did not find any significant context dependence at all (Figure 1–figure supplement 3) and hence, in my opinion, the reported difference in gazing location does not make the case for subjects ‘monitoring the context of a social scenario’ at all. This is a critical concern.</p><p><italic>Reviewer #2:</italic></p><p>I came away with a less than clear impression of what the findings had taught us. I think that this was partly a result of the way the paper was structured. The focus, in the Abstract, Introduction and initial results, was quite heavily on the mathematical technique used as opposed to the results obtained with this technique. Upon reaching the Results, there were some clearly interesting findings, but many of the interpretations were dependent upon a reverse inference from the brain regions activated (Poldrack <underline>RA, TiCS 2006</underline>), as opposed to an inference based on the task manipulation. I would therefore urge the authors to shift the emphasis in the initial part of the paper towards what their technique and dataset tells us about the dynamics of connectivity in these brain regions, as opposed to being so heavily focused on the methodology.</p><p><italic>Reviewer #3:</italic></p><p>The nature of the task in combination with the complex analysis often makes interpretation of the results complex, and the authors resort to an interpretation that does not rely on the data. There are examples of this throughout the manuscript. Here is a typical one:</p><p>“This result suggests that Structure 5 underlies the context-dependent feedback modulation of response perception linked to social reasoning in the task (‘why vM is frightened?’ or ‘is vM frightened because it was threatened by something?’ or ‘should I be concerned that vM is frightened?’).”</p><p>This kind of inference is inappropriate, and is also unnecessary.</p><p>The remaining major comments were either about the technicalities of the analysis, or about the reporting of these technicalities. Where possible, we would like you to address these technical concerns. More broadly speaking, we would like you to focus on clarifying the more technical aspects of the manuscript so that the manuscript can be clearly understood by a broad audience.</p><p>Other comments:</p><p><italic>Reviewer #1:</italic></p><p>In the subsection “Deconvolution Analysis or Cortical Information Processing,” wouldn't it be more useful task-wise to identify independent sources by using only <italic>C</italic><sup><italic>+</italic></sup> trials instead of merging data from <italic>C</italic><sup><italic>+</italic></sup> &amp; <italic>C</italic><sup><italic>–</italic></sup> trials for ICA? Did the authors do this analysis? How does it affect the results?</p><p>In the first paragraph of the subsection “Dynamic Cognitive Chain Describes Social Context Monitoring” to examine how the network interactions change as a function of context, wouldn't it be better to examine the correlations between activation differences across contexts (<italic>C</italic><sub><italic>m</italic></sub> &amp; <italic>C</italic><sub><italic>h</italic></sub> vs. <italic>C</italic><sub><italic>w</italic></sub>) rather than between <italic>C</italic><sup><italic>+</italic></sup> vs. <italic>C</italic><sup><italic>–</italic></sup> trials?</p><p>In the same subsection, as per <xref ref-type="fig" rid="fig5">Figure 5A</xref>, none of the correlations with structure 2 are significant. In that case does a causal dependence of structure 4 and 5 on structure 2 apply?</p><p>In the second paragraph of the same subsection, does this analysis include the time courses of gaze positions and scanning behavior of all contexts (<italic>C</italic><sub><italic>m</italic></sub>, <italic>C</italic><sub><italic>h</italic></sub> &amp; <italic>C</italic><sub><italic>w</italic></sub>) and trials (<italic>C</italic><sup><italic>+</italic></sup> &amp; <italic>C</italic><sup><italic>–</italic></sup>)? Did you find the timing correlations to be different across contexts?</p><p><italic>Reviewer #2:</italic></p><p>I would therefore urge the authors to shift the emphasis in the initial part of the paper towards what their technique and dataset tells us about the dynamics of connectivity in these brain regions, as opposed to being so heavily focused on the methodology. That said, there were also times where it was unclear what order different techniques were being applied, and how they were being applied. For instance, it is mentioned that ICA was first applied, but it was unclear what the input dimensions of the ICA were, or how the obtained components were subsequently used for the dDTF and PARAFAC analysis. I felt that a clear ‘analysis pipeline’ diagram, starting with raw data and ending with the key results, would be very helpful to include as a supplementary figure.</p><p>Finally, I felt that the presentation of the comparison-condition component in <xref ref-type="fig" rid="fig3 fig4">Figures 3 and 4</xref> was unclear, in that it seemed diagrammatic whereas presumably it was based upon the statistics of the comparison being performed. A more quantitative approach to presenting this data would help to make it more clear and compelling.</p><p><italic>Reviewer #3:</italic></p><p>The manuscript is written from a very technical perspective, and does not introduce the key neuroscience issues well. This makes it a very tough read, particularly for a broad interest journal such as <italic>eLife</italic>.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled “Mesoscopic brain networks regulate cognitive enchainment in social monitoring” for further consideration at <italic>eLife</italic>. Your revised article has been evaluated by Timothy Behrens (Senior Editor and Reviewing Editor) and the original reviewers.</p><p>The reviews are appended below, and the sentiment in the reviews was reiterated in discussion between reviewers. Essentially, we all remain impressed with the data, are aware that the volume and complexity of data requires innovative new analytical tools, and we still all believe that the proposed analyses are likely very interesting. However, two of the reviewers are clear (and the editorial team agree) that the manuscript cannot possibly be published in a broad interest journal in its present form. Whilst the technical details are more revealing and the claims better substantiated in this current revision, the clarity of the manuscript has, in our view, not improved. It is very difficult indeed to parse the manuscript to understand what the central contribution is. The figures are not well explained in the legends – the legends report details that might be more appropriate in a technical methods section and do not perform the main function of a figure legend, which is to explain how to read the figure.</p><p>We would like to ask you to follow the reviewers’ advice below and to restructure and rewrite the paper, so it can be followed in detail by a naïve reader.</p><p><italic>Reviewer #1:</italic></p><p>I think the authors have done a fine job revising this paper, which presents a novel analytical approach to analyzing density neurophysiological data gathered in monkeys viewing a set of different social scenarios. I'm not yet convinced that the descriptor “social scenario viewing task” is much better than “social monitoring task”– it's both a mouthful and I think still puts too much emphasis on the idea of a task. It might be more concise and precise to say the monkeys were viewing social scenarios.</p><p><italic>Reviewer #2:</italic></p><p>The authors have put quite some work into making the manuscript clearer, and providing more substantial information concerning the methodology. But the main concerns of the reviewers seem to have held: because of the task design, it is very difficult to draw any strong conclusions about the role of the identified networks in social cognition. In their own words, they acknowledge “without a proper control, we can't conclusively link our results to social cognition.” As such, the authors now explicitly state (in their response) that the main conclusion from the paper is about the development of a novel method. Not much can be learnt about the explicit meaning of the underlying cognitive processes.</p><p>The question then becomes, is this novel method of sufficiently broad interest and importance that it will change the views of the community? The authors main claim seems to be that it will reveal previously undiscovered ‘cognitive chains’. What exactly is meant by this? That brain regions are activated sequentially in response to a task, and that different brain areas will be recruited depending upon the cognitive function? On the one hand, this seems to be something that we already know from many years of MEG and EEG research in humans. On the other, it is clear that the spatial resolution of the ECoG data far surpasses this research, and the extraction of network structure is very different from what has gone before. Nevertheless, I still very much struggled to understand whether this extraction of network structure was (a) valid or (b) important. In terms of validity, if I were reviewing this paper at a methods journal, I'd expect a set of examples in simulated data to convince me that the method works well and robustly. In terms of importance, it's precisely because the task is poorly controlled that I don't get an “aha!” moment when examining the results that convinces me that it has definitely worked.</p><p>I can see that there is a lot of potential in the paper, and it seems unfair to dismiss what could be an important set of findings about a novel technique. But equally I didn't find the results and structure of the paper sufficiently compelling or clear to warrant publication at present. I'd be open to other reviewers pointing out what they felt was the evidence that the technique works convincingly, or that it has produced a particularly important result.</p><p><italic>Reviewer #3:</italic></p><p>The authors have done a good job dealing with the technical concerns and no longer over-interpret the data.</p><p>However, there still remain concerns about whether the manuscript as currently written can be understood by a broad interest readership, or even a relatively specialised readership within the field. Indeed the mathematical expertise needed even to understand the basic analysis is extreme, and the authors do a very poor job in making the analysis comprehensible. Like the other reviewers, I suspect, I am still finding it difficult to really evaluate the neuroscientific findings, as I cannot fully understand their implications.</p><p>Despite the unusual and high quality of the data and the sophisticated nature of the analysis, it is therefore very difficult to understand what we have learnt about the cognitive processes.</p><p>For example, the figure legends do not explain how to read the figures. The new diagram asked for by a different reviewer is difficult to understand.</p><p>It is essential that the authors address this in both the Abstract and the main text before this can be published in a journal such as <italic>eLife</italic>.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled “Cortical network architecture for context processing in primate brain” for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Timothy Behrens (Senior Editor).</p><p>The new manuscript is, in my view, dramatically clarified, and the data remain extremely exciting. However, in the rewrite the focus of the Abstract and the Introduction has been moved towards the technical achievements of the manuscript. This presents a difficulty for <italic>eLife</italic> because the manuscript has not been reviewed or considered as a technical manuscript. In the Discussion, the nature of the claims is much more balanced between the technical innovations and the neuroscience claims (except perhaps in the first overarching paragraph where the balance is again towards the technical). <italic>eLife</italic> cannot publish the manuscript on the basis of the technical claim alone, but I believe it will be relatively easy to adjust the Abstract and Introduction to highlight and to be clear about the new neuroscience claim.</p><p>In my view, the neuroscience claim is still not clearly stated in the Abstract or Introduction. You summarise your findings as follows in the Abstract:</p><p>“Collectively, the five structures delineated the flow of information in the network, including two isomorphic variants defining the encoding and retrieval, respectively, of contextual information.”</p><p>and in the Introduction:</p><p>“The structures we identified provide new insights on how contextual information is processed and help to identify relationships linking network communication and behavior.”</p><p>Both of these statements are descriptions of the success of the technique, and not of new neuroscience findings. In brief, what new insights do they provide?</p><p>In the Discussion you are much more clear about this in the subsection “Context is Encoded by Interactions of Large-Scale Network Structures”.</p><p>If I am correct, it seems like the key claims can be summarised as follows:</p><p>a) Large-scale network interactions are different in different contexts.</p><p>b) Bottom-up connections from posterior temporal cortex to anterior temporal cortex and medial PFC can encode a context whilst it is being first processed and held on-line.</p><p>c) These exact same brain regions exhibit the opposite top-down connectivity only seconds later when the context is being applied to incoming sensory information.</p><p>d) The extent to which the bottom-up connectivity is active during the processing of context predicts the extent to which the top-down connectivity will act when the context is later being applied.</p><p>To me, these claims seem to be striking and important and it is these claims that the manuscript has been judged on, but these claims no longer appear anywhere in the Abstract or Introduction and are only in a subsection of the Discussion. So I am asking for one further revision. In this round of revision, I am asking for changes to the Abstract, to the Introduction (and possibly also to the Discussion if you choose), which concisely and precisely highlight the new neuroscience claims, and which change the tone of the current version of the manuscript from being largely a methodological innovation, to being a balanced manuscript which introduces a new technique to make a clear and precise claim about the contextual processing of sensory information.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.06121.028</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Comments about the task</italic>:</p><p>Reviewer #1:</p><p><italic>1) The task, which the authors refer to as monitoring of a social context, involves only passive viewing without any differential response expected (or found) on the part of the observer monkey in reaction to different ‘social scenarios’. The only statistically significant difference in behavior is the reported difference in left vs. right gaze positions during the response phase of the trial when monkeys view a frightened vM vs. a neutral vM. Since the authors did not find this result to vary under different ‘social’ contexts, it remains unclear whether/what the observer monkeys made out of the different social situations examined in the study</italic>.</p><p>We agree that the observer monkey’s behavior did not directly show “social context monitoring”. Therefore, in the revised manuscript, we have de-emphasized the “social” implication of the conclusions and provided a more precise, conservative interpretation in the paper. Instead, we emphasize the main conclusion that our data reveal a method to simplify complex brain physiological data into mesoscopic network modules underlying implicit cognitive processes related to sensory input of the observer monkey, and whose explicit meaning remains an interesting subject for future investigation. In the revised Introduction we write: “Current methods for studying neurocognitive networks in humans and primates depend on drawing correlations between brain network activity and behavior. […] Here, we employ a new approach that uses an unbiased decomposition of total network activity under diverse task conditions, enabling the computational extraction of latent structure in functional network interactions and post-hoc examination of its dynamic evolution during behavior”.</p><p>However, we assert that our task was fundamentally social in nature, since the subjects viewed video clips containing explicit social interactions between a conspecific and second agent. Furthermore, while the eye scanning behavior during the Context period indeed did not show context specificity, they clearly indicated the subjects’ attempt to access the social relationship between the agents and demonstrated the subjects’ observational association with the vM. Therefore, we renamed the task a “social scenario viewing task” to avoid the implication that the task required subjects to process social contextual information. Furthermore, our functional connectivity analysis demonstrated the existence of neural networks that were not observable by external behavior. Thus, we believe that the term “social monitoring” is appropriate since it could refer to an internal process.</p><p><italic>2) In the Abstract, it might be more precise to talk about “mapping the network structure” as monkeys viewed scenes leading up to examining the valence on a conspecific's face rather than calling it a “social cognitive behavior”/“social context monitoring” since the behavior</italic> per se <italic>doesn't inform us in this regard</italic>.</p><p>We agree that the observer monkey’s behavior did not show the explicit processing of social context, but we believe there was a clear social observation was in the task (see response above). The reviewer’s proposed description is precise to a fault, but we would like to suggest that “the valence on a conspecific’s face” is part of a social context between two agents that the monkey is observing with interest, as measured by eye scanning between key elements in the scene.</p><p>To address this issue, in the Abstract, we now state: “Here, we describe the functional network structure of primate brain during social monitoring where subjects passively viewed social scenarios with staged situational contexts”.</p><p><italic>3) Methods: The task lacks a non-social control, which will in turn depend on what the authors, for the purpose of the expt., define as ‘social’. For instance, is the context of two monkeys looking at each other ‘social’ in which case, the non-social control could be another monkey looking away from the monkey on the left of the screen. Is monkey-monkey looking at each other more ‘social’ than monkey-human looking at each other? These are of course tough questions to answer from a single experiment but it will be still useful to discuss the authors' view on these issues as their basis for designing the expt</italic>.</p><p><italic>On the other hand, if the element of affect/ threatening the monkey on the left is what constitutes social in this case, a non-social control could be a non-threatening/ neutral monkey on the right or perhaps an inanimate monkey/ human with a threatening expression on it?</italic></p><p><italic>Furthermore, the monkey in the video facing an empty wall does not control for the presence of an object or an individual. There are clear sensory/perceptual differences between a monkey face, a human face, and a wall</italic>.</p><p>The reviewer’s former understanding of the social aspect in the task is correct: the interaction between a conspecific and a second agent in the video. We assert that both monkey-monkey and monkey-human scenarios are social conditions, and that monkey-wall is a non-social condition (no second agent). We agree the monkey-wall condition is not an appropriate non-social control, and without a proper control, we can’t conclusively link our results to social cognition. Therefore, we toned down the social aspect in the revised manuscript (see response above).</p><p>In the revised manuscript, we also clarified our rationale for the experimental design: “To allow the controlled recruitment of multiple interdependent cognitive processes in a simple and natural setting, we introduced a social scenario viewing task, where monkey passively viewed video clips in which another monkey (video monkey, or vM) was socially engaged with a second agent.”</p><p>Reviewer #3:</p><p><italic>It is not clear from the monkey's behaviour that the social nature of the task, rather than the perceptual differences between stimuli, is what is important. In my view this slightly confounds the clear interpretation of these signals as social signals</italic>.</p><p>The subjects’ scanning behavior during the Context period suggested that they were monitoring the relationship between the two agents. Therefore, we believe that the resultant context-dependent brain signals we measured reflect not only perceptual differences, but also social situational differences. However, we agree that further non-social control experiments are needed to definitively conclude this. Thus, in the revised manuscript, we have toned down the implication of social cognition, and instead emphasized the concept of mesoscopic network modules underlying concurrent and intertwined cognitive processes (see response above).</p><p><italic>Comments about the interpretation of the data:</italic></p><p>Reviewer #1:</p><p><italic>4) Results: The authors say that “subjects tended to focus on the right section during the Response period when the response stimuli were</italic> R<sub>f</sub> <italic>(</italic>C+R<sub>f</sub> <italic>trials)” vs</italic> C+R<sub>n</sub> <italic>trials as well as</italic> C<sup>–</sup> <italic>trials. They conclude that this “indicated that gazing behavior required not only vM responses with high emotional valence, but also the context of vM's response. This suggested that the gazing behavior by the subject represented an automatic or intuitive reaction to socially relatable scenarios (e.g. ‘vM was frightened after being threatened’)</italic>.<italic>”</italic></p><p><italic>I think the comparison that the authors analyzed suggest that monkeys are interested in knowing what is behind the curtain when vM is frightened vs. when it is not. This comparison doesn't take into account the nature of context preceding the response since what frightened the monkey – monkey vs. human vs. empty wall – was not compared against each other here. In fact, when the context was indeed taken into account, the authors did not find any significant context dependence at all (Figure 1–figure supplement 3) and hence, in my opinion, the reported difference in gazing location does not make the case for subjects ‘monitoring the context of a social scenario’ at all. This is a critical concern</italic>.</p><p>While the eye scanning behavior during the Context period suggested that the subjects were evaluating the social situation presented in the context stimuli, we agree that there’s no contextual specificity in the explicit subjects’ behavior and that the implication of the subjects “monitoring the context of a social scenario” should be presented more conservatively in the text. Therefore, we renamed the task a “social scenario viewing task” instead of “social context monitoring task”, and the cognitive behavior underlying the identified brain networks as “social monitoring” instead of “social context monitoring” (see responses above).</p><p>In the revised manuscript, we elaborated on the explanation underlying this more conservative, nuanced conclusion of social monitoring, and instead placed more emphasis on our discovery of brain networks in structured cognition that are not observable via conventional behavioral measurements. In the Abstract we write: “Contextual specificity found in these components was absent in explicit behavioral output, revealing that the connectivity organization in cognition was internally generated.”</p><p>Reviewer #2:</p><p><italic>I came away with a less than clear impression of what the findings had taught us. I think that this was partly a result of the way the paper was structured. The focus, in Abstract, Introduction and initial results, was quite heavily on the mathematical technique used as opposed to the results obtained with this technique. Upon reaching the results, there were some clearly interesting findings, but many of the interpretations were dependent upon a reverse inference from the brain regions activated (Poldrack <underline>RA, TiCS 2006</underline>), as opposed to an inference based on the task manipulation. I would therefore urge the authors to shift the emphasis in the initial part of the paper towards what their technique and dataset tells us about the dynamics of connectivity in these brain regions, as opposed to being so heavily focussed on the methodology</italic>.</p><p>In the revised manuscript, we set up our revised emphasis on neurobiology in the Introduction: “Current methods for studying neurocognitive networks in humans and primates depend on drawing correlations between brain network activity and behavior. […] Here, we employ a new approach that uses an unbiased decomposition of total network activity under diverse task conditions, enabling the computational extraction of latent structure in functional network interactions and post-hoc examination of its dynamic evolution during behavior”.</p><p>In the revised manuscript, we also clarified the key findings in the Introduction:</p><p>“Furthermore, the findings of functionally-specific brain network structures […] in long-hypothesized “cognitive chains”.</p><p>Reviewer #3:</p><p><italic>The nature of the task in combination with the complex analysis often makes interpretation of the results complex, and the authors resort to an interpretation that does not rely on the data. There are examples of this throughout the manuscript. Here is a typical one</italic>:</p><p><italic>“This result suggests that Structure 5 underlies the context-dependent feedback modulation of response perception linked to social reasoning in the task (‘why vM is frightened?’ or ‘is vM frightened because it was threatened by something?’ or ‘should I be concerned that vM is frightened?’).”</italic></p><p><italic>This kind of inference is inappropriate, and is also unnecessary</italic>.</p><p>We agree that our interpretations on the behavior were overreaching, and we removed them across the revised manuscript. In the revised Results section, our interpretations of the network structures now solely rely on the functional specificity (Dimension 1) and temporal structure (Dimension 2), which we elaborated later in the Discussion section based on reverse inference from the frequency profiles (Dimension 2) and the brain regions activated (Dimension 3).</p><p><italic>The remaining major comments were either about the technicalities of the analysis, or about the reporting of these technicalities. Where possible, we would like you to address these technical concerns. More broadly speaking, we would like you to focus on clarifying the more technical aspects of the manuscript so that the manuscript can be clearly understood by a broad audience</italic>.</p><p><italic>Other comments</italic>:</p><p>Reviewer #1:</p><p><italic>5) In the subsection “Deconvolution Analysis or Cortical Information Processing,” wouldn't it be more useful task-wise to identify independent sources by using only</italic> C<sup>+</sup> <italic>trials instead of merging data from</italic> C<sup>+</sup> <italic>&amp;</italic> C<sup>–</sup> <italic>trials for ICA? Did the authors do this analysis? How does it affect the results?</italic></p><p>In an earlier analysis presented in the 2012 Japanese Neuroscience Meeting, we screened out the <italic>C</italic><sup><italic>–</italic></sup> trials and focused only on <italic>C</italic><sup><italic>+</italic></sup> trials. The results, including the ICA results and the network modules, were very similar to those from the merged <italic>C</italic><sup><italic>+</italic></sup> and <italic>C</italic><sup><italic>–</italic></sup> data. Thus, we included <italic>C</italic><sup><italic>–</italic></sup> trials to increase the variety in experimental conditions (Dimension 1), which could provide additional information to help verify the functionality of each structure. Moreover, our deconvolution of the merged data into discrete networks highlights the strength of our method to analytically simplify complex physiological data, and might be scalable to even larger datasets.</p><p><italic>6) In the first paragraph of the subsection “Dynamic Cognitive Chain Describes Social Context Monitoring” to examine how the network interactions change as a function of context, wouldn't it be better to examine the correlations between activation differences across contexts (</italic>C<sub>m</sub> &amp; C<sub>h</sub> vs. C<sub>w</sub><italic>) rather than between</italic> C<sup>+</sup> <italic>vs</italic>. <italic>C</italic><sup><italic>–</italic></sup> <italic>trials?</italic></p><p>Agreed. We re-analyzed the data and revised <xref ref-type="fig" rid="fig5">Figure 5</xref> and the corresponding figure legends and supplemental materials. In the Results, we now state: “To achieve this, we evaluated the correlations of the structures’ activation differences […] top-down modulation can integrate response information with abstract contextual information”.</p><p><italic>In the same subsection, as per</italic> <xref ref-type="fig" rid="fig5"><italic>Figure 5A</italic></xref><italic>, none of the correlations with structure 2 are significant. In that case does a causal dependence of structure 4 and 5 on structure 2 apply?</italic></p><p>To address this concern we performed a new analysis (revised <xref ref-type="fig" rid="fig5">Figure 5A</xref>, right) showing that a significant correlation was observed only between Structures 2 and 5, suggesting that top-down modulation integrated response information with abstract contextual information. Based on the results of this further analysis, we revised the graphical relationships among the 5 structures in <xref ref-type="fig" rid="fig5">Figure 5C</xref>.</p><p><italic>In the second paragraph of the same subsection, does this analysis include the time courses of gaze positions and scanning behavior of all contexts (</italic>C<sub>m</sub><italic>,</italic> C<sub>h</sub> <italic>&amp;</italic> C<sub>w</sub><italic>) and trials (</italic>C<sup>+</sup> <italic>&amp;</italic> C<sup>–</sup><italic>)? Did you find the timing correlations to be different across contexts?</italic></p><p>The analysis included the time courses from all contexts in <italic>C+R</italic><sub><italic>f</italic></sub>. The reason is that only in <italic>C+R</italic><sub><italic>f</italic></sub> trials did we observe both scanning and gazing behaviors that were significantly different than baseline.</p><p>We clarified this in the text: “We focused on the behavior in <italic>C+R</italic><sub><italic>f</italic></sub> trials, where both scanning and gazing behaviors changed significantly during the trials. We measured cross-correlations between the temporal dynamics of each structure and the median time course of scanning frequency (blue trace in <xref ref-type="fig" rid="fig1">Figure 1B</xref>) and gaze position (blue trace in <xref ref-type="fig" rid="fig1">Figure 1C</xref>) (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).”</p><p>Also, we found no significant difference in cross-correlations across contexts. This is not surprising since no significant differences in behavior were observed across contexts (Figure 1<italic>–</italic>figure supplement 4).</p><p>Reviewer #2:</p><p><italic>I would therefore urge the authors to shift the emphasis in the initial part of the paper towards what their technique and dataset tells us about the dynamics of connectivity in these brain regions, as opposed to being so heavily focussed on the methodology. That said, there were also times where it was unclear what order different techniques were being applied, and how they were being applied. For instance, it is mentioned that ICA was first applied, but it was unclear what the input dimensions of the ICA were, or how the obtained components were subsequently used for the dDTF and PARAFAC analysis. I felt that a clear ‘analysis pipeline’ diagram, starting with raw data and ending with the key results, would be very helpful to include as a supplementary figure</italic>.</p><p>In accord with the reviewers suggestion, we revised the Abstract and Introduction, to better motivate our questions and key findings around how our methodological approach can extract highly concurrent and intertwined cognitive processes that are not be accessible by conventional behavioral measurements (see response above). That is, the neurobiological question we address is fundamentally how to access the structure of physiological information during intrinsic cognition, that at present may be ambiguously defined as mind-wandering or task-free networks.</p><p>As suggested by the reviewer, we also created a clear diagram for the analysis pipeline, including the increasing dimensionality of all variables from raw ECoG signals to the final latent network structures (new Figure 2<italic>–</italic>figure supplement 3).</p><p><italic>Finally, I felt that the presentation of the comparison-condition component in</italic> <xref ref-type="fig" rid="fig3 fig4"><italic>Figures 3 and 4</italic></xref> <italic>was unclear, in that it seemed diagrammatic whereas presumably it was based upon the statistics of the comparison being performed. A more quantitative approach to presenting this data would help to make it more clear and compelling</italic>.</p><p>We agree that it is important to show the original data in the Results, instead of a diagrammatic view. Therefore, in revised <xref ref-type="fig" rid="fig3 fig4">Figures 3A and 4A</xref>, we show the original scores from 18 comparisons and their statistical significance. The original <xref ref-type="fig" rid="fig3 fig4">Figures 3A and 4A</xref> now are combined in <xref ref-type="fig" rid="fig3s1">Figure 3<italic>–</italic>figure supplement 1</xref>.</p><p>Reviewer #3:</p><p><italic>The manuscript is written from a very technical perspective, and does not introduce the key neuroscience issues well. This makes it a very tough read, particularly for a broad interest journal such as</italic> eLife<italic>.</italic></p><p>In the revised manuscript, we redirected our emphasis to the more accessible issue of how our novel approach can extract functionally meaningful units from high dimensionality data during highly concurrent and intertwined cognitive processes. Importantly, even apparently simple forms of cognition can be complex, showing contingencies and parallel processing, and critical subtleties in the physiological structure of neural data would not be accessible by standard or traditional methods in neuroscience that depend on drawing explicit correlations between brain network activity and an empirical and quantifiable behavior. More generally, we have tried to simplify the text in key areas for the broad <italic>eLife</italic> readership.</p><p><italic>[Editors' note: further revisions were requested prior to acceptance, as described below</italic>.<italic>]</italic></p><p>Reviewer #1:</p><p><italic>I think the authors have done a fine job revising this paper, which presents a novel analytical approach to analyzing how density neurophysiological data gathered in monkeys viewing a set of different social scenarios. I'm not yet convinced that the descriptor “social scenario viewing task” is much better than “social monitoring task” – it's both a mouthful and I think still puts too much emphasis on the idea of a task. It might be more concise and precise to say the monkeys were viewing social scenarios. Other than that concern, I think the paper is ready to go</italic>.</p><p>We have significantly restructured the paper. We removed almost all the “social” parts because the social aspect of the task is only between video agents and therefore not central to the main conclusions. Instead, we focus the revision on neural processing of “context”. We study context by combining large-scale recording and analysis to decipher brain networks for fast, internal, concurrent, and interdependent cognitive processing. Our findings demonstrate that context processing is composed of network structures. These can be encoded in bottom-up interactions between sensory and association areas, and top-down interactions for behavioral modulation.</p><p>Regarding the task, we now describe it simply as “monkeys watched videos of two agents interacting in different situational contexts.”</p><p>Reviewer #2:</p><p><italic>I still remain quite unsure where I stand with this paper. The authors have put quite some work into making the manuscript clearer, and providing more substantial information concerning the methodology. But the main concerns of the reviewers seem to have held: because of the task design, it is very difficult to draw any strong conclusions about the role of the identified networks in social cognition. In their own words, they acknowledge this: “without a proper control, we can't conclusively link our results to social cognition.” As such, the authors now explicitly state (in their response) that the main conclusion from the paper is about the development of a novel method. Not much can be learnt about the explicit meaning of the underlying cognitive processes</italic>.</p><p>We agree that it’s difficult to draw firm conclusions on social cognition. Thus, we revised our paper to focus on functional network dynamics for context processing. We show how to extract computational structures from brain network dynamics, and map the internal processing of context. Our findings demonstrate the organization of a “context” network showing context is encoded in large-scale bottom-up interactions among distributed brain areas, and could later be reactivated top down to correlated eye movements.</p><p>The link between brain networks and behavior was established by a new eye movement analysis showing context and response dependence in gazing behavior (see new <xref ref-type="fig" rid="fig2">Figure 2</xref>). In the previous analysis, we compared eye movements among trials with different contexts (<italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs. <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs. <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, and <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub> vs. <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub> vs. <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>), and found no context dependence. In the new analysis, we performed the same 9 comparisons used in the brain activity analysis to identify context and response dependence: “We performed 9 pairwise comparisons on gaze positions from different context situations, separating C<sup>+</sup> and C<sup>–</sup> conditions, to examine their context and response dependence. For context dependence, we compared behaviors from trials with different context stimuli but the same response stimulus (6 comparisons: <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub>, and <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> for context dependence in <italic>R</italic><sub><italic>f</italic></sub>; <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub> vs <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub> vs <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, and <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub> vs <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub> for context dependence in <italic>R</italic><sub><italic>n</italic></sub>). For response dependence, we compared behaviors from trials with the same context stimulus but with different response stimuli (3 comparisons: <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>w</italic></sub><italic>R</italic><sub><italic>n</italic></sub>, and <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>f</italic></sub> vs <italic>C</italic><sub><italic>h</italic></sub><italic>R</italic><sub><italic>n</italic></sub>).”</p><p>By using the same comparisons for both behavior and brain activity analyses we could directly compare their context and response dependence.</p><p><italic>The question then becomes, is this novel method of sufficiently broad interest and importance that it will change the views of the community?</italic></p><p>We believe that our new method and its application to the issue of context processing in primate brain more than meets the high criteria of <italic>eLife</italic> for novelty and advance. The computational methods we use in the paper have literally never been reported before for this type of data. The question of whether it is significant in a biological meaning remains to be verified by others in the field, but we believe this is not a limitation of the study. How to handle big data from neurophysiology and extract biological meaning from its computational analyses is one of if not the most important question in neuroscience in the next 25 years. Our study provides a road map for one type of big data analysis of high density ECoG data.</p><p>Our approach of combining large-scale high-resolution neuronal recording and unbiased data-driven analysis is aimed at disentangling simultaneous and interdependent functional brain networks to provide a high-resolution brain-wide network description of context processing. Our findings demonstrate the structure of a “context” brain network showing that contextual information can be encoded in the dynamic interactions between the sensory and association areas, which could explain the neuronal basis of why context processing can affect a wide range of cognitive operations, from lower-level perception to higher-level executive functions. Moreover, our data-driven analysis also discovered other modular brain networks that either process lower-level sensory inputs for context encoding or integrate contextual information for behavioral modulation.</p><p>The brain’s ability to process contextual information provides enormous behavioral flexibility, while deficits were thought to lead to psychiatric disorders such as schizophrenia and post-traumatic stress disorder. Even though contextual information processing has been studied in a variety of cognitive domains and brain areas, a comprehensive functional view of the brain circuits that mediate contextual processing and modulation remain lacking. The value of our study is that we can use unbiased computational tools to extract structure from physiological data that appears to track actual cognitive processing. New methods should be evaluated in their potential to provide quantitative assessments of raw data. We do not want to be penalized because our data do not fit with less complex models drawn from fMRI, EEG. MEG methods.</p><p><italic>The authors main claim seems to be that it will reveal previously undiscovered ‘cognitive chains’. What exactly is meant by this? That brain regions are activated sequentially in response to a task, and that different brain areas will be recruited depending upon the cognitive function? On the one hand, this seems to be something that we already know from many years of MEG and EEG research in humans. On the other, it is clear that the spatial resolution of the ECoG data far surpasses this research, and the extraction of network structure is very different from what has gone before</italic>.</p><p>In the revision, we have eliminated discussion of “cognitive chains”. The concept we focused on instead is “sequential and modular network interactions”, which means that not only brain regions are activated at different times during a task, but their interactions contain modular structures. In the new <xref ref-type="fig" rid="fig7">Figure 7</xref>, we show that modular networks whose activity coordinated with each other in a deterministic manner, even though they were overlapping in time, frequency, and space. We believe that these multiplexed neuronal structures represent a meta-structure organization for brain network communication.</p><p><italic>Nevertheless, I still very much struggled to understand whether this extraction of network structure was (a) valid or (b) important. In terms of validity, if I were reviewing this paper at a methods journal, I'd expect a set of examples in simulated data to convince me that the method works well and robustly. In terms of importance, it's precisely because the task is poorly controlled that I don't get an “aha!” moment when examining the results that convinces me that it has definitely worked</italic>.</p><p><italic>I can see that there is a lot of potential in the paper, and it seems unfair to dismiss what could be an important set of findings about a novel technique. But equally I didn't find the results and structure of the paper sufficiently compelling or clear to warrant publication at present. I'd be open to other reviewers pointing out what they felt was the evidence that the technique works convincingly, or that it has produced a particularly important result.</italic></p><p>In summary, we have addressed these concerns by:</p><p>1) Revising the paper to focus on cortical networks underlying “context processing”, instead of “social cognition” or “cognitive chains”.</p><p>2) Providing new behavioral analyses to reveal context-dependent eye movement to enable a more direct comparison between brain activity and behavior.</p><p>3) Clarifying and simplifying the methods and results with improved text and figures.</p><p>Reviewer #3:</p><p><italic>The authors have done a good job dealing with the technical concerns and no longer over-interpret the data</italic>.</p><p><italic>However, there still remain concerns about whether the manuscript as currently written can be understood by a broad interest readership, or even a relatively specialised readership within the field. Indeed the mathematical expertise needed even to understand the basic analysis is extreme, and the authors do a very poor job in making the analysis comprehensible. Like the other reviewers, I suspect, I am still finding it difficult to really evaluate the neuroscientific findings, as I cannot fully understand their implications</italic>.</p><p><italic>Despite the unusual and high quality of the data and the sophisticated nature of the analysis, it is therefore very difficult to understand what we have learnt about the cognitive processes.</italic></p><p><italic>For example, the figure legends do not explain how to read the figures. The new diagram asked for by a different reviewer is difficult to understand</italic>.</p><p><italic>It is essential that the authors address this in both the Abstract and the main text before this can be published in a journal such as</italic> eLife<italic>.</italic></p><p>In this revision, we provided a new figure (<xref ref-type="fig" rid="fig3">Figure 3</xref>) to provide a more intuitive idea of our analysis. We also refined the main result figures (<xref ref-type="fig" rid="fig4 fig5">Figures 4 and 5</xref>), removed unnecessary supplemental figures, and move the more technical descriptions from the main text and figure legends to the Materials and methods. Furthermore, we have given the paper to two readers outside of the field, and incorporated their feedback in this revision.</p><p>For neuroscientific findings, we focus the revision on neural processing of “context”: “The findings provide fundamental insights into context processing. […] These dynamic interactions between the unimodal sensory and multimodal association areas could explain the neuronal basis of why context processing can affect a wide range of cognitive operations, from lower-level perception to higher-level executive functions.”</p><p>And: “We showed that contextual processing, from initial perception to later modulation, was represented not by sequential activation of functionally specialized brain areas, but by sequential communication among functionally specialized brain areas. Thus, we believe that the network structures we observed represent a module of modules, or “meta-module” for brain connectivity, and further investigation on this meta-structure organization for brain network communication could help determine how deficits in context processing could contribute to psychiatric disorders such as schizophrenia (<xref ref-type="bibr" rid="bib5">Barch et al., 2003</xref>) and post-traumatic stress disorder (<xref ref-type="bibr" rid="bib45">Milad et al., 2009</xref>).”</p><p><italic>[Editors' note: further revisions were requested prior to acceptance, as described below</italic>.<italic>]</italic></p><p><italic>[...] If I am correct, it seems like the key claims can be summarised as follows</italic>:</p><p><italic>a) Large-scale network interactions are different in different contexts</italic>.</p><p><italic>b) Bottom-up connections from posterior temporal cortex to anterior temporal cortex and medial PFC can encode a context whilst it is being first processed and held on-line.</italic></p><p><italic>c) These exact same brain regions exhibit the opposite top-down connectivity only seconds later when the context is being applied to incoming sensory information.</italic></p><p><italic>d) The extent to which the bottom-up connectivity is active during the processing of context predicts the extent to which the top-down connectivity will act when the context is later being applied</italic>.</p><p><italic>To me, these claims seem to be striking and important and it is these claims that the manuscript has been judged on, but these claims no longer appear anywhere in the Abstract or Introduction and are only in a subsection of the Discussion. So I am asking for one further revision. In this round of revision, I am asking for changes to the Abstract, to the Introduction (and possibly also to the Discussion if you choose), which concisely and precisely highlight the new neuroscience claims, and which change the tone of the current version of the manuscript from being largely a methodological innovation, to being a balanced manuscript which introduces a new technique to make a clear and precise claim about the contextual processing of sensory information</italic>.</p><p>We deeply appreciate and fully agreed with the comments and suggestions. In this revision, we rewrote the Abstract and significantly restructured the Introduction, with the goal to balance the paper by clarifying and emphasizing the neuroscience insights our paper provides for context processing. We also fine-tuned the Discussion accordingly.</p></body></sub-article></article>