<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">48571</article-id><article-id pub-id-type="doi">10.7554/eLife.48571</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>DeepFly3D, a deep learning-based approach for 3D limb and appendage tracking in tethered, adult <italic>Drosophila</italic></article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-143445"><name><surname>Günel</surname><given-names>Semih</given-names></name><email>semih.gunel@epfl.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-143449"><name><surname>Rhodin</surname><given-names>Helge</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2692-0801</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-143450"><name><surname>Morales</surname><given-names>Daniel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7469-0898</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-143451"><name><surname>Campagnolo</surname><given-names>João</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-142297"><name><surname>Ramdya</surname><given-names>Pavan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5425-4610</contrib-id><email>pavan.ramdya@epfl.ch</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-71957"><name><surname>Fua</surname><given-names>Pascal</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Computer Vision Laboratory, School of Computer and Communication Sciences</institution><institution>EPFL</institution><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Neuroengineering Laboratory, Brain Mind Institute &amp; Interfaculty Institute of Bioengineering, School of Life Sciences</institution><institution>EPFL</institution><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Computer Science</institution><institution>UBC</institution><addr-line><named-content content-type="city">Vancouver</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>O'Leary</surname><given-names>Timothy</given-names></name><role>Reviewing Editor</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>04</day><month>10</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e48571</elocation-id><history><date date-type="received" iso-8601-date="2019-05-18"><day>18</day><month>05</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-09-28"><day>28</day><month>09</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Günel et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Günel et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-48571-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.48571.001</object-id><p>Studying how neural circuits orchestrate limbed behaviors requires the precise measurement of the positions of each appendage in three-dimensional (3D) space. Deep neural networks can estimate two-dimensional (2D) pose in freely behaving and tethered animals. However, the unique challenges associated with transforming these 2D measurements into reliable and precise 3D poses have not been addressed for small animals including the fly, <italic>Drosophila melanogaster</italic>. Here, we present DeepFly3D, a software that infers the 3D pose of tethered, adult <italic>Drosophila</italic> using multiple camera images. DeepFly3D does not require manual calibration, uses pictorial structures to automatically detect and correct pose estimation errors, and uses active learning to iteratively improve performance. We demonstrate more accurate unsupervised behavioral embedding using 3D joint angles rather than commonly used 2D pose data. Thus, DeepFly3D enables the automated acquisition of <italic>Drosophila</italic> behavioral measurements at an unprecedented level of detail for a variety of biological applications.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>3D pose estimation</kwd><kwd>animal behavior</kwd><kwd>deep learning</kwd><kwd>computer vision</kwd><kwd>unsupervised classification</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>175667</award-id><principal-award-recipient><name><surname>Morales</surname><given-names>Daniel</given-names></name><name><surname>Ramdya</surname><given-names>Pavan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>181239</award-id><principal-award-recipient><name><surname>Morales</surname><given-names>Daniel</given-names></name><name><surname>Ramdya</surname><given-names>Pavan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001703</institution-id><institution>EPFL</institution></institution-wrap></funding-source><award-id>iPhD</award-id><principal-award-recipient><name><surname>Günel</surname><given-names>Semih</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006112</institution-id><institution>Microsoft Research</institution></institution-wrap></funding-source><award-id>JRC Project</award-id><principal-award-recipient><name><surname>Rhodin</surname><given-names>Helge</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Swiss Government Excellence Postdoctoral Scholarship</institution></institution-wrap></funding-source><award-id>2018.0483</award-id><principal-award-recipient><name><surname>Morales</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>DeepFly3D, a deep learning-based software, measures limb and appendage movements in tethered, behaving <italic>Drosophila</italic> and enables precise behavioral measurements during neural recordings, stimulation, and other biological experiments.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The precise quantification of movements is critical for understanding how neurons, biomechanics, and the environment influence and give rise to animal behaviors. For organisms with skeletons and exoskeletons, these measurements are naturally made with reference to 3D joint and appendage locations. Paired with modern approaches to simultaneously record the activity of neural populations in tethered, behaving animals (<xref ref-type="bibr" rid="bib10">Dombeck et al., 2007</xref>; <xref ref-type="bibr" rid="bib42">Seelig et al., 2010</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2018</xref>), 3D joint and appendage tracking promises to accelerate the discovery of neural control principles, particularly in the genetically tractable and numerically simple nervous system of the fly, <italic>Drosophila melanogaster</italic>.</p><p>However, algorithms for reliably estimating 3D pose in such small <italic>Drosophila</italic>-sized animals have not yet been developed. Instead, multiple alternative approaches have been taken. For example, one can affix and use small markers—reflective, colored, or fluorescent particles—to identify and reconstruct keypoints from video data (<xref ref-type="bibr" rid="bib2">Bender et al., 2010</xref>; <xref ref-type="bibr" rid="bib21">Kain et al., 1910</xref>; <xref ref-type="bibr" rid="bib47">Todd et al., 2017</xref>). Although this approach works well on humans (<xref ref-type="bibr" rid="bib29">Moeslund and Granum, 2000</xref>), in smaller, <italic>Drosophila</italic>-sized animals markers likely hamper movements and are difficult to mount on sub-millimeter scale limbs. Most importantly, measurements of one or even two markers for each leg (<xref ref-type="bibr" rid="bib47">Todd et al., 2017</xref>) cannot fully describe 3D limb kinematics. Another strategy has been to use computer vision techniques that operate without markers. However, these measurements have been restricted to 2D pose in freely behaving flies. Before the advent of deep learning, this was accomplished by matching the contours of animals seen against uniform backgrounds (<xref ref-type="bibr" rid="bib20">Isakov et al., 2016</xref>), measuring limb tip positions using complex TIRF-based imaging approaches (<xref ref-type="bibr" rid="bib28">Mendes et al., 2013</xref>), or measuring limb segments using active contours (<xref ref-type="bibr" rid="bib50">Uhlmann et al., 2017</xref>). In addition to being limited to 2D rather than 3D pose, these methods are complex, time-consuming, and error-prone in the face of long data sequences, cluttered backgrounds, fast motion, and occlusions that naturally occur when animals are observed from a single 2D perspective.</p><p>As a result, in recent years the computer vision community has largely forsaken these techniques in favor of deep learning-based methods. Consequently, the efficacy of monocular 3D human pose estimation algorithms has greatly improved. This is especially true when capturing human movements for which there is enough annotated data to train deep networks effectively. Walking and upright poses are prime examples of this, and state-of-the-art algorithms (<xref ref-type="bibr" rid="bib34">Pavlakos et al., 2017a</xref>; <xref ref-type="bibr" rid="bib48">Tome et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Popa et al., 2017</xref>; <xref ref-type="bibr" rid="bib30">Moreno-noguer, 2017</xref>; <xref ref-type="bibr" rid="bib24">Martinez et al., 2017</xref>; <xref ref-type="bibr" rid="bib27">Mehta et al., 2017</xref>; <xref ref-type="bibr" rid="bib40">Rogez et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Pavlakos et al., 2017b</xref>; <xref ref-type="bibr" rid="bib51">Zhou et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Tekin et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Sun et al., 2017</xref>) now deliver impressive real-time results in uncontrolled environments. Increased robustness to occlusions can be obtained by using multi-camera setups (<xref ref-type="bibr" rid="bib11">Elhayek et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Rhodin et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Simon et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Pavlakos et al., 2017b</xref>) and triangulating the 2D detections. This improves accuracy while making it possible to eliminate false detections.</p><p>These advances in 2D pose estimation have also recently been used to measure behavior in laboratory animals. For example, DeepLabCut provides a user-friendly interface to DeeperCut, a state-of-the-art human pose estimation network (<xref ref-type="bibr" rid="bib25">Mathis et al., 2018</xref>), and LEAP (<xref ref-type="bibr" rid="bib36">Pereira et al., 2019</xref>) can successfully track limb and appendage landmarks using a shallower network. Still, 2D pose provides an incomplete representation of animal behavior: important information can be lost due to occlusions, and movement quantification is heavily influenced by perspective.</p><p>Approaches used to translate human 2D to 3D pose have also been applied to larger animals, like lab mice and cheetahs (<xref ref-type="bibr" rid="bib32">Nath et al., 2019</xref>), but require the use of calibration boards. These techniques cannot be easily transferred for the study of small animals like <italic>Drosophila</italic>: adult flies are approximately 2.5 mm long and precisely registering multiple camera viewpoints using traditional approaches would require the fabrication of a prohibitively small checkerboard pattern, along with the tedious labor of using a small, external calibration pattern. Moreover, flies have many appendages and joints, are translucent, and in most laboratory experiments are only illuminated using infrared light (to avoid visual stimulation)—precluding the use of color information.</p><p>To overcome these challenges, we introduce DeepFly3D, a deep learning-based software pipeline that achieves comprehensive, rapid, and reliable 3D pose estimation in tethered, behaving adult <italic>Drosophila</italic> (<xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref>). DeepFly3D is applied to synchronized videos acquired from multiple cameras. It first uses a state-of-the-art deep network (<xref ref-type="bibr" rid="bib33">Newell et al., 2016</xref>) and then enforces consistency across views. This makes it possible to eliminate spurious detections, achieve high 3D accuracy, and use 3D pose errors to further fine-tune the deep network to achieve even better accuracy. To register the cameras, DeepFly3D uses a novel calibration mechanism in which the fly itself is the calibration target. During the calibration process, we also employ sparse bundle adjustment methods, as previously used for human pose estimation (<xref ref-type="bibr" rid="bib45">Takahashi et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Triggs et al., 2000</xref>; <xref ref-type="bibr" rid="bib38">Puwein et al., 2014</xref>). Thus, the user does not need to manufacture a prohibitively small calibration pattern, or repeat cumbersome calibration protocols. We explain how users can modify the codebase to extend DeepFly3D for 3D pose estimation in other animals (see Materials and methods). Finally, we demonstrate that unsupervised behavioral embedding of 3D joint angle data is robust against problematic artifacts present in embeddings of 2D pose data. In short, DeepFly3D delivers 3D pose estimates reliably, accurately, and with minimal manual intervention while also providing a critical tool for automated behavioral data analysis.</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.002</object-id><label>Figure 1.</label><caption><title>Deriving 3D pose from multiple camera views.</title><p>(<bold>A</bold>) Raw image inputs to the Stacked Hourglass deep network. (<bold>B</bold>) Probability maps output from the trained deep network. For visualization purposes, multiple probability maps have been overlaid for each camera view. (<bold>C</bold>) 2D pose estimates from the Stacked Hourglass deep network after applying pictorial structures and multi-view algorithms. (<bold>D</bold>) 3D pose derived from combining multiple camera views. For visualization purposes, 3D pose has been projected onto the original 2D camera perspectives. (<bold>E</bold>) 3D pose rendered in 3D coordinates. Immobile thorax-coxa joints and antennal joints have been removed for clarity.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig1-v2.tif"/></fig><media id="fig1video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-48571-fig1-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.48571.003</object-id><label>Figure 1—video 1.</label><caption><title>Deriving 3D pose from multiple camera views during backward walking in an optogenetically stimulated MDN&gt;CsChrimson fly.</title></caption></media></fig-group></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>DeepFly3D</title><p>The input to DeepFly3D is video data from seven cameras. These images are used to identify the 3D positions of 38 landmarks per animal: (i) five on each limb – the thorax-coxa, coxa-femur, femur-tibia, and tibia-tarsus joints as well as the pretarsus, (ii) six on the abdomen - three on each side, and (iii) one on each antenna - for measuring head rotations. Our software incorporates the following innovations designed to ensure automated, high-fidelity, and reliable 3D pose estimation.</p><sec id="s2-1-1"><title>Calibration without an external calibration pattern</title><p>Estimating 3D pose from multiple images requires calibrating the cameras to achieve a level of accuracy commensurate with the target size—a difficult challenge when measuring leg movements for an animal as small as <italic>Drosophila</italic>. Therefore, instead of using a typical external calibration grid, DeepFly3D uses the fly itself as a calibration target. It detects arbitrary points on the fly’s body and relies on bundle-adjustment (<xref ref-type="bibr" rid="bib8">Chavdarova et al., 2018</xref>) to simultaneously assign 3D locations to these points and to estimate the positions and orientations of each camera. To increase robustness, it enforces geometric constraints that apply to tethered flies with respect to limb segment lengths and ranges of motion.</p></sec><sec id="s2-1-2"><title>Geometrically consistent reconstructions</title><p>Starting with a state-of-the-art deep network for 2D keypoint detection in individual images (<xref ref-type="bibr" rid="bib33">Newell et al., 2016</xref>), DeepFly3D enforces geometric consistency constraints across multiple synchronized camera views. When triangulating 2D detections to produce 3D joint locations, it relies on pictorial structures and belief propagation message passing (<xref ref-type="bibr" rid="bib13">Felzenszwalb and Huttenlocher, 2005</xref>) to detect and further correct erroneous pose estimates.</p></sec><sec id="s2-1-3"><title>Self-supervision and active learning</title><p>DeepFly3D also uses multiple view geometry as a basis for active learning. Thanks to the redundancy inherent in obtaining multiple views of the same animal, we can detect erroneous 2D predictions for correction that would most efficiently train the 2D pose deep network. This approach greatly reduces the need for time-consuming manual labeling (<xref ref-type="bibr" rid="bib43">Simon et al., 2017</xref>). We also use pictorial structure corrections to fine-tune the 2D pose deep network. Self-supervision constitutes 85% of our training data.</p></sec></sec><sec id="s2-2"><title>2D pose performance and improvement using pictorial structures</title><p>We validated our approach using a challenging dataset of 2,063 image frames manually annotated using the DeepFly3D annotation tool and sampled uniformly from each camera. Images for testing and training were 480 × 960 pixels. The test dataset included challenging frames and occasional motion blur to increase the difficulty of pose estimation. For training, we used a final training dataset of 37,000 frames, an overwhelming majority of which were first automatically corrected using pictorial structures. On test data, we achieved a Root Mean Square Error (RMSE) of 13.9 pixels. Compared with a ground truth RMSE of 12.4 pixels – via manual annotation of 210 images by a new human expert – our Network Annotation/Manual Annotation ratio of 1.12 (13.9 pixels / 12.4 pixels) is similar to the ratio of another state-of-the-art network (<xref ref-type="bibr" rid="bib25">Mathis et al., 2018</xref>): 1.07 (2.88 pixels / 2.69 pixels). Setting a 50 pixel threshold (approximately one third the length of the femur) for PCK (percentage of correct keypoints) computation, we observed a 98.2% general accuracy before applying pictorial structures. Notably, if we reduced our threshold to 30 or 20 pixels, we still achieved 95% or 89% accuracy, respectively (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.004</object-id><label>Figure 2.</label><caption><title>Mean absolute error distribution.</title><p>(<bold>A</bold>) PCK (percentage of keypoints) accuracy as a function of mean absolute error (MAE) threshold. (<bold>B</bold>) Evaluating network prediction error in a low data regime. The Stacked Hourglass network (blue circles) shows near asymptotic prediction error (red dashed line), even when trained with only 400 annotated images. After 800 annotations, there are minimal improvements to the MAE. (<bold>C</bold>) MAE for different limb landmarks. Violin plots are overlaid with raw data points (white circles).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig2-v2.tif"/></fig><p>To test the performance of our network in a low data regime, we trained a two-stacked network using ground-truth annotations data from seven cameras (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). We compared the results to an asymptotic prediction error (i.e. the error observed when the network is trained using the full dataset of 40,000 annotated images) and to the variability observed in human annotations of 210 randomly selected images. We measured an asymptotic MAE (mean absolute error) of 10.5 pixels and a human variability MAE of 9.2 pixels. With 800 annotations, our network achieved a similar accuracy to manual annotation and was near the asymptotic prediction error. Further annotation yielded diminishing returns.</p><p>Although our network achieves high accuracy, the error is not isotropic (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The tarsus tips (i.e. pretarsus) exhibited larger error than the other joints, perhaps due to occlusions from the spherical treadmill, and higher positional variance. Increased error observed for body-coxa joints might be due to the difficulty of annotating these landmarks from certain camera views.</p><p>To correct the residual errors, we applied pictorial structures. This strategy fixed 59% of the remaining erroneous predictions, increasing the final accuracy to 99.2%, from 98.2%. These improvements are illustrated in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Pictorial structure failures were often due to pose ambiguities resulting from heavy motion blur. These remaining errors were automatically detected with multi-view redundancy using <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>, and earmarked for manual correction using the DeepFly3D GUI.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.005</object-id><label>Figure 3.</label><caption><title>Pose estimation accuracy before and after using pictorial structures.</title><p>Pixel-wise 2D pose errors/residuals (top) and their respective distributions (bottom) (<bold>A</bold>) before, or (<bold>B</bold>) after applying pictorial structures. Residuals larger than 35 pixels (red circles) represent incorrect keypoint detections. Those below this threshold (blue circles) represent correct keypoint detections.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig3-v2.tif"/></fig></sec><sec id="s2-3"><title>3D pose permits robust unsupervised behavioral classification</title><p>Unsupervised behavioral classification approaches enable the unbiased quantification of animal behavior by processing data features—image pixel intensities (<xref ref-type="bibr" rid="bib3">Berman et al., 2014</xref>; <xref ref-type="bibr" rid="bib6">Cande et al., 2018</xref>), limb markers (<xref ref-type="bibr" rid="bib47">Todd et al., 2017</xref>), or 2D pose (<xref ref-type="bibr" rid="bib36">Pereira et al., 2019</xref>)—to cluster similar behavioral epochs without user intervention and to automatically distinguish between otherwise similar actions. However, with this sensitivity may come a susceptibility to features unrelated to behavior including changes in image size or perspective resulting from differences in camera angle across experimental systems, variable mounting of tethered animals, and inter-animal morphological variability. In theory, each of these issues can be overcome—providing scale and rotational invariance—by using 3D joint angles rather than 2D pose for unsupervised embedding.</p><p>To test this possibility, we performed unsupervised behavioral classification (<xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig5">Figure 5</xref>) on video data taken during optogenetic stimulation experiments that repeatedly and reliably drove certain behaviors. Specifically, we optically activated CsChrimson (<xref ref-type="bibr" rid="bib22">Klapoetke et al., 2014</xref>) to elicit backward walking in MDN&gt;CsChrimson animals (<xref ref-type="video" rid="fig5video1">Figure 5—video 1</xref>) (<xref ref-type="bibr" rid="bib4">Bidaye et al., 2014</xref>), or antennal grooming in aDN&gt;CsChrimson animals (<xref ref-type="video" rid="fig5video2">Figure 5—video 2</xref>) (<xref ref-type="bibr" rid="bib15">Hampel et al., 2015</xref>). We also stimulated control animals lacking the UAS-CsChrimson transgene (<xref ref-type="video" rid="fig5video3">Figure 5—video 3</xref>) (MDN-GAL4/+ and aDN-GAL4/+). First, we performed unsupervised behavioral classification using 2D pose data from three adjacent cameras containing keypoints for three limbs on one side of the body. Using these data, we generated a behavioral map (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). In this map each individual cluster would ideally represent a single behavior (e.g. backward walking, or grooming) and be populated by nearly equal amounts of data from each of the three cameras. This was not the case: data from each camera covered non-overlapping regions and clusters (<xref ref-type="fig" rid="fig4">Figure 4B–D</xref>). This effect was most pronounced when comparing regions populated by cameras 1 and 2 versus camera 3. Therefore, because the underlying behaviors were otherwise identical (data across cameras were from the same animals and experimental time points), we can conclude that unsupervised behavioral classification of 2D pose data is sensitive to being corrupted by viewing angle differences.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.006</object-id><label>Figure 4.</label><caption><title>Unsupervised behavioral classification of 2D pose data is sensitive to viewing angle.</title><p>(<bold>A</bold>) Behavioral map derived using 2D pose data from three adjacent cameras (Cameras 1, 2, and 3) but the same animals and experimental time points. Shown are clusters (black outlines) that are enriched (yellow), or sparsely populated (blue) with data. Different clusters are enriched for data from either (<bold>B</bold>) camera 1, (<bold>C</bold>) camera 2, or (<bold>D</bold>) camera 3. Behavioral embeddings were derived using 1 million frames during 4 s of optogenetic stimulation of MDN&gt;CsChrimson (n = 6 flies, n = 29 trials), aDN&gt;CsChrimson (n = 6 flies, n = 30 trials), and wild-type control animals (MDN-GAL4/+: n = 4 flies, n = 20 trials. aDN-GAL4/+: n = 4 flies, n = 23 trials).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig4-v2.tif"/></fig><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.007</object-id><label>Figure 5.</label><caption><title>Unsupervised behavioral classification of 3D joint angle data.</title><p>Behavioral embeddings were calculated using 3D joint angles from the same 1 million frames used in <xref ref-type="fig" rid="fig4">Figure 4A</xref>. (<bold>A</bold>) Behavioral map combining all data during 4 s of optogenetic stimulation of MDN&gt;CsChrimson (n = 6 flies, n = 29 trials), aDN&gt;CsChrimson (n = 6 flies, n = 30 trials), and wild-type control animals (For MDN-Gal4/+, n = 4 flies, n = 20 trials. For aDN-Gal4/+ n = 4 flies, n = 23 trials). The same behavioral map is shown with only the data from (<bold>B</bold>) MDN&gt;CsChrimson stimulation, (<bold>C</bold>) aDN&gt;CsChrimson stimulation, or (<bold>D</bold>) control animal stimulation. Associated videos reveal that these distinct map regions are enriched for backward walking, antennal grooming, and forward walking, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig5-v2.tif"/></fig><media id="fig5video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-48571-fig5-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.48571.008</object-id><label>Figure 5—video 1.</label><caption><title>Representative MDN&gt;CsChrimson optogenetically activated backward walking. Orange circle indicates LED illumination and CsChrimson activation.</title></caption></media><media id="fig5video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-48571-fig5-video2.mp4"><object-id pub-id-type="doi">10.7554/eLife.48571.009</object-id><label>Figure 5—video 2.</label><caption><title>Representative aDN&gt;CsChrimson optogenetically activated antennal grooming. Orange circle indicates LED illumination and CsChrimson activation.</title></caption></media><media id="fig5video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-48571-fig5-video3.mp4"><object-id pub-id-type="doi">10.7554/eLife.48571.010</object-id><label>Figure 5—video 3.</label><caption><title>Representative control animal behavior during illumination. Orange circle indicates LED illumination and CsChrimson activation.</title></caption></media><media id="fig5video4" mime-subtype="mp4" mimetype="video" xlink:href="elife-48571-fig5-video4.mp4"><object-id pub-id-type="doi">10.7554/eLife.48571.011</object-id><label>Figure 5—video 4.</label><caption><title>Sample behaviors from 3D pose cluster enriched in backward walking.</title></caption></media><media id="fig5video5" mime-subtype="mp4" mimetype="video" xlink:href="elife-48571-fig5-video5.mp4"><object-id pub-id-type="doi">10.7554/eLife.48571.012</object-id><label>Figure 5—video 5.</label><caption><title>Sample behaviors from 3D pose cluster enriched in antennal grooming.</title></caption></media><media id="fig5video6" mime-subtype="mp4" mimetype="video" xlink:href="elife-48571-fig5-video6.mp4"><object-id pub-id-type="doi">10.7554/eLife.48571.013</object-id><label>Figure 5—video 6.</label><caption><title>Sample behaviors from 3D pose cluster enriched in forward walking.</title></caption></media></fig-group><p>By contrast, performing unsupervised behavioral classification using DeepFly3D-derived 3D joint angles resulted in a map (<xref ref-type="fig" rid="fig5">Figure 5</xref>) with a clear segregation and enrichment of clusters for different GAL4 driver lines and their associated behaviors, i.e. backward walking (<xref ref-type="video" rid="fig5video4">Figure 5—video 4</xref>), grooming (<xref ref-type="video" rid="fig5video5">Figure 5—video 5</xref>), and forward walking (<xref ref-type="video" rid="fig5video6">Figure 5—video 6</xref>). Thus, 3D pose overcomes serious issues arising from unsupervised embedding of 2D pose data, enabling more reliable and robust behavioral data analysis.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have developed DeepFly3D, a deep learning-based 3D pose estimation system that is optimized for quantifying limb and appendage movements in tethered, behaving <italic>Drosophila</italic>. By using multiple synchronized cameras and exploiting multiview redundancy, our software delivers robust and accurate pose estimation at the sub-millimeter scale. Ultimately, we may work solely with monocular images by lifting the 2D detections (<xref ref-type="bibr" rid="bib35">Pavlakos et al., 2017b</xref>) to 3D or by directly regressing to 3D (<xref ref-type="bibr" rid="bib46">Tekin et al., 2017</xref>) as has been achieved in human pose estimation studies. Our approach relies on supervised deep learning to train a neural network that detects 2D joint locations in individual camera images. Importantly, our network becomes increasingly competent as it runs: By leveraging the redundancy inherent to a multiple-camera setup, we iteratively reproject 3D pose to automatically detect and correct 2D errors, and then use these corrections to further train the network without user intervention.</p><p>None of the techniques we have put together—an approach for multiple-camera calibration that uses the animal itself rather than an external apparatus, an iterative approach to inferring 3D pose using graphical models as well as optimization based on dynamic programming and belief propagation, and a graphical user interface and active learning policy for interacting with, annotating, and correcting 3D pose data—are fly-specific. They could easily be adapted to other limbed animals, from mice to primates and humans. The only thing that would have to change significantly are the dimensions of the experimental setup. This would remove the need to deal with the very small scales <italic>Drosophila</italic> requires and would, in practice, make pose estimation easier. In the Materials and methods section, we explain in detail how organism-specific features of DeepFly3D—bone segment length, number of legs, and camera focal distance—can be modified to study, for example, humans, primates, rodents, or other insects.</p><p>As in the past, we anticipate that the development of new technologies for quantifying behavior will open new avenues and enhance existing lines of investigation. For example, deriving 3D pose using DeepFly3D can improve the resolution of studies examining how neuronal stimulation influences animal behavior (<xref ref-type="bibr" rid="bib6">Cande et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">McKellar et al., 2019</xref>), the precision and predictive power of efforts to define natural action sequences (<xref ref-type="bibr" rid="bib41">Seeds et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">McKellar et al., 2019</xref>), the assessment of interventions that target models of human disease (<xref ref-type="bibr" rid="bib12">Feany and Bender, 2000</xref>; <xref ref-type="bibr" rid="bib17">Hewitt and Whitworth, 2017</xref>), and links between neural activity and animal behavior—when coupled with recording technologies like 2-photon microscopy (<xref ref-type="bibr" rid="bib42">Seelig et al., 2010</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2018</xref>). Importantly, 3D pose improves the robustness of unsupervised behavioral classification approaches. Therefore, DeepFly3D is a critical step toward the ultimate goal of achieving fully-automated, high-fidelity behavioral data analysis.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>With synchronized <italic>Drosophila</italic> video sequences from seven cameras in hand, the first task for DeepFly3D is to detect the 2D location of 38 landmarks. These 2D locations of the same landmarks seen across multiple views are then triangulated to generate 3D pose estimates. This pipeline is depicted in <xref ref-type="fig" rid="fig6">Figure 6</xref>. First, we will describe our deep learning-based approach to detect landmarks in images. Then, we will explain the triangulation process that yields full 3D trajectories. Finally, we will describe how we identify and correct erroneous 2D detections automatically.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.014</object-id><label>Figure 6.</label><caption><title>The DeepFly3D pose estimation pipeline.</title><p>(<bold>A</bold>) Data acquisition from the multi-camera system. (<bold>B</bold>) Training and retraining of 2D pose. (<bold>C</bold>) 3D pose estimation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig6-v2.tif"/></fig><sec id="s4-1"><title>2D pose estimation</title><sec id="s4-1-1"><title>Deep network architecture</title><p>We aim to detect five joints on each limb, six on the abdomen, and one on each antenna, giving a total of 38 keypoints per time instance. To achieve this, we adapted a state-of-the-art Stacked Hourglass human pose estimation network (<xref ref-type="bibr" rid="bib33">Newell et al., 2016</xref>) by changing the input and output layers to accommodate a new input image resolution and a different number of tracked points. A single hourglass stack consists of residual bottleneck modules with max pooling, followed by up-sampling layers and skip connections. The first hourglass network begins with a convolutional layer and a pooling layer to reduce the input image size from 256 × 512 to 64 × 128 pixels. The remaining hourglass input and output tensors are 64 × 128. We used 8 stacks of hourglasses in our final implementation. The output of the network is a stack of probability maps, also known as heatmaps or confidence maps. Each probability map encodes the location of one keypoint, as the belief of the network that a given pixel contains that particular tracked point. However, probability maps do not formally define a probability distribution; their sum over all pixels does not equal 1.</p></sec><sec id="s4-1-2"><title>2D pose training dataset</title><p>We trained our network for 19 keypoints, resulting in the tracking of 38 points when both sides of the fly are taken into account. Determining which images to use for training purposes is critical. The intuitively simple approach—training with randomly selected images—may lead to only marginal improvements in overall network performance. This is because images for which network predictions can already be correctly made give rise to only small gradients during training. On the other hand, manually identifying images that may lead to incorrect network predictions is highly laborious. Therefore, to identify such challenging images, we exploited the redundancy of having multiple camera views (see section <italic>3D pose correction</italic>). Outliers in individual camera images were corrected automatically using images from other cameras, and frames that still exhibited large reprojection errors on multiple camera views were selected for manual annotation and network retraining. This combination of self supervision and active learning permits faster training using a smaller manually annotated dataset (<xref ref-type="bibr" rid="bib43">Simon et al., 2017</xref>). The full annotation and iterative training pipeline is illustrated in <xref ref-type="fig" rid="fig6">Figure 6</xref>. In total, 40,063 images were annotated: 5,063 were labeled manually in the first iteration, 29,000 by automatic correction, and 6,000 by manually correcting those proposed by the active learning strategy.</p></sec><sec id="s4-1-3"><title>Deep network training procedure</title><p>We trained our Stacked Hourglass network to regress from 256 × 512 pixel grayscale video images to multiple 64 × 128 probability maps. Specifically, during training and testing, networks output a 19 × 64 × 128 tensor; one 64 × 128 probability map per tracked point. During training, we created probability maps by embedding a 2D Gaussian with mean at the ground-truth point and 1px symmetrical extent (i.e. <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) on the diagonal of the covariance matrix. We calculated the loss as the <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> distance between the ground-truth and predicted probability maps. During testing, the final network prediction for a given point was the probability map pixel with maximum probability. We started with a learning rate of 0.0001 and then multiplied the learning rate by a factor of 0.1 once the loss function plateaued for more than five epochs. We used an RMSPROP optimizer for gradient descent, following the original Stacked Hourglass implementation, with a batch-size of eight images. Using 37,000 training images, the Stacked Hourglass network usually converges to a local minimum after 100 epochs (20 h on a single GPU).</p></sec><sec id="s4-1-4"><title>Network training details</title><p>Variations in each fly’s position across experiments are handled by the translational invariance of the convolution operation. In addition, we artificially augment training images to improve network generalization for further image variables. These variables include (i) illumination conditions – we randomly changed the brightness of images using a gamma transformation, (ii) scale – we randomly rescaled images between 0.80x - 1.20x, and (iii) rotation – we randomly rotated images and corresponding probability maps ±15°. This augmentation was enough to compensate for real differences in the size and orientation of tethered flies across experiments. Furthermore, as per general practice, the mean channel intensity was subtracted from each input image to distribute annotations symmetrically around zero. We began network training using pretrained weights from the MPII human pose dataset (<xref ref-type="bibr" rid="bib1">Andriluka et al., 2014</xref>). This dataset consists of more than 25,000 images with 40,000 annotations, possibly with multiple ground-truth human pose labels per image. Starting with a pretrained network results in faster convergence. However, in our experience, this does not affect final network accuracy in cases with a large amount of training data. We split the dataset into 37,000 training images, 2,063 testing images, and 1,000 validation images. None of these subsets shared common images or common animals, to ensure that the network could generalize across animals, and experimental setups. 5,063 of our training images were manually annotated, and the remaining data were automatically collected using belief propagation, graphical models, and active learning, (see section <italic>3D pose correction</italic>). Deep neural network parameters need to be trained on a dataset with manually annotated ground-truth key point positions. To initialize the network, we collected annotations using a custom multicamera annotation tool that we implemented in JavaScript using Google Firebase (<xref ref-type="fig" rid="fig7">Figure 7</xref>). The DeepFly3D annotation tool operates on a simple web-server, easing the distribution of annotations across users and making these annotations much easier to inspect and control.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.015</object-id><label>Figure 7.</label><caption><title>The DeepFly3D annotation tool.</title><p>This GUI allows the user to manually annotate joint positions on images from each of seven cameras. Because this tool can be accessed from a web browser, annotations can be performed in a distributed manner across multiple users more easily. A full description of the annotation tool can be found in the online documentation: <ext-link ext-link-type="uri" xlink:href="https://github.com/NeLy-EPFL/DeepFly3D">https://github.com/NeLy-EPFL/DeepFly3D</ext-link>. Scale bar is 50 pixels.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig7-v2.tif"/></fig></sec><sec id="s4-1-5"><title>Computing hardware and software</title><p>We trained our model on a desktop computing workstation running on an Intel Core i9-7900X CPU, 32 GB of DDR4 RAM, and a GeForce GTX 1080. With 37,000 manually and automatically labeled images, training takes nearly 20 h on a single GeForce GTX 1080 GPU. Our code is implemented with Python 3.6, Pytorch 0.4 and CUDA 9.2. Using this desktop configuration, our network can run at 100 Frames-Per-Second (FPS) using the 8-stack variant of the Hourglass network, and can run at 420 FPS using the smaller 2-stack version. Thanks to an effective initialization step, calibration takes 3–4 s. Error checking and error correction can be performed at 100 FPS and 10 FPS, respectively. Error correction is only performed in response to large reprojection errors and does not create a bottleneck in the overall speed of the pipeline.</p></sec><sec id="s4-1-6"><title>Accuracy analysis</title><p>Consistent with the human pose estimation literature, we report accuracy as Percentage of Correct Keypoints (PCK) and Root Mean Squared Error (RMSE). PCK refers to the percentage of detected points lying within a specific radius from the ground-truth label. We set this threshold as 50 pixels, which is roughly one third of the 3D length of the femur. The final estimated position of each keypoint was obtained by selecting the pixel with the largest probability value on the relevant probability map. We compared DeepFly3D’s annotations with manually annotated ground-truth labels to test our model’s accuracy. For RMSE, we report the square root of average pixel distance between the prediction and the ground-truth location of the tracked point. We remove trivial points such as the body-coxa and coxa-femur—which remain relatively stationary—to fairly evaluate our algorithms and to prevent these points from dominating our accuracy measurements.</p></sec></sec><sec id="s4-2"><title>From 2D landmarks to 3D trajectories</title><p>In the previous section, we described our approach to detect 38 2D landmarks. Let <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> denote the 2D position of landmark <inline-formula><mml:math id="inf4"><mml:mi>j</mml:mi></mml:math></inline-formula> in the image acquired by camera <inline-formula><mml:math id="inf5"><mml:mi>c</mml:mi></mml:math></inline-formula>. For each landmark, our task is now to estimate the corresponding 3D position, <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. To accomplish this, we used triangulation and bundle-adjustment (<xref ref-type="bibr" rid="bib16">Hartley and Zisserman, 2000</xref>) to compute 3D locations, and we used pictorial structures (<xref ref-type="bibr" rid="bib13">Felzenszwalb and Huttenlocher, 2005</xref>) to enforce geometric consistency and to eliminate potential errors caused by misdetections. We present these steps below.</p><sec id="s4-2-1"><title>Pinhole camera model</title><p>The first step is to model the projection operation that relates a specific <inline-formula><mml:math id="inf7"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> to its seven projections in each camera view <inline-formula><mml:math id="inf8"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. To make this easier, we follow standard practice and convert all Cartesian coordinates <inline-formula><mml:math id="inf9"><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> to homogeneous ones <inline-formula><mml:math id="inf10"><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> such that <inline-formula><mml:math id="inf11"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf12"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf13"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. From now on, we will assume that all points are expressed in homogeneous coordinates and omit the <inline-formula><mml:math id="inf14"><mml:mi>h</mml:mi></mml:math></inline-formula> subscript. Assuming that these coordinates are expressed in a coordinate system whose origin is in the optical center of the camera and whose z-axis is its optical axis, the 2D image projection <inline-formula><mml:math id="inf15"><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> of a 3D homogeneous point <inline-formula><mml:math id="inf16"><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> can be written as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>U</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>W</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>v</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>W</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>U</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>V</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>W</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>z</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext> with </mml:mtext></mml:mstyle><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where the 3 × 4 matrix <inline-formula><mml:math id="inf17"><mml:mi mathvariant="bold">𝐊</mml:mi></mml:math></inline-formula> is known as the <italic>intrinsic parameters matrix</italic>—scaling in the <inline-formula><mml:math id="inf18"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:mi>y</mml:mi></mml:math></inline-formula> direction and image coordinates of the principal point <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>c</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula>—that characterizes the camera settings.</p><p>In practice, the 3D points are not expressed in a camera fixed coordinate system, especially in our application where we use seven different cameras. Therefore, we use a world coordinate system that is common to all cameras. For each camera, we must therefore convert 3D coordinates expressed in this world coordinate system to camera coordinates. This requires rotating and translating the coordinates to account for the position of the camera’s optical center and its orientation. When using homogeneous coordinates, this is accomplished by multiplying the coordinate vector by a 4 × 4 <italic>extrinsic parameters matrix</italic><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mi mathvariant="bold">𝐑</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mi mathvariant="bold">𝐓</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf22"><mml:mi mathvariant="bold">𝐑</mml:mi></mml:math></inline-formula> is a 3 × 3 rotation matrix and <inline-formula><mml:math id="inf23"><mml:mi mathvariant="bold">𝐓</mml:mi></mml:math></inline-formula> a 3 × 1 translation vector. Combining <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> and <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> yields<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>U</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>W</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>v</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>W</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>U</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>V</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>W</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>z</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">P</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext mathvariant="bold">MK</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a 3 × 4 matrix.</p></sec><sec id="s4-2-2"><title>Camera distortion</title><p>The pinhole camera model described above is an idealized one. The projections of real cameras deviate from it. These deviations are referred to as distortions and must be accounted for. The most significant distortion is known as radial distortion because the error grows with the distance from the image center. For the cameras we use, radial distortion can be expressed as<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mtext> pinhole </mml:mtext></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mtext> pinhole </mml:mtext></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf25"><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> is the actual projection of a 3D point and <inline-formula><mml:math id="inf26"><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mtext>pinhole </mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi> <mml:mtext>pinhole </mml:mtext></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> is the one the pinhole model predicts. In other words, the four parameters <inline-formula><mml:math id="inf27"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mn>2</mml:mn><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mn>2</mml:mn><mml:mi>y</mml:mi></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> characterize the distortion. From now on, we will therefore write the full projection as<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf28"><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> denotes the ideal pinhole projection of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> and <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>f</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> the correction of <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>.</p></sec><sec id="s4-2-3"><title>Triangulation</title><p>We can associate to each of the seven cameras a projection function <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>π</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> like the one in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>, where <inline-formula><mml:math id="inf31"><mml:mi>c</mml:mi></mml:math></inline-formula> is the camera number. Given a 3D point and its projections <inline-formula><mml:math id="inf32"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> in the images, its 3D coordinates can be estimated by minimizing the <italic>reprojection error</italic><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf33"><mml:msub><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is one if the point was visible in image <inline-formula><mml:math id="inf34"><mml:mi>c</mml:mi></mml:math></inline-formula> and zero otherwise. In the absence of camera distortion, that is, when the projection <inline-formula><mml:math id="inf35"><mml:mi>π</mml:mi></mml:math></inline-formula> is a purely linear operation in homogeneous coordinates, this can be done for any number of cameras by solving a Singular Value Decomposition (SVD) problem (<xref ref-type="bibr" rid="bib16">Hartley and Zisserman, 2000</xref>). In the presence of distortions, we replace the observed <inline-formula><mml:math id="inf36"><mml:mi>u</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:mi>v</mml:mi></mml:math></inline-formula> coordinates of the projections by the corresponding <inline-formula><mml:math id="inf38"><mml:msub><mml:mi>u</mml:mi> <mml:mtext>pinhole </mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>u</mml:mi> <mml:mtext>pinhole </mml:mtext></mml:msub></mml:math></inline-formula> values of <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> before performing the SVD.</p></sec><sec id="s4-2-4"><title>Camera calibration</title><p>Triangulating as described above requires knowing the projection matrices <inline-formula><mml:math id="inf40"><mml:msub><mml:mi mathvariant="bold">𝐏</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> for each camera <inline-formula><mml:math id="inf41"><mml:mi>c</mml:mi></mml:math></inline-formula>, corresponding distortion parameters <inline-formula><mml:math id="inf42"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mn>2</mml:mn><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mn>2</mml:mn><mml:mi>y</mml:mi></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> of <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, together with the intrinsic parameters of focal length and principal point offset. In practice, we use the focal length and principal point offset provided by the manufacturer and estimate the remaining parameters automatically: the three translations and three rotations for each camera that define the corresponding matrix <inline-formula><mml:math id="inf43"><mml:mi mathvariant="bold">𝐌</mml:mi></mml:math></inline-formula> of extrinsic parameters along with the distortion parameters.</p><p>To avoid having to design the exceedingly small calibration pattern that more traditional methods use to estimate these parameters, we use the fly itself as calibration pattern and minimize the reprojection error of <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> for all joints simultaneously while allowing the camera parameters to also change. In other words we look for<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo>≤</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi mathvariant="bold">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf44"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi mathvariant="bold">𝐣</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the 3D locations and 2D projections of the landmarks introduced above and <inline-formula><mml:math id="inf46"><mml:mi>ρ</mml:mi></mml:math></inline-formula> denotes the Huber loss. <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> is known as bundle-adjustment (<xref ref-type="bibr" rid="bib16">Hartley and Zisserman, 2000</xref>). Huber loss is defined as<disp-formula id="equ8"><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext> for </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>δ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext> otherwise </mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Replacing the squared loss by the Huber loss makes our approach more robust to erroneous detections <inline-formula><mml:math id="inf47"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We empirically set <inline-formula><mml:math id="inf48"><mml:mi>δ</mml:mi></mml:math></inline-formula> to 20 pixels. Note that we perform this minimization with respect to ten degrees-of-freedom per camera: three translations, three rotations, and four distortions.</p><p>For this optimization to work properly, we need to initialize these 10 parameters and we need to reduce the number of outliers. To achieve this, the initial distortion parameters are set to zero. We also produce initial estimates for the three rotation and three translation parameters by measuring the distances between adjacent cameras and their relative orientations. To initialize the rotation and translation vectors, we measure the distance and the angle between adjacent cameras, from which we infer rough initial estimates. Finally, we rely on epipolar geometry (<xref ref-type="bibr" rid="bib16">Hartley and Zisserman, 2000</xref>) to automate outlier rejection. Because the cameras form a rough circle and look inward, the epipolar lines are close to being horizontal <xref ref-type="fig" rid="fig8">Figure 8A</xref>. Thus, corresponding 2D projections must belong to the same image rows, or at most a few pixels higher or lower. In practice, this means checking if all 2D predictions lie in nearly the same rows and discarding a priori those that do not.</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.016</object-id><label>Figure 8.</label><caption><title>Camera calibration.</title><p>(<bold>A</bold>) Correcting erroneous 2D pose estimations by using epipolar relationships. Only 2D pose estimates without large epipolar errors are used for calibration. <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> represents a 2D pose estimate from the middle camera. Epipolar lines are indicated as blue and red lines on the image plane. (<bold>B</bold>) The triangulated point, <inline-formula><mml:math id="inf50"><mml:msub><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula>, uses the initial camera parameters. However, due to the coarse initialization of each camera’s extrinsic properties, observations from each camera do not agree with one another and do not yield a reasonable 3D position estimate. (<bold>C</bold>) The camera locations are corrected, generating an accurate 3D position estimate by optimizing <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> using only the pruned 2D points.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig8-v2.tif"/></fig></sec></sec><sec id="s4-3"><title>3D pose correction</title><p>The triangulation procedure described above can produce erroneous results when the 2D estimates of landmarks are wrong. Additionally, it may result in implausible 3D poses for the entire animal because it treats each joint independently. To enforce more global geometric constraints, we rely on pictorial structures (<xref ref-type="bibr" rid="bib13">Felzenszwalb and Huttenlocher, 2005</xref>) as described in <xref ref-type="fig" rid="fig9">Figure 9</xref>. Pictorial structures encode the relationship between a set of variables (in this case the 3D location of separate tracked points) in a probabilistic setting using a graphical model. This makes it possible to consider multiple 2D locations <inline-formula><mml:math id="inf51"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each landmark <inline-formula><mml:math id="inf52"><mml:msub><mml:mi mathvariant="bold">𝐗</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> instead of only one. This increases the likelihood of finding the true 3D pose.</p><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.017</object-id><label>Figure 9.</label><caption><title>3D pose correction for one leg using the MAP solution and pictorial structures.</title><p>(<bold>A</bold>) Candidate 3D pose estimates for each keypoint are created by triangulating local maxima from probability maps generated by the Stacked Hourglass deep network. (<bold>B</bold>) For a selection of these candidate estimates, we can assign a probability using <xref ref-type="disp-formula" rid="equ9">Equation 8</xref>. However, calculating this probability for each pair of points is computationally intractable. (<bold>C</bold>) By exploiting the chain structure of <xref ref-type="disp-formula" rid="equ9">Equation 8</xref>, we can instead pass a probability distribution across layers using a belief propagation algorithm. Messages are passed between layers as a function of parent nodes, describing the belief of the child nodes on each parent node. Grayscale colors represent the calculated belief of each node where darker colors indicate higher belief. (<bold>D</bold>) Corrected pose estimates are obtained during the second backward iteration, by selecting the nodes with largest belief. We discard nodes (x’s) that have non-maximal belief during backwards message passing. Note that beliefs have been adjusted after forward message passing.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig9-v2.tif"/></fig><sec id="s4-3-1"><title>Generating multiple candidates</title><p>Instead of selecting landmarks as the locations with the maximum probability in maps output by our Stacked Hourglass network, we generate multiple candidate 2D landmark locations <inline-formula><mml:math id="inf53"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. From each probability map, we select 10 local probability maxima that are at least one pixel apart from one another. Then, we generate 3D candidates by triangulating 2D candidates in every tuple of cameras. Because a single point is visible from at most four cameras, this results in at most <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> candidates for each tracked point.</p></sec><sec id="s4-3-2"><title>Choosing the best candidates</title><p>To identify the best subset of resulting 3D locations, we introduce the probability distribution <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that assigns a probability to each solution <inline-formula><mml:math id="inf56"><mml:mi>L</mml:mi></mml:math></inline-formula>, consisting of 38 sets of 2D points observed from each camera. Our goal is then to find the most likely one. More formally, <inline-formula><mml:math id="inf57"><mml:mi>P</mml:mi></mml:math></inline-formula> represents the likelihood of a set of tracked points <inline-formula><mml:math id="inf58"><mml:mi>L</mml:mi></mml:math></inline-formula>, given the images, model parameters, camera calibration, and geometric constraints. In our formulation, <inline-formula><mml:math id="inf59"><mml:mi>I</mml:mi></mml:math></inline-formula> denotes the seven camera images <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo>≤</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf61"><mml:mi>θ</mml:mi></mml:math></inline-formula> represents the set of projection functions <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>π</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> for camera <inline-formula><mml:math id="inf63"><mml:mi>c</mml:mi></mml:math></inline-formula> along with a set of length distributions <inline-formula><mml:math id="inf64"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between each pair of points <inline-formula><mml:math id="inf65"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf66"><mml:mi>j</mml:mi></mml:math></inline-formula> that are connected by a limb. <inline-formula><mml:math id="inf67"><mml:mi>L</mml:mi></mml:math></inline-formula> consists of a set of tracked points <inline-formula><mml:math id="inf68"><mml:msub><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where each <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> describes a set of 2D observations <inline-formula><mml:math id="inf70"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from multiple camera views. These are used to triangulate the corresponding 3D point locations <inline-formula><mml:math id="inf71"><mml:mover accent="true"><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>. If the set of 2D observations is incomplete, as some points are totally occluded in some camera views, we triangulate the 3D point <inline-formula><mml:math id="inf72"><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> using the available ones and replace the missing observations by projecting the recovered 3D positions into the images, <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>. In the end, we aim to find the solution <inline-formula><mml:math id="inf74"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="false">argmax</mml:mo><mml:mi>L</mml:mi></mml:munder><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> This is known as Maximum a Posteriori (MAP) estimation. Using Bayes rule, we write<disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the two terms can be computed separately. We compute <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>J</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> using the probability maps <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> generated by the Stacked Hourglass network for the tracked point <inline-formula><mml:math id="inf77"><mml:mi>j</mml:mi></mml:math></inline-formula> for camera <inline-formula><mml:math id="inf78"><mml:mi>c</mml:mi></mml:math></inline-formula>. For a single joint <inline-formula><mml:math id="inf79"><mml:mi>j</mml:mi></mml:math></inline-formula> seen by camera <inline-formula><mml:math id="inf80"><mml:mi>c</mml:mi></mml:math></inline-formula>, we model the likelihood of observing that particular point using <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which can be directly read from the probability maps as the pixel intensity. Ignoring the dependency between the cameras, we write the overall likelihood as the product of the individual likelihood terms<disp-formula id="equ10"><label>(9)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>7</mml:mn></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which can be read directly from the probability maps as pixel intensities and represent the network’s confidence that a particular keypoint is located at a particular pixel. When a point is not visible from a particular camera, we assume the probability map only contains a constant non-zero probability, which does not affect the final solution. We express <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as<disp-formula id="equ11"><label>(10)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>7</mml:mn></mml:munderover><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mpadded width="+2.8pt"><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where pairwise dependencies <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> between two variables respect the segment length constraint when the variables are connected by a limb. The length of segments defined by pairs of connected 3D points follows a normal distribution. Specifically, we model <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as <inline-formula><mml:math id="inf85"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. We model the reprojection error for a particular point <inline-formula><mml:math id="inf86"><mml:mi>j</mml:mi></mml:math></inline-formula> as <inline-formula><mml:math id="inf87"><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>7</mml:mn></mml:msubsup><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> which is set to zero using the variable <inline-formula><mml:math id="inf88"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denoting the visibility of the point <inline-formula><mml:math id="inf89"><mml:mi>j</mml:mi></mml:math></inline-formula> from camera <inline-formula><mml:math id="inf90"><mml:mi>c</mml:mi></mml:math></inline-formula>. If a 2D observation for a particular camera is manually set by a user with the DeepFly3D GUI, we take it to be the only possible candidate for that particular image and we set <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to 1, where <inline-formula><mml:math id="inf92"><mml:mi>j</mml:mi></mml:math></inline-formula> denotes the manually assigned pixel location.</p></sec><sec id="s4-3-3"><title>Solving the MAP problem using the Max-Sum algorithm</title><p>For general graphs, MAP estimation with pairwise dependencies is NP-hard and therefore intractable. However, in the specific case of non-cyclical graphs, it is possible to solve the inference problem using belief propagation (<xref ref-type="bibr" rid="bib5">Bishop, 2006</xref>). Since the fly’s skeleton has a root and contains no loops, we can use a message passing approach (<xref ref-type="bibr" rid="bib13">Felzenszwalb and Huttenlocher, 2005</xref>). It is closely related to Viterbi recurrence and propagates the unary probabilities <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> between the edges of the graph starting from the root and ending at the leaf nodes. This first propagation ends with the computation of the marginal distribution for the leaf node variables. During the subsequent backward iteration, as <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for leaf node is computed, the point <inline-formula><mml:math id="inf95"><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> with maximum posterior probability is selected in <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> time, where <inline-formula><mml:math id="inf97"><mml:mi>k</mml:mi></mml:math></inline-formula> is the upper bound on the number of proposals for a single tracked point. Next, the distribution <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated, adjacent nodes for the leaf node. Continuing this process on all the remaining points results in a MAP solution for the overall distribution <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in <xref ref-type="fig" rid="fig9">Figure 9</xref>, with overall <inline-formula><mml:math id="inf100"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> computational complexity.</p></sec><sec id="s4-3-4"><title>Learning the parameters</title><p>We learn the parameters for the set of pairwise distributions <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> using a maximum likelihood process and assuming the distributions to be Gaussian. We model the segment length <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as the euclidean distance between the points <inline-formula><mml:math id="inf103"><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf104"><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>. We then solve for <inline-formula><mml:math id="inf105"><mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="false">argmax</mml:mo><mml:mi>S</mml:mi></mml:munder><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, assuming segments have a Gaussian distribution resulting from the Gaussian noise in point observations <inline-formula><mml:math id="inf106"><mml:mi>L</mml:mi></mml:math></inline-formula>. This gives us the mean and variance, defining each distribution <inline-formula><mml:math id="inf107"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We exclude the same points that we removed from the calibration procedure, that exhibit high reprojection error.</p><p>In practice, we observe a large variance for pretarsus values (<xref ref-type="fig" rid="fig10">Figure 10</xref>). This is because occlusions occasionally shorten visible tarsal segments. To eliminate the resulting bias, we treat these limbs differently from the others and model the distribution of tibia-tarsus and tarsus-tip points as a Beta distribution, with parameters found using a similar Maximum Likelihood Estimator (MLE) formulation. Assuming the observation errors to be Gaussian and zero-centered, the bundle adjustment procedure can also be understood as an MLE of the calibration parameters (<xref ref-type="bibr" rid="bib49">Triggs et al., 2000</xref>). Therefore, the entire set of parameters for the formulation can be learned using MLE. Thus, prior information about potentially occluded targets can be used to guide inference. For example, in a head-fixed rodent, the left eye may not always be visible from the right-side of the animal. This information can be incorporated into DeepFly3D’s inference system in the file, <italic>skeleton.py</italic>, by editing the function <italic>camera_see_joint</italic>. Afterwards, predictions from occluded cameras will not be used to triangulate a given 3D point. If no such information is provided, every prediction will be used to triangulate a given 3D point.</p><fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.019</object-id><label>Figure 10.</label><caption><title>Pose correction using pictorial structures.</title><p>(<bold>A</bold>) Raw input data from four cameras, focusing on the pretarsus of the middle left leg. (<bold>B</bold>) Probability maps for the pretarsus output from the Stacked Hourglass deep network. Two maxima (white arrowheads) are present on the probability maps for camera 5. The false-positive has a larger unary probability. (<bold>C</bold>) Raw predictions of 2D pose estimation without using pictorial structures. The pretarsus label is incorrectly applied (white arrowhead) in camera 5. By contrast, cameras 4, 6, and 7 are correctly labeled. (<bold>D</bold>) Corrected pose estimation using pictorial structures. The false-positive is removed due to the high error measured in <xref ref-type="disp-formula" rid="equ9">Equation 8</xref>. The newly corrected pretarsus label for camera five is shown (white arrowhead).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig10-v2.tif"/></fig><p>The pictorial structure formulation can be further expanded using temporal information, penalizing large movements of a single tracked point between two consecutive frames. However, we abstained from using temporal information more extensively for several reasons. First, temporal dependencies would introduce loops in our pictorial structures, thus making exact inference NP-hard as discussed above. This can be handled using loopy belief propagation algorithms (<xref ref-type="bibr" rid="bib31">Murphy et al., 1999</xref>) but requires multiple message passing rounds, which prevents real-time inference without any theoretical guarantee of optimal inference. Second, the rapidity of <italic>Drosophila</italic> limb movements makes it hard to assign temporal constraints, even with fast video recording. Finally, we empirically observed that the current formulation, enforcing structured poses in a single temporal frame, already eliminates an overwhelming majority of false-positives inferred during the pose estimation stage of the algorithm.</p></sec><sec id="s4-3-5"><title>Modifying DeepFly3D to study other animals</title><p>DeepFly3D does not assume a circular camera arrangement or that there is one degree of freedom in the camera network. Therefore, it could easily be adapted for 3D pose estimation in other animals, ranging from rodents to primates and humans. We illustrate this flexibility by using DeepFly3D to capture human 3D pose in the Human 3.6M Dataset (<ext-link ext-link-type="uri" xlink:href="http://vision.imar.ro/human3.6m/description.php">http://vision.imar.ro/human3.6m/description.php</ext-link>) very popular, publicly available computer vision benchmarking dataset generated using four synchronized cameras (<xref ref-type="bibr" rid="bib19">Ionescu et al., 2014</xref>; <xref ref-type="bibr" rid="bib18">Ionescu et al., 2011</xref>) (<xref ref-type="fig" rid="fig11">Figure 11</xref>).</p><fig id="fig11" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.020</object-id><label>Figure 11.</label><caption><title>DeepFly3D graphical user interface (GUI) applied to with the Human3.6M dataset (<xref ref-type="bibr" rid="bib19">Ionescu et al., 2014</xref>).</title><p>To use the DeepFly3D GUI on any new dataset (<italic>Drosophila</italic> or otherwise), users can provide an initial small set of manual annotations. Using these annotations, the software calculates the epipolar geometry, performs camera calibration, and trains the 2D pose estimation deep network. A description of how to adopt DeepFly3D for new datasets can be found in the Materials and methods section and, in greater detail, online: <ext-link ext-link-type="uri" xlink:href="https://github.com/NeLy-EPFL/DeepFly3D">https://github.com/NeLy-EPFL/DeepFly3D</ext-link>. This figure is licensed for academic use only and thus is not available under CC-BY and is exempt from the CC-BY 4.0 license.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig11-v2.tif"/></fig><p>Generally, for any new dataset, the user first needs to provide an initial set of manual annotations. The user would describe the number of tracked points and their relationships to one another in a python setup file. Then, in a configuration file, the user specifies the number of cameras along with the resolutions of input images and output probability maps. DeepFly3D will then use these initial manual annotations to (i) train the 2D Stacked Hourglass network, (ii) perform camera calibration without an external calibration pattern, (iii) learn the epipolar geometry to perform outlier detection, and (iv) learn the segment length distributions <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. After this initial bootstrapping, DeepFly3D can be then used with pictorial structures and active learning to iteratively improve pose estimation accuracy.</p><p>The initial manual annotations can be performed using the DeepFly3D Annotation GUI. Afterwards, these annotations can be downloaded from the Annotation GUI as a CSV file using the <italic>Save</italic> button (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Once the CSV file is placed in the images folder, DeepFly3D will automatically read and display the annotations. To train the Stacked Hourglass network, use the <italic>csv-path</italic> flag while running <italic>pose2d.py</italic> (found in <italic>deepfly/pose2d/</italic>). DeepFly3D will then train the Stacked Hourglass network by performing transfer learning using the large MPII dataset and the smaller set of user manual annotations.</p><p>To perform camera calibration, the user should select the <italic>Calibration</italic> button on the GUI <xref ref-type="fig" rid="fig12">Figure 12</xref>. DeepFly3D will then perform bundle adjustment (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>) and save the camera parameters in <italic>calibration.pickle</italic> (found in the images folder). The path of this file should then be added to <italic>Config.py</italic> to initialize calibration. These initial calibration parameters will then be used in further experiments for fast and accurate convergence. If the number of annotations is insufficient for accurate calibration, or if bundle adjustment is converging too slowly, an initial rough estimate of the camera locations can be set in <italic>Config.py</italic>. As long as a calibration is set in <italic>Config.py</italic>, DeepFly3D will use it as a projection matrix to calculate the epipolar geometry between cameras. This step is necessary to perform outlier detection on further calibration operations.</p><fig id="fig12" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.018</object-id><label>Figure 12.</label><caption><title>DeepFly3D graphical user interface (GUI).</title><p>The top-left buttons enable operations like 2D pose estimation, camera calibration, and saving the final results. The top-right buttons can be used to visualize the data in different ways: as raw images, probability maps, 2D pose, or the corrected pose following pictorial structures. The bottom-left buttons permit frame-by-frame navigation. A full description of the GUI can be found in the online documentation: <ext-link ext-link-type="uri" xlink:href="https://github.com/NeLy-EPFL/DeepFly3D">https://github.com/NeLy-EPFL/DeepFly3D</ext-link>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig12-v2.tif"/></fig><p>DeepFly3D will also learn the distribution <inline-formula><mml:math id="inf109"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, whose non-zero entries are found in <italic>skeleton.py</italic>. One can easily calculate these segment length distribution parameters using the functions provided with DeepFly3D. <italic>CameraNetwork</italic> class (found under <italic>deepfly/GUI/</italic>), will then automatically load the points and calibration parameters from the images folder. The function <italic>CameraNetwork.triangulate</italic> will convert 2D annotation points into 3D points using the calibration parameters. The <inline-formula><mml:math id="inf110"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> parameters can then be saved using the <italic>pickle</italic> library (the save path can be set in <italic>Config.py</italic>). The <italic>calcBoneParams</italic> method will then output the segment lengths’ mean and variance. These values will then be used with pictorial structures (<xref ref-type="disp-formula" rid="equ9">Equation 8</xref>).</p><p>We provide further technical details for how to adapt DeepFly3D to other multi-view datasets online (<ext-link ext-link-type="uri" xlink:href="https://github.com/NeLy-EPFL/DeepFly3D">https://github.com/NeLy-EPFL/DeepFly3D</ext-link> [<xref ref-type="bibr" rid="bib14">Günel et al., 2019</xref>] copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/DeepFly3D">https://github.com/elifesciences-publications/DeepFly3D</ext-link>).</p></sec></sec><sec id="s4-4"><title>Experimental setup</title><p>We positioned seven Basler acA1920-155um cameras (FUJIFILM AG, Niederhaslistrasse, Switzerland) 94 mm away from the tethered fly, resulting in a circular camera network with the animal in the center (<xref ref-type="fig" rid="fig13">Figure 13</xref>). We acquired 960 × 480 pixel video data at 100 FPS under 850 nm infrared ring light illumination (Stemmer Imaging, Pfäffikon Switzerland). Cameras were mounted with 94 mm W.D./1.00 x InfiniStix lenses (Infinity Photo-Optical GmbH, Göttingen). Optogenetic stimulation LED light was filtered out using 700 nm longpass optical filters (Edmund Optics, York UK). Each camera’s depth of field was increased using 5.8 mm aperture retainers (Infinity Photo-Optical GmbH). To automate the timing of optogenetic LED stimulation and camera acquisition triggering, we use an Arduino (Arduino, Sommerville, MA) and custom software written using the Basler camera API.</p><fig id="fig13" position="float"><object-id pub-id-type="doi">10.7554/eLife.48571.021</object-id><label>Figure 13.</label><caption><title>A schematic of the seven camera spherical treadmill and optogenetic stimulation system that was used in this study.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-fig13-v2.tif"/></fig><p>We assessed the optimal number of cameras for DeepFly3D and concluded that increasing the number of cameras increases accuracy by stabilizing triangulation. Specifically, we observed the following. (i) Calibration is not a significant source of error: calibrating with fewer than seven cameras does not dramatically increase estimation error. (ii) Having more cameras improves triangulation. Reducing the number of cameras down to four, even having calibrated with seven cameras, results in an increase of 0.05 mm triangulation error. This may be because the camera views are sufficiently different, having largely non-overlapping 2D-detection failure cases. Thus, the redundancy provided by having more cameras mitigates detection errors by finding a 3D pose that is consistent across at least two camera views.</p><sec id="s4-4-1"><title><italic>Drosophila</italic> transgenic lines</title><p><italic>UAS-CsChrimson</italic> (<xref ref-type="bibr" rid="bib22">Klapoetke et al., 2014</xref>) animals were obtained from the Bloomington Stock Center (Stock <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mo>⁢</mml:mo><mml:mn>55135</mml:mn></mml:mrow></mml:math></inline-formula>). <italic>MDN-1-Gal4</italic> (<xref ref-type="bibr" rid="bib4">Bidaye et al., 2014</xref>) (<italic>VT44845-DBD; VT50660-AD</italic>) was provided by B. Dickson (Janelia Research Campus, Ashburn). <italic>aDN-Gal4</italic> (<xref ref-type="bibr" rid="bib15">Hampel et al., 2015</xref>)(<italic>R76F12-AD; R18C11-DBD</italic>), was provided by J. Simpson (University of California, Santa Barbara). Wild-type, <italic>PR</italic> animals were provided by M. Dickinson (California Institute of Technology, Pasadena).</p></sec><sec id="s4-4-2"><title>Optogenetic stimulation experiments</title><p>Experiments were performed in the late morning or early afternoon Zeitgeber time (Z.T.), inside a dark imaging chamber. An adult female animal 2–3 days-post-eclosion (dpe), was mounted onto a custom stage (<xref ref-type="bibr" rid="bib9">Chen et al., 2018</xref>) and allowed to acclimate for 5 min on an air-supported spherical treadmill (<xref ref-type="bibr" rid="bib9">Chen et al., 2018</xref>). Optogenetic stimulation was performed using a 617 nm LED (Thorlabs, Newton, NJ) pointed at the dorsal thorax through a hole in the stage, and focused with a lens (LA1951, 01&quot; f = 25.4 mm, Thorlabs, Newton, NJ). Tethered flies were otherwise allowed to behave spontaneously. Data were acquired in 9 s epochs: 2 s baseline, 5 s with optogenetic illumination, and 2 s without stimulation. Individual flies were recorded for five trials each, with one-minute intervals. Data were excluded from analysis if flies pushed their abdomens onto the spherical treadmill—interfering with limb movements—or if flies struggled during optogenetic stimulation, pushing their forelimbs onto the stage for prolonged periods of time.</p></sec></sec><sec id="s4-5"><title>Unsupervised behavioral classification</title><p>To create unsupervised embeddings of behavioral data, we mostly followed the approach taken by <xref ref-type="bibr" rid="bib47">Todd et al. (2017)</xref> and <xref ref-type="bibr" rid="bib3">Berman et al. (2014)</xref>. We smoothed 3D pose traces using a 1€ filter. Then we converted them into angles to achieve scale and translational invariance (<xref ref-type="bibr" rid="bib7">Casiez et al., 2012</xref>). Angles were calculated by taking the dot product from sets of three connected 3D positions. For the antennae, we calculated the angle of the line defined by two antennal points with respect to the ground-plane. This way, we generated four angles per leg (two body-coxa, one coxa-femur, and one femur-tibia), two angles for the abdomen (top and bottom abdominal stripes), and a single angle for the antennae (head tilt with respect to the axis of gravity). In total, we obtained a set of 20 angles, extracted from 38 3D points.</p><p>We transformed angular time series using a Continous Wavelet Transform (CWT) to create a posture-dynamics space. We used the Morlet Wavelet as the mother wavelet, given its suitability to isolate periodic chirps of motion. We chose 25 wavelet scales to match dyadically spaced center frequencies between 5 Hz and 50 Hz. Then, we calculatd spectrograms for each postural time-series by taking the magnitudes of the wavelet coefficients. This yields a 20 × 25 = 500-dimensional time-series, which was then normalized over all frequency channels to unit length, at each time instance. Then, we could treat each feature vector from each time instance as a distribution over all frequency channels.</p><p>Later, from the posture-dynamics space, we computed a two-dimensional representation of behavior by using the non-linear embedding algorithm, t-SNE (<xref ref-type="bibr" rid="bib23">Maaten, 2008</xref>). t-SNE embedded our high-dimensional posture-dynamics space onto a 2D plane, while preserving the high-dimensional local structure, while sacrificing larger scale accuracy. We used the Kullback–Leibler (KL) divergence as the distance function in our t-SNE algorithm. KL assesses the difference between the shapes of two distributions, justifying the normalization step in the preceding step. By analyzing a multitude of plots generated with different perplexity values, we empirically found a perplexity value of 35 to best suit the features of our posture-dynamics space.</p><p>From this generated discrete space, we created a continuous 2D distribution, that we could then segment into behavioral clusters. We started by normalizing the 2D t-SNE projected space into a 1000 × 1000 matrix. Then, we applied a 2D Gaussian convolution with a kernel of size <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 10 px. Finally, we segmented this space by inverting it and applying a Watershed algorithm that separated adjacent basins, yielding a behavioral map.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Celine Magrini and Fanny Magaud for image annotation assistance, Raphael Laporte and Victor Lobato Ríos for helping to develop camera acquisition software.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Formal analysis, Supervision, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Investigation, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Software, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Project administration, Writing—review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.48571.022</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-48571-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated and analyzed during this study are included in the DeepFly3D GitHub site: <ext-link ext-link-type="uri" xlink:href="https://github.com/NeLy-EPFL/DeepFly3D">https://github.com/NeLy-EPFL/DeepFly3D</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/DeepFly3D">https://github.com/elifesciences-publications/DeepFly3D</ext-link>) and in the Harvard Dataverse.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Gunel</surname><given-names>S</given-names></name><name><surname>Rhodin</surname><given-names>H</given-names></name><name><surname>Morales</surname><given-names>D</given-names></name><name><surname>Campagnolo</surname><given-names>J</given-names></name><name><surname>Ramdya</surname><given-names>P</given-names></name><name><surname>Fua</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>aDN-GAL4 Control</data-title><source>Harvard Dataverse</source><pub-id assigning-authority="other" pub-id-type="doi">10.7910/DVN/PKKXOE</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Gunel</surname><given-names>S</given-names></name><name><surname>Rhodin</surname><given-names>H</given-names></name><name><surname>Morales</surname><given-names>D</given-names></name><name><surname>Campagnolo</surname><given-names>J</given-names></name><name><surname>Ramdya</surname><given-names>P</given-names></name><name><surname>Fua</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>MDN-GAL4 Control</data-title><source>Harvard Dataverse</source><pub-id assigning-authority="other" pub-id-type="doi">10.7910/DVN/HOLXOR</pub-id></element-citation></p><p><element-citation id="dataset3" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Gunel</surname><given-names>S</given-names></name><name><surname>Rhodin</surname><given-names>H</given-names></name><name><surname>Morales</surname><given-names>D</given-names></name><name><surname>Campagnolo</surname><given-names>J</given-names></name><name><surname>Ramdya</surname><given-names>P</given-names></name><name><surname>Fua</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>aDN-GAL4 UAS-CsChrimson</data-title><source>Harvard Dataverse</source><pub-id assigning-authority="other" pub-id-type="doi">10.7910/DVN/S4L4KX</pub-id></element-citation></p><p><element-citation id="dataset4" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Gunel</surname><given-names>S</given-names></name><name><surname>Rhodin</surname><given-names>H</given-names></name><name><surname>Morales</surname><given-names>D</given-names></name><name><surname>Campagnolo</surname><given-names>J</given-names></name><name><surname>Ramdya</surname><given-names>P</given-names></name><name><surname>Fua</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>MDN-GAL4 UAS-CsChrimson</data-title><source>Harvard Dataverse</source><pub-id assigning-authority="other" pub-id-type="doi">10.7910/DVN/8SUC9U</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Andriluka</surname> <given-names>M</given-names></name><name><surname>Pishchulin</surname> <given-names>L</given-names></name><name><surname>Gehler</surname> <given-names>P</given-names></name><name><surname>Schiele</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>2d human pose estimation: new benchmark and state of the art analysis</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>3686</fpage><lpage>3693</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2014.471</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bender</surname> <given-names>JA</given-names></name><name><surname>Simpson</surname> <given-names>EM</given-names></name><name><surname>Ritzmann</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Computer-assisted 3D kinematic analysis of all leg joints in walking insects</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e13617</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0013617</pub-id><pub-id pub-id-type="pmid">21049024</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname> <given-names>GJ</given-names></name><name><surname>Choi</surname> <given-names>DM</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mapping the stereotyped behaviour of freely moving fruit flies</article-title><source>Journal of the Royal Society Interface</source><volume>11</volume><elocation-id>20140672</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2014.0672</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bidaye</surname> <given-names>SS</given-names></name><name><surname>Machacek</surname> <given-names>C</given-names></name><name><surname>Wu</surname> <given-names>Y</given-names></name><name><surname>Dickson</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neuronal control of <italic>Drosophila</italic> walking direction</article-title><source>Science</source><volume>344</volume><fpage>97</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1126/science.1249964</pub-id><pub-id pub-id-type="pmid">24700860</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Pattern Recognition and Machine Learning</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cande</surname> <given-names>J</given-names></name><name><surname>Namiki</surname> <given-names>S</given-names></name><name><surname>Qiu</surname> <given-names>J</given-names></name><name><surname>Korff</surname> <given-names>W</given-names></name><name><surname>Card</surname> <given-names>GM</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Stern</surname> <given-names>DL</given-names></name><name><surname>Berman</surname> <given-names>GJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Optogenetic dissection of descending behavioral control in <italic>Drosophila</italic></article-title><source>eLife</source><volume>7</volume><elocation-id>e34275</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34275</pub-id><pub-id pub-id-type="pmid">29943729</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Casiez</surname> <given-names>G</given-names></name><name><surname>Roussel</surname> <given-names>N</given-names></name><name><surname>Vogel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>1€ filter: a simple speed-based low-pass filter for noisy input in interactive systems</article-title><conf-name>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ACM</conf-name><fpage>2527</fpage><lpage>2530</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chavdarova</surname> <given-names>T</given-names></name><name><surname>Baqué</surname> <given-names>P</given-names></name><name><surname>Bouquet</surname> <given-names>S</given-names></name><name><surname>Maksai</surname> <given-names>A</given-names></name><name><surname>Jose</surname> <given-names>C</given-names></name><name><surname>Lettry</surname> <given-names>L</given-names></name><name><surname>Fua</surname> <given-names>P</given-names></name><name><surname>Gool</surname> <given-names>LV</given-names></name><name><surname>Fleuret</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title> WILDTRACK: A Multi-Camera HD Dataset for Dense Unscripted Pedestrian Detection</article-title><conf-name>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>5030</fpage><lpage>5039</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>CL</given-names></name><name><surname>Hermans</surname> <given-names>L</given-names></name><name><surname>Viswanathan</surname> <given-names>MC</given-names></name><name><surname>Fortun</surname> <given-names>D</given-names></name><name><surname>Aymanns</surname> <given-names>F</given-names></name><name><surname>Unser</surname> <given-names>M</given-names></name><name><surname>Cammarato</surname> <given-names>A</given-names></name><name><surname>Dickinson</surname> <given-names>MH</given-names></name><name><surname>Ramdya</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Imaging neural activity in the ventral nerve cord of behaving adult <italic>Drosophila</italic></article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4390</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06857-z</pub-id><pub-id pub-id-type="pmid">30348941</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dombeck</surname> <given-names>DA</given-names></name><name><surname>Khabbaz</surname> <given-names>AN</given-names></name><name><surname>Collman</surname> <given-names>F</given-names></name><name><surname>Adelman</surname> <given-names>TL</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Imaging large-scale neural activity with cellular resolution in awake, mobile mice</article-title><source>Neuron</source><volume>56</volume><fpage>43</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.08.003</pub-id><pub-id pub-id-type="pmid">17920014</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Elhayek</surname> <given-names>A</given-names></name><name><surname>Aguiar</surname> <given-names>E</given-names></name><name><surname>Jain</surname> <given-names>A</given-names></name><name><surname>Tompson</surname> <given-names>J</given-names></name><name><surname>Pishchulin</surname> <given-names>L</given-names></name><name><surname>Andriluka</surname> <given-names>M</given-names></name><name><surname>Bregler</surname> <given-names>C</given-names></name><name><surname>Schiele</surname> <given-names>B</given-names></name><name><surname>Theobalt</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Efficient Convnet-Based Marker-Less motion capture in general scenes with a low number of cameras</article-title><conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2015.7299005</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feany</surname> <given-names>MB</given-names></name><name><surname>Bender</surname> <given-names>WW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A <italic>Drosophila</italic> model of Parkinson's disease</article-title><source>Nature</source><volume>404</volume><fpage>394</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1038/35006074</pub-id><pub-id pub-id-type="pmid">10746727</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felzenszwalb</surname> <given-names>PF</given-names></name><name><surname>Huttenlocher</surname> <given-names>DP</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Pictorial structures for object recognition</article-title><source>International Journal of Computer Vision</source><volume>61</volume><fpage>55</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000042934.15159.49</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Günel</surname> <given-names>S</given-names></name><name><surname>Harbulot</surname> <given-names>J</given-names></name><name><surname>Ramdya</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>DeepFly3D</data-title><source>GitHub</source><version designator="f185c48">f185c48</version><ext-link ext-link-type="uri" xlink:href="https://github.com/NeLy-EPFL/DeepFly3D">https://github.com/NeLy-EPFL/DeepFly3D</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hampel</surname> <given-names>S</given-names></name><name><surname>Franconville</surname> <given-names>R</given-names></name><name><surname>Simpson</surname> <given-names>JH</given-names></name><name><surname>Seeds</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural command circuit for grooming movement control</article-title><source>eLife</source><volume>4</volume><elocation-id>e08758</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.08758</pub-id><pub-id pub-id-type="pmid">26344548</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hartley</surname> <given-names>R</given-names></name><name><surname>Zisserman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Multiple View Geometry in Computer Vision</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hewitt</surname> <given-names>VL</given-names></name><name><surname>Whitworth</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mechanisms of Parkinson's Disease: Lessons from <italic>Drosophila</italic></article-title><source>Current Topics in Developmental Biology</source><volume>121</volume><fpage>173</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/bs.ctdb.2016.07.005</pub-id><pub-id pub-id-type="pmid">28057299</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ionescu</surname> <given-names>C</given-names></name><name><surname>Li</surname> <given-names>F</given-names></name><name><surname>Sminchisescu</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Latent structured models for human pose estimation</article-title><conf-name>2011 International Conference on Computer Vision IEEE</conf-name><fpage>2220</fpage><lpage>2227</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2011.6126500</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ionescu</surname> <given-names>C</given-names></name><name><surname>Papava</surname> <given-names>D</given-names></name><name><surname>Olaru</surname> <given-names>V</given-names></name><name><surname>Sminchisescu</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>36</volume><fpage>1325</fpage><lpage>1339</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2013.248</pub-id><pub-id pub-id-type="pmid">26353306</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isakov</surname> <given-names>A</given-names></name><name><surname>Buchanan</surname> <given-names>SM</given-names></name><name><surname>Sullivan</surname> <given-names>B</given-names></name><name><surname>Ramachandran</surname> <given-names>A</given-names></name><name><surname>Chapman</surname> <given-names>JK</given-names></name><name><surname>Lu</surname> <given-names>ES</given-names></name><name><surname>Mahadevan</surname> <given-names>L</given-names></name><name><surname>de Bivort</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Recovery of locomotion after injury in <italic>Drosophila Melanogaster</italic> depends on proprioception</article-title><source>The Journal of Experimental Biology</source><volume>219</volume><fpage>1760</fpage><lpage>1771</lpage><pub-id pub-id-type="doi">10.1242/jeb.133652</pub-id><pub-id pub-id-type="pmid">26994176</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kain</surname> <given-names>J</given-names></name><name><surname>Stokes</surname> <given-names>C</given-names></name><name><surname>Gaudry</surname> <given-names>Q</given-names></name><name><surname>Song</surname> <given-names>X</given-names></name><name><surname>Foley</surname> <given-names>J</given-names></name><name><surname>Wilson</surname> <given-names>R</given-names></name><name><surname>De Bivort</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1910">1910</year><article-title>Leg-tracking and automated behavioural classification in <italic>Drosophila</italic></article-title><source>Nature Communications</source><volume>2013</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms2908</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klapoetke</surname> <given-names>NC</given-names></name><name><surname>Murata</surname> <given-names>Y</given-names></name><name><surname>Kim</surname> <given-names>SS</given-names></name><name><surname>Pulver</surname> <given-names>SR</given-names></name><name><surname>Birdsey-Benson</surname> <given-names>A</given-names></name><name><surname>Cho</surname> <given-names>YK</given-names></name><name><surname>Morimoto</surname> <given-names>TK</given-names></name><name><surname>Chuong</surname> <given-names>AS</given-names></name><name><surname>Carpenter</surname> <given-names>EJ</given-names></name><name><surname>Tian</surname> <given-names>Z</given-names></name><name><surname>Wang</surname> <given-names>J</given-names></name><name><surname>Xie</surname> <given-names>Y</given-names></name><name><surname>Yan</surname> <given-names>Z</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Chow</surname> <given-names>BY</given-names></name><name><surname>Surek</surname> <given-names>B</given-names></name><name><surname>Melkonian</surname> <given-names>M</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name><name><surname>Constantine-Paton</surname> <given-names>M</given-names></name><name><surname>Wong</surname> <given-names>GK-S</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Independent optical excitation of distinct neural populations</article-title><source>Nature Methods</source><volume>11</volume><fpage>338</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2836</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maaten</surname> <given-names>LJP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title> Visualizing High Dimensional Data Using t-SNE</article-title><source>Journal of Machine Learning Research</source><fpage>2579</fpage><lpage>2605</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Martinez</surname> <given-names>J</given-names></name><name><surname>Hossain</surname> <given-names>R</given-names></name><name><surname>Romero</surname> <given-names>J</given-names></name><name><surname>Little</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A simple yet effective baseline for 3D human pose estimation</article-title><conf-name>ICCV</conf-name></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Mamidanna</surname> <given-names>P</given-names></name><name><surname>Cury</surname> <given-names>KM</given-names></name><name><surname>Abe</surname> <given-names>T</given-names></name><name><surname>Murthy</surname> <given-names>VN</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKellar</surname> <given-names>CE</given-names></name><name><surname>Lillvis</surname> <given-names>JL</given-names></name><name><surname>Bath</surname> <given-names>DE</given-names></name><name><surname>Fitzgerald</surname> <given-names>JE</given-names></name><name><surname>Cannon</surname> <given-names>JG</given-names></name><name><surname>Simpson</surname> <given-names>JH</given-names></name><name><surname>Dickson</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Threshold-Based ordering of sequential actions during <italic>Drosophila</italic> courtship</article-title><source>Current Biology</source><volume>29</volume><fpage>426</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.12.019</pub-id><pub-id pub-id-type="pmid">30661796</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mehta</surname> <given-names>D</given-names></name><name><surname>Sridhar</surname> <given-names>S</given-names></name><name><surname>Sotnychenko</surname> <given-names>O</given-names></name><name><surname>Rhodin</surname> <given-names>H</given-names></name><name><surname>Shafiei</surname> <given-names>M</given-names></name><name><surname>Seidel</surname> <given-names>H</given-names></name><name><surname>Xu</surname> <given-names>W</given-names></name><name><surname>Casas</surname> <given-names>D</given-names></name><name><surname>Theobalt</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Vnect: Real-Time3D Human Pose Estimation with a Single RGB Camera</article-title><conf-name>SIGGRAPH</conf-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mendes</surname> <given-names>CS</given-names></name><name><surname>Bartos</surname> <given-names>I</given-names></name><name><surname>Akay</surname> <given-names>T</given-names></name><name><surname>Márka</surname> <given-names>S</given-names></name><name><surname>Mann</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Quantification of gait parameters in freely walking wild type and sensory deprived Drosophila Melanogaster</article-title><source>eLife</source><volume>2</volume><elocation-id>e00231</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.00231</pub-id><pub-id pub-id-type="pmid">23326642</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Moeslund</surname> <given-names>TB</given-names></name><name><surname>Granum</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Multiple cues used in model-based human motion capture.</article-title><conf-name>Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)</conf-name><fpage>362</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1109/AFGR.2000.840660</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Moreno-noguer</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>3d human pose estimation from a single image via distance matrix regression</article-title><conf-name>CVPR</conf-name></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>KP</given-names></name><name><surname>Weiss</surname> <given-names>Y</given-names></name><name><surname>Jordan</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Loopy belief propagation for approximate inference: an empirical study</article-title><conf-name>Onference on Uncertainty in Artificial Intelligence</conf-name><fpage>467</fpage><lpage>475</lpage></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname> <given-names>T</given-names></name><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Chen</surname> <given-names>AC</given-names></name><name><surname>Patel</surname> <given-names>A</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Using DeepLabCut for 3D markerless pose estimation across species and behaviors</article-title><source>Nature Protocols</source><volume>14</volume><fpage>2152</fpage><lpage>2176</lpage><pub-id pub-id-type="doi">10.1038/s41596-019-0176-0</pub-id><pub-id pub-id-type="pmid">31227823</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Newell</surname> <given-names>A</given-names></name><name><surname>Yang</surname> <given-names>K</given-names></name><name><surname>Deng</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>European Conference on Computer Vision</chapter-title><source>Stacked Hourglass Networks for Human Pose Estimation</source><publisher-name>Springer</publisher-name><fpage>483</fpage><lpage>499</lpage></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pavlakos</surname> <given-names>G</given-names></name><name><surname>Zhou</surname> <given-names>X</given-names></name><name><surname>Derpanis</surname> <given-names>K</given-names></name><name><surname>Konstantinos</surname> <given-names>G</given-names></name><name><surname>Daniilidis</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>Coarse-To-Fine volumetric prediction for Single-Image 3D human pose</article-title><conf-name>CVPR</conf-name></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pavlakos</surname> <given-names>G</given-names></name><name><surname>Zhou</surname> <given-names>X</given-names></name><name><surname>Konstantinos</surname> <given-names>KDG</given-names></name><name><surname>Kostas</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>Harvesting multiple views for Marker-Less 3D human pose annotations</article-title><conf-name>In: CVPR</conf-name></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname> <given-names>TD</given-names></name><name><surname>Aldarondo</surname> <given-names>DE</given-names></name><name><surname>Willmore</surname> <given-names>L</given-names></name><name><surname>Kislin</surname> <given-names>M</given-names></name><name><surname>Wang</surname> <given-names>SS</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast animal pose estimation using deep neural networks</article-title><source>Nature Methods</source><volume>16</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><pub-id pub-id-type="pmid">30573820</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Popa</surname> <given-names>AI</given-names></name><name><surname>Zanfir</surname> <given-names>M</given-names></name><name><surname>Sminchisescu</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deep multitask architecture for integrated 2D and 3D human sensing</article-title><conf-name>In: CVPR</conf-name></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Puwein</surname> <given-names>J</given-names></name><name><surname>Ballan</surname> <given-names>L</given-names></name><name><surname>Ziegler</surname> <given-names>R</given-names></name><name><surname>Pollefeys</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Accelerated Kmeans Clustering Using Binary Random Projection</chapter-title><source>Joint Camera Pose Estimation and 3D Human Pose Estimation in a Multi-Camera Setup</source><publisher-name>springer</publisher-name><fpage>473</fpage><lpage>487</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rhodin</surname> <given-names>H</given-names></name><name><surname>Robertini</surname> <given-names>N</given-names></name><name><surname>Casas</surname> <given-names>D</given-names></name><name><surname>Richardt</surname> <given-names>C</given-names></name><name><surname>Seidel</surname> <given-names>HP</given-names></name><name><surname>Theobalt</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>General automatic human shape and motion capture using volumetric contour cues</article-title><conf-name>ECCV</conf-name></element-citation></ref><ref id="bib40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rogez</surname> <given-names>G</given-names></name><name><surname>Weinzaepfel</surname> <given-names>P</given-names></name><name><surname>Schmid</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Lcr-Net: localization-classification-regression for human pose</article-title><conf-name>In: CVPR</conf-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seeds</surname> <given-names>AM</given-names></name><name><surname>Ravbar</surname> <given-names>P</given-names></name><name><surname>Chung</surname> <given-names>P</given-names></name><name><surname>Hampel</surname> <given-names>S</given-names></name><name><surname>Midgley</surname> <given-names>FM</given-names></name><name><surname>Mensh</surname> <given-names>BD</given-names></name><name><surname>Simpson</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A suppression hierarchy among competing motor programs drives sequential grooming in Drosophila</article-title><source>eLife</source><volume>3</volume><elocation-id>e02951</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.02951</pub-id><pub-id pub-id-type="pmid">25139955</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seelig</surname> <given-names>JD</given-names></name><name><surname>Chiappe</surname> <given-names>ME</given-names></name><name><surname>Lott</surname> <given-names>GK</given-names></name><name><surname>Dutta</surname> <given-names>A</given-names></name><name><surname>Osborne</surname> <given-names>JE</given-names></name><name><surname>Reiser</surname> <given-names>MB</given-names></name><name><surname>Jayaraman</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Two-photon calcium imaging from head-fixed Drosophila during optomotor walking behavior</article-title><source>Nature Methods</source><volume>7</volume><fpage>535</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1468</pub-id><pub-id pub-id-type="pmid">20526346</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simon</surname> <given-names>T</given-names></name><name><surname>Joo</surname> <given-names>H</given-names></name><name><surname>Matthews</surname> <given-names>I</given-names></name><name><surname>Sheikh</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hand keypoint detection in single images using multiview bootstrapping</article-title><conf-name>In: CVPR</conf-name></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sun</surname> <given-names>X</given-names></name><name><surname>Shang</surname> <given-names>J</given-names></name><name><surname>Liang</surname> <given-names>S</given-names></name><name><surname>Wei</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Compositional human pose regression</article-title><conf-name>ICCV</conf-name></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Takahashi</surname> <given-names>K</given-names></name><name><surname>Mikami</surname> <given-names>D</given-names></name><name><surname>Isogawa</surname> <given-names>M</given-names></name><name><surname>Kimata</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Human pose as calibration pattern; 3D human pose estimation with multiple unsynchronized and uncalibrated cameras</article-title><conf-name>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</conf-name></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tekin</surname> <given-names>B</given-names></name><name><surname>Marquez-neila</surname> <given-names>P</given-names></name><name><surname>Salzmann</surname> <given-names>M</given-names></name><name><surname>Fua</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning to fuse 2D and 3D image cues for monocular body pose estimation</article-title><conf-name>ICCV</conf-name></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todd</surname> <given-names>JG</given-names></name><name><surname>Kain</surname> <given-names>JS</given-names></name><name><surname>de Bivort</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Systematic exploration of unsupervised methods for mapping behavior</article-title><source>Physical Biology</source><volume>14</volume><elocation-id>015002</elocation-id><pub-id pub-id-type="doi">10.1088/1478-3975/14/1/015002</pub-id><pub-id pub-id-type="pmid">28166059</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tome</surname> <given-names>D</given-names></name><name><surname>Russell</surname> <given-names>C</given-names></name><name><surname>Agapito</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Lifting from the deep: convolutional 3D pose estimation from a single image</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1701.00295">https://arxiv.org/abs/1701.00295</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Triggs</surname> <given-names>B</given-names></name><name><surname>Mclauchlan</surname> <given-names>P</given-names></name><name><surname>Hartley</surname> <given-names>R</given-names></name><name><surname>Fitzgibbon</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Vision Algorithms: Theory and Practice</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uhlmann</surname> <given-names>V</given-names></name><name><surname>Ramdya</surname> <given-names>P</given-names></name><name><surname>Delgado-Gonzalo</surname> <given-names>R</given-names></name><name><surname>Benton</surname> <given-names>R</given-names></name><name><surname>Unser</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>FlyLimbTracker: an active contour based approach for leg segment tracking in unmarked, freely behaving Drosophila</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0173433</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0173433</pub-id><pub-id pub-id-type="pmid">28453566</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhou</surname> <given-names>X</given-names></name><name><surname>Huang</surname> <given-names>Q</given-names></name><name><surname>Sun</surname> <given-names>X</given-names></name><name><surname>Xue</surname> <given-names>X</given-names></name><name><surname>Wei</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Weakly-supervised transfer for 3d human pose estimation in the wild</article-title><conf-name>IEEE International Conference on Computer Vision</conf-name></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48571.032</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>O'Leary</surname><given-names>Timothy</given-names></name><role>Reviewing Editor</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Shaevitz</surname><given-names>Josh W</given-names></name><role>Reviewer</role><aff><institution>Princeton University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;DeepFly3D, a deep learning-based approach for 3D limb and appendage tracking in tethered, adult <italic>Drosophila</italic>&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Ronald Calabrese as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Josh W Shaevitz (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Both reviewers agreed that this tools and resources article can potentially be of use to the field. By automating pose tracking a large amount of data can be extracted from experiments, enabling new scientific questions to be addressed.</p><p>However, a number of key issues with the current study were identified and outlined in the reviewer comments below. In summary, the current manuscript:</p><p>1) Lacks a comprehensive discussion of how experimental choices, e.g. camera number, affect tracking error</p><p>2) Reports a very large tracking error using a much larger training set than current state-of-the-art 2D tracking algorithms. This needs to be accounted for.</p><p>3) Lacks some key references in the field and comparison with other approaches.</p><p><italic>Reviewer #1:</italic></p><p>Günel et al. report the development of a new toolkit to perform 3D pose estimation on tethered flies. While the toolkit is certainly bound to find utility, there are not major computational advances or insights within the work. While deep learning approaches for animal pose estimation is a &quot;young field&quot; in neuroscience/ethology (i.e. (Pereira et al., 2019, Mathis et al., 2018, Nath et al., 2019), it would be important for the authors to clarify their contribution in light of this. Namely, the authors should clearly discuss the major points that potential users care about: (1) accuracy of the tool, (2) amount of data required to train networks, if required, (3) speed, and (4) usability.</p><p>While DeepFly3D provides very nice user interfaces for the toolkit, it does suffer in the other three main categories.</p><p>1) Accuracy. The authors report a 13.9-pixel error on 480 by 920 images, and consider the PCK to be 50 pixels, which is nearly as long as a leg segment – not the width! I would argue this is far too lenient of a threshold. Additionally, two points the authors need to address (1) The RMSE would be more meaningful with an estimate of the human labeling variability (i.e. what is the best the network could do?), and (2) why, given so much training data, is the performance not as accurate as other approaches (LEAP, DeepLabCut)?</p><p>i.e. in reference to:</p><p>– &quot;… 480× 960 pixels. The test dataset included challenging frames and occasional motion blur to increase the difficulty of pose estimation. For training, we used a final training dataset of 37,000 frames, an overwhelming majority of which were first automatically corrected using pictorial structures. On test data, we achieved a Root Mean Square Error (RMSE) of 13.9 pixels&quot;</p><p>– The error is quite a bit larger than other animal pose estimation approaches, i.e. on 480x640 on 3D freely moving flies DeepLabCut used ~560 frames with ~4-pixel error on 682 × 540 frames. LEAP: ~500 images to achieve ~3-pixel error on 192 × 192-pixel frames with flies. While I appreciate that a direct comparison is not fully straightforward, and the authors do not use &quot;easy&quot; points to compute the pixel error, the authors absolutely should discuss/address this point.</p><p>2) The amount of data. As this network was trained on ~37K images, it is likely expected that the end-user will use these authors pre-trained network or the human pre-trained network. Namely, creating a large dataset seems infeasible when other options exist that require much less data. Namely, DeepLabCut reports 50-200 for most applications, and LEAP requires only ~500-1,500, which is nearly a factor of 10 less images to match similar performance (I understand that 2K are hand-labeled, while the rest are not, but even that is a 4X increase over the best method, i.e. DeepLabCut). Thus, the amount of training data for the performance is very high, given current alternatives. These limitations should be discussed.</p><p>3) Speed. There is no report of the speed of the network, thus the authors should benchmark the performance on video analysis speed (only network training time is reported).</p><p>4) Since the authors needed a lot of labeled data, making this a most useful as a specialized network for tethered flies, i.e. much like OpenPose has specific networks for hands (which they could clarify). Therefore, if they provide the network weights they should show the network is useable for other labs data.</p><p>5) 3D pose in the literature. The authors do not discuss Nath et al., which is a 3D extension of DeepLabCut (bioRxiv Nov 2018, and Nature Protocols, 2019) and given it's a small field, the authors should not claim theirs is the first to do 3D pose. Beyond deep learning multiple 3D pose estimation papers exist in Neuroscience (ie. Mimica et al., 2018 – Efficient cortical coding of 3D posture in freely behaving rats).</p><p>Therefore language like this should be modified:</p><p>&quot;However, algorithms for reliably estimating 3D pose in such small animals have not yet been developed.…&quot;</p><p>&quot;However, these measurements have been restricted to 2D pose in freely behaving animals&quot;</p><p>&quot;We demonstrate more accurate unsupervised behavioral embedding using 3D joint angles rather than commonly used 2D pose data. Thus, DeepFly3D enables the automated acquisition of behavioral measurements at an unprecedented level of resolution for a variety of biological application&quot;</p><p>(also here &quot;resolution&quot; should be edited; it is not that DeepFly3D is more accurate than other alternatives.)</p><p>However, I do think these authors have a nice solution, so I do find it very fair that they state they have an alternative 3D approach, (and can highlight the advantages) solution to multi-camera estimation in body- or head- fixed animals. However, again, sparse bundle adjustment is not a new technique, so the authors should discuss this in light of the literature.</p><p>6) &quot;Here we present DeepFly3D, a software that infers the 3D pose of tethered, adult <italic>Drosophila</italic>-or other animals-using multiple camera images&quot; – they don't quantify performance for other animals, so this should be dropped from the Abstract.</p><p>7) &quot;DeepLabCut provides a user-friendly interface to DeepCut, a state-of-the-art human pose estimation network&quot; (DeepCut should be DeeperCut, which is different than DeepCut).</p><p>8) &quot;Calibration without targets&quot; is misleading. The authors do calibrate; they just use the rigid body of the fly that has set distances per limb segment. To note, this isn't always doable for other animals/videos, again another argument to drop the random &quot;this can be used for anything&quot; language…</p><p><italic>Reviewer #2:</italic></p><p>The manuscript by Gunel et al. describes a new tool for 3D pose tracking that automatically combines multiple 2D images from different viewing angles while also learning the camera model. There has been a flurry of recent papers that use deep neural networks to track pose in 2D images. This manuscript improves on these by generating 3D positions for each body part that is tracked using a combination of various algorithms that have been developed for human pose tracking. In addition, the authors provide a simple GUI that allows for part labeling and prediction correction. Overall the paper is well written and the most aspects of the system described and justified.</p><p>My only major issue is that the experimental setup is not discussed or motivated. In particular, I would like to see more information about the choices/requirements for the number of cameras and other considerations. Why do the authors use 7 cameras? I assume that with fewer cameras you get more error/occlusions/etc. Can you investigate these effects quantitatively? How should a researcher choose the number of cameras (recording from 7 cameras in synch is difficult)?</p><p>In addition, I would suggest that the authors show the distribution of 3D localization errors. Is this isotropic? Is it the same for all body parts (the Discussion subsection “Learning the parameters” suggests not)?</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48571.033</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>1) Lacks a comprehensive discussion of how experimental choices, e.g. camera number, affect tracking error</p><p>2) Reports a very large tracking error using a much larger training set than current state-of-the-art 2D tracking algorithms. This needs to be accounted for.</p><p>3) Lacks some key references in the field and comparison with other approaches.</p><p>Reviewer #1:</p><p>[…] While DeepFly3D provides very nice user interfaces for the toolkit, it does suffer in the other three main categories.</p><p>1) Accuracy. The authors report a 13.9-pixel error on 480 by 920 images, and consider the PCK to be 50 pixels, which is nearly as long as a leg segment – not the width! I would argue this is far too lenient of a threshold.</p></disp-quote><p>We illustrate the relative sizes of the error threshold (50 pixels) and limb segment lengths in <xref ref-type="fig" rid="respfig1">Author response image 1A</xref>. We set the threshold to be approximately one third the length of femur (erroneously called the “femur tibia segment” in the initial submission), which is approximately 150 pixels. We apologize for any confusion causing the reviewer to interpret this as the width and have adjusted the text accordingly. We now state: “We set this threshold as 50 pixels, which is roughly one third of the 3D length of the femur.” If we reduce our threshold to 30 or 20 pixels, we still achieve 95% or 89% accuracy, respectively. (<xref ref-type="fig" rid="respfig1">Author response image 1B</xref>).</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>Interpreting and choosing the PCK Threshold.</title><p>(<bold>a</bold>) An illustration of the length of the PCK (percentage of keypoints) error threshold (Scale bar, bottom right) for comparison with limb segment lengths. The 50 pixel threshold is approximately one-third the length of the prothoracic femur. (<bold>b</bold>) PCK accuracy as a function of mean absolute error (MAE) threshold.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-resp-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Additionally, two points the authors need to address (1) The RMSE would be more meaningful with an estimate of the human labeling variability (i.e. what is the best the network could do?)</p></disp-quote><p>We agree with the reviewer, and have now added this important measurement by calculating the accuracy of human manual-annotations as a baseline for interpreting the quality of our network predictions. We randomly selected 210 images and had a new expert manually annotate them. We observed a RMSE of 12.4 pixels. Therefore, our Network Annotation / Manual Annotation ratio of 1.12 (13.9 pixels / 12.4 pixels) is similar to the ratio reported by Mathis et al., 2018: 1.07 (2.88 pixels / 2.69 pixels) (pg. 1282 the last paragraph, and pg. 1284 the first paragraph). However, this ratio is reported for a mouse dataset, and human variability for Drosophila is not reported. We now include this information in the revised manuscript: “Compared with a ground truth RMSE of 12.4 pixels – via manual annotation of 210 images by a new human expert – our Network Annotation / Manual Annotation ratio of 1.12 (13.9 pixels / 12.4 pixels) is similar to another state-of-the-art network (Mathis et al., 2018): 1.07 (2.88 pixels / 2.69 pixels).”</p><disp-quote content-type="editor-comment"><p>[…] and (2) why, given so much training data, is the performance not as accurate as other approaches (LEAP, DeepLabCut)?</p><p>i.e. in reference to:</p><p>– &quot;… 480× 960 pixels. The test dataset included challenging frames and occasional motion blur to increase the difficulty of pose estimation. For training, we used a final training dataset of 37,000 frames, an overwhelming majority of which were first automatically corrected using pictorial structures. On test data, we achieved a Root Mean Square Error (RMSE) of 13.9 pixels&quot;</p><p>– The error is quite a bit larger than other animal pose estimation approaches, i.e. on 480x640 on 3D freely moving flies DeepLabCut used ~560 frames with ~4-pixel error on 682 × 540 frames. LEAP: ~500 images to achieve ~3-pixel error on 192 × 192-pixel frames with flies. While I appreciate that a direct comparison is not fully straightforward, and the authors do not use &quot;easy&quot; points to compute the pixel error, the authors absolutely should discuss/address this point.</p></disp-quote><p>We understand that a 13.9-pixel error may seem large when compared with other pose estimation approaches. However, as the reviewer appreciates, making a direct comparison between datasets is not fully straightforward and we believe this misconception can be explained by several differences between the various datasets:</p><p>Comparison with the LEAP dataset:</p><p>1) The ~3-pixel error reported for 192x192-pixel images in Pereira et al., 2019 (LEAP) corresponds to a 11.9-pixel≅3/2(480/192)2+(960/192)2 error for the 480x960 pixel images used in our study if we similarly rescale pose predictions and ground truth annotations. See the Appendix below for a more detailed calculation of the scaling coefficient.</p><p>2) The LEAP dataset is relatively highly preprocessed compared with our dataset: images are rotated, aligned, and thresholded to generate a featureless, static background. By contrast, our data are raw, without any further preprocessing, and therefore harder for performing pose estimation. Our multiview dataset does not lend itself as readily to preprocessing, due to the more dynamic background including a rotating spherical treadmill and legs moving on the opposite side of the body.</p><p>Comparison with the DeepLabCut dataset:</p><p>1) Although Mathis et al., 2018 perform pose estimation on larger images (682x540 pixels), flies occupy a much smaller region in each image, (~340x135 pixels when flies are parallel to the image plane and take up the largest image area). By contrast, in our dataset, flies occupy nearly the entire image (~840x320 pixels). Thus, scaling the reported error for DeepLabCut (4 pixels for 340x135 pixel images) to 840x320 pixel images results in an error of 10.1 pixels ≅4.17/2(320/135)2+(840/340)2. This value is much closer to the 13.9 pixel error that we report for DeepFly3D.</p><p>2) Mathis et al., 2018 principally illustrates the tracking of landmarks on the fly’s abdomen and head (with only one landmark on the leg). These larger body parts experience smaller deformations and perform less rapid, complex movements than the legs.</p><p>These calculations are approximate but we intend for them to provide context for the error values we report in our manuscript. In short, we believe the seemingly large tracking error of DeepFly3D is (i) due to the scale of the images (rescaled pixel-wise error will grow with increasing image size) and (ii) due to the more visually complex nature of our dataset. It should be stressed that the 2D pose module of DeepFly3D – our Stacked Hourglass network – can be replaced with any other network. This includes the networks from Mathis et al., 2018, and Pereira et al., 2019, or any other pose estimation approach as long as the network outputs pixel-wise conditional probabilities.</p><p>The relative strengths of the Stacked Hourglass network and DeepLabCut have also been discussed in a recent preprint (Graving et al., 2019; Appendix 0 Figure 7). They show that, although DeepLabCut is more accurate than the Stacked Hourglass network (15% less error on a Drosophila dataset with the same number of annotations, ~2 pixel MAE versus ~2.3 pixel MAE using the same amount of annotation), the Stacked Hourglass network is much faster than DeepLabCut (4x on average). We believe this speed-accuracy trade-off is necessary to process 700 images produced per second in our system. Similarly, LEAP is significantly less accurate than both DeepLabCut and the Stacked Hourglass network on many different datasets (Graving et al., 2019; Appendix 0 Figure 7).</p><disp-quote content-type="editor-comment"><p>2) The amount of data. As this network was trained on ~37K images, it is likely expected that the end-user will use these authors pre-trained network or the human pre-trained network. Namely, creating a large dataset seems infeasible when other options exist that require much less data. Namely, DeepLabCut reports 50-200 for most applications, and LEAP requires only ~500-1,500, which is nearly a factor of 10 less images to match similar performance (I understand that 2K are hand-labeled, while the rest are not, but even that is a 4X increase over the best method, i.e. DeepLabCut). Thus, the amount of training data for the performance is very high, given current alternatives. These limitations should be discussed.</p></disp-quote><p>In principle, our Stacked Hourglass network also does not require a large amount of data to perform well. However, like all deep models, it gradually improves in performance and generalizes better with increasing amounts of data. Thus, we used a large training set to increase our network’s robustness and utility. Importantly, our DeepFly3D toolset is designed to simplify and automate annotation such that the large-dataset high-accuracy regimes can be reached relatively easily.</p><p>To illustrate these assertions and test the performance of the network in a low data regime, we trained a single 2 stacked network using ground-truth annotations data from 7 cameras. In Figure 2B, we compare the results with an asymptotic prediction error (i.e., the error observed when the network is trained using the full dataset of 40,000 annotated images). We also compare the results to human annotation variability (‘manual annotation error’). In this particular dataset, we observed an asymptotic MAE of 10.5 pixels and a human variability MAE of 9.2 pixels. Thus, the ratio between human variability remains close to 1.12. We observe that, with 800 annotations, our network achieves a similar accuracy to manual annotation error and the asymptotic prediction error. Further annotation only generates diminishing returns.</p><disp-quote content-type="editor-comment"><p>3) Speed. There is no report of the speed of the network, thus the authors should benchmark the performance on video analysis speed (only network training time is reported).</p></disp-quote><p>We agree with the reviewer that this may be an important piece of information for the end-user. Using the desktop configuration described in our original submission, we found that our network can run at 110 Frames-Per-Second (FPS) using the 8-stack variant of the Stacked Hourglass network, and can run at 420FPS when using the smaller 2-stack variant. Thanks to our effective initialization step, we observed that calibration takes only 3-4 s. We also measured the time required for error checking and error correction steps. Error checking can be done at 100 FPS and error correction can be done in 10 FPS. Since error correction is scarcely performed in response to a large reprojection error, it does not create a bottleneck in the overall speed of the pipeline.</p><p>We now include this information in the revised manuscript as follows: “Using this desktop configuration, our network can run at 100 Frames-Per-Second (FPS) using the 8-stack variant of the Hourglass network, and can run at 420 FPS using the smaller 2-stack version. Thanks to an effective initialization step, calibration takes 3-4 s. Error checking and error correction can be performed at 100 FPS and 10 FPS, respectively. Error correction is only performed in response to large reprojection errors, and does not create a bottleneck in the overall speed of the pipeline.”</p><disp-quote content-type="editor-comment"><p>4) Since the authors needed a lot of labeled data, making this a most useful as a specialized network for tethered flies, i.e. much like OpenPose has specific networks for hands (which they could clarify). Therefore, if they provide the network weights they should show the network is useable for other labs data.</p></disp-quote><p>As illustrated in our earlier response to point #2, our Stacked Hourglass network does not require 37K images to achieve an accuracy comparable with that reported in Mathis et al., 2018. We have publicly published our dataset (<ext-link ext-link-type="uri" xlink:href="https://dataverse.harvard.edu/dataverse/DeepFly3D">https://dataverse.harvard.edu/dataverse/DeepFly3D</ext-link>), but we do not have access to video data from a synchronized, multi-camera system and thus could not test our network on another laboratory’s data. We would also like to emphasize that DeepFly3D is specialized for tethered flies but that the computational tools behind our data analysis pipeline are general, as demonstrated by our adaptation of DeepFly3D to the human H3.6m dataset (Figure 11).</p><disp-quote content-type="editor-comment"><p>5) 3D pose in the literature. The authors do not discuss Nath et al., which is a 3D extension of DeepLabCut (bioRxiv Nov 2018, and Nature Protocols, 2019) and given it's a small field, the authors should not claim theirs is the first to do 3D pose. Beyond deep learning multiple 3D pose estimation papers exist in Neuroscience (ie. Mimica et al., 2018 – Efficient cortical coding of 3D posture in freely behaving rats).</p></disp-quote><p>We thank the reviewer for pointing out relevant literature. We now also cite Nath et al., 2019. However, we would like to point out that we do not claim that DeepFly3D is the first approach to generally perform 3D pose estimation. We recognize a rich human pose estimation literature and an increasing number of studies performing pose estimation on large laboratory animals (including rodents). These studies achieve 3D pose by triangulating multiple camera views using, for example a checkerboard pattern, and readily available software. To the best of our knowledge, this checkerboard-based registration approach is what is used by the OpenCV library that is referred to in Nath et al., 2019: “3D kinematics can be reconstructed from one network being trained on multiple views, and the user needs only the camera calibration files to reconstruct the data. This camera calibration and triangulation can be done in many programs, including the free package OpenCV”.</p><p>By contrast, by developing DeepFly3D, we tried to be careful to point out that we are performing 3D pose estimation on uniquely challenging fly-sized objects. To make this clear, we stated that “techniques used to translate human 2D pose to 3D pose cannot be easily transferred for the study of small animals like Drosophila” because “precisely registering multiple camera viewpoints using traditional approaches would require the fabrication of a prohibitively small checkerboard pattern”. Therefore, we still contend that DeepFly3D provides a novel contribution to 3D pose estimation because “the unique challenges associated with transforming these 2D measurements into reliable and precise 3D poses have not been addressed for small animals including the fly, Drosophila melanogaster” and “algorithms for reliably estimating 3D pose in such small animals have not yet been developed.” Nevertheless, as described below we modified the language in our revised manuscript to further clarify these points.</p><disp-quote content-type="editor-comment"><p>Therefore language like this should be modified:</p><p>&quot;However, algorithms for reliably estimating 3D pose in such small animals have not yet been developed.…&quot;</p></disp-quote><p>We agree that our definition of “small” may have been unclear, and in light of the reviewer’s comments, we have edited this sentence. The manuscript now states: “However, algorithms for reliably estimating 3D pose in such small Drosophila-sized animals have not yet been developed.”</p><disp-quote content-type="editor-comment"><p>&quot;However, these measurements have been restricted to 2D pose in freely behaving animals&quot;</p></disp-quote><p>We agree with the reviewer that it was initially unclear that this sentence refers entirely to work on insects – for which this statement is correct. We have removed a misleading citation to human pose (Mori and Malik, 2006), added a more relevant Drosophila citation (Isakov et al., 2016), and edited two sentences to now read: “Although this approach works well on humans (Moeslund et al.), in smaller, Drosophila-sized animals markers likely hamper movements,…”, and “However, these measurements have been restricted to 2D pose in freely behaving flies.</p><disp-quote content-type="editor-comment"><p>&quot;We demonstrate more accurate unsupervised behavioral embedding using 3D joint angles rather than commonly used 2D pose data. Thus, DeepFly3D enables the automated acquisition of behavioral measurements at an unprecedented level of resolution for a variety of biological application&quot; (also here &quot;resolution&quot; should be edited; it is not that DeepFly3D is more accurate than other alternatives.)</p></disp-quote><p>We agree with the reviewer and have edited this sentence to improve clarity. Here our reference to ‘resolution’ refers to the additional information provided by the 3rd dimension – something that has not otherwise been achieved for Drosophila. We have now edited this sentence in the Abstract to say “Thus, DeepFly3D enables the automated acquisition of Drosophila behavioral measurements at an unprecedented level of detail for a variety of biological applications.”</p><disp-quote content-type="editor-comment"><p>However, I do think these authors have a nice solution, so I do find it very fair that they state they have an alternative 3D approach, (and can highlight the advantages) solution to multi-camera estimation in body- or head- fixed animals. However, again, sparse bundle adjustment is not a new technique, so the authors should discuss this in light of the literature.</p></disp-quote><p>We thank the reviewer for their positive appreciation of our approach. We have now added additional citations to the Discussion of our approach. In the same lines, we now state: “During the calibration process, we also employ sparse bundle adjustment methods, as previously used for human pose estimation.”</p><disp-quote content-type="editor-comment"><p>6) &quot;Here we present DeepFly3D, a software that infers the 3D pose of tethered, adult Drosophila-or other animals-using multiple camera images&quot; – they don't quantify performance for other animals, so this should be dropped from the Abstract.</p></disp-quote><p>We have removed this reference to “-or other animals-” from the Abstract.</p><disp-quote content-type="editor-comment"><p>7) &quot;DeepLabCut provides a user-friendly interface to DeepCut, a state-of-the-art human pose estimation network&quot; (DeepCut should be DeeperCut, which is different than DeepCut).</p></disp-quote><p>We thank the reviewer for spotting this error. We changed ‘DeepCut’ to ‘DeeperCut’ in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>8) &quot;Calibration without targets&quot; is misleading. The authors do calibrate; they just use the rigid body of the fly that has set distances per limb segment. To note, this isn't always doable for other animals/videos, again another argument to drop the random &quot;this can be used for anything&quot; language…</p></disp-quote><p>We apologize for the confusion. To clarify, our manuscript mentions “Calibration without external targets” and uses the word ‘external’ explicitly to distinguish calibrating using the fly itself versus some external device fabricated separately for this purpose (e.g., a checkerboard pattern). To fix this possible source of confusion, we have now changed the text to read “Calibration without an external calibration pattern&quot;.</p><p>Notably, our calibration procedure does not depend on a fixed limb segment length or a rigid body, but rather on the assumption that multiple cameras can see the same 3D point. However, a constant limb segment constraint is used during the automatic correction step, after the calibration. If a non-rigid body is present, we suggest discarding the segment length energy and only using the reprojection error and probability maps during the optimization. This can be done by setting the lambda_bone parameter to 0 inside the Config.py file.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The manuscript by Gunel et al. describes a new tool for 3D pose tracking that automatically combines multiple 2D images from different viewing angles while also learning the camera model. There has been a flurry of recent papers that use deep neural networks to track pose in 2D images. This manuscript improves on these by generating 3D positions for each body part that is tracked using a combination of various algorithms that have been developed for human pose tracking. In addition, the authors provide a simple GUI that allows for part labeling and prediction correction. Overall the paper is well written and the most aspects of the system described and justified.</p></disp-quote><p>We thank the reviewer for their positive appreciation of our work.</p><disp-quote content-type="editor-comment"><p>My only major issue is that the experimental setup is not discussed or motivated. In particular, I would like to see more information about the choices/requirements for the number of cameras and other considerations. Why do the authors use 7 cameras? I assume that with fewer cameras you get more error/occlusions/etc. Can you investigate these effects quantitatively? How should a researcher choose the number of cameras (recording from 7 cameras in synch is difficult)?</p></disp-quote><p>We agree with the reviewer that this is important information for an end-user. Triangulation requires that any 3D point be seen by at least two cameras. Therefore, to see landmarks on both sides of the animal, one would need to use at least 4 cameras – 2 cameras on each side of the animal. In <xref ref-type="fig" rid="respfig2">Author response image 2</xref> we examine the accuracy of 3D pose estimation as a function of number of cameras used for calibration and/or prediction.</p><fig id="respfig2"><label>Author response image 2.</label><caption><title>Triangulation error for each joint as a function of number of cameras used for calibration.</title><p>Mean absolute error (MAE) as a function of number of cameras used for triangulation and calibration, as well as human annotation versus network prediction. Calibration using all 7 cameras and human annotation is the ground truth (red circles). This ground truth is compared with using N cameras for triangulation and performing calibration from N cameras using either human annotation of images (blue circles) or network predictions of images (yellow circles). A comparison across joints (individual panels) demonstrates that certain joints (e.g., tarsus tips) are more susceptible to increased errors with fewer cameras.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-resp-fig2-v2.tif"/></fig><p>Our 3D ground truth data are based on human annotated 2D points calibrated using all 7 cameras, and triangulated again using human annotated 2D points. We observe that 3D error doubles as we reduce the number of cameras from 7 to 4. Thus, 3 cameras could be removed from the setup, but this would lead to an approximately 0.05 mm increase in mean error.</p><p>Specifically we observe that: (i) Calibration is not a significant source of error. Calibrating with N cameras (blue circles), or 7 cameras (red circles) is not very different, (ii) More cameras lead to improved triangulation. With fewer cameras, even having calibrated using all 7 cameras (red circles), results in larger error. This may be because the camera views are sufficiently different, having largely non-overlapping 2D-detection failure cases. Thus, the redundancy provided by having more cameras mitigates detection errors by finding a 3D pose that is consistent across at least two camera views. (iii) The remaining error (distance between blue and orange circles) can be explained by the 2D network detections having a larger 2D error (13.9 pixels vs. 12.4 pixels RMSE with human annotations).</p><p>On the other hand, as the reviewer points out, there are practical tradeoffs for increasing the number of cameras. Ultimately, one could potentially reduce the number of cameras from seven to two (one on each side of the animal) by training a neural network that “lifts” 2D pose into 3D from single camera images (Martinez et al., 2017).</p><p>We have now added this additional information in the revised manuscript as follows: “We assessed the optimal number of cameras for DeepFly3D and concluded that increasing the number of cameras increases accuracy by stabilizing triangulation. Specifically, we observed the following. (i) Calibration is not a significant source of error: calibrating with fewer than 7 cameras does not dramatically increase estimation error. (ii) Having more cameras improves triangulation. Reducing the number of cameras down to four, even having calibrated with 7 cameras, results in an increase of ~0.05 mm triangulation error. This may be because the camera views are sufficiently different, having largely non-overlapping 2D-detection failure cases. Thus, the redundancy provided by having more cameras mitigates detection errors by finding a 3D pose that is consistent across at least two camera views.”</p><disp-quote content-type="editor-comment"><p>In addition, I would suggest that the authors show the distribution of 3D localization errors. Is this isotropic? Is it the same for all body parts (the Discussion subsection “Learning the parameters” suggests not)?</p></disp-quote><p>As mentioned, the error is not isotropic. We now plot all of the errors for each joint in <xref ref-type="fig" rid="respfig3">Author response image 3</xref>. We believe that the tarsus tips exhibit larger error than the other joints due to the increased prevalence of occlusions from the spherical treadmill, and larger positional variance. On the other hand, increasing error for the body-coxa joints can be attributed to the difficulty of human annotation and occlusions from some perspectives.</p><fig id="respfig3"><label>Author response image 3.</label><caption><title>MAE errors for limb and antennal landmarks.</title><p>MAE as a function of landmark showing the differential distribution of errors. Violin plots are overlaid with raw data points (white circles).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48571-resp-fig3-v2.tif"/></fig><p><bold>Appendix to Reviewers</bold></p><p>1) For a single point, the error can be decomposed into the error in the x and y dimensions separately, e.g. e=ex2+ey2. Assuming the error in the x and y dimensions can be defined by the same distribution, E[ex2+ey2]=E[2ex2]=2ex^=2ey^, where ex^ and ey^ are the means of the error distribution. Scaling the image by sx and sy in the x and y dimensions, respectively, results in the errores=(sxex)2+(syey)2 and therefore E[es]=E[(sxex)2+(syey)2]=(sx)2+(sy)2ex^=(sx)2+(sy)2ey^. Then the final error for the scaled image can be approximated by E[es]=E[e](sx)2+(sy)2/2.</p><p>A much simpler argument to understand the effects of image scaling on reported error – assuming that images are scaled by one factor in both dimensions – can be summarized as follows. On the LEAP dataset, using the scale factor of the y dimension to rescale the error would result in a 15 pixel error (3 pixels * 960/192). Similarly, using the scale factor of the x dimension would result in a 7.2 pixel error (3 pixels * 480/192). Our reported error, 13.9 pixels, is between these two values. In the same manner, on the DeepLabCut dataset, using the scale factor of the x dimension to rescale the error would result in a 9.9 pixel error (4.17 pixels * 320/135). Using the scale factor of the y dimension would result in a 10.3 pixel error (4 pixels * 840/340). These results are also similar to our reported error of 13.9 pixels.</p></body></sub-article></article>