<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">31873</article-id><article-id pub-id-type="doi">10.7554/eLife.31873</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Integrative and distinctive coding of visual and conceptual object features in the ventral visual stream</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-96981"><name><surname>Martin</surname><given-names>Chris B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7014-4371</contrib-id><email>cmarti97@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-98157"><name><surname>Douglas</surname><given-names>Danielle</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-98158"><name><surname>Newsome</surname><given-names>Rachel N</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-98159"><name><surname>Man</surname><given-names>Louisa LY</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-32294"><name><surname>Barense</surname><given-names>Morgan D</given-names></name><email>barense@psych.utoronto.ca</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Psychology</institution><institution>University of Toronto</institution><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Psychology</institution><institution>Mount Allison University</institution><addr-line><named-content content-type="city">Sackville</named-content></addr-line><country>Canada</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Rotman Research Institute</institution><institution>Baycrest</institution><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Psychology</institution><institution>Queen's University</institution><addr-line><named-content content-type="city">Kingston</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-12465"><name><surname>Rust</surname><given-names>Nicole</given-names></name><role>Reviewing Editor</role><aff id="aff5"><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>02</day><month>02</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e31873</elocation-id><history><date date-type="received" iso-8601-date="2017-09-10"><day>10</day><month>09</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-02-01"><day>01</day><month>02</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Martin et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Martin et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-31873-v3.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="commentary" xlink:href="10.7554/eLife.35663"/><abstract><object-id pub-id-type="doi">10.7554/eLife.31873.001</object-id><p>A significant body of research in cognitive neuroscience is aimed at understanding how object concepts are represented in the human brain. However, it remains unknown whether and where the visual and abstract conceptual features that define an object concept are integrated. We addressed this issue by comparing the neural pattern similarities among object-evoked fMRI responses with behavior-based models that independently captured the visual and conceptual similarities among these stimuli. Our results revealed evidence for distinctive coding of visual features in lateral occipital cortex, and conceptual features in the temporal pole and parahippocampal cortex. By contrast, we found evidence for integrative coding of visual and conceptual object features in perirhinal cortex. The neuroanatomical specificity of this effect was highlighted by results from a searchlight analysis. Taken together, our findings suggest that perirhinal cortex uniquely supports the representation of fully specified object concepts through the integration of their visual and conceptual features.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.31873.002</object-id><title>eLife digest</title><p>Our ability to interact with the world depends in large part on our understanding of objects. But objects that look similar, such as a hairdryer and a gun, may do different things, while objects that look different, such as tape and glue, may have similar roles. The fact that we can effortlessly distinguish between such objects suggests that the brain combines information about an object’s visual and abstract properties.</p><p>Nevertheless, brain imaging experiments show that thinking about what an object looks like activates different brain regions to thinking about abstract knowledge. For example, thinking about an object’s appearance activates areas that support vision, whereas thinking about how to use that object activates regions that control movement. So how does the brain combine these different kinds of information?</p><p>Martin et al. asked healthy volunteers to answer questions about objects while lying inside a brain scanner. Questions about appearance (such as “is a hairdryer angular?”) activated different regions of the brain to questions about abstract knowledge (“is a hairdryer manmade?”). But both types of question also activated a region of the brain called the perirhinal cortex. When volunteers responded to either type of question, the activity in their perirhinal cortex signaled both the physical appearance of the object as well as its abstract properties, even though both types of information were not necessary for the task. This suggests that information in the perirhinal cortex reflects combinations of multiple features of objects.</p><p>These findings provide insights into a neurodegenerative disorder called semantic dementia. Patients with semantic dementia lose their general knowledge about the world. This leads to difficulties interacting with everyday objects. Patients may try to use a fork to comb their hair, for example. Notably, the perirhinal cortex is a brain region that is usually damaged in semantic dementia. Loss of combined information about the visual and abstract properties of objects may lie at the core of the observed impairments.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>semantic memory</kwd><kwd>visual cognition</kwd><kwd>integration</kwd><kwd>fMRI</kwd><kwd>perirhinal cortex</kwd><kwd>ventral visual stream</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001804</institution-id><institution>Canada Research Chairs</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000192</institution-id><institution>Ontario Ministry of Economic Development and Innovation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>PDF - 502437 - 2017</award-id><principal-award-recipient><name><surname>Martin</surname><given-names>Chris B</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Perirhinal cortex, a brain structure located in the medial temporal lobe, uniquely supports the integration of visual and conceptual object information.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Semantic memory imbues the world with meaning and shapes our understanding of the relationships among object concepts. Many neurocognitive models of semantic memory incorporate the notion that object concepts are represented in a feature-based manner (<xref ref-type="bibr" rid="bib81">Rosch and Mervis, 1975</xref>; <xref ref-type="bibr" rid="bib91">Tyler and Moss, 2001</xref>; <xref ref-type="bibr" rid="bib80">Rogers and McClelland, 2004</xref>). On this view, our understanding of the concept ‘hairdryer’ is thought to reflect knowledge of observable perceptual properties (e.g. visual form) and abstract conceptual features (e.g. ‘<italic>used to style hair</italic>’). Importantly, there is not always a one-to-one correspondence between how something looks and what it is; a hairdryer and a comb are conceptually similar despite being visually distinct, whereas a hairdryer and a gun are conceptually distinct despite being visually similar. Thus, a fully-specified representation of an object concept (i.e. one that can be distinguished from any and all other concepts), requires integration of its perceptual and conceptual features.</p><p>Neuroimaging research suggests that object features are coded in the modality-specific cortical regions that supported their processing at the time of acquisition (<xref ref-type="bibr" rid="bib89">Thompson-Schill, 2003</xref>). For example, knowledge about the visual form of an object concept is thought to be coded in occipito-temporal visual processing regions (<xref ref-type="bibr" rid="bib48">Martin and Chao, 2001</xref>). However, neurocognitive models of semantic memory differ with respect to how distributed feature representations relate to fully specified object concepts. On one view, these representations are thought to emerge through interactions among modality-specific cortical areas (<xref ref-type="bibr" rid="bib38">Kiefer and Pulvermüller, 2012</xref>; <xref ref-type="bibr" rid="bib50">Martin, 2016</xref>). Within a competing class of theories, they are thought to reflect the integration of modality-specific features in trans-modal convergence zones (<xref ref-type="bibr" rid="bib22">Damasio, 1989</xref>; <xref ref-type="bibr" rid="bib79">Rogers et al., 2004</xref>; <xref ref-type="bibr" rid="bib11">Binder and Desai, 2011</xref>), such as the anterior temporal lobes (ATL) (<xref ref-type="bibr" rid="bib71">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib90">Tranel, 2009</xref>; <xref ref-type="bibr" rid="bib76">Ralph et al., 2017</xref>).</p><p>The dominant view of the ATL as a semantic hub was initially shaped by neuropsychological investigations in individuals with semantic dementia (SD) (<xref ref-type="bibr" rid="bib71">Patterson et al., 2007</xref>). Behaviorally, SD is characterized by the progressive loss of conceptual knowledge across all receptive and expressive modalities (<xref ref-type="bibr" rid="bib97">Warrington, 1975</xref>; <xref ref-type="bibr" rid="bib35">Hodges et al., 1992</xref>). At the level of neuropathology, SD is associated with extensive atrophy of the ATL, with the earliest and most pronounced volume loss in the left temporal pole (<xref ref-type="bibr" rid="bib61">Mummery et al., 2000</xref>; <xref ref-type="bibr" rid="bib30">Galton et al., 2001</xref>). Most important from a theoretical perspective, patients with SD tend to confuse conceptually similar objects that are visually distinct (e.g. hairdryer – comb), but not visually similar objects that are conceptually distinct (e.g., hairdryer – gun), indicating that the temporal pole expresses conceptual similarity structure (<xref ref-type="bibr" rid="bib32">Graham et al., 1994</xref>; see <xref ref-type="bibr" rid="bib72">Peelen and Caramazza, 2012</xref>; <xref ref-type="bibr" rid="bib18">Chadwick et al., 2016</xref>, for related neuroimaging evidence). Taken together, these findings suggest that the temporal pole supports multi-modal integration of abstract conceptual, but not perceptual, features. Notably, however, a considerable body of research indicates that the temporal pole may not be the only ATL structure that supports feature-based integration.</p><p>The representational-hierarchical model of object coding emphasizes a role for perirhinal cortex (PRC), located in the medial ATL, in feature integration that is distinct from that of the temporal pole (<xref ref-type="bibr" rid="bib65">Murray and Bussey, 1999</xref>). Namely, within this framework PRC is thought to support the integration of conceptual <italic>and</italic> perceptual features. In line with this view, object representations in PRC have been described in terms of conceptual feature conjunctions in studies of semantic memory (<xref ref-type="bibr" rid="bib59">Moss et al., 2005</xref>; <xref ref-type="bibr" rid="bib15">Bruffaerts et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Clarke and Tyler, 2014</xref>; <xref ref-type="bibr" rid="bib21">2015</xref>; <xref ref-type="bibr" rid="bib98">Wright et al., 2015</xref>), and visual feature conjunctions in studies of visual processing (<xref ref-type="bibr" rid="bib6">Barense et al., 2005</xref>; <xref ref-type="bibr" rid="bib7">2007</xref>; <xref ref-type="bibr" rid="bib8">2012</xref>; <xref ref-type="bibr" rid="bib44">Lee et al., 2005</xref>; <xref ref-type="bibr" rid="bib24">Devlin and Price, 2007</xref>; <xref ref-type="bibr" rid="bib64">Murray et al., 2007</xref>; <xref ref-type="bibr" rid="bib68">O'Neil et al., 2009</xref>; <xref ref-type="bibr" rid="bib31">Graham et al., 2010</xref>). However, it is difficult to synthesize results from these parallel lines of research, in part, because conceptual and perceptual features tend to vary concomitantly across stimuli (<xref ref-type="bibr" rid="bib62">Mur, 2014</xref>). For example, demonstrating that ‘horse’ and ‘donkey’ are represented with greater neural pattern similarity in PRC than are ‘horse’ and ‘dolphin’ may reflect differences in conceptual or perceptual relatedness. Thus, although the representational-hierarchical account was initially formalized nearly two decades ago (<xref ref-type="bibr" rid="bib65">Murray and Bussey, 1999</xref>), direct evidence of integration across conceptual and perceptual features remains elusive.</p><p>In the current study, we used fMRI to identify where in the brain visual and conceptual object features are stored, and to determine whether and where they are integrated at the level of fully specified object representations. To this end, we first generated behavior-based models that captured the visual and conceptual similarities among a set of object concepts, ensuring that these dimensions were not confounded across stimuli (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Next, participants were scanned using task contexts that biased attention to either the conceptual or visual features of these well-characterized object concepts (<xref ref-type="fig" rid="fig2">Figure 2</xref>). We then used representational similarity analysis (RSA) (<xref ref-type="bibr" rid="bib41">Kriegeskorte and Kievit, 2013</xref>), implemented using ROI- and searchlight-based approaches, to determine where the brain-based similarity structure among object-evoked multi-voxel activity patterns could be predicted by the similarity structure in the behavior-based visual and conceptual similarity models.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.003</object-id><label>Figure 1.</label><caption><title>Behavior-based RDMs.</title><p>(<bold>A</bold>) Visual similarity rating task (top) and corresponding 40 × 40 behavior-based visual RDM (bottom). (<bold>B</bold>) Conceptual feature generation task (top), abridged feature matrix depicting the feature frequencies across participants for each concept (middle), and corresponding 40 × 40 behavior-based conceptual RDM (bottom). The dashed horizontal arrow between behavior-based RDMs denotes a second-level RSA that compared these similarity models with one another. All object concepts are listed in <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref> - Object concepts and targeted pairs. Behavior-based RDMs (together with the word2vec RDM) are contained in <xref ref-type="supplementary-material" rid="fig1sdata2">Figure 1—source data 2</xref> - Behavior-based RDMs and word2vec RDM.</p><p><supplementary-material id="fig1sdata1"><object-id pub-id-type="doi">10.7554/eLife.31873.004</object-id><label>Figure 1—source data 1.</label><caption><title>Object concepts and targeted pairs.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-31873-fig1-data1-v3.xlsx"/></supplementary-material></p><p><supplementary-material id="fig1sdata2"><object-id pub-id-type="doi">10.7554/eLife.31873.005</object-id><label>Figure 1—source data 2.</label><caption><title>Behavior-based RDMs and word2vec RDM.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-31873-fig1-data2-v3.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig1-v3"/></fig><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.006</object-id><label>Figure 2.</label><caption><title>Brain-based RDMs.</title><p>(<bold>A</bold>) Example of object-evoked neural activity patterns obtained across all eight probes in the visual task context (top), mean object-specific activity patterns averaged across repetitions (middle), and corresponding 40 × 40 brain-based visual task RDM derived from a first-level RSA (bottom). (<bold>B</bold>) Example of object-evoked neural activity patterns obtained across all eight probes in the conceptual task context (top), mean object-specific activity patterns averaged across repetitions (middle), and corresponding 40 × 40 brain-based conceptual task RDM derived from a first-level RSA (bottom).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig2-v3"/></fig><p>We predicted that lateral occipital cortex (LOC), an occipito-temporal region that has been implicated in the processing of visual form (<xref ref-type="bibr" rid="bib34">Grill-Spector et al., 1999</xref>; <xref ref-type="bibr" rid="bib39">Kourtzi and Kanwisher, 2001</xref>; <xref ref-type="bibr" rid="bib57">Milner and Goodale, 2006</xref>), would represent stored visual object features in a visual similarity code. Based on the neurocognitive models of semantic memory reviewed, we predicted that the temporal pole would represent stored conceptual object features in a conceptual similarity code (<xref ref-type="bibr" rid="bib71">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib76">Ralph et al., 2017</xref>). We also predicted conceptual similarity coding in parahippocampal cortex, which has been linked to the representation of the contextually-based co-occurrence of objects (<xref ref-type="bibr" rid="bib5">Bar, 2004</xref>; <xref ref-type="bibr" rid="bib2">Aminoff et al., 2013</xref>). Critically, objects that are regularly encountered in the same context (e.g. ‘comb’ and ‘hairdryer’ in a barbershop) often share many conceptual features (e.g. ‘<italic>used to style hair</italic>’). Thus, to the extent that shared conceptual features directly shape contextual meaning, object-evoked responses in parahippocampal cortex may express conceptual similarity structure. Returning to the primary objective of the study, we predicted that PRC would uniquely represent the visual and conceptual features that define fully-specified object concepts in an integrated similarity code.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavior-based similarity models</title><p>Using a data-driven approach, we first generated behavior-based models that captured the visual and conceptual similarities among 40 targeted object concepts (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Notably, our visual similarity model and conceptual similarity model were derived from behavioral judgments provided by two independent groups of participants. For the purpose of constructing the visual similarity model, the first group of participants (N = 1185) provided pairwise comparative similarity judgments between object concepts (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Specifically, a pair of words was presented on each trial and participants were asked to rate the visual similarity between the object concepts to which they referred using a 5-point Likert scale. Similarity ratings for each pair of object concepts were averaged across participants, normalized, and expressed within a representational dissimilarity matrix (RDM). We refer to this RDM as the <italic>behavior-based visual RDM</italic>.</p><p>For the purpose of constructing the conceptual similarity model, a second group of participants (N = 1600) completed an online feature-generation task (<xref ref-type="bibr" rid="bib55">McRae et al., 2005</xref>; <xref ref-type="bibr" rid="bib88">Taylor et al., 2012</xref>) (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Each participant was asked to generate a list of conceptual features that characterize one object concept (e.g. hairdryer: <italic>‘used to style hair’, ‘found in salons’, ‘electrically powered’, ‘blows hot air’</italic>; comb: <italic>‘used to style hair’, ‘found in salons’, ‘has teeth’, ‘made of plastic’</italic>). Conceptual similarity between all pairs of object concepts was quantified as the cosine angle between the corresponding pairs of feature vectors. With this approach, high cosine similarity between object concepts reflects high conceptual similarity. Cosine similarity values were then expressed within an RDM, which we refer to as the <italic>behavior-based conceptual RDM</italic>.</p><p>We next performed a second-level RSA to quantify the relationship between our behavior-based visual RDM and behavior-based conceptual RDM. This comparison is denoted by the gray arrow between behavior-based RDMs in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Critically, this analysis revealed that the model RDMs were not significantly correlated with one another (Kendall’s tau-a = 0.01, p=0.10), indicating that differences in visual and conceptual features were not confounded across object concepts. In other words, ensuring that these different types of features varied independently across stimuli (e.g. hairdryer – gun; hairdryer – comb), rather than concomitantly (e.g. horse – donkey; horse – dolphin), allowed us to isolate the separate influence of visual and conceptual features on the representational structure of object concepts in the brain. In this example, a hairdryer and a gun are visually similar but conceptually dissimilar, whereas a hairdryer and a comb are visually dissimilar but conceptually similar.</p></sec><sec id="s2-2"><title>Comparison of behavior-based RDMs with a corpus-based (word2vec) semantic RDM</title><p>We next sought to compare our behavior-based RDMs with a corpus-based model of conceptual similarity. To this end, we implemented a word2vec language model, which mapped 3 million words to 300 feature vectors in a high-dimensional space (<xref ref-type="bibr" rid="bib56">Mikolov et al., 2013</xref>). The model was trained using ~100 billion words from a Google News dataset. From this model, we calculated the cosine similarity between feature vectors for all pairs of words in our stimulus set. These data were expressed in a 40 × 40 word2vec RDM (<xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref> contains the word2vec RDM). Importantly, the word2vec RDM was significantly correlated with our behavior-based conceptual RDM (Kendall’s tau-a = 0.11, SE = 0.0141, p&lt;0.00001), suggesting that both models captured the conceptual similarity structure among the object concepts. However, the word2vec RDM was also significantly correlated with our behavior-based visual RDM (Kendall’s tau-a = 0.04, SE = 0.0130, p&lt;0.001). This result suggests that, in line with our objectives, the behavior-based conceptual RDM captured semantic similarity selectively defined as conceptual object features, whereas the word2vec RDM may have captured a broader definition of semantic similarity, that is, one that includes both visual semantics and abstract conceptual features. Consistent with this view, gun and hairdryer were conceptually unrelated in our behavior-based conceptual RDM (cosine = 0), whereas the word2vec RDM suggested modest conceptual similarity (cosine = 0.16). Although this difference is likely determined by multiple factors, it is important to note that gun and hairdryer had a relatively high visual similarity index in our behaviour-based visual RDM (normalized mean rating = 0.58). These data highlight a theoretically important distinction between our behaviorally derived conceptual feature-based statistics and corpus-based estimates of semantic similarity. Specifically, the former allow for distinctions between visual and conceptual object features, whereas corpus-based models may not. fMRI Task and Behavioral Results</p><p>We used fMRI to estimate the representational structure of our 40 object concepts from neural activity patterns in an independent group of participants (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Given our specific interest in understanding pre-existing representations of object concepts rather than bottom-up perceptual processing, all stimuli were presented as words. This approach ensured that conceptual and visual features were extracted from pre-existing representations of object concepts. That is to say, both conceptual and visual features were arbitrarily related to the physical input (i.e. the orthography of the word). By contrast, when pictures are used as stimuli, visual features are accessible from the pictorial cue, whereas conceptual features require abstraction from the cue. Functional brain data were acquired over eight experimental runs, each of which consisted of two blocks of stimulus presentation. All 40 object concepts were presented sequentially within each block, for a total of 16 repetitions per concept. On each trial, participants were asked to make a ‘yes/no’ property verification judgment in relation to a block-specific verification probe. Half of the blocks were associated with verification probes that encouraged processing of visual features (e.g. ‘is the object angular?”), and the other half were associated with verification probes that encouraged processing of conceptual features (e.g. ‘is the object a tool?”). Each run consisted of one visual feature verification block and one conceptual feature verification block, with order counterbalanced across runs. With this experimental design, we were able to characterize neural responses to object concepts across two task contexts: a visual task context (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) and a conceptual task context (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>Behavioral performance on the scanned property verification task indicated that participants interpreted the object concepts and property verification probes with a high degree of consistency (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Specifically, all participants (i.e. 16/16) provided the same yes/no response to the property verification task on 88.4% of all trials. Agreement was highest for the ‘living’ verification probe (96.8%) and lowest for the ‘non-tool’ verification probe (73.2%). Moreover, the proportion of trials on which all participants provided the same response did not differ between the visual feature verification task context (mean = 87.3% collapsed across all eight visual probes) and the conceptual feature verification task context (mean = 89.5% collapsed across all eight conceptual probes) (<italic>z</italic> = 0.19, p=0.85). Response latencies were also comparable across the visual feature verification task context (mean = 1361 ms, SD = 303) and the conceptual feature verification task context (mean = 1376 ms, SD = 315) (<italic>t</italic> (15)=1.00, p=0.33, 95% CI [−49.09, 17.71).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.007</object-id><label>Figure 3.</label><caption><title>fMRI feature verification task performance.</title><p>Percentage of trials on which all participants (i.e. 16/16) provided the same ‘yes/no’ response for each property verification probe.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig3-v3"/></fig></sec><sec id="s2-3"><title>ROI-based RSA: comparison of behavior-based RDMs with brain-based RDMs</title><p>We next quantified pairwise similarities between object-evoked multi-voxel activity patterns using a first-level RSA (<xref ref-type="fig" rid="fig2">Figure 2</xref>). For the purpose of conducting ROI-based RSA, we focused on multi-voxel activity patterns obtained in PRC, the temporal pole, parahippocampal cortex, and LOC. ROIs from a representative participant are presented in <xref ref-type="fig" rid="fig4">Figure 4</xref>. These ROIs were selected a priori based on empirical evidence linking their respective functional characteristics to visual object processing, conceptual object processing, or both. Our primary focus was on PRC, which has been linked to integrative coding of visual object features and conceptual object features across parallel lines of research (<xref ref-type="bibr" rid="bib6">Barense et al., 2005</xref>; <xref ref-type="bibr" rid="bib7">2007</xref>; <xref ref-type="bibr" rid="bib8">2012</xref>; <xref ref-type="bibr" rid="bib44">Lee et al., 2005</xref>; <xref ref-type="bibr" rid="bib68">O'Neil et al., 2009</xref>; <xref ref-type="bibr" rid="bib15">Bruffaerts et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Clarke and Tyler, 2014</xref>; <xref ref-type="bibr" rid="bib21">2015</xref>; <xref ref-type="bibr" rid="bib98">Wright et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Erez et al., 2016</xref>). The temporal pole has primarily been linked to processing of conceptual object properties (<xref ref-type="bibr" rid="bib61">Mummery et al., 2000</xref>; <xref ref-type="bibr" rid="bib30">Galton et al., 2001</xref>; <xref ref-type="bibr" rid="bib71">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib74">Pobric et al., 2007</xref>; <xref ref-type="bibr" rid="bib42">Lambon Ralph et al., 2009</xref>; <xref ref-type="bibr" rid="bib72">Peelen and Caramazza, 2012</xref>; <xref ref-type="bibr" rid="bib18">Chadwick et al., 2016</xref>). Parahippocampal cortex has been implicated in the conceptual processing of contextual associations, including representing the co-occurrence of objects, although its functional contributions remain less well defined than the temporal pole (<xref ref-type="bibr" rid="bib4">Bar and Aminoff, 2003</xref>; <xref ref-type="bibr" rid="bib2">Aminoff et al., 2013</xref>; <xref ref-type="bibr" rid="bib77">Ranganath and Ritchey, 2012</xref>). Lastly, LOC, which is a functionally defined region in occipito-temporal cortex, has been revealed to play a critical role in processing visual form (<xref ref-type="bibr" rid="bib34">Grill-Spector et al., 1999</xref>; <xref ref-type="bibr" rid="bib39">Kourtzi and Kanwisher, 2001</xref>; <xref ref-type="bibr" rid="bib57">Milner and Goodale, 2006</xref>). Because we did not have any a priori predictions regarding hemispheric differences, estimates of neural pattern similarities between object concepts were derived from multi-voxel activity collapsed across ROIs in the left and right hemisphere.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.008</object-id><label>Figure 4.</label><caption><title>ROIs in a representative participant.</title><p>Cortical regions examined in the ROI-based RSAs, including lateral occipital cortex (green), parahippocampal cortex (pink), perirhinal cortex (purple), and the temporal pole (cyan).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig4-v3"/></fig><p>Object-specific multi-voxel activity patterns were estimated in each run using general linear models fit to data from the visual and conceptual task contexts, separately. Mean object-specific responses were then calculated for each task context by averaging across runs. Linear correlation distances (Pearson’s r) were calculated between all pairs of object-specific multi-voxel activity patterns within each task context and expressed in participant-specific <italic>brain-based visual task RDMs</italic> and <italic>brain-based conceptual task RDMs</italic>. The brain-based visual task RDMs captured the neural pattern similarities obtained between all object concepts in the visual task context (i.e. while participants made visual feature verification judgments) (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), and the brain-based conceptual task RDMs captured the neural pattern similarities obtained between all object concepts in the conceptual task context (i.e. while participants made conceptual feature verification judgments) (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>We implemented second-level RSA to compare behavior-based visual and conceptual RDMs with the brain-based visual and conceptual task RDMs (these comparisons are denoted by the solid vertical and diagonal arrows in <xref ref-type="fig" rid="fig5">Figure 5</xref>). All RDMs were compared in each ROI using a ranked correlation coefficient (Kendall’s tau-a) as a similarity index (<xref ref-type="bibr" rid="bib66">Nili et al., 2014</xref>). Inferential statistical analyses were performed using a one-sided Wilcoxon signed-rank test, with participants as a random factor. A Bonferroni correction was applied to adjust for multiple comparisons (4 ROIs x 2 behavior-based RDMs x 2 brain-based RDMs = 16 comparisons, yielding a critical alpha of. 003). With this approach, we revealed that object concepts are represented in three distinct similarity codes that differed across ROIs: a visual similarity code, a conceptual similarity code, and an integrative code. Results from our ROI-based RSA analyses are shown in <xref ref-type="fig" rid="fig6">Figure 6</xref> and discussed in turn below.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.009</object-id><label>Figure 5.</label><caption><title>Second-level RSAs.</title><p>Solid vertical and diagonal arrows reflect second-level RSA in which behavior-based RDMs were compared with brain-based RDMs (ROI-based results in <xref ref-type="fig" rid="fig6">Figure 6</xref>, searchlight-based results in <xref ref-type="fig" rid="fig9">Figures 9</xref>, <xref ref-type="fig" rid="fig10">10</xref> and <xref ref-type="fig" rid="fig11">11</xref>). The dashed horizontal arrow between brain-based RDMs reflects second-level RSA in which neural pattern similarities from each task context were directly compared with each other (results in <xref ref-type="fig" rid="fig7">Figure 7</xref>).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig5-v3"/></fig><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.010</object-id><label>Figure 6.</label><caption><title>Comparison of behavior-based and brain-based RDMs.</title><p><bold>S</bold>imilarities between behavior-based and brain-based RDMs are plotted for (<bold>A</bold>) LOC, (<bold>B</bold>) parahippocampal cortex, (<bold>C</bold>) PRC, and (<bold>D</bold>) the temporal pole. These comparisons are denoted by the solid vertical and diagonal arrows in <xref ref-type="fig" rid="fig5">Figure 5</xref>. Similarity was quantified as the ranked correlation coefficient (Kendall’s tau-a) between behavior-based RDMs and the brain-based RDMs. Error bars indicate standard error of the mean. ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05 (Bonferroni corrected). Participant-specific Kendall’s tau-a co-efficients are contained in <xref ref-type="supplementary-material" rid="fig6sdata1">Figure 6—source data 1</xref> - Comparison of similarity models and brain-based RDMs.</p><p><supplementary-material id="fig6sdata1"><object-id pub-id-type="doi">10.7554/eLife.31873.012</object-id><label>Figure 6—source data 1.</label><caption><title>Comparison of similarity models and brain-based RDMs.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-31873-fig6-data1-v3.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig6-v3"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.31873.011</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Comparison of word2vec RDM with brain-based RDMs.</title><p><bold>S</bold>imilarities between the word2vec RDM and brain-based RDMs are plotted for (<bold>A</bold>) LOC, (<bold>B</bold>) parahippocampal cortex, (<bold>C</bold>) PRC, and (<bold>D</bold>) the temporal pole. Similarity was quantified as the ranked correlation coefficient (Kendall’s tau-a) between behavior-based RDMs and the brain-based RDMs. Error bars indicate standard error of the mean. **p&lt;0.01, *p&lt;0.05 (Bonferroni corrected). Participant-specific Kendall’s tau-a co-efficients are contained in <xref ref-type="supplementary-material" rid="fig6sdata1">Figure 6—source data 1</xref> - Comparison of similarity models and brain-based RDMs.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig6-figsupp1-v3"/></fig></fig-group></sec><sec id="s2-4"><title>Lateral occipital cortex represents object concepts in a task-dependent visual similarity code</title><p>Consistent with its well-established role in the processing of visual form, patterns of activity within LOC reflected the visual similarity of the object concepts (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Specifically, the brain-based visual task RDMs obtained across participants in LOC were significantly correlated with the behavior-based visual RDM (Kendall’s tau-a = 0.045, p&lt;0.002), but not the behavior-based conceptual RDM (Kendall’s tau-a = −0.006, p=0.72). In other words, activity patterns in LOC expressed a visual similarity structure when participants were asked to make explicit judgments about the visual features that characterized object concepts (e.g. whether an object is angular in form). By contrast, the brain-based conceptual task RDMs obtained across participants in LOC were not significantly correlated with either the behavior-based visual RDM (Kendall’s tau-a = 0.006, p=0.13) or the behavior-based conceptual RDM (Kendall’s tau-a = 0.003, p=0.65). That is to say, activity patterns in LOC expressed neither visual nor conceptual similarity structure when participants made judgments that pertained to conceptual object features (e.g. whether an object is naturally occurring). Considered together, these results suggest that LOC represented perceptual information about object concepts in a task-dependent visual similarity code. Specifically, when task demands biased attention toward visual features, signals in LOC generalized across visually related object concepts even when they are conceptually distinct (e.g. hairdryer – gun).</p></sec><sec id="s2-5"><title>Parahippocampal cortex represents object concepts in a task-dependent conceptual similarity code</title><p>Patterns of activity obtained in parahippocampal cortex, which has previously been associated with the processing of semantically-based contextual associations (<xref ref-type="bibr" rid="bib4">Bar and Aminoff, 2003</xref>; <xref ref-type="bibr" rid="bib2">Aminoff et al., 2013</xref>), reflected the conceptual similarity of the object concepts (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). First, the brain-based visual task RDMs obtained across participants in parahippocampal cortex were not significantly correlated with either the behavior-based visual RDM (Kendall’s tau-a = 0.005, p=0.26) or the behavior-based conceptual RDM (Kendall’s tau-a = 0.009, p=0.26). In other words, activity patterns in parahippocampal cortex expressed neither visual nor conceptual similarity structure when participants made judgments that pertained to conceptual object features (e.g. whether an object is symmetrical). Second, the brain-based conceptual task RDMs obtained across participants in parahippocampal cortex were not significantly related to the behavior-based visual RDM (Kendall’s tau-a = −0.008, p=0.55), but they were correlated with the behavior-based conceptual RDM (Kendall’s tau-a = 0.046, p&lt;0.002). Thus, activity patterns in parahippocampal cortex expressed a conceptual similarity structure when participants were asked to make explicit judgments about the conceptual features that characterized object concepts (e.g. whether an object is a tool). Put another way, conceptual information was represented in parahippocampal cortex in a task-dependent manner that generalized across conceptually related object concepts even when they were visually distinct (e.g. hairdryer – comb).</p></sec><sec id="s2-6"><title>The temporal pole represents object concepts in a task-invariant conceptual similarity code</title><p>In line with theoretical frameworks that have characterized the temporal pole as a semantic hub (<xref ref-type="bibr" rid="bib71">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib90">Tranel, 2009</xref>), patterns of activity within this specific ATL structure reflected the conceptual similarity of the object concepts (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). Specifically, whereas the brain-based visual task RDMs obtained across participants in the temporal pole were not significantly correlated with the behavior-based visual RDM (Kendall’s tau-a = 0.006, p=0.25), they were correlated with the behavior-based conceptual RDM (Kendall’s tau-a = 0.035, p&lt;0.001). In other words, activity patterns in the temporal pole expressed a conceptual similarity structure when participants were asked to make explicit judgments about the visual features that characterized object concepts (e.g. whether an object is elongated). Similarly, whereas the brain-based conceptual task RDMs obtained across participants in the temporal pole were not significantly correlated with the behavior-based visual RDM (Kendall’s tau-a = 0.0005, p=0.47), they were correlated with the behavior-based conceptual RDM (Kendall’s tau-a = 0.05, p&lt;0.0001). Thus, activity patterns in the temporal pole expressed a conceptual similarity structure when participants were asked to make explicit judgments about <italic>either</italic> the visual or conceptual features that characterized object concepts (e.g. whether an object is dark in color, or whether an object is pleasant). In other words, conceptual information was represented in the temporal pole in a task-invariant manner that generalized across conceptually related object concepts even when they were visually distinct (e.g. hairdryer – comb).</p></sec><sec id="s2-7"><title>Perirhinal cortex represents object concepts in a task-invariant similarity code that reflects integration of visual and conceptual features</title><p>Results obtained in PRC support the notion that this structure integrates visual and conceptual object features (6C), as first theorized in the representational-hierarchical model of object representation (<xref ref-type="bibr" rid="bib65">Murray and Bussey, 1999</xref>). Namely, we revealed that the brain-based visual task RDMs obtained across participants in PRC were significantly correlated with both the behavior-based visual RDM (Kendall’s tau-a = 0.052, p&lt;0.0001), and the behavior-based conceptual RDM (Kendall’s tau-a = 0.036, p&lt;0.0003). Similarly, the brain-based conceptual task RDMs obtained across participants were also correlated with both the behavior-based visual RDM (Kendall’s tau-a = 0.035, p&lt;0.002), and the behavior-based conceptual RDM (Kendall’s tau-a = 0.057, p&lt;0.0001). In other words, activity patterns in PRC expressed both visual and conceptual similarity structure when participants were asked to make explicit judgments about the visual features that characterized object concepts (e.g. whether an object is round) and when participants were asked to make explicit judgments about the conceptual features that characterized object concepts (e.g. whether an object is manufactured).</p><p>Numerically, patterns of activity in PRC showed more similarity to the behavior-based visual RDM than to the behavior-based conceptual RDM in the visual task context, and vice versa in the conceptual task context. Therefore, we performed a 2 [behavior-based RDMs] x 2 [brain-based task RDMs] repeated measures ANOVA to formally test for an interaction between behavior-based model and fMRI task context. For this purpose, all Kendall’s tau-a values were transformed to Pearson’s <italic>r</italic> co-efficients (<italic>r</italic> = sin (½ π tau-a), <xref ref-type="bibr" rid="bib94">Walker, 2003</xref>), which were then Fisher-z transformed. The task x model interaction neared, but did not reach, significance (F(1,15) = 3.48, p=0.082).</p><p>In sum, these findings indicate that PRC simultaneously expressed both conceptual and visual similarity structure, and did so regardless of whether participants were asked to make targeted assessments of conceptual or visual features. In other words, activity patterns in PRC captured the conceptual similarity between hairdryer and comb, as well as the visual similarity between hairdryer and gun, and did so irrespective of task context. Critically, these results were obtained despite the fact that the brain-based RDMs were orthogonal to one another (i.e. not significantly correlated). Considered together, these results suggest that, of the a priori ROIs considered, PRC represents object concepts at the highest level of specificity through integration of visual and conceptual features.</p></sec><sec id="s2-8"><title>ROI-based RSA: comparison of corpus-based (word2vec) semantic RDM with brain-based RDMs</title><p>For the purpose of comparison, we next examined similarities between the word2vec RDM and the brain-based RDMs using the same procedures described in the previous section. Results are presented in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. These analyses revealed significant positive correlations between the word2vec RDM and the brain-based conceptual task RDMs in parahippocampal cortex (Kendall’s tau-a = 0.05, p&lt;0.01), PRC (Kendall’s tau-a = 0.035, p&lt;0.01), and the temporal pole (Kendall’s tau-a = 0.029, p&lt;0.01). The word2vec RDM was also significantly correlated with the brain-based visual task RDMs in PRC (Kendall’s tau-a = 0.025, p&lt;0.05) and the temporal pole (Kendall’s tau-a = 0.027, p&lt;0.05). Notably, this pattern of results was identical to that obtained using the behavior-based conceptual RDMs in parahippocampal cortex, PRC, and the temporal pole. Interestingly, however, the word2vec RDM was also significantly correlated with the brain-based visual task RDMs in LOC (Kendall’s tau-a = 0.028, p&lt;0.05). This result is consistent with the observation that the word2vec RDM was significantly correlated with our behavior-based visual RDM, and further suggests that corpus-based models of semantic memory likely capture similarities between object concepts at the level of abstract conceptual properties and visual semantics.</p></sec><sec id="s2-9"><title>ROI-based RSA: comparisons of brain-based RDMs within ROIs</title><p>Having examined the relationships between behavior-based RDMs and brain-based RDMs, we next sought to directly characterize the relationships between brain-based conceptual and visual RDMs within each ROI (these comparisons are denoted by the dashed horizontal arrow in the bottom of <xref ref-type="fig" rid="fig5">Figure 5</xref>). These analyses were conducted using the same methodological procedures used to compare behavior-based RDMs with brain-based RDMs in the previous section. A Bonferroni correction was applied to adjust for multiple comparisons (16 brain-based comparisons, yielding a critical alpha of. 003). Using second-level RSAs, we asked whether the brain-based visual task RDMs and brain-based conceptual task RDMs had a common similarity structure within a given ROI. Results are plotted in <xref ref-type="fig" rid="fig7">Figure 7A</xref>. Importantly, we found a significant positive correlation in PRC (Kendall’s tau-a = 0.063, p&lt;0.0001), and a trend toward a significant correlation in the temporal pole (Kendall’s tau-a = 0.032, p=0.012). Conversely, brain-based visual and conceptual task RDMs were not significantly correlated in either parahippocampal cortex (Kendall’s tau-a = 0.008, p=0.12), or LOC (Kendall’s tau-a = −0.008, p=0.92). These results suggest that object concepts were represented similarly within PRC, and to a lesser extent within the temporal pole, regardless of whether they were encountered in a visual or conceptual task context.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.013</object-id><label>Figure 7.</label><caption><title>Comparison of brain-based RDMs.</title><p>(<bold>A</bold>) Similarities between brain-based visual task RDMs and brain-based conceptual task RDMs within lateral occipital cortex (LOC), parahippocampal cortex (PHC), perirhinal cortex (PRC), and the temporal pole (TP). These comparisons are denoted by the dashed horizontal arrow in the bottom of <xref ref-type="fig" rid="fig5">Figure 5</xref>. (<bold>B</bold>) Similarities between brain-based visual task RDMs across different ROIs. Labels on the x-axis denote the ROIs being compared. (<bold>C</bold>) Similarities between brain-based conceptual task RDMs across different ROIs. Similarity was quantified as the ranked correlation coefficient (Kendall’s tau-a) between behavior-based RDMs and the brain-based RDMs. Error bars indicate standard error of the mean. ***p&lt;0.001 (Bonferroni corrected),~p &lt; 0.05 (uncorrected). Participant-specific Kendall’s tau-a co-efficients are contained in <xref ref-type="supplementary-material" rid="fig7sdata1">Figure 7—source data 1</xref> - Comparison of brain-based RDMs.</p><p><supplementary-material id="fig7sdata1"><object-id pub-id-type="doi">10.7554/eLife.31873.014</object-id><label>Figure 7—source data 1.</label><caption><title>Comparison of brain-based RDMs.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-31873-fig7-data1-v3.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig7-v3"/></fig></sec><sec id="s2-10"><title>ROI-based RSA: comparisons of brain-based RDMs across ROIs</title><p>We next conducted second-level RSAs to quantify representational similarities between the brain-based visual task RDMs obtained across different ROIs. In other words, we asked whether activity in different ROIs (e.g. PRC and LOC) reflected similar representational distinctions across object concepts within the visual task context. Results are plotted in <xref ref-type="fig" rid="fig7">Figure 7B</xref>. Interestingly, these analyses did not reveal any significant results between any of our ROIs (all Kendall’s tau-a &lt;0.01, all p&gt;0.07). These findings indicate that PRC and LOC, two regions that expressed a visual similarity code, represented different aspects of the visual object features.</p><p>Finally, we quantified the representational similarities between the brain-based conceptual task RDMs obtained across different ROIs. In other words, we asked whether activity in different ROIs (e.g. PRC and the temporal pole) reflected similar representational distinctions across object concepts within the conceptual task context. Results are plotted in <xref ref-type="fig" rid="fig7">Figure 7C</xref>. This set of analyses did not reveal any significant results between any of our ROIs (all Kendall’s tau-a &lt;0.016, all p&gt;0.012). These findings indicate that the three regions that expressed a conceptual similarity code (i.e., PRC, parahippocampal cortex, and temporal pole), represented different aspects of the conceptual object features.</p></sec><sec id="s2-11"><title>ROI-based RSA: comparisons of within-object multi-voxel activity patterns across different task contexts</title><p>The RSAs reported thus far have quantified relationships among behavior-based and brain-based RDMs that reflected similarities between different object concepts (e.g. between ‘hairdryer’ and ‘comb’). We next quantified within-object similarities (e.g. between ‘hairdryer’ and ‘hairdryer’) across visual and conceptual task contexts (e.g. ‘is it living?’ or ‘is it angular?”) using first-level RSAs. Specifically, we calculated one dissimilarity value (1 – Pearson’s <italic>r</italic>) between the mean multi-voxel activity patterns evoked by a given object concept across different task contexts. These 40 within-object dissimilarity values were expressed along the diagonal of an RDM for each ROI in each participant, separately (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). We next calculated mean within-object dissimilarity by averaging across the diagonal of each RDM for the purpose of performing statistical inference.</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.015</object-id><label>Figure 8.</label><caption><title>Comparison of within-object multi-voxel activity patterns across different task contexts.</title><p>(<bold>A</bold>) Depiction of first-level RSA procedure for quantifying within-object multi-voxel activity patterns across the visual and conceptual task contexts. (<bold>B</bold>) Mean similarities between within-object multi-voxel activity patterns across different task contexts within each region of interest. Similarity was quantified as the linear correlation coefficient (Pearson’s <italic>r</italic>) between object-evoked multi-voxel activity patterns. Lateral occipital cortex (LOC), parahippocampal cortex (PHC), perirhinal cortex (PRC), and the temporal pole (TP). Error bars indicate standard error of the mean. **p&lt;0.01, *p&lt;0.05 (Bonferroni corrected). Participant-specific Pearson’s <italic>r</italic> co-efficients are contained in <xref ref-type="supplementary-material" rid="fig8sdata1">Figure 8—source data 1</xref> - Comparison of within-object similarity across task contexts.</p><p><supplementary-material id="fig8sdata1"><object-id pub-id-type="doi">10.7554/eLife.31873.016</object-id><label>Figure 8—source data 1.</label><caption><title>Comparison of within-object similarity across task contexts.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-31873-fig8-data1-v3.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig8-v3"/></fig><p>Results are presented in <xref ref-type="fig" rid="fig8">Figure 8B</xref>. Within-object similarity did not differ from zero in either LOC (Pearson’s <italic>r</italic> = 0.007, p=0.20) or parahippocampal cortex (Pearson’s <italic>r</italic> = −0.008, p=0.87), suggesting that a given object concept was represented differently across the visual and conceptual task contexts in these ROIs. These findings are consistent with the task-dependent nature of the similarity codes we observed in these regions (<xref ref-type="fig" rid="fig6">Figure 6A and B</xref>). Conversely, within-object similarity was significantly greater than zero in the temporal pole (Pearson’s <italic>r</italic> = 0.34, p&lt;0.05, Bonferroni corrected for four comparisons), indicating that this structure represents a given object concept similarly across different task contexts. This observation is consistent with results from the previous section which revealed that the similarities between object concepts in the temporal pole are preserved across task contexts (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). These findings reflected the fact that the same conceptual object information (e.g. ‘<italic>used to style hair</italic>’ and ‘<italic>found in salons</italic>’) was carried in multi-voxel activity patterns obtained in each task context (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). Within-object similarity was also significantly greater than zero in PRC (Pearson’s <italic>r</italic> = 0.41, p&lt;0.01, Bonferroni corrected for four comparisons), again indicating that a given object concept was represented similarly across different task contexts. This finding dovetails with our result from the previous section which revealed that the similarities between object concepts in PRC were preserved across task contexts (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). When considered together, we interpret this pattern of results in PRC as further evidence of integrative coding, reflecting the fact that this structure carried the same conceptual (e.g. ‘<italic>used to style hair</italic>’ and ‘<italic><underline>found in salons</underline></italic>’) and visual (e.g. visually similar to a gun) object information in both task contexts (<xref ref-type="fig" rid="fig6">Figure 6C</xref>).</p></sec><sec id="s2-12"><title>Searchlight-based RSA: comparisons of behavior-based RDMs with brain-based RDMs</title><sec id="s2-12-1"><title>Perirhinal cortex is the only cortical region that supports integrative coding of conceptual and visual object features</title><p>We next implemented a whole-volume searchlight-based RSA to further characterize the neuroanatomical specificity of our ROI-based results. Specifically, we sought to determine whether object representations in PRC expressed visual and conceptual similarity structure within overlapping or distinct populations of voxels. If PRC does indeed support the integrative coding of visual and conceptual object features, then the same set of voxels should express both types of similarity codes. If PRC does not support the integrative coding of visual and conceptual object features, then different subsets of voxels should express these different similarity codes. More generally, data-driven searchlight mapping allowed us to explore whether any other regions of the brain showed evidence for integrative coding of visual and conceptual features in a manner comparable to that observed in PRC. To this end, we performed searchlight RSA using multi-voxel activity patterns restricted to a 100 voxel ROI that was iteratively swept across the entire cortical surface (<xref ref-type="bibr" rid="bib40">Kriegeskorte et al., 2006</xref>; <xref ref-type="bibr" rid="bib70">Oosterhof et al., 2011</xref>). In each searchlight ROI, the behavior-based RDMs were compared with the brain-based RDMs using a procedure identical to that implemented in our ROI-based RSA. These comparisons are depicted by the solid black vertical and diagonal arrows in <xref ref-type="fig" rid="fig5">Figure 5</xref>. The obtained similarity values (Pearson’s <italic>r</italic>) were Fisher-<italic>z</italic> transformed and mapped to the center of each ROI for each participant separately. With this approach, we obtained participant-specific similarity maps for all comparisons, which were then standardized and subjected to a group-level statistical analysis. A threshold-free cluster enhancement (TFCE) method was used to correct for multiple comparisons with a cluster threshold of p&lt;0.05 (<xref ref-type="bibr" rid="bib83">Smith and Nichols, 2009</xref>).</p><p>Statistically thresholded group-level similarity maps depicting cortical regions in which behavior- and brain-based RDMs were significantly correlated are presented for the visual task context in <xref ref-type="fig" rid="fig9">Figure 9</xref>, and for the conceptual task context in <xref ref-type="fig" rid="fig10">Figure 10</xref>. Corresponding cluster statistics, co-ordinates, and neuroanatomical labels are reported in <xref ref-type="table" rid="table1">Table 1</xref>. Importantly, results from our whole-volume searchlight mapping analysis showed a high degree of consistency with our ROI-based results. First, we found evidence for visual similarity coding in the visual task context in aspects of right LOC (<xref ref-type="fig" rid="fig9">Figure 9A</xref>), as well as aspects of early visual cortex, posterior parietal cortex, and areas of medial and lateral ventral temporal cortex (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). Next, we revealed conceptual similarity coding in the conceptual task context within a cluster of voxels that straddled the border between right parahippocampal cortex and PRC (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). Although this cluster was only partially situated with parahippocampal cortex, it is interesting to note that its posterior extent did slightly encroach upon anterior aspects of the parahippocampal place area (PPA; functionally defined using a group-level GLM (scenes &gt; objects); <xref ref-type="bibr" rid="bib26">Epstein and Kanwisher, 1998</xref>), which has previously been linked to the representation of abstract conceptual information (<xref ref-type="bibr" rid="bib1">Aminoff et al., 2007</xref>; <xref ref-type="bibr" rid="bib3">Baldassano et al., 2013</xref>; <xref ref-type="bibr" rid="bib47">Marchette et al., 2015</xref>). Moreover, we found evidence of conceptual similarity coding in bilateral aspects of the temporal pole in both task contexts (<xref ref-type="fig" rid="fig9">Figures 9B</xref> and <xref ref-type="fig" rid="fig10">10B</xref>), an observation that is consistent with results from multiple prior studies that have demonstrated conceptual similarity structure this aspect of the ATL (<xref ref-type="bibr" rid="bib72">Peelen and Caramazza, 2012</xref>; <xref ref-type="bibr" rid="bib18">Chadwick et al., 2016</xref>; cf <xref ref-type="bibr" rid="bib28">Fairhall and Caramazza, 2013</xref>). Finally, and most importantly, results from the whole-brain searchlight revealed evidence for visual similarity coding and conceptual similarity coding in PRC in both task contexts (<xref ref-type="fig" rid="fig9">Figures 9</xref> and <xref ref-type="fig" rid="fig10">10</xref>). This result dovetails with findings from previous RSA-based fMRI research that has demonstrated conceptual similarity coding in PRC (<xref ref-type="bibr" rid="bib15">Bruffaerts et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Clarke and Tyler, 2014</xref>; cf <xref ref-type="bibr" rid="bib28">Fairhall and Caramazza, 2013</xref>).</p><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.017</object-id><label>Figure 9.</label><caption><title>Visual task context representational similarity searchlight mapping results.</title><p>(<bold>A</bold>) Cortical regions in which the brain-based visual task RDMs were significantly correlated with the behavior-based visual RDM. (<bold>B</bold>) Cortical regions in which the brain-based visual task RDMs were significantly correlated with the behavior-based conceptual RDM. The correlation coefficients (Kendall’s tau-a) obtained between behavior-based RDMs and brain-based RDMs were Fisher-<italic>z</italic> transformed and mapped to the voxel at the centre of each searchlight. Similarity maps were corrected for multiple comparisons using threshold-free cluster enhancement with a corrected statistical threshold of p&lt;0.05 on the cluster level (<xref ref-type="bibr" rid="bib83">Smith and Nichols, 2009</xref>). Outlines are shown for the lateral occipital cortex (green), parahippocampal cortex (pink), perirhinal cortex (purple), and the temporal pole (cyan).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig9-v3"/></fig><fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.018</object-id><label>Figure 10.</label><caption><title>Conceptual task context representational similarity searchlight mapping results.</title><p>(<bold>A</bold>) Cortical regions in which the brain-based conceptual task RDMs were significantly correlated with the behavior-based visual RDM. (<bold>B</bold>) Cortical regions in which the brain-based conceptual task RDMs were significantly correlated with the behavior-based conceptual RDM. The correlation coefficients (Kendall’s tau-a) obtained between behavior-based RDMs and brain-based RDMs were Fisher-<italic>z</italic> transformed and mapped to the voxel at the centre of each searchlight. Similarity maps were corrected for multiple comparisons using threshold-free cluster enhancement with a corrected statistical threshold of p&lt;0.05 on the cluster level (<xref ref-type="bibr" rid="bib83">Smith and Nichols, 2009</xref>). Outlines are shown for the lateral occipital cortex (green), parahippocampal cortex (pink), perirhinal cortex (purple), and the temporal pole (cyan).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig10-v3"/></fig><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.019</object-id><label>Table 1.</label><caption><title>Clusters in which behavior-based RDMs were significantly correlated with brain-based RDMs as revealed using representational similarity searchlight analyses, with corresponding cluster extent, peak <italic>z</italic>-values, and MNI co-ordinates<sup>1</sup>.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Region</th><th colspan="3">Cluster extent</th><th>Peak <italic>z</italic>-value</th><th>X</th><th>Y</th><th>Z</th></tr></thead><tbody><tr><td colspan="8"><bold>Visual task context</bold></td></tr><tr><td colspan="8"><italic>Behavior-Based Visual RDM – Brain-Based Visual Task RDM</italic></td></tr><tr><td colspan="2" valign="top">Mid calcarine</td><td colspan="2" valign="top">1660</td><td valign="top">5.79</td><td valign="top">-2</td><td valign="top">−74</td><td valign="top">12</td></tr><tr><td colspan="2" valign="top">R lateral occipital cortex</td><td colspan="2" valign="top">455</td><td valign="top">3.89</td><td valign="top">50</td><td valign="top">−66</td><td valign="top">4</td></tr><tr><td colspan="2" valign="top">R perirhinal cortex</td><td colspan="2" valign="top">112</td><td valign="top">3.64</td><td valign="top">34</td><td valign="top">−12</td><td valign="top">−34</td></tr><tr><td colspan="2" valign="top">L superior parietal lobule</td><td colspan="2" valign="top">110</td><td valign="top">3.21</td><td valign="top">−32</td><td valign="top">−40</td><td valign="top">44</td></tr><tr><td colspan="2" valign="top">L perirhinal cortex</td><td colspan="2" valign="top">76</td><td valign="top">2.85</td><td valign="top">−30</td><td valign="top">−12</td><td valign="top">−36</td></tr><tr><td colspan="2" valign="top">R superior parietal lobule</td><td colspan="2" valign="top">48</td><td valign="top">2.64</td><td valign="top">38</td><td valign="top">−54</td><td valign="top">54</td></tr><tr><td colspan="2" valign="top">R fusiform gyrus</td><td colspan="2" valign="top">45</td><td valign="top">2.77</td><td valign="top">40</td><td valign="top">−46</td><td valign="top">−20</td></tr><tr><td colspan="2" valign="top">R precuneus</td><td colspan="2" valign="top">29</td><td valign="top">2.66</td><td valign="top">12</td><td valign="top">−76</td><td valign="top">48</td></tr><tr><td colspan="2" valign="top">R Inferior Temporal Gyrus</td><td colspan="2" valign="top">9</td><td valign="top">2.52</td><td valign="top">44</td><td valign="top">−22</td><td valign="top">−28</td></tr><tr><td colspan="8"><italic>Behavior-Based Conceptual RDM – Brain-Based Visual Task RDM</italic></td></tr><tr><td colspan="3" valign="top">L Perirhinal Cortex</td><td valign="top">368</td><td valign="top">3.96</td><td valign="top">−24</td><td valign="top">2</td><td valign="top">−38</td></tr><tr><td colspan="3" valign="top">R Perirhinal Cortex</td><td valign="top">232</td><td valign="top">3.26</td><td valign="top">22</td><td valign="top">2</td><td valign="top">−36</td></tr><tr><td colspan="8"><italic>Overlap</italic></td></tr><tr><td colspan="3" valign="top">L Perirhinal Cortex</td><td valign="top">22</td><td valign="top"/><td valign="top">−30</td><td valign="top">-8</td><td valign="top">−38</td></tr><tr><td colspan="8"><bold>Conceptual task context</bold></td></tr><tr><td colspan="8"><italic>Behavior-Based Conceptual RDM – Brain-Based Conceptual Task RDM</italic></td></tr><tr><td colspan="3" valign="top">L Perirhinal Cortex</td><td valign="top">79</td><td valign="top">2.88</td><td valign="top">−30</td><td valign="top">−10</td><td valign="top">−34</td></tr><tr><td colspan="3" valign="top">R Parahippocampal Cortex</td><td valign="top">64</td><td valign="top">2.94</td><td valign="top">30</td><td valign="top">−24</td><td valign="top">−24</td></tr><tr><td colspan="3" valign="top">L Temporal Pole</td><td valign="top">61</td><td valign="top">2.89</td><td valign="top">−34</td><td valign="top">4</td><td valign="top">−26</td></tr><tr><td colspan="3" valign="top">R Temporal Pole</td><td valign="top">25</td><td valign="top">2.70</td><td valign="top">24</td><td valign="top">12</td><td valign="top">−36</td></tr><tr><td colspan="8"><italic>Behavior-Based Visual RDM – Brain-Based Conceptual Task RDM</italic></td></tr><tr><td colspan="3" valign="top">L Perirhinal Cortex</td><td valign="top">98</td><td valign="top">4.87</td><td valign="top">−26</td><td valign="top">-4</td><td valign="top">−10</td></tr><tr><td colspan="3" valign="top">R Perirhinal Cortex</td><td valign="top">26</td><td valign="top">3.01</td><td valign="top">28</td><td valign="top">−12</td><td valign="top">−34</td></tr><tr><td colspan="8"><italic>Overlap</italic></td></tr><tr><td colspan="3" valign="top">L Perirhinal Cortex</td><td valign="top">31</td><td valign="top"/><td valign="top">−26</td><td valign="top">-8</td><td valign="top">−42</td></tr><tr><td colspan="8"><bold>Overlap across all Behavior-Based RDMs and Brain-Based RDMs</bold></td></tr><tr><td colspan="3" valign="top">L Perirhinal Cortex</td><td valign="top">16</td><td valign="top"/><td valign="top">−30</td><td valign="top">-8</td><td valign="top">−36</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup>MNI co-ordinates are reported for the peak voxel in individual clusters and the centre of mass for cluster overlap.</p></fn></table-wrap-foot></table-wrap><p>Although suggestive, neither the searchlight- nor the ROI-based RSA results presented thus far necessarily imply integrative coding in PRC. Indeed, it is possible that visual and conceptual object information was carried in spatially distinct sub-regions of this structure. To examine this issue, we first asked whether any voxels showed both visual and conceptual similarity coding in the visual task context (similarity maps in <xref ref-type="fig" rid="fig9">Figure 9A and B</xref>, respectively) using a voxel overlap analysis (<xref ref-type="fig" rid="fig11">Figure 11A</xref>). Importantly, we revealed a contiguous cluster of voxels that was unique to left PRC in which both behavior-based RDMs predicted the similarity structure in the brain-based visual task RDMs. This result indicated that a subset of voxels in PRC carried information about visual <italic>and</italic> conceptual object information even when task demands biased attention toward visual object features. We next asked whether any voxels showed both visual and conceptual similarity coding in the conceptual task context (similarity maps in <xref ref-type="fig" rid="fig10">Figure —figure supplement 10A and B</xref>, respectively) using a second voxel overlap analysis (<xref ref-type="fig" rid="fig11">Figure 11B</xref>). This analysis also revealed a contiguous cluster of voxels that was unique to left PRC in which both behavior-based RDMs predicted the similarity structure in the brain-based conceptual task RDMs. This finding indicated that a subset of voxels in PRC carried information about visual <italic>and</italic> conceptual object information when task demands biased attention toward conceptual object features.</p><fig id="fig11" position="float"><object-id pub-id-type="doi">10.7554/eLife.31873.020</object-id><label>Figure 11.</label><caption><title>Overlap of searchlight similarity maps.</title><p>(<bold>A</bold>) Overlap between similarity maps obtained in the visual task context (i.e. overlapping voxels from <xref ref-type="fig" rid="fig9">Figure 9A and B</xref>). (<bold>B</bold>) Overlap between similarity maps obtained in the conceptual task context (i.e. overlapping voxels from <xref ref-type="fig" rid="fig10">Figure 10A and B</xref>). (<bold>C</bold>) Overlap across brain-behavior similarity maps across both task contexts (i.e. overlapping voxels from <xref ref-type="fig" rid="fig9">Figures 9A, B</xref>, <xref ref-type="fig" rid="fig10">10A and B</xref>). Outlines are shown for the lateral occipital cortex (green), parahippocampal cortex (pink), perirhinal cortex (purple), and the temporal pole (cyan).</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-31873-fig11-v3"/></fig><p>In a final step using a third voxel overlap analysis, we examined whether any voxels showed both visual and conceptual similarity coding in both the visual and conceptual task contexts (<xref ref-type="fig" rid="fig11">Figure 11C</xref>). This analysis revealed a contiguous cluster of voxels in left PRC in which both behavior-based RDMs predicted the similarity structure captured by both brain-based RDMs. This result indicated that a subset of voxels that were unique to PRC carried information about visual <italic>and</italic> conceptual object information regardless of whether task demands biased attention toward visual <italic>or</italic> conceptual object features. Ultimately, this pattern of results suggests that not only does PRC carry both visual and conceptual object information, but it does so in the same subset of voxels.</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Decades of research has been aimed at understanding how object concepts are represented in the brain (<xref ref-type="bibr" rid="bib97">Warrington, 1975</xref>; <xref ref-type="bibr" rid="bib35">Hodges et al., 1992</xref>; <xref ref-type="bibr" rid="bib49">Martin et al., 1995</xref>; <xref ref-type="bibr" rid="bib65">Murray and Bussey, 1999</xref>; <xref ref-type="bibr" rid="bib19">Chen et al., 2017</xref>), yet the fundamental question of whether and where their visual and conceptual features are integrated remains unanswered. Progress toward this end has been hindered by the fact that these features tend to vary concomitantly across object concepts. Here, we used a data-driven approach to systematically select a set of object concepts in which visual and conceptual features varied independently (e.g. hairdryer – comb, which are conceptually similar but visually distinct; hairdryer – gun, which are visually similar but conceptually distinct). Using RSA of fMRI data, we revealed novel evidence of task-dependent visual similarity coding in LOC, task-dependent conceptual similarity coding in parahippocampal cortex, task-invariant coding in the temporal pole, and task-invariant integrative coding in PRC.</p><p>Several aspects of our data provide novel support for the notion that PRC uniquely represents the visual and conceptual features that define fully specified object concepts in an integrated similarity code. First, this was the only region of the brain in which both visual <italic>and</italic> conceptual object coding was revealed. Moreover, these effects were observed regardless of whether fMRI task demands biased attention toward visual <italic>or</italic> conceptual object features. These results are particularly striking given the fact that they were revealed using a behavior-based visual similarity model and a behavior-based conceptual similarity model that were orthogonal to one another. In other words, the degree of similarity between multi-voxel activity patterns obtained while participants made conceptual judgments, such as whether a ‘hairdryer’ is man-made or a ‘gun’ is pleasant, was captured by the degree of visual similarity between these object concepts. Likewise, the degree of similarity between multi-voxel activity patterns obtained while participants made visual judgments, such as whether a ‘hairdryer’ is angular or a ‘comb’ is elongated, was captured by the degree of conceptual similarity between these object concepts. In both cases, PRC carried information about pre-existing representations of object features that were neither required to perform the immediate task at hand, nor correlated with the features that did in fact have task-relevant diagnostic value. Moreover, we also found that the brain-based visual task RDMs and brain-based conceptual task RDMs were correlated with one another across task contexts in PRC. That is to say, the similarity between ‘hairdryer’ and ‘gun’ was comparable regardless of whether task demands biased attention toward visual or conceptual features. Likewise, we also revealed that PRC also represented a given object concept similarly across task contexts, that is, ‘hairdryer’ evoked a pattern of activation that was comparable across task contexts. When considered together, these results suggest that, at the level of PRC, it may not be possible to fully disentangle conceptual and perceptual information. An important but challenging objective for future research will be to determine whether this pattern of results can be replicated at the level of individual neurons.</p><p>What is the behavioral relevance of fully specified object representations in which visual and conceptual features are integrated? It has previously been suggested that such representations allow for discrimination among stimuli with extensive feature overlap, such as exemplars from the same category (<xref ref-type="bibr" rid="bib65">Murray and Bussey, 1999</xref>; <xref ref-type="bibr" rid="bib67">Noppeney et al., 2007</xref>; <xref ref-type="bibr" rid="bib31">Graham et al., 2010</xref>; <xref ref-type="bibr" rid="bib21">Clarke and Tyler, 2015</xref>). In line with this view, individuals with medial ATL lesions that include PRC typically have more pronounced conceptual impairments related to living than non-living things (<xref ref-type="bibr" rid="bib96">Warrington and Shallice, 1984</xref>; <xref ref-type="bibr" rid="bib60">Moss et al., 1997</xref>; <xref ref-type="bibr" rid="bib14">Bozeat et al., 2003</xref>), and more striking perceptual impairments for objects that are visually similar as compared to visually distinct (<xref ref-type="bibr" rid="bib7">Barense et al., 2007</xref>, <xref ref-type="bibr" rid="bib10">Barense et al., 2010</xref>; <xref ref-type="bibr" rid="bib43">Lee et al., 2006</xref>). Functional MRI studies in neurologically healthy individuals have also demonstrated increased PRC engagement for living as compared to non-living objects (<xref ref-type="bibr" rid="bib59">Moss et al., 2005</xref>), for known as compared to novel faces (<xref ref-type="bibr" rid="bib9">Barense et al., 2011</xref>; <xref ref-type="bibr" rid="bib73">Peterson et al., 2012</xref>), and for faces or conceptually meaningless stimuli with high feature overlap as compared to low feature overlap (<xref ref-type="bibr" rid="bib68">O'Neil et al., 2009</xref>; <xref ref-type="bibr" rid="bib8">Barense et al., 2012</xref>). In a related manner, fully specified object representations in PRC have also been implicated in long-term memory judgments. For example, PRC has been linked to explicit recognition memory judgments when previously studied and novel items are from the same stimulus category (<xref ref-type="bibr" rid="bib52">Martin et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">2016</xref>; <xref ref-type="bibr" rid="bib53">2018</xref>), and when subjects make judgments about their lifetime of experience with a given object concept (<xref ref-type="bibr" rid="bib25">Duke et al., 2017</xref>). Common among these task demands is the requirement to discriminate among highly similar stimuli. In such scenarios, a fully specified representation that reflects the integration of perceptual and conceptual features necessarily enables more fine-grained distinctions than a purely perceptual or conceptual representation.</p><p>This study also has significant implications for prominent neurocognitive models of semantic memory in which the ATL is characterized as a semantic hub (<xref ref-type="bibr" rid="bib78">Rogers et al., 2006</xref>; <xref ref-type="bibr" rid="bib71">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib90">Tranel, 2009</xref>). On this view, the bilateral ATLs are thought to constitute a trans-modal convergence zone that abstracts conceptual information from the co-occurrence of features otherwise represented in a distributed manner across modality-specific cortical nodes. Consistent with this idea, we have shown that a behavior-based conceptual similarity model predicted the similarity structure of neural activity patterns in the temporal pole, irrespective of task context. Specifically, neural activity patterns associated with conceptually similar object concepts that are visually distinct (e.g. hairdryer – comb) were more comparable than were conceptually dissimilar concepts that are visually similar (e.g. hairdryer – gun), even when task demands required a critical assessment of visual features. This observation, together with results obtained in PRC, demonstrates a representational distinction between these ATL structures, a conclusion that dovetails with recent evidence indicating that this region is not functionally homogeneous (<xref ref-type="bibr" rid="bib12">Binney et al., 2010</xref>; <xref ref-type="bibr" rid="bib63">Murphy et al., 2017</xref>). Ultimately, this outcome suggests that some ATL sub-regions play a prominent role in task-invariant extraction of conceptual object properties (e.g. temporal pole), whereas others appear to make differential contributions to the task-invariant integration of perceptual and conceptual features (e.g. PRC) (<xref ref-type="bibr" rid="bib76">Ralph et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Chen et al., 2017</xref>).</p><p>Convergent evidence from studies of functional and structural connectivity in humans, non-human primates, and rodents have revealed that PRC is connected to the temporal pole, parahippocampal cortex, LOC, and nearly all other unimodal and polymodal sensory regions in neocortex (<xref ref-type="bibr" rid="bib86">Suzuki and Amaral, 1994</xref>; <xref ref-type="bibr" rid="bib17">Burwell and Amaral, 1998</xref>; <xref ref-type="bibr" rid="bib37">Kahn et al., 2008</xref>; <xref ref-type="bibr" rid="bib54">McLelland et al., 2014</xref>; <xref ref-type="bibr" rid="bib87">Suzuki and Naya, 2014</xref>; <xref ref-type="bibr" rid="bib95">Wang et al., 2016</xref>; <xref ref-type="bibr" rid="bib99">Zhuo et al., 2016</xref>). Importantly, our results have linked LOC to the representation of visual object features, and the temporal pole and parahippocampal cortex to the representation of conceptual object features. Thus, PRC has the connectivity properties that make it well suited to be a trans-modal convergence zone capable of integrating object features that are both visual and conceptual in nature. An interesting challenge for future research will be to determine how differentially attending to specific types of object features shapes functional connectivity profiles between these regions.</p><p>Although speculative, results from the current study suggest that attention may modulate information both within and between the ROIs examined. First, we see visual similarity coding in LOC only when task demands biased attention to visual object features, and conceptual similarity coding in parahippocampal cortex only when task demands biased attention to conceptual object features. Second, we saw a trend toward an interaction between behavior-based models and fMRI task context in PRC, such that visual similarity coding was more pronounced in the visual task context than was conceptual similarity coding, and vice versa. Thus, attending to specific types of features did not merely manifest as univariate gain modulation. Rather, attention appeared to modulated multi-voxel activity patterns.</p><p>Another novel aspect of our findings is that parahippocampal cortex exhibited conceptual similarity coding in the conceptual task context. Interestingly, it has been suggested that this structure broadly contributes to cognition by processing contextual associations, including the co-occurrence of objects within a context (<xref ref-type="bibr" rid="bib5">Bar, 2004</xref>; <xref ref-type="bibr" rid="bib2">Aminoff et al., 2013</xref>). Critically, objects that regularly co-occur in the same context (e.g. ‘comb’ and ‘hairdryer’ in a barbershop) often share many conceptual features (e.g. functional properties such as ‘<italic>used to style hair</italic>’), but do not necessarily share many visual features. Thus, object-evoked responses in parahippocampal cortex may express feature-based conceptual similarity structure because objects with many shared conceptual features bring to mind an associated context, whereas objects that are visually similar but conceptually distinct do not (e.g. hairdryer and gun). We note, however, that the current study was not designed to test-specific hypotheses about the contextual co-occurrence of objects, or how co-occurrence relates to conceptual feature statistics. Ultimately, a mechanistic account of object-based coding in PHC will require further research using a carefully selected stimulus set in which the strength of contextual associations (i.e. co-occurrence) between object concepts is not confounded with conceptual features.</p><p>In summary, this study sheds new light on our understanding of how object concepts are represented in the brain. Specifically, we revealed that PRC represented object concepts in a task-invariant, integrative similarity code that captured the visual and conceptual relatedness among stimuli. Most critically, this result was obtained despite systematically dissociating visual and conceptual features across object concepts. Moreover, the striking neuroanatomical specificity of this result suggests that PRC uniquely supports integration across these fundamentally different types of features. Ultimately, this pattern of results implicates PRC in the representation of fully-specified objects.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><sec id="s4-1-1"><title>Behavior-based visual similarity rating task and conceptual feature generation task</title><p>A total of 2846 individuals completed online behavioral tasks using Amazon’s Mechanical Turk (<ext-link ext-link-type="uri" xlink:href="https://www.mturk.com">https://www.mturk.com</ext-link>). Data from 61 participants were discarded due to technical errors, incomplete submissions, or missed catch trials. Of the remaining 2785 participants, 1185 completed the visual similarity rating task (616 males, 569 females; age range = 18–53; mean age = 30.1), and 1600 completed the semantic feature generation task (852 males, 748 females; age range = 18–58 years; mean age = 31.7). These sample sizes are proportionally in line with those reported by <xref ref-type="bibr" rid="bib55">McRae et al., 2005</xref>. Individuals who completed the visual similarity rating task were excluded from completing the feature generation task, and vice versa. All participants provided informed consent and were compensated for their time. Both online tasks were approved by the University of Toronto Ethics Review Board.</p></sec><sec id="s4-1-2"><title>Brain-based fMRI task</title><p>A separate group consisting of sixteen right-handed participants took part in the fMRI experiment (10 female; age range = 19–29 years; mean age = 23.1 years). This sample size is in line with extant fMRI studies that have used comparable analytical procedures to test hypotheses pertaining to object representation in the ventral visual stream and ATL (<xref ref-type="bibr" rid="bib15">Bruffaerts et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Devereux et al., 2013</xref>; <xref ref-type="bibr" rid="bib52">Martin et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">2016</xref>; <xref ref-type="bibr" rid="bib53">2018</xref>; <xref ref-type="bibr" rid="bib20">Clarke and Tyler, 2014</xref>;<xref ref-type="bibr" rid="bib18">Chadwick et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Erez et al., 2016</xref>; <xref ref-type="bibr" rid="bib13">Borghesani et al., 2016</xref>). Due to technical problems, we were unable to obtain data from one experimental run in two different participants. No participants were removed due to excessive motion using a criterion of 1.5 mm of translational displacement. All participants gave informed consent, reported that they were native English speakers, free of neurological and psychiatric disorders, and had normal or corrected to normal vision. Participants were compensated $50. This study was approved by the Baycrest Hospital Research Ethics Board.</p></sec></sec><sec id="s4-2"><title>Stimuli</title><p>As a starting point, we chained together a list of 80 object concepts in such a way that adjacent items in the list alternated between being conceptually similar but visually distinct and visually similar but conceptually distinct (e.g. bullet – gun – hairdryer – comb; bullet and gun are conceptually but not visually similar, whereas gun and hairdryer are visually but not conceptually similar, and hairdryer and comb are conceptually but not visually similar, etc.). Our initial stimulus set was established using the authors’ subjective impressions. The visual and conceptual similarities between all pairs of object concepts were then quantified by human observers in the context of a visual similarity rating task and a conceptual feature generation task, respectively. Results from these behavioral tasks were then used to select 40 object concepts used throughout the current study.</p><p>Participants who completed the visual similarity rating task were presented with 40 pairs of words and asked to rate visual similarity between the object concepts to which they referred (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Responses were made using a 5-point scale (very dissimilar, somewhat dissimilar, neutral, somewhat similar, very similar). Each participant was also presented with four catch trials on which an object concept was paired with itself. Across participants, 95.7% of catch trials were rated as being very similar. Data were excluded from 28 participants who did not rate all four catch trials as being at least ‘somewhat similar’. Every pair of object concepts from the initial set of 80 object concepts (3160) was rated by 15 different participants.</p><p>We next quantified conceptual similarities between object concepts based on responses obtained in a conceptual feature generation task (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), following task instructions previously described by <xref ref-type="bibr" rid="bib55">McRae et al., 2005</xref>. Each participant was presented with one object concept and asked to produce a list of up to 15 different types of descriptive features, including functional properties (e.g. what it is used for, where it is used, and when it is used), physical properties (e.g. how it looks, sounds, smells, feels, and tastes), and other facts about it, such as the category to which it belongs or other encyclopedic facts (e.g. where it is from). One example object and its corresponding features from a normative database were presented as an example (<xref ref-type="bibr" rid="bib55">McRae et al., 2005</xref>). Interpretation and organization of written responses were guided by criteria described by <xref ref-type="bibr" rid="bib55">McRae et al., 2005</xref>. Features were obtained from 20 different participants for each object concept. Data were excluded from 33 participants who failed to list any features. A total of 4851 unique features were produced across all 80 object concepts and participants. Features listed by fewer than 4 out of 20 participants were considered to be unreliable and discarded for the purpose of all subsequent analyses, leaving 723 unique features. This exclusion criterion is proportionally comparable to that used by <xref ref-type="bibr" rid="bib55">McRae et al., 2005</xref>. On average, each of the 80 object concepts was associated with 10.6 features.</p><p>We used a data-driven approach to select a subset of 40 object concepts from the initial 80-item set. These 40 object concepts are reflected in the behavior-based visual and conceptual RDMs, and were used as stimuli in our fMRI experiment. Specifically, we first ensured that each object concept was visually similar, but conceptually dissimilar, to at least one other item (e.g. hairdryer – gun), and conceptually similar, but visually dissimilar, to at least one different item (e.g. hairdryer – comb). Second, in an effort to ensure that visual and conceptual features varied independently across object concepts, stimuli were selected such that the corresponding behavior-based visual and conceptual similarity models were not correlated with one another.</p></sec><sec id="s4-3"><title>Behavior-based RDMs</title><sec id="s4-3-1"><title>Behavior-based visual RDM</title><p>A behavior-based model that captured visual dissimilarities between all pairs of object concepts included in the fMRI experiment (40 object concepts) was derived from the visual similarity judgments obtained from our online rating task. Specifically, similarity ratings for each pair of object concepts were averaged across participants, normalized, and expressed within a 40 × 40 RDM (1 – averaged normalized rating). Thus, the value in a given cell of this RDM reflects the visual similarity of the object concepts at that intersection. This behavior-based visual RDM is our visual dissimilarity model.</p></sec><sec id="s4-3-2"><title>Behavior-based conceptual RDM</title><p>A behavior-based model that captured conceptual dissimilarities between all pairs of object concepts included in the fMRI experiment was derived from data obtained in our online feature-generation task. In order to ensure that the semantic relationships captured by our conceptual similarity model were not influenced by verbal descriptions of visual attributes, we systematically removed features that characterized either visual form or color (e.g. ‘is round’ or ‘is red’). Using these criteria a total of 58 features (8% of the total number of features provided) were removed. We next quantified conceptual similarity using a concept-feature matrix in which columns corresponded to object concepts (i.e. 40 columns) and rows to the conceptual features associated with those objects (i.e. 282 rows) (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, center). Specifically, we computed the cosine angle between each row; cosine similarity reflects the conceptual distances between object concepts such that high cosine similarities between items denote short conceptual distance. The conceptual dissimilarities between all pairs of object concepts were expressed as a 40 × 40 RDM. The value within each cell of the conceptual model RDM was calculated as 1 – the cosine similarity value between the corresponding object concepts. This behavior-based conceptual RDM is our conceptual dissimilarity model.</p></sec><sec id="s4-3-3"><title>Behavior-based RSA: comparison of behavior-based RDMs</title><p>We next quantified similarity between our behavior-based visual RDM and behavior-based conceptual RDM using Kendall’s tau-a as the relatedness measure. This ranked correlation coefficient is the most appropriate inferential statistic to use when comparing sparse RDMs that predict many tied ranks (i.e. both models predict complete dissimilarity between many object pairs; <xref ref-type="bibr" rid="bib66">Nili et al., 2014</xref>). Statistical analysis of model similarity was performed using a stimulus-label randomization test (10,000 iterations) that simulated the null hypothesis of unrelated RDMs (i.e. zero correlation) based on the obtained variance. Significance was assessed through comparison of the obtained Kendall’s tau-a coefficient to the equivalent distribution of ranked null values. As noted in the Results section, this analysis revealed that our behavior-based visual and conceptual RDMs were not significantly correlated (Kendall’s tau-a = 0.01, p=0.10). Moreover, inclusion of the 58 features that described color and visual form in the behavior-based conceptual RDM did not significantly alter its relationship with the visual behavior-based visual RDM (Kendall’s tau-a = 0.01, p=0.09).</p></sec></sec><sec id="s4-4"><title>Experimental procedures: fMRI feature verification task</title><p>During scanning, participants completed a feature verification task that required a yes/no judgment indicating whether a given feature was applicable to a specific object concept on a trial-by-trial basis. We systematically varied the feature verification probes in a manner that established a visual feature verification task context and conceptual feature verification task context. Verification probes comprising the visual task context were selected to encourage processing of the visual semantic features that characterize each object concept (i.e. shape, color, and surface detail). To this end, eight specific probes were used: shape [(angular, rounded), (elongated, symmetrical)], color (light, dark), and surface (smooth, rough). Notably, all features are associated with two opposing probes (e.g. angular and rounded; natural and manufactured) to ensure that participants made an equal number of ‘yes’ and ‘no’ responses. Verification probes comprising the conceptual feature verification task context were selected to encourage processing of the abstract conceptual features that characterize each object concept (i.e. animacy, origin, function, and affective associations). To this end, eight specific verification probes were used: (living, non-living), (manufactured, natural), (tool, non-tool), (pleasant, unpleasant).</p></sec><sec id="s4-5"><title>Procedures</title><p>The primary experimental task was evenly divided over eight runs of functional data acquisition. Each run lasted 7 m 56 s and was evenly divided into two blocks, each of which corresponded to either a visual verification task context or a conceptual feature verification task context. The order of task blocks was counter-balanced across participants. Each block was associated with a different feature verification probe, with the first and second block in each run separated by 12 s of rest. Blocks began with an 8 s presentation of a feature verification probe that was to be referenced for all intra-block trials. With this design, each object concept was repeated 16 times: eight repetitions across the visual feature verification task context and eight repetitions across the conceptual feature verification task context. Behavioral responses were recorded using an MR-compatible keypad.</p><p>Stimuli were centrally presented for 2 s and each trial was separated by a jittered period of baseline fixation that ranged 2–6 s. Trial order and jitter interval were optimized for each run using the OptSeq2 algorithm (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/optseq/">http://surfer.nmr.mgh.harvard.edu/optseq/</ext-link>), with unique sequences and timing across counterbalanced versions of the experiment. Stimulus presentation and timing was controlled by E-Prime 2.0 (Psychology Software Tools, Pittsburgh, PA).</p></sec><sec id="s4-6"><title>Experimental procedure: fMRI functional localizer task</title><p>Following completion of the main experimental task, each participant completed an independent functional localizer scan that was subsequently used to identify LOC. Participants viewed objects, scrambled objects, words, scrambled words, faces, and scenes in separate 24 s blocks (12 functional volumes). Within each block, 32 images were presented for 400 ms each with a 350 ms ISI. There were four groups of six blocks, with each group separated by a 12 s fixation period, and each block corresponding to a different stimulus category. Block order (i.e. stimulus category) was counterbalanced across groups. All stimuli were presented in the context of a 1-back task to ensure that participants remained engaged throughout the entire scan. Presentation of images within blocks was pseudo-random with 1-back repetition occurring 1–2 times per block.</p></sec><sec id="s4-7"><title>ROI definitions</title><p>We performed RSA in four a priori defined ROIs. The temporal pole, PRC, and parahippocampal cortex were manually defined in both the left and right hemisphere on each participant’s high-resolution anatomical image according to established MR-based protocols (<xref ref-type="bibr" rid="bib75">Pruessner et al., 2002</xref>), with adjustment of posterior border of parahippocampal cortex using anatomical landmarks described by <xref ref-type="bibr" rid="bib29">Frankó et al. (2014</xref>). Lateral occipital cortex was defined as the set of contiguous voxels located along the lateral extent of the occipital lobe that responded more strongly to intact than scrambled objects (p&lt;0.001, uncorrected; <xref ref-type="bibr" rid="bib46">Malach et al., 1995</xref>).</p></sec><sec id="s4-8"><title>fMRI data acquisition</title><p>Scanning was performed using a 3.0 T Siemens MAGNETOM Trio MRI scanner at the Rotman Research Institute at Baycrest Hospital using a 32-channel receiver head coil. Each scanning session began with the acquisition of a whole-brain high-resolution magnetization-prepared rapid gradient-echo T1-weighted structural image (repetition time = 2 s, echo time = 2.63 ms, flip angle = 9°, field of view = 25.6 cm<sup>2</sup>, 160 oblique axial slices, 192 × 256 matrix, slice thickness = 1 mm). During each of eight functional scanning runs comprising the main experimental task, a total of 238 T2*-weighted echo-planar images were acquired using a two-shot gradient echo sequence (200 × 200 mm field of view with a 64 × 64 matrix size), resulting in an in-plane resolution of 3.1 × 3.1 mm for each of 40 2 mm axial slices that were acquired in an interleaved manner along the axis of the hippocampus. The inter-slice gap was 0.5 mm; repetition time = 2 s; echo time = 30 ms; flip angle = 78°). These parameters yielded coverage of the majority of cortex, excluding only the most superior aspects of the frontal and parietal lobes. During a single functional localizer scan, a total of 360 T2*-weighted echo-planar images were acquired using the same parameters reported for the main experimental task. Lastly, a B0 field map was collected following completion of the functional localizer scan </p></sec><sec id="s4-9"><title>fMRI data analysis software</title><p>Preprocessing and GLM analyses were performed in FSL5 (<xref ref-type="bibr" rid="bib82">Smith et al., 2004</xref>). Representational similarity analyses were performed using CoSMoMVPA (<ext-link ext-link-type="uri" xlink:href="http://www.cosmomvpa.org/">http://www.cosmomvpa.org/</ext-link>; <xref ref-type="bibr" rid="bib69">Oosterhof et al., 2016</xref>).</p></sec><sec id="s4-10"><title>Preprocessing and estimation of object-specific multi-voxel activity patterns</title><p>Images were initially skull-stripped using a brain extraction tool (BET, <xref ref-type="bibr" rid="bib84">Smith, 2002</xref>) to remove non-brain tissue from the image. Data were then corrected for slice-acquisition time, high-pass temporally filtered (using a 50-s period cut-off for event-related runs, and a 128 s period cut-off for the blocked localizer run), and motion corrected (MCFLIRT, <xref ref-type="bibr" rid="bib36">Jenkinson et al., 2002</xref>). Functional runs were registered to each participant’s high-resolution MPRAGE image using FLIRT boundary-based registration with B0-fieldmap correction. The resulting unsmoothed data were analyzed using first-level FEAT (v6.00; fsl.fmrib.ox.ac.uk/fsl/fslwiki) in each participant’s native anatomical space. Parameter estimates of BOLD response amplitude were computed using FILM, with a general linear model that included temporal autocorrelation correction and six motion parameters as nuisance covariates. Each trial (i.e. object concept) was modeled with a delta function corresponding to the stimulus presentation onset and then convolved with a double-gamma hemodynamic response function. Separate response-amplitude (β) images were created for each object concept (n = 40), in each run (n = 8), in each property verification task context (n = 2). Obtained β images were converted into <italic>t</italic>-statistic maps; previous research has demonstrated a modest advantage for <italic>t</italic>-maps over β images in the context of multi-voxel pattern analysis (<xref ref-type="bibr" rid="bib58">Misaki et al., 2010</xref>). In a final step, we created mean object-specific <italic>t</italic>-maps by averaging across runs. These data were used for all subsequent similarity analyses.</p></sec><sec id="s4-11"><title>Representational similarity analysis (RSA)</title><sec id="s4-11-1"><title>ROI-based first-level RSA</title><p>We used linear correlations to quantify the participant-specific dissimilarities (1 – Pearson’s r) between all object-evoked multi-voxel activity patterns (n = 40) within each ROI (n = 4). Participant-specific dissimilarity measures were expressed in 40 × 40 RDMs for each verification task context (n = 2), separately. Thus, for each ROI, each participant had one RDM that reflected the dissimilarity structure from the visual feature verification task context (i.e. brain-based visual task RDM), and one RDM that reflected the dissimilarity structure from the conceptual verification task context (i.e., brain-based conceptual task RDM).</p></sec><sec id="s4-11-2"><title>ROI-based second-level RSA</title><p>We performed second-level RSAs, that is, we compared RDMs derived from first-level RSAs, to quantify similarities among behavior-based RDMs and brain-based RDMs. Similarity was quantified in each participant using the ranked correlation coefficient (Kendall’s tau-a) between RDMs. Inferential statistical analyses were performed using a one-sided Wilcoxon signed-rank test across subject-specific RDM correlations to test for significance. This non-parametric test provides valid inference and treats the variation across subjects as a random effect, thus supporting generalization of results beyond the sample. A Bonferroni correction was applied in each analysis to compensate for the number of second-level comparisons.</p></sec><sec id="s4-11-3"><title>Searchlight-based RSA</title><p>Whole-volume RSA was implemented using 100-voxel surface-based searchlights (<xref ref-type="bibr" rid="bib40">Kriegeskorte et al., 2006</xref>; <xref ref-type="bibr" rid="bib70">Oosterhof et al., 2011</xref>). Each surface-based searchlight referenced the 100 nearest voxels to the searchlight center based on geodesic distance on the cortical surface. Neural estimates of dissimilarity (i.e. RDMs) were calculated in each searchlight using the same approach implemented in our ROI-based RSA. Correlations between behavior-based RDMs were also quantified using the same approach. The correlation coefficients obtained between behavior-based RDMs and brain-based RDMs were then Fisher-<italic>z</italic> transformed and mapped to the voxel at the centre of each searchlight to create a whole-brain similarity map. Participant-specific similarity maps were then normalized to a standard MNI template using FNIRT (<xref ref-type="bibr" rid="bib33">Greve and Fischl, 2009</xref>). To assess the statistical significance of searchlight maps across participants, all maps were corrected for multiple comparisons without choosing an arbitrary uncorrected threshold using threshold-free cluster enhancement (TFCE) with a corrected statistical threshold of p&lt;0.05 on the cluster level (<xref ref-type="bibr" rid="bib83">Smith and Nichols, 2009</xref>). A Monte Carlo simulation permuting condition labels was used to estimate a null TFCE distribution. First, 100 null searchlight maps were generated for each participant by randomly permuting condition labels within each obtained searchlight RDM. Next, 10,000 null TFCE maps were constructed by randomly sampling from these null data sets in order to estimate a null TFCE distribution (<xref ref-type="bibr" rid="bib85">Stelzer et al., 2013</xref>). The resulting surface-based statistically thresholded <italic>z</italic>-score were projected onto the PALS-B12 surface atlas in CARET version 5.6. (<ext-link ext-link-type="uri" xlink:href="http://www.nitrc.org/projects/caret/">http://www.nitrc.org/projects/caret/</ext-link>; <xref ref-type="bibr" rid="bib92">Van Essen et al., 2001</xref>; <xref ref-type="bibr" rid="bib93">Van Essen, 2005</xref>).</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Canadian Natural Sciences Engineering Research Council (Discovery and Accelerator Grants to MDB.; Postdoctoral Fellowship award to CBM), the James S. McDonnell Foundation (Scholar Award to MDB), and the Canada Research Chairs Program (MDB).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Formal analysis, Investigation</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study was approved by the Institutional Review Board at the University of Toronto (REB # 23778) and the Research Ethics Board at Baycrest Hospital (REB # 15-06). Informed consent was obtained from each participant before the experiment, including consent to publish anonymized results.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.31873.021</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-31873-transrepform-v3.docx"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aminoff</surname> <given-names>E</given-names></name><name><surname>Gronau</surname> <given-names>N</given-names></name><name><surname>Bar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The parahippocampal cortex mediates spatial and nonspatial associations</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>1493</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl078</pub-id><pub-id pub-id-type="pmid">16990438</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aminoff</surname> <given-names>EM</given-names></name><name><surname>Kveraga</surname> <given-names>K</given-names></name><name><surname>Bar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The role of the parahippocampal cortex in cognition</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>379</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.009</pub-id><pub-id pub-id-type="pmid">23850264</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassano</surname> <given-names>C</given-names></name><name><surname>Beck</surname> <given-names>DM</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Differential connectivity within the Parahippocampal Place Area</article-title><source>NeuroImage</source><volume>75</volume><fpage>228</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.02.073</pub-id><pub-id pub-id-type="pmid">23507385</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name><name><surname>Aminoff</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cortical analysis of visual context</article-title><source>Neuron</source><volume>38</volume><fpage>347</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00167-3</pub-id><pub-id pub-id-type="pmid">12718867</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Visual objects in context</article-title><source>Nature Reviews Neuroscience</source><volume>5</volume><fpage>617</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1038/nrn1476</pub-id><pub-id pub-id-type="pmid">15263892</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barense</surname> <given-names>MD</given-names></name><name><surname>Bussey</surname> <given-names>TJ</given-names></name><name><surname>Lee</surname> <given-names>AC</given-names></name><name><surname>Rogers</surname> <given-names>TT</given-names></name><name><surname>Davies</surname> <given-names>RR</given-names></name><name><surname>Saksida</surname> <given-names>LM</given-names></name><name><surname>Murray</surname> <given-names>EA</given-names></name><name><surname>Graham</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Functional specialization in the human medial temporal lobe</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>10239</fpage><lpage>10246</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2704-05.2005</pub-id><pub-id pub-id-type="pmid">16267231</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barense</surname> <given-names>MD</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name><name><surname>Graham</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The human medial temporal lobe processes online representations of complex objects</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>2963</fpage><lpage>2974</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.05.023</pub-id><pub-id pub-id-type="pmid">17658561</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barense</surname> <given-names>MD</given-names></name><name><surname>Groen</surname> <given-names>II</given-names></name><name><surname>Lee</surname> <given-names>AC</given-names></name><name><surname>Yeung</surname> <given-names>LK</given-names></name><name><surname>Brady</surname> <given-names>SM</given-names></name><name><surname>Gregori</surname> <given-names>M</given-names></name><name><surname>Kapur</surname> <given-names>N</given-names></name><name><surname>Bussey</surname> <given-names>TJ</given-names></name><name><surname>Saksida</surname> <given-names>LM</given-names></name><name><surname>Henson</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Intact memory for irrelevant information impairs perception in amnesia</article-title><source>Neuron</source><volume>75</volume><fpage>157</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.05.014</pub-id><pub-id pub-id-type="pmid">22794269</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barense</surname> <given-names>MD</given-names></name><name><surname>Henson</surname> <given-names>RN</given-names></name><name><surname>Graham</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Perception and conception: temporal lobe activity during complex discriminations of familiar and novel faces and objects</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>3052</fpage><lpage>3067</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00010</pub-id><pub-id pub-id-type="pmid">21391761</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barense</surname> <given-names>MD</given-names></name><name><surname>Rogers</surname> <given-names>TT</given-names></name><name><surname>Bussey</surname> <given-names>TJ</given-names></name><name><surname>Saksida</surname> <given-names>LM</given-names></name><name><surname>Graham</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Influence of conceptual knowledge on visual object discrimination: insights from semantic dementia and MTL amnesia</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>2568</fpage><lpage>2582</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq004</pub-id><pub-id pub-id-type="pmid">20150429</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname> <given-names>JR</given-names></name><name><surname>Desai</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The neurobiology of semantic memory</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>527</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.10.001</pub-id><pub-id pub-id-type="pmid">22001867</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binney</surname> <given-names>RJ</given-names></name><name><surname>Embleton</surname> <given-names>KV</given-names></name><name><surname>Jefferies</surname> <given-names>E</given-names></name><name><surname>Parker</surname> <given-names>GJ</given-names></name><name><surname>Ralph</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The ventral and inferolateral aspects of the anterior temporal lobe are crucial in semantic memory: evidence from a novel direct comparison of distortion-corrected fMRI, rTMS, and semantic dementia</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>2728</fpage><lpage>2738</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq019</pub-id><pub-id pub-id-type="pmid">20190005</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borghesani</surname> <given-names>V</given-names></name><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>Buiatti</surname> <given-names>M</given-names></name><name><surname>Amadon</surname> <given-names>A</given-names></name><name><surname>Eger</surname> <given-names>E</given-names></name><name><surname>Piazza</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Word meaning in the ventral visual path: a perceptual to conceptual gradient of semantic coding</article-title><source>NeuroImage</source><volume>143</volume><fpage>128</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.068</pub-id><pub-id pub-id-type="pmid">27592809</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bozeat</surname> <given-names>S</given-names></name><name><surname>Ralph</surname> <given-names>MA</given-names></name><name><surname>Graham</surname> <given-names>KS</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Wilkin</surname> <given-names>H</given-names></name><name><surname>Rowland</surname> <given-names>J</given-names></name><name><surname>Rogers</surname> <given-names>TT</given-names></name><name><surname>Hodges</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A duck with four legs: Investigating the structure of conceptual knowledge using picture drawing in semantic dementia</article-title><source>Cognitive Neuropsychology</source><volume>20</volume><fpage>27</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1080/02643290244000176</pub-id><pub-id pub-id-type="pmid">20957563</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruffaerts</surname> <given-names>R</given-names></name><name><surname>Dupont</surname> <given-names>P</given-names></name><name><surname>Peeters</surname> <given-names>R</given-names></name><name><surname>De Deyne</surname> <given-names>S</given-names></name><name><surname>Storms</surname> <given-names>G</given-names></name><name><surname>Vandenberghe</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Similarity of fMRI activity patterns in left perirhinal cortex reflects semantic similarity between words</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>18597</fpage><lpage>18607</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1548-13.2013</pub-id><pub-id pub-id-type="pmid">24259581</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckley</surname> <given-names>MJ</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Perirhinal cortical contributions to object perception</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>100</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.01.008</pub-id><pub-id pub-id-type="pmid">16469525</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burwell</surname> <given-names>RD</given-names></name><name><surname>Amaral</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical afferents of the perirhinal, postrhinal, and entorhinal cortices of the rat</article-title><source>The Journal of Comparative Neurology</source><volume>398</volume><fpage>179</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(19980824)398:2&lt;179::AID-CNE3&gt;3.0.CO;2-Y</pub-id><pub-id pub-id-type="pmid">9700566</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chadwick</surname> <given-names>MJ</given-names></name><name><surname>Anjum</surname> <given-names>RS</given-names></name><name><surname>Kumaran</surname> <given-names>D</given-names></name><name><surname>Schacter</surname> <given-names>DL</given-names></name><name><surname>Spiers</surname> <given-names>HJ</given-names></name><name><surname>Hassabis</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Semantic representations in the temporal pole predict false memories</article-title><source>Proceedings of the National Academy of Sciences</source><volume>113</volume><fpage>10180</fpage><lpage>10185</lpage><pub-id pub-id-type="doi">10.1073/pnas.1610686113</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>L</given-names></name><name><surname>Lambon Ralph</surname> <given-names>MA</given-names></name><name><surname>Rogers</surname> <given-names>TT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A unified model of human semantic knowledge and its disorders</article-title><source>Nature Human Behaviour</source><volume>1</volume><elocation-id>0039</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-016-0039</pub-id><pub-id pub-id-type="pmid">28480333</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname> <given-names>A</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Object-specific semantic coding in human perirhinal cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>4766</fpage><lpage>4775</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2828-13.2014</pub-id><pub-id pub-id-type="pmid">24695697</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname> <given-names>A</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Understanding what we see: How we derive meaning from vision</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>677</fpage><lpage>687</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.08.008</pub-id><pub-id pub-id-type="pmid">26440124</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Damasio</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>The Brain Binds Entities and Events by Multiregional Activation from Convergence Zones</article-title><source>Neural Computation</source><volume>1</volume><fpage>123</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1162/neco.1989.1.1.123</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devereux</surname> <given-names>BJ</given-names></name><name><surname>Clarke</surname> <given-names>A</given-names></name><name><surname>Marouchos</surname> <given-names>A</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational similarity analysis reveals commonalities and differences in the semantic processing of words and objects</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>18906</fpage><lpage>18916</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3809-13.2013</pub-id><pub-id pub-id-type="pmid">24285896</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devlin</surname> <given-names>JT</given-names></name><name><surname>Price</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Perirhinal contributions to human visual perception</article-title><source>Current Biology</source><volume>17</volume><fpage>1484</fpage><lpage>1488</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.07.066</pub-id><pub-id pub-id-type="pmid">17764947</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duke</surname> <given-names>D</given-names></name><name><surname>Martin</surname> <given-names>CB</given-names></name><name><surname>Bowles</surname> <given-names>B</given-names></name><name><surname>McRae</surname> <given-names>K</given-names></name><name><surname>Köhler</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Perirhinal cortex tracks degree of recent as well as cumulative lifetime experience with object concepts</article-title><source>Cortex</source><volume>89</volume><fpage>61</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2017.01.015</pub-id><pub-id pub-id-type="pmid">28236751</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname> <given-names>R</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A cortical representation of the local visual environment</article-title><source>Nature</source><volume>392</volume><fpage>598</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1038/33402</pub-id><pub-id pub-id-type="pmid">9560155</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erez</surname> <given-names>J</given-names></name><name><surname>Cusack</surname> <given-names>R</given-names></name><name><surname>Kendall</surname> <given-names>W</given-names></name><name><surname>Barense</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Conjunctive Coding of Complex Object Features</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>2271</fpage><lpage>2282</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv081</pub-id><pub-id pub-id-type="pmid">25921583</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname> <given-names>SL</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain regions that represent amodal conceptual knowledge</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>10552</fpage><lpage>10558</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0051-13.2013</pub-id><pub-id pub-id-type="pmid">23785167</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frankó</surname> <given-names>E</given-names></name><name><surname>Insausti</surname> <given-names>AM</given-names></name><name><surname>Artacho-Pérula</surname> <given-names>E</given-names></name><name><surname>Insausti</surname> <given-names>R</given-names></name><name><surname>Chavoix</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Identification of the human medial temporal lobe regions on magnetic resonance images</article-title><source>Human Brain Mapping</source><volume>35</volume><fpage>248</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1002/hbm.22170</pub-id><pub-id pub-id-type="pmid">22936605</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galton</surname> <given-names>CJ</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Graham</surname> <given-names>K</given-names></name><name><surname>Lambon-Ralph</surname> <given-names>MA</given-names></name><name><surname>Williams</surname> <given-names>G</given-names></name><name><surname>Antoun</surname> <given-names>N</given-names></name><name><surname>Sahakian</surname> <given-names>BJ</given-names></name><name><surname>Hodges</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Differing patterns of temporal atrophy in Alzheimer's disease and semantic dementia</article-title><source>Neurology</source><volume>57</volume><fpage>216</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1212/WNL.57.2.216</pub-id><pub-id pub-id-type="pmid">11468305</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname> <given-names>KS</given-names></name><name><surname>Barense</surname> <given-names>MD</given-names></name><name><surname>Lee</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Going beyond LTM in the MTL: a synthesis of neuropsychological and neuroimaging findings on the role of the medial temporal lobe in memory and perception</article-title><source>Neuropsychologia</source><volume>48</volume><fpage>831</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2010.01.001</pub-id><pub-id pub-id-type="pmid">20074580</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname> <given-names>KS</given-names></name><name><surname>Hodges</surname> <given-names>JR</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>The relationship between comprehension and oral reading in progressive fluent aphasia</article-title><source>Neuropsychologia</source><volume>32</volume><fpage>299</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(94)90133-3</pub-id><pub-id pub-id-type="pmid">8202225</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname> <given-names>DN</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>NeuroImage</source><volume>48</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id><pub-id pub-id-type="pmid">19573611</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname> <given-names>K</given-names></name><name><surname>Kushnir</surname> <given-names>T</given-names></name><name><surname>Edelman</surname> <given-names>S</given-names></name><name><surname>Avidan</surname> <given-names>G</given-names></name><name><surname>Itzchak</surname> <given-names>Y</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Differential processing of objects under various viewing conditions in the human lateral occipital complex</article-title><source>Neuron</source><volume>24</volume><fpage>187</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80832-6</pub-id><pub-id pub-id-type="pmid">10677037</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hodges</surname> <given-names>JR</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Oxbury</surname> <given-names>S</given-names></name><name><surname>Funnell</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Semantic dementia. Progressive fluent aphasia with temporal lobe atrophy</article-title><source>Brain : A Journal of Neurology</source><volume>115</volume><fpage>1783</fpage><lpage>1806</lpage><pub-id pub-id-type="doi">10.1093/brain/115.6.1783</pub-id><pub-id pub-id-type="pmid">1486461</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Bannister</surname> <given-names>P</given-names></name><name><surname>Brady</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1132</pub-id><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahn</surname> <given-names>I</given-names></name><name><surname>Andrews-Hanna</surname> <given-names>JR</given-names></name><name><surname>Vincent</surname> <given-names>JL</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Buckner</surname> <given-names>RL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Distinct cortical anatomy linked to subregions of the medial temporal lobe revealed by intrinsic functional connectivity</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>129</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1152/jn.00077.2008</pub-id><pub-id pub-id-type="pmid">18385483</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiefer</surname> <given-names>M</given-names></name><name><surname>Pulvermüller</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Conceptual representations in mind and brain: theoretical developments, current evidence and future directions</article-title><source>Cortex</source><volume>48</volume><fpage>805</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2011.04.006</pub-id><pub-id pub-id-type="pmid">21621764</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kourtzi</surname> <given-names>Z</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Representation of perceived object shape by the human lateral occipital complex</article-title><source>Science</source><volume>293</volume><fpage>1506</fpage><lpage>1509</lpage><pub-id pub-id-type="doi">10.1126/science.1061133</pub-id><pub-id pub-id-type="pmid">11520991</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Information-based functional brain mapping</article-title><source>PNAS</source><volume>103</volume><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id><pub-id pub-id-type="pmid">16537458</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Kievit</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id><pub-id pub-id-type="pmid">23876494</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambon Ralph</surname> <given-names>MA</given-names></name><name><surname>Pobric</surname> <given-names>G</given-names></name><name><surname>Jefferies</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Conceptual knowledge is underpinned by the temporal pole bilaterally: convergent evidence from rTMS</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>832</fpage><lpage>838</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn131</pub-id><pub-id pub-id-type="pmid">18678765</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>AC</given-names></name><name><surname>Buckley</surname> <given-names>MJ</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name><name><surname>Emery</surname> <given-names>T</given-names></name><name><surname>Hodges</surname> <given-names>JR</given-names></name><name><surname>Graham</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Differentiating the roles of the hippocampus and perirhinal cortex in processes beyond long-term declarative memory: a double dissociation in dementia</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>5198</fpage><lpage>5203</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3157-05.2006</pub-id><pub-id pub-id-type="pmid">16687511</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>AC</given-names></name><name><surname>Buckley</surname> <given-names>MJ</given-names></name><name><surname>Pegman</surname> <given-names>SJ</given-names></name><name><surname>Spiers</surname> <given-names>H</given-names></name><name><surname>Scahill</surname> <given-names>VL</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name><name><surname>Bussey</surname> <given-names>TJ</given-names></name><name><surname>Davies</surname> <given-names>RR</given-names></name><name><surname>Kapur</surname> <given-names>N</given-names></name><name><surname>Hodges</surname> <given-names>JR</given-names></name><name><surname>Graham</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Specialization in the medial temporal lobe for processing of objects and scenes</article-title><source>Hippocampus</source><volume>15</volume><fpage>782</fpage><lpage>797</lpage><pub-id pub-id-type="doi">10.1002/hipo.20101</pub-id><pub-id pub-id-type="pmid">16010661</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehky</surname> <given-names>SR</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural representation for object recognition in inferotemporal cortex</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>23</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.12.001</pub-id><pub-id pub-id-type="pmid">26771242</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Reppas</surname> <given-names>JB</given-names></name><name><surname>Benson</surname> <given-names>RR</given-names></name><name><surname>Kwong</surname> <given-names>KK</given-names></name><name><surname>Jiang</surname> <given-names>H</given-names></name><name><surname>Kennedy</surname> <given-names>WA</given-names></name><name><surname>Ledden</surname> <given-names>PJ</given-names></name><name><surname>Brady</surname> <given-names>TJ</given-names></name><name><surname>Rosen</surname> <given-names>BR</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title><source>PNAS</source><volume>92</volume><fpage>8135</fpage><lpage>8139</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.18.8135</pub-id><pub-id pub-id-type="pmid">7667258</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marchette</surname> <given-names>SA</given-names></name><name><surname>Vass</surname> <given-names>LK</given-names></name><name><surname>Ryan</surname> <given-names>J</given-names></name><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Outside Looking In: Landmark Generalization in the Human Navigational System</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>14896</fpage><lpage>14908</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2270-15.2015</pub-id><pub-id pub-id-type="pmid">26538658</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>A</given-names></name><name><surname>Chao</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Semantic memory and the brain: structure and processes</article-title><source>Current Opinion in Neurobiology</source><volume>11</volume><fpage>194</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(00)00196-3</pub-id><pub-id pub-id-type="pmid">11301239</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>A</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Lalonde</surname> <given-names>FM</given-names></name><name><surname>Wiggs</surname> <given-names>CL</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Discrete cortical regions associated with knowledge of color and knowledge of action</article-title><source>Science</source><volume>270</volume><elocation-id>102</elocation-id><lpage>105</lpage><pub-id pub-id-type="doi">10.1126/science.270.5233.102</pub-id><pub-id pub-id-type="pmid">7569934</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>GRAPES-Grounding representations in action, perception, and emotion systems: How object properties and categories are represented in the human brain</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>979</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0842-3</pub-id><pub-id pub-id-type="pmid">25968087</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>CB</given-names></name><name><surname>Cowell</surname> <given-names>RA</given-names></name><name><surname>Gribble</surname> <given-names>PL</given-names></name><name><surname>Wright</surname> <given-names>J</given-names></name><name><surname>Köhler</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Distributed category-specific recognition-memory signals in human perirhinal cortex</article-title><source>Hippocampus</source><volume>26</volume><fpage>423</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1002/hipo.22531</pub-id><pub-id pub-id-type="pmid">26385759</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>CB</given-names></name><name><surname>McLean</surname> <given-names>DA</given-names></name><name><surname>O'Neil</surname> <given-names>EB</given-names></name><name><surname>Köhler</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Distinct familiarity-based response patterns for faces and buildings in perirhinal and parahippocampal cortex</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>10915</fpage><lpage>10923</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0126-13.2013</pub-id><pub-id pub-id-type="pmid">23804111</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>CB</given-names></name><name><surname>Sullivan</surname> <given-names>JA</given-names></name><name><surname>Wright</surname> <given-names>J</given-names></name><name><surname>Köhler</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How landmark suitability shapes recognition memory signals for objects in the medial temporal lobes</article-title><source>NeuroImage</source><volume>166</volume><fpage>425</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.11.004</pub-id><pub-id pub-id-type="pmid">29108942</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McLelland</surname> <given-names>VC</given-names></name><name><surname>Chan</surname> <given-names>D</given-names></name><name><surname>Ferber</surname> <given-names>S</given-names></name><name><surname>Barense</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Stimulus familiarity modulates functional connectivity of the perirhinal cortex and anterior hippocampus during visual discrimination of faces and objects</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>117</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00117</pub-id><pub-id pub-id-type="pmid">24624075</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McRae</surname> <given-names>K</given-names></name><name><surname>Cree</surname> <given-names>GS</given-names></name><name><surname>Seidenberg</surname> <given-names>MS</given-names></name><name><surname>McNorgan</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Semantic feature production norms for a large set of living and nonliving things</article-title><source>Behavior Research Methods</source><volume>37</volume><fpage>547</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.3758/BF03192726</pub-id><pub-id pub-id-type="pmid">16629288</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mikolov</surname> <given-names>T</given-names></name><name><surname>Chen</surname> <given-names>K</given-names></name><name><surname>Corrado</surname> <given-names>G</given-names></name><name><surname>Dean</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Efficient Estimation of Word Representations in Vector Space</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arXiv.org/abs/1301.3781">https://arXiv.org/abs/1301.3781</ext-link></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Milner</surname> <given-names>D</given-names></name><name><surname>Goodale</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>The Visual Brain in Action</source><publisher-loc>Oxford, United Kingdom</publisher-loc><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780198524724.001.0001</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Misaki</surname> <given-names>M</given-names></name><name><surname>Kim</surname> <given-names>Y</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title><source>NeuroImage</source><volume>53</volume><fpage>103</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.051</pub-id><pub-id pub-id-type="pmid">20580933</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moss</surname> <given-names>HE</given-names></name><name><surname>Rodd</surname> <given-names>JM</given-names></name><name><surname>Stamatakis</surname> <given-names>EA</given-names></name><name><surname>Bright</surname> <given-names>P</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Anteromedial temporal cortex supports fine-grained differentiation among objects</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>616</fpage><lpage>627</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh163</pub-id><pub-id pub-id-type="pmid">15342435</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moss</surname> <given-names>HE</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name><name><surname>Jennings</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>When leopards lose their spots: Knowledge of visual properties in category-specific deficits for living things</article-title><source>Cognitive Neuropsychology</source><volume>14</volume><fpage>901</fpage><lpage>950</lpage><pub-id pub-id-type="doi">10.1080/026432997381394</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mummery</surname> <given-names>CJ</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Price</surname> <given-names>CJ</given-names></name><name><surname>Ashburner</surname> <given-names>J</given-names></name><name><surname>Frackowiak</surname> <given-names>RS</given-names></name><name><surname>Hodges</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A voxel-based morphometry study of semantic dementia: relationship between temporal lobe atrophy and semantic memory</article-title><source>Annals of Neurology</source><volume>47</volume><fpage>36</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1002/1531-8249(200001)47:1&lt;36::AID-ANA8&gt;3.0.CO;2-L</pub-id><pub-id pub-id-type="pmid">10632099</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mur</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>What's the difference between a tiger and a cat? From visual object to semantic concept via the perirhinal cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>10462</fpage><lpage>10464</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2248-14.2014</pub-id><pub-id pub-id-type="pmid">25100581</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>C</given-names></name><name><surname>Rueschemeyer</surname> <given-names>SA</given-names></name><name><surname>Watson</surname> <given-names>D</given-names></name><name><surname>Karapanagiotidis</surname> <given-names>T</given-names></name><name><surname>Smallwood</surname> <given-names>J</given-names></name><name><surname>Jefferies</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fractionating the anterior temporal lobe: MVPA reveals differential responses to input and conceptual modality</article-title><source>NeuroImage</source><volume>147</volume><fpage>19</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.067</pub-id><pub-id pub-id-type="pmid">27908787</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>EA</given-names></name><name><surname>Bussey</surname> <given-names>TJ</given-names></name><name><surname>Saksida</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual perception and memory: a new view of medial temporal lobe function in primates and rodents</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>99</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113046</pub-id><pub-id pub-id-type="pmid">17417938</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>EA</given-names></name><name><surname>Bussey</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Perceptual-mnemonic functions of the perirhinal cortex</article-title><source>Trends in Cognitive Sciences</source><volume>3</volume><fpage>142</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(99)01303-0</pub-id><pub-id pub-id-type="pmid">10322468</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Wingfield</surname> <given-names>C</given-names></name><name><surname>Walther</surname> <given-names>A</given-names></name><name><surname>Su</surname> <given-names>L</given-names></name><name><surname>Marslen-Wilson</surname> <given-names>W</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A toolbox for representational similarity analysis</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id><pub-id pub-id-type="pmid">24743308</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noppeney</surname> <given-names>U</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name><name><surname>Moss</surname> <given-names>H</given-names></name><name><surname>Stamatakis</surname> <given-names>EA</given-names></name><name><surname>Bright</surname> <given-names>P</given-names></name><name><surname>Mummery</surname> <given-names>C</given-names></name><name><surname>Price</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Temporal lobe lesions and semantic impairment: a comparison of herpes simplex virus encephalitis and semantic dementia</article-title><source>Brain</source><volume>130</volume><fpage>1138</fpage><lpage>1147</lpage><pub-id pub-id-type="doi">10.1093/brain/awl344</pub-id><pub-id pub-id-type="pmid">17251241</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Neil</surname> <given-names>EB</given-names></name><name><surname>Cate</surname> <given-names>AD</given-names></name><name><surname>Köhler</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Perirhinal cortex contributes to accuracy in recognition memory and perceptual discriminations</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>8329</fpage><lpage>8334</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0374-09.2009</pub-id><pub-id pub-id-type="pmid">19571124</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>CoSMoMVPA: Multi-Modal Multivariate Pattern Analysis of Neuroimaging Data in Matlab/GNU Octave</article-title><source>Frontiers in Neuroinformatics</source><volume>10</volume><elocation-id>27</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00027</pub-id><pub-id pub-id-type="pmid">27499741</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Wiestler</surname> <given-names>T</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name><name><surname>Diedrichsen</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A comparison of volume-based and surface-based multi-voxel pattern analysis</article-title><source>NeuroImage</source><volume>56</volume><fpage>593</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.04.270</pub-id><pub-id pub-id-type="pmid">20621701</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Nestor</surname> <given-names>PJ</given-names></name><name><surname>Rogers</surname> <given-names>TT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Where do you know what you know? The representation of semantic knowledge in the human brain</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>976</fpage><lpage>987</lpage><pub-id pub-id-type="doi">10.1038/nrn2277</pub-id><pub-id pub-id-type="pmid">18026167</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Conceptual object representations in human anterior temporal cortex</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>15728</fpage><lpage>15736</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1953-12.2012</pub-id><pub-id pub-id-type="pmid">23136412</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname> <given-names>MA</given-names></name><name><surname>Cacciamani</surname> <given-names>L</given-names></name><name><surname>Barense</surname> <given-names>MD</given-names></name><name><surname>Scalf</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The perirhinal cortex modulates V2 activity in response to the agreement between part familiarity and configuration familiarity</article-title><source>Hippocampus</source><volume>22</volume><fpage>1965</fpage><lpage>1977</lpage><pub-id pub-id-type="doi">10.1002/hipo.22065</pub-id><pub-id pub-id-type="pmid">22987675</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pobric</surname> <given-names>G</given-names></name><name><surname>Jefferies</surname> <given-names>E</given-names></name><name><surname>Ralph</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Anterior temporal lobes mediate semantic representation: mimicking semantic dementia by using rTMS in normal participants</article-title><source>PNAS</source><volume>104</volume><fpage>20137</fpage><lpage>20141</lpage><pub-id pub-id-type="doi">10.1073/pnas.0707383104</pub-id><pub-id pub-id-type="pmid">18056637</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pruessner</surname> <given-names>JC</given-names></name><name><surname>Köhler</surname> <given-names>S</given-names></name><name><surname>Crane</surname> <given-names>J</given-names></name><name><surname>Pruessner</surname> <given-names>M</given-names></name><name><surname>Lord</surname> <given-names>C</given-names></name><name><surname>Byrne</surname> <given-names>A</given-names></name><name><surname>Kabani</surname> <given-names>N</given-names></name><name><surname>Collins</surname> <given-names>DL</given-names></name><name><surname>Evans</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Volumetry of temporopolar, perirhinal, entorhinal and parahippocampal cortex from high-resolution MR images: considering the variability of the collateral sulcus</article-title><source>Cerebral Cortex</source><volume>12</volume><fpage>1342</fpage><lpage>1353</lpage><pub-id pub-id-type="doi">10.1093/cercor/12.12.1342</pub-id><pub-id pub-id-type="pmid">12427684</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ralph</surname> <given-names>MA</given-names></name><name><surname>Jefferies</surname> <given-names>E</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Rogers</surname> <given-names>TT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The neural and computational bases of semantic cognition</article-title><source>Nature Reviews Neuroscience</source><volume>18</volume><fpage>42</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.150</pub-id><pub-id pub-id-type="pmid">27881854</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranganath</surname> <given-names>C</given-names></name><name><surname>Ritchey</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Two cortical systems for memory-guided behaviour</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>713</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.1038/nrn3338</pub-id><pub-id pub-id-type="pmid">22992647</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname> <given-names>TT</given-names></name><name><surname>Hocking</surname> <given-names>J</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name><name><surname>Mechelli</surname> <given-names>A</given-names></name><name><surname>Gorno-Tempini</surname> <given-names>ML</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Price</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Anterior temporal cortex and semantic memory: Reconciling findings from neuropsychology and functional imaging. <italic>Cognitive, Affective, &amp;</italic></article-title><source>Behavioral Neuroscience</source><volume>6</volume><fpage>201</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.3758/CABN.6.3.201</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname> <given-names>TT</given-names></name><name><surname>Lambon Ralph</surname> <given-names>MA</given-names></name><name><surname>Garrard</surname> <given-names>P</given-names></name><name><surname>Bozeat</surname> <given-names>S</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name><name><surname>Hodges</surname> <given-names>JR</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Structure and deterioration of semantic memory: a neuropsychological and computational investigation</article-title><source>Psychological Review</source><volume>111</volume><fpage>205</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.111.1.205</pub-id><pub-id pub-id-type="pmid">14756594</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rogers</surname> <given-names>TT</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Semantic Cognition: A Parallel Distributed Processing Approach</source><publisher-loc>London</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosch</surname> <given-names>E</given-names></name><name><surname>Mervis</surname> <given-names>CB</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Family resemblances: Studies in the internal structure of categories</article-title><source>Cognitive Psychology</source><volume>7</volume><fpage>573</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(75)90024-9</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Beckmann</surname> <given-names>CF</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Johansen-Berg</surname> <given-names>H</given-names></name><name><surname>Bannister</surname> <given-names>PR</given-names></name><name><surname>De Luca</surname> <given-names>M</given-names></name><name><surname>Drobnjak</surname> <given-names>I</given-names></name><name><surname>Flitney</surname> <given-names>DE</given-names></name><name><surname>Niazy</surname> <given-names>RK</given-names></name><name><surname>Saunders</surname> <given-names>J</given-names></name><name><surname>Vickers</surname> <given-names>J</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>De Stefano</surname> <given-names>N</given-names></name><name><surname>Brady</surname> <given-names>JM</given-names></name><name><surname>Matthews</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>NeuroImage</source><volume>23</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><volume>44</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Fast robust automated brain extraction</article-title><source>Human Brain Mapping</source><volume>17</volume><fpage>143</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1002/hbm.10062</pub-id><pub-id pub-id-type="pmid">12391568</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stelzer</surname> <given-names>J</given-names></name><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Statistical inference and multiple testing correction in classification-based multi-voxel pattern analysis (MVPA): random permutations and cluster size control</article-title><source>NeuroImage</source><volume>65</volume><fpage>69</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.09.063</pub-id><pub-id pub-id-type="pmid">23041526</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname> <given-names>WA</given-names></name><name><surname>Amaral</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Perirhinal and parahippocampal cortices of the macaque monkey: cortical afferents</article-title><source>The Journal of Comparative Neurology</source><volume>350</volume><fpage>497</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1002/cne.903500402</pub-id><pub-id pub-id-type="pmid">7890828</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname> <given-names>WA</given-names></name><name><surname>Naya</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The perirhinal cortex</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>39</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-071013-014207</pub-id><pub-id pub-id-type="pmid">25032492</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname> <given-names>KI</given-names></name><name><surname>Devereux</surname> <given-names>BJ</given-names></name><name><surname>Acres</surname> <given-names>K</given-names></name><name><surname>Randall</surname> <given-names>B</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Contrasting effects of feature-based statistics on the categorisation and basic-level identification of visual objects</article-title><source>Cognition</source><volume>122</volume><fpage>363</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2011.11.001</pub-id><pub-id pub-id-type="pmid">22137770</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson-Schill</surname> <given-names>SL</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neuroimaging studies of semantic memory: inferring &quot;how&quot; from &quot;where&quot;</article-title><source>Neuropsychologia</source><volume>41</volume><fpage>280</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1016/S0028-3932(02)00161-6</pub-id><pub-id pub-id-type="pmid">12457754</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tranel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The left temporal pole is important for retrieving words for unique concrete entities</article-title><source>Aphasiology</source><volume>23</volume><fpage>867</fpage><lpage>884</lpage><pub-id pub-id-type="doi">10.1080/02687030802586498</pub-id><pub-id pub-id-type="pmid">20161625</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyler</surname> <given-names>LK</given-names></name><name><surname>Moss</surname> <given-names>HE</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Towards a distributed account of conceptual knowledge</article-title><source>Trends in Cognitive Sciences</source><volume>5</volume><fpage>244</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01651-X</pub-id><pub-id pub-id-type="pmid">11390295</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Drury</surname> <given-names>HA</given-names></name><name><surname>Dickson</surname> <given-names>J</given-names></name><name><surname>Harwell</surname> <given-names>J</given-names></name><name><surname>Hanlon</surname> <given-names>D</given-names></name><name><surname>Anderson</surname> <given-names>CH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An integrated software suite for surface-based analyses of cerebral cortex</article-title><source>Journal of the American Medical Informatics Association</source><volume>8</volume><fpage>443</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1136/jamia.2001.0080443</pub-id><pub-id pub-id-type="pmid">11522765</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A Population-Average, Landmark- and Surface-based (PALS) atlas of human cerebral cortex</article-title><source>NeuroImage</source><volume>28</volume><fpage>635</fpage><lpage>662</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.06.058</pub-id><pub-id pub-id-type="pmid">16172003</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>JMASM9: Converting Kendall's Tau For Correlational Or Meta-Analytic Analyses</article-title><source>Journal of Modern Applied Statistical Methods</source><volume>2</volume><fpage>525</fpage><lpage>530</lpage><pub-id pub-id-type="doi">10.22237/jmasm/1067646360</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>SF</given-names></name><name><surname>Ritchey</surname> <given-names>M</given-names></name><name><surname>Libby</surname> <given-names>LA</given-names></name><name><surname>Ranganath</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Functional connectivity based parcellation of the human medial temporal lobe</article-title><source>Neurobiology of Learning and Memory</source><volume>134</volume><fpage>123</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2016.01.005</pub-id><pub-id pub-id-type="pmid">26805590</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warrington</surname> <given-names>EK</given-names></name><name><surname>Shallice</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Category specific semantic impairments</article-title><source>Brain</source><volume>107</volume><fpage>829</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.1093/brain/107.3.829</pub-id><pub-id pub-id-type="pmid">6206910</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warrington</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>The selective impairment of semantic memory</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>27</volume><fpage>635</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.1080/14640747508400525</pub-id><pub-id pub-id-type="pmid">1197619</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wright</surname> <given-names>P</given-names></name><name><surname>Randall</surname> <given-names>B</given-names></name><name><surname>Clarke</surname> <given-names>A</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The perirhinal cortex and conceptual processing: Effects of feature-based statistics following damage to the anterior temporal lobes</article-title><source>Neuropsychologia</source><volume>76</volume><fpage>192</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.01.041</pub-id><pub-id pub-id-type="pmid">25637774</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuo</surname> <given-names>J</given-names></name><name><surname>Fan</surname> <given-names>L</given-names></name><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Yu</surname> <given-names>C</given-names></name><name><surname>Jiang</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Connectivity profiles reveal a transition subarea in the parahippocampal region that integrates the anterior temporal-posterior medial systems</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>2782</fpage><lpage>2795</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1975-15.2016</pub-id><pub-id pub-id-type="pmid">26937015</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.31873.024</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Rust</surname><given-names>Nicole</given-names></name><role>Reviewing Editor</role><aff id="aff6"><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Integrative and distinctive coding of perceptual and conceptual object features in the ventral visual stream&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Anna C Schapiro (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>This paper aims to uncover where in the brain perceptual and conceptual information is integrated. The authors compare behavioral and neural similarity amongst objects processed in visual and conceptual task contexts. They find that visual structure is represented in LOC, conceptual structure is represented in the temporal pole and parahippocampal cortex, and that perirhinal cortex is uniquely sensitive to both perceptual and conceptual information, across task contexts, suggesting that this is where conceptual and perceptual information is integrated.</p><p>The reviewers commented on the elegant nature of the study design, the stringent analysis, and how clean results were in the context of brain areas that are often difficult to get signals from (with fMRI). The paper was judged to be of broad interest and impactful.</p><p>Essential revisions:</p><p>The following issues were highlighted as items that must be addressed before publication.</p><p>1) It looks like the statistics were computed using objects – not participants – as the random effects factor, which makes it unclear if we can generalize the findings to the population. The correlation between behavioral and neural RDMs could be calculated for each individual, and then statistical testing could be done across these 16 (fisher-transformed) values. Or it may make sense to run the permutation test within each individual and then compute group statistics across the 16 zscores of the individuals relative to their own null distributions. The object-based statistics are useful, but additionally reporting these participant-based statistics will allow the reader to understand whether these findings are likely to generalize to new participants.</p><p>2) These analyses tell us about how relationships between objects are represented similarly or differently in different task contexts, but it seems like it would be useful to also report how similarly the objects themselves are represented in different task contexts. In other words, in an RDM with visual brain response as rows and conceptual as columns, does perirhinal (and perhaps temporal pole) have the strongest diagonal in that matrix?</p><p>3) There looks to be a strong interaction effect in perirhinal cortex, with its patterns of activity showing more similarity to the behavior-based visual RDM vs. the behavior-based conceptual RDM when the task is visual, and vice versa when the task is conceptual. Could the authors assess this interaction? If the authors are concerned about the assumptions of an ANOVA being violated, then perhaps a non-parametric test of an interaction can be used. The presence of an interaction does not in any way contradict what the authors are stating, but would instead add to it by suggesting that, on top of PRC representing both conceptual and perceptual information regardless of task, there is a small modulation by the task state or attentional state.</p><p>4) There is something a little odd about the depiction of the visual and conceptual RDMs in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Why are there vertical &quot;streaks&quot; though the columns? Shouldn't these RDMs be completely symmetrical? Perhaps this is an artifact of the normalization procedure, which is mentioned in the methods but not described in detail. As a general point, although it is fine to scale the RDMs to use the full light/dark range, I think that it would give a better idea of the similarity space if the actual values of the similarity measures were used rather than percentile scores. Also, in the uploaded RDMs, it is puzzling to me that the values for dissimilar pairs are all 1 for the visual RDM, as this was based on a 5-point Likert scale. Surely the same value wasn't obtained for every object pairing.</p><p>5) Although I think that the procedure used to obtain behavioral measures of conceptual and perceptual similarity is a strength of the paper, it would be useful to know how these measures compare to other measures of conceptual similarity, like WordNet distances, or text corpus co-occurrence, and whether similar results are obtained when these other measures are used.</p><p>6) Although the paper focuses primarily on the ROIs, the results of the searchlight analysis will be of considerable interest to many readers and deserve more emphasis. I suggest that the brains in <xref ref-type="fig" rid="fig7">Figure 7A and 7B</xref> be made larger (perhaps by removing the RDMs, which take up considerable space but aren't really essential). In addition, it would be useful to plot the borders of the ROIs as outlines on the brains so that the consistency between the ROI boundaries and the searchlight analyses could be visually assessed. (As an aside, the fact that PRC is the only area showing overlap between the visual and conceptual effects in the searchlight analysis is very impressive and really underscores the strength of the effect.)</p><p>7) The results in PHC are very interesting, but the use of the PHC ROI is not strongly justified in the paper, and the implications of this result are not discussed at all. Although the paper states that PHC has been implicated by previous work on conceptual knowledge, the papers that are referenced (by Bar and Aminoff, and Ranganath) discussed a very specific kind of conceptual knowledge: knowledge about co-occurrence of objects within the same context. In my opinion, an important aspect of the current results is that they offer an important new data point in support of this idea. One possible interpretation of the PHC results is that participants bring to mind a contextual setting for the object when they perform the conceptual tasks but not when they perform the perceptual tasks. For example, &quot;comb&quot; and &quot;hairdryer&quot; might bring to mind a bathroom or a barbershop, but only when thinking about the conceptual meaning of these objects, not when thinking about their shape or color.</p><p>8) It would be useful to have more precise information about the locus of the PHC effect relative to the &quot;parahippocampal place area&quot; (PPA), which tends to extend posterior of PHC proper. Several papers suggest an anterior/posterior division within the PPA whereby the more anterior portions represent more abstract/conceptual information and the more posterior portions represent more visual/spatial information (e.g. Baldassano et al., 2013; Marchette et al., 2015; Aminoff et al., 2006). The functional localizer includes both scenes and objects so it should be possible for the authors to identify the PPA, and thus report whether their effects are in PPA or not, and if so, if they are in the more anterior portion.</p><p>9) I wonder if the authors could comment on whether they think the results would change if pictures, rather than words, were used for the objects in the fMRI experiment. For example, might LOC represent visual similarity structure in a task-invariant way if pictures are presented, because their visual features would be processed more automatically than if words are presented, and visual features of the objects have to be brought to mind? I was also curious if the authors could explain their motivation for using words rather than images – was it to force participants to bring to mind both visual and conceptual information?</p><p>10) It is notable that conceptual effects were limited to PRC, PHC, and TP, and were not found in other regions of the brain. What implications does this have in light of previous work that has found a wider distribution of conceptual regions? For example, Fairhall and Caramazza (2013) identified &quot;amodal&quot; conceptual processing in inferior temporal gyrus, posterior cingulate, angular gyrus, prefrontal regions? What about the ventral stream regions outside of LO, PRC, and PHC, like the fusiform gyrus? (I'm not actually suggesting that the authors use these regions as ROIs, but more focus on the whole-brain results and comparison to these earlier findings would be recommended.)</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Integrative and distinctive coding of visual and conceptual object features in the ventral visual stream&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Timothy Behrens (Senior Editor), a Reviewing Editor, and three reviewers.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>1) It was a great idea to put the ROI boundaries on the brains depicting the searchlight results. I'd recommend making the ROI borders a bit thicker, though, and perhaps changing their colors, to make them easier to see. The perirhinal and parahippocampal ROIs are fairly clear, but the LOC and temporal pole less so.</p><p>2) Now that the ROI outlines have been added to the searchlight results, it is apparent (<xref ref-type="fig" rid="fig10">Figure 10B</xref>) that the locus of conceptual decoding in parahippocampal cortex straddles the perirhinal/parahippocampal border. This fact is perhaps worth mentioning in the text. At present, the overlap with anterior PPA is emphasized, but one might equally emphasize that the overlap with PPA – and even PHC – is only partial.</p><p>3) Also, there seems to be an error in the caption for <xref ref-type="fig" rid="fig10">Figure 10B</xref>, which is described as depicting correlation between brain-based VISUAL and behavior-based conceptual RDMs – both should be conceptual. The label on the figure itself is different (and presumably correct).</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.31873.025</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The following issues were highlighted as items that must be addressed before publication.</p><p>1) It looks like the statistics were computed using objects – not participants – as the random effects factor, which makes it unclear if we can generalize the findings to the population. The correlation between behavioral and neural RDMs could be calculated for each individual, and then statistical testing could be done across these 16 (fisher-transformed) values. Or it may make sense to run the permutation test within each individual and then compute group statistics across the 16 zscores of the individuals relative to their own null distributions. The object-based statistics are useful, but additionally reporting these participant-based statistics will allow the reader to understand whether these findings are likely to generalize to new participants.</p></disp-quote><p>We thank the reviewers for this important suggestion, and agree that reporting results from participant-based random-effects analyses will provide a more complete understanding of our data. As such, we now report results from inferential statistical analyses, using participants as a random factor, for all ROI-based aspects of our study. Source data files containing participant-specific correlations are now linked to each ROI-based results figure. We tested for significant positive correlations in each analysis by calculating participant-specific similarity indices (Kendall’s tau-a) between RDMs, which were then tested across using a one-sided Wilcoxon signed-rank test. This non-parametric test provides valid inference and treats the variation across participants as a random effect, thus supporting generalization of results beyond the sample (Nili et al., 2014).</p><p>Our new results are described throughout the Results section of our revised manuscript. In short, the overall pattern of results revealed by our random-effects analyses was not meaningfully different from that reported in our initial submission. Specifically, we still obtain evidence for visual similarity coding in LOC, conceptual similarity coding in the temporal pole and parahippocampal cortex, and both visual and conceptual similarity coding in PRC. Given the similarities in results across statistical procedures, together with an interest in avoiding confusion, we have replaced (rather than augmented) all fixed-effects results with these statistically more stringent random-effects results.</p><disp-quote content-type="editor-comment"><p>2) These analyses tell us about how relationships between objects are represented similarly or differently in different task contexts, but it seems like it would be useful to also report how similarly the objects themselves are represented in different task contexts. In other words, in an RDM with visual brain response as rows and conceptual as columns, does perirhinal (and perhaps temporal pole) have the strongest diagonal in that matrix?</p></disp-quote><p>We have addressed this theoretically important question using the analytical approach recommended by the reviewers (i.e., a first-level RSA). All pertinent results are reported in a new paragraph titled “ROI-Based RSA: Comparisons of Within-Object Multi-Voxel Activity Patterns Across Different Task Contexts”. Specifically, we first calculated one dissimilarity value (1 – Pearson’s <italic>r</italic>) between the mean multi-voxel activity patterns evoked by a given object concept across different task contexts. These 40 within-object dissimilarity values were expressed along the diagonal of an RDM for each ROI in each participant (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). We next calculated mean within-object dissimilarity (i.e., the mean of the diagonal in the RDM) for the purpose of performing statistical inference.</p><p>Results are presented in <xref ref-type="fig" rid="fig8">Figure 8B</xref>. Within-object similarity did not differ from zero in either LOC (Pearson’s r =.007, p =.20) or parahippocampal cortex (Pearson’s r = -.008, p =.87), suggesting that a given object concept was represented differently across the visual and conceptual task contexts in these ROIs. These findings are consistent with the task-dependent nature of the similarity codes we observed in these regions (<xref ref-type="fig" rid="fig6">Figures 6A, 6B</xref>). Conversely, within-object similarity was significantly greater than zero in PRC (Pearson’s r = 0.41, p &lt;.01, Bonferroni corrected for four comparisons), and the temporal pole (Pearson’s r = 0.34, p &lt;.05, Bonferroni corrected for four comparisons). These results indicate that a given object concept was represented similarly across task contexts in these structures. In the temporal pole, this reflected the fact that conceptual object information (e.g., “used to style hair” and “found in salons”) was emphasized across task contexts (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). By contrast, we view high within-object similarity in PRC as further evidence of integrative coding, as this structure appeared to carry conceptual (e.g., “used to style hair” and “found in salons”) and visual (e.g., visually similar to a gun) object information in both task contexts (<xref ref-type="fig" rid="fig6">Figure 6C</xref>).</p><disp-quote content-type="editor-comment"><p>3) There looks to be a strong interaction effect in perirhinal cortex, with its patterns of activity showing more similarity to the behavior-based visual RDM vs. the behavior-based conceptual RDM when the task is visual, and vice versa when the task is conceptual. Could the authors assess this interaction? If the authors are concerned about the assumptions of an ANOVA being violated, then perhaps a non-parametric test of an interaction can be used. The presence of an interaction does not in any way contradict what the authors are stating, but would instead add to it by suggesting that, on top of PRC representing both conceptual and perceptual information regardless of task, there is a small modulation by the task state or attentional state.</p></disp-quote><p>The notion of task-based modulation is an interesting proposition that has been widely discussed in the context of representational accounts of PRC functioning (see Graham et al., 2010 for review). Here, we addressed this essential issue by first converting Kendall’s tau-a values obtained in PRC to Pearson’s <italic>r</italic> coefficients (<italic>r</italic> = sin (½ π tau-a)), which were then Fisher-<italic>z</italic> transformed. These transformations were guided by procedures developed by Walker (2003). In line with the reviewers’ suspicion, a 2 (task context) x 2 (behaviour-based model) repeated-measures ANOVA revealed a behavior-based model x fMRI task context task interaction that neared, but did not reach, significance (F(1,15) = 3.48, p =.082). This result is now reported in our revised our manuscript (subsection “Perirhinal Cortex Represents Object Concepts in a Task-Invariant Similarity Code that Reflects Integration of Visual and Conceptual Features”, second paragraph).</p><disp-quote content-type="editor-comment"><p>4) There is something a little odd about the depiction of the visual and conceptual RDMs in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Why are there vertical &quot;streaks&quot; though the columns? Shouldn't these RDMs be completely symmetrical? Perhaps this is an artifact of the normalization procedure, which is mentioned in the methods but not described in detail. As a general point, although it is fine to scale the RDMs to use the full light/dark range, I think that it would give a better idea of the similarity space if the actual values of the similarity measures were used rather than percentile scores. Also, in the uploaded RDMs, it is puzzling to me that the values for dissimilar pairs are all 1 for the visual RDM, as this was based on a 5-point Likert scale. Surely the same value wasn't obtained for every object pairing.</p></disp-quote><p>We are grateful to the reviewers for noting the asymmetries in our depictions of the behaviour-based RDMs. This was indeed an artifact of the normalization procedure. The issue has been rectified in our revised manuscript, where we now depict raw dissimilarity values (1 – Kendall’s tau-a), rather than percentile scores, in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p><p>With respect to values in the behaviour-based visual RDM, it is the case that the majority of object pairs were unanimously (N = 15) endorsed with the lowest similarity rating. For example, all participants responded “not at all visually similar (response option 1)” when asked to rate the visual similarity between antlers – hairdryer, gun – pillow, and pen – compass. On average, this was the case for the relationship between a given object concept and 35.95 out of 40 other object concepts. This pattern reflects the result of a deliberate data-driven approach to select a stimulus set in which visual and conceptual similarities were not confounded across objects. Beginning with an initial stimulus set of 80 items, we systematically excluded object concepts that were visually similar (even to a small degree) to many other objects. Similar criteria were used with respect to conceptual similarity, resulting in an average of 35.2 pairs of object concepts for which there were zero common conceptual features. Selecting stimuli that yielded sparse behaviour-based RDMs was necessary to ensure that conceptual similarities did not correlate with visual similarities. Ultimately, we view the sparsity of our behavior-based RDMs as a strength of our experimental design.</p><disp-quote content-type="editor-comment"><p>5) Although I think that the procedure used to obtain behavioral measures of conceptual and perceptual similarity is a strength of the paper, it would be useful to know how these measures compare to other measures of conceptual similarity, like WordNet distances, or text corpus co-occurrence, and whether similar results are obtained when these other measures are used.</p></disp-quote><p>The reviewer has raised an important question regarding the degree to which our results generalize across conceptual similarity models obtained using different procedures. To address this issue, we have implemented a word2vec language model, which mapped 3 million words to 300 feature vectors in a high-dimensional space. The model was trained using ~100 billion words from a Google News dataset. From this model we calculated the cosine similarity (i.e., semantic similarity) between feature vectors for all pairs of words in our stimulus set. These data were expressed in a 40x40 word2vec RDM. The word2vec model is now included in <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>.</p><p>Importantly, the word2vec RDM was significantly correlated with our behavior-based conceptual RDM (Kendall’s tau-a =.11, SE =.0141, <italic>p</italic> &lt;.00001), suggesting that both models capture the conceptual similarity structure among the object concepts. However, the word2vec RDM was also significantly correlated with our behavior-based visual RDM (Kendall’s tau-a =.04, SE =.0130, <italic>p</italic> &lt;.001). This result suggests that, whereas our behavior-based conceptual RDM captured semantic similarity as it is narrowly defined as conceptual object features, the word2vec RDM may have captured a broader definition of semantic similarity, i.e., one that includes both visual semantics and abstract conceptual features. Consistent with this view, gun and hairdryer were conceptually unrelated in our behavior-based conceptual RDM (cosine = 0), whereas the word2vec RDM suggested modest conceptual similarity (cosine =.16). Although this difference is likely determined by multiple factors, it is important to note that gun and hairdryer had a relatively high visual similarity index in our behaviour-based visual RDM (normalized mean rating =.58). Ultimately, these data highlight a theoretically important distinction between behaviorally-derived conceptual feature-based statistics and corpus-based estimates of semantic similarity. We report comparisons between our behavior-based RDMs and the word2vec RDM in a new section titled “Comparison of Behavior-Based RDMs with a Corpus-Based Semantic RDM<italic>”</italic>.</p><p>For the purpose of comparison, we next compared the word2vec RDM with brain-based RDMs using the same procedures described in the previous section. Results are presented in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. These analyses revealed significant positive correlations between the word2vec RDM and the brain-based conceptual task RDMs in parahippocampal cortex (Kendall’s tau-a =.05, <italic>p</italic> &lt;.01), PRC (Kendall’s tau-a =.035, <italic>p</italic> &lt;.01), and the temporal pole (Kendall’s tau-a =.029, <italic>p</italic> &lt;.01). The word2vec RDM was also significantly correlated with the brain-based visual task RDMs in PRC (Kendall’s tau-a =.025, <italic>p</italic> &lt;.05) and the temporal pole (Kendall’s tau-a =.027, <italic>p</italic> &lt;.05). Thus, the pattern of results obtained using the word2vec RDM was identical to that obtained using the behavior-based conceptual RDMs in parahippocampal cortex, PRC, and the temporal pole. Interestingly, however, the word2vec RDM was also significantly correlated with the brain-based visual task RDMs in LOC (Kendall’s tau-a =.028, <italic>p</italic> &lt;.05). This result is consistent with the observation that the word2vec RDM was significantly correlated with our behavior-based visual RDM, and further suggests that corpus-based models of semantic memory likely capture similarities between object concepts at the level of abstract conceptual properties and visual semantics. These results are reported in a new section titled “ROI-Based RSA: Comparison of Corpus-Based (word2vec) Semantic RDM with Brain-Based RDMs”.</p><disp-quote content-type="editor-comment"><p>6) Although the paper focuses primarily on the ROIs, the results of the searchlight analysis will be of considerable interest to many readers and deserve more emphasis. I suggest that the brains in <xref ref-type="fig" rid="fig7">Figure 7A and 7B</xref> be made larger (perhaps by removing the RDMs, which take up considerable space but aren't really essential). In addition, it would be useful to plot the borders of the ROIs as outlines on the brains so that the consistency between the ROI boundaries and the searchlight analyses could be visually assessed. (As an aside, the fact that PRC is the only area showing overlap between the visual and conceptual effects in the searchlight analysis is very impressive and really underscores the strength of the effect.)</p></disp-quote><p>We thank the reviewers for this suggestion. We have increased the size of the searchlight similarity maps and deconstructed what was previously a composite figure. Results are now presented across three separate figures: similarity maps from the visual task context in <xref ref-type="fig" rid="fig9">Figure 9</xref>, from the conceptual task context in <xref ref-type="fig" rid="fig10">Figure 10</xref>, and similarity map overlap in <xref ref-type="fig" rid="fig11">Figure 11</xref>. Moreover, we now project the borders of each ROI onto the brains. Thanks to this advice, the neuroanatomical specificity of our results is now particularly apparent.</p><disp-quote content-type="editor-comment"><p>7) The results in PHC are very interesting, but the use of the PHC ROI is not strongly justified in the paper, and the implications of this result are not discussed at all. Although the paper states that PHC has been implicated by previous work on conceptual knowledge, the papers that are referenced (by Bar and Aminoff, and Ranganath) discussed a very specific kind of conceptual knowledge: knowledge about co-occurrence of objects within the same context. In my opinion, an important aspect of the current results is that they offer an important new data point in support of this idea. One possible interpretation of the PHC results is that participants bring to mind a contextual setting for the object when they perform the conceptual tasks but not when they perform the perceptual tasks. For example, &quot;comb&quot; and &quot;hairdryer&quot; might bring to mind a bathroom or a barbershop, but only when thinking about the conceptual meaning of these objects, not when thinking about their shape or color.</p></disp-quote><p>We agree that we should have more explicitly motivated the inclusion of PHC as an ROI. Accordingly, we have revised our Introduction to provide rationale for targeting PHC (and all other ROIs). Specifically, we describe the proposal that the PHC is necessary for processing contextual associations, including the co-occurrence of objects in a common context (e.g., “comb” and “hairdryer” in a barbershop; Aminoff et al., 2013). We reason that objects that are regularly encountered in the same context often share many conceptual features (e.g., “used to style hair”). To the extent that shared conceptual features directly shape contextual meaning, object-evoked responses in parahippocampal cortex may express conceptual similarity structure.</p><p>In our revised Discussion (seventh paragraph), we now consider the implications of our findings in PHC for pertinent theoretical models. We agree that the correlation we revealed between our behaviour-based conceptual RDM and brain-based conceptual task RDMs could be interpreted as support for the notion that PHC represents contextual associations. We note, however, that the current study was not designed to test specific hypotheses about the contextual co-occurrence of objects, or how co-occurrence relates to conceptual feature statistics. Ultimately, a mechanistic account of object-based coding in PHC will require further research using a carefully selected stimulus set in which the strength of contextual associations (i.e., co-occurrence) between object concepts is not confounded with conceptual features.</p><disp-quote content-type="editor-comment"><p>8) It would be useful to have more precise information about the locus of the PHC effect relative to the &quot;parahippocampal place area&quot; (PPA), which tends to extend posterior of PHC proper. Several papers suggest an anterior/posterior division within the PPA whereby the more anterior portions represent more abstract/conceptual information and the more posterior portions represent more visual/spatial information (e.g. Baldassano et al., 2013; Marchette et al., 2015; Aminoff et al., 2006). The functional localizer includes both scenes and objects so it should be possible for the authors to identify the PPA, and thus report whether their effects are in PPA or not, and if so, if they are in the more anterior portion.</p></disp-quote><p>We have conducted a new analysis to better understand how conceptual coding in PHC relates to PPA. Specifically, we first identified PPA in individual participants using whole-brain univariate GLMs (scenes &gt; objects, liberal statistical threshold of <italic>p</italic> &lt;.01 uncorrected). Using this approach, we localized a contiguous cluster of voxels that extended from posterior PHC into lingual gyrus in 14/16 participants. We next identified group-level PPA ROIs by performing a whole-brain random-effects univariate analysis with these 14 participants (FDR <italic>p</italic> &lt;.05; right PPA peak MNI coordinates 22, -41, -13, cluster extant of 544 voxels with 27% overlapping posterior PHC; left PPA peak MNI coordinates -23, -44, -12, cluster extant of 512 voxels with 29% overlap with posterior PHC). Interestingly, the anterior extant of the right PPA ROI did overlap (26 voxels) with the cluster of voxels in right PHC that showed evidence of conceptual coding in the conceptual task (64 voxels). Thus, 41% of the conceptual coding cluster in PHC was situated within PPA. However, this set of voxels represented only 5% of PPA. This result is now reported in our revised Results section (subsection “Perirhinal Cortex is the Only Cortical Region that Supports Integrative Coding of Conceptual and Visual Object Features”, second paragraph).</p><disp-quote content-type="editor-comment"><p>9) I wonder if the authors could comment on whether they think the results would change if pictures, rather than words, were used for the objects in the fMRI experiment. For example, might LOC represent visual similarity structure in a task-invariant way if pictures are presented, because their visual features would be processed more automatically than if words are presented, and visual features of the objects have to be brought to mind? I was also curious if the authors could explain their motivation for using words rather than images – was it to force participants to bring to mind both visual and conceptual information?</p></disp-quote><p>The question of whether our results would change if pictures were used is very interesting, in particular as it relates to integrative coding in PRC. First, our rationale for using words, rather than images, in the current study reflects our specific interest in understanding pre-existing representations of object concepts, rather than bottom-up perceptual processing. Within this context, using words ensured that conceptual and visual features were extracted from pre-existing representations of object concepts. That is to say, both conceptual and visual features were arbitrarily related to the physical input (i.e., the orthography of the word). We are now explicit about this point in our revised manuscript (subsection “fMRI Task and Behavioral Results”, first paragraph).</p><p>We are actively conducting follow-up research that investigates where in the brain the content of bottom-up perceptual object processing interfaces with corresponding abstract conceptual knowledge. One challenge associated with using pictures is identifying prototypical pictorial exemplars that do not have idiosyncratic visual properties that bias attention toward individual features and away from the multiple features that together lend themselves to conceptual meaning. Moreover, using pictures may introduce systematic differences between decision making strategies in the visual and conceptual verification task contexts. For example, when pictures are used as stimuli, visual features are accessible from the pictorial cue, whereas conceptual features require abstraction from the cue.</p><p>These issues notwithstanding, our prediction is that using pictures would reveal task-invariant visual similarity coding in LOC, in line with the reviewers’ suggestion. In PHC, we would still predict conceptual similarity coding, but we do not have strong predictions about whether using pictures would influence task specificity. We would not anticipate any significant differences between words and pictures in the temporal pole, as results from previous research, including that conducted with semantic dementia, have linked this structure to conceptual coding irrespective of input modality (Patterson et al., 2007). Most importantly, we predict that PRC would show evidence of integrative coding regardless of whether words or pictures are used as stimuli.</p><disp-quote content-type="editor-comment"><p>10) It is notable that conceptual effects were limited to PRC, PHC, and TP, and were not found in other regions of the brain. What implications does this have in light of previous work that has found a wider distribution of conceptual regions? For example, Fairhall and Caramazza (2013) identified &quot;amodal&quot; conceptual processing in inferior temporal gyrus, posterior cingulate, angular gyrus, prefrontal regions? What about the ventral stream regions outside of LO, PRC, and PHC, like the fusiform gyrus? (I'm not actually suggesting that the authors use these regions as ROIs, but more focus on the whole-brain results and comparison to these earlier findings would be recommended.)</p></disp-quote><p>We agree that it is important to situate our results into the context of related studies. To this end, we first note that previous fMRI studies of semantic memory have reliably identified a network of cortical regions that contribute to semantic memory decisions, including inferior frontal gyrus, superior frontal gyrus, middle temporal gyrus, ventromedial prefrontal cortex, fusiform gyrus, the posterior cingulate, supramarginal gyrus, and the angular gyrus (Binder et al., 2009). Interestingly, when we investigated brain regions more strongly activated by our conceptual task context relative to our visual task context using a whole-brain univariate GLM analysis (FDR p &lt;. 05), we found a set of regions that converges with this network. Namely, we see differential conceptual task involvement in inferior frontal gyrus, fusiform gyrus, middle temporal gyrus, and inferior temporal gyrus (<xref ref-type="fig" rid="respfig1">Author Response Image 1</xref>).</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.31873.023</object-id><label>Author response image 1.</label><caption/><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-31873-resp-fig1-v3"/></fig><p>Although these consistencies between the current study and previous research pertaining to semantic memory are encouraging, they do not speak to the important question of whether a given region actually represents semantic content. Rather, this is typically inferred when representational distinctions captured by conceptual similarity models map onto corresponding distinctions among object-specific neural representations. Results from RSA-based studies that have used this approach tend to reveal semantic similarity coding in a relatively limited number of brain regions. For example, Bruffaerts et al., (2013), and Clarke and Tyler (2014) found semantic similarity effects in PRC only. Related studies point to the temporal pole as a region that supports conceptual similarity coding (Peelen and Caramazza, 2012; Chadwick et al., 2016). Lastly, an examination of semantic similarities between stimulus categories found evidence for object-based similarity coding in only middle temporal gyrus and the posterior cingulate (Fairhall and Caramazza (2013).</p><p>Against this background, we are not surprised by the fact that we revealed conceptual similarity coding in only three brain regions. Further, we emphasize that our effects in PRC and the temporal pole are consistent with those reported by Bruffaerts et al. (2013), Clarke and Tyler (2014), Peelen and Caramazza (2012), and Chadwick et al. (2016). We now acknowledge these consistencies in our manuscript where we report searchlight-based RSA results (subsection “Perirhinal Cortex is the Only Cortical Region that Supports Integrative Coding of Conceptual and Visual Object Features”).</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>1) It was a great idea to put the ROI boundaries on the brains depicting the searchlight results. I'd recommend making the ROI borders a bit thicker, though, and perhaps changing their colors, to make them easier to see. The perirhinal and parahippocampal ROIs are fairly clear, but the LOC and temporal pole less so.</p><p>We agree that the ROI boundaries could be more salient. Accordingly, we have increased the thickness of all borders in <xref ref-type="fig" rid="fig9">Figures 9</xref>, <xref ref-type="fig" rid="fig10">10</xref>, and 11. We have also slightly modified some colors to improve visibility.</p><p>2) Now that the ROI outlines have been added to the searchlight results, it is apparent (<xref ref-type="fig" rid="fig10">Figure 10B</xref>) that the locus of conceptual decoding in parahippocampal cortex straddles the perirhinal/parahippocampal border. This fact is perhaps worth mentioning in the text. At present, the overlap with anterior PPA is emphasized, but one might equally emphasize that the overlap with PPA – and even PHC – is only partial.</p></disp-quote><p>The point regarding the neuroanatomical specificity of the searchlight result in question is well taken. As such, we now describe the cluster in which we find evidence of conceptual similarity coding in the conceptual task context as being situated at the parahippocampal / perirhinal border. Moreover, we have softened our language regarding overlap with PPA. The revised text reads as follows:</p><disp-quote content-type="editor-comment"><p>“Next, we revealed conceptual similarity coding in the conceptual task context within a cluster of voxels that straddled the border between right parahippocampal cortex and PRC (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). Although this cluster was only partially situated with parahippocampal cortex, it is interesting to note that its posterior extent did slightly encroach upon anterior aspects of the parahippocampal place area (PPA; functionally defined using a group-level GLM (scenes &gt; objects); Epstein and Kanwisher, 1998), which has previously been linked to the representation of abstract conceptual information (Aminoff et al., 2006; Baldassano et al., 2013; Marchette et al., 2015).”</p><p>3) Also, there seems to be an error in the caption for <xref ref-type="fig" rid="fig10">Figure 10B</xref>, which is described as depicting correlation between brain-based VISUAL and behavior-based conceptual RDMs – both should be conceptual. The label on the figure itself is different (and presumably correct).</p></disp-quote><p>We are grateful to the reviewer/Editor for having caught this error. The caption has been revised to indicate that the similarity map depicts significant correlations between brain-based conceptual task RDMs and the behavior-based conceptual RDM.</p></body></sub-article></article>