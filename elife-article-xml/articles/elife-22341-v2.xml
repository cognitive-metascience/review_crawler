<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">22341</article-id><article-id pub-id-type="doi">10.7554/eLife.22341</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Bottom-up and top-down computations in word- and face-selective cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-71138"><name><surname>Kay</surname><given-names>Kendrick N</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6604-9155</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-72297"><name><surname>Yeatman</surname><given-names>Jason D</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2686-1293</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Center for Magnetic Resonance Research, Department of Radiology</institution>, <institution>University of Minnesota</institution>, <addr-line><named-content content-type="city">Minneapolis</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Institute for Learning and Brain Sciences</institution>, <institution>University of Washington</institution>, <addr-line><named-content content-type="city">Seattle</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Speech and Hearing Sciences</institution>, <institution>University of Washington</institution>, <addr-line><named-content content-type="city">Seattle</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Reviewing editor</role><aff id="aff4"><institution>University of Pennsylvania</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>kay@umn.edu</email> (KNK);</corresp><corresp id="cor2"><email>jyeatman@uw.edu</email> (JDY)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>22</day><month>02</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e22341</elocation-id><history><date date-type="received"><day>13</day><month>10</month><year>2016</year></date><date date-type="accepted"><day>19</day><month>02</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Kay et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Kay et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-22341-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.22341.001</object-id><p>The ability to read a page of text or recognize a person's face depends on category-selective visual regions in ventral temporal cortex (VTC). To understand how these regions mediate word and face recognition, it is necessary to characterize how stimuli are represented and how this representation is used in the execution of a cognitive task. Here, we show that the response of a category-selective region in VTC can be computed as the degree to which the low-level properties of the stimulus match a category template. Moreover, we show that during execution of a task, the bottom-up representation is scaled by the intraparietal sulcus (IPS), and that the level of IPS engagement reflects the cognitive demands of the task. These results provide an account of neural processing in VTC in the form of a model that addresses both bottom-up and top-down effects and quantitatively predicts VTC responses.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22341.001">http://dx.doi.org/10.7554/eLife.22341.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.22341.002</object-id><title>eLife digest</title><p>As your eyes scan this page, your visual system performs a series of computations that allow you to derive meaning from the printed words. The visual system solves this task with such apparent ease that you may never have thought about the challenges that your brain must overcome for you to read a page of text. The brain must overcome similar challenges to enable you to recognize the faces of your friends.</p><p>Two factors affect how the neurons in the visual system respond to what you are looking at: the physical features of the object and your cognitive state (for example, your knowledge, past experiences, and the cognitive demands of the task at hand). To figure out exactly how these factors influence the responses of the neurons, Kay and Yeatman used functional magnetic resonance imaging to scan the brains of human volunteers as they viewed different images (some of which were of faces or words). The volunteers had to perform various tasks while viewing the images. These tasks included focusing their attention on a small dot, categorizing the image, and stating whether the image had previously been shown.</p><p>From the brain imaging data, Kay and Yeatman developed a model of the brain circuits that enable faces and words to be recognized. The model separately characterizes the influence of physical features and the influence of cognitive state, and describes several different types of processing: how the brain represents what is seen, how it makes a decision about how to respond, and how it changes its own activity when carrying out the decision. The model has been made freely available online so that other researchers can reproduce and build upon Kay and Yeatman’s findings.</p><p>The current model is not perfect, and it does not describe neural activity in fine detail. However, by obtaining new experimental measurements, the model could be systematically improved.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22341.002">http://dx.doi.org/10.7554/eLife.22341.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>computational model</kwd><kwd>visual cortex</kwd><kwd>fMRI</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009607</institution-id><institution>McDonnell Center for Systems Neuroscience</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Kay</surname><given-names>Kendrick N</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007268</institution-id><institution>Washington University in St. Louis</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Kay</surname><given-names>Kendrick N</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>BCS-1551330</award-id><principal-award-recipient><name><surname>Yeatman</surname><given-names>Jason D</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A computational model reveals how response properties of category-selective regions in the visual cortex reflect both bottom-up stimulus-driven signals and top-down attentional signals from the parietal cortex.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>How does visual cortex work? One approach to answering this question consists in building functional models that characterize the computations that are implemented by neurons and their circuitry (<xref ref-type="bibr" rid="bib38">Hubel and Wiesel, 1963</xref>; <xref ref-type="bibr" rid="bib35">Heeger et al., 1996</xref>). This approach has been fruitful for the front end of the visual system, where relatively simple image computations have been shown to characterize the spiking activity of neurons in the retina, thalamus, and V1 (<xref ref-type="bibr" rid="bib13">Carandini et al., 2005</xref>; <xref ref-type="bibr" rid="bib95">Wu et al., 2006</xref>). Based on this pioneering work in electrophysiology, researchers have extended the modeling approach to characterize responses in human visual cortex, as measured by functional magnetic resonance imaging (fMRI) (<xref ref-type="bibr" rid="bib90">Wandell, 1999</xref>; <xref ref-type="bibr" rid="bib24">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="bib48">Kay et al., 2008</xref>).</p><p>Models of early visual processing have been able to offer accurate explanations of low-level perceptual functions such as contrast detection (<xref ref-type="bibr" rid="bib72">Ress et al., 2000</xref>; <xref ref-type="bibr" rid="bib73">Ress and Heeger, 2003</xref>) and orientation discrimination (<xref ref-type="bibr" rid="bib5">Bejjanki et al., 2011</xref>). However, these models are insufficient to explain high-level perceptual functions such as the ability to read a page of text or recognize a face. These abilities are believed to depend on category-selective regions in ventral temporal cortex (VTC), but the computations that give rise to category-selective responses are poorly understood.</p><p>The goal of the present study is to develop a model that predicts fMRI responses in high-level visual cortex of human observers while they perform different cognitive tasks on a wide range of images. We seek a model that is fully computable—that is, a model that can operate on any arbitrary visual image and quantitatively predict BOLD responses and behavior (<xref ref-type="bibr" rid="bib48">Kay et al., 2008</xref>; <xref ref-type="bibr" rid="bib39">Huth et al., 2012</xref>; <xref ref-type="bibr" rid="bib55">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib96">Yamins et al., 2014</xref>). Achieving this goal requires four innovations: First, we need to develop a forward model that characterizes the relationship between visual inputs and the BOLD response in word- and face-selective cortex. Second, we need to dissociate bottom-up stimulus-driven effects from modulation by top-down cognitive processes and characterize how these processes alter the stimulus representation. Third, we need to localize the source of the top-down effects and integrate bottom-up and top-down computations into a single consolidated model. Finally, the neural computations should be linked to the measured behavior of the visual observer. In this study, we make progress on these four innovations and develop a model that characterizes bottom-up and top-down computations in word- and face-selective cortex.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>VTC responses depend on both stimulus properties and cognitive task</title><p>Ventral temporal cortex (VTC) is divided into a mosaic of high-level visual regions that respond selectively to specific image categories, and are believed to play an essential role in object perception (<xref ref-type="bibr" rid="bib47">Kanwisher, 2010</xref>; <xref ref-type="bibr" rid="bib19">Dehaene and Cohen, 2011</xref>). We focus on two specific VTC regions, the visual word form area (VWFA), which selectively responds to words (<xref ref-type="bibr" rid="bib14">Cohen et al., 2000</xref>, <xref ref-type="bibr" rid="bib15">2002</xref>; <xref ref-type="bibr" rid="bib89">Wandell et al., 2012</xref>), and the fusiform face area (FFA), which selectively responds to faces (<xref ref-type="bibr" rid="bib45">Kanwisher et al., 1997</xref>; <xref ref-type="bibr" rid="bib31">Grill-Spector and Weiner, 2014</xref>).</p><p>We measured blood oxygenation level dependent (BOLD) responses to a set of carefully controlled images while manipulating the cognitive task that the subjects performed on the stimuli. The first task was designed to minimize the influence of cognitive processes on sensory processing of the stimulus. Subjects performed a demanding perceptual task on a small dot (0.12° × 0.12°) presented at fixation. In this <italic>fixation task</italic>, the presented stimuli are irrelevant to the subject, and we interpret evoked activity as reflecting primarily the intrinsic, bottom-up response from VTC. We acknowledge that the fixation task may not perfectly isolate bottom-up responses. For example, high-contrast stimuli may automatically attract attention. Moreover, there are other potential interpretations of the fixation task: for example, allocating attention to the small fixation dot might engage active suppression of responses to the presented stimuli.</p><p>To a first approximation, much of the variance in the bottom-up fixation responses from VWFA and FFA is explained by the category of stimulus (<xref ref-type="fig" rid="fig1">Figure 1d</xref>, red lines). However, we find that responses are not invariant to low-level properties of the stimulus: both image contrast and phase coherence modulate response amplitudes. For example, the response to a word in VWFA is 2.4 times stronger when the word is presented at 100% contrast as compared to 3% contrast. These bottom-up effects (see also <xref ref-type="bibr" rid="bib69">Rainer et al., 2001</xref>; <xref ref-type="bibr" rid="bib3">Avidan et al., 2002</xref>; <xref ref-type="bibr" rid="bib99">Yue et al., 2011</xref>; <xref ref-type="bibr" rid="bib63">Nasr et al., 2014</xref>) may be somewhat surprising given that theories of word recognition generally posit that the VWFA response is invariant to low-level features (<xref ref-type="bibr" rid="bib18">Dehaene and Cohen, 2007</xref>, <xref ref-type="bibr" rid="bib19">2011</xref>; <xref ref-type="bibr" rid="bib68">Price and Devlin, 2011</xref>). In fact, it is currently debated whether the VWFA should be considered a visual area or a ‘meta modal’ language region (<xref ref-type="bibr" rid="bib71">Reich et al., 2011</xref>; <xref ref-type="bibr" rid="bib84">Striem-Amit et al., 2012</xref>). Our measurements indicate that when top-down signals are minimized, word- and face-selective cortex is sensitive to low-level image properties, and that an accurate model of the computations performed by these regions must consider not only the stimulus category but also low-level features of the stimulus.<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.22341.003</object-id><label>Figure 1.</label><caption><title>VTC responses depend on both stimulus properties and cognitive task.</title><p>(<bold>a</bold>) <italic>Stimuli</italic>. Stimuli included faces, words, and noise patterns presented at different contrasts and phase-coherence levels, as well as full-contrast polygons, checkerboards, and houses. (<bold>b</bold>) <italic>Trial design</italic>. Each trial consisted of four images drawn from the same stimulus type. (<bold>c</bold>) <italic>Tasks</italic>. On a given trial, subjects performed one of three tasks. (<bold>d</bold>) <italic>Evoked responses in VWFA (top) and FFA (bottom) for different stimuli and tasks</italic>. Color of <italic>x</italic>-axis label indicates the perceived stimulus category as reported by the subjects. Error bars indicate bootstrapped 68% CIs.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22341.003">http://dx.doi.org/10.7554/eLife.22341.003</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22341-fig1-v2"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22341.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Comprehensive summary of fMRI measurements.</title><p>Black bars indicate responses (beta weights) evoked by different stimuli and tasks. Red lines indicate the average response across stimuli, computed separately for each task. Error bars indicate bootstrapped 68% CIs (resampling subjects with replacement). Percentages in ROI labels indicate the strength of the response observed during the categorization and one-back tasks relative to the fixation task. For example, in FFA, the average response across stimuli during the one-back task is 79% stronger than the average response across stimuli during the fixation task. Task effects are substantially stronger in VWFA and FFA than in early visual areas V1–V3. The larger apparent task modulation in V1 compared to V2 and V3 might due to small eye movements that may have been made during the categorization and one-back tasks. Our interpretation of the observed IPS activity during the fixation task is that this activity reflects the decision-making process involved in judging the color of the fixation dot. Support for this interpretation comes from the fact that the root-mean-square contrast of the stimuli, computed over a small region surrounding the fixation dot (0.36° × 0.36°), correlates strongly with IPS responses during the fixation task (<italic>r</italic> = 0.86).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22341.004">http://dx.doi.org/10.7554/eLife.22341.004</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22341-fig1-figsupp1-v2"/></fig></fig-group></p><p>We also measured VTC responses while subjects performed a <italic>categorization task</italic>, in which the subject reports the perceived category of the stimulus, and a <italic>one-back task</italic>, in which the subject detects consecutive repetitions of stimulus frames. Despite the presentation of identical stimuli across the three tasks, there are substantial changes in evoked VTC responses. Responses are larger for the categorization (<xref ref-type="fig" rid="fig1">Figure 1d</xref>, green lines) and one-back tasks (<xref ref-type="fig" rid="fig1">Figure 1d</xref>, blue lines) compared to the fixation task (<xref ref-type="fig" rid="fig1">Figure 1d</xref>, red lines), and we interpret these response increases as reflecting top-down modulation. In some cases, the top-down modulation is even larger than the modulation achieved by manipulation of the stimulus. For example, the VWFA response to 3%-contrast words during the one-back task exceeds the response to 100%-contrast words. Note that the task effects cannot be explained simply by differences in spatial attention: the one-back task produces substantially larger responses than the categorization task despite the fact that both tasks require the locus of spatial attention to be on the stimulus. Task effects in lower-level areas exist but are smaller in size (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>A potential explanation of the top-down modulation is differences in task difficulty (<xref ref-type="bibr" rid="bib72">Ress et al., 2000</xref>). For example, it is presumably more difficult to perceive low-contrast stimuli than high-contrast stimuli, and this may explain why there is a large response enhancement for low- but not high-contrast stimuli (see VWFA contrast-response function for word stimuli in <xref ref-type="fig" rid="fig1">Figure 1d</xref>). Later in this paper, we provide a computational mechanism that could underlie the psychological concept of task difficulty.</p><p>In summary, our measurements indicate that VTC responses cannot be interpreted without specifying the cognitive state of the observer. A complete model of the computations performed by VWFA and FFA must consider the cognitive task in addition to stimulus properties.</p></sec><sec id="s2-2"><title>Model of bottom-up computations in VTC</title><p>Before addressing the influence of top-down factors, we first develop a model of bottom-up responses in VWFA and FFA. Although the field has long understood that stimulus category is a good predictor of evoked responses (<xref ref-type="bibr" rid="bib45">Kanwisher et al., 1997</xref>; <xref ref-type="bibr" rid="bib56">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib31">Grill-Spector and Weiner, 2014</xref>), we do not yet have a computational explanation of this phenomenon. In other words, although we are able to use our own visual systems to assign a label such as ‘word’ or ‘face’ to describe the data, we have not yet identified the operations that enable our visual systems to derive these labels in the first place. An additional limitation of our conceptual understanding is that it fails to account for the sensitivity of VWFA and FFA to low-level image properties. We therefore ask: Is it possible to develop a quantitative characterization of the bottom-up computations that can reproduce observed stimulus selectivity in human VTC?</p><p>Extending an existing computational model of fMRI responses in the visual system (<xref ref-type="bibr" rid="bib48">Kay et al., 2008</xref>, <xref ref-type="bibr" rid="bib50">2013b</xref>, <xref ref-type="bibr" rid="bib51">2013c</xref>), we conceive of a model involving two stages of image computations (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). The first stage consists of a set of local oriented filters, akin to what has been used to model physiological responses in V1 (<xref ref-type="bibr" rid="bib42">Jones and Palmer, 1987</xref>; <xref ref-type="bibr" rid="bib13">Carandini et al., 2005</xref>). The second stage consists of a normalized dot product applied to the outputs of the first stage. This dot product computes how well a given stimulus matches a category template (for example, a word template for VWFA, a face template for FFA). We construct category templates directly from the stimulus set used in the experiment; in a later section we explore how well this approach generalizes. The present model, termed the <italic>Template model</italic>, is almost certainly an oversimplification of the complex nonlinear processing performed in VTC. Nevertheless, the model is theoretically motivated, consistent with hierarchical theories of visual processing (<xref ref-type="bibr" rid="bib27">Fukushima, 1980</xref>; <xref ref-type="bibr" rid="bib35">Heeger et al., 1996</xref>; <xref ref-type="bibr" rid="bib81">Serre et al., 2007</xref>; <xref ref-type="bibr" rid="bib21">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="bib77">Rolls, 2012</xref>), and provides a useful starting point for characterizing the computations that underlie word- and face-selectivity. Moreover, unlike recently popular deep neural network models that also involve hierarchical processing (<xref ref-type="bibr" rid="bib55">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib96">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Güçlü and van Gerven, 2015</xref>), the model we propose is parsimonious with only three free parameters, and is therefore straightforward to fit and interpret (see <xref ref-type="fig" rid="fig2">Figure 2a</xref> and Materials and methods).<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.22341.005</object-id><label>Figure 2.</label><caption><title>Model of bottom-up computations in VTC.</title><p>(<bold>a</bold>) <italic>Model architecture.</italic> The predicted response of the Template model is given by a series of image computations (see Materials and methods). (<bold>b</bold>) <italic>Cross-validation performance.</italic> Black bars indicate bottom-up stimulus-driven responses measured during the fixation task, dark lines and dark dots indicate model predictions (leave-one-stimulus-out cross-validation), and light lines and light dots indicate model fits (no cross-validation). Scatter plots in the inset compare model predictions against the data. The Template model is compared to the Category model which simply predicts a fixed response level for stimuli from the preferred stimulus category and a different response level for all other stimuli (the slight decrease in response as a function of contrast is a result of the cross-validation process). (<bold>c</bold>) <italic>Comparison of performance against control models.</italic> Bars indicate leave-one-stimulus-out cross-validation performance. Error bars indicate 68% CIs, obtained by bootstrapping (resampling subjects with replacement). Solid horizontal lines indicate the noise ceiling, that is, the maximum possible performance given measurement variability in the data. Dotted horizontal lines indicate the cross-validation performance of a model that predicts the same response level for each data point (this corresponds to <italic>R</italic><sup>2</sup> = 0 in the conventional definition of <italic>R</italic><sup>2</sup> where variance is computed relative to the mean). The performance of the Template model degrades if the second stage of nonlinearities is omitted (Template model (only subtractive normalization)) or if the first stage of the model involving V1-like filtering is omitted (Template model (omit first stage)). The plot also shows that the precise configuration of the template is important for achieving high model performance (Template model (non-selective, mixed, random templates)). (<bold>d</bold>) <italic>Performance as a function of spatial frequency tuning.</italic> Here we manipulate the spatial frequency tuning of the filters in the Template model (while fixing spatial frequency bandwidth at one octave). The Template model uses a single set of filters at a spatial frequency tuning of 4 cycles/degree.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22341.005">http://dx.doi.org/10.7554/eLife.22341.005</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22341-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22341.006</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Testing the Template model on a wide range of stimuli.</title><p>(<bold>a</bold>) <italic>Stimuli</italic>. We collected an additional dataset consisting of 92 images from a previous study by <xref ref-type="bibr" rid="bib56">Kriegeskorte et al. (2008)</xref> (all images shown), along with 22 images from the original experiment (three images shown). We assessed model accuracy using 20-fold cross-validation across stimuli (see Materials and methods for details). (<bold>b</bold>) <italic>Performance of Template model (original)</italic>. Black bars indicate data from FFA, with error bars indicating 68% CIs (error across trials). Red lines and red dots indicate model predictions. Inset shows the category template used in the model. The model performs poorly. (<bold>c</bold>) <italic>Performance of Template model (half-max average)</italic>. This model derives the category template by computing (in the V1-like representation) the centroid of all stimuli in the training set that evoke at least half of the maximum response. Performance improves. (<bold>d</bold>) <italic>Performance of Template model (half-max cluster)</italic>. This model derives multiple category templates by performing <italic>k</italic>-means clustering (in the V1-like representation) on all stimuli in the training set that evoke at least half of the maximum response. Performance further improves, resolving both underprediction of responses (for example, green arrow in panel b) and overprediction of responses (for example, blue arrow in panel (<bold>b</bold>). (<bold>e</bold>) <italic>Results for VWFA.</italic> Similar responses are observed across the 92 Kriegeskorte images. Responses are well predicted by the original Template model, up to the level of measurement noise in this region.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22341.006">http://dx.doi.org/10.7554/eLife.22341.006</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22341-fig2-figsupp1-v2"/></fig></fig-group></p><p>Applying the Template model to responses measured during the fixation task, we find that the Template model accurately predicts a large amount of variance in the responses of VWFA and FFA (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). The model outperforms a phenomenological model, termed the <italic>Category model</italic>, that posits that perceived stimulus category is sufficient to predict the response of category-selective regions. Notably, the Template model is able to predict the response to non-preferred stimulus categories in each ROI. This suggests that responses to non-preferred stimuli are meaningful and the result of a well-defined computation performed by the visual system (<xref ref-type="bibr" rid="bib34">Haxby et al., 2001</xref>). The model also outperforms simplified versions of the Template model that include only one of the two processing stages, as well as versions of the Template model in which the category template lacks tuning (non-selective template), is equally weighted between words and faces (mixed template), or is constructed randomly (random template) (<xref ref-type="fig" rid="fig2">Figure 2c</xref>).</p><p>The experiment we have conducted explores a limited range of stimuli. To further assess how well the Template model generalizes, we collected an additional dataset that includes 92 images taken from a previous study of object representation (<xref ref-type="bibr" rid="bib56">Kriegeskorte et al., 2008</xref>). In its original instantiation (<xref ref-type="fig" rid="fig2">Figure 2a</xref>), the Template model uses category templates tailored to the stimuli in the main experiment, and we find that this instantiation of the Template model does not generalize well to the larger stimulus set (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>). However, by implementing a simple model extension in which we use a data-driven approach to estimate category templates, we find that the Template model achieves a reasonable level of accuracy on the new stimulus set (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1d</xref>). This finding validates the basic architecture of the Template model, demonstrates how the Template model might be extended to account for increasingly large ranges of measurements, and provides a promising method to model response properties of other high-level visual regions not investigated here (for example, place- and limb-selective cortex).</p><p>The Template model advances us towards a computational understanding of VTC by demonstrating that BOLD responses in VTC can be predicted based on a template-matching operation on incoming visual inputs filtered by early visual cortex. The present results indicate that although high-level representations are not identical to low-level properties, they are built from, and fundamentally tied to, low-level properties through a series of linear and nonlinear operations. This conclusion is consistent with classic hierarchical theories of visual cortex (<xref ref-type="bibr" rid="bib27">Fukushima, 1980</xref>; <xref ref-type="bibr" rid="bib35">Heeger et al., 1996</xref>; <xref ref-type="bibr" rid="bib81">Serre et al., 2007</xref>; <xref ref-type="bibr" rid="bib21">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="bib77">Rolls, 2012</xref>) and recent evidence that visual features may explain semantic representations found in high-level visual cortex (<xref ref-type="bibr" rid="bib43">Jozwik et al., 2016</xref>). Our model can be viewed as a potential mechanism for how semantic tuning properties emerge in visual cortex (<xref ref-type="bibr" rid="bib39">Huth et al., 2012</xref>). Our results indicate when studying high-level sensory representations in the brain, a precise characterization of the stimulus still matters.</p></sec><sec id="s2-3"><title>Top-down modulation acts as a stimulus-specific scaling</title><p>While the Template model explains bottom-up responses of VWFA and FFA as indexed by the fixation task, it does not explain why responses are higher in these areas during the categorization and one-back tasks (see <xref ref-type="fig" rid="fig1">Figure 1d</xref>). This is simply because the stimuli are identical across the three tasks and the response of the Template model, like that of many computational models of visual processing, is solely a function of the stimulus. Before we can design a model to capture the top-down effects, we must first understand exactly how top-down signals shape the VTC response.</p><p>By visualizing VTC responses as points in a multi-dimensional neural space with VWFA, FFA, and hV4 BOLD response amplitudes as the axes, we see that the responses to words and faces lie on specific manifolds, appearing as ‘arms’ that emanate from the origin (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Importantly, we observe that the categorization and one-back tasks act as a scaling mechanism on the representation observed during the fixation task. The scaling mechanism moves the representation of each stimulus along the arms and away from the origin. Moreover, the amount of scaling is not constant across stimuli but is stimulus-specific, and this is most evident when considering the lowest contrast stimuli (<xref ref-type="fig" rid="fig3">Figure 3</xref>, black dots).<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.22341.007</object-id><label>Figure 3.</label><caption><title>Top-down stimulus-specific scaling of VTC representation.</title><p>(<bold>a</bold>) <italic>Responses plotted in multi-dimensional neural space</italic>. Each dot indicates ROI (VWFA, FFA) responses to a stimulus. In each plot, the black line indicates a linear decision boundary separating words and faces (nearest-centroid classifier, angular distance). (<bold>b</bold>) <italic>Schematics of potential top-down mechanisms</italic> (these models are formally evaluated in <xref ref-type="fig" rid="fig5">Figure 5c</xref>; see Materials and methods section ‘IPS-scaling model’ for details). (<bold>c</bold>) <italic>Categorization and one-back tasks produce stimulus-specific scaling</italic>. Arrows indicate the change in representation compared to the fixation task. (<bold>d</bold>) <italic>Scaling improves readout</italic>. Each data point indicates the signed Euclidean distance between the word-face decision boundary (as determined from the one-back task) and the neural response to a single stimulus. Lines join data points that correspond to the same stimulus. The scaling observed during the categorization and one-back tasks moves responses away from the decision boundary, thereby improving signal-to-noise ratio. (<bold>e</bold>) <italic>Separation of other stimulus categories</italic>. Including hV4 as a third dimension reveals that stimuli categorized as neither words nor faces manifest as a third ‘arm’ that emanates from the origin. Although not reported to be a word by the subjects, the polygon stimulus behaves similarly to word stimuli.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22341.007">http://dx.doi.org/10.7554/eLife.22341.007</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22341-fig3-v2"/></fig></p><p>The visualization also shows that substantial responses to non-preferred categories are present in each ROI (for example, faces in VWFA, words in FFA) and that these responses are scaled during the stimulus-directed tasks. Thus, not only is information regarding non-preferred categories present in each ROI, but this information is actively modulated when subjects perform a perceptual task on those categories. These observations support the view that the brain uses a distributed strategy for perceptual processing and that category-selective regions are components of a more general network of regions that coordinate to extract visual information (<xref ref-type="bibr" rid="bib34">Haxby et al., 2001</xref>; <xref ref-type="bibr" rid="bib17">Cox and Savoy, 2003</xref>). An alternative scheme, more in line with a modular view of perceptual processing (<xref ref-type="bibr" rid="bib46">Kanwisher and Wojciulik, 2000</xref>; <xref ref-type="bibr" rid="bib4">Baldauf and Desimone, 2014</xref>), is area-specific enhancement, in which the representation of a stimulus is enhanced only in the region that is selective for that stimulus (for example, enhancement of words only in VWFA, enhancement of faces only in FFA). This scheme is not supported by our measurements (<xref ref-type="fig" rid="fig3">Figures 3b and 3c</xref>; formal model evaluation is performed in a later section). Rather, response scaling occurs even for non-preferred stimulus categories, and the amount of scaling varies as a function of stimulus properties such as image contrast.</p><p>A simple interpretation of the scaling effects is that they serve to increase signal-to-noise ratio in visually evoked responses in VTC (<xref ref-type="bibr" rid="bib9">Brouwer and Heeger, 2013</xref>). For example, assuming that one use of the stimulus representation in VTC is to discriminate whether the presented stimulus is a word or face (or, more generally, identify the category of the stimulus [<xref ref-type="bibr" rid="bib21">DiCarlo et al., 2012</xref>]), the scaling induced by the stimulus-directed tasks serves to increase the distance of neural responses from a linear decision boundary that separates words and faces (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). Interestingly, the categorization and one-back tasks appear to act via the same scaling mechanism. The stronger scaling observed for the one-back task might be a consequence of increased amplitude or duration of neural activity. These results suggest that, at least for the perceptual tasks sampled here and the spatial scale of neural activity measured in this study, top-down cognitive processes do not impart additional tuning or selectivity but serve to amplify the selectivity that is already computed by visual cortex.</p></sec><sec id="s2-4"><title>IPS is the source of top-down modulation to VTC</title><p>To design a plausible model that can predict top-down effects, we next turn to identifying the neural circuitry that generates task modulations in VTC. There are two candidate mechanisms. The first is that sensitivity to task is locally generated from the neuronal architecture of VTC itself. We explore an alternative hypothesis whereby top-down modulation is induced by input from another brain region that is sensitive to task demands. To identify this region, we perform a connectivity analysis in which we first subtract the bottom-up signal in VTC, as given by responses measured during the fixation task, from responses measured during the categorization and one-back tasks. We then correlate these residuals, which isolate the top-down signal, against the responses of every cortical location.</p><p>Applying this connectivity analysis to our data, we find that responses in the intraparietal sulcus (IPS) predict the top-down enhancement of VTC responses (<xref ref-type="fig" rid="fig4">Figure 4b</xref>) better than responses in any other region of cortex. As a control, if we omit the subtraction step and simply correlate raw VTC responses with the responses of different cortical locations, we find that the correlation is instead strongest with a range of areas spanning occipital cortex (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). This indicates that the VTC response is a mixture of bottom-up and top-down effects and that the top-down influence from the IPS becomes clear only when bottom-up effects are removed. Comparing our results to a publicly available atlas (<xref ref-type="bibr" rid="bib91">Wang et al., 2015</xref>), we estimate that the source of top-down modulation is localized to the IPS-0 and IPS-1 subdivisions of the IPS (see also <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.22341.008</object-id><label>Figure 4.</label><caption><title>IPS is the source of top-down modulation to VTC.</title><p>(<bold>a</bold>) <italic>Correlation with raw VTC response</italic>. This map depicts the correlation between the VTC response observed during the categorization and one-back tasks with the response at each cortical location (inset shows an unsmoothed and unthresholded map). Positive correlations are broadly distributed across occipital cortex. Results are shown for subjects with whole-brain coverage (<italic>n</italic> = 3); results for other subjects with partial-brain coverage (<italic>n</italic> = 6) are shown in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. (<bold>b</bold>) <italic>Correlation with top-down component of VTC response</italic>. After removing bottom-up responses (fixation task), the correlation is spatially localized to a hotspot in IPS-0/1. (<bold>c</bold>) <italic>Tractography using diffusion MRI</italic>. We find that the vertical occipital fasciculus (<xref ref-type="bibr" rid="bib98">Yeatman et al., 2014</xref>) connects VWFA and FFA to the IPS hotspot in each subject for which diffusion data were collected (<italic>n</italic> = 8) (rendering shows a representative subject).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22341.008">http://dx.doi.org/10.7554/eLife.22341.008</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22341-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22341.009</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Maps of top-down connectivity to VTC.</title><p>This figure shows thresholded and unthresholded maps for individual subjects and group averages (same format as <xref ref-type="fig" rid="fig4">Figure 4b</xref>; all maps shown on the <italic>fsaverage</italic> surface). At the lower right of each map is the range of values used for the colormap. The left two columns show the results obtained for the six subjects with partial brain coverage. Group average results for these subjects are shown in the last row. The right two columns show the results obtained for the three subjects with full brain coverage. Group average results for these subjects are shown in the third to last row. Group average results for all subjects are shown in the second to last row. The last row shows the results obtained from a control analysis in which we generate individual-subject maps by correlating cortical responses with random Gaussian noise and then average these maps across subjects. This control analysis produces no substantial correlations. Notice that the peak correlation is found in and around IPS-0/1 for both the group of subjects with partial brain coverage (red arrow) and the group of subjects with full brain coverage (green arrow). Some variability in the location of the peak correlation is expected given that there are limits on the degree to which functional areas can be aligned across subjects based solely on anatomical features.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22341.009">http://dx.doi.org/10.7554/eLife.22341.009</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22341-fig4-figsupp1-v2"/></fig></fig-group></p><p>Previous research has identified IPS as playing a key role in controlling spatial attention (<xref ref-type="bibr" rid="bib78">Saalmann et al., 2007</xref>; <xref ref-type="bibr" rid="bib58">Lauritzen et al., 2009</xref>). Our results extend these findings by showing that, despite the fact that spatial attention is always directed towards the foveal stimulus during the categorization and one-back tasks, the amount of modulation from the IPS is flexible and varies depending on properties of the stimulus and demands of the task. For example, during the categorization task, the observed enhancement for low-contrast stimuli is much larger than that for high-contrast stimuli. This mechanism could explain the finding that difficult tasks enhance visual responses (<xref ref-type="bibr" rid="bib72">Ress et al., 2000</xref>).</p><p>The direct influence of IPS on neural responses in VTC is consistent with anatomical measurements demonstrating the existence of a large white-matter pathway connecting dorsal and ventral visual cortex, called the vertical occipital fasciculus (VOF) (<xref ref-type="bibr" rid="bib97">Yeatman et al., 2013</xref>, <xref ref-type="bibr" rid="bib98">2014</xref>; <xref ref-type="bibr" rid="bib85">Takemura et al., 2016</xref>). Using diffusion-weighted MRI and tractography (data acquired in 8 of 9 subjects), we show that the VOF specifically connects the VWFA and FFA with the functionally identified peak region in the IPS (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). The VWFA falls within the ventral terminations of the VOF for seven subjects and, for the eighth, the VWFA is 2.7 mm anterior to the VOF, well within the margin of error for tractography (<xref ref-type="bibr" rid="bib41">Jeurissen et al., 2011</xref>). The FFA falls within the ventral terminations of the VOF for all eight subjects. These results provide an elegant example of how anatomy subserves function, and sets the stage for a circuit-level computational model that, guided by anatomical constraints, characterizes the computations that emerge from interactions between multiple brain regions.</p></sec><sec id="s2-5"><title>Model of top-down computations in VTC</title><p>The previous two sections provide critical insights that set the stage for building a quantitative model that predicts top-down effects in VTC. Building upon the observation that top-down modulation acts as a scaling mechanism on responses in VWFA and FFA (see <xref ref-type="fig" rid="fig3">Figure 3</xref>) and the observation that top-down effects are correlated with the IPS signal (see <xref ref-type="fig" rid="fig4">Figure 4</xref>), we propose that the magnitude of the IPS response to a stimulus indicates the amount of top-down scaling that is applied to bottom-up sensory responses in VTC (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). We implement this model, termed the <italic>IPS-scaling model</italic>, using response magnitudes extracted from a broad anatomical mask of the IPS. This strategy helps avoids the overfitting that might ensue from a more specific voxel-selection procedure tailored to the fine-scale and potentially idiosyncratic pattern of results from the connectivity analysis. For example, if we were to select the single cortical location in the IPS that best correlates with the top-down modulation of VTC, this would make voxel selection a critical part of the model and render the modeling analysis circular (<xref ref-type="bibr" rid="bib57">Kriegeskorte et al., 2009</xref>). Nevertheless, the selection procedure is not completely independent, so the modeling results should not be viewed as providing independent evidence for the involvement of the IPS.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.22341.010</object-id><label>Figure 5.</label><caption><title>Model of top-down computations in VTC.</title><p>(<bold>a</bold>) <italic>Model architecture.</italic> The predicted response during the stimulus-directed tasks (categorization task, one-back task) is given by scaling the bottom-up response, with the amount of scaling proportional to the IPS signal. (<bold>b</bold>) <italic>Cross-validation performance.</italic> Same format as <xref ref-type="fig" rid="fig2">Figure 2b</xref>. The arrows highlight an example of how the bottom-up response (red arrow) is multiplied by the IPS signal (green arrow) to produce the predicted response (blue arrow). (<bold>c</bold>) <italic>Comparison of performance against alternative models.</italic> Same format as <xref ref-type="fig" rid="fig2">Figure 2c</xref> (some error bars do not include the bar height; this is a consequence of the bootstrap procedure). Although the Additive and Scaling models perform well, note that these are <italic>ad hoc</italic>, phenomenological models. For instance, the Scaling model (task-specific) posits separate parameters for the amount of scaling under the categorization and one-back tasks. However, such a model does not explain <italic>why</italic> there is a different amount of scaling, whereas the IPS-scaling model provides such an explanation.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22341.010">http://dx.doi.org/10.7554/eLife.22341.010</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22341-fig5-v2"/></fig></p><p>We find that the IPS-scaling model accurately characterizes the observed data (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). For example, notice that the FFA response to faces increases gradually for each contrast increment during the fixation task (relatively unsaturated contrast-response function, red arrow). When subjects perform the one-back task, we observe a U-shaped contrast-response function in IPS (green arrow); multiplication of the two functions predicts a contrast-response function that is highly saturated and accurately matches the observed contrast-response function in FFA during the one-back task (blue arrow).</p><p>Importantly, the IPS-scaling model uses a single set of scale and offset parameters on the IPS response and accurately predicts scaling of VTC responses across the categorization and one-back tasks (<xref ref-type="fig" rid="fig5">Figure 5b</xref>, top plot). This finding suggests that the scaling of VTC by IPS is a general mechanism supporting perception and is independent of the specific cognitive task performed by the observer. Furthermore, the scale and offset parameters that are estimated from the data show that when IPS exhibits close to zero evoked activity (for example, FACE at 100%-contrast; see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), the corresponding scaling factor is close to one. This has a sensible interpretation: when IPS is inactive, we observe only the bottom-up response in VTC and no top-down modulation.</p><p>We assessed the cross-validation performance of the IPS-scaling model in comparison to several alternative models of top-down modulation (including those schematized earlier in <xref ref-type="fig" rid="fig3">Figure 3b</xref>). In line with earlier observations (<xref ref-type="fig" rid="fig3">Figure 3b and c</xref>), we find that a model positing enhancement for only the preferred stimulus category of each area (Area-specific enhancement model) does not optimally describe the data. We find that a phenomenological scaling model (Scaling model (task-specific)) outperforms a phenomenological additive model (Additive model (task-specific)), confirming earlier observations that the top-down modulation is a scaling effect (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). This conclusion is further supported by the higher performance observed when the IPS interacts with VTC multiplicatively (IPS-scaling model) compared to when it interacts additively (IPS-additive model). Finally, we find that the performance of the IPS-scaling model degrades if the IPS input into the model is shuffled across conditions (IPS-scaling model (shuffle, shuffle within task)), confirming that top-down modulation from the IPS is dependent on the stimulus and task.</p><p>Is the IPS the only region that induces top-down modulation of VTC? Inspection of the connectivity results (see <xref ref-type="fig" rid="fig4">Figure 4b</xref>) reveals that the top-down residuals in VTC are correlated, to a lesser extent, with the responses of other regions. These weaker correlations might be incidental, or might capture other important signals. Given that the IPS-scaling model accounts for nearly all of the variance induced by top-down modulation of VTC (see <xref ref-type="fig" rid="fig5">Figure 5b and c</xref>), we suggest that it is sufficient to consider only the IPS for the current set of measurements. However, future measurements that employ new stimulus manipulations and other cognitive tasks may reveal the role of a more extensive brain network. The IPS-scaling model can be extended to account for new measurements by systematically parameterizing the connectivity with additional brain regions. For example, some models of reading posit that language-related regions can directly influence the VWFA (<xref ref-type="bibr" rid="bib87">Twomey et al., 2011</xref>), suggesting that to account for measurements made during a more naturalistic reading task, it may be necessary to include Broca’s area in the model.</p></sec><sec id="s2-6"><title>Model of perceptual decision-making in IPS</title><p>Although informative, the finding that IPS provides top-down stimulus-specific scaling of VTC is an incomplete explanation, as the burden of explaining the top-down effects is simply shifted to the IPS. We are thus left wondering: is it possible to explain the response profile of the IPS? In particular, can we explain why the IPS is more active for certain stimuli compared to others? Answering these questions will provide a critical link between cognitive state and IPS activity.</p><p>Inspired by previous research on perceptual decision-making (<xref ref-type="bibr" rid="bib82">Shadlen and Newsome, 2001</xref>; <xref ref-type="bibr" rid="bib37">Heekeren et al., 2004</xref>; <xref ref-type="bibr" rid="bib30">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib53">Kayser et al., 2010a</xref>), we implement a <italic>Drift diffusion model</italic> that attempts to account for IPS responses measured during the categorization task (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). The model uses VTC responses during the fixation task as a measure of sensory evidence, and posits that the IPS accumulates evidence from VTC over time and exhibits an activity level that is monotonically related to accumulation time. For example, when VTC responses are small, as is the case for low-contrast stimuli, sensory evidence for stimulus category is weak, leading to long accumulation times (indexed by measurements of reaction time during the experiment), and large IPS responses.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.22341.011</object-id><label>Figure 6.</label><caption><title>Model of perceptual decision-making in IPS.</title><p>(<bold>a</bold>) <italic>Model architecture.</italic> We implement a model that links the stimulus representation in VTC to a decision-making process occurring in IPS. The model first uses the bottom-up VTC response as a measure of sensory evidence and predicts reaction times in the categorization task. The model then predicts the IPS response as a monotonically increasing function of reaction time. Note that this model does not involve stochasticity in the evidence-accumulation process, and is therefore a simplified version of the classic drift diffusion model (<xref ref-type="bibr" rid="bib70">Ratcliff, 1978</xref>). (<bold>b</bold>) <italic>Cross-validation performance.</italic> Same format as <xref ref-type="fig" rid="fig2">Figure 2b</xref> (except that reaction times are modeled in the left plot). (<bold>c</bold>) <italic>Comparison of performance against control models.</italic> The performance of the Drift diffusion model does not degrade substantially if a single threshold is used, thus justifying this simplification. Performance degrades if axis-aligned category vectors are used, supporting the assertion that responses of multiple VTC regions are used by subjects in deciding image category. (<bold>d</bold>) <italic>Overall model architecture</italic>. This schematic summarizes all components of our computational model (<xref ref-type="fig" rid="fig2">Figures 2a</xref>, <xref ref-type="fig" rid="fig5">5a</xref> and <xref ref-type="fig" rid="fig6">6a</xref>). Bottom-up visual information is encoded in the VTC fixation response (green box; Template model), fixation responses are routed to the IPS for evidence accumulation (purple box; Drift diffusion model), and then feedback from the IPS to VTC causes top-down modulation during the categorization and one-back tasks (yellow box; IPS-scaling model).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.22341.011">http://dx.doi.org/10.7554/eLife.22341.011</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-22341-fig6-v2"/></fig></p><p>Our implementation of the Drift diffusion model involves two steps. First, we use VTC responses during the fixation task (reflecting sensory evidence) to predict reaction times measured in the categorization task. The quality of the predictions is quite high (<xref ref-type="fig" rid="fig6">Figure 6b</xref>, left). Second, we apply a simple monotonic function to the reaction times measured during the categorization task to predict the level of response in the IPS (see Materials and methods). The rationale is that neural activity in IPS is expected to be sustained over the duration of the decision-making process (<xref ref-type="bibr" rid="bib82">Shadlen and Newsome, 2001</xref>), and so the total amount of neural activity integrated over time should be larger for longer decisions. Assuming that the BOLD signal reflects convolution of a sluggish hemodynamic response function and fine-scale neural activity dynamics, small differences in the duration of neural activity (for example, between 0 and 2 s) are expected to manifest in differences in BOLD amplitudes (<xref ref-type="bibr" rid="bib53">Kayser et al., 2010a</xref>) and only minimally in the shapes of BOLD timecourses. The cross-validated predictions of our proposed model explain substantial variance in IPS (<xref ref-type="fig" rid="fig6">Figure 6b</xref>, right).</p><p>It is possible to offer a psychological explanation of IPS activity as reflecting task difficulty—for example, we can posit that IPS activity is enhanced for low-contrast stimuli because the observer works harder to perceive these stimuli. The value of the model we have proposed is that it provides a quantitative and formal explanation of the computations that underlie ‘difficulty’. According to the model, categorization of low-contrast stimuli is difficult because the IPS computations required to perform the task involve longer accumulation time, and this is reflected in the fact that IPS response magnitudes increase monotonically with reaction time. Thus, our model performs several critical functions: it relates the cognitive task performed by the subject to IPS activity, proposes a computational explanation of task difficulty, and posits that top-down modulation of VTC by IPS is a direct consequence of fulfilling task demands. We have substantiated this hypothesis for the categorization task and suggest that this will serve as a foundation for modeling more complex cognitive tasks such as one-back.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In summary, we have measured and modeled how bottom-up and top-down factors shape responses in word- and face-selective cortex. A template operation on low-level visual properties generates a bottom-up stimulus representation, while top-down modulation from the IPS scales this representation in the service of the behavioral goals of the observer. We develop a computational approach that posits explicit models of the information processing performed by a network of interacting sensory and cognitive regions of the brain and validate this model on experimental data. We make publicly available data and open-source software code implementing the model at <xref ref-type="bibr" rid="bib52">Kay, 2017</xref> (with a copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/vtcipsmodel">https://github.com/elifesciences-publications/vtcipsmodel</ext-link>).</p><p>The model we propose is valuable because it integrates and explains a range of different stimulus and task manipulations that affect responses in VWFA, FFA, and IPS. Response properties in these regions can now be interpreted using a series of simple, well-defined computations that can be applied to arbitrary images. However, it is also important to recognize the limitations of the model. First, we have tested the model on only a limited range of stimuli and cognitive tasks. Second, the accuracy with which the model accounts for the data is reasonable but by no means perfect. For instance, the Template model does not capture the step-like response profile of VTC as phase coherence is varied (see <xref ref-type="fig" rid="fig2">Figure 2b</xref>), and the accuracy with which the model accounts for a wide range of stimuli that includes faces, animals, and objects, is moderate at best (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Third, we have thus far characterized VTC and IPS responses at only a coarse spatiotemporal scale (that is, BOLD responses averaged over specific regions-of-interest). Given these limitations, the present work constitutes a first step towards the goal of developing a comprehensive computational model of human high-level visual cortex. We have provided data and code so that other researchers can build on our approach, for example, by testing the generalizability of the model to other stimuli and tasks, extending and improving the model, and comparing the model against alternative models.</p><p>The fact that cognitive factors substantially affect stimulus representation in visual cortex highlights the importance of tightly controlling and manipulating cognitive state when investigating stimulus selectivity. In the present measurements, the most striking example comes from stimulus contrast. When subjects perform the fixation task, the contrast-response function (CRF) in VWFA is monotonically increasing, whereas during the one-back task, the CRF flips in sign and is monotonically decreasing (see <xref ref-type="fig" rid="fig1">Figure 1d</xref>). This effect (similar to what is reported in <xref ref-type="bibr" rid="bib62">Murray and He, 2006</xref>) is puzzling if we interpret the CRFs as indicating sensitivity to the stimulus contrast, but is sensible if we interpret the CRFs as instead reflecting the interaction of stimulus properties and cognitive processes. The influence of cognition on visual responses forces us to reconsider studies that report unexpected tuning properties in VTC and IPS, such as tuning to linguistic properties of text in VWFA (<xref ref-type="bibr" rid="bib88">Vinckier et al., 2007</xref>) and object selectivity in parietal cortex (<xref ref-type="bibr" rid="bib80">Sereno and Maunsell, 1998</xref>). In experiments that do not tightly control the cognitive processes executed by the observer, it is impossible to distinguish sensory effects from cognitive effects. Our quantitative model of VTC-IPS interactions provides a principled baseline on which to re-interpret past findings, design follow-up experiments, and guide data analysis.</p><p>There is a large body of literature on characterizing and modeling the effect of spatial attention on contrast-response functions (<xref ref-type="bibr" rid="bib59">Luck et al., 1997</xref>; <xref ref-type="bibr" rid="bib6">Boynton, 2009</xref>; <xref ref-type="bibr" rid="bib74">Reynolds and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib40">Itthipuripat et al., 2014</xref>). Although several models have been proposed, none of these straightforwardly account for the present set of measurements. The <italic>response-gain</italic> model (<xref ref-type="bibr" rid="bib60">McAdams and Maunsell, 1999</xref>) posits that attention causes a multiplicative scaling of contrast-response functions. Our observations are consistent with the general notion of response scaling (see <xref ref-type="fig" rid="fig3">Figure 3</xref>), but importantly, we find that the amount of scaling differs for different stimuli. Whereas the response-gain model implies that scaling is constant and therefore contrast-response functions should grow steeper during the stimulus-directed tasks, we find the opposite (see <xref ref-type="fig" rid="fig1">Figure 1d</xref>). The <italic>contrast-gain</italic> model (<xref ref-type="bibr" rid="bib75">Reynolds et al., 2000</xref>) posits that attention causes a leftward shift of contrast-response functions (as if contrast were increased). This model does not account for our measurements, since the stimulus-directed tasks can generate responses to low-contrast stimuli that are larger than responses to high-contrast stimuli (see <xref ref-type="fig" rid="fig1">Figure 1d</xref>). Finally, the <italic>additive-shift</italic> model (<xref ref-type="bibr" rid="bib12">Buracas and Boynton, 2007</xref>) posits that attention causes an additive increment to contrast-response functions; we find that our observations are better explained by a scaling, not additive, mechanism (see <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig5">5c</xref>). Thus, the effects we report are novel, and previous models of attention cannot explain these effects. Furthermore, by investigating responses to a wide range of stimuli (including manipulations of not only contrast but also phase coherence and stimulus category) and by characterizing the source of attentional signals, our work develops a more comprehensive picture of information processing in the visual system.</p><p>There are a number of research questions that remain unresolved. First, our connectivity analysis and modeling of VTC-IPS interactions are based on the correlation of BOLD responses and do not provide information regarding the directionality, or timing, of neural interactions. In other words, our correlational results do not, in and of themselves, prove that the IPS causes VTC modulation; rather, we are imposing an interpretation of the results in the context of a computational model. We note that our interpretation is in line with previous work on perceptual decision-making showing top-down influence (Granger causality) of IPS on visual cortex (<xref ref-type="bibr" rid="bib54">Kayser et al., 2010b</xref>). Our working hypothesis is that sensory information arrives at VTC (as indexed by fixation responses), these signals are routed to IPS for evidence accumulation, and then feedback from the IPS modulates the VTC response (as indexed by categorization and one-back responses). Temporally resolved measurements of neural activity (for example, EEG, MEG, ECoG) will be necessary to test this hypothesis. Second, the scaling of BOLD response amplitudes by IPS is consistent with at least two potential mechanisms at neural level: the IPS may be inducing a scaling on neural activity in VTC or, alternatively, a sustainment of neural activity in VTC. Some support for the latter comes from a recent study demonstrating that ECoG responses in FFA exhibit sustained activity that is linked to long reaction times in a face gender discrimination task (<xref ref-type="bibr" rid="bib29">Ghuman et al., 2014</xref>). Finally, IPS is part of larger brain networks involved in attention (<xref ref-type="bibr" rid="bib16">Corbetta and Shulman, 2002</xref>) and decision-making (<xref ref-type="bibr" rid="bib30">Gold and Shadlen, 2007</xref>), and identifying the computational roles of other regions in these networks is necessary for a comprehensive understanding of the neural mechanisms of perception.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>Eleven subjects participated in this study. Two subjects were excluded due to inability to identify VWFA in one subject and low signal-to-noise ratio in another subject, leaving a total of nine usable subjects (age range 25–32; six males, three females). All subjects were healthy right-handed monolingual native-English speakers, had normal or corrected-to-normal visual acuity, and were naive to the purposes of the experiment. Informed written consent was obtained from all subjects, and the experimental protocol was approved by the Washington University in St. Louis Institutional Review Board. Each subject participated in 1–3 scanning sessions, over the course of which anatomical data (T1-weighted high-resolution anatomical volume, diffusion-weighted MRI data) and functional data (retinotopic mapping, functional localizer, main experiment) were collected.</p></sec><sec id="s4-2"><title>Visual stimuli</title><p>Stimuli were presented using an NEC NP-V260X projector. The projected image was focused onto a backprojection screen and subjects viewed this screen via a mirror mounted on the RF coil. The projector operated at a resolution of 1024 × 768 at 60 Hz, and the viewing distance was 340 cm. A Macintosh laptop controlled stimulus presentation using code based on the Psychophysics Toolbox (<xref ref-type="bibr" rid="bib7">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib66">Pelli, 1997</xref>). Approximate gamma correction was performed by taking the square root of pixel intensity values before stimulus presentation. Behavioral responses were recorded using a button box.</p><p>The experiment consisted of 22 types of stimuli. All stimuli were small grayscale images (approximately 2° × 2°) presented at fixation. Each stimulus type consisted of 10 distinct images (for example, 10 different faces for a face stimulus), and a subset of these images were presented on each given trial.</p><sec id="s4-2-1"><title>Face</title><p>This stimulus consisted of a face pictured from a frontal viewpoint. Ten distinct faces were prepared. Faces were masked using a circle with diameter 2°. The outer 0.25° of the mask was smoothly ramped using a cosine function.</p></sec><sec id="s4-2-2"><title>Word</title><p>This stimulus consisted of a 5-letter word. Ten distinct words were prepared. Letters were white on a gray background, generated using the Helvetica font, and occupied a rectangular region measuring 3.15° × 1.05°.</p></sec><sec id="s4-2-3"><title>Phase coherence</title><p>These stimuli consisted of the FACE and WORD stimuli prepared at four phase-coherence levels, 0%, 25%, 50%, and 75% (8 stimuli total). To achieve this, for each of the ten images from each stimulus type, the portion of the image within a fixed region (FACE: 2° × 2° square; WORD: 3.15° × 1.05° rectangle) was extracted, and its phase spectrum was blended, to different degrees, with a randomly generated phase spectrum. For example, 25% coherence indicates that the phase of each Fourier component was set to a value that lies at 75% of the angular distance from the original phase to the phase in the randomly generated spectrum.</p></sec><sec id="s4-2-4"><title>Noise</title><p>This stimulus is the same as the FACE stimulus at 0% phase coherence. For brevity, we refer to this stimulus as NOISE.</p></sec><sec id="s4-2-5"><title>Contrast</title><p>These stimuli consisted of the FACE, WORD, and NOISE stimuli prepared at three contrast levels (nine stimuli total). The contrasts of the original stimuli were taken to be 100%, and different contrast levels were achieved by scaling pixel intensity values towards the background value. Contrast levels of 4%, 6%, and 10% were used for the FACE and NOISE stimuli, and contrast levels of 3%, 5%, and 8% were used for the WORD stimulus. This choice of contrast levels matches the average root-mean-square (RMS) contrast across stimulus types (for example, the average RMS contrast for the FACE stimulus at 4% contrast is approximately equal to the average RMS contrast for the WORD stimulus at 3% contrast). Note that a contrast level of 0% was achieved by estimating responses to blank trials (see <italic>GLM analysis</italic>).</p></sec><sec id="s4-2-6"><title>Polygon</title><p>This stimulus consisted of a string of three polygons (each chosen randomly from a set of polygons). Polygons were white, unfilled, on a gray background, and occupied a region similar in size to that of the WORD stimulus. Ten distinct strings were prepared.</p></sec><sec id="s4-2-7"><title>Checkerboard</title><p>This stimulus consisted of alternating black and white square checks. Ten checkerboards were prepared by varying check size from 0.03125° to 0.5° using ten equally spaced steps on a logarithmic scale. The <italic>x</italic>- and <italic>y</italic>-positions of each checkerboard were set randomly. Checkerboards were masked using a circle with diameter 2°.</p></sec><sec id="s4-2-8"><title>House</title><p>This stimulus consisted of a house pictured from a frontal viewpoint. Ten distinct houses were prepared. Houses were masked using a 2° × 2° square. The outer 0.25° of the mask was smoothly ramped using a cosine function.</p></sec></sec><sec id="s4-3"><title>Experimental design and tasks</title><p>Stimuli were presented in 4 s trials, one stimulus per trial. In a trial, four images from a given stimulus type (for example, FACE, 10% contrast) were presented sequentially using an 800 ms ON, 200 ms OFF duty cycle. To generate the sequence of four images, we first randomly selected four distinct images out of the ten images associated with the stimulus type. Then, for certain trials (details below), we modified the sequence to include a repetition by randomly selecting one of the images (excluding the first) and replacing that image with the previous image. Throughout stimulus presentation, a small dot (0.12° × 0.12°) was present at the center of the display. The dot switched to a new randomly selected color every 600 ms using a set of six possible colors: magenta, red, yellow, green, cyan, and blue.</p><p>In the experiment, two of the stimuli were duplicated (FACE and WORD), yielding a total of 24 stimulus conditions. Data corresponding to these duplicate stimuli are not used in this paper. Each run began and ended with a 16 s baseline period in which no stimuli were presented. During a run, each of the 24 stimulus conditions was presented three times. Six blank trials (no stimulus) were also included. The order of stimulus and blank trials was random, subject to the constraints that blank trials could not occur first nor last, blank trials could not occur consecutively, and no stimulus condition could occur consecutively. During the baseline periods and blank trials, the small central dot was still present. A randomly selected two of the three trials associated with each stimulus condition were modified to include an image repetition (as described previously). Each run lasted 344 s (5.7 min).</p><p>For each run, subjects were instructed to maintain fixation on the central dot while performing one of three tasks. In the <italic>fixation task</italic>, subjects were instructed to press a button whenever the central dot turned red, and were additionally reminded to not confuse the red and magenta colors. In the <italic>categorization task</italic>, subjects were instructed to report for each stimulus trial whether they perceived a word, a face, or neither (‘other’). Responses were made using three different buttons, and subjects were reminded to make only one response for each 4 s trial. Note that it is possible that responses are made prior to the completion of the four images that comprise a trial. In the <italic>one-back task</italic>, subjects were instructed to press a button whenever an image was repeated twice in a row, and were informed that repetitions occurred only within stimulus trials and not across trials. Subjects were warned that although some stimuli are faint (low contrast), they should still try their best to perform the categorization and one-back tasks. Subjects were also informed that some trials are blank trials and that responses were not expected on these trials. Subjects were familiarized with the stimuli and tasks before the actual experiment was conducted.</p><p>Subjects performed each of the three tasks four times during the course of the experiment, yielding a total of 3 tasks × 4 runs = 12 runs. The physical stimulus sequence (including the temporal ordering of stimulus images and dot colors) was held constant across tasks. This was accomplished by generating four distinct stimulus sequences and cycling through the sequences and tasks. Specifically, the order of stimulus sequences was ABCD ABCD ABCD, where each letter corresponds to a distinct sequence, and the order of tasks was XYZ XYZ XYZ XYZ, where each letter corresponds to a distinct task. The order of tasks was counterbalanced across subjects. Each stimulus and task combination (for example, CHECKERBOARD during one-back task) occurred a total of 3 trials × 4 runs = 12 times over the course of the experiment.</p></sec><sec id="s4-4"><title>MRI data acquisition</title><p>MRI data were collected at the Neuroimaging Laboratory at the Washington University in St. Louis School of Medicine using a modified 3T Siemens Skyra scanner and a 32-channel RF coil. For functional data, 28 oblique slices covering occipitotemporal cortex were defined: slice thickness 2.5 mm, slice gap 0 mm, field-of-view 200 mm × 200 mm, phase-encode direction anterior-posterior. A T2*-weighted, single-shot, gradient-echo EPI sequence was used: matrix size 80 × 80, TR 2 s, TE 30 ms, flip angle 77°, nominal spatial resolution 2.5 mm × 2.5 mm × 2.5 mm. Fieldmaps were acquired for post-hoc correction of EPI spatial distortion. To achieve comprehensive coverage for localization of top-down effects, a whole-brain version of the protocol involving 58 slices and a multiband (<xref ref-type="bibr" rid="bib25">Feinberg et al., 2010</xref>) factor of 2 was used in three of the nine subjects. In addition to functional data, T1-weighted anatomical data (MPRAGE sequence, 0.8 mm resolution) and diffusion-weighted data (spin-echo EPI sequence, 2 mm resolution, 84 directions, <italic>b</italic>-values of 1500 and 3000) were acquired. The diffusion sequence was acquired twice, reversing the phase-encode direction, in order to compensate for spatial distortions. Diffusion data were not acquired for one subject due to time constraints.</p></sec><sec id="s4-5"><title>Behavioral analysis</title><p>Behavioral results for the categorization task are used in the present study. We analyzed both reaction times (RT) and category judgments. We defined RT as the time elapsed between the onset of the first of the four images in a given trial and the button press. Trials in which no buttons were pressed were ignored. For each subject, we summarized RTs by computing the median RT across trials for each stimulus. To obtain group-averaged RTs, we added a constant to each subject's RTs in order to match the mean RT to the grand mean across subjects and then computed the mean and standard error across subjects (this normalization procedure compensates for additive offsets in RT across subjects). Category judgments were analyzed by calculating percentages of trials on which a given subject categorized a given stimulus into each of the three categories (word, face, other). Subjects were highly consistent in their judgments: for each stimulus, the most frequently reported category was the same across subjects and was reported more than 85% of the time. Category judgments obtained from the categorization task are used in the labeling and interpretation of experimental results (for example, <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig3">3</xref>).</p></sec><sec id="s4-6"><title>Diffusion analysis</title><p>Subject motion was corrected by co-registering each volume to the average of the non-diffusion-weighted <italic>b</italic> = 0 images. Gradient directions were adjusted to account for the co-registration. From pairs of volumes acquired with reversed phase-encode directions, the susceptibility-induced off-resonance field was estimated using a method similar to that described in <xref ref-type="bibr" rid="bib2">Andersson et al. (2003)</xref> as implemented in FSL (<xref ref-type="bibr" rid="bib83">Smith et al., 2004</xref>). Eddy currents were corrected using FSL’s <italic>eddy</italic> tool. The <italic>b</italic> = 3000 measurements were used to estimate fiber orientation distribution functions for each voxel using constrained spherical deconvolution as implemented in <italic>mrtrix</italic> (<xref ref-type="bibr" rid="bib86">Tournier et al., 2007</xref>) (CSD, <italic>l<sub>max</sub></italic> = 4), and fiber tracts were estimated using probabilistic tractography (500,000 fibers). For each subject, we identified the vertical occipital fasciculus (VOF) using a previously published algorithm (<xref ref-type="bibr" rid="bib98">Yeatman et al., 2014</xref>), and then quantified the Euclidean distance from the VOF terminations to word- and face-selective regions in VTC and the task-related hotspot in the IPS.</p></sec><sec id="s4-7"><title>Pre-processing of anatomical and functional data</title><p>The T1-weighted anatomical volume acquired for each subject was processed using FreeSurfer (<xref ref-type="bibr" rid="bib26">Fischl, 2012</xref>). The results were used to create a cortical surface reconstruction positioned halfway between the pial surface and the boundary between gray and white matter. We used the <italic>fsaverage</italic> surface from FreeSurfer to define anatomical ROIs (details below). These ROIs were transformed to native subject space by performing nearest-neighbor interpolation on the spherical surfaces created by FreeSurfer (these surfaces reflect folding-based alignment of individual subject surfaces to the <italic>fsaverage</italic> surface).</p><p>Functional data were pre-processed by performing slice time correction, fieldmap-based spatial undistortion, motion correction, and registration to the subject-native anatomical volume. The combined effects of distortion, motion, and registration were corrected using a single cubic interpolation of the slice time corrected volumes. Interpolations were performed directly at the vertices of the subject’s cortical surface, thereby avoiding unnecessary interpolation and improving spatial resolution (<xref ref-type="bibr" rid="bib44">Kang et al., 2007</xref>).</p></sec><sec id="s4-8"><title>GLM analysis</title><p>The pre-processed fMRI data were analyzed using GLMdenoise (<xref ref-type="bibr" rid="bib49">Kay et al., 2013a</xref>) (<ext-link ext-link-type="uri" xlink:href="http://kendrickkay.net/GLMdenoise/">http://kendrickkay.net/GLMdenoise/</ext-link>), a data-driven denoising method that derives estimates of correlated noise from the data and incorporates these estimates as nuisance regressors in a general linear model (GLM) analysis of the data. For our experiment, we coded each stimulus and task combination as a separate condition and also included the blank trials, producing a total of (24 stimulus + 1 blank) × 3 tasks = 75 conditions. The response to blank trials was interpreted as the response to a 0%-contrast stimulus. Estimates of BOLD response amplitudes (beta weights) were converted to units of percent BOLD signal change by dividing amplitudes by the mean signal intensity observed at each vertex. To obtain ROI responses, beta weights were averaged across the vertices composing each ROI. Error bars (68% CIs) on beta weights were obtained by bootstrapping runs.</p><p>Group-averaged beta weights were calculated using a procedure that compensates for large intrinsic variation in percent BOLD change across subjects. First, the beta weights obtained for each subject in a given ROI were normalized to be a unit-length vector (for example, <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">b</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf2"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>b</mml:mi></mml:mstyle><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>i</mml:mi></mml:mstyle></mml:msub></mml:mrow></mml:math></inline-formula> indicates beta weights for the <italic>i</italic>th subject (1 x <italic>n</italic>), <inline-formula><mml:math id="inf3"><mml:mrow><mml:mrow><mml:mo>‖</mml:mo> <mml:mrow/><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates <italic>L</italic><sub>2</sub>-norm, and <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi mathvariant="bold">i</mml:mi></mml:msub><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates normalized beta weights for the <italic>i</italic>th subject). Next, normalized beta weights were averaged across subjects, using bootstrapping to obtain error bars (68% CIs). Finally, the resulting group-averaged beta weights were multiplied by a scalar such that the mean of the beta weights is equal to the mean of the original unnormalized beta weights obtained from all subjects. The motivation of this last step is to produce interpretable units of percent BOLD change instead of normalized units. Note that in some cases, beta weights are repeated for easier visualization (for example, in <xref ref-type="fig" rid="fig1">Figure 1</xref>, NOISE at 100% contrast is the same data point as FACE at 0% phase coherence). Group-averaged beta weights were used in computational modeling.</p></sec><sec id="s4-9"><title>Region-of-interest (ROI) definition</title><p>Visual field maps were defined using the population receptive field (pRF) technique applied to retinotopic mapping data (<xref ref-type="bibr" rid="bib24">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="bib50">Kay et al., 2013b</xref>). Subjects participated in 2–4 runs (300 s each) in which they viewed slowly moving apertures (bars, wedges, rings) filled with a colorful texture of objects, faces, and words placed on an achromatic pink-noise background. The aperture and texture were updated at 5 Hz, and blank periods were included in the design (<xref ref-type="bibr" rid="bib24">Dumoulin and Wandell, 2008</xref>). A semi-transparent fixation grid was superimposed on top of the stimuli (<xref ref-type="bibr" rid="bib79">Schira et al., 2009</xref>). Stimuli occupied a circular region with diameter 10° and the viewing distance was 251 cm. A small semi-transparent central dot (0.15° × 0.15°) was present throughout the experiment and changed color every 1–5 s. Subjects were instructed to maintain fixation on the dot and to press a button whenever its color changed. The time-series data from this experiment were modeled using the Compressive Spatial Summation model (<xref ref-type="bibr" rid="bib50">Kay et al., 2013b</xref>) as implemented in analyzePRF (<ext-link ext-link-type="uri" xlink:href="http://kendrickkay.net/analyzePRF">http://kendrickkay.net/analyzePRF/</ext-link>). Angle and eccentricity estimates provided by the model were then visualized on cortical surface reconstructions and used to define V1, V2, V3, and hV4 (<xref ref-type="bibr" rid="bib8">Brewer et al., 2005</xref>). Due to the limited amount of pRF data acquired, there was insufficient signal-to-noise ratio to define visual field maps in parietal cortex.</p><p>Category-selective regions FFA and VWFA were defined using functional localizers (<xref ref-type="bibr" rid="bib93">Weiner and Grill-Spector, 2010</xref>, <xref ref-type="bibr" rid="bib94">2011</xref>). Subjects participated in two runs (336 s each) in which they viewed blocks of words, faces, abstract objects, and noise patterns. Each block lasted 16 s and consisted of 16 images presented at a rate of 1 Hz. The images differed from those in the main experiment. In each run, the four stimulus types were presented four times each in pseudorandom order, with occasional 16 s blank periods. A semi-transparent fixation grid was superimposed on top of the stimuli (<xref ref-type="bibr" rid="bib79">Schira et al., 2009</xref>). Stimuli occupied a 4° × 4° square region, with the words, faces, and objects occupying the central 3° × 3° of this region. The viewing distance was 340 cm. Subjects were instructed to maintain central fixation and to press a button when the same image is presented twice in a row. The time-series data from this experiment were analyzed using a GLM to estimate the amplitude of the BOLD response to the four stimulus categories.</p><p>To define FFA and VWFA, responses to the four stimulus categories were visualized on cortical surface reconstructions. FFA and VWFA were defined based on stimulus selectivity, anatomical location, and topological relationship to retinotopic areas (<xref ref-type="bibr" rid="bib93">Weiner and Grill-Spector, 2010</xref>; <xref ref-type="bibr" rid="bib97">Yeatman et al., 2013</xref>; <xref ref-type="bibr" rid="bib92">Weiner et al., 2014</xref>). We defined FFA as face-selective cortex (responses to faces greater than the average response to the other three categories) located on the fusiform gyrus. We included in the definition both the posterior fusiform gyrus (pFus-faces/FFA-1) and middle fusiform gyrus (mFus-faces/FFA-2) subdivisions of FFA (<xref ref-type="bibr" rid="bib92">Weiner et al., 2014</xref>). We defined VWFA as word-selective cortex (responses to words greater than the average response to the other three categories) located in and around the left occipitotemporal sulcus. In some subjects, multiple word-selective patches were found, and all of these patches were included in the definition of VWFA.</p><p>Anatomically-defined ROIs were also created (see <xref ref-type="fig" rid="fig4">Figure 4a and b</xref>). Based on curvature values on the <italic>fsaverage</italic> surface, we created an anatomical mask of the IPS by selecting the posterior segment of the intraparietal sulcus (<xref ref-type="bibr" rid="bib67">Pitzalis et al., 2012</xref>). Using the atlas of visual topographic organization provided by Wang et <italic>al</italic>. (<xref ref-type="bibr" rid="bib91">Wang et al., 2015</xref>), we estimate that this IPS mask overlaps V3A, V3B, IPS-0, IPS-1, and IPS-2. The locations of IPS-0/1/2/3 from the atlas are shown in <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. We also created an anatomical mask of VTC by computing the union of the <italic>fusiform</italic> and <italic>inferiortemporal</italic> parcels provided by the FreeSurfer Desikan-Killiany atlas (<xref ref-type="bibr" rid="bib20">Desikan et al., 2006</xref>) and trimming the anterior extent of the result to include only visually responsive cortex. The VTC mask includes both FFA and VWFA as well as surrounding cortex.</p><p>In our data, we find that word-selective visual cortex in some subjects is confined to the left hemisphere, consistent with previous studies (<xref ref-type="bibr" rid="bib97">Yeatman et al., 2013</xref>). Therefore, to ease interpretation, we restricted our analysis to VWFA, FFA, VTC, and IPS taken from the left hemisphere. In addition, we restricted the definition of V1, V2, V3, hV4, VTC, and IPS to include only vertices exhibiting response amplitudes in the main experiment that are positive on average. This procedure excludes voxels with peripheral receptive fields which typically exhibit negative BOLD responses to centrally presented stimuli.</p></sec><sec id="s4-10"><title>Task-based functional connectivity</title><p>To identify the cortical region that generates top-down effects in VWFA and FFA, we performed a simple connectivity analysis. First, we averaged BOLD responses across our VTC mask, given that top-down effects appear broadly across VTC. Next, we identified the component of the VTC response that is of no interest, specifically, the bottom-up stimulus-driven response. Our estimate of this component is given by our measurement of VTC responses during the fixation task (22 stimuli + 1 blank = 23 values). We then subtracted the bottom-up component from the VTC response measured during the categorization task (23 values) and one-back task (23 values). This produced a set of residuals (46 values) that reflect the top-down effect in VTC. Finally, we correlated the residuals with the responses of each cortical location in our dataset during the categorization and one-back tasks (46 values). The cortical location that best correlates with the residuals is interpreted as a candidate region that supplies top-down modulation to VTC.</p><p>The results were visualized by averaging correlation values across subjects based on the <italic>fsaverage</italic> cortical alignment and plotting the results on the <italic>fsaverage</italic> surface. Results from the three subjects for which whole-brain fMRI data were acquired are shown in <xref ref-type="fig" rid="fig4">Figure 4a and b</xref>. Results from the remaining six subjects with limited fMRI coverage are provided in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. Note that the correlation-based analysis we have used is most suitable for connectivity effects that are additive in nature (for example, IPS providing additive enhancement to VTC). However, the modulation is more accurately characterized as a multiplicative, or scaling, effect (see <xref ref-type="fig" rid="fig3">Figure 3b and c</xref>). The advantage of correlation is that it is robust to noise and computationally efficient; we perform a more precise evaluation of different top-down mechanisms in the computational modeling section below.</p><p>There are three important differences between the connectivity analysis described here and conventional correlation-based resting-state functional connectivity (RSFC) (<xref ref-type="bibr" rid="bib11">Buckner et al., 2013</xref>) and the psychophysiological interactions (PPI) technique (<xref ref-type="bibr" rid="bib64">O'Reilly et al., 2012</xref>). One is that our connectivity is performed on data that have explicit manipulation of stimulus and task (unlike RSFC). Another is that we analyze the data explicitly in terms of information-processing operations performed by the brain (unlike PPI). In other words, functional connectivity is characterized, not as correlated signal fluctuations, but as a direct consequence of information-processing operations. A third difference is that our connectivity is performed on beta weights that pool across trials (<xref ref-type="bibr" rid="bib76">Rissman et al., 2004</xref>), as opposed to raw BOLD time-series. This concentrates the analysis on brain responses that are reliably driven by the stimulus and task, and de-emphasizes trial-to-trial fluctuations in cognitive performance (<xref ref-type="bibr" rid="bib23">Donner et al., 2013</xref>).</p></sec><sec id="s4-11"><title>Computational modeling</title><p>We developed a computational model to account for BOLD responses measured in VTC and IPS. The model is composed of three components, each of which addresses a different aspect of the data (<xref ref-type="fig" rid="fig6">Figure 6d</xref>). The first component (<italic>Template model</italic>) specifies how a given stimulus drives bottom-up VTC responses as measured during the fixation task; the second component (<italic>IPS-scaling model</italic>) specifies how top-down modulation from the IPS during the categorization and one-back tasks affects VTC responses; and the third component (<italic>Drift-diffusion model</italic>) specifies how accumulation of evidence from VTC predicts reaction times and IPS responses during the categorization task. Note that although the three model components could be yoked together (for example, the output from the Template model could serve as the input to the Drift-diffusion model), in our model implementations, we adopt the approach of isolating each model component so that the quality of each component can be assessed independently of the others.</p><p>For all three model components, computational modeling was performed using nonlinear least-squares optimization (MATLAB Optimization Toolbox). Leave-one-stimulus-out cross-validation was used to assess model accuracy (thus, we assess the ability of models to generalize to stimuli that the models have not been trained on). Note that the use of cross-validation enables fair comparison of models that have different levels of flexibility (or, informally, different numbers of free parameters). This is because models that are overly complex will tend to fit noise in the training data and thereby generalize poorly to the testing data (<xref ref-type="bibr" rid="bib33">Hastie et al., 2001</xref>).</p><p>Accuracy was quantified as the percentage of variance explained (<italic>R</italic><sup>2</sup>) between cross-validated predictions of the data (aggregated across cross-validation iterations) and the actual data. In the case of beta weights, variance was computed relative to 0% BOLD signal change (<xref ref-type="bibr" rid="bib50">Kay et al., 2013b</xref>). In certain cases, accuracy is reported using Pearson’s correlation (<italic>r</italic>); this metric assesses performance relative to the mean. To assess reliability of cross-validation results, model fitting and cross-validation were repeated for each bootstrap of the group-averaged data (resampling subjects with replacement). For benchmarks on cross-validation performance, we calculated noise ceilings using Monte Carlo simulations (<xref ref-type="bibr" rid="bib50">Kay et al., 2013b</xref>) and quantified the performance of a flat-response model that predicts the same response level for each data point.</p></sec><sec id="s4-12"><title>Template model</title><sec id="s4-12-1"><title>Basic model description</title><p>The <italic>Template model</italic> specifies the stimulus properties that drive bottom-up responses in VTC. The model accepts as input a grayscale image and produces as output the predicted response in VWFA and FFA during the fixation task. In brief, the model processes the image using a set of V1-like Gabor filters and then computes a normalized dot product between filter outputs and a category template. The category template can be viewed as capturing the prototypical image statistics of a word (VWFA) or face (FFA). The Template model makes no claim as to how the brain might develop category templates; they might be genetically hard-wired (<xref ref-type="bibr" rid="bib47">Kanwisher, 2010</xref>) or arise from experience with the environment (<xref ref-type="bibr" rid="bib28">Gauthier et al., 1999</xref>). The central claim is that the bottom-up information computed by VTC is, at least to a first approximation, the output of a template operation applied to the stimulus.</p><p>The Template model is related to our previously developed Second-order contrast (SOC) model (<xref ref-type="bibr" rid="bib51">Kay et al., 2013c</xref>). Similar to the SOC model, the Template model has a cascade architecture involving two stages of filtering, rectification, and normalization. The first stage of the Template model is taken directly from the SOC model, and the properties of this stage (for example, filter design) were not tweaked to fit the data. The main difference between the Template and SOC models is that the Template model incorporates a specific second-stage filter (the template), whereas the SOC model uses a variance-like operation in the second stage that captures generic sensitivity to second-order contrast. Whether the Template model captures certain response properties, such as invariance to font in VWFA (<xref ref-type="bibr" rid="bib19">Dehaene and Cohen, 2011</xref>) or coarse luminance-contrast selectivity in FFA (<xref ref-type="bibr" rid="bib65">Ohayon et al., 2012</xref>), is an empirical question that can only be resolved through quantitative evaluation on experimental data. For example, our measurements indicate that VWFA responds strongly to polygons (<xref ref-type="fig" rid="fig1">Figure 1d</xref>); the Template model already accounts for this effect (<xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p></sec><sec id="s4-12-2"><title>Model details</title><p>The first stage of the Template model involves computing a V1-like representation of the image. The image is first resized to 250 pixels × 250 pixels, and luminance values are mapped to the range [–0.5,0.5], which has the effect of mapping the gray background to 0. The model then calculates V1 energy in the same way as the SOC model (<xref ref-type="bibr" rid="bib51">Kay et al., 2013c</xref>). Specifically, the image is projected onto a set of isotropic Gabor filters occurring at eight orientations, two quadrature phases, and a range of positions (63 <italic>x</italic>-positions × 63 <italic>y</italic>-positions). Filters are constructed at a single scale with a peak spatial frequency tuning of 4 cycles per degree (see <xref ref-type="fig" rid="fig2">Figure 2d</xref>) and a spatial frequency bandwidth of 1 octave (full-width at half-maximum of the amplitude spectrum). Filters are scaled such that filter responses to full-contrast optimal sinusoidal gratings are equal to one. Outputs of quadrature-phase filters are squared, summed, and square-rooted, analogous to the complex-cell energy model (<xref ref-type="bibr" rid="bib1">Adelson and Bergen, 1985</xref>).</p><p>After computing V1 energy, the model applies divisive normalization (<xref ref-type="bibr" rid="bib36">Heeger, 1992</xref>), again analogous to the SOC model. The output of each filter is divided by the average output across filter orientations at the same position:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>r</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>⟮</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfrac><mml:mo>⟯</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the normalized filter output at a given position and orientation, <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the filter output at a given position and orientation, <italic><inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> is the total number of orientations, and <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are parameters that control the strength of the normalization. For simplicity and to reduce the potential for overfitting, we do not fit <italic><inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> and <italic><inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> but simply use <italic><inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> = 1 and <italic><inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> = 0.5, values determined from our previous study (<xref ref-type="bibr" rid="bib51">Kay et al., 2013c</xref>).</p><p>At this point in the model, the representation of the image is a 3D matrix of dimensions 63 <italic>x</italic>-positions × 63 <italic>y</italic>-positions × 8 orientations. To visualize this representation, a hue-saturation-value image is used (see <xref ref-type="fig" rid="fig2">Figure 2a</xref>). For each position, a set of 8 vectors is constructed with vector angles corresponding to the filter orientation and vector lengths corresponding to normalized filter output. These vectors are averaged and an image pixel is used to summarize the result. Specifically, the hue of a pixel indicates the angle of the vector average and the value of the pixel indicates the length of the vector average.</p><p>The second stage of the Template model involves taking the V1-like representation of the image and comparing it to a category template to generate the predicted response. Specifically, the response is computed as<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>×</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⋅</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <italic><inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> is the predicted response, <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the 3D matrix with the V1-like representation of the image, <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the category template, <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the average of the elements in <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates positive half-wave rectification, and <italic><inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic>, <italic><inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic>, and <italic><inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> are free parameters (three free parameters).</p><p>There are three basic steps in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. The first step is a filtering operation, accomplished by computing the dot product between the stimulus and the template (<inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⋅</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). Intuitively, this operation quantifies the similarity between the stimulus and the template. The second step is the subtraction of average stimulus energy (<inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) with a free parameter controlling the strength of the subtractive normalization. This subtraction can be interpreted as penalizing non-specific energy in the stimulus, thereby inducing preference for stimulus energy that conforms to the category template. (An alternative interpretation is that the subtraction provides flexibility with respect to the overall mean of the template.) To ease interpretation and ensure that negative responses are not obtained, the result of the subtraction is positively rectified (<inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). The third step is division by average stimulus energy (/ (<inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>)) with a free parameter controlling the strength of the divisive normalization. This division penalizes non-specific energy in the stimulus, similar to subtractive normalization, but induces a different response geometry (<xref ref-type="bibr" rid="bib100">Zetzsche et al., 1999</xref>). In summary, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> computes a dot product between the stimulus and the template that is normalized subtractively and divisively by the average stimulus energy.</p><p>Where does the category template in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> come from? Given that we do not have sufficient sampling of stimuli to directly estimate templates from the data (but see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), we adopted the simple strategy of constructing templates from our stimulus set. Specifically, we took the WORD and FACE stimuli at 100% contrast and used the first stage of the Template model to compute a V1-like representation of these stimuli. This produced for each category, ten points in a 63 × 63 × 8 = 31,752 dimensional space. We then computed the centroid of the ten points, producing a category template (example shown in <xref ref-type="fig" rid="fig2">Figure 2a</xref>). Because the category template is constructed from the same stimuli used in our experiment, it is guaranteed that the Template model predicts large responses to the preferred category (for example, using a category template constructed from the face stimuli guarantees that the face stimuli produce large responses from the model). However, there is no guarantee that the model will accurately account for responses to the other stimuli used in our experiment.</p></sec><sec id="s4-12-3"><title>Model fitting</title><p>The Template model was fit to the fixation responses of VWFA and FFA. Model outputs were calculated for all ten images associated with a given stimulus type and then averaged to obtain the final model output for that stimulus type. To aid model fitting, the <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⋅</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> quantities were pre-computed and pre-conditioned by dividing each quantity by the mean of that quantity across stimuli. After pre-conditioning, a variety of initial seeds for <italic>b</italic> and <italic>c</italic> were evaluated in order to avoid local minima. Specifically, we performed optimization starting from initial seeds corresponding to every combination of <italic>b</italic> and <italic>c</italic>, where <italic>b</italic> is chosen from {0 .5 1 1.5 2 3 5} and <italic>c</italic> is chosen from {.01 .05 .1 .5 1 5 10}.</p></sec><sec id="s4-12-4"><title>Alternative models</title><p>(1) The <italic>Category model</italic> predicts a fixed response level for stimuli from the preferred stimulus category (word for VWFA, face for FFA) and a different response level for all other stimuli (two free parameters, one for each response level). Category judgments provided by the subjects were used to determine category membership; for example, words and faces at 0% and 25% phase coherence were reported by subjects as ‘other’, and are hence not considered to be words and faces by the Category model. (2–3) We evaluated simplified versions of the second-stage normalization used in the Template model. One version, <italic>Template model (only subtractive normalization)</italic>, omits the divisive normalization and thus characterizes responses as a simple linear function of V1-like normalized filter outputs (two free parameters, <italic>a</italic> and <italic>b</italic>), whereas the other version, <italic>Template model (only divisive normalization)</italic>, omits the subtractive normalization (two free parameters, <italic>a</italic> and <italic>c</italic>). (4) In <italic>Template model (omit first stage)</italic>, the first stage of the model is omitted and the template operation is performed on a pixel representation of the image, that is, <italic>S</italic> refers to the original image instead of the V1-like representation of the image (three free parameters, <italic>a</italic>, <italic>b</italic>, and <italic>c</italic>). (5–7) We evaluated the effect of using different templates in the Template model (each model has three free parameters, <italic>a</italic>, <italic>b</italic>, and <italic>c</italic>). <italic>Template model (non-selective template)</italic> uses a template consisting of all ones. <italic>Template model (mixed template)</italic> uses a template generated by unit-length normalizing both the word and face templates and then averaging the templates together. <italic>Template model (random template)</italic> uses a template generated by drawing uniform random values from the range [0,1].</p></sec></sec><sec id="s4-13"><title>IPS-scaling model</title><sec id="s4-13-1"><title>Basic model description</title><p>The <italic>IPS-scaling model</italic> predicts top-down modulation of VTC by taking into account measurements of IPS activity. The model accepts as input the response in VTC (either VWFA or FFA) during the fixation task and the response in IPS during the stimulus-directed tasks (categorization, one-back), and produces as output the predicted response in VTC during the stimulus-directed tasks. Intuitively, the model answers the question: how much is the bottom-up response in VTC enhanced by the IPS when the subject performs a task on the stimulus? The model can be viewed as a formal implementation of the concept of stimulus-specific scaling (schematized in <xref ref-type="fig" rid="fig3">Figure 3b</xref>, lower right). Similar ideas regarding top-down scaling induced by the IPS can be found in previous work (<xref ref-type="bibr" rid="bib54">Kayser et al., 2010b</xref>).</p></sec><sec id="s4-13-2"><title>Model details</title><p>The IPS-scaling model multiplies the bottom-up response in VTC measured during the fixation task by a scaled version of the IPS response observed during a stimulus-directed task:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>⋅</mml:mo><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the predicted response in <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mi>T</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> during the stimulus-directed task, <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the bottom-up response in <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mi>T</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the response in <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> during the stimulus-directed task, and <italic><inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> and <italic><inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> are parameters that allow a scale and offset to be applied to the IPS response (two free parameters). The final scaling factor that is applied to <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is shown in <xref ref-type="fig" rid="fig5">Figure 5b</xref>. The measurements of <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> activity used in the model are extracted using a broad anatomical mask of the <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (see <italic>Region-of-interest (ROI) definition</italic>).</p></sec><sec id="s4-13-3"><title>Model fitting</title><p>The IPS-scaling model was fit to the fixation, categorization, and one-back responses observed in VWFA and FFA. Leave-one-out cross-validation was performed by systematically leaving out each of the categorization and one-back responses. Since measurement noise is present in the fixation responses, treating the fixation responses as exact estimates of bottom-up responses would result in suboptimal model performance (especially in the case of bottom-up responses that are near zero). We therefore devised a procedure that allows flexibility in estimating bottom-up responses (see light lines in <xref ref-type="fig" rid="fig5">Figure 5b</xref>). In the procedure, a separate parameter is used to model the bottom-up response associated with each stimulus. During model fitting, these bottom-up parameters are initially set to be equal to the measured fixation responses, parameters of the model excluding the bottom-up parameters are optimized, and then all parameters are optimized simultaneously. This procedure was also used for the alternative models described below. Note that the IPS-scaling model uses flexible parameters to accommodate bottom-up stimulus selectivity and does not attempt to characterize the image-processing computations that underlie bottom-up responses (such computations are in the purview of the Template model).</p></sec><sec id="s4-13-4"><title>Alternative models</title><p>(1) The <italic>Task-invariant model</italic> posits that top-down modulation does not occur and that a fixed set of responses can characterize all three tasks (zero free parameters). (2–5) We evaluated several phenomenological models for purposes of comparison. The <italic>Additive model</italic> (schematized in <xref ref-type="fig" rid="fig3">Figure 3b</xref>, upper left) predicts responses during stimulus-directed tasks by adding a constant to bottom-up responses (one free parameter). The <italic>Scaling model</italic> (schematized in <xref ref-type="fig" rid="fig3">Figure 3b</xref>, lower left) predicts responses during stimulus-directed tasks by multiplying bottom-up responses by a constant (one free parameter). The <italic>Additive model (task-specific)</italic> and <italic>Scaling model (task-specific)</italic> are identical to the previous two models, except that separate constants are used for the categorization and one-back tasks (two free parameters). (6) The <italic>Area-specific enhancement model</italic> (schematized in <xref ref-type="fig" rid="fig3">Figure 3b</xref>, upper right) is identical to the Scaling model (task-specific) except that scaling is applied only to the stimuli preferred by a given area, that is, words in VWFA and faces in FFA (two free parameters). (7) The <italic>IPS-additive model</italic> predicts responses during stimulus-directed tasks by adding a scaled version of the IPS response to bottom-up responses in VTC (two free parameters). (8–9) To assess the specificity of the IPS enhancement, we evaluated variants of the IPS-scaling model. In the <italic>IPS-scaling (shuffle)</italic> model, IPS responses are shuffled across stimuli and tasks (restricted to the stimulus-directed tasks) before being used in the model (two free parameters). In the <italic>IPS-scaling (shuffle within task) model</italic>, IPS responses are shuffled across stimuli but not across tasks before being used in the model (two free parameters).</p></sec></sec><sec id="s4-14"><title>Drift diffusion model</title><sec id="s4-14-1"><title>Basic model description</title><p>The <italic>Drift diffusion model</italic> specifies the decision-making operations that underlie performance of the categorization task, and is based upon past research on perceptual decision-making (<xref ref-type="bibr" rid="bib82">Shadlen and Newsome, 2001</xref>; <xref ref-type="bibr" rid="bib37">Heekeren et al., 2004</xref>; <xref ref-type="bibr" rid="bib30">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib53">Kayser et al., 2010a</xref>). The model accepts as input fixation responses in VTC and produces as output predicted reaction times and IPS responses for the categorization task. The basic idea is that VTC responses provide evidence regarding which stimulus category has been presented to the subject, and this evidence is accumulated over time by the IPS in order to make a final decision regarding stimulus category.</p></sec><sec id="s4-14-2"><title>Model details</title><p>First, we collect fixation responses in hV4, VWFA, and FFA and divide each set of responses by their mean. This normalization ensures that different ROIs have similar units. Then, for each stimulus category (word, face, other), we compute the centroid of the fixation responses associated with that category, interpret this centroid as a vector, and normalize the vector to unit length. This procedure generates category vectors, defined in a three-dimensional neural space, that point in the directions of the ‘arms’ of the manifold of the VTC representation (see <xref ref-type="fig" rid="fig3">Figure 3e</xref>).</p><p>Next, we take the VTC fixation response for a given stimulus and project this response onto the category vector associated with that stimulus. The working hypothesis is that this operation is performed by neurons in IPS and that the magnitude of the projection indicates the strength of evidence for that specific category. For example, there might be an IPS neuron that receives information from VTC and responds strongly when the VTC response is consistent with the category vector corresponding to a word.</p><p>In accordance with drift diffusion models, we posit that evidence is accumulated until a threshold is reached, at which point the decision is made. This generates a prediction of the reaction time required to perform the categorization task on the stimulus:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>⋅</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the predicted reaction time, <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the VTC fixation response, <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the category vector associated with the stimulus, <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a parameter that controls the threshold, and <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a parameter that compensates for non-decision time (for example, motor response) (two free parameters). <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⋅</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is interpreted as a drift rate, and <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>⋅</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the time required to reach the threshold (see <xref ref-type="fig" rid="fig6">Figure 6a</xref>). Note that our instantiation of the drift diffusion model is relatively simple, as it is non-stochastic and does not characterize trial-to-trial variability. Thus, it can be viewed as a simplified version of the classic drift diffusion model (<xref ref-type="bibr" rid="bib70">Ratcliff, 1978</xref>). Also, being non-stochastic, our model bears similarity to the linear ballistic accumulator model (<xref ref-type="bibr" rid="bib10">Brown and Heathcote, 2008</xref>), a model that also uses the idea of evidence accumulation (see <xref ref-type="bibr" rid="bib22">Donkin et al., 2011</xref> for discussion of these different models).</p><p>Given that neuronal responses in parietal cortex reflect the duration of the decision-making process (<xref ref-type="bibr" rid="bib82">Shadlen and Newsome, 2001</xref>), we can use RT to predict IPS activity. A detailed model relating RT to BOLD measurements of IPS activity requires precise characterization of neural dynamics during decision-making and IPS subdivisions that might represent evidence accumulation for different stimulus categories. For the purposes of this study, we use a simple model that posits a monotonically increasing relationship between RT and the IPS response:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>⋅</mml:mo><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the predicted <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> response, <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the observed reaction time for a given stimulus, <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the hyperbolic tangent function intended as a generic sigmoidal nonlinearity, and <italic><inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic>, <italic><inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic>, <italic><inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic>, and <italic><inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> are free parameters (four free parameters).</p></sec><sec id="s4-14-3"><title>Alternative models</title><p>(1) The <italic>Drift diffusion model (separate thresholds)</italic> uses a separate threshold parameter for each stimulus category (four free parameters, one for non-decision time and three for thresholds). This allows us to assess the validity of having a single threshold parameter in the model. (2) The <italic>Drift diffusion model (axis-aligned category vectors)</italic> uses category vectors that are aligned with the axes of the multi-dimensional neural space (four free parameters, similar to the previous model). For example, in this model, the word category vector is a vector that is one along the VWFA axis and zero along the hV4 and FFA axes. This model tests the idea that evidence for words and faces is contributed only by the VTC regions selective for those categories.</p></sec></sec><sec id="s4-15"><title>Additional wide-range-of-stimuli dataset</title><sec id="s4-15-1"><title>Experimental design</title><p>To assess the generalization performance of the Template model, we collected an additional dataset involving a wider range of stimuli. This dataset was collected from one subject (an author; male; age 34). Informed written consent was obtained, and the protocol was approved by the University of Minnesota Institutional Review Board. The experiment was similar in design to the main experiment. Stimuli included 22 images from the main experiment (one image from each of the 22 stimulus types), 92 images from a previous study investigating object representation in ventral temporal cortex (<xref ref-type="bibr" rid="bib56">Kriegeskorte et al., 2008</xref>), and 19 other images not used in this paper. As in the main experiment, images were approximately 2° × 2° in size. In each 4 s trial, a single image was flashed using an 800 ms ON, 200 ms OFF duty cycle. During a run, each image was presented in one trial, and 11 blank trials were also included. Each run lasted 608 s (10.1 min), and a total of 10 runs were collected. During stimulus presentation, the subject performed a variant of the fixation task. A small dot (0.1°×0.1°) was present at the center of the display and switched to one of five shades of red (ranging from (40,0,0) to (255,0,0) in five equally spaced increments) every 1200 ms (repetitions allowed). The subject was instructed to press a button whenever the luminance of the central dot increased and a different button whenever the luminance decreased.</p></sec><sec id="s4-15-2"><title>MRI data acquisition</title><p>MRI data were collected at the Center for Magnetic Resonance Research at the University of Minnesota using a 7T Siemens Magnetom scanner and a custom 4-channel-transmit, 32-channel-receive RF head coil. Stimuli were presented using a Cambridge Research Systems BOLDscreen 32 LCD monitor (resolution 1920 × 1080 at 120 Hz; viewing distance 189.5 cm). Functional data were acquired using 84 oblique slices covering occipitotemporal cortex: slice thickness 0.8 mm, slice gap 0 mm, field-of-view 160 mm (FE) × 129.6 mm (PE), phase-encode direction inferior-superior. A T2*-weighted, single-shot, gradient-echo EPI sequence was used: matrix size 200 × 162, TR 2.2 s, TE 22.4 ms, flip angle 80°, phase partial Fourier 6/8, in-plane acceleration factor (iPAT) 3, slice acceleration factor (multiband (<xref ref-type="bibr" rid="bib61">Moeller et al., 2010</xref>)) 2, nominal spatial resolution 0.8 mm × 0.8 mm × 0.8 mm.</p></sec><sec id="s4-15-3"><title>Data analysis</title><p>After pre-processing, the functional data were averaged across the thickness of gray matter and then analyzed using GLMdenoise (as in the main experiment). Beta weights extracted from FFA and VWFA were then modeled using several variants of the Template model. Model accuracy was quantified using 20-fold cross-validation (random subsets of the stimuli). (1) The <italic>Template model (original)</italic> is the same model used in the main experiment (three free parameters). Importantly, the category template in the model is fixed and not adjusted to the new dataset. (2) The <italic>Template model (half-max average)</italic> is a simple extension of the Template model in which the category template is estimated as follows: on each cross-validation iteration, using only the training set, identify responses that are at least half of the maximum response and then compute the centroid (in the V1-like representation) of the stimuli corresponding to these responses (three free parameters plus nonparametric fitting of the template). Note that this procedure is cross-validated in the sense that the category template is fit only to the training set; whether the estimated category template generalizes to novel stimuli is an empirical question that is assessed through cross-validation. (3) The <italic>Template model (half-max cluster)</italic> further extends the model to accommodate multiple category templates. The logic is that just as V1 models use filters at multiple orientations and spatial scales to characterize the overall V1 response, we might conceptualize FFA and VWFA as containing multiple templates tuned to different types of stimuli (for example, different face viewpoints, different fonts). First, we identify responses that are at least half of the maximum response from the training set. We then perform <italic>k</italic>-means clustering (in the V1-like representation) on the stimuli corresponding to these responses. We use a cosine metric to quantify distance, and we select the solution that minimizes cluster assignment error across 100 random initializations of centroid positions. The obtained centroids are unit-length normalized and then used as category templates in the Template model. For simplicity and to avoid overfitting, we compute the predicted response as a simple sum across the independent responses of different category templates and use the same parameter values (<italic>a</italic>, <italic>b</italic>, <italic>c</italic> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) for different templates. We systematically vary the number of clusters from 1 through 8, and select the number that maximizes cross-validation performance (four free parameters—three for <italic>a</italic>, <italic>b</italic>, and <italic>c</italic>, one for the number of clusters—plus nonparametric fitting of templates).</p></sec></sec><sec id="s4-16"><title>Code availability</title><p>Software code implementing the model proposed in this paper is available at <ext-link ext-link-type="uri" xlink:href="http://cvnlab.net/vtcipsmodel/">http://cvnlab.net/vtcipsmodel/</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank K Grill-Spector for providing the face and house stimuli used in the main experiment, R Kiani and N Kriegeskorte for providing the object stimuli used in the retinotopic mapping experiment, A Vu and E Yacoub for collecting pilot data, C Gratton, M Harms, and L Ramsey for scanning assistance, and K Weiner for assistance with ROI definition. We also thank P Elder, C Gratton, S Petersen, A Rokem, A Vogel, and J Winawer for helpful discussions. This work was supported by the McDonnell Center for Systems Neuroscience and Arts and Sciences at Washington University (KNK) and NSF Grant BCS-1551330 (JDY). Computations were performed using the facilities of the Washington University Center for High Performance Computing, which were partially provided through grant NCRR 1S10RR022984-01A1.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>KNK, Wrote the paper, Designed the experiments, Conducted the experiments, Analyzed the functional and behavioral data</p></fn><fn fn-type="con" id="con2"><p>JDY, Wrote the paper, Designed the experiments, Analyzed the diffusion data</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Informed written consent was obtained from all subjects, and the experimental protocol was approved by the Washington University in St. Louis Institutional Review Board and the University of Minnesota Institutional Review Board.</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname><given-names>EH</given-names></name><name><surname>Bergen</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatiotemporal energy models for the perception of motion</article-title><source>Journal of the Optical Society of America A</source><volume>2</volume><fpage>284</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.2.000284</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>JL</given-names></name><name><surname>Skare</surname><given-names>S</given-names></name><name><surname>Ashburner</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging</article-title><source>NeuroImage</source><volume>20</volume><fpage>870</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00336-7</pub-id><pub-id pub-id-type="pmid">14568458</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avidan</surname><given-names>G</given-names></name><name><surname>Harel</surname><given-names>M</given-names></name><name><surname>Hendler</surname><given-names>T</given-names></name><name><surname>Ben-Bashat</surname><given-names>D</given-names></name><name><surname>Zohary</surname><given-names>E</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Contrast sensitivity in human visual areas and its relationship to object recognition</article-title><source>Journal of Neurophysiology</source><volume>87</volume><fpage>3102</fpage><lpage>3116</lpage><pub-id pub-id-type="pmid">12037211</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldauf</surname><given-names>D</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural mechanisms of object-based attention</article-title><source>Science</source><volume>344</volume><fpage>424</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1126/science.1247003</pub-id><pub-id pub-id-type="pmid">24763592</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bejjanki</surname><given-names>VR</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Lu</surname><given-names>ZL</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Perceptual learning as improved probabilistic inference in early sensory areas</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>642</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1038/nn.2796</pub-id><pub-id pub-id-type="pmid">21460833</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boynton</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A framework for describing the effects of attention on visual responses</article-title><source>Vision Research</source><volume>49</volume><fpage>1129</fpage><lpage>1143</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.11.001</pub-id><pub-id pub-id-type="pmid">19038281</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brewer</surname><given-names>AA</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Wade</surname><given-names>AR</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Visual field maps and stimulus selectivity in human ventral occipital cortex</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1102</fpage><lpage>1109</lpage><pub-id pub-id-type="doi">10.1038/nn1507</pub-id><pub-id pub-id-type="pmid">16025108</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname><given-names>GJ</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Categorical clustering of the neural representation of color</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>15454</fpage><lpage>15465</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2472-13.2013</pub-id><pub-id pub-id-type="pmid">24068814</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>SD</given-names></name><name><surname>Heathcote</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The simplest complete model of choice response time: linear ballistic accumulation</article-title><source>Cognitive Psychology</source><volume>57</volume><fpage>153</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2007.12.002</pub-id><pub-id pub-id-type="pmid">18243170</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Krienen</surname><given-names>FM</given-names></name><name><surname>Yeo</surname><given-names>BT</given-names></name><name><surname>Btt</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Opportunities and limitations of intrinsic functional connectivity MRI</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>832</fpage><lpage>837</lpage><pub-id pub-id-type="doi">10.1038/nn.3423</pub-id><pub-id pub-id-type="pmid">23799476</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buracas</surname><given-names>GT</given-names></name><name><surname>Boynton</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The effect of spatial attention on contrast response functions in human visual cortex</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>93</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3162-06.2007</pub-id><pub-id pub-id-type="pmid">17202476</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Demb</surname><given-names>JB</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Do we know what the early visual system does?</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>10577</fpage><lpage>10597</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3726-05.2005</pub-id><pub-id pub-id-type="pmid">16291931</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name><name><surname>Lehéricy</surname><given-names>S</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Hénaff</surname><given-names>MA</given-names></name><name><surname>Michel</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The visual word form area: spatial and temporal characterization of an initial stage of reading in normal subjects and posterior split-brain patients</article-title><source>Brain</source><volume>123 Pt 2</volume><fpage>291</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1093/brain/123.2.291</pub-id><pub-id pub-id-type="pmid">10648437</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Lehéricy</surname><given-names>S</given-names></name><name><surname>Chochon</surname><given-names>F</given-names></name><name><surname>Lemer</surname><given-names>C</given-names></name><name><surname>Rivaud</surname><given-names>S</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Language-specific tuning of visual cortex? functional properties of the visual word form area</article-title><source>Brain</source><volume>125</volume><fpage>1054</fpage><lpage>1069</lpage><pub-id pub-id-type="doi">10.1093/brain/awf094</pub-id><pub-id pub-id-type="pmid">11960895</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title><source>Nature Reviews Neuroscience</source><volume>3</volume><fpage>215</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1038/nrn755</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>DD</given-names></name><name><surname>Savoy</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Functional magnetic resonance imaging (fMRI) &quot;brain reading&quot;: detecting and classifying distributed patterns of fMRI activity in human visual cortex</article-title><source>NeuroImage</source><volume>19</volume><fpage>261</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00049-1</pub-id><pub-id pub-id-type="pmid">12814577</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Cultural recycling of cortical maps</article-title><source>Neuron</source><volume>56</volume><fpage>384</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.004</pub-id><pub-id pub-id-type="pmid">17964253</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The unique role of the visual word form area in reading</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>254</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.04.003</pub-id><pub-id pub-id-type="pmid">21592844</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname><given-names>RS</given-names></name><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Quinn</surname><given-names>BT</given-names></name><name><surname>Dickerson</surname><given-names>BC</given-names></name><name><surname>Blacker</surname><given-names>D</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Maguire</surname><given-names>RP</given-names></name><name><surname>Hyman</surname><given-names>BT</given-names></name><name><surname>Albert</surname><given-names>MS</given-names></name><name><surname>Killiany</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>NeuroImage</source><volume>31</volume><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id><pub-id pub-id-type="pmid">16530430</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donkin</surname><given-names>C</given-names></name><name><surname>Brown</surname><given-names>S</given-names></name><name><surname>Heathcote</surname><given-names>A</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Diffusion versus linear ballistic accumulation: different models but the same conclusions about psychological processes?</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>18</volume><fpage>61</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.3758/s13423-010-0022-4</pub-id><pub-id pub-id-type="pmid">21327360</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Sagi</surname><given-names>D</given-names></name><name><surname>Bonneh</surname><given-names>YS</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Retinotopic patterns of correlated fluctuations in visual cortex reflect the dynamics of spontaneous perceptual suppression</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>2188</fpage><lpage>2198</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3388-12.2013</pub-id><pub-id pub-id-type="pmid">23365254</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Population receptive field estimates in human visual cortex</article-title><source>NeuroImage</source><volume>39</volume><fpage>647</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id><pub-id pub-id-type="pmid">17977024</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinberg</surname><given-names>DA</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Auerbach</surname><given-names>E</given-names></name><name><surname>Ramanna</surname><given-names>S</given-names></name><name><surname>Gunther</surname><given-names>M</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Multiplexed Echo planar imaging for sub-second whole brain FMRI and fast diffusion imaging</article-title><source>PLoS One</source><volume>5</volume><elocation-id>e15710</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0015710</pub-id><pub-id pub-id-type="pmid">21187930</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Freesurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukushima</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Neocognitron: a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</article-title><source>Biological Cybernetics</source><volume>36</volume><fpage>193</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1007/BF00344251</pub-id><pub-id pub-id-type="pmid">7370364</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname><given-names>I</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Anderson</surname><given-names>AW</given-names></name><name><surname>Skudlarski</surname><given-names>P</given-names></name><name><surname>Gore</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Activation of the middle fusiform 'face area' increases with expertise in recognizing novel objects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>568</fpage><lpage>573</lpage><pub-id pub-id-type="doi">10.1038/9224</pub-id><pub-id pub-id-type="pmid">10448223</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghuman</surname><given-names>AS</given-names></name><name><surname>Brunet</surname><given-names>NM</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Konecky</surname><given-names>RO</given-names></name><name><surname>Pyles</surname><given-names>JA</given-names></name><name><surname>Walls</surname><given-names>SA</given-names></name><name><surname>Destefino</surname><given-names>V</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Richardson</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamic encoding of face information in the human fusiform gyrus</article-title><source>Nature Communications</source><volume>5</volume><elocation-id>5672</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms6672</pub-id><pub-id pub-id-type="pmid">25482825</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id><pub-id pub-id-type="pmid">24962370</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id><pub-id pub-id-type="pmid">26157000</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Friedman</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</source><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Furey</surname><given-names>ML</given-names></name><name><surname>Ishai</surname><given-names>A</given-names></name><name><surname>Schouten</surname><given-names>JL</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Computational models of cortical visual processing</article-title><source>PNAS</source><volume>93</volume><fpage>623</fpage><lpage>627</lpage><pub-id pub-id-type="doi">10.1073/pnas.93.2.623</pub-id><pub-id pub-id-type="pmid">8570605</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Normalization of cell responses in cat striate cortex</article-title><source>Visual Neuroscience</source><volume>9</volume><fpage>181</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1017/S0952523800009640</pub-id><pub-id pub-id-type="pmid">1504027</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heekeren</surname><given-names>HR</given-names></name><name><surname>Marrett</surname><given-names>S</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A general mechanism for perceptual decision-making in the human brain</article-title><source>Nature</source><volume>431</volume><fpage>859</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1038/nature02966</pub-id><pub-id pub-id-type="pmid">15483614</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Receptive fields of cells in striate cortex of very young, visually inexperienced kittens</article-title><source>Journal of Neurophysiology</source><volume>26</volume><fpage>994</fpage><lpage>1002</lpage><pub-id pub-id-type="pmid">14084171</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title><source>Neuron</source><volume>76</volume><fpage>1210</fpage><lpage>1224</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id><pub-id pub-id-type="pmid">23259955</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itthipuripat</surname><given-names>S</given-names></name><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Deering</surname><given-names>S</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sensory gain outperforms efficient readout mechanisms in predicting attention-related improvements in behavior</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>13384</fpage><lpage>13398</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2277-14.2014</pub-id><pub-id pub-id-type="pmid">25274817</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeurissen</surname><given-names>B</given-names></name><name><surname>Leemans</surname><given-names>A</given-names></name><name><surname>Jones</surname><given-names>DK</given-names></name><name><surname>Tournier</surname><given-names>JD</given-names></name><name><surname>Sijbers</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Probabilistic fiber tracking using the residual bootstrap with constrained spherical deconvolution</article-title><source>Human Brain Mapping</source><volume>32</volume><fpage>461</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1002/hbm.21032</pub-id><pub-id pub-id-type="pmid">21319270</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>JP</given-names></name><name><surname>Palmer</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>The two-dimensional spatial structure of simple receptive fields in cat striate cortex</article-title><source>Journal of Neurophysiology</source><volume>58</volume><fpage>1187</fpage><lpage>1211</lpage><pub-id pub-id-type="pmid">3437330</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual features as stepping stones toward semantics: explaining object similarity in IT and perception with non-negative least squares</article-title><source>Neuropsychologia</source><volume>83</volume><fpage>201</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.10.023</pub-id><pub-id pub-id-type="pmid">26493748</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kang</surname><given-names>X</given-names></name><name><surname>Yund</surname><given-names>EW</given-names></name><name><surname>Herron</surname><given-names>TJ</given-names></name><name><surname>Woods</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Improving the resolution of functional brain imaging: analyzing functional data in anatomical space</article-title><source>Magnetic Resonance Imaging</source><volume>25</volume><fpage>1070</fpage><lpage>1078</lpage><pub-id pub-id-type="doi">10.1016/j.mri.2006.12.005</pub-id><pub-id pub-id-type="pmid">17707169</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>Journal of Neuroscience</source><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="pmid">9151747</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Wojciulik</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Visual attention: insights from brain imaging</article-title><source>Nature Reviews Neuroscience</source><volume>1</volume><fpage>91</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1038/35039043</pub-id><pub-id pub-id-type="pmid">11252779</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional specificity in the human brain: a window into the functional architecture of the mind</article-title><source>PNAS</source><volume>107</volume><fpage>11163</fpage><lpage>11170</lpage><pub-id pub-id-type="doi">10.1073/pnas.1005062107</pub-id><pub-id pub-id-type="pmid">20484679</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Prenger</surname><given-names>RJ</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Identifying natural images from human brain activity</article-title><source>Nature</source><volume>452</volume><fpage>352</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1038/nature06713</pub-id><pub-id pub-id-type="pmid">18322462</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Dougherty</surname><given-names>RF</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>GLMdenoise: a fast, automated technique for denoising task-based fMRI data</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>247</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00247</pub-id><pub-id pub-id-type="pmid">24381539</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Mezer</surname><given-names>A</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Compressive spatial summation in human visual cortex</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>481</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1152/jn.00105.2013</pub-id><pub-id pub-id-type="pmid">23615546</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Mezer</surname><given-names>A</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2013">2013c</year><article-title>A two-stage cascade model of BOLD responses in human visual cortex</article-title><source>PLoS Computational Biology</source><volume>9</volume><elocation-id>e1003079</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003079</pub-id><pub-id pub-id-type="pmid">23737741</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>vtcipsmodel</data-title><source>GitHub</source><version>a2d0770ac8f2d5041af0487ee72f670c42b5e4a1</version><uri xlink:href="http://cvnlab.net/vtcipsmodel/">http://cvnlab.net/vtcipsmodel/</uri></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>AS</given-names></name><name><surname>Buchsbaum</surname><given-names>BR</given-names></name><name><surname>Erickson</surname><given-names>DT</given-names></name><name><surname>D'Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010a</year><article-title>The functional anatomy of a perceptual decision in the human brain</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>1179</fpage><lpage>1194</lpage><pub-id pub-id-type="doi">10.1152/jn.00364.2009</pub-id><pub-id pub-id-type="pmid">20032247</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>AS</given-names></name><name><surname>Erickson</surname><given-names>DT</given-names></name><name><surname>Buchsbaum</surname><given-names>BR</given-names></name><name><surname>D'Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010b</year><article-title>Neural representations of relevant and irrelevant features in perceptual decision making</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>15778</fpage><lpage>15789</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3163-10.2010</pub-id><pub-id pub-id-type="pmid">21106817</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Simmons</surname><given-names>WK</given-names></name><name><surname>Bellgowan</surname><given-names>PS</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Circular analysis in systems neuroscience: the dangers of double dipping</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>535</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1038/nn.2303</pub-id><pub-id pub-id-type="pmid">19396166</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lauritzen</surname><given-names>TZ</given-names></name><name><surname>D'Esposito</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Silver</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Top-down flow of visual spatial attention signals from parietal to occipital cortex</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>18</elocation-id><pub-id pub-id-type="doi">10.1167/9.13.18</pub-id><pub-id pub-id-type="pmid">20055551</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Neural mechanisms of spatial selective attention in areas V1, V2, and V4 of macaque visual cortex</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>24</fpage><lpage>42</lpage><pub-id pub-id-type="pmid">9120566</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAdams</surname><given-names>CJ</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4</article-title><source>Journal of Neuroscience</source><volume>19</volume><fpage>431</fpage><lpage>441</lpage><pub-id pub-id-type="pmid">9870971</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Olman</surname><given-names>CA</given-names></name><name><surname>Auerbach</surname><given-names>E</given-names></name><name><surname>Strupp</surname><given-names>J</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Multiband multislice GE-EPI at 7 tesla, with 16-fold acceleration using partial parallel imaging with application to high spatial and temporal whole-brain fMRI</article-title><source>Magnetic Resonance in Medicine</source><volume>63</volume><fpage>1144</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1002/mrm.22361</pub-id><pub-id pub-id-type="pmid">20432285</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>SO</given-names></name><name><surname>He</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Contrast invariance in the human lateral occipital complex depends on attention</article-title><source>Current Biology</source><volume>16</volume><fpage>606</fpage><lpage>611</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.02.019</pub-id><pub-id pub-id-type="pmid">16546086</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Echavarria</surname><given-names>CE</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Thinking outside the box: rectilinear shapes selectively activate scene-selective cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>6721</fpage><lpage>6735</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4802-13.2014</pub-id><pub-id pub-id-type="pmid">24828628</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Reilly</surname><given-names>JX</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Johansen-Berg</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Tools of the trade: psychophysiological interactions and functional connectivity</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>7</volume><fpage>604</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1093/scan/nss055</pub-id><pub-id pub-id-type="pmid">22569188</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohayon</surname><given-names>S</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>What makes a cell face selective? the importance of contrast</article-title><source>Neuron</source><volume>74</volume><fpage>567</fpage><lpage>581</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.024</pub-id><pub-id pub-id-type="pmid">22578507</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitzalis</surname><given-names>S</given-names></name><name><surname>Fattori</surname><given-names>P</given-names></name><name><surname>Galletti</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The functional role of the medial motion area V6</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>6</volume><elocation-id>91</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2012.00091</pub-id><pub-id pub-id-type="pmid">23335889</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>CJ</given-names></name><name><surname>Devlin</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The interactive account of ventral occipitotemporal contributions to reading</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>246</fpage><lpage>253</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.04.001</pub-id><pub-id pub-id-type="pmid">21549634</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rainer</surname><given-names>G</given-names></name><name><surname>Augath</surname><given-names>M</given-names></name><name><surname>Trinath</surname><given-names>T</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Nonmonotonic noise tuning of BOLD fMRI signal to natural images in the visual cortex of the anesthetized monkey</article-title><source>Current Biology</source><volume>11</volume><fpage>846</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1016/S0960-9822(01)00242-1</pub-id><pub-id pub-id-type="pmid">11516645</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><elocation-id>59</elocation-id><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reich</surname><given-names>L</given-names></name><name><surname>Szwed</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Amedi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A ventral visual stream reading center independent of visual experience</article-title><source>Current Biology</source><volume>21</volume><fpage>363</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.01.040</pub-id><pub-id pub-id-type="pmid">21333539</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ress</surname><given-names>D</given-names></name><name><surname>Backus</surname><given-names>BT</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Activity in primary visual cortex predicts performance in a visual detection task</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>940</fpage><lpage>945</lpage><pub-id pub-id-type="doi">10.1038/78856</pub-id><pub-id pub-id-type="pmid">10966626</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ress</surname><given-names>D</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neuronal correlates of perception in early visual cortex</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>414</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1038/nn1024</pub-id><pub-id pub-id-type="pmid">12627164</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The normalization model of attention</article-title><source>Neuron</source><volume>61</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id><pub-id pub-id-type="pmid">19186161</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Pasternak</surname><given-names>T</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Attention increases sensitivity of V4 neurons</article-title><source>Neuron</source><volume>26</volume><fpage>703</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)81206-4</pub-id><pub-id pub-id-type="pmid">10896165</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rissman</surname><given-names>J</given-names></name><name><surname>Gazzaley</surname><given-names>A</given-names></name><name><surname>D'Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Measuring functional connectivity during distinct stages of a cognitive task</article-title><source>NeuroImage</source><volume>23</volume><fpage>752</fpage><lpage>763</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.06.035</pub-id><pub-id pub-id-type="pmid">15488425</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Invariant visual object and face recognition: neural and computational bases, and a model, VisNet</article-title><source>Frontiers in Computational Neuroscience</source><volume>6</volume><elocation-id>35</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2012.00035</pub-id><pub-id pub-id-type="pmid">22723777</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saalmann</surname><given-names>YB</given-names></name><name><surname>Pigarev</surname><given-names>IN</given-names></name><name><surname>Vidyasagar</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural mechanisms of visual attention: how top-down feedback highlights relevant locations</article-title><source>Science</source><volume>316</volume><fpage>1612</fpage><lpage>1615</lpage><pub-id pub-id-type="doi">10.1126/science.1139140</pub-id><pub-id pub-id-type="pmid">17569863</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schira</surname><given-names>MM</given-names></name><name><surname>Tyler</surname><given-names>CW</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Spehar</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The foveal confluence in human visual cortex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>9050</fpage><lpage>9058</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1760-09.2009</pub-id><pub-id pub-id-type="pmid">19605642</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>AB</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Shape selectivity in primate lateral intraparietal cortex</article-title><source>Nature</source><volume>395</volume><fpage>500</fpage><lpage>503</lpage><pub-id pub-id-type="doi">10.1038/26752</pub-id><pub-id pub-id-type="pmid">9774105</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname><given-names>T</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Kouh</surname><given-names>M</given-names></name><name><surname>Cadieu</surname><given-names>C</given-names></name><name><surname>Knoblich</surname><given-names>U</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A quantitative theory of immediate visual recognition</article-title><source>Progress in Brain Research</source><volume>165</volume><fpage>33</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(06)65004-8</pub-id><pub-id pub-id-type="pmid">17925239</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural basis of a perceptual decision in the parietal cortex (area LIP) of the rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>1916</fpage><lpage>1936</lpage><pub-id pub-id-type="pmid">11600651</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Johansen-Berg</surname><given-names>H</given-names></name><name><surname>Bannister</surname><given-names>PR</given-names></name><name><surname>De Luca</surname><given-names>M</given-names></name><name><surname>Drobnjak</surname><given-names>I</given-names></name><name><surname>Flitney</surname><given-names>DE</given-names></name><name><surname>Niazy</surname><given-names>RK</given-names></name><name><surname>Saunders</surname><given-names>J</given-names></name><name><surname>Vickers</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>De Stefano</surname><given-names>N</given-names></name><name><surname>Brady</surname><given-names>JM</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>NeuroImage</source><volume>23 Suppl 1</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Striem-Amit</surname><given-names>E</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Amedi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reading with sounds: sensory substitution selectively activates the visual word form area in the blind</article-title><source>Neuron</source><volume>76</volume><fpage>640</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.026</pub-id><pub-id pub-id-type="pmid">23141074</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takemura</surname><given-names>H</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Yeatman</surname><given-names>JD</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A major human white matter pathway between dorsal and ventral visual cortex</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>2205</fpage><lpage>2214</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv064</pub-id><pub-id pub-id-type="pmid">25828567</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tournier</surname><given-names>JD</given-names></name><name><surname>Calamante</surname><given-names>F</given-names></name><name><surname>Connelly</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Robust determination of the fibre orientation distribution in diffusion MRI: non-negativity constrained super-resolved spherical deconvolution</article-title><source>NeuroImage</source><volume>35</volume><fpage>1459</fpage><lpage>1472</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.02.016</pub-id><pub-id pub-id-type="pmid">17379540</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Twomey</surname><given-names>T</given-names></name><name><surname>Kawabata Duncan</surname><given-names>KJ</given-names></name><name><surname>Price</surname><given-names>CJ</given-names></name><name><surname>Devlin</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Top-down modulation of ventral occipito-temporal responses during visual word recognition</article-title><source>NeuroImage</source><volume>55</volume><fpage>1242</fpage><lpage>1251</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.01.001</pub-id><pub-id pub-id-type="pmid">21232615</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinckier</surname><given-names>F</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Jobert</surname><given-names>A</given-names></name><name><surname>Dubus</surname><given-names>JP</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hierarchical coding of letter strings in the ventral stream: dissecting the inner organization of the visual word-form system</article-title><source>Neuron</source><volume>55</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.05.031</pub-id><pub-id pub-id-type="pmid">17610823</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Rauschecker</surname><given-names>AM</given-names></name><name><surname>Yeatman</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Learning to see words</article-title><source>Annual Review of Psychology</source><volume>63</volume><fpage>31</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-120710-100434</pub-id><pub-id pub-id-type="pmid">21801018</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Computational neuroimaging of human visual cortex</article-title><source>Annual Review of Neuroscience</source><volume>22</volume><fpage>145</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.22.1.145</pub-id><pub-id pub-id-type="pmid">10202535</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Mruczek</surname><given-names>RE</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Probabilistic maps of visual topography in human cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id><pub-id pub-id-type="pmid">25452571</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Golarai</surname><given-names>G</given-names></name><name><surname>Caspers</surname><given-names>J</given-names></name><name><surname>Chuapoco</surname><given-names>MR</given-names></name><name><surname>Mohlberg</surname><given-names>H</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The mid-fusiform sulcus: a landmark identifying both cytoarchitectonic and functional divisions of human ventral temporal cortex</article-title><source>NeuroImage</source><volume>84</volume><fpage>453</fpage><lpage>465</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.068</pub-id><pub-id pub-id-type="pmid">24021838</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Sparsely-distributed organization of face and limb activations in human ventral temporal cortex</article-title><source>NeuroImage</source><volume>52</volume><fpage>1559</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.04.262</pub-id><pub-id pub-id-type="pmid">20457261</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Not one extrastriate body area: using anatomical landmarks, hMT+, and visual field maps to parcellate limb-selective activations in human lateral occipitotemporal cortex</article-title><source>NeuroImage</source><volume>56</volume><fpage>2183</fpage><lpage>2199</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.03.041</pub-id><pub-id pub-id-type="pmid">21439386</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>MC</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Complete functional characterization of sensory neurons by system identification</article-title><source>Annual Review of Neuroscience</source><volume>29</volume><fpage>477</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113024</pub-id><pub-id pub-id-type="pmid">16776594</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeatman</surname><given-names>JD</given-names></name><name><surname>Rauschecker</surname><given-names>AM</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Anatomy of the visual word form area: adjacent cortical circuits and long-range white matter connections</article-title><source>Brain and Language</source><volume>125</volume><fpage>146</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2012.04.010</pub-id><pub-id pub-id-type="pmid">22632810</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeatman</surname><given-names>JD</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Mezer</surname><given-names>A</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The vertical occipital fasciculus: a century of controversy resolved by in vivo measurements</article-title><source>PNAS</source><volume>111</volume><fpage>E5214</fpage><lpage>E5223</lpage><pub-id pub-id-type="doi">10.1073/pnas.1418503111</pub-id><pub-id pub-id-type="pmid">25404310</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yue</surname><given-names>X</given-names></name><name><surname>Cassidy</surname><given-names>BS</given-names></name><name><surname>Devaney</surname><given-names>KJ</given-names></name><name><surname>Holt</surname><given-names>DJ</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Lower-level stimulus features strongly influence responses in the fusiform face area</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>35</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq050</pub-id><pub-id pub-id-type="pmid">20375074</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zetzsche</surname><given-names>C</given-names></name><name><surname>Krieger</surname><given-names>G</given-names></name><name><surname>Wegmann</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The atoms of vision: cartesian or polar?</article-title><source>Journal of the Optical Society of America A</source><volume>16</volume><fpage>1554</fpage><lpage>1565</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.16.001554</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.22341.012</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Reviewing editor</role><aff id="aff5"><institution>University of Pennsylvania</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Bottom-up and top-down computations in word- and face-selective cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and David Van Essen as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Floris de Lange (Reviewer #1); Geoffrey K. Aguirre (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This study used fMRI and computational modeling to investigate bottom-up and top-down contributions to activation patterns in the ventral temporal cortex (VTC) while human subjects viewed different stimuli (face or text of different contrasts and phase coherences) under different task conditions (fixation, one-back, and categorization). They show that responses in human FFA and VWFA are strongly influenced by both stimulus properties and task demands. They also identify a region of the IPS as a source of attentional modulation of the VTC. Finally, they propose a template-matching model that can reasonably account for the observed bottom-up and top-down activity fluctuations in VTC. Given that analyses of these types of experimental data have typically been framed more qualitatively, the reviewers agree that a quantitative and rigorous modeling approach is certainly a welcome contribution. The model also is noted as having the virtue of simplicity.</p><p>Nonetheless, the reviewers were also in agreement that there are a number of major issues that need to be addressed, described below.</p><p>Essential revisions:</p><p>1) Scope and readability: This paper contains an ambitious number of ideas. A challenge for the reader is that the model details are the primary product of the work, yet most of these details are relegated to Methods or Supplementary Materials. Here are some suggestions:</p><p>a) Consider moving <xref ref-type="fig" rid="fig2">Figure 2</xref> to the supplement. This is a nice result, but it is not needed for the subsequent modeling work.</p><p>b) Re-arrange the results to present: 1) &quot;VTC responses depend on both stimulus properties and cognitive task&quot;, 2) &quot;Model of bottom-up computations in VTC&quot;. Observe that this model accounts for the fixation data but not the task data, 3) &quot;IPS is the source of top-down modulation to VTC&quot;, 4) &quot;Model of top-down computations in VTC&quot;, 5) &quot;Model of perceptual decision-making in IPS&quot;.</p><p>c) Provide some detail in the main text regarding each model. In particular, the primary features of the model that impacts inference should be described. For example, the fact that the category template model is generated from the stimuli themselves is quite relevant.</p><p>d) Move the DTI results to the supplement or remove. It is certainly worthwhile noting that the VTC and IPS are connected by a white matter pathway. However, Jason's prior work demonstrates the existence of this fiber tract, and the current paper simply re-demonstrates the pathway. In the absence of some (e.g.) individual difference measurement or other use of the data to support inferences beyond the mere existence of the pathway, this result is not particularly additive.</p><p>2) Connection to literature of top-down effects on contrast-sensitivity. The paper ignores a large body of work that has also tried to formalize top-down affects such as attention on contrast-sensitivity in early visual cortex of humans and monkeys. This literature has described contrast-gain, response-gain, and additive-offsets of contrast sensitivity with top-down modulation due to attention. Some connection should be made between these results and this vast literature.</p><p>Moreover, how can the results be reconciled with findings from other groups that areas such as EBA and FFA also have category-selective responses in congenitally blind people? Does it pose a problem for their framework?</p><p>3) Model analysis and comparison.</p><p>a) It would be useful to more completely explain the model comparisons, and the work that the various parameter of each model perform in fitting the data. For example, the category template model provides good fits to the fixation-response data across contrast and phase coherence. How does these fits depend on the Gabor filter bank, the category template, and/or the parameters available in the normalization step? Moreover, the authors conclude that a scaling mechanism is in place for amplification of VTC responses. The alternative models that they draw in <xref ref-type="fig" rid="fig2">Figure 2</xref> seem to make qualitatively quite similar predictions. How do they arrive at the conclusion of which mechanism is most supported by the data? Formal model comparison would be useful. Finally, why don't the top-down and drift-diffusion models make use of the category template model that is fit to the fixation data? This seems like a missed opportunity. Is this because the model fits are poor or a technical limitation?</p><p>b) How general is the model for categorical representation in FFA and VWFA? The model transforms images into weightings of a V1-like representation (gabor wavelet pyramid) and then projects these onto a template for faces and text. This amounts to little more than putting a linear classifier on a V1 like representation, which would seem much too simplistic to account for known properties of invariance in high-order visual cortex to things like view angle, lighting, rotation, scale, etc. Moreover, how well can the model predict responses to non-template stimuli? Because the templates are built as the centroid of responses of the actual stimuli used, it is not surprising that it can explain the face and word responses. As the authors point out, the more important tests are for stimuli outside of those that the model is built on. In <xref ref-type="fig" rid="fig4">Figure 4</xref> the phase coherence manipulation (as well as noise and some alternative stimuli) are relevant. However, the phase coherence manipulation in the data of <xref ref-type="fig" rid="fig4">Figure 4</xref> look more like a step function then what the model predicts, suggesting poor fits. Moreover, the supplementary data show that the only way to save the template model for a more general set of stimuli is to basically build the templates based on what stimuli evoked half-maximal response, which is like tweaking the templates to match the responses. While the legend mentions train and test sets, which would suggest cross-validation that might alleviate some of these concerns, the details are omitted. If train and test are different sets of responses to the same presentations of images, then the cross-validation would say more about the consistency in response then whether the model generalizes to new stimuli.</p><p>c) It seems a bit unfair to compare the parameterized category template model with a uniform category model. The category model has no contrast-sensitivity and thus gives a flat response across contrast in <xref ref-type="fig" rid="fig4">Figure 4</xref>. How much difference would there be between models, if you just allowed the category model to be contrast-sensitive? Likewise, the category model does seem to step for phase coherence, but no description was given for this; is this because at the lower phase coherences the stimulus category cannot be discerned? Also, how is it that some of the category model fits show some decrease in response as a function of contrast for the preferred category? In general, it would be useful to move some of the supplementary material that describes the performance of other models (including for the RT data) to the main text, since the model comparisons are of central importance to this study.</p><p>4) IPS fit circularity. Despite the assurances in Methods, the top-down IPS model faces a bit of a circularity problem. The IPS region was found using a search process that sought signals that could account for task effects. It is therefore not surprising to learn (<xref ref-type="fig" rid="fig4">Figure 4</xref>, bottom) that these signals well fit the data. Indeed, the r=0.96 fit is a bit worrying in this regard and suggests the possibility of over-fitting. Here are some options to rectify: a) Explicitly describe the fitting as a demonstration exercise, not to be taken as independent evidence of the role of the IPS; b) Obtain data from a new set of subjects, using the IPS region identified in the initial set of subjects to replicate the result.</p><p>5) Consistency of model framing of parietal cortex. The parietal cortex is first modeled as the source of an attention signal that modulates VTC, and then also as an accumulator of evidence from VTC. Is this plausible, and can the fMRI data support this conclusion? For example, given high task difficulty, does strong activation of IPS imply strong neural activation to provide attentional control or weak but temporally extended neural activation to accumulate weak evidence?</p><p>6) Parietal cortex correlation analysis. The suggested effect of attention is posited to be a gain effect – yet when looking for the source of this gain signal, the fixation condition is subtracted off. If it really is a gain, subtraction would be expected to leave some stimulus-driven effect in the other conditions, and correlations to parietal cortex could be due in part to this residual stimulus representation.</p><p>7) Correlation is not causation. A correlation with parietal cortex and VTC modulation is reported and interpreted as a causative signal from IPS areas. But the analysis is a correlation; it does not prove which way causation goes.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Bottom-up and top-down computations in word- and face-selective cortex&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by David Van Essen (Senior editor) and a Reviewing editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>1) The new discussion that makes connections to existing literature on top-down effects on contrast sensitivity is a welcome addition (Discussion section paragraph four). It would likely benefit readers to ascribe specific references to the various models discussed.</p><p>2) The cross-validation model comparisons could also be described better. For example, <xref ref-type="fig" rid="fig5">Figure 5c</xref> shows 10 models + 2 benchmarks in the fig, but only 7 are described (briefly) in the text. It would also be useful to describe the number of free parameters in each model. How do goodness of fits compare when taking into account the different degrees of freedom?</p><p>3) <xref ref-type="fig" rid="fig5">Figure 5</xref>: please indicate meaning of colors, arrows, etc. in the legend, not just the associated text.</p><p>4) DDM (<xref ref-type="fig" rid="fig6">Figure 6</xref>): It would be helpful to readers to clarify the terminology related to the decision model. A DDM has a single decision variable that has Brownian-like dynamics; here the model appears to be closer to a race between linear ballistic accumulators (three in the case of the 3AFC categorization task). It also would be useful to discuss these results in the context of a host of findings that have largely highlighted how difficult it is to use slow BOLD signals to make inferences about accumulator-like activity occurring over relatively short timescales, as in this study (see a nice discussion of these issues in Krueger et al., 2017, &quot;Evidence accumulation detected in BOLD signal using slow perceptual decision making,&quot; J Neurosci Methods).</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.22341.013</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions:</italic> </p><p><italic>1) Scope and readability: This paper contains an ambitious number of ideas. A challenge for the reader is that the model details are the primary product of the work, yet most of these details are relegated to Methods or Supplementary Materials. Here are some suggestions:</italic> </p><p><italic>a) Consider moving <xref ref-type="fig" rid="fig2">Figure 2</xref> to the supplement. This is a nice result, but it is not needed for the subsequent modeling work.</italic> </p><p><italic>b) Re-arrange the results to present: 1) &quot;VTC responses depend on both stimulus properties and cognitive task&quot;, 2) &quot;Model of bottom-up computations in VTC&quot;. Observe that this model accounts for the fixation data but not the task data, 3) &quot;IPS is the source of top-down modulation to VTC&quot;, 4) &quot;Model of top-down computations in VTC&quot;, 5) &quot;Model of perceptual decision-making in IPS&quot;.</italic> </p><p><italic>c) Provide some detail in the main text regarding each model. In particular, the primary features of the model that impacts inference should be described. For example, the fact that the category template model is generated from the stimuli themselves is quite relevant.</italic> </p><p><italic>d) Move the DTI results to the supplement or remove. It is certainly worthwhile noting that the VTC and IPS are connected by a white matter pathway. However, Jason's prior work demonstrates the existence of this fiber tract, and the current paper simply re-demonstrates the pathway. In the absence of some (e.g.) individual difference measurement or other use of the data to support inferences beyond the mere existence of the pathway, this result is not particularly additive.</italic> </p><p>We appreciate the suggestions, and we are aware that our work is technically dense. We wholeheartedly agree that the model details are critical; it is always challenging to balance details against brevity in this type of work.</p><p>We think <xref ref-type="fig" rid="fig2">Figure 2</xref> is quite important—it is a nice way of understanding the structure of the data, and is independent of the modeling effort. The revised introductory paragraph of the corresponding section ‘Top-down modulation acts as a stimulus-specific scaling’ makes clear that analysis sets the stage for the subsequent modeling effort.</p><p>Also, we think the DTI results are an important and necessary precursor for building a model that posits a direct relationship between signals measured in two spatially separated brain regions (VTC and IPS). Previous work did not examine the relationship between VOF endpoints and functionally defined FFA ROIs in individual subjects. We believe that establishing the spatial proximity of the FFA and VWFA to VOF fibers that connect to the functionally defined IPS hotspot demonstrates that the model in the next section is anatomically plausible.</p><p>We re-arrange the results, use better subheadings, and expand introductory paragraphs, as per suggestion (b). We add detail regarding the various models in the main text, as per suggestion (c). We also merge the model comparison results into the main text, which further fleshes out the modeling work. These changes are located throughout the body of the main text as well as in <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig5">5</xref> and <xref ref-type="fig" rid="fig6">6</xref>.</p><p><italic>2) Connection to literature of top-down effects on contrast-sensitivity. The paper ignores a large body of work that has also tried to formalize top-down affects such as attention on contrast-sensitivity in early visual cortex of humans and monkeys. This literature has described contrast-gain, response-gain, and additive-offsets of contrast sensitivity with top-down modulation due to attention. Some connection should be made between these results and this vast literature.</italic> </p><p>We agree; we had a section discussing this literature in an early version of the paper, but that section was removed for length reasons.</p><p>We discuss the literature on top-down effects on contrast-sensitivity (Discussion section paragraph four).</p><p><italic>Moreover, how can the results be reconciled with findings from other groups that areas such as EBA and FFA also have category-selective responses in congenitally blind people? Does it pose a problem for their framework?</italic> </p><p>It is certainly possible that there exist a variety of types of top-down signals, and we are open to the possibility that our current model is incomplete. For example, we might suspect that the VWFA can receive top-down signals from language areas but that these signals do not play a large role in the three tasks sampled in our experiment. With further experimental measurements, our framework could be expanded to accommodate additional types of top- down signals. This line of thinking is discussed in the paper (subsection “Model of top-down computations in VTC”).</p><p><italic>3) Model analysis and comparison.</italic> </p><p><italic>a) It would be useful to more completely explain the model comparisons, and the work that the various parameter of each model perform in fitting the data. For example, the category template model provides good fits to the fixation-response data across contrast and phase coherence. How does these fits depend on the Gabor filter bank, the category template, and/or the parameters available in the normalization step?</italic> </p><p>This is a good question. The Gabor filter bank was taken identically from the previous computational model that we developed (SOC model). Thus, there was no tweaking of the filter bank to fit the data, thereby avoiding overfitting. Exploring the dependence of the model on the filter bank will require additional measurements and will be the focus of future work. As for the category template and normalization step, these were indeed investigated, but results were presented in the supplement.</p><p>We merge the model comparisons (previously in the supplement) into the main text. This shows how the category template and the normalization step affect model performance (subsection “Model of bottom-up computations in VTC”; <xref ref-type="fig" rid="fig2">Figure 2C</xref>), and shows a brief investigation of spatial frequency properties (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Also, we revise the text to explicitly mention that the filter bank is taken directly from the previous SOC model (subsection “Template model”).</p><p><italic>Moreover, the authors conclude that a scaling mechanism is in place for amplification of VTC responses. The alternative models that they draw in <xref ref-type="fig" rid="fig2">Figure 2</xref> seem to make qualitatively quite similar predictions. How do they arrive at the conclusion of which mechanism is most supported by the data? Formal model comparison would be useful.</italic> </p><p>Formal model comparison is performed; we apologize that this was not clear in the original version of the paper.</p><p>We implement the ‘Area-specific enhancement’ model (formal implementation of this model was not present in the original paper but now exists in <xref ref-type="fig" rid="fig5">Figure 5</xref>). We also alert the reader that formal evaluation of models is performed (subsection “Top-down modulation acts as a stimulus-specific scaling”).</p><p><italic>Finally, why don't the top-down and drift-diffusion models make use of the category template model that is fit to the fixation data? This seems like a missed opportunity. Is this because the model fits are poor or a technical limitation?</italic> </p><p>We appreciate the reviewer’s attention to detail. It is not a technical limitation. Rather, given that the model fits are good but not perfect, we decided to isolate the performance evaluation of each component of the model (Template, IPS-scaling, Drift-diffusion). This approach allows us to cleanly assess the quality of each component. For example, if the Template model had been used in the implementation of the IPS-scaling model, then it would not be clear whether shortcomings in cross-validation performance of the IPS-scaling model were due to inaccuracies in the Template model or due to actual inaccuracies in the modeling of top-down modulation.</p><p>We revise the manuscript to address and discuss this issue (subsection “Computational modelling”).</p><p> <italic>b) How general is the model for categorical representation in FFA and VWFA? The model transforms images into weightings of a V1-like representation (gabor wavelet pyramid) and then projects these onto a template for faces and text. This amounts to little more than putting a linear classifier on a V1 like representation, which would seem much too simplistic to account for known properties of invariance in high-order visual cortex to things like view angle, lighting, rotation, scale, etc.</italic> </p><p>It may very well be the case that high-level visual cortex has complex processing dedicated to achieving invariances, and it may seem that certain models are too simplistic to implement such behaviors. But our view is that the precise numbers matter and that there is no choice but to collect appropriate experimental data and perform quantitative modeling work to see to what extent a particular model can account for the data. As a case in point, in previous work we explored the issue of position and size invariance and found that, perhaps surprisingly, a relatively simple computational mechanism can account for experimentally measured effects (Kay et al., 2013).</p><p>We believe that the field needs increased effort towards developing general, predictive, and interpretable models that are directly validated against empirical data. Even though our current model does not perfectly account for all possible experimental manipulations, we believe there is value in starting somewhere and defining a framework that can be refined in future studies. A logical extension of the present work is to extend the model to address spatial invariance/tolerance, given our previous work on this topic (Kay et al., Current Biology, 2015). Investigating and validating an extended model will require a larger and more extensive stimulus set.</p><p><italic>Moreover, how well can the model predict responses to non-template stimuli? Because the templates are built as the centroid of responses of the actual stimuli used, it is not surprising that it can explain the face and word responses. As the authors point out, the more important tests are for stimuli outside of those that the model is built on.</italic> </p><p>We agree with the sentiment here: testing generalization is important. For the main set of data, we use cross-validation across stimuli, which provides some assessment of generalization power. We find that the model predicts the response to non-preferred stimuli, meaning that the relative response amplitude in VWFA and FFA depends on the similarity of the visual stimulus to the template. For example, the model predicts the relative VWFA response to polygons, houses, faces, and noise patterns, even though these stimuli were not a part of the word template. The additional larger set of data that we included provides even better assessment of generalization power (for these data, we also used cross-validation across stimuli).</p><p><italic>In <xref ref-type="fig" rid="fig4">Figure 4</xref> the phase coherence manipulation (as well as noise and some alternative stimuli) are relevant. However, the phase coherence manipulation in the data of <xref ref-type="fig" rid="fig4">Figure 4</xref> look more like a step function then what the model predicts, suggesting poor fits.</italic> </p><p>Yes, the model does not satisfactorily capture the step-like shape of the phase-coherence responses (as mentioned in the Discussion section paragraph two). This means that there is room for improvement. It is interesting to consider potential paths forward. One route is to posit that there may be some sort of sigmoidal nonlinearity in the second-stage filtering process. A different route is to explore the possibility that during the fixation task, there was some amount of “unavoidable attention to the stimulus”, in the sense that beyond a certain phase-coherence level, the stimulus automatically attracts attention. In this case, the step-like shape might reflect a combination of the true bottom-up response and some accidental top-down modulation.</p><p><italic>Moreover, the supplementary data show that the only way to save the template model for a more general set of stimuli is to basically build the templates based on what stimuli evoked half-maximal response, which is like tweaking the templates to match the responses. While the legend mentions train and test sets, which would suggest cross-validation that might alleviate some of these concerns, the details are omitted. If train and test are different sets of responses to the same presentations of images, then the cross-validation would say more about the consistency in response then whether the model generalizes to new stimuli.</italic> </p><p>The reviewer is correct that for the larger set of data in the supplement, we allowed fitting of the templates used in the Template model. The reviewer is also correct that cross-validation is important and that different cross-validation schemes assess different types of generalization. In the present case, the models are cross-validated using train and test sets that reflect different stimuli (as opposed to trials); thus, stimuli in the test set are not involved in template fitting, and modeling procedures genuinely assess how well the models generalize to new stimuli. This information was provided in the Methods section ‘Additional wide-range-of-stimuli dataset’.</p><p>To the caption of <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, we add some information regarding cross-validation and refer the reader to the appropriate Methods section for details.</p><p> <italic>c) It seems a bit unfair to compare the parameterized category template model with a uniform category model. The category model has no contrast-sensitivity and thus gives a flat response across contrast in <xref ref-type="fig" rid="fig4">Figure 4</xref>. How much difference would there be between models, if you just allowed the category model to be contrast-sensitive?</italic> </p><p>We are not sure why it would be fair to allow contrast sensitivity to the uniform category model. Arguably, the whole point of the category model is that representations might depend solely on the category of a stimulus, not on low-level visual properties, nor the specific exemplar of a category that is presented. It is possible to tack on ad hoc tuning for visual properties to the category model, but we are not sure what insight this would provide.</p><p>To be clear, from a conceptual point of view, the Template model is not incompatible with the category model. One can view the Template model as a quantitative elaboration of the category model, showing how category selectivity is generated.</p><p><italic>Likewise, the category model does seem to step for phase coherence, but no description was given for this; is this because at the lower phase coherences the stimulus category cannot be discerned?</italic> </p><p>That is correct; the category assignments used in the category model are based on the subjects’ behavioral reports. This is described in the Methods. We acknowledge that the step-like phase coherence response is one aspect of the data that is not accurately predicted by the Template model (see Point 10).</p><p>To prevent confusion, we add text specifically addressing the phase coherence cases (subsection “Template model”).</p><p><italic>Also, how is it that some of the category model fits show some decrease in response as a function of contrast for the preferred category?</italic> </p><p>What is shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> are cross-validated predictions of the various models using leave-one- stimulus-out cross-validation. Thus, even though the category model is theoretically binary in nature (i.e. response for words vs. response for non-words), because of quantitative model fitting and cross-validation, it is possible for predicted values to be non-binary. This is why the category model exhibits a decrease in predicted response as a function of contrast.</p><p>We add some clarification regarding the response decrease (see <xref ref-type="fig" rid="fig2">Figure 2B</xref> caption).</p><p><italic>In general, it would be useful to move some of the supplementary material that describes the performance of other models (including for the RT data) to the main text, since the model comparisons are of central importance to this study.</italic> </p><p>We agree, and we merge all of the model cross-validation results into the main text (see <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig5">5</xref> and <xref ref-type="fig" rid="fig6">6</xref>).</p><p><italic>4) IPS fit circularity. Despite the assurances in Methods, the top-down IPS model faces a bit of a circularity problem. The IPS region was found using a search process that sought signals that could account for task effects. It is therefore not surprising to learn (<xref ref-type="fig" rid="fig4">Figure 4</xref>, bottom) that these signals well fit the data. Indeed, the r=0.96 fit is a bit worrying in this regard and suggests the possibility of over-fitting. Here are some options to rectify: a) Explicitly describe the fitting as a demonstration exercise, not to be taken as independent evidence of the role of the IPS; b) Obtain data from a new set of subjects, using the IPS region identified in the initial set of subjects to replicate the result.</italic> </p><p>We agree with the reviewer’s concerns (hence the text in the Methods), though we differ in how deleterious the effects are deemed to be. Given that we are not claiming to have strong evidence for precise localization of the top-down region, we prefer the approach of using a liberal anatomical mask over the suggested strategy of functionally identifying the region in one set of subjects and using this region in a separate set of subjects.</p><p>We revise the text to explicitly acknowledge that the top-down IPS model should not be taken as independent evidence of the involvement of the IPS (“subsection “Model of top-down computations in VTC”). We also enlarge the maps in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> so that it is easier to examine the localization results.</p><p><italic>5) Consistency of model framing of parietal cortex. The parietal cortex is first modeled as the source of an attention signal that modulates VTC, and then also as an accumulator of evidence from VTC. Is this plausible, and can the fMRI data support this conclusion? For example, given high task difficulty, does strong activation of IPS imply strong neural activation to provide attentional control or weak but temporally extended neural activation to accumulate weak evidence?</italic> </p><p>Yes, our model framing implies two views of the parietal cortex. One, parietal cortex is viewed as an accumulator of evidence. Two, it is also viewed as providing top-down modulation of VTC. We see no reason that these two roles cannot co-exist. Of course, this characterization of parietal cortex could be strengthened by additional experimental evidence. In particular, the reviewer alludes to the issue of temporal dynamics, which we agree is very interesting to consider (and is discussed in paragraph five of the Discussion section). We imagine that during high task difficulty, IPS activity is temporally extended to accumulate weak evidence and that this in turns induces temporally extended top-down modulation of VTC.</p><p>We revise the caption of <xref ref-type="fig" rid="fig6">Figure 6D</xref> in order to clarify the architecture of our model.</p><p><italic>6) Parietal cortex correlation analysis. The suggested effect of attention is posited to be a gain effect – yet when looking for the source of this gain signal, the fixation condition is subtracted off. If it really is a gain, subtraction would be expected to leave some stimulus-driven effect in the other conditions, and correlations to parietal cortex could be due in part to this residual stimulus representation.</italic> </p><p>The reviewer is correct that the correlation procedure involves subtraction of fixation responses whereas the top-down effect is more accurately described as a gain-like effect.</p><p>The correlation procedure is primarily intended to localize the cortical region involved in the top-down modulation, as opposed to providing a precise quantification of the top-down effect. The advantage of correlation is that it is robust to measurement noise and is computationally efficient. On the other hand, it is technically challenging to produce a robust and efficient method that characterizes multiplicative interactions (such a method would presumably involve some sort of regularization as well as nonlinear optimization for every voxel in the brain).</p><p>We point out that quantitative examination of the top-down effect is provided in <xref ref-type="fig" rid="fig5">Figure 5</xref>. There, we directly compare multiplicative and additive mechanisms, and it is shown that having the IPS interact multiplicatively (IPS-scaling model) leads to better cross-validated predictions than when it interacts additively (IPS-additive model).</p><p>We revise the manuscript to discuss these issues (subsections “Task-based functional connectivity” and “Model of top-down computations in VTC”).</p><p><italic>7) Correlation is not causation. A correlation with parietal cortex and VTC modulation is reported and interpreted as a causative signal from IPS areas. But the analysis is a correlation; it does not prove which way causation goes.</italic></p><p>We agree with the reviewer.</p><p>To improve clarity, we revise the paper to explicitly state that the correlational results do not in themselves prove causation but that we are imposing an interpretation of the results in the context of a computational model (Discussion section).</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p><italic>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</italic> </p><p><italic>1) The new discussion that makes connections to existing literature on top-down effects on contrast sensitivity is a welcome addition (Discussion section paragraph four). It would likely benefit readers to ascribe specific references to the various models discussed.</italic> </p><p>As suggested, we insert a key reference for each model discussed (Discussion section paragraph four).</p><p><italic>2) The cross-validation model comparisons could also be described better. For example, <xref ref-type="fig" rid="fig5">Figure 5C</xref> shows 10 models + 2 benchmarks in the fig, but only 7 are described (briefly) in the text.</italic> </p><p>We include additional description of the model comparisons of <xref ref-type="fig" rid="fig5">Figure 5C</xref> (subsection “Model of top-down computations in VTC”).</p><p><italic>It would also be useful to describe the number of free parameters in each model. How do goodness of fits compare when taking into account the different degrees of freedom?</italic> </p><p>We agree that reporting the numbers of parameters is useful for describing the models, and we clarify this issue in the Methods section.</p><p>There are multiple methods for assessing model accuracy. One method, alluded to by the reviewer, is to use simple goodness-of-fit and penalize models based on numbers of free parameters. Another method is to use cross-validation. We have chosen to use cross-validation in our study as it is simple and makes minimal assumptions about the nature of the noise distribution. Since cross-validation already controls for model complexity, the performances of the models in our study can be directly compared.</p><p>We include detail on the numbers of parameters for all of the models used in this study. Also, we explicitly discuss the issue of model complexity and cross-validation (subsection “Computational modelling”).</p><p><italic>3) <xref ref-type="fig" rid="fig5">Figure 5</xref>: please indicate meaning of colors, arrows, etc. in the legend, not just the associated text.</italic> </p><p>We add the requested description to the figure legend.</p><p><italic>4) DDM (<xref ref-type="fig" rid="fig6">Figure 6</xref>): It would be helpful to readers to clarify the terminology related to the decision model. A DDM has a single decision variable that has Brownian-like dynamics; here the model appears to be closer to a race between linear ballistic accumulators (three in the case of the 3AFC categorization task).</italic> </p><p>We thank the reviewer for this pointer. We appreciate the distinctions now, having reviewed the relevant literature.</p><p>We clarify how our model relates to existing models of evidence accumulation (<xref ref-type="fig" rid="fig6">Figure 6</xref> and subsection “Drift diffusion model”).</p><p><italic>It also would be useful to discuss these results in the context of a host of findings that have largely highlighted how difficult it is to use slow BOLD signals to make inferences about accumulator-like activity occurring over relatively short timescales, as in this study (see a nice discussion of these issues in Krueger et al., 2017, &quot;Evidence accumulation detected in BOLD signal using slow perceptual decision making,&quot; J Neurosci Methods).</italic> </p><p>Given that the BOLD signal can be interpreted as a convolution of a sluggish hemodynamic response function with neural activity dynamics, it can be expected that although relatively small differences in neural activity durations (e.g. ranging from 0–2 s) will not show up very well in the temporal shape of the BOLD response, they will show up in BOLD response amplitudes, as previously noted (Kayser et al., 2010).</p><p>We agree that there may be special experimental tricks that can be used to glean information regarding accumulator-like activity in the shape of the BOLD response, as discussed in the paper mentioned by the reviewer.</p><p>We explicitly address the issue of neural activity dynamics and its expected impact on the BOLD response (subsection “Model of perceptual decision-making in IPS”). We also insert some additional references to the work of Kayser and colleagues.</p></body></sub-article></article>