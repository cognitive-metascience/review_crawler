<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">50804</article-id><article-id pub-id-type="doi">10.7554/eLife.50804</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Energy efficient synaptic plasticity</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-152144"><name><surname>Li</surname><given-names>Ho Ling</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5654-0183</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-14680"><name><surname>van Rossum</surname><given-names>Mark CW</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6525-6814</contrib-id><email>mark.vanrossum@nottingham.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution>School of Psychology, University of Nottingham</institution><addr-line><named-content content-type="city">Nottingham</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>School of Mathematical Sciences, University of Nottingham</institution><addr-line><named-content content-type="city">Nottingham</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>13</day><month>02</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e50804</elocation-id><history><date date-type="received" iso-8601-date="2019-08-03"><day>03</day><month>08</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-02-10"><day>10</day><month>02</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Li and van Rossum</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Li and van Rossum</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-50804-v2.pdf"/><abstract><p>Many aspects of the brain’s design can be understood as the result of evolutionary drive toward metabolic efficiency. In addition to the energetic costs of neural computation and transmission, experimental evidence indicates that synaptic plasticity is metabolically demanding as well. As synaptic plasticity is crucial for learning, we examine how these metabolic costs enter in learning. We find that when synaptic plasticity rules are naively implemented, training neural networks requires extremely large amounts of energy when storing many patterns. We propose that this is avoided by precisely balancing labile forms of synaptic plasticity with more stable forms. This algorithm, termed synaptic caching, boosts energy efficiency manifold and can be used with any plasticity rule, including back-propagation. Our results yield a novel interpretation of the multiple forms of neural synaptic plasticity observed experimentally, including synaptic tagging and capture phenomena. Furthermore, our results are relevant for energy efficient neuromorphic designs.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>The brain expends a lot of energy. While the organ accounts for only about 2% of a person’s bodyweight, it is responsible for about 20% of our energy use at rest. Neurons use some of this energy to communicate with each other and to process information, but much of the energy is likely used to support learning. A study in fruit flies showed that insects that learned to associate two stimuli and then had their food supply cut off, died 20% earlier than untrained flies. This is thought to be because learning used up the insects’ energy reserves.</p><p>If learning a single association requires so much energy, how does the brain manage to store vast amounts of data? Li and van Rossum offer an explanation based on a computer model of neural networks. The advantage of using such a model is that it is possible to control and measure conditions more precisely than in the living brain.</p><p>Analysing the model confirmed that learning many new associations requires large amounts of energy. This is particularly true if the memories must be stored with a high degree of accuracy, and if the neural network contains many stored memories already. The reason that learning consumes so much energy is that forming long-term memories requires neurons to produce new proteins. Using the computer model, Li and van Rossum show that neural networks can overcome this limitation by storing memories initially in a transient form that does not require protein synthesis. Doing so reduces energy requirements by as much as 10-fold.</p><p>Studies in living brains have shown that transient memories of this type do in fact exist. The current results hence offer a hypothesis as to how the brain can learn in a more energy efficient way. Energy consumption is thought to have placed constraints on brain evolution. It is also often a bottleneck in computers. By revealing how the brain encodes memories energy efficiently, the current findings could thus also inspire new engineering solutions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>synaptic plasticity</kwd><kwd>computational models</kwd><kwd>metabolism</kwd><kwd>synaptic consolidation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000275</institution-id><institution>Leverhulme Trust</institution></institution-wrap></funding-source><award-id>RPG-2017-404</award-id><principal-award-recipient><name><surname>van Rossum</surname><given-names>Mark CW</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/R030952/1</award-id><principal-award-recipient><name><surname>van Rossum</surname><given-names>Mark CW</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The metabolic efficiency of learning in networks is improved by using forms of synaptic plasticity with different levels of persistence.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The human brain only weighs 2% of the total body mass but is responsible for 20% of resting metabolism (<xref ref-type="bibr" rid="bib2">Attwell and Laughlin, 2001</xref>; <xref ref-type="bibr" rid="bib17">Harris et al., 2012</xref>). The brain’s energy need is believed to have shaped many aspects of its design, such as its sparse coding strategy (<xref ref-type="bibr" rid="bib23">Levy and Baxter, 1996</xref>; <xref ref-type="bibr" rid="bib22">Lennie, 2003</xref>), the biophysics of the mammalian action potential (<xref ref-type="bibr" rid="bib1">Alle et al., 2009</xref>; <xref ref-type="bibr" rid="bib11">Fohlmeister, 2009</xref>), and synaptic failure (<xref ref-type="bibr" rid="bib24">Levy and Baxter, 2002</xref>; <xref ref-type="bibr" rid="bib17">Harris et al., 2012</xref>). As the connections in the brain are adaptive, one can design synaptic plasticity rules that further reduce the energy required for information transmission, for instance by sparsifying connectivity (<xref ref-type="bibr" rid="bib35">Sacramento et al., 2015</xref>). But in addition to the costs associated to neural information processing, experimental evidence suggests that memory formation, presumably corresponding to synaptic plasticity, is itself an energetically expensive process as well (<xref ref-type="bibr" rid="bib26">Mery and Kawecki, 2005</xref>; <xref ref-type="bibr" rid="bib30">Plaçais and Preat, 2013</xref>; <xref ref-type="bibr" rid="bib18">Jaumann et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Plaçais et al., 2017</xref>).</p><p>To estimate the amount of energy required for plasticity, <xref ref-type="bibr" rid="bib26">Mery and Kawecki (2005)</xref> subjected fruit flies to associative conditioning spaced out in time, resulting in long-term memory formation. After training, the fly’s food supply was cut off. Flies exposed to the conditioning died some 20% quicker than control flies, presumably due to the metabolic cost of plasticity. Likewise, fruit flies doubled their sucrose consumption during the formation of aversive long-term memory (<xref ref-type="bibr" rid="bib29">Plaçais et al., 2017</xref>), while forcing starving fruit flies to form such memories reduced lifespan by 30% (<xref ref-type="bibr" rid="bib30">Plaçais and Preat, 2013</xref>). A massed learning protocol, where pairings are presented rapidly after one another, leads to less permanent forms of learning that don’t require protein synthesis. Notably this form of learning is energetically less costly (<xref ref-type="bibr" rid="bib26">Mery and Kawecki, 2005</xref>; <xref ref-type="bibr" rid="bib30">Plaçais and Preat, 2013</xref>). In rats (<xref ref-type="bibr" rid="bib14">Gold, 1986</xref>) and humans (<xref ref-type="bibr" rid="bib16">Hall et al., 1989</xref>, but see <xref ref-type="bibr" rid="bib3">Azari, 1991</xref>) beneficial effects of glucose on memory have been reported, although the intricate regulation of energy complicates interpretation of such experiments (<xref ref-type="bibr" rid="bib9">Craft et al., 1994</xref>).</p><p>Motivated by the experimental results, we analyze the metabolic energy required to form associative memories in neuronal networks. We demonstrate that traditional learning algorithms are metabolically highly inefficient. Therefore, we introduce a synaptic caching algorithm that is consistent with synaptic consolidation experiments, and distributes learning over transient and persistent synaptic changes. This algorithm increases efficiency manifold. Synaptic caching yields a novel interpretation to various aspects of synaptic physiology, and suggests more energy efficient neuromorphic designs.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Inefficiency of perceptron learning</title><p>To examine the metabolic energy cost associated to synaptic plasticity, we first study the perceptron. A perceptron is a single artificial neuron that attempts to binary classify input patterns. It forms the core of many artificial networks and has been used to model plasticity in cerebellar Purkinje cells. We consider the common case where the input patterns are random patterns each associated to a randomly chosen binary output. Upon presentation of a pattern, the perceptron output is calculated and compared to the desired output. The synaptic weights are modified according to the perceptron learning rule, <xref ref-type="fig" rid="fig1">Figure 1A</xref>. This is repeated until all patterns are classified correctly (<xref ref-type="bibr" rid="bib34">Rosenblatt, 1962</xref>, see Materials and methods). Typically, the learning takes multiple iterations over the whole dataset (’epochs’).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Energy efficiency of perceptron learning.</title><p>(<bold>a</bold>) A perceptron cycles through the patterns and updates its synaptic weights until all patterns produce their correct target output. (<bold>b</bold>) During learning the synaptic weights follow approximately a random walk (red path) until they find the solution (grey region). The energy consumed by the learning corresponds to the total length of the path (under the <inline-formula><mml:math id="inf1"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> norm). (<bold>c</bold>) The energy required to train the perceptron diverges when storing many patterns (red curve). The minimal energy required to reach the correct weight configuration is shown for comparison (green curve). (<bold>d</bold>) The inefficiency, defined as the ratio between actual and minimal energy plotted in panel c, diverges as well (black curve). The overlapping blue curve corresponds to the theory, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> in the text.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50804-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Energy inefficiency as a function of exponent <inline-formula><mml:math id="inf2"><mml:mi>α</mml:mi></mml:math></inline-formula> in the energy function.</title><p>The energy inefficiency of perceptron learning for various energy variants. The energy inefficiency of perceptron learning when the energy associated to synaptic update is <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and the exponent <inline-formula><mml:math id="inf4"><mml:mi>α</mml:mi></mml:math></inline-formula> is varied (green curve). The case <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> is used throughout the main text. The inefficiency is the ratio between the energy needed to train the perceptron and the energy required to set the weights directly to their final value. When <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the energy is equal to the number of updates made. When <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the energy is the sum of individual update amounts. When <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>α</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> it costs less energy to make many small weight updates compared to one large one. When <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>≿</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, this effect is so strong that even the random walk of the perceptron is less costly than directly setting the weights to their final value. We consider <inline-formula><mml:math id="inf10"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>α</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to be the biologically relevant regime. Also shown is the inefficiency when only potentiation costs energy, and depression comes at no cost that is <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>α</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> (overlapping cyan curve). This has virtually identical (in)efficiency.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50804-fig1-figsupp1-v2.tif"/></fig></fig-group><p>As it is not well known how much metabolic energy is required to modify a biological synapse, and how this depends on the amount of change and the sign of the change, we propose a parsimonious model. We assume that the metabolic energy for every modification of a synaptic weight is proportional to the amount of change, no matter if this is positive or negative. The total metabolic cost <inline-formula><mml:math id="inf12"><mml:mi>M</mml:mi></mml:math></inline-formula> (in arbitrary units) to train a perceptron is the sum over the weight changes of synapses<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>perc</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf13"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of synapses, <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> denotes the synaptic weight at synapse <inline-formula><mml:math id="inf15"><mml:mi>i</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf16"><mml:mi>T</mml:mi></mml:math></inline-formula> is the total number of time-steps required to learn the classification. The exponent <inline-formula><mml:math id="inf17"><mml:mi>α</mml:mi></mml:math></inline-formula> is set to one, but our results below are similar whenever <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>α</mml:mi><mml:mo>≲</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. As there is evidence that synaptic depression involves different pathways than synaptic potentiation (e.g. <xref ref-type="bibr" rid="bib15">Hafner et al., 2019</xref>), we also tried a variant of the cost function where only potentiation costs energy and depression does not. This does not change our results, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>.</p><p>Learning can be understood as a search in the space of synaptic weights for a weight vector that leads to correct classification of all patterns, <xref ref-type="fig" rid="fig1">Figure 1B</xref>. The synaptic weights approximately follow a random walk (Materials and methods), and the metabolic cost is proportional to the length of this walk under the <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> norm, <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. The perceptron learning rule is energy inefficient, because repeatedly, weight modifications made to correctly classify one pattern are partly undone when learning another pattern. However, as both processes require energy, this is inefficient.</p><p>The energy required by the perceptron learning rule depends on the number of patterns <inline-formula><mml:math id="inf20"><mml:mi>P</mml:mi></mml:math></inline-formula> to be classified. The set of correct weights spans a cone in <inline-formula><mml:math id="inf21"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional space (grey region in <xref ref-type="fig" rid="fig1">Figure 1B</xref>). As the number of patterns to be classified increases, the cone containing correct weights shrinks and the random walk becomes longer (<xref ref-type="bibr" rid="bib13">Gardner, 1987</xref>). Near the critical capacity of the perceptron (<inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), the number of epochs required diverges as <inline-formula><mml:math id="inf23"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, <xref ref-type="bibr" rid="bib27">Opper (1988)</xref>. The energy required, which is proportional to the number of updates that the weights undergo, follows a similar behavior, <xref ref-type="fig" rid="fig1">Figure 1C</xref>.</p><p>It is useful to consider the theoretical minimal energy required to classify all patterns. The most energy efficient algorithm would somehow directly set the synaptic weights to their desired final values. Geometrically, the random walk trajectory of the synaptic weights to the target is replaced by a path straight to the correct weights (green arrow in <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Given the initial weights <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the final weights <inline-formula><mml:math id="inf25"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the energy required in this idealized case is<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>While the minimal energy also grows with memory load (Materials and methods), it increases less steeply, <xref ref-type="fig" rid="fig1">Figure 1C</xref>.</p><p>We express the metabolic efficiency of a learning algorithm as the ratio between the energy the algorithm requires and the minimal energy (the gap between the two log-scale curves in <xref ref-type="fig" rid="fig1">Figure 1C</xref>). As the number of patterns increases, the inefficiency of the perceptron rule rapidly grows as (see Materials and methods)<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mfrac><mml:msub><mml:mi>M</mml:mi><mml:mi>perc</mml:mi></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which fits the simulations very well, <xref ref-type="fig" rid="fig1">Figure 1D</xref>, black curve and dashed blue curve.</p><p>There is evidence that both cerebellar and cortical neurons are operating close to their maximal memory capacity (<xref ref-type="bibr" rid="bib6">Brunel et al., 2004</xref>; <xref ref-type="bibr" rid="bib7">Brunel, 2016</xref>). Indeed, it would appear wasteful if this were not the case. However, the above result demonstrates that for instance classifying 1900 patterns by a neuron with 1000 synapses with the traditional perceptron learning requires about ∼900 times more energy than minimally required. As the fruit-fly experiments indicate that even storing a single association in long-term memory is already metabolically expensive, storing many memories would thus require very large amounts of energy if the biology would naively implement these learning rules.</p></sec><sec id="s2-2"><title>Synaptic caching</title><p>How can the conflicting demands of energy efficiency and high storage capacity be met? The minimal energy argument presented above suggests a way to increase energy efficiency. There are forms of plasticity –﻿ anesthesia-resistant memory in flies and early-LTP/LTD in mammals –﻿ that decay and do not require protein synthesis. Such transient synaptic changes can be induced using a massed, instead of a spaced, stimulus presentation protocol. Fruit-fly experiments show that this form of plasticity is much less energy-demanding than long-term memory (<xref ref-type="bibr" rid="bib26">Mery and Kawecki, 2005</xref>; <xref ref-type="bibr" rid="bib30">Plaçais and Preat, 2013</xref>; <xref ref-type="bibr" rid="bib29">Plaçais et al., 2017</xref>). In mammals, there is evidence that synaptic consolidation, but not transient plasticity, is suppressed under low-energy conditions (<xref ref-type="bibr" rid="bib31">Potter et al., 2010</xref>). Inspired by these findings, we propose that the transient form of plasticity constitutes a synaptic variable that accumulates the synaptic changes across multiple updates in a less expensive transient form of memory; only occasionally the changes are consolidated. We call this <italic>synaptic caching.</italic></p><p>Specifically, we assume that each synapse is comprised of a transient component <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and a persistent component <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. The total synaptic weight is their sum, <inline-formula><mml:math id="inf28"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. We implement synaptic caching as follows, <xref ref-type="fig" rid="fig2">Figure 2A</xref>: For every presented pattern, changes in the synaptic strength are calculated according to the perceptron rule and are accumulated in the transient component that decays exponentially to zero. If, however, the absolute value of the transient component of a synapse exceeds a certain consolidation threshold, all synapses of that neuron are consolidated (vertical dashed line in <xref ref-type="fig" rid="fig2">Figure 2A</xref>); the value of the transient component is added to the persistent weight; and the transient weight is reset to zero.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Synaptic caching algorithm.</title><p>(<bold>a</bold>) Changes in the synaptic weights are initially stored in metabolically cheaper transient decaying weights. Here, two example weight traces are shown (blue and magenta). The total synaptic weight is composed of transient and persistent forms. Whenever any of the transient weights exceed the consolidation threshold, the weights become persistent and the transient values are reset (vertical dashed line). The corresponding energy consumed during the learning process consists of two terms: the energy cost of maintenance is assumed to be proportional to the magnitude of the transient weight (shaded area in top traces); energy cost for consolidation is incurred at consolidation events. (<bold>b</bold>) The total energy is composed of the energy to occasionally consolidate and the energy to support transient plasticity. Here, it is minimal for an intermediate consolidation threshold. (<bold>c</bold>) The amount of energy required for learning with synaptic caching, in the absence of decay of the transient weights (black curve). When there is no decay and no maintenance cost, the energy equals the minimal one (green line) and the efficiency gain is maximal. As the maintenance cost increases, the optimal consolidation threshold decreases (lower panel) and the total energy required increases, until no efficiency is gained at all by synaptic caching.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50804-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Synaptic caching in a spiking neuron with a biologically plausible perceptron-like learning rule.</title><p>To demonstrate the generality of our results, independent of learning rule or implementation, we implement a spiking biophysical perceptron. <xref ref-type="bibr" rid="bib10">D'Souza et al. (2010)</xref> proposed perceptron-like learning by combining synaptic spike-time dependent plasticity (STDP) with spike-frequency adaptation (SFA). In their model, the leaky integrate-and-fire neuron receives auditory input and delayed visual input. The neuron’s objective is to balance its auditory response <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝒘</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">𝒙</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to its visual response <inline-formula><mml:math id="inf30"><mml:mi>V</mml:mi></mml:math></inline-formula> by adjusting the weights <inline-formula><mml:math id="inf31"><mml:mi mathvariant="bold-italic">𝒘</mml:mi></mml:math></inline-formula> of its auditory synapses through STDP. The visual input is the supervisory signal. We use 100 auditory inputs, and measure the energy for the neuron to learn <inline-formula><mml:math id="inf32"><mml:mi mathvariant="bold-italic">𝒘</mml:mi></mml:math></inline-formula> so that each auditory input pattern becomes associated to a (binary) visual input. We repeatedly present patterns <inline-formula><mml:math id="inf33"><mml:msup><mml:mi mathvariant="bold-italic">𝒙</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>, each with two activated auditory inputs until <inline-formula><mml:math id="inf34"><mml:mi mathvariant="bold-italic">𝒘</mml:mi></mml:math></inline-formula> stabilized as D’Souza et al. The training is considered successful if the auditory responses of all the input patterns associated to the same binary visual input fall within two standard deviations from the mean auditory response of those patterns, and are at least five standard deviations away from the mean auditory response of other patterns. Synaptic caching is implemented as in the main text by splitting <inline-formula><mml:math id="inf35"><mml:mi mathvariant="bold-italic">𝒘</mml:mi></mml:math></inline-formula> into persistent forms and transient forms. We consider the optimal scenario where the transient weights do not decay and have no maintenance cost. Also in the biophysical implementation of perceptron learning, synaptic caching (green curve) saves a significant amount of energy compared to without caching (red curve), suggesting that synaptic caching works universally regardless of learning algorithm or biophysical implementation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50804-fig2-figsupp1-v2.tif"/></fig></fig-group><p>The efficiency gain of synaptic caching depends on the limitations of transient plasticity. If the transient synaptic component could store information indefinitely at no metabolic cost, consolidation could be postponed until the end of learning and the energy would equal the minimal energy <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. Hence the efficiency gain would be maximal. However, we assume that the efficiency gain of synaptic caching is limited because of two effects: (1) The transient component decays exponentially (with a time-constant <inline-formula><mml:math id="inf36"><mml:mi>τ</mml:mi></mml:math></inline-formula>). (2) There might be a maintenance cost associated to maintaining the transient component. Biophysically, transient plasticity might correspond to an increased/decreased vesicle release rate (<xref ref-type="bibr" rid="bib28">Padamsey and Emptage, 2014</xref>; <xref ref-type="bibr" rid="bib8">Costa et al., 2015</xref>) so that it diverges from its optimal value (<xref ref-type="bibr" rid="bib24">Levy and Baxter, 2002</xref>).</p><p>To estimate the energy saved by synaptic caching, we assume that the maintenance cost is proportional to the transient weight itself and incurred every time-step <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> (shaded area in the top traces of <xref ref-type="fig" rid="fig2">Figure 2A</xref>)<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>trans</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>While experiments indicate that transient plasticity is metabolically far less demanding than the persistent form, the precise value of the maintenance cost is not known. We encode it in the constant <inline-formula><mml:math id="inf38"><mml:mi>c</mml:mi></mml:math></inline-formula>; the theory also includes the case that <inline-formula><mml:math id="inf39"><mml:mi>c</mml:mi></mml:math></inline-formula> is zero. It is straightforward to include a cost term for changing the transient weight (Materials and methods); such a cost would reduce the efficiency gain attainable by synaptic caching.</p><p>Next, we need to include the energetic cost of consolidation. Currently it is unknown how different components of synaptic consolidation, such as signaling, protein synthesis, transport to the synapses and changing the synapse, contribute to this cost. We assume the metabolic cost to consolidate the synaptic weights is <inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>cons</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This is identical to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, but in contrast to standard perceptron learning where synapses are consolidated every time a weight is updated, now changes in the persistent component <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> only occur when consolidation occurs. One could add a maintenance cost term to the persistent weight as well, in that case postponing consolidation would save even more energy.</p></sec><sec id="s2-3"><title>Efficiency gain from synaptic caching</title><p>To maximize the efficiency gain achieved by synaptic caching one needs to tune the consolidation threshold, <xref ref-type="fig" rid="fig2">Figure 2B</xref>. When the threshold is low, consolidation occurs often and the energy approaches the one without synaptic caching. When on the other hand the consolidation threshold is high, the expensive consolidation process occurs rarely, but the maintenance cost of transient plasticity is high; moreover, the decay will lead to forgetting of unconsolidated memories, slowing down learning and increasing the energy cost. Thus, the consolidation energy decreases for larger thresholds, whereas the maintenance energy increases, <xref ref-type="fig" rid="fig2">Figure 2B</xref> (see Materials and methods). As a result of this trade-off, there is an optimal threshold –﻿ which depends on the decay and the maintenance cost –﻿ that balances persistent and transient forms of plasticity. To analyze the efficiency gain below, we numerically optimize the consolidation threshold.</p><p>First, we consider the case when the transient component does not decay. <xref ref-type="fig" rid="fig2">Figure 2C</xref> shows the energy required to train the perceptron. When the maintenance cost is absent (<inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), consolidation is best postponed until the end of the learning and the energy is as low as the theoretical minimal bound. As <inline-formula><mml:math id="inf43"><mml:mi>c</mml:mi></mml:math></inline-formula> increases, it becomes beneficial to consolidate more often, that is the optimal threshold decreases, <xref ref-type="fig" rid="fig2">Figure 2C</xref> bottom panel. The required energy increases until the maintenance cost becomes so high that it is better to consolidate after every update, the transient weights are not used, and no energy is saved with synaptic caching. The efficiency is well estimated by analysis presented in the Materials and methods, <xref ref-type="fig" rid="fig2">Figure 2C</xref> (theory).</p><p>Next, we consider what happens when the transient plasticity decays. We examine the energy and learning time as a function of the decay rate for various levels of maintenance cost, <xref ref-type="fig" rid="fig3">Figure 3</xref>. As stated above, if there is no decay, efficiency gain can be very high; the consolidation threshold has no impact on the learning time, <xref ref-type="fig" rid="fig3">Figure 3</xref> bottom. In the other limit, when the decay is rapid (right-most region), it is best to consolidate frequently as otherwise information is lost. As expected, the metabolic cost is high in this case.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Synaptic caching and decaying transient plasticity.</title><p>The amount of energy required, the optimal consolidation threshold, and the learning time as a function of the decay rate of transient plasticity for various values of the maintenance cost. Broadly, stronger decay will increase the energy required and hence reduce efficiency. With weak decay and small maintenance cost, the most energy-saving strategy is to accumulate as many changes in the transient forms as possible, thus increasing the learning time (darker curves). However, when maintenance cost is high, it is optimal to reduce the threshold and hence learning time. Dashed lines denote the results without synaptic caching.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50804-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>The effects of consolidation threshold on energy cost and learning time.</title><p>(<bold>a</bold>) Parametric plot of learning time vs energy while the consolidation threshold <inline-formula><mml:math id="inf44"><mml:mi>θ</mml:mi></mml:math></inline-formula> is varied. The threshold value runs from to 10 in steps of 0.5. For small maintenance costs, the threshold determines a trade-off between either a short learning time or a low energy (e.g. black curve). At higher maintenance costs, the most energy efficient threshold also leads to a short learning time. Average over 100 runs; parameter: <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. (<bold>b</bold>) Similar to the perceptron results in panel a, the effects of consolidation threshold on energy cost and learning time for training in a multi-layer network vary depending on the maintenance cost <inline-formula><mml:math id="inf46"><mml:mi>c</mml:mi></mml:math></inline-formula>. Here, the threshold starts at 0.005 and is in increments of 0.005. When <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (black dots, each representing a unique consolidation threshold), there is a trade-off between shorter learning time and lower energy cost. When <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula> (red dots), the result is similar to the perceptron result with <inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>, where optimizing learning time or energy cost leads to a similar threshold. Parameters: <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, required accuracy <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mn>0.93</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50804-fig3-figsupp1-v2.tif"/></fig></fig-group><p>The regime of intermediate decay is quite interesting. When maintenance cost is high, it is of primary importance to keep learning time short, and in fact the learning time can be lower than in a perceptron without decay, <xref ref-type="fig" rid="fig3">Figure 3</xref>, bottom, light curves. When on the other hand maintenance cost is low, the optimal solution is to set the consolidation threshold high so as to minimize the number of consolidation events, even if this means a longer learning time, <xref ref-type="fig" rid="fig3">Figure 3</xref>, bottom, dark curves.</p><p>For intermediate decay rates, the consolidation threshold trades off between learning time and energy efficiency, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>. That is, by setting the consolidation threshold the perceptron can learn either rapidly or efficiently. Such a trade-off could be of biological relevance. We found a similar trade-off in multi-layer perceptrons (see below), <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>. (although we found no evidence that learning can be sped up there).</p><p>In summary, when the transient component decays the learning dynamics is altered, and synaptic caching can not only reduce metabolic cost but can also reduce learning time.</p><p>Next, to show that synaptic caching is a general principle, we implement synaptic caching in a spiking neural network with a biologically plausible perceptron-like learning rule proposed by <xref ref-type="bibr" rid="bib10">D'Souza et al. (2010)</xref>. The optimal scenario, where the transient weights do not decay and have no maintenance cost, is assumed. The network is able to save 80% of the energy with synaptic caching, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. Hence, efficiency gains from synaptic caching do not rely on exact implementation.</p><p>In the above implementation of synaptic caching, consolidation of all synapses was triggered when transient plasticity at a single synapse exceeded a certain threshold. This resembles the synaptic tagging and capture phenomenon where plasticity induction leads to transient changes and sets a tag; only strong enough stimulation results in proteins being synthesized and being delivered to all tagged synapses, consolidating the changes (<xref ref-type="bibr" rid="bib12">Frey and Morris, 1997</xref>; <xref ref-type="bibr" rid="bib4">Barrett et al., 2009</xref>). There is a number of ways synapses could interact, <xref ref-type="fig" rid="fig4">Figure 4A</xref>. First, consolidation might be set to occur whenever transient plasticity at a synapse crosses the threshold and only that synapse is consolidated. Second, a hypothetical signal might send to the soma and consolidation of all synapses occurs once transient plasticity at any synapse crosses the threshold (used in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig5">5</xref>). Third, a hypothetical signal might be accumulated in or near the soma and consolidation of all synapses occurs once this total transient plasticity across synapses crosses the threshold. Only cases 2 and 3 are consistent with synaptic tagging and capture experiments, where consolidation of one synapse also leads to consolidation of another synapse that would otherwise decay back to baseline (<xref ref-type="bibr" rid="bib12">Frey and Morris, 1997</xref>; <xref ref-type="bibr" rid="bib36">Sajikumar et al., 2005</xref>). However, all variants lead to comparable efficiency gains, <xref ref-type="fig" rid="fig4">Figure 4B</xref>.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Comparison of various variants of the synaptic caching algorithm.</title><p>(<bold>a</bold>) Schematic representation of variants to decide when consolidation occurs. From top to bottom: (1) Consolidation (indicated by the star) occurs whenever transient plasticity at a synapse crosses the consolidation threshold and only that synapse is consolidated. (2) Consolidation of all synapses occurs once transient plasticity at any synapse crosses the threshold. (3) Consolidation of all synapses occurs once the total transient plasticity across synapses crosses the threshold. (<bold>b</bold>) Energy required to teach the perceptron is comparable across algorithm variants. Consolidation thresholds were optimized for each algorithm and each maintenance cost of transient plasticity individually. In this simulation the transient plasticity did not decay.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50804-fig4-v2.tif"/></fig><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Energy cost to train a multilayer back-propagation network to classify digits from the MNIST data set.</title><p>(<bold>a</bold>) Energy rises with the accuracy of identifying the digits from a held-out test data. Except for the larger learning rates, the energy is independent of the learning rate <inline-formula><mml:math id="inf53"><mml:mi>η</mml:mi></mml:math></inline-formula>. Inset shows some MNIST examples. (<bold>b</bold>) Comparison of energy required to train the network with/without synaptic caching, and the minimal energy. As for the perceptron and depending on the cost of transient plasticity, synaptic caching can reduce energy need manifold. (<bold>c</bold>) There is an optimal number of hidden units that minimizes metabolic cost. Both with and without synaptic caching, energy needs are high when the number of hidden units is barely sufficient or very large. Parameters for transient plasticity in (<bold>b</bold>) and (<bold>c</bold>): <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50804-fig5-v2.tif"/></fig><p>In summary, we see that synaptic caching can in principle achieve large efficiency gains, bringing efficiency close to the theoretical minimum.</p></sec><sec id="s2-4"><title>Synaptic caching in multilayer networks</title><p>Since the perceptron is a rather restrictive framework, we wondered whether the efficiency gain of synaptic caching can be transferred to multilayer networks. Therefore, we implement a multi-layer network trained with back-propagation. Back-propagation networks learn the associations of patterns by approaching the minimum of the error function through stochastic gradient descent. We use a network with one hidden layer with by default 100 units to classify hand-written digits from the MNIST dataset. As we train the network, we intermittently interrupt the learning to measure the energy consumed for plasticity thus far and measure the performance on a held-out test-set. This yields a curve relating energy to accuracy.</p><p>Similar to a perceptron, learning without synaptic caching is metabolically expensive in a back-propagation network. Until reaching maximal accuracy, energy rises approximately exponentially with accuracy, after which additional energy do not lead to further improvement. When the learning rate is sufficiently small, the metabolic cost of plasticity is independent of the learning rate. At larger learning rates, learning no longer converges and energy goes up steeply without an increase in accuracy, <xref ref-type="fig" rid="fig5">Figure 5A</xref>. With the exception of these very large rates, these results show that lowering the learning rate does not save energy.</p><p>Similar to the perceptron, we evaluate how much energy would be required to directly set the synaptic weights to their final values. Traditional learning without synaptic caching is once again energetically inefficient, expending at least ∼20 times more energy compared to this theoretical minimum whatever the desired accuracy level is, <xref ref-type="fig" rid="fig5">Figure 5B</xref>. However, by splitting the weights into persistent synaptic weights and transient synaptic caching weights, the network can save substantial amounts of energy. As for the perceptron, depending on the decay and the maintenance cost the energy ranges from as little as the minimum to as much as the energy required without caching. Thus, the efficiency gain of synaptic caching found for the perceptron carries over to multilayer networks.</p><p>It might seem that smaller networks would be metabolically less costly, because small networks simply contain fewer synapses to modify. On the other hand, we saw above that for the perceptron metabolic costs rise rapidly when cramming many patterns into it. We wondered therefore how energy cost depends on network size in the multilayer network. Since the number of input units is fixed to the image size and the number of output units equals the ten output categories, we adjust the number of hidden units.</p><p>The network fails to reach the desired accuracy if the number of hidden units is too small, <xref ref-type="fig" rid="fig5">Figure 5C</xref>. When the network size is barely above the minimum requirement, the network has to compensate the lack of hidden units with longer training time and hence a larger energy expenditure. However, very large networks also require more energy. These results show that from an energy perspective there exists an optimal number of neurons to participate in memory formation. The optimal number depends on the accuracy requirement; as expected, higher accuracies require more hidden units and energy.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Experiments on formation of a long-term memory of a single association suggest that synaptic plasticity is an energetically expensive process. We have shown that energy requirements rise steeply as memory load or designated accuracy level increase. This indicates trade-offs between energy consumption, and network capacity and performance. To improve efficiency, we have proposed an algorithm named synaptic caching that temporarily stores changes in the synaptic strength in transient forms of plasticity, and only occasionally consolidates into the persistent forms. Depending on the characteristics (decay and maintenance cost) of transient plasticity, this can lead to large energy savings in the energy required for synaptic plasticity. We stress that from an algorithmic point of view, synaptic caching can be applied to any synaptic learning algorithm (unsupervised, reinforcement, supervised) and does not have specific requirements. Further savings might be possible by adjusting the consolidation threshold as learning progresses and by being pathway-specific (<xref ref-type="bibr" rid="bib20">Leibold and Monsalve-Mercado, 2016</xref>).</p><p>The implementation of a consolidation threshold is similar to what has been observed in physiology, in particular in the synaptic tagging and capture literature (<xref ref-type="bibr" rid="bib32">Redondo and Morris, 2011</xref>). Our results thus give a novel interpretation of those findings. Synaptic consolidation is known to be affected by reward, novelty and punishment (<xref ref-type="bibr" rid="bib32">Redondo and Morris, 2011</xref>), which is compatible with a metabolic perspective as energy is expended only when the stimulus is worth remembering. In addition, our results for instance explain why consolidation is competitive, but transient plasticity is less so (<xref ref-type="bibr" rid="bib37">Sajikumar et al., 2014</xref>), namely the formation of long-term memory is precious. Consistent with this, there is evidence that encouraging consolidation increases energy consumption (<xref ref-type="bibr" rid="bib29">Plaçais et al., 2017</xref>). We also predict that the transient weight changes act as an accumulative threshold for consolidation. That is, sufficient transient plasticity should trigger consolidation, even in the absence of other consolidation triggers. Future characterization of the energy budget of synaptic plasticity should allow more precise predictions of our theory.</p><p>Combining persistent and transient storage mechanisms is a strategy well known in traditional computer systems to provide a faster and often energetically cheaper access to memory. In computer systems, permanent storage of memories typically requires transmission of all information across multiple transient cache systems until reaching a long-term storage device. The transfer of information is often a bottleneck in computer architectures and consumes considerable power in modern computers (<xref ref-type="bibr" rid="bib19">Kestor et al., 2013</xref>). However, in the nervous system transient and persistent synapses appear to exist next to each other. Local consolidation in a synapse does not require moving information. Using this setup, biology appears to have found a more efficient way to store information.</p><p>Memory stability has long fascinated researchers (<xref ref-type="bibr" rid="bib33">Richards and Frankland, 2017</xref>), and in some cases forgetting can be beneficial (<xref ref-type="bibr" rid="bib5">Brea et al., 2014</xref>). Splitting plasticity into transient and persistent forms might prevent catastrophic forgetting in networks (<xref ref-type="bibr" rid="bib21">Leimer et al., 2019</xref>). Here, we argue that the main benefit of more transient forms of plasticity is to permit the network to explore the weight space to find a desirable weight configuration using less energy. While this work focuses solely on the metabolic cost of synaptic plasticity, the brain also expends significant amounts of energy on spiking, synaptic transmission, and maintaining resting potential. Learning rules can be designed to reduce costs associated to computation once learning has finished (<xref ref-type="bibr" rid="bib35">Sacramento et al., 2015</xref>). It would be of interest to next understand the precise interaction of computation and plasticity cost during and after learning.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Energy efficiency of the perceptron</title><p>For perceptron, we can calculate the energy efficiency of both the classical perceptron and the gain achieved by synaptic caching. We first consider the case that transient plasticity does not decay, as this allows important theoretical simplifications. In the perceptron learning to classify binary patterns <xref ref-type="disp-formula" rid="equ16">Equation 8</xref>, the weight updates are either <inline-formula><mml:math id="inf57"><mml:mrow><mml:mo>+</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf58"><mml:mrow><mml:mo>-</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf59"><mml:mi>η</mml:mi></mml:math></inline-formula> is the learning rate, so that the energy spent (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) per update per synapse equals <inline-formula><mml:math id="inf61"><mml:mi>η</mml:mi></mml:math></inline-formula>. Hence the total energy spent to classify all patterns <inline-formula><mml:math id="inf62"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>perc</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf63"><mml:mi>K</mml:mi></mml:math></inline-formula> is the total number of updates. <xref ref-type="bibr" rid="bib27">Opper (1988)</xref> showed that learning time diverges as <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>K</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We found the numerator numerically to yield <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To calculate the efficiency, we compare this to the minimal energy necessary to reach the final weight vector in the perceptron. We approximate the weight trajectory followed by the perceptron algorithm by a random walk. After <inline-formula><mml:math id="inf66"><mml:mi>K</mml:mi></mml:math></inline-formula> updates of step-size <inline-formula><mml:math id="inf67"><mml:mi>η</mml:mi></mml:math></inline-formula> the weights approximate a Gaussian distribution with zero mean and variance <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. By short-cutting the random walk, the minimal energy required to reach the weight vector is <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>min</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msqrt><mml:mfrac><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mfrac></mml:msqrt><mml:mo>⁢</mml:mo><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>K</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>. Hence, we find for the inefficiency (see <xref ref-type="fig" rid="fig1">Figure 1D</xref>)<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mrow><mml:mfrac><mml:msub><mml:mi>M</mml:mi><mml:mi>perc</mml:mi></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Simulations show that the variance in the weights is actually about 20% smaller than a random walk, likely reflecting correlations in the learning process not captured in the random walk approximation. This explains most of the slight deviation in the ineffeciency between theory and simulation, <xref ref-type="fig" rid="fig1">Figure 1D</xref>.</p></sec><sec id="s4-2"><title>Efficiency of synaptic caching</title><p>To calculate the efficiency gained with synaptic caching, we need to calculate both the consolidation energy and the maintenance energy. The consolidation energy equals the number of consolidation events times the size of the updates. The size of the weight updates is equal to the consolidation threshold <inline-formula><mml:math id="inf70"><mml:mi>θ</mml:mi></mml:math></inline-formula>, while the number of consolidation events follows from a random walk argument as <inline-formula><mml:math id="inf71"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mo>⌈</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>η</mml:mi></mml:mrow><mml:mo>⌉</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. The ceiling function expresses the fact that when the threshold is smaller than learning rate, consolidation will always occur; we temporarily ignore this scenario. In addition, at the end of learning all remaining transient plasticity is consolidated, which requires an energy <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Assuming that the probability distribution of transient weights, <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, has reached steady state at the end of learning, it has a triangular shape (see below) and mean absolute value <inline-formula><mml:math id="inf74"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, so that the total consolidation energy<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>cons</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mi>θ</mml:mi></mml:mfrac></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The energy associated to the transient plasticity is (again assuming that <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has reached steady state)<disp-formula id="equ7"><label>(4)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>trans</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf76"><mml:mi>T</mml:mi></mml:math></inline-formula> is the number of time-steps required for learning. We find numerically that <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:math></inline-formula>. Hence the total energy when using synaptic caching is <inline-formula><mml:math id="inf78"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>cache</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>cons</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>trans</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The optimal threshold <inline-formula><mml:math id="inf79"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> is given by <inline-formula><mml:math id="inf80"><mml:mrow><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>cons</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>trans</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, or<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>at which the energy is <inline-formula><mml:math id="inf81"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>cache</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>K</mml:mi></mml:msqrt><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:msqrt></mml:mrow><mml:mo>/</mml:mo><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>. And so the efficiency of synaptic caching is <inline-formula><mml:math id="inf82"><mml:mrow><mml:mfrac><mml:msub><mml:mi>M</mml:mi><mml:mi>cache</mml:mi></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:mfrac></mml:msqrt><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>. However, as consolidation can maximally occur only once per time-step, <inline-formula><mml:math id="inf83"><mml:msub><mml:mi>M</mml:mi><mml:mi>cons</mml:mi></mml:msub></mml:math></inline-formula> cannot exceed <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>M</mml:mi><mml:mi>perc</mml:mi></mml:msub></mml:math></inline-formula> so that the inefficiency is<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mrow><mml:mfrac><mml:msub><mml:mi>M</mml:mi><mml:mi>cache</mml:mi></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt><mml:mo>,</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msqrt><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This equation reasonably matches the simulations, <xref ref-type="fig" rid="fig2">Figure 2C</xref> (labeled ’theory’).</p><p>One can include a cost for changing the transient weight, so that <inline-formula><mml:math id="inf85"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>trans</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf86"><mml:mi>b</mml:mi></mml:math></inline-formula> codes the cost of making a change. Assuming that consolidating immediately after a weight change does not incur this cost, this yields an extra term in <xref ref-type="disp-formula" rid="equ7">Equation4</xref> of <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mo>⌈</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>η</mml:mi></mml:mrow><mml:mo>⌉</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Such costs will reduce the efficiency gain achievable by synaptic caching. When <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>b</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, it is always cheaper to consolidate.</p></sec><sec id="s4-3"><title>Decaying transient plasticity</title><p>When transient plasticity decays, the situation is more complicated as the learning time depends on the strength of the decay and to our knowledge no analytical expression exists for it. However, it is still possible to estimate the <italic>power</italic>, that is the energy per time unit, for both the transient component, denoted <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>m</mml:mi><mml:mi>trans</mml:mi></mml:msub></mml:math></inline-formula>, and the consolidation component, <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>m</mml:mi><mml:mi>cons</mml:mi></mml:msub></mml:math></inline-formula>. Under the random walk approximation every time the perceptron output does not match the desired output, the transient weight <inline-formula><mml:math id="inf91"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is updated with an amount <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> drawn from a distribution <inline-formula><mml:math id="inf93"><mml:mi>Q</mml:mi></mml:math></inline-formula>, with zero mean and variance <inline-formula><mml:math id="inf94"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>. Given the update probability <inline-formula><mml:math id="inf95"><mml:mi>p</mml:mi></mml:math></inline-formula>, that is the fraction of patterns not yet classified correctly, one has <inline-formula><mml:math id="inf96"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>η</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf97"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, so that <inline-formula><mml:math id="inf98"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. We assume that the synaptic update rate decreases very slowly as learning progresses, hence <inline-formula><mml:math id="inf99"><mml:mi>p</mml:mi></mml:math></inline-formula> is quasi-stationary.</p><p>Every time-step <inline-formula><mml:math id="inf100"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> the transient weights decay with a time-constant <inline-formula><mml:math id="inf101"><mml:mi>τ</mml:mi></mml:math></inline-formula>. The synapse is consolidated and <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is reset to zero whenever the absolute value of the caching weight <inline-formula><mml:math id="inf103"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> exceeds <inline-formula><mml:math id="inf104"><mml:mi>θ</mml:mi></mml:math></inline-formula>. Given <inline-formula><mml:math id="inf105"><mml:mi>p</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf106"><mml:mi>τ</mml:mi></mml:math></inline-formula>, we would like to know: 1) how often consolidation events occur which gives consolidation power and 2) the maintenance power <inline-formula><mml:math id="inf107"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>trans</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This problem is similar to the random walk to threshold model used for integrate-and-fire neurons, but here there are two thresholds: <inline-formula><mml:math id="inf108"><mml:mi>θ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Under the assumptions of small updates and a smooth resulting distribution, the evolution of the probability distribution <inline-formula><mml:math id="inf110"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is described by the Fokker-Planck equation, which in the steady state gives<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mo>∂</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mfrac><mml:msup><mml:mo>∂</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The last term is a source term that describes the re-insertion of weights by the reset process. The boundary conditions are <inline-formula><mml:math id="inf111"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>±</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. While <inline-formula><mml:math id="inf112"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is continuous in <inline-formula><mml:math id="inf113"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, the source introduces a cusp in <inline-formula><mml:math id="inf114"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at the reset value. Conservation of probability ensures that <inline-formula><mml:math id="inf115"><mml:mi>r</mml:mi></mml:math></inline-formula> equals the outgoing flux at the boundaries. One finds<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>erfi</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>erfi</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf116"><mml:mrow><mml:mrow><mml:mi>erfi</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>erf</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf117"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>τ</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and with normalization factor<disp-formula id="equ12"><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>;</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt><mml:mi>σ</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf118"><mml:mmultiscripts><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:none/><mml:mprescripts/><mml:mn>2</mml:mn><mml:none/></mml:mmultiscripts></mml:math></inline-formula> is the generalized hypergeometric function. (In the limit of no decay this becomes a triangular distribution <inline-formula><mml:math id="inf119"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.)</p><p>We obtain maintenance power<disp-formula id="equ13"><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(5)</mml:mtext></mml:mtd><mml:mtd><mml:mrow/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mi>N</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(6)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mi>Z</mml:mi></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>θ</mml:mi><mml:mi>σ</mml:mi></mml:mrow><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt></mml:mfrac><mml:mo>−</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>For small <inline-formula><mml:math id="inf120"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula>, that is small decay, this is linear in <inline-formula><mml:math id="inf121"><mml:mi>θ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf122"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>trans</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>. It saturates for large <inline-formula><mml:math id="inf123"><mml:mi>θ</mml:mi></mml:math></inline-formula> because then the decay dominates and the threshold is hardly ever reached.</p><p>The consolidation rate follows from Fick’s law<disp-formula id="equ14"><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The consolidation power is<disp-formula id="equ15"><label>(7)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>cons</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In the limit of no decay one has <inline-formula><mml:math id="inf124"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, so that <inline-formula><mml:math id="inf125"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>cons</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Strictly speaking this approximates learning with a random walk process and assumes local consolidation, <xref ref-type="fig" rid="fig4">Figure 4A</xref>. However, <xref ref-type="disp-formula" rid="equ13 equ15">Equations 6 and 7</xref> give a good prediction of the simulation when provided with the time-varying update probability from the simulation, <xref ref-type="fig" rid="fig6">Figure 6</xref>.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Maintenance and consolidation power.</title><p>Power (energy per epoch) of the perceptron vs epoch. Solid curves are from simulation, dashed curves are the theoretical predictions, <xref ref-type="disp-formula" rid="equ13 equ15">Equations 6 and 7</xref>, with <inline-formula><mml:math id="inf126"><mml:mi>σ</mml:mi></mml:math></inline-formula> calculated by using the perceptron update rate <inline-formula><mml:math id="inf127"><mml:mi>p</mml:mi></mml:math></inline-formula> extracted from the simulation. Both powers are well described by the theory. Parameters: <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf129"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50804-fig6-v2.tif"/></fig></sec><sec id="s4-4"><title>Simulations</title><sec id="s4-4-1"><title>Perceptron</title><p>Unless stated otherwise, we use a perceptron with <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula> input units to classify <inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> random binary (±1 with equal probability) input patterns <inline-formula><mml:math id="inf133"><mml:msup><mml:mi mathvariant="bold-italic">𝒙</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>, each to be associated to a randomly assigned desired binary output <inline-formula><mml:math id="inf134"><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>. Each input unit is connected with a weight <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> signifying the strength of the connection. An ’always-on’ bias unit with corresponding weight is included to adjust the threshold of the perceptron. The perceptron output <inline-formula><mml:math id="inf136"><mml:mi>y</mml:mi></mml:math></inline-formula> of a pattern is determined by the Heaviside step function <inline-formula><mml:math id="inf137"><mml:mi mathvariant="normal">Θ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf138"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">w</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="bold-italic">𝒙</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. If for a given pattern <inline-formula><mml:math id="inf139"><mml:mi>p</mml:mi></mml:math></inline-formula>, the output does not match the desired pattern output, <inline-formula><mml:math id="inf140"><mml:mi mathvariant="bold-italic">𝒘</mml:mi></mml:math></inline-formula> is adjusted according to<disp-formula id="equ16"><label>(8)</label><mml:math id="m16"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the learning rate <inline-formula><mml:math id="inf141"><mml:mi>η</mml:mi></mml:math></inline-formula> can be set to one without loss of generality. The perceptron algorithm cycles through all patterns until classified correctly. In principle, the magnitude of the weight vector, and hence the minimal energy, can be arbitrarily small for a noise-free binary perceptron. However, this paradox is resolved as soon as robustness to any post-synaptic noise is required.</p></sec><sec id="s4-4-2"><title>Multilayer networks</title><p>For the multilayer networks trained on MNIST, we use networks with one hidden layer, logistic units, and one-hot encoding at the output. Weights are updated according to the mean squared error back-propagation rule without regularization.</p><p>Simulation scripts for both the perceptron and the multilayer network can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/vanrossumlab/li_vanrossum_19">https://github.com/vanrossumlab/li_vanrossum_19</ext-link>. (<xref ref-type="bibr" rid="bib25">Li and van Rossum, 2020</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/li_vanrossum_19">https://github.com/elifesciences-publications/li_vanrossum_19</ext-link>).</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This project is supported by the Leverhulme Trust with grant number RPG-2017–404. MvR is supported by Engineering and Physical Sciences Research Council (EPSRC) grant EP/R030952/1. We would like to thank Joao Sacramento and Simon Laughlin for discussion and inputs.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Investigation</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Supervision, Investigation</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-50804-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Simulation scripts can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/vanrossumlab/li_vanrossum_19">https://github.com/vanrossumlab/li_vanrossum_19</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/li_vanrossum_19">https://github.com/elifesciences-publications/li_vanrossum_19</ext-link>).</p><p>The following previously published dataset was used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Cortes</surname><given-names>C</given-names></name><name><surname>Burges</surname><given-names>CJC</given-names></name></person-group><year iso-8601-date="1999">1999</year><data-title>Data from: The MNIST database of handwritten digits</data-title><source>The MNIST database of handwritten digits</source><pub-id assigning-authority="other" pub-id-type="archive" xlink:href="http://yann.lecun.com/exdb/mnist/">mnist</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alle</surname> <given-names>H</given-names></name><name><surname>Roth</surname> <given-names>A</given-names></name><name><surname>Geiger</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Energy-efficient action potentials in hippocampal mossy fibers</article-title><source>Science</source><volume>325</volume><fpage>1405</fpage><lpage>1408</lpage><pub-id pub-id-type="doi">10.1126/science.1174331</pub-id><pub-id pub-id-type="pmid">19745156</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attwell</surname> <given-names>D</given-names></name><name><surname>Laughlin</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An energy budget for signaling in the grey matter of the brain</article-title><source>Journal of Cerebral Blood Flow &amp; Metabolism</source><volume>21</volume><fpage>1133</fpage><lpage>1145</lpage><pub-id pub-id-type="doi">10.1097/00004647-200110000-00001</pub-id><pub-id pub-id-type="pmid">11598490</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azari</surname> <given-names>NP</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Effects of glucose on memory processes in young adults</article-title><source>Psychopharmacology</source><volume>105</volume><fpage>521</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1007/BF02244373</pub-id><pub-id pub-id-type="pmid">1771220</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname> <given-names>AB</given-names></name><name><surname>Billings</surname> <given-names>GO</given-names></name><name><surname>Morris</surname> <given-names>RG</given-names></name><name><surname>van Rossum</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>State based model of long-term potentiation and synaptic tagging and capture</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000259</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000259</pub-id><pub-id pub-id-type="pmid">19148264</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brea</surname> <given-names>J</given-names></name><name><surname>Urbanczik</surname> <given-names>R</given-names></name><name><surname>Senn</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A normative theory of forgetting: lessons from the fruit fly</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003640</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003640</pub-id><pub-id pub-id-type="pmid">24901935</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname> <given-names>N</given-names></name><name><surname>Hakim</surname> <given-names>V</given-names></name><name><surname>Isope</surname> <given-names>P</given-names></name><name><surname>Nadal</surname> <given-names>JP</given-names></name><name><surname>Barbour</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Optimal information storage and the distribution of synaptic weights: perceptron versus purkinje cell</article-title><source>Neuron</source><volume>43</volume><fpage>745</fpage><lpage>757</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.08.023</pub-id><pub-id pub-id-type="pmid">15339654</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Is cortical connectivity optimized for storing information?</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>749</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1038/nn.4286</pub-id><pub-id pub-id-type="pmid">27065365</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname> <given-names>RP</given-names></name><name><surname>Froemke</surname> <given-names>RC</given-names></name><name><surname>Sjöström</surname> <given-names>PJ</given-names></name><name><surname>van Rossum</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Unified pre- and postsynaptic long-term plasticity enables reliable and flexible learning</article-title><source>eLife</source><volume>4</volume><elocation-id>e09457</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.09457</pub-id><pub-id pub-id-type="pmid">26308579</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craft</surname> <given-names>S</given-names></name><name><surname>Murphy</surname> <given-names>C</given-names></name><name><surname>Wemstrom</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Glucose effects on complex memory and nonmemory tasks: the influence of age, sex, and glucoregulatory response</article-title><source>Psychobiology</source><volume>22</volume><fpage>95</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.3758/BF03327086</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D'Souza</surname> <given-names>P</given-names></name><name><surname>Liu</surname> <given-names>SC</given-names></name><name><surname>Hahnloser</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perceptron learning rule derived from spike-frequency adaptation and spike-time-dependent plasticity</article-title><source>PNAS</source><volume>107</volume><fpage>4722</fpage><lpage>4727</lpage><pub-id pub-id-type="doi">10.1073/pnas.0909394107</pub-id><pub-id pub-id-type="pmid">20167805</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fohlmeister</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A nerve model of greatly increased energy-efficiency and encoding flexibility over the Hodgkin-Huxley model</article-title><source>Brain Research</source><volume>1296</volume><fpage>225</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2009.06.101</pub-id><pub-id pub-id-type="pmid">19596283</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frey</surname> <given-names>U</given-names></name><name><surname>Morris</surname> <given-names>RG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Synaptic tagging and long-term potentiation</article-title><source>Nature</source><volume>385</volume><fpage>533</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/385533a0</pub-id><pub-id pub-id-type="pmid">9020359</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Maximum storage capacity in neural networks</article-title><source>Europhysics Letters</source><volume>4</volume><fpage>481</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1209/0295-5075/4/4/016</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Glucose modulation of memory storage processing</article-title><source>Behavioral and Neural Biology</source><volume>45</volume><fpage>342</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1016/S0163-1047(86)80022-X</pub-id><pub-id pub-id-type="pmid">3718398</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafner</surname> <given-names>AS</given-names></name><name><surname>Donlin-Asp</surname> <given-names>PG</given-names></name><name><surname>Leitch</surname> <given-names>B</given-names></name><name><surname>Herzog</surname> <given-names>E</given-names></name><name><surname>Schuman</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Local protein synthesis is a ubiquitous feature of neuronal pre- and postsynaptic compartments</article-title><source>Science</source><volume>364</volume><elocation-id>eaau3644</elocation-id><pub-id pub-id-type="doi">10.1126/science.aau3644</pub-id><pub-id pub-id-type="pmid">31097639</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname> <given-names>JL</given-names></name><name><surname>Gonder-Frederick</surname> <given-names>LA</given-names></name><name><surname>Chewning</surname> <given-names>WW</given-names></name><name><surname>Silveira</surname> <given-names>J</given-names></name><name><surname>Gold</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Glucose enhancement of performance on memory tests in young and aged humans</article-title><source>Neuropsychologia</source><volume>27</volume><fpage>1129</fpage><lpage>1138</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(89)90096-1</pub-id><pub-id pub-id-type="pmid">2812297</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>JJ</given-names></name><name><surname>Jolivet</surname> <given-names>R</given-names></name><name><surname>Attwell</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Synaptic energy use and supply</article-title><source>Neuron</source><volume>75</volume><fpage>762</fpage><lpage>777</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.019</pub-id><pub-id pub-id-type="pmid">22958818</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaumann</surname> <given-names>S</given-names></name><name><surname>Scudelari</surname> <given-names>R</given-names></name><name><surname>Naug</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Energetic cost of learning and memory can cause cognitive impairment in honeybees</article-title><source>Biology Letters</source><volume>9</volume><elocation-id>20130149</elocation-id><pub-id pub-id-type="doi">10.1098/rsbl.2013.0149</pub-id><pub-id pub-id-type="pmid">23784929</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kestor</surname> <given-names>G</given-names></name><name><surname>Gioiosa</surname> <given-names>R</given-names></name><name><surname>Kerbyson</surname> <given-names>DJ</given-names></name><name><surname>Hoisie</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Quantifying the energy cost of data movement in scientific applications</article-title><conf-name>IEEE International Symposium on Workload Characterization (IISWC)</conf-name><pub-id pub-id-type="doi">10.1109/IISWC.2013.6704670</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leibold</surname> <given-names>C</given-names></name><name><surname>Monsalve-Mercado</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Asymmetry of neuronal combinatorial codes arises from minimizing synaptic weight change</article-title><source>Neural Computation</source><volume>28</volume><fpage>1527</fpage><lpage>1552</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00854</pub-id><pub-id pub-id-type="pmid">27348595</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Leimer</surname> <given-names>P</given-names></name><name><surname>Herzog</surname> <given-names>M</given-names></name><name><surname>Senn</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Synaptic weight decay with selective consolidation enables fast learning without catastrophic forgetting</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/613265</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lennie</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The cost of cortical computation</article-title><source>Current Biology</source><volume>13</volume><fpage>493</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1016/S0960-9822(03)00135-0</pub-id><pub-id pub-id-type="pmid">12646132</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname> <given-names>WB</given-names></name><name><surname>Baxter</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Energy efficient neural codes</article-title><source>Neural Computation</source><volume>8</volume><fpage>531</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1162/neco.1996.8.3.531</pub-id><pub-id pub-id-type="pmid">8868566</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname> <given-names>WB</given-names></name><name><surname>Baxter</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Energy-efficient neuronal computation via quantal synaptic failures</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>4746</fpage><lpage>4755</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-11-04746.2002</pub-id><pub-id pub-id-type="pmid">12040082</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>HL</given-names></name><name><surname>van Rossum</surname> <given-names>MCW</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>li_vanrossum_19</data-title><source>GitHub</source><version designator="9fe7761">9fe7761</version><ext-link ext-link-type="uri" xlink:href="https://github.com/vanrossumlab/li_vanrossum_19">https://github.com/vanrossumlab/li_vanrossum_19</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mery</surname> <given-names>F</given-names></name><name><surname>Kawecki</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A cost of long-term memory in <italic>Drosophila</italic></article-title><source>Science</source><volume>308</volume><elocation-id>1148</elocation-id><pub-id pub-id-type="doi">10.1126/science.1111331</pub-id><pub-id pub-id-type="pmid">15905396</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Opper</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Learning times of neural networks: exact solution for a PERCEPTRON algorithm</article-title><source>Physical Review A</source><volume>38</volume><fpage>3824</fpage><lpage>3826</lpage><pub-id pub-id-type="doi">10.1103/PhysRevA.38.3824</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padamsey</surname> <given-names>Z</given-names></name><name><surname>Emptage</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Two sides to long-term potentiation: a view towards reconciliation</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>369</volume><elocation-id>20130154</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0154</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plaçais</surname> <given-names>P-Y</given-names></name><name><surname>de Tredern</surname> <given-names>É</given-names></name><name><surname>Scheunemann</surname> <given-names>L</given-names></name><name><surname>Trannoy</surname> <given-names>S</given-names></name><name><surname>Goguel</surname> <given-names>V</given-names></name><name><surname>Han</surname> <given-names>K-A</given-names></name><name><surname>Isabel</surname> <given-names>G</given-names></name><name><surname>Preat</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Upregulated energy metabolism in the <italic>Drosophila</italic> mushroom body is the trigger for long-term memory</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>11510</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15510</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plaçais</surname> <given-names>PY</given-names></name><name><surname>Preat</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>To favor survival under food shortage, the brain disables costly memory</article-title><source>Science</source><volume>339</volume><fpage>440</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1126/science.1226018</pub-id><pub-id pub-id-type="pmid">23349289</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potter</surname> <given-names>WB</given-names></name><name><surname>O'Riordan</surname> <given-names>KJ</given-names></name><name><surname>Barnett</surname> <given-names>D</given-names></name><name><surname>Osting</surname> <given-names>SM</given-names></name><name><surname>Wagoner</surname> <given-names>M</given-names></name><name><surname>Burger</surname> <given-names>C</given-names></name><name><surname>Roopra</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Metabolic regulation of neuronal plasticity by the energy sensor AMPK</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e8996</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0008996</pub-id><pub-id pub-id-type="pmid">20126541</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redondo</surname> <given-names>RL</given-names></name><name><surname>Morris</surname> <given-names>RG</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Making memories last: the synaptic tagging and capture hypothesis</article-title><source>Nature Reviews Neuroscience</source><volume>12</volume><fpage>17</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1038/nrn2963</pub-id><pub-id pub-id-type="pmid">21170072</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname> <given-names>BA</given-names></name><name><surname>Frankland</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The persistence and transience of memory</article-title><source>Neuron</source><volume>94</volume><fpage>1071</fpage><lpage>1084</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.037</pub-id><pub-id pub-id-type="pmid">28641107</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rosenblatt</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1962">1962</year><chapter-title>Principles of neurodynamics: perceptrons and the theory of brain mechanisms' to Van Der Malsburg C. (1986) Frank Rosenblatt: Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms</chapter-title><person-group person-group-type="editor"><name><surname>Palm</surname> <given-names>G</given-names></name><name><surname>Aertsen</surname> <given-names>A</given-names></name></person-group><source>Brain Theory</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-70911-1_20</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sacramento</surname> <given-names>J</given-names></name><name><surname>Wichert</surname> <given-names>A</given-names></name><name><surname>van Rossum</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Energy efficient sparse connectivity from imbalanced synaptic plasticity rules</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004265</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004265</pub-id><pub-id pub-id-type="pmid">26046817</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sajikumar</surname> <given-names>S</given-names></name><name><surname>Navakkode</surname> <given-names>S</given-names></name><name><surname>Sacktor</surname> <given-names>TC</given-names></name><name><surname>Frey</surname> <given-names>JU</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Synaptic tagging and cross-tagging: the role of protein kinase mzeta in maintaining long-term potentiation but not long-term depression</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>5750</fpage><lpage>5756</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1104-05.2005</pub-id><pub-id pub-id-type="pmid">15958741</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sajikumar</surname> <given-names>S</given-names></name><name><surname>Morris</surname> <given-names>RG</given-names></name><name><surname>Korte</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Competition between recently potentiated synaptic inputs reveals a winner-take-all phase of synaptic tagging and capture</article-title><source>PNAS</source><volume>111</volume><fpage>12217</fpage><lpage>12221</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403643111</pub-id><pub-id pub-id-type="pmid">25092326</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.50804.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Senn</surname><given-names>Walter</given-names> </name><role>Reviewer</role><aff><institution>University of Bern</institution><country>Switzerland</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The authors investigate the issue of energy consumption in synapses. They show that under a strategy in which much of the information about weights is kept in temporary storage while permanent weight changes are rare, energy consumption can be reduced by an order of magnitude. There has been a great deal of work on energy consumption associated with action potentials and synaptic plasticity, but this is, to our knowledge, the first to consider energy efficiency in the context of learning. As such it fills an important gap in our understanding of synaptic plasticity. This paper should appeal to anybody who is interested either in synaptic plasticity or energy efficiency in the brain. It may also be important for learning in artificial systems, where energy costs for training networks that are small by brain standards can exceed millions of kilowatt-hours.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Energy efficient synaptic plasticity&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Ronald Calabrese as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Walter Senn (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The authors investigate the issue of energy consumption in synapses. They show that under a strategy in which much of the information about weights is kept in temporary storage while permanent weight changes are rare, energy consumption can be reduced by an order of magnitude.</p><p>This is an interesting paper that formalizes the concept of energy efficiency in learning. While work exists that considers the energy consumption of action potentials and synaptic plasticity, the formalization of energy efficiency in the context of learning is new, and worth a publication. The paper is well written and the math seems to be sound.</p><p>Essential revisions:</p><p>1) The authors introduce two components of the synaptic strength, a quickly decaying component that is transcribed into a long-lasting component when a threshold is crossed. The quickly decaying component requires less energy, and thus learning becomes a trade-off between energy efficient storage in the decaying component and energy costly storage in the sustained component.</p><p>The decay will lead to forgetting of unconsolidated memories and to a slow-down of learning, together with an increase of energy. As far as we could tell, the paper does not consider the speed of learning, and instead only asks for an energy efficient learning up to a certain degree of accuracy. The authors should provide learning curves for different consolidation thresholds and decay rates. Ideal would be a plot of energy versus learning time – presumably the lower the energy, the longer it takes to achieve a given set of accuracy, although we admit that's only a guess. On the other hand, there may be an optimal threshold.</p><p>2) Previous work [e.g., Ziegler, Zenke,.…, Gerstner, 2015, &quot;From synapses to behavioural modelling&quot;; Zenke et al., 2017, &quot;Continual Learning Through Synaptic Intelligence&quot;] has shown a benefit for the 2-stage synapses model in terms of learning and forgetting. Is there a similar benefit for this 2-stage model? This may simply be a Discussion point, referring to the analysis asked in the point (1) above.</p><p>3) From a biological point of view, it is clear that the change as well as the maintenance of synaptic weight can cost energy. Nevertheless, we find it strange that the authors analyze the perceptron learning rule according to a change-only energy cost function while the synaptic caching rule is analyzed by a combination of change (late-phase) and maintenance (early-phase) cost function. How critical is the phase-cost relation? Is it also efficient if the late-phase costs maintenance (e.g., by having a larger synaptic apparatus) and the early-phase costs energy dependent on its change? What is the energy consumption of the perceptron learning rule considering the maintenance cost function?</p><p>4) Please test for comparison the energy consumption of other learning rules performing perceptron learning (e.g., D’Souza et al., 2010).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.50804.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The authors introduce two components of the synaptic strength, a quickly decaying component that is transcribed into a long-lasting component when a threshold is crossed. The quickly decaying component requires less energy, and thus learning becomes a trade-off between energy efficient storage in the decaying component and energy costly storage in the sustained component.</p><p>The decay will lead to forgetting of unconsolidated memories and to a slow-down of learning, together with an increase of energy. As far as we could tell, the paper does not consider the speed of learning, and instead only asks for an energy efficient learning up to a certain degree of accuracy. The authors should provide learning curves for different consolidation thresholds and decay rates. Ideal would be a plot of energy versus learning time – presumably the lower the energy, the longer it takes to achieve a given set of accuracy, although we admit that's only a guess. On the other hand, there may be an optimal threshold.</p></disp-quote><p>The reviewers raise an interesting, subtle issue that we now discuss in much more detail in Results subsection “Efficiency gain from synaptic caching”. First, we first study a potential trade-off between energy and learning time in the perceptron. We now plot the relation between the learning time and the energy spend (Figure 3—figure supplement 1A). Depending on maintenance cost <italic>c</italic>, there are regimes where these form a trade-off:</p><p>When the decay is slow and <italic>c</italic> is small, the most energy-saving scenario is to accumulate as many changes as possible in the transient plasticity that still allow the perceptron to converge, resulting in long learning time. In this case there is a trade-off between learning time and energy.</p><p>However, when <italic>c</italic> is bigger, the perceptron has to choose an smaller threshold so that maintenance cost is limited. In this regime the energy-optimal threshold is close to the threshold that gives minimal learning time (Figure 3—figure supplement 1A).</p><p>Finally, when there is no decay, the energy cost depends on consolidation threshold, but the learning time does not change regardless of the threshold. Hence, there is no interaction between learning time and energy cost.</p><p>We also researched this issue in MLPs and found a similar picture: in some regimes there is a trade-off, but not in others (Figure 3—figure supplement 1B).</p><disp-quote content-type="editor-comment"><p>2) Previous work [e.g., Ziegler, Zenke,.…, Gerstner, 2015, &quot;From synapses to behavioural modelling&quot;; Zenke et al., 2017, &quot;Continual Learning Through Synaptic Intelligence&quot;] has shown a benefit for the 2-stage synapses model in terms of learning and forgetting. Is there a similar benefit for this 2-stage model? This may simply be a Discussion point, referring to the analysis asked in the point (1) above.</p></disp-quote><p>The main benefit of having 2-stage synapses is to save energy that would otherwise be expended if the network has to consolidate every single weight change. As described above, a decaying 2-state synapse model can under some circumstances reduce perceptron training time. However, we are not sure whether this generalizes to multi-layer perceptrons.</p><p>We now cite a recent study by Leimer, Herzog and Senn that shows how a similar 2-state model can prevent catastrophic forgetting (we thought this was the most relevant paper as the Zenke study does not employ a 2-state model, while Ziegler et al. do not show a functional advantage).</p><disp-quote content-type="editor-comment"><p>3) From a biological point of view, it is clear that the change as well as the maintenance of synaptic weight can cost energy. Nevertheless, we find it strange that the authors analyze the perceptron learning rule according to a change-only energy cost function while the synaptic caching rule is analyzed by a combination of change (late-phase) and maintenance (early-phase) cost function. How critical is the phase-cost relation? Is it also efficient if the late-phase costs maintenance (e.g., by having a larger synaptic apparatus) and the early-phase costs energy dependent on its change? What is the energy consumption of the perceptron learning rule considering the maintenance cost function?</p></disp-quote><p>We now discuss these two variants in detail:</p><p>If the transient component (early-phase) costs energy dependent on its change, the theory can be straightforwardly extended (see Materials and methods, subsection “Efficiency of synaptic caching”). As expected this extra cost can only diminish the benefit of synaptic caching.</p><p>If, on the other hand, there is a late-phase maintenance cost during learning, postponing consolidation will reduce costs further (see Results subsection, “Synaptic caching”).</p><p>These arguments only consider the cost of learning and not the cost of computing once the network has learned. We are currently studying this much more complicated situation. We have shown earlier that learning rules can be designed to reduce costs associated to computation (Sacramento et al., 2015), but the precise interaction of computation cost and plasticity cost is not clear at the moment.</p><disp-quote content-type="editor-comment"><p>4) Please test for comparison the energy consumption of other learning rules performing perceptron learning (e.g., D’Souza et al., 2010).</p></disp-quote><p>We have now implemented the learning rule from D’Souza et al., 2010, and find that indeed large energy savings can still be achieved (see new Figure 2—figure supplement 1), showing that the synaptic caching principle is general and does not depend on implementation details.</p></body></sub-article></article>